{"id": "46793", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=46793", "title": "Death Valley National Park", "text": "National park in California and Nevada, United States\nDeath Valley National Park is a national park of the United States that straddles the California\u2013Nevada border, east of the Sierra Nevada. The park boundaries include Death Valley, the northern section of Panamint Valley, the southern section of Eureka Valley and most of Saline Valley.\nThe park occupies an interface zone between the arid Great Basin and Mojave deserts, protecting the northwest corner of the Mojave Desert and its diverse environment of salt-flats, sand dunes, badlands, valleys, canyons and mountains.\nDeath Valley is the largest national park in the contiguous United States, as well as the hottest, driest and lowest of all the national parks in the United States. It contains Badwater Basin, the second-lowest point in the Western Hemisphere and lowest in North America at below sea level. More than 93% of the park is a designated wilderness area.\nThe park is home to many species of plants and animals which have adapted to the harsh desert environment including creosote bush, Joshua tree, bighorn sheep, coyote, and the endangered Death Valley pupfish, a survivor from much wetter times. UNESCO included Death Valley as the principal feature of its Mojave and Colorado Deserts Biosphere Reserve in 1984.\nA series of Native American groups inhabited the area from as early as 7000\u00a0BCE, most recently the Timbisha around 1000\u00a0CE who migrated between winter camps in the valleys and summer grounds in the mountains. A group of European-Americans, lost in the valley in 1849 while looking for a shortcut to the gold fields of California, gave this valley its grim name, even though only one of their group died there.\nSeveral short-lived boom towns sprang up during the late 19th and early 20th centuries to mine gold and silver. The only long-term profitable ore to be mined was borax, which was transported out of the valley with twenty-mule teams. The valley later became the subject of books, radio programs, television series, and movies. Tourism expanded in the 1920s when resorts were built around Stovepipe Wells and Furnace Creek. Death Valley National Monument was declared in 1933 and the park was substantially expanded and became a national park in 1994.\nThe natural environment of the area has been shaped largely by its geology. The valley is actually a graben with the oldest rocks being extensively metamorphosed and at least 1.7\u00a0billion years old. Ancient, warm, shallow seas deposited marine sediments until rifting opened the Pacific Ocean. Additional sedimentation occurred until a subduction zone formed off the coast. The subduction uplifted the region out of the sea and created a line of volcanoes. Later the crust started to pull apart, creating the current Basin and Range landform. Valleys filled with sediment and, during the wet times of glacial periods, with lakes, such as Lake Manly.\nDeath Valley is the fifth-largest American national park and the largest in the contiguous United States. It is also larger than the states of Rhode Island and Delaware combined, and nearly as large as Puerto Rico. In 2013, Death Valley National Park was designated as a dark sky park by the International Dark-Sky Association.\nGeographic setting.\nThere are two major valleys in the park, Death Valley and Panamint Valley. Both of these valleys were formed within the last few million years and both are bounded by north\u2013south-trending mountain ranges. These and adjacent valleys follow the general trend of Basin and Range topography with one modification: there are parallel strike-slip faults that perpendicularly bound the central extent of Death Valley. The result of this shearing action is additional extension in the central part of Death Valley which causes a slight widening and more subsidence there.\nUplift of surrounding mountain ranges and subsidence of the valley floor are both occurring. The uplift on the Black Mountains is so fast that the alluvial fans (fan-shaped deposits at the mouth of canyons) there are small and steep compared to the huge alluvial fans coming off the Panamint Range. Fast uplift of a mountain range in an arid environment often does not allow its canyons enough time to cut a classic V-shape all the way down to the stream bed. Instead, a V-shape ends at a slot canyon halfway down, forming a 'wine glass canyon.' Sediment is deposited on a small and steep alluvial fan.\nAt below sea level at its lowest point, Badwater Basin on Death Valley's floor is the second-lowest depression in the Western Hemisphere (behind Laguna del Carb\u00f3n in Argentina), while Mount Whitney, only to the west, rises to and is the tallest mountain in the contiguous United States. This topographic relief is the greatest elevation gradient in the contiguous United States and is the terminus point of the Great Basin's southwestern drainage. Although the extreme lack of water in the Great Basin makes this distinction of little current practical use, it does mean that in wetter times the lake that once filled Death Valley (Lake Manly) was the last stop for water flowing in the region, meaning the water there was saturated in dissolved materials. Thus, the salt pans in Death Valley are among the largest in the world and are rich in minerals, such as borax and various salts and hydrates. The largest salt pan in the park extends from the Ashford Mill Site to the Salt Creek Hills, covering some of the valley floor. The best known playa in the park is the Racetrack, known for its moving rocks.\nClimate.\nAccording to the K\u00f6ppen climate classification system, Death Valley National Park has a hot desert climate (\"BWh\"). The plant hardiness zone at Badwater Basin is 9b with an average annual extreme minimum temperature of .\nDeath Valley is the hottest and driest place in North America due to its lack of surface water and low relief. It is so frequently the hottest spot in the United States that many tabulations of the highest daily temperatures in the country omit Death Valley as a matter of course.\nOn the afternoon of July 10, 1913, the United States Weather Bureau recorded a high temperature of at Greenland Ranch (now Furnace Creek) in Death Valley. This temperature stands as the highest ambient air temperature ever recorded at the surface of the Earth. (A report of a temperature of recorded in Libya in 1922 was later determined to be inaccurate.) Daily summer temperatures of or greater are common, as well as below freezing nightly temperatures in the winter. July is the hottest month, with an average high of and an average low of . December is the coldest month, with an average high of and an average low of . The record low is . There are an average of 197.3 days annually with highs of or higher and 146.9 days annually with highs of or higher. Freezing temperatures of or lower occur on an average of 8.6 days annually.\nSeveral of the larger Death Valley springs derive their water from a regional aquifer, which extends as far east as southern Nevada and Utah. Much of the water in this aquifer has been there for many thousands of years, since the Pleistocene ice ages, when the climate was cooler and wetter. Today's drier climate does not provide enough precipitation to recharge the aquifer at the rate at which water is being withdrawn.\nThe highest range within the park is the Panamint Range, with Telescope Peak being its highest point at . The Death Valley region is a transitional zone in the northernmost part of the Mojave Desert and consists of five mountain ranges removed from the Pacific Ocean. Three of these are significant barriers: the Sierra Nevada, the Argus Range, and the Panamint Range. Air masses tend to lose moisture as they are forced up over mountain ranges, in what climatologists call a rainshadow effect.\nThe exaggerated rain shadow effect for the Death Valley area makes it North America's driest spot, receiving about of rainfall annually at Badwater, and some years fail to register any measurable rainfall. Annual average precipitation varies from overall below sea level to over in the higher mountains that surround the valley. When rain does arrive it often does so in intense storms that cause flash floods which remodel the landscape and sometimes create very shallow ephemeral lakes.\nThe hot, dry climate makes it difficult for soil to form. Mass wasting, the down-slope movement of loose rock, is therefore the dominant erosive force in mountainous areas, resulting in \"skeletonized\" ranges (mountains with very little soil on them). Sand dunes in the park, while famous, are not nearly as widespread as their fame or the dryness of the area may suggest. The Mesquite Flat dune field is the most easily accessible from the paved road just east of Stovepipe Wells in the north-central part of the valley and is primarily made of quartz sand. Another dune field is just to the north but is instead mostly composed of travertine sand. The highest dunes in the park, and some of the highest in North America, are located in the Eureka Valley about to the north of Stovepipe Wells, while the Panamint Valley dunes and the Saline Valley dunes are located west and northwest of the town, respectively. The Ibex dune field is near the seldom-visited Ibex Hill in the southernmost part of the park, just south of the Saratoga Springs marshland. All the latter four dune fields are accessible only via unpaved roads. Prevailing winds in the winter come from the north, and prevailing winds in the summer come from the south. Thus, the overall position of the dune fields remains more or less fixed.\nThere are rare exceptions to the dry nature of the area. In 2005, an unusually wet winter created a 'lake' in the Badwater Basin and led to the greatest wildflower season in the park's history. In October 2015, a \"1000 year flood event\" with over three inches of rain caused major damage in Death Valley National Park. A similar widespread storm in August 2022 damaged pavement and deposited debris on nearly every road, trapping 1,000 residents and visitors overnight.\nHuman history.\nEarly inhabitants and transient populations.\nFour Native American cultures are known to have lived in the area during the last 10,000\u00a0years. The first known group, the Nevares Spring People, were hunters and gatherers who arrived in the area perhaps 9,000\u00a0years ago (7000\u00a0BC) when there were still small lakes in Death Valley and neighboring Panamint Valley. A much milder climate persisted at that time, and large game animals were still plentiful. By 5,000\u00a0years ago (3000\u00a0BC) the Mesquite Flat People displaced the Nevares Spring People. Around 2,000\u00a0years ago the Saratoga Spring People moved into the area, which by then was probably already a hot, dry desert. This culture was more advanced at hunting and gathering and was skillful at handcrafts. They also left mysterious stone patterns in the valley.\nOne thousand years ago, the nomadic Timbisha (formerly called Shoshone and also known as Panamint or Koso) moved into the area and hunted game and gathered mesquite beans along with pinyon pine nuts. Because of the wide altitude differential between the valley bottom and the mountain ridges, especially on the west, the Timbisha practiced a vertical migration pattern. Their winter camps were located near water sources in the valley bottoms. As the spring and summer progressed and the weather warmed, grasses and other plant food sources ripened at progressively higher altitudes. November found them at the very top of the mountain ridges where they harvested pine nuts before moving back to the valley bottom for winter.\nThe California Gold Rush brought the first people of European descent known to visit the immediate area. In December 1849 two groups of California Gold Country-bound travelers with perhaps 100 wagons total stumbled into Death Valley after getting lost on what they thought was a shortcut off the Old Spanish Trail. Called the Bennett-Arcane Party, they were unable to find a pass out of the valley for weeks; they were able to find fresh water at various springs in the area, but were forced to eat several of their oxen to survive. They used the wood of their wagons to cook the meat and make jerky. The place where they did this is today referred to as \"Burnt Wagons Camp\" and is located near Stovepipe Wells.\nAfter abandoning their wagons, they eventually were able to hike out of the valley. Just after leaving the valley, one of the women in the group turned and said, \"Goodbye Death Valley,\" giving the valley its name. Included in the party was William Lewis Manly whose autobiographical book \"Death Valley in '49\" detailed this trek and popularized the area (geologists later named the prehistoric lake that once filled the valley after him).\nBoom and bust.\nThe ores that are most famously associated with the area were also the easiest to collect and the most profitable: evaporite deposits such as salts, borate, and talc. Borax was found by Rosie and Aaron Winters near The Ranch at Death Valley (then called Greenland) in 1881. Later that same year, the Eagle Borax Works became Death Valley's first commercial borax operation. William Tell Coleman built the Harmony Borax Works plant and began to process ore in late 1883 or early 1884, continuing until 1888. This mining and smelting company produced borax to make soap and for industrial uses. The end product was shipped out of the valley to the Mojave railhead in 10-ton-capacity wagons pulled by \"twenty-mule teams\" that were actually teams of 18 mules and two horses each.\nThe teams averaged an hour and required about 30\u00a0days to complete a round trip. The trade name \"20-Mule Team Borax\" was established by Francis Marion Smith's Pacific Coast Borax Company after Smith acquired Coleman's borax holdings in 1890. A memorable advertising campaign used the wagon's image to promote the Boraxo brand of granular hand soap and the Death Valley Days radio and television programs. In 1914, the Death Valley Railroad was built to serve mining operations on the east side of the valley. Mining continued after the collapse of Coleman's empire, and by the late 1920s the area was the world's number one source of borax. Some four to six million years old, the Furnace Creek Formation is the primary source of borate minerals gathered from Death Valley's playas.\nOther visitors stayed to prospect for and mine deposits of copper, gold, lead, and silver. These sporadic mining ventures were hampered by their remote location and the harsh desert environment. In December 1903, two men from Ballarat were prospecting for silver. One was an out-of-work Irish miner named Jack Keane and the other was a one-eyed Basque butcher named Domingo Etcharren. Quite by accident, Keane discovered an immense ledge of free-milling gold by the duo's work site and named the claim the Keane Wonder Mine. This started a minor and short-lived gold rush into the area. The Keane Wonder Mine, along with mines at Rhyolite, Skidoo and Harrisburg, were the only ones to extract enough metal ore to make them worthwhile. Outright shams such as Leadfield also occurred, but most ventures quickly ended after a short series of prospecting mines failed to yield evidence of significant ore (these mines now dot the entire area and are a significant hazard to anyone who enters them). The boom towns which sprang up around these mines flourished during the first decade of the 1900s, but soon declined after the Panic of 1907.\nEarly tourism.\nThe first documented tourist facilities in Death Valley were a set of tent houses built in the 1920s where Stovepipe Wells is now located. People flocked to resorts built around natural springs thought to have curative and restorative properties. In 1927, Pacific Coast Borax turned the crew quarters of its Furnace Creek Ranch into a resort, creating the Furnace Creek Inn and resort. The spring at Furnace Creek was harnessed to develop the resort, and as the water was diverted, the surrounding marshes and wetlands started to shrink.\nSoon the valley was a popular winter destination. Other facilities started off as private getaways but were later opened to the public. Most notable among these was Death Valley Ranch, better known as Scotty's Castle. This large ranch home built in the Spanish Revival style became a hotel in the late 1930s and, largely because of the fame of Death Valley Scotty, a tourist attraction. Death Valley Scotty, whose real name was Walter Scott, was a gold miner who pretended to be the owner of \"his castle\", which he claimed to have built with profits from his gold mine. Neither claim was true, but the real owner, Chicago millionaire Albert Mussey Johnson, encouraged the myth. When asked by reporters what his connection was to Walter Scott's castle, Johnson replied that he was Mr. Scott's banker.\nProtection and later history.\nPresident Herbert Hoover proclaimed a national monument in and around Death Valley on February 11, 1933, setting aside almost of southeastern California and small parts of Nevada.\nThe Civilian Conservation Corps (CCC) developed infrastructure in Death Valley National Monument during the Great Depression and on into the early 1940s. The main CCC camp was located at Cow Creek, just north of the visitors center at Furnace Creek. The CCC built barracks, graded of roads, installed water and telephone lines, and a total of 76 buildings. Trails in the Panamint Range were built to points of scenic interest, and an adobe village, laundry and trading post were constructed for the Timbisha Shoshone Tribe. Five campgrounds, restrooms, an airplane landing field and picnic facilities were also built.\nIn 1942, the former CCC main camp at Cow Creek was repurposed for a short period to hold sixty-six Japanese American detainees who had been incarcerated at Manzanar.\nThe creation of the monument resulted in a temporary closing of the lands to prospecting and mining. However, Death Valley was quickly reopened to mining by Congressional action in June 1933. As improvements in mining technology allowed lower grades of ore to be processed, and new heavy equipment allowed greater amounts of rock to be moved, mining in Death Valley changed. Gone were the days of the \"single-blanket, jackass prospector\" long associated with the romantic west. Open pit and strip mines scarred the landscape as international mining corporations bought claims in highly visible areas of the national monument. The public outcry that ensued led to greater protection for all national park and monument areas in the United States. In 1976, Congress passed the Mining in the Parks Act, which closed Death Valley National Monument to the filing of new mining claims, banned open-pit mining and required the National Park Service to examine the validity of tens of thousands of pre-1976 mining claims. Mining was allowed to resume on a limited basis in 1980 with stricter environmental standards. The last mine in the park, Billie Mine, closed in 2005.\nIn 1952 President Harry Truman added the Devils Hole to Death Valley National Monument; it is the only habitat of the Devils Hole pupfish.\nDeath Valley National Monument was designated a biosphere reserve in 1984. On October 31, 1994, the monument was expanded by and re-designated as a national park, via congressional passage of the California Desert Protection Act (Public Law 103\u2013433). Consequently, the elevated status for Death Valley made it the largest national park in the contiguous United States. On March 12, 2019, the John D. Dingell Jr. Conservation, Management, and Recreation Act added to the park.\nMany of the larger cities and towns within the boundary of the regional groundwater flow system that the park and its plants and animals rely upon are experiencing some of the fastest growth rates of any place in the United States. Notable examples within a radius of Death Valley National Park include Las Vegas and Pahrump, Nevada. In the case of Las Vegas, the local Chamber of Commerce estimates that 6,000 people are moving to the city every month. Between 1985 and 1995, the population of the Las Vegas Valley increased from 550,700 to 1,138,800.\nIn 1977, parts of Death Valley were used by director George Lucas as a filming location for \"Star Wars\", providing the setting for the fictional planet Tatooine.\nGeologic history.\nThe park has a diverse and complex geologic history. Since its formation, the area that comprises the park has experienced at least four major periods of extensive volcanism, three or four periods of major sedimentation, and several intervals of major tectonic deformation where the crust has been reshaped. Two periods of glaciation (a series of ice ages) have also had effects on the area, although no glaciers ever existed in the ranges now in the park.\nBasement and Pahrump Group.\nLittle is known about the history of the oldest exposed rocks in the area due to extensive metamorphism (alteration of rock by heat and pressure). Radiometric dating gives an age of 1,700\u00a0million years for the metamorphism during the Proterozoic. About 1,400\u00a0million years ago a mass of granite now in the Panamint Range intruded this complex. Uplift later exposed these rocks to nearly 500\u00a0million years of erosion.\nThe Proterozoic sedimentary formations of the Pahrump Group were deposited on these basement rocks. This occurred following uplift and erosion of any earlier sediments from the Proterozoic basement rocks. The Pahrump is composed of arkose conglomerate (quartz clasts in a concrete-like matrix) and mudstone in its lower part, followed by dolomite from carbonate banks topped by algal mats as stromatolites, and finished with basin-filling sediment derived from the above, including possible glacial till from the hypothesized Snowball Earth glaciation. The very youngest rocks in the Pahrump Group are basaltic lava flows.\nRifting and deposition.\nA rift opened and subsequently flooded the region as part of the breakup of the supercontinent Rodinia in the Neoproterozoic (by about 755\u00a0million years ago) and the creation of the Pacific Ocean. A shoreline similar to the present Atlantic Ocean margin of the United States lay to the east. An algal mat-covered carbonate bank was deposited, forming the Noonday Dolomite. Subsidence of the region occurred as the continental crust thinned and the newly formed Pacific widened, forming the Ibex Formation. An angular unconformity (an uneven gap in the geologic record) followed.\nA true ocean basin developed to the west, breaking all the earlier formations along a steep front. A wedge of clastic sediment then began to accumulate at the base of the two underwater precipices, starting the formation of opposing continental shelves. Three formations developed from sediment that accumulated on the wedge. The region's first known fossils of complex life are found in the resulting formations. Notable among these are the Ediacara fauna and trilobites, the evolution of the latter being part of the Cambrian Explosion of life.\nThe sandy mudflats gave way about 550 million years ago to a carbonate platform (similar to the one around the present-day Bahamas), which lasted for the next 300 million years of Paleozoic time (refer to the middle of the ). Death Valley's position was then within ten or twenty degrees of the Paleozoic equator. Thick beds of carbonate-rich sediments were periodically interrupted by periods of emergence. Although details of geography varied during this immense interval of time, a north-northeastern coastline trend generally ran from Arizona up through Utah. The resulting eight formations and one group are thick and underlay much of the Cottonwood, Funeral, Grapevine, and Panamint ranges.\nCompression and uplift.\nIn the early-to-mid- Mesozoic the western edge of the North American continent was pushed against the oceanic plate under the Pacific Ocean, creating a subduction zone. A subduction zone is a type of contact between different crustal plates where heavier crust slides below lighter crust. Erupting volcanoes and uplifting mountains were created as a result, and the coastline was pushed to the west. The Sierran Arc started to form to the northwest from heat and pressure generated from subduction, and compressive forces caused thrust faults to develop.\nA long period of uplift and erosion was concurrent with and followed the above events, creating a major unconformity, which is a large gap in the geologic record. Sediments worn off the Death Valley region were carried both east and west by wind and water. No Jurassic- to Eocene-aged sedimentary formations exist in the area, except for some possibly Jurassic-age volcanic rocks (see the top of the ).\nStretching and lakes.\nBasin and Range-associated stretching of large parts of crust below southwestern United States and northwestern Mexico started around 16 million years ago and the region is still spreading. This stretching began to affect the Death and Panamint valleys area by 3\u00a0million years ago. Before this, rocks now in the Panamint Range were on top of rocks that would become the Black Mountains and the Cottonwood Mountains. Lateral and vertical transport of these blocks was accomplished by movement on normal faults. Right-lateral movement along strike-slip faults that run parallel to and at the base of the ranges also helped to develop the area. Torsional forces, probably associated with northwesterly movement of the Pacific plate along the San Andreas Fault (west of the region), is responsible for the lateral movement.\nIgneous activity associated with this stretching occurred from 12\u00a0million to 4\u00a0million years ago. Sedimentation is concentrated in valleys (basins) from material eroded from adjacent ranges. The amount of sediment deposited has roughly kept up with this subsidence, resulting in the retention of more or less the same valley floor elevation over time.\nPleistocene ice ages started 2 million years ago, and melt from alpine glaciers on the nearby Sierra Nevada Mountains fed a series of lakes that filled Death and Panamint valleys and surrounding basins (see the top of the ). The lake that filled Death Valley was the last of a chain of lakes fed by the Amargosa and Mojave Rivers, and possibly also the Owens River. The large lake that covered much of Death Valley's floor, which geologists call Lake Manly, started to dry up 10,500\u00a0years ago. Salt pans and playas were created as ice age glaciers retreated, thus drastically reducing the lakes' water source. Only faint shorelines are left.\nBiology.\nHabitat varies from salt pan at below sea level to the sub-alpine conditions found on the summit of Telescope Peak, which rises to . Vegetation zones include creosote bush, desert holly, and mesquite at the lower elevations and sage up through shadscale, blackbrush, Joshua tree, pinyon-juniper, to limber pine and bristlecone pine woodlands. The salt pan is devoid of vegetation, and the rest of the valley floor and lower slopes have sparse cover, although where water is available, an abundance of vegetation is usually present.\nThese zones and the adjacent desert support a variety of wildlife species, including 51 species of native mammals, 307 species of birds, 36 species of reptiles, 3 species of amphibians, and 2 species of native fish.\nSmall mammals are more numerous than large mammals, such as bighorn sheep, coyotes, bobcats, kit foxes, cougars, and mule deer. Mule deer are present in the pinyon/juniper associations of the Grapevine, Cottonwood, and Panamint ranges. Bighorn sheep are a rare species of mountain-dwelling sheep that exist in isolated bands in the Sierra and in Death Valley. These are highly adaptable animals and can eat almost any plant. They have no known predators, but humans and burros compete for habitat.\nThe ancestors of the Death Valley pupfish swam to the area from the Colorado River via a long-since dried-up system of rivers and lakes (see Lake Manly). They now live in two separate populations: one in Salt Creek and another in Cottonball Marsh. Death Valley is one of the hottest and driest places in North America, yet it is home to over 1,000 species of plants; 23 of which, including the very rare rock lady (\"Holmgrenanthe\"), are not found anywhere else.\nAdaptation to the dry environment is key. For example, creosote bush and mesquite have tap-root systems that can extend down in order to take advantage of a year-round supply of ground water. The diversity of Death Valley's plant communities results partly from the region's location in a transition zone between the Mojave Desert, the Great Basin Desert and the Sonoran Desert. This location, combined with the great relief found within the park, supports vegetation typical of three biotic life zones: the lower Sonoran, the Canadian, and the arctic/alpine in portions of the Panamint Range. Based on the Munz and Keck (1968) classifications, seven plant communities can be categorized within these life zones, each characterized by dominant vegetation and representative of three vegetation types: scrub, desert woodland, and coniferous forest. Microhabitats further subdivide some communities into zones, especially on the valley floor.\nUnlike more typical locations across the Mojave Desert, many of the water-dependent Death Valley habitats possess a diversity of plant and animal species that are not found anywhere else in the world. The existence of these species is due largely to a unique geologic history and the process of evolution that has progressed in habitats that have been isolated from one another since the Pleistocene epoch.\nActivities.\nSightseeing is available by personal automobile, four-wheel drive, motorcycle, bicycle, mountain bike (on established roadways only), and hiking. State Route 190, the Badwater Road, the Scotty's Castle Road, and paved roads to Dante's View and Wildrose provide access to the major scenic viewpoints and historic points of interest. More than of unpaved and four-wheel-drive roads provide access to wilderness hiking, camping, and historical sites. Unlike many other national parks in the U.S. there are no formal entrance stations, and instead entry fees can be paid at the visitor centers, ranger stations, or various fee machines around the park. There are hiking trails of varying lengths and difficulties, but most backcountry areas are accessible only by cross-country hiking. The peak season for visiting the park is from October to May, avoiding summer extreme temperatures. Costumed living history tours of the historic Scotty's Castle were suspended in October 2015 due to extensive flood damage to the buildings and grounds. It remains closed to the public.\nThere are nine designated campgrounds within the park, and overnight backcountry camping permits are available at the visitor center. Xanterra Parks &amp; Resorts owns and operates a private resort, the Oasis at Death Valley, which comprises two separate and distinct hotels: the Inn at Death Valley is a four-star historic hotel, and the Ranch at Death Valley is a three-star ranch-style property reminiscent of the mining and prospecting days. Panamint Springs Resort is in the western part of the park. Death Valley Lodging Company operates the Stovepipe Wells Resort under a concession permit. There are a few motels near entrances to the park, in Shoshone, Death Valley Junction, Beatty, and Pahrump.\nFurnace Creek Visitor Center is located on CA-190 and includes exhibits and a film about the park's geology, climate, wildlife and natural history, as well as human history and pioneer experience. During the winter season\u2014November through April\u2014rangers offer interpretive tours and a variety of walks, talks, and presentations about Death Valley cultural and natural history. The maintains a bookstore.\nThe northeast corner of Saline Valley has several developed hot spring pools accessible by several hours' drive on unpaved roads or by flying a personal aircraft to the Chicken Strip\u2014an uncharted airstrip a short walk from the springs.\nDeath Valley National Park is a popular location for stargazing as it has one of the darkest night skies in the United States. Despite its remote location, air quality and night visibility are threatened by civilization. In particular, light pollution is introduced by nearby Las Vegas. The darkest skies are located in the northwest of the park; Ubehebe Crater is a Bortle class 1 or \"excellent dark sky\" site. The Andromeda Galaxy and the Triangulum Galaxy are visible to the unaided eye under these conditions, and the Milky Way casts shadows; optical phenomena such as zodiacal light or \"false dawn\" and gegenschein are also visible to the unaided eye under these conditions. Most southern regions of the park are Bortle class 2 or \"average dark sky\" sites.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46794", "revid": "22651524", "url": "https://en.wikipedia.org/wiki?curid=46794", "title": "Black Narcissus (1947 movie)", "text": ""}
{"id": "46795", "revid": "31403940", "url": "https://en.wikipedia.org/wiki?curid=46795", "title": "Mono Lake", "text": "Endorheic lake in California\nMono Lake ( ) is a saline soda lake in Mono County, California, formed at least 760,000 years ago as a terminal lake in an endorheic basin. The lack of an outlet causes high levels of salts to accumulate in the lake which make its water alkaline.\nThe desert lake has an unusually productive ecosystem based on brine shrimp, which thrive in its waters, and provides critical habitat for two million annual migratory birds that feed on the shrimp and alkali flies (\"Ephydra hians\"). Historically, the native Kutzadika'a people ate the alkali flies' pupae, which live in the shallow waters around the edge of the lake.\nWhen the city of Los Angeles diverted water from the freshwater streams flowing into the lake, the lake level dropped, imperiling the migratory birds. The Mono Lake Committee formed in response and won a legal battle that forced Los Angeles to partially replenish the lake level.\nGeology.\nMono Lake lies within the Mono Basin, an endorheic basin with no outlet to the ocean. Dissolved salts in the runoff thus remain in the lake, raising the water's pH and salt concentration. The tributaries of Mono Lake include Lee Vining Creek, Rush Creek and Mill Creek which flows through Lundy Canyon.\nGeological forces formed the basin over the last five million years: basin and range crustal stretching and associated volcanism and faulting at the base of the Sierra Nevada.\nFrom 4.5 to 2.6 million years ago, large volumes of basalt were extruded around what is now Cowtrack Mountain (east and south of Mono Basin); eventually covering and reaching a maximum thickness of . Later volcanism in the area occurred 3.8 million to 250,000 years ago. This activity was northwest of Mono Basin and included the formation of Aurora Crater, Beauty Peak, Cedar Hill (later an island in the highest stands of Mono Lake), and Mount Hicks.\nLake Russell was the prehistoric predecessor to Mono Lake, during the Pleistocene. Its shoreline reached the modern-day elevation of , about higher than the present-day lake. As of 1.6 million years ago, Lake Russell discharged to the northeast, into the Walker River drainage. After the Long Valley Caldera eruption 760,000 years ago, Lake Russell discharged into Adobe Lake to the southeast, then into the Owens River, and eventually into Lake Manly in Death Valley. Prominent shore lines of Lake Russell, called strandlines by geologists, can be seen west of Mono Lake.\nThe area around Mono Lake is currently geologically active. Volcanic activity is related to the Mono\u2013Inyo Craters: the most recent eruption occurred 350 years ago, resulting in the formation of Paoha Island. Panum Crater (on the south shore of the lake) is an example of a combined rhyolite dome and cinder cone.\nTufa towers.\nMany columns of limestone rise above the surface of Mono Lake. These limestone towers consist primarily of calcium carbonate minerals such as calcite (CaCO3). This type of limestone rock is called tufa, a term for limestone that forms at low to moderate temperatures.\nTufa tower formation.\nMono Lake is a highly alkaline lake, or soda lake. Alkalinity is a measure of how many bases are in a solution, and how well the solution can neutralize acids. Carbonate (CO32-) and bicarbonate (HCO3\u2212) are both bases. Hence, Mono Lake has a very high content of dissolved inorganic carbon. Through supply of calcium ions (Ca2+), the water will precipitate carbonate-minerals such as calcite (CaCO3). Subsurface waters enter the bottom of Mono Lake through small springs. High concentrations of dissolved calcium ions in these subsurface waters cause huge amounts of calcite to precipitate around the spring orifices.\nThe tufa originally formed at the bottom of the lake. It took many decades, or even centuries, to form the well-known tufa towers. When lake levels fell, the tufa towers rose above the water surface and stand as the pillars seen today (see Mono lake#Lake Level History for more information).\nTufa morphology.\nDescription of the Mono Lake tufa dates back to the 1880s, when Edward S. Dana and Israel C. Russell made the first systematic descriptions of the Mono Lake tufa. The tufa occurs as \"modern\" tufa towers. There are tufa sections from old shorelines, when the lake levels were higher. These pioneering works on tufa morphology are cited by researchers and were confirmed by James R. Dunn in 1953. The tufa types can roughly be divided into three main categories based on morphology:\nOver time, many hypotheses have been developed regarding the formation of the large thinolite crystals (also referred to as glendonite) in thinolitic tufa. It was relatively clear that the thinolites represented a calcite pseudomorph after some unknown original crystal. The original crystal was only determined when the mineral ikaite was discovered in 1963. Ikaite, or hexahydrated CaCO3, is metastable and only crystallizes at near-freezing temperatures. It is also believed that calcite crystallization inhibitors, such as phosphate, magnesium, and organic carbon, may help stabilize ikaite. When heated, ikaite breaks down and becomes replaced by smaller crystals of calcite. In the Ikka Fjord of Greenland, ikaite was also observed to grow in columns similar to the tufa towers of Mono Lake. This has led scientists to believe that thinolitic tufa is an indicator of past climates in Mono Lake because they reflect very cold temperatures.\nTufa chemistry.\nRussell (1883) studied the chemical composition of the different tufa types in Lake Lahontan, a large Pleistocene system of multiple lakes in California, Nevada, and Oregon. Not surprisingly, it was found that the tufas consisted primarily of CaO and CO2. However, they also contain minor constituents of MgO (~2 wt%), Fe/Al-oxides (.25-1.29 wt%), and PO5 (0.3 wt%).\nLimnology.\nThe limnology of the lake shows that it contains approximately 280 million tons of dissolved salts, with salinity varying with the amount of water in the lake at any given time. Before 1941, average salinity was approximately 50\u00a0grams per liter (g/L) (compared to a value of 31.5 g/L for the world's oceans). In January 1982, when the lake reached its lowest level of , the salinity had nearly doubled to 99 g/L. In 2002, it was measured at 78 g/L and is expected to stabilize at an average of 69 g/L as the lake replenishes over the next 20 years.\nAn unintended consequence of ending the water diversions was the onset of a period of \"meromixis\" in Mono Lake. In the time before this, Mono Lake was typically \"monomictic\"; which means that at least once each year the deeper waters and the shallower waters of the lake mixed thoroughly, thus bringing oxygen and other nutrients to the deep waters. In meromictic lakes, the deeper waters do not undergo this mixing; the deeper layers are more saline than the water near the surface, and are typically nearly devoid of oxygen. As a result, becoming meromictic greatly changes a lake's ecology.\nMono Lake has experienced meromictic periods in the past; the most recent episode of meromixis, brought on by the end of water diversions, commenced in 1994 and ended by 2004.\nLake-level history.\nAn essential characteristic of Mono Lake is that it is a closed lake, meaning it has no outflow. Water can only escape the lake if it evaporates or is lost to groundwater. This may cause closed lakes to become very saline. The reconstruction of historical Mono Lake levels using carbon and oxygen isotopes has also revealed a correlation with well-documented changes in climate.\nIn the recent past, Earth experienced periods of increased glaciation known as ice ages. This geological period of ice ages is known as the Pleistocene, which lasted until ~11 ka. Lake levels in Mono Lake can reveal how the climate fluctuated. For example, during the Pleistocene, when the climate was colder, the lake level was higher because there was less evaporation and more precipitation. Following the Pleistocene, the lake level was generally lower due to increased evaporation and decreased precipitation associated with a warmer climate.\nThe lake level has fluctuated during the Holocene, since the end of the ice ages. The Holocene high point is at elevation , reached in approximately 1820 BCE. The low point before modern diversions is at elevation , reached in 143 CE. The lowest modern level due to diversions is at , reached in 1980.\nEcology.\nAquatic life.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nThe hypersalinity and high alkalinity (pH=10 or equivalent to 4 milligrams of NaOH per liter of water) of the lake mean that no fish are native to the lake. An attempt by the California Department of Fish and Game to stock the lake failed.\nThe whole food chain of the lake is based on the high population of single-celled planktonic algae present in the photic zone of the lake. These algae reproduce rapidly during winter and early spring after winter runoff brings nutrients to the surface layer of water. By March, the lake is \"as green as pea soup\" with photosynthesizing algae.\nThe lake is famous for the Mono Lake brine shrimp, \"Artemia monica\", a tiny species of brine shrimp, no bigger than a thumbnail, that is endemic to the lake. During the warmer summer months, an estimated 4\u20136 trillion brine shrimp inhabit the lake. Brine shrimp have no nutritional value for humans but are a staple for birds in the region. The brine shrimp feed on microscopic algae.\nAlkali flies, \"Ephydra hians,\" live along the lake's shores and swim underwater, encased in small air bubbles, to graze and lay eggs. These flies are an important food source for migratory and nesting birds.\nEight nematode species were found living in the littoral sediment:\nBirds.\nMono Lake is a vital resting and eating stop for migratory shorebirds and has been recognized as a site of international importance by the Western Hemisphere Shorebird Reserve Network.\nNearly 2,000,000 waterbirds, including 35 species of shorebirds, use Mono Lake to rest and eat for at least part of the year. Some shorebirds that depend on the resources of Mono Lake include American avocets, killdeer, and sandpipers. One to two million eared grebes and phalaropes use Mono Lake during their long migrations.\nLate every summer, tens of thousands of Wilson's phalaropes and red-necked phalaropes arrive from their nesting grounds, and feed until they continue their migration to South America or the tropical oceans, respectively.\nIn addition to migratory birds, a few species spend several months nesting at Mono Lake. Mono Lake has the second largest nesting population of California gulls, \"Larus californicus\", second only to the Great Salt Lake in Utah. Since abandoning the landbridged Negit Island in the late 1970s, California gulls have moved to some nearby islets and have established new, if less protected, nesting sites. Cornell University and Point Blue Conservation Science have continued the study of nesting populations in Mono Lake that began 35 years ago. Snowy plovers also arrive at Mono Lake each spring to nest along the northern and eastern shores.\nHistory.\nNative Americans.\nThe indigenous people of Mono Lake are from a band of the Northern Paiute, called the Kutzadika'a. They speak the Northern Paiute language. The Kutzadika'a traditionally forage alkali fly pupae, called kutsavi in their language.\nThe term \"Mono\" is derived from \"Monachi\", a Yokuts term for the tribes that live on both the east and west side of the Sierra Nevada.\nDuring early contact, the first known Mono Lake Paiute chief was Captain John.\nThe Mono tribe has two bands: Eastern and Western. The Eastern Mono joined the Western Mono bands' villages annually at Hetch Hetchy Valley, Yosemite Valley, and along the Merced River to gather acorns, different plant species, and to trade. The Western Mono and Eastern Mono traditionally lived in the south-central Sierra Nevada foothills, including Historical Yosemite Valley.\nPresent day Mono Reservations are currently located in Big Pine, Bishop, and several in Madera County and Fresno County, California.\nConservation efforts.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nThe city of Los Angeles diverted water from the Owens River into the Los Angeles Aqueduct in 1913. In 1941, the Los Angeles Department of Water and Power extended the Los Angeles Aqueduct system farther northward into the Mono Basin with the completion of the Mono Craters Tunnel between the Grant Lake Reservoir on Rush Creek and the Upper Owens River. So much water was diverted that evaporation soon exceeded inflow and the surface level of Mono Lake fell rapidly. By 1982, the lake was reduced to , 69 percent of its 1941 surface area. By 1990, the lake had dropped 45 vertical feet and had lost half its volume relative to the 1941 pre-diversion water level. As a result, alkaline sands and formerly submerged tufa towers became exposed, the water salinity doubled, and Negit Island became a peninsula, exposing the nests of California gulls to predators (such as coyotes), and forcing the gull colony to abandon this site.\nIn 1974, ecologist David Gaines and his student David Winkler studied the Mono Lake ecosystem and became instrumental in alerting the public of the effects of the lower water level with Winkler's 1976 ecological inventory of the Mono Basin. The National Science Foundation funded the first comprehensive ecological study of Mono Lake, conducted by Gaines and undergraduate students. In June 1977, the Davis Institute of Ecology at the University of California published a report, \"An Ecological Study of Mono Lake, California,\" which alerted California to the ecological dangers posed by the diversion of water from the lake for municipal use.\nGaines formed the Mono Lake Committee in 1978. He and Sally Judy, a UC Davis student, led the committee and pursued an informational tour of California. They joined the Audubon Society in a now-famous court battle, National Audubon Society v. Superior Court, to protect Mono Lake through state public trust laws. While these efforts have resulted in positive change, the surface level is still below historical levels, and exposed shorelines are a source of significant alkaline dust during periods of high winds.\nOwens Lake, the once-navigable terminus of the Owens River, which had sustained a healthy ecosystem, is now a dry lakebed during dry years due to water diversions beginning in the 1920s. Mono Lake was spared this fate when the California State Water Resources Control Board (after over a decade of litigation) issued an order (SWRCB Decision 1631) to protect Mono Lake and its tributary streams on September 28, 1994. SWRCB Board Vice-chair Marc Del Piero was the sole Hearing Officer (see D-1631). In 1941 the surface level was at above sea level. As of October 2022, Mono Lake was at above sea level. The lake level of above sea level is the goal, designed to ensure that the lake would be able to reach and sustain a minimum surface level that is generally agreed to be the minimum for keeping the ecosystem healthy. It has been more difficult during years of drought in the American West.\nIn popular culture.\nArtwork.\nIn 1968, the artist Robert Smithson made \"Mono Lake Non-Site (Cinders near Black Point)\" using pumice collected while visiting Mono on July 27, 1968, with his wife Nancy Holt and Michael Heizer (both prominent visual artists). In 2004, Nancy Holt made a short film entitled \"Mono Lake\" using Super 8 footage and photographs of this trip. An audio recording by Smithson and Heizer, two songs by Waylon Jennings, and Michel Legrand's \"Le Jeu\", the main theme of Jacques Demy's film \"Bay of Angels\" (1963), were used for the soundtrack.\n\"The Diver\", a photo taken by Aubrey Powell of Hipgnosis for Pink Floyd's album \"Wish You Were Here\" (1975), features what appears to be a man diving into a lake, creating no ripples. The photo was taken at Mono Lake, and the tufa towers are a prominent part of the landscape. The effect was actually created when the diver performed a handstand underwater until the ripples dissipated.\nIn print.\nMark Twain's \"Roughing It\", published in 1872, provides an informative early description of Mono Lake in its natural condition in the 1860s. Twain found the lake to be lying \"in a lifeless, treeless, hideous desert... the loneliest place on earth.\"\nIn film.\nA scene featuring a volcano in the film \"Fair Wind to Java\" (1953) was shot at Mono Lake.\nMost of the film \"High Plains Drifter\" (1973) by Clint Eastwood was shot on the southern shores of Mono Lake in the 1970s. An entire town was built here for the film, and later removed when shooting was complete.\nIn music.\nThe music video for glam metal band Cinderella's 1988 power ballad \"Don't Know What You Got ('Till It's Gone)\" was filmed by the lake.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46796", "revid": "4689380", "url": "https://en.wikipedia.org/wiki?curid=46796", "title": "Alkaline", "text": ""}
{"id": "46797", "revid": "49867084", "url": "https://en.wikipedia.org/wiki?curid=46797", "title": "Death Valley", "text": "Valley in the Mojave Desert, Eastern California\nDeath Valley ( ) is a desert valley in Eastern California, United States, in the northern Mojave Desert, bordering the Great Basin Desert. It is thought to be the hottest place on Earth during summer.\nDeath Valley's Badwater Basin is the point of lowest elevation in North America, at below sea level. It is east-southeast of Mount Whitney \u2013 the highest point in the contiguous United States, with an elevation of 14,505 feet (4,421 m). On the afternoon hours of July10, 1913, the United States Weather Bureau recorded a temperature of 134\u00a0\u00b0F (56.7\u00a0\u00b0C) at Furnace Creek in Death Valley, which stands as the highest ambient air temperature ever recorded on the surface of the Earth. However, this reading and several others taken in that period are disputed by some modern experts.\nLying mostly in Inyo County, California, near the border of California and Nevada, in the Great Basin, east of the Sierra Nevada mountains, Death Valley constitutes much of Death Valley National Park and is the principal feature of the Mojave and Colorado Deserts Biosphere Reserve. It runs from north to south between the Amargosa Range on the east and the Panamint Range on the west. The Grapevine Mountains and the Owlshead Mountains form its northern and southern boundaries, respectively. It has an area of about . The highest point in Death Valley National Park is Telescope Peak, in the Panamint Range, which has an elevation of .\nA group of European-American pioneers got lost in the valley in the winter of 1849\u20131850, while looking for a shortcut to the gold fields of California, giving Death Valley its grim name. Although only one of the group members died there, they all assumed that the valley would be their grave. Death Valley is home to the Timbisha tribe of Native Americans, formerly known as the Panamint Shoshone, who have inhabited the valley for at least the past millennium.\nEtymology.\nThe native name for Death Valley is T\u00fcmpisa in the Timbisha language, meaning \"rock paint\". This is in reference to the rich sources of red ochre in the valley.\nGeology.\nDeath Valley is a graben\u2014a downdropped block of land between two mountain ranges. It lies at the southern end of a geological trough, Walker Lane, which runs north to Oregon. The valley is bisected by a right lateral strike slip fault system, comprising the Death Valley Fault and the Furnace Creek Fault. The eastern end of the left lateral Garlock Fault intersects the Death Valley Fault. Furnace Creek and the Amargosa River flow through part of the valley and eventually disappear into the sands of the valley floor.\nDeath Valley also contains salt pans. According to current geological consensus, at various times during the middle of the Pleistocene era, which ended roughly 10,000\u201312,000 years ago, an inland lake, Lake Manly, formed in Death Valley. The lake was nearly long and deep, the end-basin in a chain of lakes that began with Mono Lake in the north and continued through basins down the Owens River Valley, through Searles and China Lakes and the Panamint Valley, to the immediate west.\nAs the area turned to desert the water evaporated, leaving an abundance of evaporitic salts, such as common sodium salts and borax, which were later exploited during the modern history of the region, primarily 1883 to 1907.\nClimate.\nDeath Valley has a subtropical, hot desert climate (K\u00f6ppen: \"BWh\"), with long, extremely hot summers; short, warm winters; and little rainfall.\nThe valley is extremely dry because it lies in the rain shadow of four major mountain ranges (including the Sierra Nevada and Panamint Range). Moisture moving inland from the Pacific Ocean must pass eastward over the mountains to reach Death Valley; as air masses are forced upward by each range, they cool and moisture condenses, to fall as rain or snow on the western slopes. When the air masses reach Death Valley, most of the moisture in the air has already been lost and thus there is little left to fall as precipitation.\nThe extreme heat of Death Valley is attributable to a confluence of geographic and topographic factors. Scientists have identified a number of key contributors:\nSevere heat and dryness contribute to perpetual drought-like conditions in Death Valley and prevent much cloud formation from passing through the confines of the valley, where precipitation is often in the form of a virga.\nThe depth and shape of Death Valley strongly influence its climate. The valley is a long, narrow basin that descends below sea level and is walled by high, steep mountain ranges. The clear, dry air and sparse plant cover allow sunlight to heat the desert surface. Summer nights provide little relief: overnight lows may dip just into the range. Moving masses of super-heated air blow through the valley, creating extremely high ambient temperatures.\nThe hottest air temperature ever recorded in Death Valley was , on July 10, 1913, at Greenland Ranch (now Furnace Creek), which, as of today, is the highest atmospheric temperature ever recorded on the Earth's surface. (A report of a temperature of in Libya in 1922 was later determined to be inaccurate.) During the heat wave that peaked with that record, five consecutive days reached or higher. Some modern meteorologists now dispute the accuracy of the 1913 temperature measurement. On June 30, 2013, a verified temperature of was recorded and is tied with Mitribah, Kuwait, for the hottest reliably measured air temperature ever recorded on Earth. A temperature of was recorded at the Furnace Creek weather station on August 16, 2020, but has not yet been officially verified. The valley again recorded that temperature on July 9, 2021; however, that temperature has not yet been officially verified either. The valley's lowest temperature, recorded at Greenland Ranch (now Furnace Creek) on January 2, 1913, was .\nThe highest surface temperature ever recorded in Death Valley was , on July 15, 1972, at Furnace Creek, which is the highest ground surface temperature ever recorded on earth, as well as the only recorded surface temperature of above .\nThe greatest number of consecutive days with a maximum temperature of at least was 154, in the summer of 2001. The summer of 1996 had 40 days over , and 105 days over . The summer of 1917 had 52 days when the temperature reached or above, 43 of them consecutive.\nThe highest overnight or low temperature recorded in Death Valley is , recorded on July 5, 1918. However, this value is disputed; a record high low of on July 12, 2012, is considered reliable. This is one of the highest values ever recorded. Also on July 12, 2012, the mean 24-hour temperature recorded at Death Valley was , which makes it the world's warmest 24-hour temperature on record. July 2024 was the hottest month ever recorded in Death Valley, with a mean daily average temperature over the month of .\nFour major mountain ranges lie between Death Valley and the ocean, each one adding to an increasingly drier rain shadow effect, and in 1929, 1953, and 1989, no rain was recorded for the whole year. The period from 1931 to 1934 was the driest stretch on record, with only of rain over a 40-month period.\nThe average annual precipitation in Death Valley is , while the Greenland Ranch station averaged . The wettest month on record is January 1995, when fell on Death Valley. The wettest period on record was mid-2004 to mid-2005, in which nearly of rain fell in total, leading to ephemeral lakes in the valley and the region and tremendous wildflower blooms. Snow with accumulation has only been recorded in January 1922, while scattered flakes have been recorded on other occasions.\nFlooding.\nIn 2005, Death Valley received four times its average annual rainfall of . As it has done before for hundreds of years, the lowest spot in the valley filled with a wide, shallow lake, but the extreme heat and aridity immediately began evaporating the ephemeral lake.\nThe pair of images (seen at right) from NASA's Landsat 5 satellite documents the short history of Death Valley's Lake Badwater: formed in February 2005 (top) and evaporated by February 2007 (bottom). In 2005, a big pool of greenish water stretched most of the way across the valley floor. By May 2005, the valley floor had resumed its more familiar role as Badwater Basin, salt-coated salt flats. In time, this freshly dissolved and recrystallized salt will darken.\nThe western margin of Death Valley is traced by alluvial fans. During flash floods, rainfall from the steep mountains to the west pours through narrow canyons, picking up everything from fine clay to large rocks. When these torrents reach the mouths of the canyons, they widen and slow, branching out into distributary channels. The paler the fans, the younger they are.\nEcology.\nIn spite of the overwhelming heat and sparse rainfall, Death Valley exhibits considerable biodiversity. Flowers, watered by snowmelt, carpet the desert floor each spring, continuing into June. Bighorn sheep, red-tailed hawks, and feral donkeys may be seen. Death Valley has over 600 springs and ponds. Salt Creek, a mile-long shallow depression in the center of the valley, supports Death Valley pupfish. These isolated pupfish populations are remnants of the wetter Pleistocene climate.\nDarwin Falls, on the western edge of Death Valley Monument, falls into a large pond surrounded by willows and cottonwood trees. More than 80 species of bird have been recorded around the pond.\nEfflorescence, also known as salt flowers, is a rare occurrence in Death Valley that occurs when rain soaks into the soil and dissolves salt beneath the surface causing the ground to appear as if there is a light dusting of snow.\nHistory.\nDeath Valley is home to the Timbisha tribe of Native Americans, formerly known as the Panamint Shoshone, who have inhabited the valley for at least the past millennium. The Timbisha name for the valley, \"t\u00fcmpisa\", means \"rock paint\" and refers to the red ocher paint that can be made from a type of clay found in the valley. Some families still live in the valley at Furnace Creek. Another village was in Grapevine Canyon near the present site of Scotty's Castle. It was called \"maahunu\" in the Timbisha language, whose meaning is uncertain, although it is known that \"hunu\" means 'canyon'.\nThe valley received its English name in 1849 during the California Gold Rush. It was called Death Valley by prospectors and others who sought to cross the valley on their way to the gold fields, after 13 pioneers perished from one early expedition of wagon trains. During the 1850s, gold and silver were extracted in the valley. In the 1880s, borax was discovered and extracted by mule-drawn wagons.\nDeath Valley National Monument was proclaimed on February 11, 1933, by President Herbert Hoover, placing the area under federal protection. In 1994, the monument was redesignated as Death Valley National Park, as well as being substantially expanded to include Saline and Eureka Valleys.\nNotable attractions and locations.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIn literature and the arts.\nFilms.\nA number of movies have been filmed in Death Valley, including:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46799", "revid": "19404073", "url": "https://en.wikipedia.org/wiki?curid=46799", "title": "Long Valley", "text": "Long Valley may refer to:\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Topics referred to by the same termThis page lists articles about distinct geographical locations with the same name. "}
{"id": "46800", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=46800", "title": "Long valley", "text": ""}
{"id": "46801", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=46801", "title": "Mono lake", "text": ""}
{"id": "46802", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=46802", "title": "Simple continued fraction", "text": "Number represented as a0+1/(a1+1/...)\nA simple or regular continued fraction is a continued fraction with numerators all equal to one, and denominators built from a sequence formula_1 of integer numbers. The sequence can be finite or infinite, resulting in a finite (or terminated) continued fraction like\nformula_2\nor an infinite continued fraction like\nformula_3\nTypically, such a continued fraction is obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on. In the \"finite\" case, the iteration/recursion is stopped after finitely many steps by using an integer in lieu of another continued fraction. In contrast, an \"infinite\" continued fraction is an infinite expression. In either case, all integers in the sequence, other than the first, must be positive. The integers formula_4 are called the coefficients or terms of the continued fraction.\nSimple continued fractions have a number of remarkable properties related to the Euclidean algorithm for integers or real numbers. Every rational number has two closely related expressions as a finite continued fraction, whose coefficients ai can be determined by applying the Euclidean algorithm to formula_5. The numerical value of an infinite continued fraction is irrational; it is defined from its infinite sequence of integers as the limit of a sequence of values for finite continued fractions. Each finite continued fraction of the sequence is obtained by using a finite prefix of the infinite continued fraction's defining sequence of integers. Moreover, every irrational number formula_6 is the value of a \"unique\" infinite regular continued fraction, whose coefficients can be found using the non-terminating version of the Euclidean algorithm applied to the incommensurable values formula_6 and 1. This way of expressing real numbers (rational and irrational) is called their \"continued fraction representation\".\nMotivation and notation.\nConsider, for example, the rational number , which is around 4.4624. As a first approximation, start with 4, which is the integer part; = 4 +. The fractional part is the reciprocal of which is about 2.1628. Use the integer part, 2, as an approximation for the reciprocal to obtain a second approximation of 4 + = 4.5. Now, = 2 +;\nthe remaining fractional part, , is the reciprocal of , and is around 6.1429. Use 6 as an approximation for this to obtain 2 + as an approximation for and 4 +, about 4.4615, as the third approximation. Further, = 6 +. Finally, the fractional part, , is the reciprocal of 7, so its approximation in this scheme, 7, is exact (= 7 +) and produces the exact expression formula_8 for .\nThat expression is called the continued fraction representation of . This can be represented by the abbreviated notation = [4; 2, 6, 7]. It is customary to place a semicolon after the first number to indicate that it is the whole part. Some older textbooks use all commas in the (\"n\" + 1)-tuple, for example, [4, 2, 6, 7].\nIf the starting number is rational, then this process exactly parallels the Euclidean algorithm applied to the numerator and denominator of the number. In particular, it must terminate and produce a finite continued fraction representation of the number. The sequence of integers that occur in this representation is the sequence of successive quotients computed by the Euclidean algorithm. If the starting number is irrational, then the process continues indefinitely. This produces a sequence of approximations, all of which are rational numbers, and these converge to the starting number as a limit. This is the (infinite) continued fraction representation of the number. Examples of continued fraction representations of irrational numbers are:\nContinued fractions are, in some ways, more \"mathematically natural\" representations of a real number than other representations such as decimal representations, and they have several desirable properties:\nFormulation.\nA continued fraction in canonical form is an expression of the form\nformula_9\nwhere \"ai\" are integer numbers, called the \"coefficients\" or \"terms\" of the continued fraction.\nWhen the expression contains finitely many terms, it is called a \"finite\" continued fraction.\nWhen the expression contains infinitely many terms, it is called an \"infinite\" continued fraction.\nWhen the terms eventually repeat from some point onwards, the continued fraction is called \"periodic\".\nThus, all of the following illustrate valid finite simple continued fractions:\nFor simple continued fractions of the form\n formula_10\nthe formula_11 term can be calculated from the following recursive sequence:\nformula_12\nwhere formula_13 and formula_14.\nfrom which it can be understood that the formula_11 sequence stops if formula_16 is an integer.\nNotations.\nConsider a continued fraction expressed as\nformula_17\nBecause such a continued fraction expression may take a significant amount of vertical space, a number of methods have been tried to shrink it.\nGottfried Leibniz sometimes used the notation\nformula_18\nThus to incorporate a new term into a rational approximation, only the two previous convergents are necessary. The initial \"convergents\" (required for the first two terms) are 0\u20441 and 1\u20440. For example, here are the convergents for [0;1,5,2,2].\nWhen using the Babylonian method to generate successive approximations to the square root of an integer, if one starts with the lowest integer as first approximant, the rationals generated all appear in the list of convergents for the continued fraction. Specifically, the approximants will appear on the convergents list in positions 0,\u20091,\u20093,\u20097,\u200915,\u2009...\u2009,\u20092\"k\"\u22121, ... For example, the continued fraction expansion for formula_19 is [1;\u20091,\u20092,\u20091,\u20092,\u20091,\u20092,\u20091,\u20092,\u2009...]. Comparing the convergents with the approximants derived from the Babylonian method:\n\"x\"0 \n 1 \n\"x\"1 \n (1 + ) \n 2\n\"x\"2 \n (2 + ) \n\"x\"3 \n \nProperties.\nThe Baire space is a topological space on infinite sequences of natural numbers. The infinite continued fraction provides a homeomorphism from the Baire space to the space of irrational real numbers (with the subspace topology inherited from the usual topology on the reals). The infinite continued fraction also provides a map between the quadratic irrationals and the dyadic rationals, and from other irrationals to the set of infinite strings of binary numbers (i.e. the Cantor set); this map is called the Minkowski question-mark function. The mapping has interesting self-similar fractal properties; these are given by the modular group, which is the subgroup of M\u00f6bius transformations having integer values in the transform. Roughly speaking, continued fraction convergents can be taken to be M\u00f6bius transformations acting on the (hyperbolic) upper half-plane; this is what leads to the fractal self-symmetry.\nThe limit probability distribution of the coefficients in the continued fraction expansion of a random variable uniformly distributed in (0, 1) is the Gauss\u2013Kuzmin distribution.\nSome useful theorems.\nIf formula_20 formula_21 formula_22 formula_23 is an infinite sequence of positive integers, define the sequences formula_24 and formula_25 recursively:\nTheorem 1. For any positive real number formula_26\nformula_27\nTheorem 2. The convergents of formula_28 formula_21 formula_22 formula_31 are given by\nformula_32\nor in matrix form,formula_33\nTheorem 3. If the formula_34th convergent to a continued fraction is formula_35 then\nformula_36\nor equivalently\nformula_37\nCorollary 1: Each convergent is in its lowest terms (for if formula_24 and formula_25 had a nontrivial common divisor it would divide formula_40 which is impossible).\nCorollary 2: The difference between successive convergents is a fraction whose numerator is unity:\nformula_41\nCorollary 3: The continued fraction is equivalent to a series of alternating terms:\nformula_42\nCorollary 4: The matrix\nformula_43\nhas determinant formula_44, and thus belongs to the group of\nformula_45 unimodular matrices formula_46\nCorollary 5: The matrixformula_47\nhas determinant formula_48, or equivalently,formula_49meaning that the odd terms monotonically decrease, while the even terms monotonically increase.\nCorollary 6: The denominator sequence formula_50 satisfies the recurrence relation formula_51, and grows at least as fast as the Fibonacci sequence, which itself grows like formula_52 where formula_53 is the golden ratio.\nTheorem 4. Each (formula_54th) convergent is nearer to a subsequent (formula_34th) convergent than any preceding (formula_56th) convergent is. In symbols, if the formula_34th convergent is taken to be formula_58 then\nformula_59\nfor all formula_60 \nCorollary 1: The even convergents (before the formula_34th) continually increase, but are always less than formula_62\nCorollary 2: The odd convergents (before the formula_34th) continually decrease, but are always greater than formula_62\nTheorem 5.\nformula_65\nCorollary 1: A convergent is nearer to the limit of the continued fraction than any fraction whose denominator is less than that of the convergent.\nCorollary 2: A convergent obtained by terminating the continued fraction just before a large term is a close approximation to the limit of the continued fraction.Theorem 6: Consider the set of all open intervals with end-points formula_66. Denote it as formula_67. Any open subset of formula_68 is a disjoint union of sets from formula_67.Corollary: The infinite continued fraction provides a homeomorphism from the Baire space to formula_68.\nSemiconvergents.\nIf\nformula_71\nare consecutive convergents, then any fractions of the form\n formula_72\nwhere formula_73 is an integer such that formula_74, are called \"semiconvergents\", \"secondary convergents\", or \"intermediate fractions\". The formula_75-st semiconvergent equals the mediant of the formula_73-th one and the convergent formula_77. Sometimes the term is taken to mean that being a semiconvergent excludes the possibility of being a convergent (i.e., formula_78), rather than that a convergent is a kind of semiconvergent.\nIt follows that semiconvergents represent a monotonic sequence of fractions between the convergents formula_79 (corresponding to formula_80) and formula_81 (corresponding to formula_82). The consecutive semiconvergents formula_83 and formula_84 satisfy the property formula_85.\nIf a rational approximation formula_86 to a real number formula_87 is such that the value formula_88 is smaller than that of any approximation with a smaller denominator, then formula_86 is a semiconvergent of the continued fraction expansion of formula_87. The converse is not true, however.\nBest rational approximations.\nOne can choose to define a \"best rational approximation\" to a real number x as a rational number , \"d\" &gt; 0, that is closer to x than any approximation with a smaller or equal denominator. The simple continued fraction for x can be used to generate \"all\" of the best rational approximations for x by applying these three rules:\nFor example, 0.84375 has continued fraction [0;1,5,2,2]. Here are all of its best rational approximations.\nThe strictly monotonic increase in the denominators as additional terms are included permits an algorithm to impose a limit, either on size of denominator or closeness of approximation.\nThe \"half rule\" mentioned above requires that when a is even, the halved term a/2 is admissible if and only if |\"x\" \u2212 [\"a\"0 ; \"a\"1, ..., \"a\"\"k\" \u2212 1]| &gt; |\"x\" \u2212 [\"a\"0 ; \"a\"1, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\"/2]|. This is equivalent to:\n[\"a\"\"k\"; \"a\"\"k\" \u2212 1, ..., \"a\"1] &gt; [\"a\"\"k\"; \"a\"\"k\" + 1, ...].\nThe convergents to x are \"best approximations\" in a much stronger sense than the one defined above. Namely, n/d is a convergent for x if and only if |\"dx\" \u2212 \"n\"| has the smallest value among the analogous expressions for all rational approximations m/c with \"c\" \u2264 \"d\"; that is, we have |\"dx\" \u2212 \"n\"| &lt; |\"cx\" \u2212 \"m\"| so long as \"c\" &lt; \"d\". (Note also that |\"dkx\" \u2212 \"nk\"| \u2192 0 as \"k\" \u2192 \u221e.)\nBest rational within an interval.\nA rational that falls within the interval (\"x\",\u2009\"y\"), for , can be found with the continued fractions for x and y. When both x and y are irrational and\n\"x\" \n [\"a\"0; \"a\"1, \"a\"2, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\", \"a\"\"k\" + 1, ...]\n\"y\" \n [\"a\"0; \"a\"1, \"a\"2, ..., \"a\"\"k\" \u2212 1, \"b\"\"k\", \"b\"\"k\" + 1, ...]\nwhere x and y have identical continued fraction expansions up through \"a\"\"k\"\u22121, a rational that falls within the interval (\"x\",\u2009\"y\") is given by the finite continued fraction,\n\"z\"(\"x\",\"y\") \n [\"a\"0; \"a\"1, \"a\"2, ..., \"a\"\"k\" \u2212 1, min(\"a\"\"k\", \"b\"\"k\") + 1]\nThis rational will be best in the sense that no other rational in (\"x\",\u2009\"y\") will have a smaller numerator or a smaller denominator.\nIf x is rational, it will have \"two\" continued fraction representations that are \"finite\", \"x\"1 and \"x\"2, and similarly a rational\u00a0y will have two representations, \"y\"1 and \"y\"2. The coefficients beyond the last in any of these representations should be interpreted as +\u221e; and the best rational will be one of \"z\"(\"x\"1,\u2009\"y\"1), \"z\"(\"x\"1,\u2009\"y\"2), \"z\"(\"x\"2,\u2009\"y\"1), or \"z\"(\"x\"2,\u2009\"y\"2).\nFor example, the decimal representation 3.1416 could be rounded from any number in the interval [3.14155,\u20093.14165). The continued fraction representations of 3.14155 and 3.14165 are\n3.14155 \n [3; 7, 15, 2, 7, 1, 4, 1, 1] \n [3; 7, 15, 2, 7, 1, 4, 2]\n3.14165 \n [3; 7, 16, 1, 3, 4, 2, 3, 1] \n [3; 7, 16, 1, 3, 4, 2, 4]\nand the best rational between these two is\n[3; 7, 16] \n 3.1415929...\nThus, is the best rational number corresponding to the rounded decimal number 3.1416, in the sense that no other rational number that would be rounded to 3.1416 will have a smaller numerator or a smaller denominator.\nInterval for a convergent.\nA rational number, which can be expressed as finite continued fraction in two ways,\n\"z\" \n [\"a\"0; \"a\"1, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\", 1] \n [\"a\"0; \"a\"1, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\" + 1] \nwill be one of the convergents for the continued fraction expansion of a number, if and only if the number is strictly between (see https://)\n\"x\" \n [\"a\"0; \"a\"1, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\", 2] \n and\n\"y\" \n [\"a\"0; \"a\"1, ..., \"a\"\"k\" \u2212 1, \"a\"\"k\" + 2] \nThe numbers x and y are formed by incrementing the last coefficient in the two representations for z. It is the case that \"x\" &lt; \"y\" when k is even, and \"x\" &gt; \"y\" when k is odd.\nFor example, the number (Zu's fraction) has the continued fraction representations\n = [3; 7, 15, 1] = [3; 7, 16]\nand thus is a convergent of any number strictly between\nLegendre's theorem on continued fractions.\nIn his \"Essai sur la th\u00e9orie des nombres\" (1798), Adrien-Marie Legendre derives a necessary and sufficient condition for a rational number to be a convergent of the continued fraction of a given real number. A consequence of this criterion, often called Legendre's theorem within the study of continued fractions, is as follows:\nTheorem. If \"\u03b1\" is a real number and \"p\", \"q\" are positive integers such that formula_91, then \"p\"/\"q\" is a convergent of the continued fraction of \"\u03b1\".\nThis theorem forms the basis for Wiener's attack, a polynomial-time exploit of the RSA cryptographic protocol that can occur for an injudicious choice of public and private keys (specifically, this attack succeeds if the prime factors of the public key \"n\" = \"pq\" satisfy \"p\" &lt; \"q\" &lt; 2\"p\" and the private key \"d\" is less than (1/3)\"n\"1/4).\nComparison.\nConsider \"x\" \n [\"a\"0; \"a\"1, ...] and \"y\" \n [\"b\"0; \"b\"1, ...]. If k is the smallest index for which \"a\"\"k\" is unequal to \"b\"\"k\" then \"x\" &lt; \"y\" if (\u22121)\"k\"(\"a\"\"k\" \u2212 \"b\"\"k\") &lt; 0 and \"y\" &lt; \"x\" otherwise.\nIf there is no such k, but one expansion is shorter than the other, say \"x\" \n [\"a\"0; \"a\"1, ..., \"a\"\"n\"] and \"y\" \n [\"b\"0; \"b\"1, ..., \"b\"\"n\", \"b\"\"n\" + 1, ...] with \"a\"\"i\" \n \"b\"\"i\" for 0 \u2264 \"i\" \u2264 \"n\", then \"x\" &lt; \"y\" if n is even and \"y\" &lt; \"x\" if n is odd.\nContinued fraction expansion of \u03c0 and its convergents.\nTo calculate the convergents of \u03c0 we may set \"a\"0 \n \u230a\u03c0\u230b \n 3, define \"u\"1 \n \u2248 7.0625 and \"a\"1 \n \u230a\"u\"1\u230b \n 7, \"u\"2 \n \u2248 15.9966 and \"a\"2 \n \u230a\"u\"2\u230b \n 15, \"u\"3 \n \u2248 1.0034. Continuing like this, one can determine the infinite continued fraction of \u03c0 as\n[3;7,15,1,292,1,1...] (sequence in the OEIS).\nThe fourth convergent of \u03c0 is [3;7,15,1] = = 3.14159292035..., sometimes called Mil\u00fc, which is fairly close to the true value of \u03c0.\nLet us suppose that the quotients found are, as above, [3;7,15,1]. The following is a rule by which we can write down at once the convergent fractions which result from these quotients without developing the continued fraction.\nThe first quotient, supposed divided by unity, will give the first fraction, which will be too small, namely, . Then, multiplying the numerator and denominator of this fraction by the second quotient and adding unity to the numerator, we shall have the second fraction, , which will be too large. Multiplying in like manner the numerator and denominator of this fraction by the third quotient, and adding to the numerator the numerator of the preceding fraction, and to the denominator the denominator of the preceding fraction, we shall have the third fraction, which will be too small. Thus, the third quotient being 15, we have for our numerator (22\u2009\u00d7\u200915 \n 330) + 3 \n 333, and for our denominator, (7\u2009\u00d7\u200915 \n 105) + 1 \n 106. The third convergent, therefore, is . We proceed in the same manner for the fourth convergent. The fourth quotient being 1, we say 333 times 1 is 333, and this plus 22, the numerator of the fraction preceding, is 355; similarly, 106 times 1 is 106, and this plus 7 is 113.\nIn this manner, by employing the four quotients [3;7,15,1], we obtain the four fractions:\nTo sum up, the pattern is\nThese convergents are alternately smaller and larger than the true value of \u03c0, and approach nearer and nearer to \u03c0. The difference between a given convergent and \u03c0 is less than the reciprocal of the product of the denominators of that convergent and the next convergent. For example, the fraction is greater than \u03c0, but \u2212 \u03c0 is less than \u00a0=\u00a0 (in fact, \u2212 \u03c0 is just more than = ).\nThe demonstration of the foregoing properties is deduced from the fact that if we seek the difference between one of the convergent fractions and the next adjacent to it we shall obtain a fraction of which the numerator is always unity and the denominator the product of the two denominators. Thus the difference between and is , in excess; between and , , in deficit; between and , , in excess; and so on. The result being, that by employing this series of differences we can express in another and very simple manner the fractions with which we are here concerned, by means of a second series of fractions of which the numerators are all unity and the denominators successively be the product of every two adjacent denominators. Instead of the fractions written above, we have thus the series:\nThe first term, as we see, is the first fraction; the first and second together give the second fraction, ; the first, the second and the third give the third fraction , and so on with the rest; the result being that the series entire is equivalent to the original value.\nNon-simple continued fraction.\nA non-simple continued fraction is an expression of the form\nformula_92\nwhere the \"a\"\"n\" (\"n\" &gt; 0) are the partial numerators, the \"b\"\"n\" are the partial denominators, and the leading term \"b\"0 is called the \"integer\" part of the continued fraction.\nTo illustrate the use of non-simple continued fractions, consider the following example. The sequence of partial denominators of the simple continued fraction of \u03c0 does not show any obvious pattern:\nformula_93\nor\nformula_94\nHowever, several non-simple continued fractions for \u03c0 have a perfectly regular structure, such as:\nformula_95\nformula_96\nformula_97\nThe first two of these are special cases of the function with \u03c0 = 4\u2009arctan\u200a(1) and the fourth and fifth one can be derived using the Wallis product.\nformula_98\nThe continued fraction of formula_99 above consisting of cubes uses the Nilakantha series and an exploit from Leonhard Euler.\nOther continued fraction expansions.\nPeriodic continued fractions.\nThe numbers with periodic continued fraction expansion are precisely the irrational solutions of quadratic equations with rational coefficients; rational solutions have finite continued fraction expansions as previously stated. The simplest examples are the golden ratio \u03c6 = [1;1,1,1,1,1...] and \u221a2 = [1;2,2,2,2...], while \u221a14 = [3;1,2,1,6,1,2,1,6...] and \u221a42 = [6;2,12,2,12,2,12...]. All irrational square roots of integers have a special form for the period; a symmetrical string, like the empty string (for \u221a2) or 1,2,1 (for \u221a14), followed by the double of the leading integer.\nA property of the golden ratio \u03c6.\nBecause the continued fraction expansion for \u03c6 does not use any integers greater than 1, \u03c6 is one of the most \"difficult\" real numbers to approximate with rational numbers. Hurwitz's theorem states that any irrational number k can be approximated by infinitely many rational with\nformula_100\nWhile virtually all real numbers k will eventually have infinitely many convergents whose distance from k is significantly smaller than this limit, the convergents for \u03c6 (i.e., the numbers , , , , etc.) consistently \"toe the boundary\", keeping a distance of almost exactly formula_101 away from \u03c6, thus never producing an approximation nearly as impressive as, for example, for \u03c0. It can also be shown that every real number of the form , where a, b, c, and d are integers such that \"a\"\u2009\"d\" \u2212 \"b\"\u2009\"c\" = \u00b11, shares this property with the golden ratio \u03c6; and that all other real numbers can be more closely approximated.\nRegular patterns in continued fractions.\nWhile there is no discernible pattern in the simple continued fraction expansion of \u03c0, there is one for \"e\", the base of the natural logarithm:\nformula_102\nwhich is a special case of this general expression for positive integer n:\nformula_103\nAnother, more complex pattern appears in this continued fraction expansion for positive odd n:\nformula_104\nwith a special case for \"n\" = 1:\nformula_105\nOther continued fractions of this sort are\nformula_106\nwhere n is a positive integer; also, for integer n:\nformula_107\nwith a special case for \"n\" = 1:\nformula_108\nIf \"I\"\"n\"(\"x\") is the modified, or hyperbolic, Bessel function of the first kind, we may define a function on the rationals by\nformula_109\nwhich is defined for all rational numbers, with p and q in lowest terms. Then for all nonnegative rationals, we have\nformula_110\nwith similar formulas for negative rationals; in particular we have\nformula_111\nMany of the formulas can be proved using Gauss's continued fraction.\nTypical continued fractions.\nMost irrational numbers do not have any periodic or regular behavior in their continued fraction expansion. Nevertheless, for almost all numbers on the unit interval, they have the same limit behavior.\nThe arithmetic average diverges: formula_112, and so the coefficients grow arbitrarily large: formula_113. In particular, this implies that almost all numbers are well-approximable, in the sense thatformula_114Khinchin proved that the geometric mean of \"a\"\"i\" tends to a constant (known as Khinchin's constant):formula_115Paul L\u00e9vy proved that the nth root of the denominator of the nth convergent converges to L\u00e9vy's constant\nformula_116Lochs' theorem states that the convergents converge exponentially at the rate offormula_117\nApplications.\nPell's equation.\nContinued fractions play an essential role in the solution of Pell's equation. For example, for positive integers p and q, and non-square n, it is true that if \"p\"2 \u2212 \"nq\"2 = \u00b11, then is a convergent of the regular continued fraction for \u221a. The converse holds if the period of the regular continued fraction for \u221a is 1, and in general the period describes which convergents give solutions to Pell's equation.\nDynamical systems.\nContinued fractions also play a role in the study of dynamical systems, where they tie together the Farey fractions which are seen in the Mandelbrot set with Minkowski's question-mark function and the modular group Gamma.\nThe backwards shift operator for continued fractions is the map \"h\"(\"x\") \n 1/x \u2212 \u230a1/x\u230b called the Gauss map, which lops off digits of a continued fraction expansion: \"h\"([0; \"a\"1, \"a\"2, \"a\"3, ...]) \n [0; \"a\"2, \"a\"3, ...]. The transfer operator of this map is called the Gauss\u2013Kuzmin\u2013Wirsing operator. The distribution of the digits in continued fractions is given by the zero'th eigenvector of this operator, and is called the Gauss\u2013Kuzmin distribution.\nCataldi represented a continued fraction as formula_118 &amp; formula_119 &amp; formula_120 &amp; formula_121 with the dots indicating where the following fractions went.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46809", "revid": "15941940", "url": "https://en.wikipedia.org/wiki?curid=46809", "title": "Basel, Switzerland", "text": ""}
{"id": "46811", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=46811", "title": "Long Valley Caldera", "text": "Landform near Mammoth Mountain, California, USA\nLong Valley Caldera is a volcanic caldera in eastern California that is adjacent to Mammoth Mountain. The valley is one of the Earth's largest calderas, measuring about long (east-west), wide (north-south), and up to deep.\nLong Valley was formed 760,000 years ago when a very large eruption released hot ash that later cooled to form the Bishop tuff that is common to the area. The eruption emptied the magma chamber under the area to the point of collapse. The second phase of the eruption released pyroclastic flows that burned and buried thousands of square miles. Ash from this eruption blanketed much of the western part of what is now the United States.\nGeography.\nThe caldera is a giant bowl-shaped depression, approximately long, surrounded by mountains except to the southeast. The elevation of the bottom of the bowl ranges from , being higher in the west.\nNear the center of the bowl, magmatic uplift has formed a resurgent dome. The southeastern slope from the caldera down towards Bishop is filled with the Bishop Tuff, solidified ash that was ejected during the eruption that created the caldera. The Bishop tuff is thick in the caldera floor, and is cut by the Owens River Gorge, formed during the Pleistocene when the caldera filled with water and overtopped its rim.\nThe rim of the caldera is formed from pre-existing rock, rising about above the caldera floor. However, the eastern rim is lower, only about .\nMammoth Mountain is a lava dome complex west of the structural rim of the caldera, consisting of about 12 rhyodacite and dacite overlapping domes. These domes formed in a long series of eruptions from 110,000 to 57,000 years ago, building a volcano that reaches in elevation.\nThe Mono\u2013Inyo Craters are a volcanic chain situated along a narrow, north\u2013south-trending fissure system extending along the western rim of the caldera from Mammoth Mountain to the north shore of Mono Lake. The Mono-Inyo Craters erupted from 40,000 to 600 years ago, from a magma source separate from the Long Valley Caldera.\nThe caldera has an extensive hydrothermal system. Casa Diablo Hot Springs at the base of the resurgent dome hosts a geothermal power plant. Hot Creek cuts into part of the resurgent dome and passes through hot springs. The warm water of Hot Creek supports many trout, and is used at the Hot Creek Fish Hatchery. The creek was closed to swimming in 2006 after geothermal activity in the area increased. The area has a number of other hot springs, some of which are open to bathers.\nGeology.\nThe source of volcanism at Long Valley is still an active subject of research and debate. Most studies link volcanic activity to regional extension from the Basin and Range Province. Intrusions of mantle-derived basalt rise into the deep crust which supplies heat and volatiles that generate and repeatedly recharge a shallow silicic reservoir by partial melting of continental crust. Long Valley is not above a hotspot, nor is it the result of subduction as in the Cascades.\nThe known volcanic history of Long Valley Caldera area started a few million years ago when magma began to collect several miles below the surface. Volcanic activity became concentrated in the vicinity of the present site of Long Valley Caldera 3.1 to 2.5 million years ago with eruptions of rhyodacite followed by high-silica rhyolite from 2.1 to 0.8 million years ago. After some time, a cluster of mostly rhyolitic volcanoes formed in the area. All told, about were covered by lava.\nAll but one of these volcanoes, 1\u20132-million-year-old Glass Mountain (made of obsidian), were destroyed by the major (VEI-7) eruption of the area 760,000 years ago, which released of material from vents just inside the margin of the caldera. (The 1980 Mount St. Helens eruption was a VEI-5 eruption releasing .) About half of this material was ejected in a series of pyroclastic flows of a very hot () mixture of gases, pumice, and volcanic ash that covered the surrounding area hundreds of feet deep. One lobe of this material moved south into Owens Valley, past present-day Big Pine. Another lobe moved west over the crest of the Sierra Nevada and into the drainage of the San Joaquin River. The rest of the pyroclastic material, along with of other matter, was blown as far as into the air where winds distributed it as far away as eastern Nebraska and Kansas.\nThe eruption initially produced a caldera deep. However, much of the ejecta went straight up, fell down, and filled the initial caldera about two-thirds full.\nEruptions.\nSubsequent eruptions from the Long Valley magma chamber were confined within the caldera with extrusions of relatively hot (crystal-free) rhyolite 700,000 to 600,000 years ago as the caldera floor was uplifted to form the resurgent dome followed by extrusions of cooler, crystal-rich moat rhyolite at 200,000-year intervals (500,000, 300,000, and 100,000 years ago) in clockwise succession around the dome. The declining volcanic activity and increasingly crystalline lava extruded over the last 650,000 years, as well as other trends, suggest that the magma reservoir under the caldera has now largely crystallized and is unlikely to produce large-scale eruptions in the future.\nThe Long Valley volcano is unusual in that it has produced eruptions of both basaltic and silicic lava in the same geological place.\nWater from the Owens River filled the caldera to a depth of as of 600,000 years ago. At that time, the lake surface was at an elevation near . The lake drained sometime in the last 100,000 years after it overtopped the southern rim of the caldera, eroded the sill, and created the Owens River Gorge. A human-made dam in the gorge has created Crowley Lake, a partial restoration of the original lake. Since the great eruption, many hot springs developed in the area, and the resurgent dome has uplifted.\nDuring the last ice age, glaciers filled the canyons leading to Long Valley, but the valley floor was clear of ice. Excellent examples of terminal moraines can be seen at Long Valley. Laurel Creek, Convict Creek, and McGee Creek each have prominent moraines.\nRecent activity.\nIn May 1980, a strong earthquake swarm that included four Richter magnitude 6 earthquakes struck the southern margin of the Long Valley Caldera. It was associated with a dome-shaped uplift of the caldera floor. These events marked the onset of the latest period of caldera unrest that is ongoing. This ongoing unrest includes recurring earthquake swarms and continued dome-shaped uplift of the central section of the caldera accompanied by changes in thermal springs and gas emissions. After the quake, a secondary access road was created as a potential escape route for the town of Mammoth Lakes. Its name at first was proposed as the \"Mammoth Escape Route\" but was changed to the Mammoth Scenic Loop after Mammoth-area businesses and landowners complained.\nIn 1982, the United States Geological Survey under the Volcano Hazards Program began an intensive effort to monitor and study geologic unrest in Long Valley Caldera. The goal is to provide residents and civil authorities with reliable information on the nature of the potential hazards posed by this unrest and timely warning of an impending volcanic eruption, should it develop. Most, perhaps all, volcanic eruptions are preceded and accompanied by geophysical and geochemical changes in the volcanic system. Common precursory indicators of volcanic activity include increased seismicity, ground deformation, and variations in the nature and rate of gas emissions.\nHydrothermal system.\nThe Long Valley Caldera hosts an active hydrothermal system that includes hot springs, fumaroles (steam vents), and mineral deposits. Hot springs exist primarily in the eastern half of the caldera where land-surface elevations are relatively low; fumaroles exist primarily in the western half where elevations are higher. Mineral deposits from thermal activity are found on an uplifted area called the resurgent dome, at Little Hot Creek springs, Hot Creek Gorge, and other locations in the south and east moats of the caldera.\nHot springs discharge primarily in Hot Creek Gorge, along Little Hot Creek, and in the Alkali Lakes area. The largest springs are in Hot Creek Gorge where about per second of thermal water discharge and account for about 80% of the total thermal water discharge in the caldera. At the other extreme are springs at Hot Creek Fish Hatchery which contain a small component (2\u20135%) of thermal water that raises water temperatures about higher than background temperatures. Use of the warm spring water in the hatchery has increased fish production because trout growth rates are faster in the warm water than in ambient stream temperatures in Long Valley.\nIn hydrothermal systems, the circulation of groundwater is driven by a combination of topography and heat sources. In Long Valley Caldera, the system is recharged primarily from snowmelt in the highlands around the western and southern rims of the caldera. The water from snowmelt and rainfall infiltrates to depths of a few kilometers, where it is heated to at least by hot rock near geologically young intrusions. Upflow occurs in the west moat where the heated water with lower density rises along steeply inclined fractures to depths of . This hydrothermal fluid flows laterally, down the hydraulic gradient, from the west to the southeast around the resurgent dome and then eastward to discharge points along Hot Creek and around Crowley Lake. Reservoir temperatures in the volcanic fill decline from near the Inyo Craters to near Crowley Lake due to a combination of heat loss and mixing with cold water.\nHot Creek has been a popular swimming hole for decades. Over a dozen people have died in Hot Creek since the late 1960s, but most of these deaths happened to people who ignored the numerous warning signs and attempted to use the hydrothermal pools as hot tubs (like the stream portion of the creek, these pools alternate in temperature, but the eruptions in the pools are of super-heated water in already very hot water). Recent geothermal instability has led to its temporary closure for swimming. Officials are unsure of when (if ever) Hot Creek will officially reopen for swimming.\nHydrothermal activity has altered many rocks in the caldera, transforming them into travertine and clay. At the Huntley clay mine, white chalky clay called kaolinite is mined; the kaolinite is exposed on the resurgent dome and appears as a brilliant white band.\nTourism and hiking.\nThe largest tourist attraction in the caldera is the Mammoth Mountain Ski Area: the area offers skiing and snowboarding in the winter, and mountain biking in the summer. The Hot Creek tourist attraction was closed to swimming in 2006 due to increased geothermal activity.\nHiking and off-road vehicle driving is available throughout the caldera, and in the glacial valleys of the Sherwin Range, immediately to the south of the caldera. Hikers can hike to several lakes in these glacial valleys, including Valentine Lake, Convict Lake, Lake Dorothy, and Laurel Lakes. Crowley Lake, at the south end of the caldera, is noted for its fishing.\nThe nearest hotel accommodations to the caldera are in Mammoth Lakes. There are also campgrounds scattered throughout the caldera, and in the mountains near the edge of the caldera.\nFatalities.\nIn April 2006, three members of the Mammoth Mountain Ski Area ski patrol died while on duty. All three died from suffocation by carbon dioxide when they fell into a fumarole on the slopes of the mountain while attempting to fence it off.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46812", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=46812", "title": "Death valley", "text": ""}
{"id": "46814", "revid": "2544398", "url": "https://en.wikipedia.org/wiki?curid=46814", "title": "Nom de plume", "text": ""}
{"id": "46815", "revid": "15941946", "url": "https://en.wikipedia.org/wiki?curid=46815", "title": "William Fitzgerald Jenkins", "text": ""}
{"id": "46816", "revid": "6781", "url": "https://en.wikipedia.org/wiki?curid=46816", "title": "Dominique Aury", "text": ""}
{"id": "46817", "revid": "47233251", "url": "https://en.wikipedia.org/wiki?curid=46817", "title": "Mount Vesuvius", "text": "Active stratovolcano in the Gulf of Naples, Italy\nMount Vesuvius ( ) is a somma\u2013stratovolcano located on the Gulf of Naples in Campania, Italy, about east of Naples and a short distance from the shore. It is one of several volcanoes forming the Campanian volcanic arc. Vesuvius consists of a large cone partially encircled by the steep rim of a summit caldera, resulting from the collapse of an earlier, much higher structure.\nThe eruption of Mount Vesuvius in 79 AD destroyed the Roman cities of Pompeii, Herculaneum, Oplontis, Stabiae and other settlements. The eruption ejected a cloud of stones, ash and volcanic gases to a height of , erupting molten rock and pulverized pumice at the rate of per second. More than 1,000 people are thought to have died in the eruption, though the exact toll is unknown. The only surviving witness account consists of two letters by Pliny the Younger to the historian Tacitus.\nVesuvius has erupted many times since. It is the only volcano on Europe's mainland to have erupted in the last hundred years. It is regarded as one of the most dangerous volcanoes in the world because 3,000,000 people live near enough to be affected by an eruption, with at least 600,000 in the danger zone. Eruptions tend to be violent and explosive; these are known as Plinian eruptions.\nMythology.\nVesuvius has a long historic and literary tradition. It was considered a divinity of the Genius type at the time of the eruption of AD 79: it appears under the inscribed name Vesuvius as a serpent in the decorative frescos of many , or household shrines, surviving from Pompeii. An inscription from Capua to indicates that he was worshipped as a power of Jupiter; that is, \"Jupiter Vesuvius\".\nThe Romans regarded Mount Vesuvius as being devoted to Hercules. The historian Diodorus Siculus relates a tradition that Hercules, in the performance of his labors, passed through the country of nearby Cumae on his way to Sicily and found there a place called \"the Phlegraean Plain\" (, 'fiery'), \"from the mountain which of old spouted forth a huge fire [...] the mountain is called Vesuvius.\" It was inhabited by giant bandits, \"the sons of the Earth. With the gods' assistance, he pacified the region and continued. The facts behind the tradition, if any, remain unknown, as does whether was named after it. An epigram by the poet Martial in AD 88 suggests that both Venus, patroness of Pompeii, and Hercules were worshipped in the region devastated by the eruption of 79.\nEtymology.\n was a name of the volcano in frequent use by the authors of the late Roman Republic and the early Roman Empire. Its collateral forms were , , and . Writers in ancient Greek used or . Many scholars since then have offered an etymology. Given that peoples of varying ethnicity and language occupied Campania during the Roman Iron Age, the etymology depends to a large degree on the presumption of what language was spoken there at the time. Naples was settled by Greeks, as the name \"\", \"New City\", testifies. The Oscans, an Italic people, lived in the countryside. The Latins also competed for the occupation of Campania. Etruscan settlements were in the vicinity. Other peoples of unknown provenance are said to have been there at some time by various ancient authors.\nSome theories about its origin are:\nTopography.\nVesuvius is a \"humpbacked\" peak, consisting of a large cone () partially encircled by the steep rim of a summit caldera caused by the collapse of an earlier (and originally much higher) structure called Mount Somma. The Gran Cono was produced during the A.D. 79 eruption. For this reason, the volcano is also called Somma-Vesuvius or Somma-Vesuvio.\nThe caldera started forming during an eruption around 17,000\u201318,000 years ago and was enlarged by later paroxysmal eruptions, ending in the one of AD 79. This structure has given its name to the term \"somma volcano\", which describes any volcano with a summit caldera surrounding a newer cone.\nThe cliffs forming the northern ridge of Monte Somma's caldera rim reach a maximum height of at Punta Nasone. The summit of the main cone of Vesuvius is above sea level and more than above the long valley of Atrio di Cavallo (the northern floor of Monte Somma's caldera).\nThe volcano's slopes are scarred by lava flows, while the rest are heavily vegetated, with scrub and forests at higher altitudes and vineyards lower down.\nFormation.\nVesuvius is a stratovolcano and was formed as a result of the collision of two tectonic plates, the African and the Eurasian. The former was subducted at a convergent boundary beneath the latter, deeper into the earth. As the water-saturated sediments of the African oceanic plate were pushed to hotter depths inside the planet, the water boiled off and lowered the melting point of the upper mantle enough to partially melt the rocks. Because magma is less dense than the solid rock around it, it was pushed upward. Finding a weak spot at the Earth's surface, it broke through, thus forming the volcano.\nThe volcano is one of several forming the Campanian volcanic arc. Others include Campi Flegrei, a large caldera a few kilometers to the north-west, and Ischia, a volcanic island to the west, and several undersea volcanoes to the south. The arc forms the southern end of a larger chain of volcanoes produced by the subduction process described above, which extends northwest along the length of Italy as far as Monte Amiata in Southern Tuscany. Vesuvius is the only one to have erupted in recent history, although some of the others have erupted within the last few hundred years. Many are either extinct or have not erupted for tens of thousands of years.\nEruptions.\nMount Vesuvius has erupted many times. Numerous others preceded the eruption in AD 79 in prehistory, including at least three significantly larger; an example is the Avellino eruption around 1800 BC, which engulfed several Bronze Age settlements. Since AD 79, the volcano has also erupted repeatedly, in 172, 203, 222, possibly in 303, 379, 472, 512, 536, 685, 787, around 860, around 900, 968, 991, 999, 1006, 1037, 1049, around 1073, 1139, 1150, and there may have been eruptions in 1270, 1347, and 1500.\nThe volcano erupted again in 1631, six times in the 18th century (including 1779 and 1794), eight times in the 19th century (notably in 1872), and in 1906, 1929 and 1944. There have been no eruptions since 1944, and none of the eruptions after AD 79 were as large or destructive as the Pompeian one.\nThe eruptions vary greatly in severity but are characterized by explosive outbursts of the kind dubbed Plinian after Pliny the Younger, a Roman writer who published a detailed description of the AD 79 eruption, including his uncle's death. On occasion, eruptions from Vesuvius have been so large that the whole of southern Europe has been blanketed by ash; in 472 and 1631, Vesuvian ash fell on Constantinople (Istanbul), over away. A few times since 1944, landslides in the crater have raised clouds of ash dust, raising false alarms of an eruption.\nSince 1750, seven of the eruptions of Vesuvius have had durations of more than five years; only Mount Etna has had as many long-duration eruptions in the last 270 years. The two most recent eruptions of Vesuvius (1875\u20131906 and 1913\u20131944) each lasted more than 30 years.\nVesuvius is still regarded as an active volcano, although its current activity produces little more than sulfur-rich steam from vents at the bottom and walls of the crater.\nLayers of lava, ash, scoria and pumice make up the volcanic peak. Their mineralogy is variable, but generally silica-undersaturated and rich in potassium, with phonolite produced in the more explosive eruptions (e.g. the eruption in 1631 displaying a complete stratigraphic and petrographic description: phonolite was firstly erupted, followed by a tephritic phonolite and finally a phonolitic tephrite).\nVolcanic explosivity index.\nAccording to the Smithsonian Institution's Global Volcanism Program, Vesuvius has had 54 confirmed eruptions during the Holocene Epoch (the last 11,700 years). A volcanic explosivity index (VEI) has been assigned to all but one of these eruptions.\n&lt;templatestyles src=\"Template:Bar chart/styles.css\"/&gt;\nBefore AD 79.\nScientific knowledge of the geologic history of Vesuvius comes from core samples taken from a plus borehole on the flanks of the volcano, extending into Mesozoic rock. Cores were dated by potassium\u2013argon and argon\u2013argon dating. The area has been subject to volcanic activity for at least 400,000 years; the lowest layer of eruption material from the Somma caldera lies on top of the 40,000-year\u2011old Campanian ignimbrite produced by the Campi Flegrei complex. The volcanic complex stands on a large, sedimentary plain.\nSeveral surviving works written over the 200\u00a0years preceding the AD 79 eruption describe the mountain as having had a volcanic nature, although Pliny the Elder did not depict the mountain in this way in his \"Natural History\":\nEruption of AD 79.\nIn AD 79, Vesuvius erupted in one of the most catastrophic eruptions of all time. Historians have learned about the eruption from the eyewitness account of Pliny the Younger, a Roman administrator and poet. Several dates are given in the surviving copies of the letters. The latest evidence supports earlier findings and indicates that the eruption occurred after 17 October.\nThe volcano ejected a cloud of stones, ashes and volcanic gases to a height of , spewing molten rock and pulverized pumice at the rate of per second, ultimately releasing 100,000 times the thermal energy released by the Hiroshima-Nagasaki bombings. The cities of Pompeii and Herculaneum were destroyed by pyroclastic surges and the ruins buried under tens of metres of tephra.\nPrecursors and foreshocks.\nThe AD 79 eruption was preceded by a powerful earthquake in 62, which caused widespread destruction around the Bay of Naples, and particularly to Pompeii. Some of the damage had still not been repaired when the volcano erupted. The deaths of 600 sheep from \"tainted air\" in the vicinity of Pompeii indicates that the earthquake of AD 62 may have been related to new activity by Vesuvius.\nThe Romans grew accustomed to minor earth tremors in the region; the writer Pliny the Younger even wrote that they \"were not particularly alarming because they are frequent in Campania\". Small earthquakes started taking place four days before the eruption becoming more frequent over the next four days, but the warnings were not recognized.\nScientific analysis.\nReconstructions of the eruption and its effects vary considerably in the details but have the same overall features. The eruption lasted two days. The morning of the first day was perceived as normal by the only eyewitness to leave a surviving document, Pliny the Younger. In the middle of the day, an explosion threw up a high-altitude column from which ash and pumice began to fall, blanketing the area. Rescues and escapes occurred during this time. At some time in the night or early the next day, pyroclastic surges in the close vicinity of the volcano began. Lights were seen on the peak, interpreted as fires. People as far away as Misenum fled for their lives. The flows were rapid-moving, dense and very hot, knocking down, wholly or partly, all structures in their path, incinerating or suffocating all population remaining there and altering the landscape, including the coastline. Additional light tremors accompanied these and a mild tsunami in the Bay of Naples. By late afternoon of the second day, the eruption was over, leaving only haze in the atmosphere through which the sun shone weakly.\nThe latest scientific studies of the ash produced by Vesuvius reveal a multi-phase eruption. The initial major explosion produced a column of ash and pumice ranging between high, which rained on Pompeii to the southeast but not on Herculaneum upwind. The chief energy supporting the column came from the escape of steam superheated by the magma, created from seawater seeping over time into the deep faults of the region, which interacted with magma.\nSubsequently, the cloud collapsed as the gases expanded and lost their capability to support their solid contents, releasing it as a pyroclastic surge, which first reached Herculaneum but not Pompeii. Additional blasts reinstituted the column. The eruption alternated between Plinian and Pel\u00e9an six times. Surges 3 and 4 are believed by the authors to have buried Pompeii. Surges are identified in the deposits by dune and cross-bedding formations, which are not produced by fallout.\nAnother study used the magnetic characteristics of over 200 samples of roof-tile and plaster fragments collected around Pompeii to estimate the equilibrium temperature of the pyroclastic flow. The magnetic study revealed that on the first day of the eruption a fall of white pumice containing clastic fragments of up to fell for several hours. It heated the roof tiles up to . This period would have been the last opportunity to escape.\nThe collapse of the Plinian columns on the second day caused pyroclastic density currents (PDCs) that devastated Herculaneum and Pompeii. The depositional temperature of these pyroclastic surges reached up to . Any population remaining in structural refuges could not have escaped, as gases of incinerating temperatures surrounded the city. The lowest temperatures were in rooms under collapsed roofs, at approximately .\nThe two Plinys.\nThe only surviving eyewitness account of the event consists of two letters by Pliny the Younger to the historian Tacitus. Pliny the Younger describes, amongst other things, the last days in the life of his uncle, Pliny the Elder. Observing the first volcanic activity from Misenum across the Bay of Naples from the volcano, approximately , the elder Pliny launched a rescue fleet and went himself to the rescue of a personal friend. His nephew declined to join the party. One of the nephew's letters relates what he could discover from witnesses of his uncle's experiences. In a second letter, the younger Pliny details his own observations after the departure of his uncle.\nThe two men saw an extraordinarily dense cloud rising rapidly above the peak. This cloud and a request by a messenger for an evacuation by sea prompted the elder Pliny to order rescue operations in which he sailed away to participate. His nephew attempted to resume a normal life, but that night a tremor awoke him and his mother, prompting them to abandon the house for the courtyard. Further tremors near dawn caused the population to abandon the village and caused disastrous wave action in the Bay of Naples.\nA massive black cloud with lightning obscured the early-morning light, a scene Pliny describes as sheet lightning. The cloud obscured Point Misenum near at hand and the island of Capraia (Capri) across the bay. Fearing for their lives, the population began to flee the shore along the road. An ash rain fell, causing Pliny to shake it off periodically to avoid being buried. Later that same day, the pumice and ash stopped falling, and the sun shone weakly through the cloud, encouraging Pliny and his mother to return to their home and wait for news of Pliny the Elder.\nPliny's uncle, Pliny the Elder, was in command of the Roman fleet at Misenum and had meanwhile decided to investigate the phenomenon at close hand in a light vessel. As the ship was preparing to leave the area, a messenger came from his friend Rectina (wife of Tascius) living on the coast near the foot of the volcano, explaining that her party could only get away by sea and asking for rescue. Pliny ordered the immediate launching of the fleet galleys to the evacuation of the coast. He continued in his light ship to the rescue of Rectina's party.\nHe set off across the bay but, in the shallows on the other side, encountered thick showers of hot cinders, lumps of pumice and pieces of rock. Advised by the helmsman to turn back, he stated, \"Fortune favors the brave\" and ordered him to continue to Stabiae (about 4.5\u00a0km from Pompeii).\nPliny the Elder and his party saw what they believed to be flames coming from several parts of the crater. After staying overnight, the party was driven from the building by an accumulation of material, presumably tephra, which threatened to block all egress. They woke Pliny, who had been napping and emitting loud snoring. They elected to take to the fields with pillows tied to their heads to protect them from the raining debris. They approached the beach again, but the wind prevented the ships from leaving. Pliny sat down on a sail that had been spread for him and could not rise, even with assistance, when his friends departed. Though Pliny the Elder died, his friends ultimately escaped by land.\nIn the first letter to Tacitus, Pliny the Younger suggested that his uncle's death was due to the reaction of his weak lungs to a cloud of poisonous, sulphurous gas that wafted over the group. However, Stabiae was 16\u00a0km from the vent (roughly where the modern town of Castellammare di Stabia is situated), and his companions were unaffected by the volcanic gases. It is more likely that the corpulent Pliny died from another cause, such as a stroke or heart attack. His body was found with no apparent injuries the next day, after dispersal of the plume.\nCasualties.\nAlong with Pliny the Elder, the only other noble casualties of the eruption to be known by name were Agrippa (a son of the Herodian Jewish princess Drusilla and the procurator Antonius Felix) and his wife.\nBy 2003, around 1,044\u00a0casts made from impressions of bodies in the ash deposits had been recovered in and around Pompeii, with the scattered bones of another 100. The remains of about 332 bodies have been found at Herculaneum (300 in arched vaults discovered in 1980). What percentage these numbers are of the total dead or the percentage of the dead to the total number at risk remain unknown.\nThirty-eight percent of the 1,044 were found in the ash fall deposits, the majority inside buildings. These are thought to have been killed mainly by roof collapses, with the smaller number of victims found outside of buildings probably being killed by falling roof slates or by larger rocks thrown out by the volcano. The remaining 62% of remains found at Pompeii were in the pyroclastic surge deposits, and thus were probably killed by them \u2013 probably from a combination of suffocation from inhaling ashes and blast and debris thrown around. Examination of cloth, frescoes and skeletons shows that, in contrast to the victims found at Herculaneum, it is unlikely that high temperatures were a significant cause of the destruction at Pompeii. Herculaneum, much closer to the crater, was saved from tephra falls by the wind direction but was buried under of material deposited by pyroclastic surges. Likely, most of the known victims in this town were killed by the surges.\nPeople in Herculaneum, caught on the former seashore by the first surge, died of thermal shock. The rest were concentrated in arched chambers at a density of as high as three persons per square metre. As only of the coast have been excavated, further casualties may be discovered.\nLater eruptions from the 3rd to the 19th centuries.\nSince the eruption of AD 79, Vesuvius has erupted around three dozen times.\nThe volcano became quiescent at the end of the 13th century, and in the following years, it again became covered with gardens and vineyards as old. Even the inside of the crater was moderately filled with shrubbery.\nEruptions in the 20th century.\nIn March 1944, the United States Army Air Forces (USAAF) 340th Bombardment Group was based at Pompeii Airfield near Terzigno, Italy, just a few kilometres from the eastern base of the volcano. The tephra and hot ash from multiple days of the eruption damaged the fabric control surfaces, the engines, the Plexiglas windscreens and the gun turrets of the 340th's B-25 Mitchell medium bombers. Estimates ranged from 78 to 88 aircraft destroyed.\nThe eruption could be seen from Naples. Different perspectives and the damage caused to the local villages were recorded by USAAF photographers and other personnel based nearer to the volcano.\nFuture.\nLarge Vesuvian eruptions which emit volcanic material in quantities of about , the most recent of which overwhelmed Pompeii and Herculaneum, have happened after periods of inactivity of a few thousand years. Sub-Plinian eruptions producing about , such as those of 472 and 1631, have been more frequent with a few hundred years between them. From the 1631 eruption until 1944, there was a comparatively small eruption every few years, emitting 0.001\u20130.01\u00a0km3 of magma. For Vesuvius, the amount of magma expelled in an eruption increases roughly linearly with the interval since the previous one, and at a rate of around for each year. This gives an approximate figure of for an eruption after 80\u00a0years of inactivity.\nMagma sitting in an underground chamber for many years will start to see higher melting point constituents such as olivine crystallizing out. The effect is to increase the concentration of dissolved gases (mostly sulfur dioxide and carbon dioxide) in the remaining liquid magma, making the subsequent eruption more violent. As gas-rich magma approaches the surface during an eruption, the huge drop in internal pressure caused by the reduction in weight of the overlying rock (which drops to zero at the surface) causes the gases to come out of solution, the volume of gas increasing explosively from nothing to perhaps many times that of the accompanying magma. Additionally, the removal of the higher melting point material will raise the concentration of felsic components such as silicates, potentially making the magma more viscous, adding to the explosive nature of the eruption.\nThe government emergency plan for an eruption therefore assumes that the worst case will be an eruption of similar size and type to the 1631 VEI 4 eruption. In this scenario, the volcano's slopes, extending out to about from the vent, may be exposed to pyroclastic surges sweeping down them, whilst much of the surrounding area could suffer from tephra falls. Because of prevailing winds, towns and cities south and east of the volcano are most at risk from this. It is assumed that tephra accumulation exceeding \u2014at which point people are at risk from collapsing roofs\u2014may extend out as far as Avellino to the east or Salerno to the south-east. Near Naples, this tephra fall hazard is assumed to extend barely past the volcano's slopes to the northwest. The specific areas affected by the ash cloud depend upon the circumstances surrounding the eruption.\nThe plan assumes between two weeks and 20 days notice of an eruption and foresees the emergency evacuation of 600,000\u00a0people, almost entirely comprising all those living in the \"zona rossa\" (\"red zone\"), i.e. at greatest risk from pyroclastic flows. The evacuation, by train, ferry, car, and bus, is planned to take about seven days, and the evacuees would mostly be sent to other parts of the country, rather than to safe areas in the local Campania region, and may have to stay away for several months. However, the dilemma that would face those implementing the plan is when to start this massive evacuation: If it starts too late, thousands could be killed, whereas if it is started too early, the indicators of an eruption may turn out to be a false alarm. In 1984, 40,000\u00a0people were evacuated from the Campi Flegrei area, another volcanic complex near Naples, but no eruption occurred.\nOngoing efforts are being made by the government at various levels (especially of Campania) to reduce the population living in the red zone, by demolishing illegally constructed buildings, establishing a national park around the whole volcano to prevent the future construction of buildings and by offering sufficient financial incentives to people for moving away. One of the underlying goals is to reduce the time needed to evacuate the area, over the following twenty to thirty years (i.e. by 2023\u20132033), to two or three days.\nThe volcano is closely monitored by the Osservatorio Vesuvio in Ercolano with extensive networks of seismic and gravimetric stations, a combination of a GPS-based geodetic array and satellite-based synthetic aperture radar to measure ground movement and by local surveys and chemical analyses of gases emitted from fumaroles. All of this is intended to track magma rising underneath the volcano.\nThe official INGV monitoring bulletin from the Vesuvius Observatory, as of July 2024, classifies Mount Vesuvius at a Green Alert Level. This indicates a state of low volcanic activity. The surveillance system has not detected any significant changes in Vesuvius' activity state. The low-energy earthquakes are attributable to gravitational subsidence activity of rocks inside the crater.\nNational park.\nThe area around Vesuvius was officially declared a national park on 5 June 1995. The summit of Vesuvius is open to visitors, and there is a small network of paths around the volcano that are maintained by the park authorities on weekends. There is access by road to within of the summit (measured vertically), but after that, access is on foot only. There is a spiral walkway around the volcano from the road to the crater.\nFunicular.\nMount Vesuvius' first funicular \u2013 a type of vertical transport that uses two opposing, interconnected, rail-guided passenger cars always moving in concert \u2013 opened in 1880, subsequently destroyed by the March 1944 eruption.\n\"Funicul\u00ec, Funicul\u00e0\", a Neapolitan language song, was written to commemorate the opening of the first funicular on Mount Vesuvius.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46818", "revid": "41935478", "url": "https://en.wikipedia.org/wiki?curid=46818", "title": "Surveillance aircraft", "text": "Aircraft designed for sustained observation\nSurveillance aircraft or observation aircraft are aircraft used for aerial surveillance. They are primarily operated by military forces and government agencies in roles including intelligence gathering, maritime patrol, battlefield and airspace surveillance, observation (e.g. artillery spotting), and law enforcement.\nSurveillance aircraft usually carry limited defensive armament, if any. They do not require high-performance capability or stealth characteristics and may be modified civilian aircraft. Surveillance aircraft have also included moored balloons (e.g. TARS) and unmanned aerial vehicles (UAVs).\nHistory.\nPre World War I.\nThe French were the first to adopt hydrogen-filled balloons on the battlefield for reconnaissance. In the early 1790s, the French would deploy a hydrogen-filled balloon that held two soldiers: one who possessed a telescope, and the other would relay information to troops on the ground. These balloons did not cross into enemy lines; they were deployed on friendly lines for the purpose of surveillance from a higher point of view. These balloons formed the first air force in 1794, which was referred to as the Compagnie d'A\u00e9ronautiers. Also in 1794, during the Battle of Fleurus, the French Aerostatic Corps balloon \"L'Entreprenant\" remained afloat for nine hours. French officers used the balloon to observe the movements of the Austrian Army, dropping notes to the ground for collection by the French Army and also signaled messages using semaphores.\nThis method of surveillance would eventually be adopted by the Union Army in the Civil War. American inventor Thadeus Low proposed this invention to President Abraham Lincoln, to which a similar idea would be adopted. The Union Army would use balloons that could hold as many as five soldiers, and they would use telegraphs to relay information.\nIn the 1880s, a British meteorologist named Douglas Archibald experimented with unmanned surveillance vehicles. Douglas rigged cameras to a kite and used a long cable attached to the kite's string to activate the shutter. This invention would eventually catch the eyes of American Army Corporal William Eddy.\nDuring the Spanish-American War of 1898, Eddy adopted his own version of Archibald\u2019s kite-mounted camera. Eddy\u2019s kite was responsible for creating the first-ever military aerial surveillance photos.\nWorld War I.\nOne of the first aircraft used for surveillance was the Rumpler Taube during World War I, when aviators like Fred Zinn evolved entirely new methods of reconnaissance and photography. The translucent wings of the plane made it very difficult for ground-based observers to detect a Taube at an altitude above 400 m. The French also called this plane \"the Invisible Aircraft\", and it is sometimes also referred to as the \"world's very first stealth plane\". German Taube aircraft were able to detect the advancing Russian army during the Battle of Tannenberg (1914).\nAircraft were initially used for reconnaissance missions. The pilots of these initial aircraft would track the movement of enemy troops using photographs. These photos would be used to understand enemy formations and create maps that would eventually be used by infantry. By 1916, these aircraft would assist in the spotting of artillery, and the guidance and coordination of infantry. These aircraft forced enemy troops to camouflage their position to hide from aerial observation.\nEventually, surveillance aircraft would be highly valued due to commander\u2019s reliance on their information. However, surveillance aircraft would fly a low, slow, and predictable flight path, and with the introduction of aerial combat, surveillance aircraft were an easy target.\nWorld War II.\nPre-war, the British built and flew two Fleet Shadower aircraft, including the General Aircraft Fleet Shadower, that could follow and observe the enemy fleet at a distance. However, they were made obsolete by the 1940s with the introduction of airborne radar.\nAir observation posts were developed during World War II. Light aircraft such as the Auster were used by the British Royal Artillery for artillery spotting. By the mid-1960s, air observation was generally taken over by light observation helicopters.\nCold War.\nSpy flights were a source of major contention between the United States and the Soviet Union during most of the 1960s. Due to the difficulty of surveillance in the USSR, US policymakers established the National Reconnaissance Office. To combat this difficulty of surveillance, the US military developed the U2. This aircraft could fly at altitudes of 70,000 feet to avoid detection from KGB surveillance. The U2 was also equipped with a Hycon 73B camera. This camera was capable of capturing details as small as 2.5 feet wide. In 1962, a U2 captured images that discovered nuclear missiles in Cuba. These photos would initiate what is known as the Cuban Missile Crisis.\nAerial Reconnaissance was dangerous: Out of 152 cryptologists who died in the Cold War, 64 of them were participating in aerial reconnaissance missions. During the time period of 1945-1977, more than forty reconnaissance aircraft were shot down in the European and Pacific areas.\nThe US Military originally used standard aircraft like B-29s for reconnaissance missions. Eventually, variants of the aircraft were designed for reconnaissance, e.g. the C-130 and RC-130. These repurposed aircraft were sometimes referred to as \u201cferret\u201d aircraft, and intelligence personnel commanding these aircraft were nicknamed \u201cbackenders\u201d.\nThe United States also performed surveillance using repurposed Ryan Firebee unmanned target drones. Variants of these vehicles, designated the Model 147, could fly for 2500 miles.\nIn May 1991, the Department of the Navy reported that at least one UAV was airborne at all times during Operation Desert Storm.\nWar on terror.\nDuring the global war on terror, the US military developed defenses to surveillance aircraft to combat surveillance use. The United States military used precision cameras, drones that detect drones, and direct-energy weapons that disrupt control links and GPS navigation.\nRoles.\nMaritime patrol.\nThe main components of maritime surveillance consist of sightings from ship captains and aircraft pilots. However, due to the radar horizon, surveillance aircraft are preferred as they can identify targets hundreds of miles further than vessels. An example of this today is the Coast Guard\u2019s use of unmanned aerial systems (UASs) to improve their capabilities while reducing the risk for service members. Currently, the Coast Guard has roughly 250 drone certified officers across the US. The main uses of UASs within maritime activities are search and rescue operations and responding to different environmental disasters. The Coast Guard\u2019s use of unmanned drones specifically led them to creating an \u201cUnmanned Systems Strategic Plan.\u201d This plan would expand the use of current aerial surveillance systems to new challenges such as drug trafficking surveillance, migrant interdiction, and ice operations. With regards to environmental tasks, UASs will be expanded to address marine safety, fishing activity, and navigational uses. The Coast Guard outlines the future of aerial surveillance in maritime patrol as improving current UAS systems, integrating improved sensors and AI/ML, and creating more organized command and control plans/operations.\nMaritime patrol aircraft are typically large, slow machines capable of flying continuously for many hours, with a wide range of sensors. Such aircraft include the Hawker-Siddeley Nimrod, the Breguet Atlantique, the Tupolev Tu-95, the Lockheed P-2 Neptune and the Lockheed P-3 Orion/CP-140 Aurora. Smaller ship-launched observation seaplanes were used from World War I through World War II.\nLaw enforcement.\nUnmanned aircraft systems (UAS) are being increasingly deployed by U.S. law enforcement agencies. In August 2023, a Congressional Research Service to members of Congress described the multiple uses of these aircraft, including general surveillance and intelligence or evidence gathering. Unmanned surveillance drones can also be used to identify the locations of suspects who may be hiding or analyze the physical layout of a room before officers enter. Furthermore, unmanned surveillance drones can be used by law enforcement to light up large areas where it may be dark and difficult for officers to use traditional means of illumination. There are a few federal laws that apply to the use of unmanned surveillance systems, the Federal Aviation Administration (FAA) currently only has 2 options for the use of this technology by law enforcement. The first is that they can only operate them under 400 feet and need to maintain visual of the aircraft. Second, operators of the aircraft need to receive specific license and certifications to operate them. In response to the few and vague laws, the Department of Justice (DOJ) and Department of Homeland Security (DHS) has created policies to regulate the use and deployment of these drones domestically.\nPredator UAVs have been used by the US for border patrol.\nCurrent military applications.\nUnmanned Aerial Vehicle (UAV) surveillance aircraft have been \"deployed or are under development in many countries, including Israel, Iran, the UK, the United States, Canada, China, India, South Africa and Pakistan.\" Most air forces around the world lack dedicated surveillance planes.\nSeveral countries adapt aircraft for electronic intelligence (ELINT) gathering. The Beech RC-12 Super King Air and Boeing RC-135 Rivet Joint are examples of this activity.\nUnmanned surveillance UAVs include both airships\u2014such as Sky Sentinel and HiSentinel 80\u2014and airplanes.\nSouth China Sea.\nThe United States military has flown reconnaissance flights, called sensitive reconnaissance operations (SRO) by the U.S. Air Force, to monitor expansionist developments by the People\u2019s Republic of China, North Korea, and Russia in the Indo-Pacific region for decades; however, recent operations in the region have focused on monitoring movements by the People\u2019s Republic of China. More than ten different aircraft are used for SRO missions in the theater, including manned aircraft USAF RC-135 Rivet Joint and U-2 Dragon Lady, and the unmanned aircraft RQ-4 Global Hawk. Reconnaissance missions are capable of changing course within minutes to monitor activity and therefore used for reconnaissance missions more often than satellites, which can take hours or days to change position and are vulnerable to anti-satellite weapons.\nRussian invasion of Ukraine.\nSmall unmanned drones have been used by the Ukrainian military to identify enemy units and navigate artillery fire for safer and more efficient attacks on Russian targets, record propaganda videos of ambushes for posting on social media, and document alleged Russian war crimes and damages. Class I and III drone systems, classified by NATO as those of less than 150 kilograms and more than 600 kilograms, respectively, have been the most frequently used in the region. Turkish Bayraktar TB2 military drones have often been utilized by Ukraine in both reconnaissance and strike missions, and both Ukrainian and Russian militaries have used hobby drones donated to them by civilians, such as DJI Mavic mini drones, to conduct surveillance and strikes on enemy troops.\nIsrael-Hamas War.\nThe United States military had flown MQ-9 Reapers, unmanned aerial vehicles capable of more than 20 consecutive hours of flight, over the Gaza Strip for at least a month after the surprise attack on Israel by Hamas on October 7, 2023. According to the U.S. Defense Department, flights collected surveillance with the purpose of locating hostages taken by Hamas during the surprise attack on Israel and finding signs of life, but did not aid Israeli military ground operations. The British military also carried out flights over Gaza to locate hostages initially using unarmed Shadow R1 aircraft. As of March 2024, the Israeli military conducted hundreds of flight hours and almost 100 sorties in Gaza using the Oron reconnaissance aircraft, previously used as a business jet and upgraded to include advanced sensors and defense systems.\nIsrael-Hezbollah Conflict.\nOn June 18, 2024, Hezbollah released drone footage capturing sensitive sites in northern Israel, including military complexes and naval bases around Haifa. This action showcased areas such as the Rafael Military Industries Complex and various naval facilities. Hezbollah's campaign aims to intimidate and threaten Israel by displaying its surveillance capabilities and asserting its ability to penetrate Israeli defenses. This act highlights Hezbollah's growing technological and operational threats against Israel's security.\nBusiness aircraft.\nWith smaller equipment, long-range business aircraft can be modified in surveillance aircraft to perform specialized missions cost-effectively, from ground surveillance to maritime patrol:\nCurrent civilian applications.\nDrones are increasingly used in conservation work to complete tasks such as mapping forest cover, tracking wildlife, and enforcing environmental laws by catching illegal loggers or poachers.\nMonitoring protests.\nSurveillance drones, helicopters, and airplanes were deployed over 15 cities during the 2020 George Floyd protests. Unmanned aircraft were used to track the movements of protestors and to provide aerial views of violent acts and arson. The recorded video was sent to a digital network that could be accessed by various federal agencies and local law enforcement for use in criminal investigations. However, the National Air Security Operations Center stated the drones flew at a height that made it impossible to identify individuals or license plates.\nBorder patrol.\nSurveillance aircraft have recently been used to patrol maritime borders that are much longer than land borders and typically have less personnel. The Schengen Area in the European Union has recently used it to monitor their southern border in the Mediterranean. They gather intelligence including illegal crossings, search and rescue operations, smuggling, and fishing. Belgium has also deployed drones to monitor irregular maritime activity and to find children lost on the beach.\nEthics and regulations.\nPublic opinion.\nA 2014 survey from the Pew Center showed that pluralities or majorities of people in 39 of 44 countries oppose American drone strikes in the Middle East. Only in Israel, Kenya, and the USA do at least half of the public support American drone strikes. Additionally, following the Edward Snowden incident, concern within the US is only increasing regarding the government respecting people\u2019s privacy and civil liberties. Regarding the use of surveillance drones domestically in the US, the public tends to consider the benefits of this kind of surveillance versus the risks to individual privacy. Findings from an ethical analysis suggest people understand the benefits UAVs contribute to protecting the public while at the same time poses a risk to individual safety. A report from 2014 found 70%-73% of U.S. adults believed government use of surveillance drones was \u201cexcessive\u201d and \u201cviolates personal privacy.\u201d Subsequently, only 39% believed it \u201cincreased public safety\u201d and only 10% believed it was \u201cnecessary\u201d for surveillance. Furthermore, the public is more opposed to surveillance drones being in the hands of private individuals and businesses, rather than the government.\nApplicable law.\nIn the U.S., case law holds that airborne surveillance does not violate privacy rights protected under the 14th Amendment of the Constitution, so long as unmanned aircraft systems are not in \"general public use\". The lack of widespread use of such systems justifies individuals' reasonable expectations of privacy again this type of surveillance.\nIn the European Union, Article 7 of the Charter of Fundamental Rights of the European Union 2000 provides that people have a right of privacy and Article 8 protects the right to one's individual personal data. Under these provisions, aerial surveillance of public spaces would be lawful but surveillance of one's private home be subject to administrative oversight.\nThe Regulation of Investigatory Powers Act (RIPA) of 2000 applies to air surveillance in the United Kingdom. RIPA prohibits large-scale and generalized surveillance, and RIPA authorization is required for individualized surveillance of private residences.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46821", "revid": "36452830", "url": "https://en.wikipedia.org/wiki?curid=46821", "title": "Barry Took", "text": "English comedian\nBarry Took (19 June 1928\u00a0\u2013 31 March 2002) was an English writer, television presenter and comedian. His decade-and-a-half writing partnership with Marty Feldman led to the television series \"Bootsie and Snudge\", the radio comedy \"Round the Horne\" and other projects.\nHe is also remembered in the UK for presenting \"Points of View\", a BBC Television programme featuring viewers' letters on the BBC's output, and the BBC Radio 4 programme \"The News Quiz\".\nTook was known as the \"Father of Monty Python\", for bringing together the comedy performers who would establish \"Monty Python's Flying Circus\".\nEarly life and education.\nThe son of a manager at the Danish Bacon Company, Took was born in Victoria Road, Muswell Hill, north London, and lived in Winton Avenue, Bounds Green. When evacuated to Wisbech in Cambridgeshire during the Second World War, he ran away from his assigned home there, cycling 20 miles to Peterborough in order to get a train back to London. He attended Stationers School but left at the age of 15. His elder brother Philip would eventually work for the US Space Program before dying as a young man.\nCareer.\nWith his limited education, Took found work as an office boy for a publisher and a cinema projectionist. During his period of National Service in the Royal Air Force, in which he played the trumpet, he began performing and later worked as a stand-up comedian, eventually becoming a West End revue performer, working on \"For Amusement Only\" and \"For Adults Only\".\nTook's best comedy writing was done in collaboration with Marty Feldman, whom he first met in 1954. The two men wrote for several television shows in the 1950s and 1960s, including \"The Army Game\" and its spin-off \"Bootsie and Snudge\". He co-wrote \"Beyond Our Ken\" for two series (1958\u201359) with Eric Merriman for BBC Radio before leaving after a disagreement with his fellow writer. With Marty Feldman he wrote most episodes of \"Round the Horne\"; the intermittent partnership between them continued until 1974.\nIn the late 1960s, Took became comedy advisor to the BBC, and was responsible for bringing together the performers who formed \"Monty Python's Flying Circus\" before he moved to the US to work briefly on \"Rowan and Martin's Laugh In\". He returned to the UK in early 1970 and was involved in setting up the BBC series \"The Goodies\", although he had returned to take up the position of Head of Light Entertainment at London Weekend Television. He resigned from this position when Stella Richman, his superior and the Director of Programming, was dismissed. \"On the Move\" (1975\u201376), a programme linked to a national campaign to promote adult literacy, was written by Took and featured Bob Hoskins and Donald Gee. He was involved in two further television series in support of this initiative, \"Your Move\" and \"Write Away\".\nIn 1977, Took hosted his own comedy sketch show, \"Took and Co\". Also featuring Robin Bailey, Chris Emmett, Andrew Sachs and Gwen Taylor. The series ran for seven episodes late at night on ITV.\nIn 1979, he became chairman of \"The News Quiz\" on BBC Radio 4, a role he filled until 1981 and again from 1986 to 1995. In the same year he became a presenter of \"Points of View\", staying with the programme for over seven years.\nIn 1983 he became known to a younger TV audience when he presented the quiz, 'What's it', on the BBC1 Saturday morning show, \"Saturday Superstore\", in its first series.\nTook also hosted the BBC Radio 2 comedy panel game \"The Impressionists\", which included Peter Goodwright, Roger Kitter, David Jason and Dave Evans and, in 1998, the single-series revival of \"Twenty Questions\" titled \"Guess What?\".\nHe had seven books published, including his autobiography and several histories of comedy. He also wrote Kenneth Williams's life story for the \"Oxford Dictionary of National Biography\" in 1996.\nPersonal life and final years.\nDuring his time with the Royal Air Force Took met his first wife, Dorothy \"Dot\" Bird, who was serving in the Women's Royal Air Force. They married in 1950 and had three children (Barry, Susan and David), but were later divorced. In 1964, he married Lynden \"Lyn\" Leonard, this second marriage resulting in a daughter named Elinor. The couple separated in 1999, and eventually divorced. He also spoke publicly about his experiences with depression and of undergoing extensive psychotherapy for several years.\nAfter suffering from bladder cancer during the 1970s, he was diagnosed with cancer of the oesophagus in 1999, and suffered a stroke four weeks after undergoing major surgery. He died on 31 March 2002, aged 73.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46823", "revid": "48116544", "url": "https://en.wikipedia.org/wiki?curid=46823", "title": "George V", "text": "King of the United Kingdom from 1910 to 1936\nGeorge V (George Frederick Ernest Albert; 3 June 1865 \u2013 20 January 1936) was King of the United Kingdom and the British Dominions, and Emperor of India, from 6 May 1910 until his death in 1936.\nGeorge was born during the reign of his paternal grandmother, Queen Victoria, as the second son of the Prince and Princess of Wales (later King Edward VII and Queen Alexandra). He was third in the line of succession to the British throne behind his father, and his elder brother, Prince Albert Victor. From 1877 to 1892, George served in the Royal Navy, until his elder brother's unexpected death in January 1892 put him directly in line for the throne. The next year George married his brother's former fianc\u00e9e, Princess Victoria Mary of Teck, and they had six children. When Queen Victoria died in 1901, George's father ascended the throne as Edward VII, and George was created Prince of Wales. He became king-emperor on his father's death in 1910.\nGeorge's reign saw the rise of socialism, communism, fascism, Irish republicanism, and the Indian independence movement. All of these developments radically changed the political landscape of the British Empire, which itself reached its territorial peak by the beginning of the 1920s. The Parliament Act 1911 established the supremacy of the elected British House of Commons over the unelected House of Lords. As a result of the First World War, the empires of his first cousins Tsar Nicholas II of Russia and Kaiser Wilhelm II of Germany fell, while the British Empire expanded to its greatest effective extent. In 1917, George became the first monarch of the House of Windsor, which he renamed from the House of Saxe-Coburg and Gotha as a result of anti-German public sentiment. He appointed the first Labour ministry in 1924, and the 1931 Statute of Westminster recognised the Empire's Dominions as separate, independent states within the British Commonwealth of Nations.\nGeorge suffered from smoking-related health problems during his later reign. On his death in January 1936, he was succeeded by his eldest son, Edward VIII. Edward abdicated in December of that year and was succeeded by his younger brother Albert, who took the regnal name George VI.\nEarly life and education.\nGeorge was born on 3 June 1865, at Marlborough House, London. He was the second son of Albert Edward, Prince of Wales, and Alexandra, Princess of Wales. His father was the eldest son of Queen Victoria and Prince Albert, and his mother was the eldest daughter of King Christian IX and Queen Louise of Denmark. He was baptised at Windsor Castle on 7 July 1865 by the Archbishop of Canterbury, Charles Longley.\nAs a younger son of the Prince of Wales, there was little expectation that George would become king. He was third in line to the throne, after his father and elder brother, Prince Albert Victor. George was only 17\u00a0months younger than Albert Victor, and the two princes were educated together. John Neale Dalton was appointed as their tutor in 1871. Neither Albert Victor nor George excelled intellectually. As their father thought that the navy was \"the very best possible training for any boy\", in September 1877, when George was 12\u00a0years old, both brothers joined the cadet training ship HMS \"Britannia\" at Dartmouth, Devon.\nNaval service.\nFor three years from 1879, the princes served on , accompanied by Dalton. They toured the colonies of the British Empire in the Caribbean, South Africa and Australia, and visited Norfolk, Virginia, as well as South America, the Mediterranean, Egypt, and East Asia. In 1881 on a visit to Japan, George had a local artist tattoo a blue and red dragon on his arm, and was received in an audience by the Emperor Meiji; George and his brother presented Empress Haruko with two wallabies from Australia. At Jerusalem in 1882, the princes attended a Sephardic Passover dinner, and got tattoos of the Jerusalem Cross to commemorate their visit twenty years after their father had obtained the same tattoo. George wrote of the experience \"I was tattooed by the same man who tattooed Papa.\" Dalton wrote an account of their journey entitled \"The Cruise of HMS Bacchante\". Between Melbourne and Sydney, Dalton recorded a sighting of the \"Flying Dutchman\", a mythical ghost ship. When they returned to Britain, the Queen complained that her grandsons could not speak French or German, and so they spent six months in Lausanne in an ultimately unsuccessful attempt to learn another language. After Lausanne, the brothers were separated; Albert Victor attended Trinity College, Cambridge, while George continued in the Royal Navy. He travelled the world, visiting many areas of the British Empire. During his naval career he commanded \"Torpedo Boat 79\" in home waters, then on the North America and West Indies Station. His last active service was in command of HMS \"Melampus\" in 1891\u20131892. From then on, his naval rank was largely honorary.\nMarriage.\nAs a young man destined to serve in the navy, George served for many years under the command of his uncle Prince Alfred, Duke of Edinburgh, who was stationed in Malta. There, he grew close to and fell in love with his cousin Princess Marie of Edinburgh. His grandmother, father and uncle all approved the match, but his own mother and Marie's mother opposed it. The Princess of Wales thought the family was too pro-German, and the Duchess of Edinburgh disliked England. The Duchess, the only daughter of Alexander II of Russia, resented the fact that, as the wife of a younger son of the British sovereign, she had to yield precedence to George's mother, whose father had been a minor German prince before being called unexpectedly to the throne of Denmark. Guided by her mother, Marie refused George when he proposed to her. She married Ferdinand, Crown Prince of Romania, in 1893.\nIn November 1891, George's brother, Albert Victor, became engaged to their second cousin once removed Princess Victoria Mary of Teck, known as \"May\" within the family. Her parents were Francis, Duke of Teck (a member of a morganatic, cadet branch of the House of W\u00fcrttemberg), and Princess Mary Adelaide of Cambridge, a male-line granddaughter of George\u00a0III and a first cousin of Queen Victoria.\nOn 14 January 1892, six weeks after the formal engagement, Albert Victor died of pneumonia during an influenza pandemic, leaving George second in line to the throne and likely to succeed after his father. George had only just recovered from a serious illness himself, having been confined to bed for six weeks with typhoid fever, the disease that was thought to have killed his grandfather Prince Albert. Queen Victoria still regarded Princess May as a suitable match for her grandson, and George and May grew close during their shared period of mourning.\nA year after Albert Victor's death, George proposed to May and was accepted. They married on 6 July 1893 at the Chapel Royal in St James's Palace, London. Throughout their lives, they remained devoted to each other. George was, on his own admission, unable to express his feelings easily in speech, but they often exchanged loving letters and notes of endearment.\nDuke of York.\nThe death of his elder brother effectively ended George's naval career, as he was now second in line to the throne, after his father. George was created Duke of York, Earl of Inverness, and Baron Killarney by Queen Victoria on 24 May 1892, and received lessons in constitutional history from J. R. Tanner.\nThe Duke and Duchess of York had five sons and a daughter. Randolph Churchill claimed that George was a strict father, to the extent that his children were terrified of him, and that George had remarked to the Earl of Derby: \"My father was frightened of his mother, I was frightened of my father, and I am damned well going to see to it that my children are frightened of me.\" In reality, there is no direct source for the quotation and it is likely that George's parenting style was little different from that adopted by most people at the time. Whether this was the case or not, his children did seem to resent his strict nature, his son Prince Henry going as far as to describe him as a \"terrible father\" in later years.\nThey lived mainly at York Cottage, a relatively small house in Sandringham, Norfolk, where their way of life mirrored that of a comfortable middle-class family rather than royalty. George preferred a simple, almost quiet, life, in marked contrast to the lively social life pursued by his father. His official biographer, Harold Nicolson, later despaired of George's time as Duke of York, writing: \"He may be all right as a young midshipman and a wise old king, but when he was Duke of York\u00a0... he did nothing at all but kill [\"i.e.\" shoot] animals and stick in stamps.\" George was an avid stamp collector, which Nicolson disparaged, but George played a large role in building the Royal Philatelic Collection into the most comprehensive collection of United Kingdom and Commonwealth stamps in the world, in some cases setting record purchase prices for items.\nIn October 1894, George's maternal uncle-by-marriage, Alexander III of Russia, died. At the request of his father, \"out of respect for poor dear Uncle Sasha's memory\", George joined his parents in Saint Petersburg for the funeral. He and his parents remained in Russia for the wedding a week later of the new Russian emperor, his maternal first cousin Nicholas II, to one of George's paternal first cousins, Princess Alix of Hesse and by Rhine, who had once been considered as a potential bride for George's elder brother.\nPrince of Wales.\nAs Duke of York, George carried out a wide variety of public duties. On the death of Queen Victoria on 22 January 1901, George's father ascended the throne as King Edward VII. George inherited the title of Duke of Cornwall, and for much of the rest of that year, he was known as the Duke of Cornwall and York.\nIn 1901, the Duke and Duchess toured the British Empire. Their tour included Gibraltar, Malta, Port Said, Aden, Ceylon, Singapore, Australia, New Zealand, Mauritius, South Africa, Canada, and the Colony of Newfoundland. The tour was designed by Colonial Secretary Joseph Chamberlain with the support of Prime Minister Lord Salisbury to reward the Dominions for their participation in the South African War of 1899\u20131902. George presented thousands of specially designed South African War medals to colonial troops. In South Africa, the royal party met civic leaders, African leaders, and Boer prisoners, and was greeted by elaborate decorations, expensive gifts, and fireworks displays. Despite this, not all residents responded favourably to the tour. Many white Cape Afrikaners resented the display and expense, the war having weakened their capacity to reconcile their Afrikaner-Dutch culture with their status as British subjects. Critics in the English-language press decried the enormous cost at a time when families faced severe hardship.\nIn Australia, George opened the first session of the Australian Parliament on the creation of the Commonwealth of Australia. In New Zealand, he praised the military values, bravery, loyalty, and obedience to duty of New Zealanders, and the tour gave New Zealand a chance to show off its progress, especially in its adoption of up-to-date British standards in communications and the processing industries. The implicit goal was to advertise New Zealand's attractiveness to tourists and potential immigrants, while avoiding news of growing social tensions, by focusing the attention of the British press on a land few knew about. On his return to Britain, in a speech at Guildhall, London, George warned of \"the impression which seemed to prevail among [our] brethren across the seas, that the Old Country must wake up if she intends to maintain her old position of pre-eminence in her colonial trade against foreign competitors.\"\nOn 9 November 1901, George was created Prince of Wales and Earl of Chester. George's father wished to prepare him for his future role as king. In contrast to Edward himself, whom Queen Victoria had deliberately excluded from state affairs, George was given wide access to state documents by his father. George in turn allowed his wife access to his papers, as he valued her counsel and she often helped write his speeches. As Prince of Wales, he supported reforms in naval training, including cadets being enrolled at the ages of twelve and thirteen, and receiving the same education, whatever their class and eventual assignments. The reforms were implemented by the then Second (later First) Sea Lord, Sir John Fisher.\nFrom November 1905 to March 1906, George and May toured British India, where he was disgusted by racial discrimination and campaigned for greater involvement of Indians in the government of the country. The tour was almost immediately followed by a trip to Spain for the wedding of King Alfonso XIII to George's cousin Victoria Eugenie of Battenberg, at which the bride and groom narrowly avoided assassination when the driver of their coach and more than a dozen spectators were killed by a bomb thrown by an anarchist, Mateu Morral. A week after returning to Britain, George and May travelled to Norway for the coronation of King Haakon VII, George's cousin and brother-in-law, and Queen Maud, George's sister.\nReign.\nOn 6\u00a0May 1910, Edward\u00a0VII died, and George became king. He wrote in his diary:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I have lost my best friend and the best of fathers\u00a0... I never had a [cross] word with him in my life. I am heart-broken and overwhelmed with grief but God will help me in my responsibilities and darling May will be my comfort as she has always been. May God give me strength and guidance in the heavy task which has fallen on me.\nGeorge had never liked his wife's habit of signing official documents and letters as \"Victoria Mary\" and insisted she drop one of those names. They both thought she should not be called Queen Victoria, and so she became Queen Mary. Later that year, a radical propagandist, Edward Mylius, published a lie that George had secretly married in Malta as a young man, and that consequently his marriage to Queen Mary was bigamous. The lie had first surfaced in print in 1893, but George had shrugged it off as a joke. In an effort to kill off rumours, Mylius was arrested, tried and found guilty of criminal libel, and was sentenced to a year in prison.\nGeorge objected to the anti-Catholic wording of the Accession Declaration that he would be required to make at the opening of his first parliament. He made it known that he would refuse to open parliament unless it was changed. As a result, the Accession Declaration Act 1910 shortened the declaration and removed the most offensive phrases.\nGeorge and Mary's coronation took place at Westminster Abbey on 22\u00a0June 1911, and was celebrated by the Festival of Empire in London. In July, the King and Queen visited Ireland for five days; they received a warm welcome, with thousands of people lining the route of their procession to cheer.\nLater in 1911, the King and Queen travelled to India for the Delhi Durbar, where they were presented to an assembled audience of Indian dignitaries and princes as the Emperor and Empress of India on 12\u00a0December 1911. George wore the newly created Imperial Crown of India at the ceremony and declared the shifting of the Indian capital from Calcutta to Delhi. He was the only Emperor of India to be present at his own Delhi Durbar.\nAs he and Mary travelled throughout the subcontinent, George took the opportunity to indulge in big game hunting in Nepal, shooting 21\u00a0tigers, 8\u00a0rhinoceroses and a bear over 10\u00a0days. He was a keen and expert marksman. On a later occasion, on 18\u00a0December 1913, he was part of a shooting party that shot over a thousand pheasants in six hours (about one bird every 20\u00a0seconds) while visiting the home of Lord Burnham. Even George had to acknowledge that \"we went a little too far\" that day.\nNational politics.\nGeorge inherited the throne at a politically turbulent time. Lloyd George's People's Budget had been rejected the previous year by the Conservative and Unionist-dominated House of Lords, contrary to the normal convention that the Lords did not veto money bills. Liberal Prime Minister H. H. Asquith had asked the previous king to give an undertaking that he would create sufficient Liberal peers to allow the passage of Liberal legislation. Edward had reluctantly agreed, provided the Lords rejected the budget after two successive general elections. After the January 1910 general election, the Conservative peers allowed the budget, for which the government now had an electoral mandate, to pass without a vote.\nAsquith attempted to curtail the power of the Lords through constitutional reforms, which were again blocked by the Upper House. A constitutional conference on the reforms broke down in November 1910 after 21 meetings. Asquith and Lord Crewe, Liberal leader in the Lords, asked George to grant a dissolution, leading to a second general election, and to promise to create sufficient Liberal peers if the Lords blocked the legislation again. If George refused, the Liberal government would otherwise resign, which would have given the appearance that the monarch was taking sides \u2013 with \"the peers against the people\" \u2013 in party politics. The King's two private secretaries, the Liberal Lord Knollys and the Unionist Lord Stamfordham, gave George conflicting advice. Knollys advised George to accept the Cabinet's demands, while Stamfordham advised George to accept the resignation. Like his father, George reluctantly agreed to the dissolution and creation of peers, although he felt his ministers had taken advantage of his inexperience to browbeat him. After the December 1910 general election, the Lords let the bill pass on hearing of the threat to swamp the house with new peers. The subsequent Parliament Act 1911 permanently removed \u2013 with a few exceptions \u2013 the power of the Lords to veto bills. George later came to feel that Knollys had withheld information from him about the willingness of the opposition to form a government if the Liberals had resigned.\nThe 1910 general elections had left the Liberals as a minority government dependent upon the support of the Irish Nationalist Party. As desired by the Nationalists, Asquith introduced legislation that would give Ireland Home Rule, but the Conservatives and Unionists opposed it. As tempers rose over the Home Rule Bill, which would never have been possible without the Parliament Act, relations between the elderly Knollys and the Conservatives became poor, and he was pushed into retirement. Desperate to avoid the prospect of civil war in Ireland between Unionists and Nationalists, George called a meeting of all parties at Buckingham Palace in July 1914 in an attempt to negotiate a settlement. After four days the conference ended without an agreement. Political developments in Britain and Ireland were overtaken by events in Europe, and the issue of Irish Home Rule was suspended for the duration of the war.\nFirst World War.\nOn 4 August 1914, George wrote in his diary, \"I held a council at 10:45 to declare war with Germany. It is a terrible catastrophe but it is not our fault.\u00a0... Please to God it may soon be over.\" From 1914 to 1918, Britain and its allies were at war with the Central Powers, led by the German Empire. German Kaiser Wilhelm II, who for the British public came to symbolise all the horrors of the war, was the King's first cousin. George's paternal grandfather was Prince Albert of Saxe-Coburg and Gotha; consequently, the King and his children bore the German titles Prince and Princess of Saxe-Coburg and Gotha and Duke and Duchess of Saxony. Queen Mary, although born in England like her mother, was the daughter of the Duke of Teck, a descendant of the German Dukes of W\u00fcrttemberg. George had brothers-in-law and cousins who were British subjects but who bore German titles such as Duke and Duchess of Teck, Prince and Princess of Battenberg, and Prince and Princess of Schleswig-Holstein. When H. G. Wells wrote about Britain's \"alien and uninspiring court\", George replied: \"I may be uninspiring, but I'll be damned if I'm alien.\"\nOn 17 July 1917, George appeased British nationalist feelings by issuing a royal proclamation that changed the name of the British royal house from the German-sounding House of Saxe-Coburg and Gotha to the House of Windsor. He and all his British relatives relinquished their German titles and styles and adopted British-sounding surnames. George compensated his male relatives by giving them British peerages. His cousin Prince Louis of Battenberg, who earlier in the war had been forced to resign as First Sea Lord through anti-German feeling, became Louis Mountbatten, 1st Marquess of Milford Haven, while Queen Mary's brothers became Adolphus Cambridge, 1st Marquess of Cambridge, and Alexander Cambridge, 1st Earl of Athlone.\nIn letters patent gazetted on 11 December 1917, the King restricted the style of \"Royal Highness\" and the titular dignity of \"Prince (or Princess) of Great Britain and Ireland\" to the children of the Sovereign, the children of the sons of the Sovereign and the eldest living son of the eldest son of a Prince of Wales. The letters patent also stated that \"the titles of Royal Highness, Highness or Serene Highness, and the titular dignity of Prince and Princess shall cease except those titles already granted and remaining unrevoked\". George's relatives who fought on the German side, such as Ernest Augustus, Crown Prince of Hanover, and Charles Edward, Duke of Saxe-Coburg and Gotha, had their British peerages suspended by a 1919 Order in Council under the provisions of the Titles Deprivation Act 1917. Under pressure from his mother, George also removed the Garter flags of his German relations from St George's Chapel, Windsor Castle.\nWhen Tsar Nicholas II of Russia, George's first cousin, was overthrown in the Russian Revolution of 1917, the British government offered political asylum to the Tsar and his family, but worsening conditions for the British people, and fears that revolution might come to the British Isles, led George to think that the presence of the Romanovs would be seen as inappropriate. Despite the later claims of Lord Mountbatten of Burma that Prime Minister David Lloyd George was opposed to the rescue of the Russian imperial family, the letters of Lord Stamfordham suggest that it was George\u00a0V who opposed the idea against the advice of the government. Advance planning for a rescue was undertaken by MI1, a branch of the British secret service, but because of the strengthening position of the Bolshevik revolutionaries and wider difficulties with the conduct of the war, the plan was never put into operation. Nicholas and his immediate family remained in Russia, where they were killed by the Bolsheviks in 1918. George wrote in his diary: \"It was a foul murder. I was devoted to Nicky, who was the kindest of men and thorough gentleman: loved his country and people.\" The following year, Nicholas's mother, Marie Feodorovna, and other members of the extended Russian imperial family were rescued from Crimea by a British warship.\nTwo months after the end of the war, the King's youngest son, John, died aged\u00a013 after a lifetime of ill health. George was informed of his death by Queen Mary, who wrote, \"[John] had been a great anxiety to us for many years\u00a0... The first break in the family circle is hard to bear but people have been so kind &amp; sympathetic &amp; this has helped us much.\"\nIn May 1922, George toured Belgium and northern France, visiting the First World War cemeteries and memorials being constructed by the Imperial War Graves Commission. The event was described in a poem, \"The King's Pilgrimage\" by Rudyard Kipling. The tour, and one short visit to Italy in 1923, were the only times George agreed to leave the United Kingdom on official business after the end of the war.\nPost-war reign.\nBefore the First World War, most of Europe was ruled by monarchs related to George, but during and after the war, the monarchies of Austria, Germany, Greece, and Spain, like Russia, fell to revolution and war. In March 1919, Lieutenant-Colonel Edward Lisle Strutt was dispatched on the personal authority of the King to escort the former Emperor Charles I of Austria and his family to safety in Switzerland. In 1922, a Royal Navy ship was sent to Greece to rescue his cousins Prince and Princess Andrew.\nPolitical turmoil in Ireland continued as the Nationalists fought for independence; George expressed his horror at government-sanctioned killings and reprisals to Prime Minister Lloyd George. At the opening session of the Parliament of Northern Ireland on 22 June 1921, the King appealed for conciliation in a speech part drafted by General Jan Smuts and approved by Lloyd George. A few weeks later, a truce was agreed. Negotiations between Britain and the Irish secessionists led to the signing of the Anglo-Irish Treaty. By the end of 1922, Ireland was partitioned, the Irish Free State was established, and Lloyd George was out of office.\nGeorge and his advisers were concerned about the rise of socialism and the growing labour movement, which they mistakenly associated with republicanism. The socialists no longer believed in their anti-monarchical slogans and were ready to come to terms with the monarchy if it took the first step. George adopted a more democratic, inclusive stance that crossed class lines and brought the monarchy closer to the public and the working class\u2014a dramatic change for the King, who was most comfortable with naval officers and landed gentry. He cultivated friendly relations with moderate Labour Party politicians and trade union officials. His abandonment of social aloofness conditioned the royal family's behaviour and enhanced its popularity during the economic crises of the 1920s and for over two generations thereafter.\nThe years between 1922 and 1929 saw frequent changes in government. In 1924, George appointed the first Labour Prime Minister, Ramsay MacDonald, in the absence of a clear majority for any one of the three major parties. George's tact in appointing the first Labour government (which lasted less than a year) allayed the suspicions of the party's sympathisers that he would work against their interests. During the General Strike of 1926, George advised the government of Conservative Stanley Baldwin against taking inflammatory action, and took exception to suggestions that the strikers were \"revolutionaries\" saying, \"Try living on their wages before you judge them.\"\nIn 1926, George hosted an Imperial Conference in London at which the Balfour Declaration accepted the growth of the British Dominions into self-governing \"autonomous Communities within the British Empire, equal in status, in no way subordinate one to another\". The Statute of Westminster 1931 formalised the Dominions' legislative independence and established that the succession to the throne could not be changed unless all the Parliaments of the Dominions as well as the Parliament at Westminster agreed. The Statute's preamble described the monarch as \"the symbol of the free association of the members of the British Commonwealth of Nations\", who were \"united by a common allegiance\".\nIn the wake of a world financial crisis, George encouraged the formation of a National Government in 1931 led by MacDonald and Baldwin, and volunteered to reduce the civil list to help balance the budget. He was concerned by the rise to power in Germany of Adolf Hitler and the Nazi Party. In 1934, George bluntly told the German ambassador Leopold von Hoesch that Germany was now the peril of the world, and that there was bound to be a war within ten years if Germany went on at the present rate; he warned the British ambassador in Berlin, Eric Phipps, to be suspicious of the Nazis.\nIn 1932, George agreed to deliver a Royal Christmas speech on the radio, an event that became annual thereafter. He was not in favour of the innovation originally but was persuaded by the argument that it was what his people wanted. By the Silver Jubilee of his reign in 1935, he had become a well-loved king, saying in response to the crowd's adulation, \"I cannot understand it, after all I am only a very ordinary sort of fellow.\"\nGeorge's relationship with his eldest son and heir, Edward, deteriorated in these later years. George was disappointed in Edward's failure to settle down in life and appalled by his many affairs with married women. In contrast, he was fond of his second son, Prince Albert (later George\u00a0VI), and doted on his eldest granddaughter, Princess Elizabeth; he nicknamed her \"Lilibet\", and she affectionately called him \"Grandpa England\". In 1935, George said of his son Edward: \"After I am dead, the boy will ruin himself within 12\u00a0months\", and of Albert and Elizabeth: \"I pray to God my eldest son will never marry and have children, and that nothing will come between Bertie and Lilibet and the throne\".\nDeclining health and death.\nThe First World War took a toll on George's health: he was seriously injured on 28 October 1915 when thrown by his horse at a troop review in France, and his heavy smoking exacerbated recurring breathing problems. He suffered from chronic bronchitis. In 1925, on the instruction of his doctors, he was reluctantly sent on a recuperative private cruise in the Mediterranean; it was his third trip abroad since the war, and his last. In November\u00a01928, he fell seriously ill with septicaemia, which localised between the base of his right lung and diaphragm in the form of an empyema that required drainage. For the next two years his son Edward took over many of his duties. In 1929, the suggestion of a further rest abroad was rejected by the King \"in rather strong language\". Instead, he retired for three months to Craigweil House, Aldwick, in the seaside resort of Bognor, Sussex. As a result of his stay, the town acquired the suffix \"Regis\" \u2013 Latin for \"of the King\". A myth later grew that his last words, on being told that he would soon be well enough to revisit the town, were \"Bugger Bognor!\"\nGeorge never fully recovered. In his final year, he was occasionally administered oxygen. The death of his favourite sister, Victoria, in December 1935 depressed him deeply. On the evening of 15\u00a0January 1936, George took to his bedroom at Sandringham House complaining of a cold; he remained in the room until his death. He became gradually weaker, drifting in and out of consciousness. Prime Minister Stanley Baldwin later said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;... each time he became conscious it was some kind inquiry or kind observation of someone, some words of gratitude for kindness shown. But he did say to his secretary when he sent for him: \"How is the Empire?\" An unusual phrase in that form, and the secretary said: \"All is well, sir, with the Empire\", and the King gave him a smile and relapsed once more into unconsciousness.\nBy 20 January, George was close to death. His physicians, led by Lord Dawson of Penn, issued a bulletin with the words \"The King's life is moving peacefully towards its close.\" Dawson's private diary, unearthed after his death and made public in 1986, reveals that George's last words, a mumbled \"God damn you!\", were addressed to his nurse, Catherine Black, when she gave him a sedative that night. Dawson, who supported the \"gentle growth of euthanasia\", admitted in the diary that he ended the King's life:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;At about 11 o'clock it was evident that the last stage might endure for many hours, unknown to the Patient but little comporting with that dignity and serenity which he so richly merited and which demanded a brief final scene. Hours of waiting just for the mechanical end when all that is really life has departed only exhausts the onlookers &amp; keeps them so strained that they cannot avail themselves of the solace of thought, communion or prayer. I therefore decided to determine the end and injected (myself) morphia gr.3/4 [grains] and shortly afterwards cocaine gr.1 [grains] into the distended jugular vein\u00a0... In about 1/4 an hour \u2013 breathing quieter \u2013 appearance more placid \u2013 physical struggle gone.Dawson wrote that he acted to preserve the King's dignity, to prevent further strain on the family, and so that George's death at 11:55\u00a0pm could be announced in the morning edition of \"The Times\" newspaper rather than \"less appropriate\u00a0... evening journals\". Neither Queen Mary, who was intensely religious and might not have sanctioned euthanasia, nor the Prince of Wales were consulted. The royal family did not want the King to endure pain and suffering and did not want his life prolonged artificially but neither did they approve Dawson's actions. \"British Path\u00e9\" announced the King's death the following day, in which he was described as \"for each one of us, more than a King, a father of a great family\".\nThe German composer Paul Hindemith went to a BBC studio on the morning after the King's death and in six hours wrote \"Trauermusik\" (\"Mourning Music\"), for viola and orchestra. It was performed that same evening in a live broadcast by the BBC, with Adrian Boult conducting the BBC Symphony Orchestra and the composer as soloist.\nAt the procession to George's lying in state in Westminster Hall, the cross surmounting the Imperial State Crown atop George's coffin fell off and landed in the gutter as the cort\u00e8ge turned into New Palace Yard. George's eldest son and successor, Edward VIII, saw it fall and wondered whether it was a bad omen for his new reign. As a mark of respect to their father, George's four surviving sons \u2013 Edward, Albert, Henry, and George \u2013 mounted the guard, known as the Vigil of the Princes, at the catafalque on the night before the funeral. The vigil was not repeated until the death of George's daughter-in-law, Queen Elizabeth The Queen Mother, in 2002. George V was interred at St George's Chapel, Windsor Castle, on 28 January 1936. Edward abdicated before the year was out, leaving Albert to ascend the throne as George VI.\nEstate.\nGeorge V reportedly saved \u00a3487,000 from the Civil List over the course of his reign. During the financial negotiations carried out during the abdication of Edward VIII, it was revealed that George V had amassed a private fortune of approximately \u00a33,000,000 by the time he died; under the terms of his will, his four surviving younger children Albert, Mary, Henry and George each received \u00a3750,000. His will was read at a private family meeting at Sandringham on 22 January 1936; when the new king (Edward VIII) queried why no provision had been made for him under his father's will, the late king's solicitor Sir Bernard Halsey-Bircham explained that George V had assumed that his eldest son would have built up a substantial fortune of his own from the revenues of the Duchy of Cornwall during his tenure as Prince of Wales.\nLegacy.\nGeorge V disliked sitting for portraits and despised modern art; he was so displeased by one portrait by Charles Sims that he ordered it to be burned. He did admire sculptor Bertram Mackennal, who created statues of George for display in Madras and Delhi, and William Reid Dick, whose statue of George V stands outside Westminster Abbey, London.\nAlthough he and his wife occasionally toured the British Empire, George preferred to stay at home pursuing his hobbies of stamp collecting and game shooting and lived a life that later biographers would consider dull because of its conventionality. He was earnestly devoted to Britain and its Empire. He explained, \"it has always been my dream to identify myself with the great idea of Empire.\" He appeared hard-working and became widely admired by the people of Britain and the Empire, as well as \"the Establishment\". In the words of historian David Cannadine, King George V and Queen Mary were an \"inseparably devoted couple\" who upheld \"character\" and \"family values\".\nGeorge established a standard of conduct for British royalty that reflected the values and virtues of the upper middle-class rather than upper-class lifestyles or vices. Acting within his constitutional bounds, he dealt skilfully with a succession of crises: Ireland, the First World War, and the first socialist minority government in Britain. He was by temperament a traditionalist who never fully appreciated or approved the revolutionary changes under way in British society. Nevertheless, he invariably wielded his influence as a force of neutrality and moderation, seeing his role as mediator rather than final decision maker.\nTitles, honours and arms.\nAs Duke of York, George's arms were the royal arms, with an inescutcheon of the arms of Saxony, all differenced with a label of three points argent, the centre point bearing an anchor azure. The anchor was removed from his coat of arms as the Prince of Wales. As King, he bore the royal arms. In 1917, he removed, by warrant, the Saxony inescutcheon from the arms of all male-line descendants of the Prince Consort domiciled in the United Kingdom (although the royal arms themselves had never borne the shield).\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46825", "revid": "302229", "url": "https://en.wikipedia.org/wiki?curid=46825", "title": "Otto Hahn", "text": "German nuclear chemist and Nobel laureate (1879\u20131968)\nOtto Hahn (; 8 March 1879\u00a0\u2013 28 July 1968) was a German chemist who was a pioneer in the field of radiochemistry. He is referred to as the father of nuclear chemistry and discoverer of nuclear fission, the science behind nuclear reactors and nuclear weapons. Hahn and Lise Meitner discovered isotopes of the radioactive elements radium, thorium, protactinium and uranium. He also discovered the phenomena of atomic recoil and nuclear isomerism, and pioneered rubidium\u2013strontium dating. In 1938, Hahn, Meitner and Fritz Strassmann discovered nuclear fission, for which Hahn alone was awarded the 1944 Nobel Prize in Chemistry.\nA graduate of the University of Marburg, which awarded him a doctorate in 1901, Hahn studied under Sir William Ramsay at University College London and at McGill University in Montreal, Canada, under Ernest Rutherford, where he discovered several new radioactive isotopes. He returned to Germany in 1906; Emil Fischer let him use a former woodworking shop in the basement of the Chemical Institute at the University of Berlin as a laboratory. Hahn completed his habilitation in early 1907 and became a \"Privatdozent\". In 1912, he became head of the Radioactivity Department of the newly founded Kaiser Wilhelm Institute for Chemistry (KWIC). Working with Austrian physicist Lise Meitner in the building that now bears their names, they made a series of groundbreaking discoveries, culminating with her isolation of the longest-lived isotope of protactinium in 1918.\nDuring World War I Hahn served with a \"Landwehr\" regiment on the Western Front, and with the chemical warfare unit headed by Fritz Haber on the Western, Eastern and Italian fronts, earning the Iron Cross (2nd Class) for his part in the First Battle of Ypres. After the war he became the head of the KWIC, while remaining in charge of his own department. Between 1934 and 1938, he worked with Strassmann and Meitner on the study of isotopes created by neutron bombardment of uranium and thorium, which led to the discovery of nuclear fission. He was an opponent of Nazism and the persecution of Jews by the Nazi Party that caused the removal of many of his colleagues, including Meitner, who was forced to flee Germany in 1938. Nonetheless, during World War II, he worked on the German nuclear weapons program, cataloguing the fission products of uranium. At the end of the war he was arrested by the Allied forces and detained in Farm Hall with nine other German scientists, from July 1945 to January 1946.\nHahn served as the last president of the Kaiser Wilhelm Society for the Advancement of Science in 1946 and as the founding president of its successor, the Max Planck Society from 1948 to 1960. In 1959, he co-founded the Federation of German Scientists, a non-governmental organisation committed to the ideal of responsible science. As he worked to rebuild German science, he became one of the most influential and respected citizens of post-war West Germany.\nEarly life and education.\nOtto Hahn was born in Frankfurt am Main on 8 March 1879, the youngest son of Heinrich Hahn, a prosperous glazier and founder of the Glasbau Hahn company, and Charlotte Hahn (n\u00e9e Giese). He had an older half-brother Karl, his mother's son from her previous marriage, and two older brothers, Heiner and Julius. The family lived above his father's workshop. The younger three boys were educated at the \"Klinger Oberrealschule\" in Frankfurt. At the age of 15, Otto began to take a special interest in chemistry, and carried out simple experiments in the laundry room of the family home. His father wanted him to study architecture, as he had built or acquired several residential and business properties, but Otto persuaded him that his ambition was to become an industrial chemist.\nIn 1897, after passing his \"Abitur\", Hahn began to study chemistry at the University of Marburg. His subsidiary subjects were mathematics, physics, mineralogy and philosophy. Hahn joined the Students' Association of Natural Sciences and Medicine, a student fraternity and a forerunner of today's \"Landsmannschaft Nibelungi\" (Coburger Convent der akademischen Landsmannschaften und Turnerschaften). He spent his third and fourth semesters at the University of Munich, studying organic chemistry under Adolf von Baeyer, physical chemistry under Wilhelm Muthmann, and inorganic chemistry under Karl Andreas Hofmann. In 1901, Hahn received his doctorate in Marburg for a dissertation entitled \"On Bromine Derivates of Isoeugenol\", a topic in classical organic chemistry. He completed his one-year military service (instead of the usual two because he had a doctorate) in the 81st Infantry Regiment, but unlike his brothers, did not apply for a commission. He then returned to the University of Marburg, where he worked for two years as assistant to his doctoral supervisor, \"Geheimrat\" professor Theodor Zincke.\nEarly career in London and Canada.\nDiscovery of radiothorium and other \"new elements\".\nHahn's intention was still to work in industry. He received an offer of employment from Eugen Fischer, the director of Kalle &amp; Co. (and the father of organic chemist Hans Fischer), but a condition of employment was that Hahn had to have lived in another country and have a reasonable command of another language. With this in mind, and to improve his knowledge of English, Hahn took up a post at University College London in 1904, working under Sir William Ramsay, who was known for having discovered the noble gases. Here Hahn worked on radiochemistry, at that time a very new field. In early 1905, in the course of his work with salts of radium, Hahn discovered a new substance he called radiothorium (thorium-228), which at that time was believed to be a new radioactive element. In fact, it was an isotope of the known element thorium; the concept of an isotope, along with the term, was coined in 1913 by the British chemist Frederick Soddy.\nRamsay was enthusiastic when yet another new element was found in his institute, and he intended to announce the discovery in a correspondingly suitable way. In accordance with tradition this was done before the committee of the venerable Royal Society. At the session of the Royal Society on 16 March 1905 Ramsay communicated Hahn's discovery of radiothorium. The \"Daily Telegraph\" informed its readers:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Very soon the scientific papers will be agog with a new discovery which has been added to the many brilliant triumphs of Gower Street. Dr. Otto Hahn, who is working at University College, has discovered a new radioactive element, extracted from a mineral from Ceylon, named Thorianite, and possibly, it is conjectured, the substance which renders thorium radioactive. Its activity is at least 250,000 times as great as that of thorium, weight for weight. It gives off a gas (generally called an emanation), identical with the radioactive emanation from thorium. Another theory of deep interest is that it is the possible source of a radioactive element possibly stronger in radioactivity than radium itself, and capable of producing all the curious effects which are known of radium up to the present. \u2013 The discoverer read a paper on the subject to the Royal Society last week, and this should rank, when published, among the most original of recent contributions to scientific literature.\nHahn published his results in the \"Proceedings of the Royal Society\" on 24 May 1905. It was the first of more than 250 scientific publications in the field of radiochemistry. At the end of his time in London, Ramsay asked Hahn about his plans for the future, and Hahn told him about the job offer from Kalle &amp; Co. Ramsay told him radiochemistry had a bright future, and that someone who had discovered a new radioactive element should go to the University of Berlin. Ramsay wrote to Emil Fischer, the head of the chemistry institute there, who replied that Hahn could work in his laboratory, but could not be a \"Privatdozent\" because radiochemistry was not taught there. At this point, Hahn decided that he first needed to know more about the subject, so he wrote to the leading expert on the field, Ernest Rutherford. Rutherford agreed to take Hahn on as an assistant, and Hahn's parents undertook to pay Hahn's expenses.\nFrom September 1905 until mid-1906, Hahn worked with Rutherford's group in the basement of the Macdonald Physics Building at McGill University in Montreal. There was some scepticism about the existence of radiothorium, which Bertram Boltwood memorably described as a compound of thorium X and stupidity. Boltwood was soon convinced that it did exist, although he and Hahn differed on what its half-life was. William Henry Bragg and Richard Kleeman had noted that the alpha particles emitted from radioactive substances always had the same energy, providing a second way of identifying them, so Hahn set about measuring the alpha particle emissions of radiothorium. In the process, he found that a precipitation of thorium A (polonium-216) and thorium B (lead-212) also contained a short-lived \"element\", which he named thorium C (which was later identified as polonium-212). Hahn was unable to separate it, and concluded that it had a very short half-life (it is about 300 ns). He also identified radioactinium (thorium-227) and radium D (later identified as lead-210). Rutherford remarked that: \"Hahn has a special nose for discovering new elements.\"\nChemical Institute in Berlin.\nDiscovery of mesothorium I.\nIn 1906, Hahn returned to Germany, where Fischer placed at his disposal a former woodworking shop (\"Holzwerkstatt\") in the basement of the Chemical Institute to use as a laboratory. Hahn equipped it with electroscopes to measure alpha and beta particles and gamma rays. In Montreal these had been made from discarded coffee tins; Hahn made the ones in Berlin from brass, with aluminium strips insulated with amber. These were charged with hard rubber sticks that he rubbed against the sleeves of his suit. It was not possible to conduct research in the wood shop, but Alfred Stock, the head of the inorganic chemistry department, let Hahn use a space in one of his two private laboratories. Hahn purchased two milligrams of radium from Friedrich Oskar Giesel, the discoverer of emanium (radon), for 100 marks a milligram (), and obtained thorium for free from Otto Kn\u00f6fler, whose Berlin firm was a major producer of thorium products.\nIn the space of a few months Hahn discovered mesothorium I (radium-228), mesothorium II (actinium-228), and\u00a0\u2013 independently from Boltwood\u00a0\u2013 the mother substance of radium, ionium (later identified as thorium-230). In subsequent years, mesothorium I assumed great importance because, like radium-226 (discovered by Pierre and Marie Curie), it was ideally suited for use in medical radiation treatment, but cost only half as much to manufacture. Along the way, Hahn determined that just as he was unable to separate thorium from radiothorium, so he could not separate mesothorium I from radium.\nIn Canada there had been no requirement to be circumspect when addressing the egalitarian New Zealander Rutherford, but many people in Germany found his manner off-putting, and characterised him as an \"Anglicised Berliner\". Hahn completed his habilitation in early 1907, and became a \"Privatdozent\". A thesis was not required; the Chemical Institute accepted one of his publications on radioactivity instead. Most of the organic chemists at the Chemical Institute did not regard Hahn's work as real chemistry. Fischer objected to Hahn's contention in his habilitation colloquium that many radioactive substances existed in such tiny amounts that they could only be detected by their radioactivity, venturing that he had always been able to detect substances with his keen sense of smell, but soon gave in. One department head remarked: \"it is incredible what one gets to be a \"Privatdozent\" these days!\"\nPhysicists were more accepting of Hahn's work, and he began attending a colloquium at the Physics Institute conducted by Heinrich Rubens. It was at one of these colloquia where, on 28 September 1907, he made the acquaintance of the Austrian physicist Lise Meitner. Almost the same age as himself, she was only the second woman to receive a doctorate from the University of Vienna, and had already published two papers on radioactivity. Rubens suggested her as a possible collaborator. So began the thirty-year collaboration and lifelong close friendship between the two scientists.\nIn Montreal, Hahn had worked with physicists including at least one woman, Harriet Brooks, but it was difficult for Meitner at first. Women were not yet admitted to universities in Prussia. Meitner was allowed to work in the wood shop, which had its own external entrance, but could not enter the rest of the institute, including Hahn's laboratory space upstairs. If she wanted to go to the toilet, she had to use one at the restaurant down the street. The following year, women were admitted to universities, and Fischer lifted the restrictions and had women's toilets installed in the building.\nDiscovery of radioactive recoil.\nHarriet Brooks observed a radioactive recoil in 1904, but interpreted it wrongly. Hahn and Meitner succeeded in demonstrating the radioactive recoil incident to alpha particle emission and interpreted it correctly. Hahn pursued a report by Stefan Meyer and Egon Schweidler of a decay product of actinium with a half-life of about 11.8 days. Hahn determined that it was actinium X (radium-223). He also discovered that at the moment when a radioactinium (thorium-227) atom emits an alpha particle, it does so with great force, and the actinium X experiences a recoil. This is enough to free it from chemical bonds, and it has a positive charge, and can be collected at a negative electrode.\nHahn was thinking only of actinium, but on reading his paper, Meitner told him that he had found a new way of detecting radioactive substances. They set up some tests, and soon found actinium C'' (thallium-207) and thorium C'' (thallium-208). The physicist Walther Gerlach described radioactive recoil as \"a profoundly significant discovery in physics with far-reaching consequences\".\nKaiser Wilhelm Institute for Chemistry.\nIn 1910, Hahn was appointed professor by the Prussian Minister of Culture and Education, August von Trott zu Solz. Two years later, Hahn became head of the Radioactivity Department of the newly founded Kaiser Wilhelm Institute for Chemistry (KWIC) in Berlin-Dahlem (in what is today the Hahn-Meitner-Building of the Free University of Berlin). This came with an annual salary of 5,000 marks (). In addition, he received 66,000 marks in 1914 () from Kn\u00f6fler for the mesothorium process, of which he gave 10 per cent to Meitner. The new institute was inaugurated on 23 October 1912 in a ceremony presided over by Kaiser Wilhelm II. The Kaiser was shown glowing radioactive substances in a dark room.\nThe move to new accommodation was fortuitous, as the wood shop had become heavily contaminated by radioactive liquids that had been spilt, and radioactive gases that had vented and then decayed and settled as radioactive dust, making sensitive measurements impossible. To ensure that their clean new laboratories stayed that way, Hahn and Meitner instituted strict procedures. Chemical and physical measurements were conducted in different rooms, people handling radioactive substances had to follow protocols that included not shaking hands, and rolls of toilet paper were hung next to every telephone and door handle. Strongly radioactive substances were stored in the old wood shop, and later in a purpose-built radium house on the institute grounds.\nWorld War I.\nIn July 1914\u2014shortly before the outbreak of World War I\u2014Hahn was recalled to active duty with the army in a \"Landwehr\" regiment. They marched through Belgium, where the platoon he commanded was armed with captured machine guns. He was awarded the Iron Cross (2nd Class) for his part in the First Battle of Ypres. He was a joyful participant in the Christmas truce of 1914, and was commissioned as a lieutenant. In mid-January 1915, he was summoned to meet chemist Fritz Haber, who explained his plan to break the trench deadlock with chlorine gas. Hahn raised the issue that the Hague Convention banned the use of projectiles containing poison gases, but Haber explained that the French had already initiated chemical warfare with tear gas grenades, and he planned to get around the letter of the convention by releasing gas from cylinders instead of shells.\nHaber's new unit was called Pioneer Regiment 35. After brief training in Berlin, Hahn, together with physicists James Franck and Gustav Hertz, was sent to Flanders again to scout for a site for a first gas attack. He did not witness the attack because he and Franck were off selecting a position for the next attack. Transferred to Poland, at the Battle of Bolim\u00f3w on 12 June 1915, they released a mixture of chlorine and phosgene gas. Some German troops were reluctant to advance when the gas started to blow back, so Hahn led them across No Man's land. He witnessed the death agonies of Russians they had poisoned, and unsuccessfully attempted to revive some with gas masks. On their next attempt on 7 July, the gas again blew back on German lines, and Hertz was poisoned. This assignment was interrupted by a mission at the front in Flanders and again in 1916 by a mission to Verdun to introduce shells filled with phosgene to the Western Front. Then once again he was hunting along both fronts for sites for gas attacks. In December 1916 he joined the new gas command unit at Imperial Headquarters.\nBetween operations, Hahn returned to Berlin, where he was able to slip back to his old laboratory and work with Meitner, continuing with their research. In September 1917 he was one of three officers, disguised in Austrian uniforms, sent to the Isonzo front in Italy to find a suitable location for an attack, using newly developed rifled \"minenwerfers\" that simultaneously hurled hundreds of containers of poison gas onto enemy targets. They selected a site where the Italian trenches were sheltered in a deep valley so that a gas cloud would persist. The following Battle of Caporetto broke the Italian lines, and the Central Powers overran much of northern Italy. That summer Hahn was accidentally poisoned by phosgene while testing a new model of gas mask. At the end of the war he was in the field in mufti on a secret mission to test a pot that heated and released a cloud of arsenicals.\nDiscovery of protactinium.\nIn 1913, chemists Frederick Soddy and Kasimir Fajans independently observed that alpha decay caused atoms to move down two places on the periodic table, while the loss of two beta particles restored it to its original position. Under the resulting reorganisation of the periodic table, radium was placed in group II, actinium in group III, thorium in group IV and uranium in group VI. This left a gap between thorium and uranium. Soddy predicted that this unknown element, which he referred to (after Dmitri Mendeleev) as \"ekatantalium\", would be an alpha emitter with chemical properties similar to tantalum. It was not long before Fajans and Oswald Helmuth G\u00f6hring discovered it as a decay product of a beta-emitting product of thorium. Based on the radioactive displacement law of Fajans and Soddy, this was an isotope of the missing element, which they named \"brevium\" after its short half-life. However, it was a beta emitter, and therefore could not be the mother isotope of actinium. This had to be another isotope of the same element.\nHahn and Meitner set out to find the missing mother isotope. They developed a new technique for separating the tantalum group from pitchblende, which they hoped would speed the isolation of the new isotope. The work was interrupted by the First World War. Meitner became an X-ray nurse, working in Austrian Army hospitals, but she returned to the Kaiser Wilhelm Institute in October 1916. Hahn joined the new gas command unit at Imperial Headquarters in Berlin in December 1916 after travelling between the western and eastern front, Berlin and Leverkusen between mid-1914 and late 1916.\nMost of the students, laboratory assistants and technicians had been called up, so Hahn, who was stationed in Berlin between January and September 1917, and Meitner had to do everything themselves. By December 1917 she was able to isolate the substance, and after further work were able to prove that it was indeed the missing isotope. Meitner submitted her and Hahn's findings for publication in March 1918 to the scientific paper \"Physikalischen Zeitschrift\" under the title (\"The Mother Substance of Actinium; A New Radioactive Element with a Long Lifetime\"). Although Fajans and G\u00f6hring had been the first to discover the element, custom required that an element was represented by its longest-lived and most abundant isotope, and while brevium had a half-life of 1.7 minutes, Hahn and Meitner's isotope had one of 32,500 years. The name brevium no longer seemed appropriate. Fajans agreed to Meitner and Hahn naming the element \"protoactinium\".\nIn June 1918, Soddy and John Cranston announced that they had extracted a sample of the isotope, but unlike Hahn and Meitner were unable to describe its characteristics. They acknowledged Hahn\u00b4s and Meitner's priority, and agreed to the name. The connection to uranium remained a mystery, as neither of the known isotopes of uranium decayed into protactinium. It remained unsolved until the mother isotope, uranium-235, was discovered in 1929. For their discovery Hahn and Meitner were repeatedly nominated for the Nobel Prize in Chemistry in the 1920s by several scientists, among them Max Planck, Heinrich Goldschmidt, and Fajans himself. In 1949, the International Union of Pure and Applied Chemistry (IUPAC) named the new element definitively protactinium, and confirmed Hahn and Meitner as discoverers.\nDiscovery of nuclear isomerism.\nWith the discovery of protactinium, most of the decay chains of uranium had been mapped. When Hahn returned to his work after the war, he looked back over his 1914 results, and considered some anomalies that had been dismissed or overlooked. He dissolved uranium salts in a hydrofluoric acid solution with tantalic acid. First the tantalum in the ore was precipitated, then the protactinium. In addition to the uranium X1 (thorium-234) and uranium X2 (protactinium-234), Hahn detected traces of a radioactive substance with a half-life of between 6 and 7 hours. There was one isotope known to have a half-life of 6.2 hours, mesothorium II (actinium-228). This was not in any probable decay chain, but it could have been contamination, as the KWIC had experimented with it. Hahn and Meitner demonstrated in 1919 that when actinium is treated with hydrofluoric acid, it remains in the insoluble residue. Since mesothorium II was an isotope of actinium, the substance was not mesothorium II; it was protactinium. Hahn was now confident enough he had found something that he named his new isotope \"uranium Z\". In February 1921, he published the first report on his discovery.\nHahn determined that uranium Z had a half-life of around 6.7 hours (with a two per cent margin of error) and that when uranium X1 decayed, it became uranium X2 about 99.75 per cent of the time, and uranium Z around 0.25 per cent of the time. He found that the proportion of uranium X to uranium Z extracted from several kilograms of uranyl nitrate remained constant over time, strongly indicating that uranium X was the mother of uranium Z. To prove this, Hahn obtained a hundred kilograms of uranyl nitrate; separating the uranium X from it took weeks. He found that the half-life of the parent of uranium Z differed from the known 24-day half-life of uranium X1 by no more than two or three days, but was unable to get a more accurate value. Hahn concluded that uranium Z and uranium X2 were both the same isotope of protactinium (protactinium-234), and they both decayed into uranium II (uranium-234), but with different half-lives.\nUranium Z was the first example of nuclear isomerism. Walther Gerlach later remarked that this was \"a discovery that was not understood at the time but later became highly significant for nuclear physics\". Not until 1936 was Carl Friedrich von Weizs\u00e4cker able to provide a theoretical explanation of the phenomenon. For this discovery, whose full significance was recognised by very few, Hahn was again proposed for the Nobel Prize in Chemistry by Bernhard Naunyn, Goldschmidt and Planck.\n\"Applied Radiochemistry\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nAs a young graduate student at the University of California at Berkeley in the mid-1930s and in connection with our work with plutonium a few years later, I used his book \"Applied Radiochemistry\" as my bible. This book was based on a series of lectures which Professor Hahn had given at Cornell in 1933; it set forth the \"laws\" for the co-precipitation of minute quantities of radioactive materials when insoluble substances were precipitated from aqueous solutions. I recall reading and rereading every word in these laws of co-precipitation many times, attempting to derive every possible bit of guidance for our work, and perhaps in my zealousness reading into them more than the master himself had intended. I doubt that I have read sections in any other book more carefully or more frequently than those in Hahn's \"Applied Radiochemistry\". In fact, I read the entire volume repeatedly and I recall that my chief disappointment with it was its length. It was too short.\nGlenn Seaborg\nIn 1924, Hahn was elected to full membership of the Prussian Academy of Sciences in Berlin, by a vote of thirty white balls to two black. While still remaining the head of his own department, he became Deputy Director of the KWIC in 1924, and succeeded Alfred Stock as the director in 1928. Meitner became the director of the Physical Radioactivity Division, while Hahn headed the Chemical Radioactivity Division.\nIn the early 1920s, Hahn created a new line of research. Using the \"emanation method\", which he had recently developed, and the \"emanation ability\", he founded what became known as \"applied radiochemistry\" for the researching of general chemical and physical-chemical questions. In 1936 Cornell University Press published a book in English (and later in Russian) titled \"Applied Radiochemistry\", which contained the lectures given by Hahn when he was a visiting professor at Cornell University in Ithaca, New York, in 1933. This publication had a major influence on almost all nuclear chemists and physicists in the United States, the United Kingdom, France, and the Soviet Union during the 1930s and 1940s. Hahn is referred to as the father of nuclear chemistry, which emerged from applied radiochemistry.\nNazi Germany.\nImpact of Nazism.\nFritz Strassmann had come to the KWIC to study under Hahn to improve his employment prospects. After the Nazi Party (NSDAP) came to power in Germany in 1933, Strassmann declined a lucrative offer of employment because it required political training and Nazi Party membership. Later, rather than become a member of a Nazi-controlled organisation, Strassmann resigned from the Society of German Chemists when it became part of the Nazi German Labour Front. As a result, he could neither work in the chemical industry nor receive his habilitation, the prerequisite for an academic position. Meitner persuaded Hahn to hire Strassmann as an assistant. Soon he would be credited as a third collaborator on the papers they produced, and would sometimes even be listed first.\nHahn spent February to June 1933 in the United States and Canada as a visiting professor at Cornell University. He gave an interview to the Toronto \"Star Weekly\" in which he painted a flattering portrait of Adolf Hitler: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I am not a Nazi. But Hitler is the hope, the powerful hope, of German youth... At least 20 million people revere him. He began as a nobody, and you see what he has become in ten years... In any case for the youth, for the nation of the future, Hitler is a hero, a F\u00fchrer, a saint... In his daily life he is almost a saint. No alcohol, not even tobacco, no meat, no women. In a word: Hitler is an unequivocal Christ.\nThe April 1933 Law for the Restoration of the Professional Civil Service banned Jews and communists from academia. Meitner was exempt from its impact because she was an Austrian rather than a German citizen. Haber was likewise exempt as a veteran of World War I, but chose to resign his directorship of the Kaiser Wilhelm Institute of Physical Chemistry and Electrochemistry in protest on 30 April 1933. The directors of the other Kaiser Wilhelm Institutes, even the Jewish ones, complied with the new law, which applied to the KWS as a whole and those Kaiser Wilhelm institutes with more than 50% state support, which exempted the KWI for Chemistry. Hahn therefore did not have to fire any of his own full-time staff, but as the interim director of Haber's institute, he dismissed a quarter of its staff, including three department heads. Gerhart Jander was appointed the new director of Haber's old institute, and reoriented it towards chemical warfare research.\nLike most KWS institute directors, Haber had accrued a large discretionary fund. It was his wish that it be distributed to the dismissed staff to facilitate their emigration. Hahn brokered a deal whereby 10 per cent of the funds would be allocated to Haber's people and the rest to KWS, but the Rockefeller Foundation insisted that the funds be used for their original scientific research or else be returned. In August 1933 the administrators of the KWS were alerted that several boxes of Rockefeller Foundation-funded equipment were about to be shipped to Herbert Freundlich, one of the department heads that Hahn had dismissed, who was now working in England. Ernst Telschow, a Nazi Party member, was in charge while Planck, the president of the KWS since 1930, was on vacation, and he ordered the shipment halted. Hahn complied, but he disagreed with the decision on the grounds that funds from abroad should not be diverted to military research, which the KWS was increasingly undertaking. When Planck returned from vacation, he ordered Hahn to expedite the shipment.\nHaber died on 29 January 1934. A memorial service was held on the first anniversary of his death. University professors were forbidden to attend, so they sent their wives in their place. Hahn, Planck and Joseph Koeth attended, and gave speeches. The ageing Planck did not seek re-election, and was succeeded in 1937 as president by Carl Bosch, a winner of the Nobel Prize in Chemistry and the chairman of the board of IG Farben, a company which had bankrolled the Nazi Party since 1932. Telschow became Secretary of the KWS. He was an enthusiastic supporter of the Nazis, but was also loyal to Hahn, being one of his former students, and Hahn welcomed his appointment. Hahn's chief assistant, Otto Erbacher, became the KWI for Chemistry's party steward (\"Vertrauensmann\").\nRubidium\u2013strontium dating.\nWhile Hahn was in North America in 1905\u20131906, his attention had been drawn to a mica-like mineral from Manitoba that contained rubidium. He had studied the radioactive decay of rubidium-87, and had estimated its half-life at 2 x 1011 years. It occurred to him that by comparing the quantity of strontium in the mineral (which had once been rubidium) with that of the remaining rubidium, he could measure the age of the mineral, assuming that his original calculation of the half-life was reasonably accurate. This would be a superior dating method to studying the decay of uranium, because some of the uranium turns into helium, which then escapes, resulting in rocks appearing to be younger than they really were. Jacob Papish helped Hahn obtain several kilograms of the mineral.\nIn 1937, Strassmann and Ernst Walling extracted 253.4 milligrams of strontium carbonate from 1,012 grams of the mineral, all of which was the strontium-87 isotope, indicating that it had all been produced from radioactive decay of rubidium-87. The age of the mineral had been estimated at 1,975 million years from uranium minerals in the same deposit, which implied that the half-life of rubidium-87 was 2.3 x 1011 years: quite close to Hahn's original calculation. Rubidium\u2013strontium dating became a widely used technique for dating rocks in the 1950s, when mass spectrometry became common.\nDiscovery of nuclear fission.\nAfter James Chadwick discovered the neutron in 1932, Ir\u00e8ne Curie and Fr\u00e9d\u00e9ric Joliot irradiated aluminium foil with alpha particles. They found that this results in a short-lived radioactive isotope of phosphorus. They noted that positron emission continued after the neutron emissions ceased. Not only had they discovered a new form of radioactive decay, they had transmuted an element into a hitherto unknown radioactive isotope of another, thereby inducing radioactivity where there had been none before. Radiochemistry was now no longer confined to certain heavy elements, but extended to the entire periodic table. Chadwick noted that being electrically neutral, neutrons could penetrate the atomic nucleus more easily than protons or alpha particles. Enrico Fermi and his colleagues in Rome picked up on this idea, and began irradiating elements with neutrons.\nThe radioactive displacement law of Fajans and Soddy said that beta decay causes isotopes to move one element up on the periodic table, and alpha decay causes them to move two down. When Fermi's group bombarded uranium atoms with neutrons, they found a complex mix of half-lives. Fermi therefore concluded that the new elements with atomic numbers greater than 92 (known as transuranium elements) had been created. Meitner and Hahn had not collaborated for many years, but Meitner was eager to investigate Fermi's results. Hahn, initially, was not, but he changed his mind when Aristid von Grosse suggested that what Fermi had found was an isotope of protactinium. They set out to determine whether or not the 13-minute isotope was indeed an isotope of protactinium.\nBetween 1934 and 1938, Hahn, Meitner and Strassmann found a great number of radioactive transmutation products, all of which they regarded as transuranic. At that time, the existence of actinides was not yet established, and uranium was wrongly believed to be a group 6 element similar to tungsten. It followed that the first transuranic elements would be similar to group 7 to 10 elements, i.e. rhenium and platinoids. They established the presence of multiple isotopes of at least four such elements, and (mistakenly) identified them as elements with atomic numbers 93 through 96. They were the first scientists to measure the 23-minute half-life of uranium-239 and to establish chemically that it was an isotope of uranium, but were unable to continue this work to its logical conclusion and identify the real element 93. They identified ten different half-lives, with varying degrees of certainty. To account for them, Meitner had to hypothesise a new class of reaction and the alpha decay of uranium, neither of which had ever been reported before, and for which physical evidence was lacking. Hahn and Strassmann refined their chemical procedures, while Meitner devised new experiments to shine more light on the reaction processes.\nIn May 1937, they issued parallel reports, one in the \"Zeitschrift f\u00fcr Physik\" with Meitner as the principal author, and one in the \"Chemische Berichte\" with Hahn as the principal author. Hahn concluded his by stating emphatically: (\"Above all, their chemical distinction from all previously known elements needs no further discussion\"). Meitner, however, was increasingly uncertain. She considered the possibility that the reactions were from different isotopes of uranium; three were known: uranium-238, uranium-235 and uranium-234. However, when she calculated the neutron cross section, it was too large to be anything other than the most abundant isotope, uranium-238. She concluded that it must be another case of the nuclear isomerism that Hahn had discovered in protactinium. She therefore ended her report on a very different note to Hahn, reporting that: (\"The processes must be neutron capture by uranium-238, which leads to three isomeric nuclei of uranium-239. This result is very difficult to reconcile with current concepts of the nucleus.\")\nWith the \"Anschluss\", Germany's annexation of Austria on 12 March 1938, Meitner lost her Austrian citizenship, and fled to Sweden. She carried only a little money, but before she left, Hahn gave her a diamond ring he had inherited from his mother. Meitner continued to correspond with Hahn by mail. In late 1938 Hahn and Strassmann found evidence of isotopes of an alkaline earth metal in their sample. Finding a group 2 metal was problematic, because it did not logically fit with the other elements found thus far. Hahn initially suspected it to be radium, produced by splitting off two alpha-particles from the uranium nucleus, but chipping off two alpha particles via this process was unlikely. The idea of turning uranium into barium (by removing around 100 nucleons) was seen as preposterous.\nDuring a visit to Copenhagen on 10 November, Hahn discussed these results with Niels Bohr, Meitner, and Otto Robert Frisch. Further refinements of the technique, leading to the decisive experiment on 16\u201317 December 1938, produced puzzling results: the three isotopes consistently behaved not as radium, but as barium. Hahn, who did not inform the physicists in his Institute, described the results exclusively in a letter to Meitner on 19 December:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We are more and more coming to the awful conclusion that our Ra isotopes behave not like Ra, but like Ba... Perhaps you can come up with some fantastic explanation. We ourselves realize that it \"can't\" actually burst apart into Ba. Now we want to test whether the Ac-isotopes derived from the \"Ra\" behave not like Ac but like La.\nIn her reply, Meitner concurred. \"At the moment, the interpretation of such a thoroughgoing breakup seems very difficult to me, but in nuclear physics we have experienced so many surprises, that one cannot unconditionally say: 'it is impossible'.\" On 22 December 1938, Hahn sent a manuscript to \"Naturwissenschaften\" reporting their radiochemical results, which were published on 6 January 1939. On 27 December, Hahn telephoned the editor of the \"Naturwissenschaften\" and requested an addition to the article, speculating that some platinum group elements previously observed in irradiated uranium, which were originally interpreted as transuranium elements, could in fact be technetium (then called \"masurium\"), mistakenly believing that the atomic masses had to add up rather than the atomic numbers. By January 1939, he was sufficiently convinced of the formation of light elements that he published a new revision of the article, retracting former claims of observing transuranic elements and neighbours of uranium.\nAs a chemist, Hahn was reluctant to propose a revolutionary discovery in physics, but Meitner and Frisch worked out a theoretical interpretation of nuclear fission, a term appropriated by Frisch from biology. In January and February they published two articles discussing and experimentally confirming their theory. In their second publication on nuclear fission, Hahn and Strassmann used the term \"Uranspaltung\" (uranium fission) for the first time, and predicted the existence and liberation of additional neutrons during the fission process, opening up the possibility of a nuclear chain reaction. This was shown to be the case by Fr\u00e9d\u00e9ric Joliot and his team in March 1939. Edwin McMillan and Philip Abelson used the cyclotron at the Berkeley Radiation Laboratory to bombard uranium with neutrons, and were able to identify an isotope with a 23-minute half-life that was the daughter of uranium-239, and therefore the real element 93, which they named neptunium. \"There goes a Nobel Prize\", Hahn remarked.\nAt the KWIC, Kurt Starke independently produced element 93, using only the weak neutron sources available there. Hahn and Strassmann then began researching its chemical properties. They knew that it should decay into the real element 94, which according to the latest version of the liquid drop model of the nucleus propounded by Bohr and John Archibald Wheeler, would be even more fissile than uranium-235, but were unable to detect its radioactive decay. They concluded that it must have an extremely long half-life, perhaps millions of years. Part of the problem was that they still believed that element 94 was a platinoid, which confounded their attempts at chemical separation.\nWorld War II.\nOn 24 April 1939, Paul Harteck and his assistant, Wilhelm Groth, had written to the Armed Forces High Command (OKW), alerting it to the possibility of the development of an atomic bomb. In response, the Army Weapons Branch (HWA) had established a physics section under the nuclear physicist Kurt Diebner. After World War II broke out on 1 September 1939, the HWA moved to control the German nuclear weapons program. From then on, Hahn participated in a ceaseless series of meetings related to the project. After the Director of the Kaiser Wilhelm Institute for Physics, Peter Debye, left for the United States in 1940 and never returned, Diebner was installed as its director. Hahn reported to the HWA on the progress of his research. Together with his assistants, Hans-Joachim Born, Siegfried Fl\u00fcgge, Hans G\u00f6tte, Walter Seelmann-Eggebert and Strassmann, he catalogued about one hundred fission product isotopes. They also investigated means of isotope separation; the chemistry of element 93; and methods for purifying uranium oxides and salts.\nOn the night of 15 February 1944, the KWIC building was struck by a bomb. Hahn's office was destroyed, along with his correspondence with Rutherford and other researchers, and many of his personal possessions. The office was the intended target of the raid, which had been ordered by Brigadier General Leslie Groves, the director of the Manhattan Project, in the hope of disrupting the German uranium project. Albert Speer, the Reich Minister of Armaments and War Production, arranged for the institute to move to Tailfingen (today part of Albstadt) in southern Germany. All work in Berlin ceased by July. Hahn and his family moved to the house of a textile manufacturer there.\nLife became precarious for those married to Jewish women. One was Philipp Hoernes, a chemist working for Auergesellschaft, the firm that mined the uranium ore used by the project. After the firm let him go in 1944, Hoernes faced being conscripted for forced labour. At the age of 60, it was doubtful that he would survive. Hahn and Nikolaus Riehl arranged for Hoernes to work at the KWIC, claiming that his work was essential to the uranium project and that uranium was highly toxic, making it hard to find people to work with it. Hahn was aware that uranium ore was fairly safe in the laboratory, although not so much for the 2,000 female slave labourers from the Sachsenhausen concentration camp who mined it in Oranienburg. Another physicist with a Jewish wife was Heinrich Rausch von Traubenberg. Hahn certified that his work was important to the war effort, and that his wife Maria, who had a doctorate in physics, was required as his assistant. After he died on 19 September 1944, Maria faced being sent to a concentration camp. Hahn mounted a lobbying campaign to get her released, but to no avail, and she was sent to the Theresienstadt Ghetto in January 1945. She survived the war, and was reunited with her daughters in England.\nPost-war.\nIncarceration in Farm Hall.\nOn 25 April 1945, an armoured task force from the British\u2212American Alsos Mission arrived in Tailfingen, and surrounded the KWIC. Hahn was informed that he was under arrest. When asked about reports related to his secret work on uranium, Hahn replied \"I have them all here\" and handed over 150 reports. He was taken to Hechingen, where he joined Erich Bagge, Horst Korsching, Max von Laue, Carl Friedrich von Weizs\u00e4cker and Karl Wirtz. They were then taken to a dilapidated ch\u00e2teau in Versailles, where they heard about the signing of the German Instrument of Surrender at Reims on 7 May. Over the following days they were joined by Kurt Diebner, Walther Gerlach, Paul Harteck and Werner Heisenberg. All were physicists except Hahn and Harteck, who were chemists, and all had worked on the German nuclear weapons program except von Laue, although he was well aware of it.\nThey were moved to the Ch\u00e2teau de Facqueval in Modave, Belgium, where Hahn used the time to work on his memoirs and then, on 3 July, were flown to England. They arrived at Farm Hall, Godmanchester, near Cambridge, on 3 July. While they were there, all their conversations, indoors and out, were covertly recorded with hidden microphones. They were given British newspapers, which Hahn was able to read. He was greatly disturbed by their reports of the Potsdam Conference, where German territory was ceded to Poland and the USSR. In August 1945, the German scientists were informed of the atomic bombing of Hiroshima. Up to this point the scientists, except Harteck, were completely certain that their project was further advanced than any in other countries, and the Alsos Mission's chief scientist, Samuel Goudsmit, did nothing to correct this impression. Now the reason for their incarceration in Farm Hall suddenly became apparent.\nAs they recovered from the shock of the announcement, they began to rationalise what had happened. Hahn noted that he was glad that they had not succeeded, and von Weizs\u00e4cker suggested that they should claim that they had not wanted to. They drafted a memorandum on the project, noting that fission was discovered by Hahn and Strassmann. The revelation that Nagasaki had been destroyed by a plutonium bomb came as another shock, as it meant that the Allies had not only been able to conduct uranium enrichment, but had mastered nuclear reactor technology as well. The memorandum became the first draft of a postwar apologia. The idea that Germany had lost the war because its scientists were morally superior was as outrageous as it was unbelievable, but struck a chord in postwar German academe. It infuriated Goudsmit, whose parents had been murdered in Auschwitz. On 3 January 1946, six months after they had arrived at Farm Hall, the group was allowed to return to Germany. Hahn, Heisenberg, von Laue and von Weizs\u00e4cker were brought to G\u00f6ttingen, which was controlled by the British occupation authorities.\nThe Nobel Prize in Chemistry 1944.\nOn 16 November 1945 the Royal Swedish Academy of Sciences announced that Hahn had been awarded the 1944 Nobel Prize in Chemistry \"for his discovery of the fission of heavy atomic nuclei.\" Hahn was still at Farm Hall when the announcement was made; thus, his whereabouts were a secret, and it was impossible for the Nobel committee to send him a congratulatory telegram. Instead, he learned about his award on 18 November through the \"Daily Telegraph\". His fellow interned scientists celebrated his award by giving speeches, making jokes, and composing songs.\nHahn had been nominated for the chemistry and the physics Nobel prizes many times even before the discovery of nuclear fission. Several more followed for the discovery of fission. The Nobel prize nominations were vetted by committees of five, one for each award. Although Hahn and Meitner received nominations for physics, radioactivity and radioactive elements had traditionally been seen as the domain of chemistry, and so the Nobel Committee for Chemistry evaluated the nominations. The committee received reports from Theodor Svedberg and Arne Westgren. These chemists were impressed by Hahn's work, but felt that of Meitner and Frisch was not extraordinary, and did not understand why the physics community regarded their work as seminal. As for Strassmann, although his name was on the papers, there was a long-standing policy of conferring awards on the most senior scientist in a collaboration. The committee therefore recommended that Hahn alone be given the chemistry prize.\nUnder Nazi rule, Germans had been forbidden to accept Nobel prizes after the Nobel Peace Prize had been awarded to Carl von Ossietzky in 1936. The Nobel Committee for Chemistry's recommendation was therefore rejected by the Royal Swedish Academy of Sciences in 1944, which also decided to defer the award for one year. When the Academy reconsidered the award in September 1945, the war was over and thus the German boycott had ended. Also, the chemistry committee had now become more cautious, as it was apparent that much research had taken place in the United States in secret, and suggested deferring for another year, but the Academy was swayed by G\u00f6ran Liljestrand, who argued that it was important for the Academy to assert its independence from the Allies of World War II, and award the prize to a German, as it had done after World War I when it had awarded it to Fritz Haber. Hahn therefore became the sole recipient of the 1944 Nobel Prize for Chemistry.\nThe invitation to attend the Nobel festivities was transmitted via the British Embassy in Stockholm. On 4 December, Hahn was persuaded by two of his Alsos captors, American Lieutenant Colonel Horace K. Calvert and British Lieutenant Commander Eric Welsh, to write a letter to the Nobel committee accepting the prize but stating that he would not be able to attend the award ceremony on 10 December since his captors would not allow him to leave Farm Hall. When Hahn protested, Welsh reminded him that Germany had lost the war. Under the Nobel Foundation statutes, Hahn had six months to deliver the Nobel Prize lecture, and until 1 October 1946 to cash the 150,000 Swedish krona cheque.\nHahn was repatriated from Farm Hall on 3 January 1946, but it soon became apparent that difficulties obtaining permission to travel from the British government meant that he would be unable to travel to Sweden before December 1946. Accordingly, the Academy of Sciences and the Nobel Foundation obtained an extension from the Swedish government. Hahn attended the year after he was awarded the prize. On 10 December 1946, the anniversary of the death of Alfred Nobel, King Gustav V of Sweden presented him with his Nobel Prize medal and diploma. Hahn gave 10,000 krona of his prize to Strassmann, who refused to use it.\nFounder and President of the Max Planck Society.\nThe suicide of Albert V\u00f6gler on 14 April 1945 left the KWS without a president. The British chemist Bertie Blount was placed in charge of its affairs while the Allies decided what to do with it, and he decided to install Max Planck as an interim president. Now aged 87, Planck was in the small town of Rog\u00e4tz, in an area that the Americans were preparing to hand over to the Soviet Union. The Dutch astronomer Gerard Kuiper from the Alsos Mission fetched Planck in a Jeep and brought him to G\u00f6ttingen on 16 May. Planck wrote to Hahn, who was still in captivity in England, on 25 July, and informed Hahn that the directors of the KWS had voted to make him the next president, and asked if he would accept the position. Hahn did not receive the letter until September, and did not think he was a good choice, as he regarded himself as a poor negotiator, but his colleagues persuaded him to accept. After his return to Germany, he assumed the office on 1 April 1946.\nAllied Control Council Law No. 25 on the control of scientific research dated 29 April 1946 restricted German scientists to conducting basic research only, and on 11 July the Allied Control Council dissolved the KWS on the insistence of the Americans, who considered that it had been too close to the national socialist regime, and was a threat to world peace. However, the British, who had voted against the dissolution, were more sympathetic, and offered to let the Kaiser Wilhelm Society continue in the British Zone, on one condition: that the name be changed. Hahn and Heisenberg were distraught at this prospect. To them it was an international brand that represented political independence and scientific research of the highest order. Hahn noted that it had been suggested that the name be changed during the Weimar Republic, but the Social Democratic Party of Germany had been persuaded not to. To Hahn, the name represented the good old days of the German Empire, however authoritarian and undemocratic it was, before the hated Weimar Republic. Heisenberg asked Niels Bohr for support, but Bohr recommended that the name be changed. Lise Meitner wrote to Hahn, explaining that:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Outside of Germany it is considered so obvious that the tradition from the period of Kaiser Wilhelm has been disastrous and that changing the name of the KWS is desirable, that no one understands the resistance against it. For the idea, that the Germans are the chosen people and have the right to use any and all means to subordinate the \"inferior\" people, has been expressed over and over again by historians, philosophers, and politicians and finally the Nazis tried to translate it into fact... The best people among the English and Americans wish that the best Germans would understand that there should be a definitive break with this tradition, which has brought the entire world and Germany itself the greatest misfortune. And as a small sign of German understanding the name of the KWS should be changed. What's in a name, if it is a matter of the existence of Germany and thereby Europe?\nIn September 1946, a new Max Planck Society was established at Bad Driburg in the British Zone. On 26 February 1948, after the US and British zones were fused into Bizonia, it was dissolved to make way for the Max Planck Society, with Hahn as the founding president. It took over the 29 institutes of the former Kaiser Wilhelm Society that were located in the British and American zones. When the Federal Republic of Germany (or West-Germany) was formed in 1949, the five institutes located in the French zone joined them. The KWIC, now under Strassmann, built and renovated new accommodation in Mainz, but work proceeded slowly, and it did not relocate from Tailfingen until 1949. Hahn's insistence on retaining Telschow as the general secretary nearly caused a rebellion against his presidency. In his efforts to rebuild German science, Hahn was generous in issuing \"persilschein\" (whitewash certificates), writing one for Gottfried von Droste, who had joined the \"Sturmabteilung\" (SA) in 1933 and the NSDAP in 1937, and wore his SA uniform at the KWIC, and for Heinrich H\u00f6rlein and Fritz ter Meer from IG Farben. Hahn served as president of the Max Planck Society until 1960, and succeeded in regaining the renown that had once been enjoyed by the Kaiser Wilhelm Society. New institutes were founded and old ones expanded, the budget rose from 12 million Deutsche Marks in 1949 (equivalent to \u20ac million in 2021) to 47 million in 1960 (equivalent to \u20ac million in 2021), and the workforce grew from 1,400 to nearly 3,000.\nSpokesman for social responsibility.\nAfter the Second World War, Hahn came out strongly against the use of nuclear energy for military purposes. He saw the application of his scientific discoveries to such ends as a misuse, or even a crime. The historian Lawrence Badash wrote: \"His wartime recognition of the perversion of science for the construction of weapons, and his postwar activity in planning the direction of his country's scientific endeavours now inclined him increasingly toward being a spokesman for social responsibility.\"\nIn early 1954, he wrote the article \"Cobalt 60\u00a0\u2013 Danger or Blessing for Mankind?\", about the misuse of atomic energy, which was widely reprinted and transmitted in the radio in Germany, Norway, Austria, and Denmark, and in an English version worldwide via the BBC. The international reaction was encouraging. The following year he initiated and organised the Mainau Declaration of 1955, in which he and other international Nobel Prize-winners called attention to the dangers of atomic weapons and urgently warned the nations of the world against the use of \"force as a final resort\", and which was issued a week after the similar Russell-Einstein Manifesto. In 1956, Hahn repeated his appeal with the signature of 52 of his Nobel colleagues from all parts of the world.\nHahn was also instrumental in and one of the authors of the G\u00f6ttingen Manifesto of 13 April 1957, in which, together with 17 leading German atomic scientists, he protested against a proposed nuclear arming of the West German armed forces (\"Bundeswehr\"). This resulted in Hahn receiving an invitation to meet the Chancellor of Germany, Konrad Adenauer and other senior officials, including the Defense Minister, Franz Josef Strauss, and Generals Hans Speidel and Adolf Heusinger (who had both been generals in the Nazi era). The two generals argued that the \"Bundeswehr\" needed nuclear weapons, and Adenauer accepted their advice. A communiqu\u00e9 was drafted that said that the Federal Republic did not manufacture nuclear weapons, and would not ask its scientists to do so. Instead, the German forces were equipped with US nuclear weapons.\nOn 13 November 1957, in the \"Konzerthaus\" (Concert Hall) in Vienna, Hahn warned of the \"dangers of A- and H-bomb-experiments\", and declared that \"today war is no means of politics anymore\u00a0\u2013 it will only destroy all countries in the world\". His highly acclaimed speech was transmitted internationally by the Austrian radio, \u00d6sterreichischer Rundfunk (\u00d6R). On 28 December 1957, Hahn repeated his appeal in an English translation for the Bulgarian Radio in Sofia, which was broadcast in all Warsaw pact states.\nIn 1959 Hahn co-founded in Berlin the Federation of German Scientists (VDW), a non-governmental organisation, which has been committed to the ideal of responsible science. The members of the Federation feel committed to taking into consideration the possible military, political, and economic implications and possibilities of atomic misuse when carrying out their scientific research and teaching. With the results of its interdisciplinary work the VDW not only addresses the general public, but also the decision-makers at all levels of politics and society. Right up to his death, Otto Hahn never tired of warning of the dangers of the nuclear arms race between the great powers and of the radioactive contamination of the planet.\nLawrence Badash wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The important thing is not that scientists may disagree on where their responsibility to society lies, but that they are conscious that a responsibility exists, are vocal about it, and when they speak out they expect to affect policy. Otto Hahn, it would seem, was even more than just an example of this twentieth-century conceptual evolution; he was a leader in the process.\nHe was one of the signatories of the agreement to convene a convention for drafting a world constitution. As a result, for the first time in human history, a World Constituent Assembly convened to draft and adopt a Constitution for the Federation of Earth.\nPrivate life.\nIn June 1911, while attending a conference in Stettin, Hahn met Edith Junghans (1887\u20131968), a student at the Royal School of Art in Berlin. They saw each other again in Berlin, and became engaged in November 1912. On 22 March 1913 the couple were married in Stettin, where Edith's father, Paul Ferdinand Junghans, was a high-ranking law officer and President of the City Parliament until his death in 1915. After a honeymoon at Punta San Vigilio on Lake Garda in Italy, they visited Vienna, and then Budapest, where they stayed with George de Hevesy.\nThey had one child, Hanno Hahn, who was born on 9 April 1922. Hanno enlisted in the army in 1942, and served on the Eastern Front in World War II as a panzer commander. He lost an arm in combat. After the war he became an art historian and architectural researcher (at the Hertziana in Rome), known for his discoveries in the early Cistercian architecture of the 12th century. In August 1960, while on a study trip in France, Hanno died in a car accident, together with his wife and assistant Ilse Hahn n\u00e9e Pletz. They left a fourteen-year-old son, Dietrich Hahn.\nIn 1990, the Hanno and Ilse Hahn Prize for outstanding contributions to Italian art history was established in memory of Hanno and Ilse Hahn to support young and talented art historians. It is awarded biennially by the Bibliotheca Hertziana \u2013 Max Planck Institute for Art History in Rome.\nDeath and legacy.\nDeath.\nHahn was shot in the back in October 1951 by a disgruntled inventor who wished to highlight the neglect of his ideas by mainstream scientists. Hahn was injured in a motor vehicle accident in 1952, and had a minor heart attack the following year. In 1962, he published a book, (lit.\u2009'\"From Radiothorium to Uranium Fission\"'). It was released in English in 1966 with the title \"Otto Hahn: A Scientific Autobiography\", with an introduction by Glenn Seaborg. The success of this book may have prompted him to write another, fuller autobiography, \"Otto Hahn. Mein Leben\", but before it could be published, he fractured one of the vertebrae in his neck while getting out of a car. He gradually became weaker and died in G\u00f6ttingen on 28 July 1968. His wife Edith survived him by only a fortnight. He was buried in the Stadtfriedhof in G\u00f6ttingen.\nThe day after his death, the Max Planck Society published the following obituary notice:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;On 28 July, in his 90th year, our Honorary President Otto Hahn passed away. His name will be recorded in the history of humanity as the founder of the atomic age. In him Germany and the world have lost a scholar who was distinguished in equal measure by his integrity and personal humility. The Max Planck Society mourns its founder, who continued the tasks and traditions of the Kaiser Wilhelm Society after the war, and mourns also a good and much loved human being, who will live in the memories of all who had the chance to meet him. His work will continue. We remember him with deep gratitude and admiration. \nFritz Strassmann wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The number of those who had been able to be near Otto Hahn is small. His behaviour was completely natural for him, but for the next generations he will serve as a model, regardless of whether one admires in the attitude of Otto Hahn his humane and scientific sense of responsibility or his personal courage.\nOtto Robert Frisch recalled:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Hahn remained modest and informal all his life. His disarming frankness, unfailing kindness, good common sense, and impish humour will be remembered by his many friends all over the world.\nThe Royal Society in London wrote in an obituary:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It was remarkable, how, after the war, this rather unassuming scientist who had spent a lifetime in the laboratory, became an effective administrator and an important public figure in Germany. Hahn, famous as the discoverer of nuclear fission, was respected and trusted for his human qualities, simplicity of manner, transparent honesty, common sense and loyalty.\nLegacy.\nHahn is considered the father of radiochemistry and nuclear chemistry. He is chiefly remembered for the discovery of nuclear fission, the basis of nuclear power and nuclear weapons. Glenn Seaborg wrote that \"it has been given to very few men to make contributions to science and to humanity of the magnitude of those made by Otto Hahn\". His award of the 1944 Nobel Prize for Chemistry was in recognition for this discovery. However later commentators have argued that Lise Meitner's exclusion reflected sexism and antisemitism within the Nobel Committee. Conflict between chemists and physicists and the theorists and experimentalists also played a role. Hahn's efforts to rehabilitate the image of Germany after the war have also been viewed as problematic. Hahn has been described as politically passive during the Nazi era, suggesting that while he was not a party member, he tolerated colleagues who were and thus shared moral complicity. In a letter to James Franck dated 22 February 1946, Meitner wrote:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nHonours and awards.\nDuring his lifetime Hahn was awarded orders, medals, scientific prizes, and fellowships of Academies, Societies, and Institutions from all over the world. At the end of 1999, the German news magazine \"Focus\" published an inquiry of 500 leading natural scientists, engineers, and physicians about the most important scientists of the 20th century. In this poll Hahn was elected third (with 81 points), after the theoretical physicists Albert Einstein and Max Planck, and thus the most significant chemist of his time.\nAs well as the Nobel Prize in Chemistry (1944), Hahn was awarded:\nHahn became the honorary president of the Max Planck Society in 1962.\nHe was an honorary fellow of University College London,\nObjects named after Hahn include:\nProposals were made at various times, first in 1971 by American chemists, that the newly synthesised element 105 should be named \"hahnium\" in Hahn's honour, but in 1997 the IUPAC named it dubnium, after the Russian research centre in Dubna. In 1992 element 108 was discovered by a German research team, and they proposed the name hassium (after Hesse). In spite of the long-standing convention to give the discoverer the right to suggest a name, a 1994 IUPAC committee recommended that it be named \"hahnium\". After protests from the German discoverers, the name hassium (Hs) was adopted internationally in 1997.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46826", "revid": "82307", "url": "https://en.wikipedia.org/wiki?curid=46826", "title": "Windsor", "text": "Windsor may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "46827", "revid": "49059523", "url": "https://en.wikipedia.org/wiki?curid=46827", "title": "Jesse Owens", "text": "American track and field athlete (1913\u20131980)\nJames Cleveland \"Jesse\" Owens (September 12, 1913 \u2013 March 31, 1980) was an American track and field athlete who made history at the 1936 Olympic Games by winning four gold medals, setting Olympic records in each event. He is widely regarded as one of the greatest athletes in track and field history.\nOwens excelled in events like short sprints and the long jump and was recognized in his lifetime as \"perhaps the greatest and most famous athlete in track and field history\". He won four events, set five world records and tied another\u2014all in less than an hour, at the 1935 Big Ten Championships in Ann Arbor, Michigan, a feat that has never been equaled and has been called \"the greatest 45 minutes ever in sport\". He won four NCAA titles in both 1935 and 1936, bringing his total to eight\u2014an unparalleled achievement that remains unmatched to this day.\nHe achieved international fame at the 1936 Summer Olympics in Berlin, Germany, by winning four gold medals: 100 meters, long jump, 200 meters, and 4 \u00d7 100-meter relay. He was the most successful athlete at the Games and, as a black American man, was credited by ESPN with \"single-handedly crushing Hitler's myth of Aryan supremacy\".\nThe Jesse Owens Award is USA Track &amp; Field's highest accolade for the year's best track and field athlete. In a 1950 Associated Press poll, Owens was voted the greatest track and field athlete for the first half of the century. In 1999, he was on the six-man short-list for the BBC's Sports Personality of the Century. That same year, he was ranked the sixth greatest North American athlete of the twentieth century and the highest-ranked in his sport by ESPN.\nEarly life and education.\nJesse Owens, originally known as \"J. C.\", was the youngest of ten children (three girls and seven boys) born to Henry Cleveland Owens [1881\u20131942] (a sharecropper) and Mary Emma Fitzgerald in Oakville, Alabama, on September 12, 1913. He was the grandson of a slave. At the age of nine, he and his family moved to Cleveland, Ohio for better opportunities as part of the Great Migration (1910\u201370) when millions of African Americans left the segregated and rural South for the urban and industrial North. When his new teacher asked his name to enter in her roll book, he said \"J. C.\", but because of his strong Southern accent, she thought he said \"Jesse\". The name stuck, and he was known as Jesse Owens for the rest of his life.\nIn his younger years, Owens took different menial jobs in his spare time: he delivered groceries, loaded freight cars, and worked in a shoe repair shop while his father and older brother worked at a steel mill. During this period, Owens realized that he had a passion for running. Throughout his life, Owens attributed the success of his athletic career to the encouragement of Charles Riley, his junior high school track coach at Fairmount Junior High School. Since Owens worked after school, Riley allowed him to practice before school instead.\nOwens and Minnie Ruth Solomon (1915\u20132001) met at Fairmont Junior High School in Cleveland when he was 15 and she was 13. They dated steadily through high school. Ruth gave birth to their first daughter Gloria in 1932. They married on July 5, 1935, and had two more daughters together: Marlene, born in 1937, and Beverly, born in 1940. They remained married until his death in 1980.\nOwens first came to national attention when he was a student of East Technical High School in Cleveland; he equaled the world record of 9.4 seconds in the dash, broke the national high school record with 20.7 seconds in the 220 yards (201 m) dash, and long-jumped at the 1933 National High School Championship in Chicago. His 100-yard dash remained the national high school record until 1967, while his 200-yard dash held the national record for 20 years.\nCareer.\nOhio State University.\nOwens attended Ohio State University after his father found employment, which ensured that the family could be supported. Affectionately known as the \"Buckeye Bullet\" and under the coaching of Larry Snyder, Owens won a record eight individual NCAA championships, four each in 1935 and 1936. His career total of eight individual NCAA titles remains the most, despite only two years of Varsity competition\u2014which included an undefeated junior year in 1936 where he won all 42 events he entered. Though Owens enjoyed athletic success, he had to live off campus with other African-American athletes. When he traveled with the team, Owens was restricted to ordering carry-out or eating at \"blacks-only\" restaurants. Similarly, he had to stay at \"blacks-only\" hotels. Owens did not receive a scholarship for his efforts, so he continued to work part-time jobs to pay for school.\nDay of days\nMay 25, 1935, is remembered as the day when Jesse Owens won four events and established six world records in athletics at the Big Ten Championships. On that day, Owens battled through a lower back injury and set five world records and tied a sixth in a span of 45 minutes from 3:15\u20134 p.m. during the Big Ten meet at Ferry Field in Ann Arbor, Michigan. He equaled the world record for the 100-yard dash (9.4 seconds) (not to be confused with the 100-meter dash), and set world records in the long jump (, a world record that would last for 25 years); sprint (20.3 seconds); and 220-yard low hurdles (22.6 seconds, becoming the first to break 23 seconds). Both 220-yard records had also beaten the metric records for 200 meters (flat and hurdles), which counted as two additional world records from the same performances. In 2005, University of Central Florida professor of sports history Richard C. Crepeau chose these wins on one day as the most impressive athletic achievement since 1850.\n1936 Big Ten Championships\nAt the 1936 Big Ten Championships, Owens dominated the competition, winning the long jump, 100-yard dash, 220-yard dash, and 100-yard low hurdles. With these victories, he concluded his Big Ten Championship career undefeated\u2014nine titles in nine events.\nUSA Track and Field Championships.\nAt the 1934 USA Indoor Track and Field Championships, Owens captured the long jump gold with a world-record leap of . Two years later, at his final appearance at the Outdoor Championships in 1936, he shattered the long jump world record once again with a remarkable jump of 26 feet, 8\u00bc inches. That same meet, he also set a new championship record in the 100 meters, clocking in at 10.4 seconds. Over the course of his career at these championships, Owens amassed a total of six gold medals\u2014five in the long jump and one in the 100 meters.\n1936 Berlin Summer Olympics.\nOn December 4, 1935, NAACP Secretary Walter Francis White wrote a letter to Owens, but never sent it. He was trying to dissuade Owens from taking part in the 1936 Summer Olympics in Nazi Germany, arguing that an African American should not promote a racist regime after what his race had suffered at the hands of racists in his own country. In the months prior to the Games, a movement gained momentum in favor of a boycott. Owens was convinced by the NAACP to declare: \"If there are minorities in Germany who are being discriminated against, the United States should withdraw from the 1936 Olympics.\" Yet he and others eventually took part after Avery Brundage, president of the American Olympic Committee branded them \"un-American agitators\".\nIn 1936, Owens and his United States teammates sailed on the SS \"Manhattan\" and arrived in Germany to compete at the Summer Olympics in Berlin. Just before the competitions, founder of Adidas athletic shoe company Adi Dassler visited Owens in the Olympic village and persuaded Owens to wear Gebr\u00fcder Dassler Schuhfabrik shoes; this was the first sponsorship for a male African American athlete.\nOn August 3, Owens won the 100 m dash with a time of 10.3 seconds, defeating a teammate and a college friend Ralph Metcalfe by a tenth of a second and defeating Tinus Osendarp of the Netherlands by two-tenths of a second.\nOn August 4, he won the long jump with a leap of (3\u00bc inches short of his own world record). He initially credited this achievement to the technical advice that he received from Luz Long, the German competitor whom he defeated, but later admitted that this was not true, as he and Long did not meet until after the competition was over.\nOn August 5, he won the 200 meter sprint with a time of 20.7 seconds, defeating fellow American teammate Mack Robinson (the older brother of Jackie Robinson).\nOn August 9, Owens won his fourth gold medal in the 4 \u00d7 100 m sprint relay when head coach Lawson Robertson replaced Jewish-American sprinters Marty Glickman and Sam Stoller with Owens and Ralph Metcalfe, who teamed with Frank Wykoff and Foy Draper to set a world record of 39.8 seconds in the event. Owens had initially protested the last-minute switch, but assistant coach Dean Cromwell said to him, \"You'll do as you are told.\" Owens's record-breaking performance of four gold medals was not equaled until Carl Lewis won gold medals in the same events at the 1984 Summer Olympics in Los Angeles. Owens had set the world record in the long jump with a leap of in 1935, the year before the Berlin Olympics, and this record stood for 25 years until it was broken in 1960 by countryman Ralph Boston. Coincidentally, Owens was a spectator at the 1960 Summer Olympics in Rome when Boston took the gold medal in the long jump.\nThe long-jump victory is documented, along with many other 1936 events, in the 1938 film \"Olympia\" by Leni Riefenstahl. On August 1, 1936, Nazi Germany's leader, Adolf Hitler, shook hands with the German victors only and then left the stadium. International Olympic Committee president Henri de Baillet-Latour insisted that Hitler greet every medalist or none at all. Hitler opted for the latter and skipped all further medal presentations.\nOwens ran his first race on Day 2 of the Olympics (August 2). That day, He ran in the first (10:30 a.m.) and second (3:00 p.m.) qualifying rounds for the 100-meter final. He tied the Olympic and world record in the first race and broke them in the second race, but the new time was not recognized, because it was wind-assisted. Later the same day, Owens's African-American team-mate Cornelius Johnson won gold in the high jump final (which began at 5:00 p.m.) with a new Olympic record of 2.03 meters. Hitler did not publicly congratulate any of the medal winners this time; even so, the communist New York City newspaper the \"Daily Worker\" claimed Hitler received all the track winners except Johnson and left the stadium as a \"deliberate snub\" after watching Johnson's winning jump. Hitler was subsequently accused of failing to acknowledge Owens (who won gold medals on August 3, 4 (two), and 9) or shake his hand. Owens responded to these claims at the time:\nHitler had a certain time to come to the stadium and a certain time to leave. It happened he had to leave before the victory ceremony after the 100 meters [race began at 5:45 p.m.]. But before he left I was on my way to a broadcast and passed near his box. He waved at me and I waved back. I think it was bad taste to criticize the \"man of the hour\" in another country.\nIn an article dated August 4, 1936, the African-American newspaper editor Robert L. Vann describes witnessing Hitler \"salute\" Owens for having won gold in the 100 m sprint (August 3):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And then\u00a0... wonder of wonders\u00a0... I saw Herr Adolph [\"sic\"] Hitler, salute this lad. I looked on with a heart which beat proudly as the lad who was crowned king of the 100 meters event, get an ovation the like of which I have never heard before. I saw Jesse Owens greeted by the Grand Chancellor of this country as a brilliant sun peeped out through the clouds. I saw a vast crowd of some 85,000 or 90,000 people stand up and cheer him to the echo.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46828", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=46828", "title": "Fertilisation", "text": "Union of opposite-sex gametes in sexual reproduction to form a zygote\nFertilisation or fertilization (see spelling differences), also known as generative fertilisation, syngamy and impregnation, is the fusion of gametes to give rise to a zygote and initiate its development into a new individual organism or offspring. While processes such as insemination or pollination, which happen before the fusion of gametes, are also sometimes informally referred to as fertilisation, these are technically separate processes. The cycle of fertilisation and development of new individuals is called sexual reproduction. During double fertilisation in angiosperms, the haploid male gamete combines with two haploid polar nuclei to form a triploid primary endosperm nucleus by the process of vegetative fertilisation.\nDiscovery.\nIn antiquity, Aristotle conceived the formation of new individuals through fusion of male and female fluids, with form and function emerging gradually, in a mode called by him as epigenetic.\nIn 1784, Spallanzani established the need of interaction between the female's ovum and male's sperm to form a zygote in frogs. In 1827, Karl Ernst von Baer observed a therian mammalian egg for the first time. Oscar Hertwig (1876), in Germany, described the fusion of nuclei of spermatozoa and of ova from sea urchin.\nEvolution.\nThe evolution of fertilisation is related to the origin of meiosis, as both are part of sexual reproduction, originated in eukaryotes. One hypothesis states that meiosis originated from mitosis.\nFertilisation in plants.\nThe gametes that participate in fertilisation of plants are the sperm (male) and the egg (female) cell. Various plant groups have differing methods by which the gametes produced by the male and female gametophytes come together and are fertilised. In bryophytes and pteridophytic land plants, fertilisation of the sperm and egg takes place within the archegonium. In seed plants, the male gametophyte is formed within a pollen grain. After pollination, the pollen grain germinates, and a pollen tube grows and penetrates the ovule through a tiny pore called a micropyle. The sperm are transferred from the pollen through the pollen tube to the ovule where the egg is fertilised. In flowering plants, two sperm cells are released from the pollen tube, and a second fertilisation event occurs involving the second sperm cell and the central cell of the ovule, which is a second female gamete.\nPollen tube growth.\nUnlike animal sperm which is motile, the sperm of most seed plants is immotile and relies on the pollen tube to carry it to the ovule where the sperm is released. The pollen tube penetrates the stigma and elongates through the extracellular matrix of the style before reaching the ovary. Then near the receptacle, it breaks through the ovule through the micropyle (an opening in the ovule wall) and the pollen tube \"bursts\" into the embryo sac, releasing sperm. The growth of the pollen tube has been believed to depend on chemical cues from the pistil, however these mechanisms were poorly understood until 1995. Work done on tobacco plants revealed a family of glycoproteins called TTS proteins that enhanced growth of pollen tubes. Pollen tubes in a sugar free pollen germination medium and a medium with purified TTS proteins both grew. However, in the TTS medium, the tubes grew at a rate 3x that of the sugar-free medium. TTS proteins were also placed on various locations of semi in vivo pollinated pistils, and pollen tubes were observed to immediately extend toward the proteins. Transgenic plants lacking the ability to produce TTS proteins had slower pollen tube growth and reduced fertility.\nRupture of pollen tube.\nThe rupture of the pollen tube to release sperm in \"Arabidopsis\" has been shown to depend on a signal from the female gametophyte. Specific proteins called FER protein kinases present in the ovule control the production of highly reactive derivatives of oxygen called reactive oxygen species (ROS). ROS levels have been shown via GFP to be at their highest during floral stages when the ovule is the most receptive to pollen tubes, and lowest during times of development and following fertilisation. High amounts of ROS activate Calcium ion channels in the pollen tube, causing these channels to take up Calcium ions in large amounts. This increased uptake of calcium causes the pollen tube to rupture, and release its sperm into the ovule. Pistil feeding assays in which plants were fed diphenyl iodonium chloride (DPI) suppressed ROS concentrations in \"Arabidopsis\", which in turn prevented pollen tube rupture.\nFlowering plants.\nAfter being fertilised, the ovary starts to swell and develop into the fruit. With multi-seeded fruits, multiple grains of pollen are necessary for syngamy with each ovule. The growth of the pollen tube is controlled by the vegetative (or tube) cytoplasm. Hydrolytic enzymes are secreted by the pollen tube that digest the female tissue as the tube grows down the stigma and style; the digested tissue is used as a nutrient source for the pollen tube as it grows. During pollen tube growth towards the ovary, the generative nucleus divides to produce two separate sperm nuclei (haploid number of chromosomes) \u2013 a growing pollen tube therefore contains three separate nuclei, two sperm and one tube. The sperms are interconnected and dimorphic, the large one, in a number of plants, is also linked to the tube nucleus and the interconnected sperm and the tube nucleus form the \"male germ unit\".\nDouble fertilisation is the process in angiosperms (flowering plants) in which two sperm from each pollen tube fertilise two cells in a female gametophyte (sometimes called an embryo sac) that is inside an ovule. After the pollen tube enters the gametophyte, the pollen tube nucleus disintegrates and the two sperm cells are released; one of the two sperm cells \"fertilises\" the egg cell (at the bottom of the gametophyte near the micropyle), forming a diploid (2n) zygote. This is the point when fertilisation actually occurs; pollination and fertilisation are two separate processes. The nucleus of the other sperm cell fuses with two haploid polar nuclei (contained in the central cell) in the centre of the gametophyte. The resulting cell is triploid (3n). This triploid cell divides through mitosis and forms the endosperm, a nutrient-rich tissue, inside the seed. The two central-cell maternal nuclei (polar nuclei) that contribute to the endosperm arise by mitosis from the single meiotic product that also gave rise to the egg. Therefore, maternal contribution to the genetic constitution of the triploid endosperm is double that of the embryo.\nOne primitive species of flowering plant, \"Nuphar polysepala\", has endosperm that is diploid, resulting from the fusion of a sperm with one, rather than two, maternal nuclei. It is believed that early in the development of angiosperm lineages, there was a duplication in this mode of reproduction, producing seven-celled/eight-nucleate female gametophytes, and triploid endosperms with a 2:1 maternal to paternal genome ratio.\nIn many plants, the development of the flesh of the fruit is proportional to the percentage of fertilised ovules. For example, with watermelon, about a thousand grains of pollen must be delivered and spread evenly on the three lobes of the stigma to make a normal sized and shaped fruit.\nSelf-pollination and outcrossing.\nOutcrossing, or cross-fertilisation, and self-fertilisation represent different strategies with differing benefits and costs. An estimated 48.7% of plant species are either dioecious or self-incompatible obligate outcrossers. It is also estimated that about 42% of flowering plants exhibit a mixed mating system in nature.\nIn the most common kind of mixed mating system, individual plants produce a single type of flower and fruits may contain self-fertilised, outcrossed or a mixture of progeny types. The transition from cross-fertilisation to self-fertilisation is the most common evolutionary transition in plants, and has occurred repeatedly in many independent lineages. About 10\u201315% of flowering plants are predominantly self-fertilising.\nUnder circumstances where pollinators or mates are rare, self-fertilisation offers the advantage of reproductive assurance. Self-fertilisation can therefore result in improved colonisation ability. In some species, self-fertilisation has persisted over many generations. \"Capsella rubella\" is a self-fertilising species that became self-compatible 50,000 to 100,000 years ago. \"Arabidopsis thaliana\" is a predominantly self-fertilising plant with an out-crossing rate in the wild of less than 0.3%; a study suggested that self-fertilisation evolved roughly a million years ago or more in \"A. thaliana\". In long-established self-fertilising plants, the masking of deleterious mutations and the production of genetic variability is infrequent and thus unlikely to provide a sufficient benefit over many generations to maintain the meiotic apparatus. Consequently, one might expect self-fertilisation to be replaced in nature by an ameiotic asexual form of reproduction that would be less costly. However the actual persistence of meiosis and self-fertilisation as a form of reproduction in long-established self-fertilising plants may be related to the immediate benefit of efficient recombinational repair of DNA damage during formation of germ cells provided by meiosis at each generation.\nFertilisation in animals.\nThe mechanics behind fertilisation has been studied extensively in sea urchins and mice. This research addresses the question of how the sperm and the appropriate egg find each other and the question of how only one sperm gets into the egg and delivers its contents. There are three steps to fertilisation that ensure species-specificity:\nInternal vs. external.\nConsideration as to whether an animal (more specifically a vertebrate) uses internal or external fertilisation is often dependent on the method of birth. Oviparous animals laying eggs with thick calcium shells, such as chickens, or thick leathery shells generally reproduce via internal fertilisation so that the sperm fertilises the egg without having to pass through the thick, protective, tertiary layer of the egg. Ovoviviparous and viviparous animals also use internal fertilisation. Although some organisms reproduce via amplexus, they may still use internal fertilisation, as with some salamanders. Advantages of internal fertilisation include minimal waste of gametes, greater chance of individual egg fertilisation, longer period of egg protection, and selective fertilisation. Many females have the ability to store sperm for extended periods of time and can fertilise their eggs at their own desire.\nOviparous animals producing eggs with thin tertiary membranes or no membranes at all, on the other hand, use external fertilisation methods. Such animals may be more precisely termed ovuliparous. External fertilisation is advantageous in that it minimises contact (which decreases the risk of disease transmission), and greater genetic variation.\nSea urchins.\nSperm find the eggs via chemotaxis, a type of ligand/receptor interaction. Resact is a 14 amino acid peptide purified from the jelly coat of \"A. punctulata\" that attracts the migration of sperm.\nAfter finding the egg, the sperm penetrates the jelly coat through a process called sperm activation. In another ligand/receptor interaction, an oligosaccharide component of the egg binds and activates a receptor on the sperm and causes the acrosomal reaction. The acrosomal vesicles of the sperm fuse with the plasma membrane and are released. In this process, molecules bound to the acrosomal vesicle membrane, such as bindin, are exposed on the surface of the sperm. These contents digest the jelly coat and eventually the vitelline membrane. In addition to the release of acrosomal vesicles, there is explosive polymerisation of actin to form a thin spike at the head of the sperm called the acrosomal process.\nThe sperm binds to the egg through another ligand reaction between receptors on the vitelline membrane. The sperm surface protein bindin, binds to a receptor on the vitelline membrane identified as EBR1.\nFusion of the plasma membranes of the sperm and egg are likely mediated by bindin. At the site of contact, fusion causes the formation of a fertilisation cone.\nMammals.\nMale mammals internally fertilise females and ejaculate semen through the penis during copulation. After ejaculation, many sperm move to the upper vagina (via contractions from the vagina) through the cervix and across the length of the uterus to meet the ovum. In cases where fertilisation occurs, the female usually ovulates during a period that extends from hours before copulation to a few days after; therefore, in most mammals, it is more common for ejaculation to precede ovulation than vice versa.\nWhen sperm are deposited into the anterior vagina, they are not capable of fertilisation (i.e., non-capacitated) and are characterised by slow linear motility patterns. This motility, combined with muscular contractions enables sperm transport towards the uterus and oviducts. There is a pH gradient within the micro-environment of the female reproductive tract such that the pH near the vaginal opening is lower (approximately 5) than the oviducts (approximately 8). The sperm-specific pH-sensitive calcium transport protein called CatSper increases the sperm cell permeability to calcium as it moves further into the reproductive tract. Intracellular calcium influx contributes to sperm capacitation and hyperactivation, causing a more violent and rapid non-linear motility pattern as sperm approach the oocyte. The capacitated spermatozoon and the oocyte meet and interact in the \"ampulla\" of the fallopian tube. Rheotaxis, thermotaxis and chemotaxis are known mechanisms that guide sperm towards the egg during the final stage of sperm migration. Spermatozoa respond (see Sperm thermotaxis) to the temperature gradient of ~2\u00a0\u00b0C between the oviduct and the ampulla, and chemotactic gradients of progesterone have been confirmed as the signal emanating from the cumulus oophorus cells surrounding rabbit and human oocytes. Capacitated and hyperactivated sperm respond to these gradients by changing their behaviour and moving towards the cumulus-oocyte complex. Other chemotactic signals such as formyl Met-Leu-Phe (fMLF) may also guide spermatozoa.\nThe zona pellucida, a thick layer of extracellular matrix that surrounds the egg and is similar to the role of the vitelline membrane in sea urchins, binds the sperm. Unlike sea urchins, the sperm binds to the egg before the acrosomal reaction. ZP3, a glycoprotein in the zona pellucida, is responsible for egg/sperm adhesion in humans. The receptor galactosyltransferase (GalT) binds to the N-acetylglucosamine residues on the ZP3 and is important for binding with the sperm and activating the acrosome reaction. ZP3 is sufficient though unnecessary for sperm/egg binding. Two additional sperm receptors exist: a 250kD protein that binds to an oviduct secreted protein, and SED1, which independently binds to the zona. After the acrosome reaction, the sperm is believed to remain bound to the zona pellucida through exposed ZP2 receptors. These receptors are unknown in mice but have been identified in guinea pigs.\nIn mammals, the binding of the spermatozoon to the GalT initiates the acrosome reaction. This process releases the hyaluronidase that digests the matrix of hyaluronic acid in the vestments around the oocyte. Additionally, heparin-like glycosaminoglycans (GAGs) are released near the oocyte that promote the acrosome reaction. Fusion between the oocyte plasma membranes and sperm follows and allows the sperm nucleus, the typical centriole, and atypical centriole that is attached to the flagellum, but not the mitochondria, to enter the oocyte. The protein CD9 likely mediates this fusion in mice (the binding homologue). The egg \"activates\" itself upon fusing with a single sperm cell and thereby changes its cell membrane to prevent fusion with other sperm. Zinc atoms are released during this activation.\nThis process ultimately leads to the formation of a diploid cell called a zygote. The zygote divides to form a blastocyst and, upon entering the uterus, implants in the endometrium, beginning pregnancy. Embryonic implantation not in the uterine wall results in an ectopic pregnancy that can kill the mother.\nIn such animals as rabbits, coitus induces ovulation by stimulating the release of the pituitary hormone gonadotropin; this release greatly increases the likelihood of pregnancy.\nHumans.\nFertilisation in humans is the union of a human egg and sperm, usually occurring in the ampulla of the fallopian tube, producing a single celled zygote, the first stage in the development of a genetically unique organism, and initiating embryonic development. Scientists discovered the dynamics of human fertilisation in the nineteenth century.\nThe term \"conception\" commonly refers to \"the process of becoming pregnant involving fertilisation or implantation or both\". Its use makes it a subject of semantic arguments about the beginning of pregnancy, typically in the context of the abortion debate.\nUpon gastrulation, which occurs around 16 days after fertilisation, the implanted blastocyst develops three germ layers, the endoderm, the ectoderm and the mesoderm, and the genetic code of the father becomes fully involved in the development of the embryo; later twinning is impossible. Additionally, interspecies hybrids survive only until gastrulation and cannot further develop.\nHowever, some human developmental biology literature refers to the \"conceptus\" and such medical literature refers to the \"products of conception\" as the post-implantation embryo and its surrounding membranes. The term \"conception\" is not usually used in scientific literature because of its variable definition and connotation.\nInsects.\nInsects in different groups, including the Odonata (dragonflies and damselflies) and the Hymenoptera (ants, bees, and wasps) practise delayed fertilisation. Among the Odonata, females may mate with multiple males, and store sperm until the eggs are laid. The male may hover above the female during egg-laying (oviposition) to prevent her from mating with other males and replacing his sperm; in some groups such as the darters, the male continues to grasp the female with his claspers during egg-laying, the pair flying around in tandem. Among social Hymenoptera, honeybee queens mate only on mating flights, in a short period lasting some days; a queen may mate with eight or more drones. She then stores the sperm for the rest of her life, perhaps for five years or more.\nFertilisation in fungi.\nIn many fungi (except chytrids), as in some protists, fertilisation is a two step process. First, the cytoplasms of the two gamete cells fuse (called plasmogamy), producing a dikaryotic or heterokaryotic cell with multiple nuclei. This cell may then divide to produce dikaryotic or heterokaryotic hyphae. The second step of fertilisation is karyogamy, the fusion of the nuclei to form a diploid zygote.\nIn chytrid fungi, fertilisation occurs in a single step with the fusion of gametes, as in animals and plants.\nFertilisation in protists.\nFertilisation in protozoa.\nThere are three types of fertilisation processes in protozoa:\nFertilisation in algae.\nAlgae, like some land plants, undergo alternation of generations. Some algae are isomorphic, where both the sporophyte (2n) and gameteophyte (n) are the same morphologically. When algae reproduction is described as oogamous, the male and female gametes are different morphologically, where there is a large non-motile egg for female gametes, and the male gamete are uniflagellate (motile). Via the process of syngamy, these will form a new zygote, regenerating the sporophyte generation again.\nFertilisation and genetic recombination.\nMeiosis results in a random segregation of the genes that each parent contributes. Each parent organism is usually identical save for a fraction of their genes; each gamete is therefore genetically unique. At fertilisation, parental chromosomes combine. In humans, (2\u00b2\u00b2)\u00b2 = 17.6x1012 chromosomally different zygotes are possible for the non-sex chromosomes, even assuming no chromosomal crossover. If crossover occurs once, then on average (4\u00b2\u00b2)\u00b2 = 309x1024 genetically different zygotes are possible for every couple, not considering that crossover events can take place at most points along each chromosome. The X and Y chromosomes undergo no crossover events and are therefore excluded from the calculation. The mitochondrial DNA is only inherited from the maternal parent.\nThe sperm aster and zygote centrosomes.\nShortly after the sperm fuse with the egg, the two sperm centrioles form the embryo first centrosome and microtubule aster. The sperm centriole, found near the male pronucleus, recruit egg Pericentriolar material proteins forming the zygote first centrosome. This centrosome nucleates microtubules in the shape of stars called astral microtubules. The microtubules span the whole valium of the egg, allowing the egg pronucleus to use the cables to get to the male pronucleus. As the male and female pronuclei approach each other, the single centrosome split into two centrosomes located in the interphase between the pronuclei. Then the centrosome via the astral microtubules polarises the genome inside the pronuclei.\nParthenogenesis.\nOrganisms that normally reproduce sexually can also reproduce via parthenogenesis, wherein an unfertilised female gamete produces viable offspring. These offspring may be clones of the mother, or in some cases genetically differ from her but inherit only part of her DNA. Parthenogenesis occurs in many plants and animals and may be induced in others through a chemical or electrical stimulus to the egg cell. In 2004, Japanese researchers led by Tomohiro Kono succeeded after 457 attempts to merge the ova of two mice by blocking certain proteins that would normally prevent the possibility; the resulting embryo normally developed into a mouse.\nAllogamy and autogamy.\nAllogamy, which is also known as cross-fertilisation, refers to the fertilisation of an egg cell from one individual with the male gamete of another.\nAutogamy which is also known as self-fertilisation, occurs in such hermaphroditic organisms as plants and flatworms; therein, two gametes from one individual fuse.\nOther variants of bisexual reproduction.\nSome relatively unusual forms of reproduction are:\nGynogenesis: A sperm stimulates the egg to develop without fertilisation or syngamy. The sperm may enter the egg.\nHybridogenesis: One genome is eliminated to produce haploid eggs.\nCanina meiosis: (sometimes called \"permanent odd polyploidy\") one genome is transmitted in the Mendelian fashion, others are transmitted clonally.\nBenefits of cross-fertilisation.\nThe major benefit of cross-fertilisation is generally thought to be the avoidance of inbreeding depression. Charles Darwin, in his 1876 book \"The Effects of Cross and Self Fertilisation in the Vegetable Kingdom\" (pages 466-467) summed up his findings in the following way.\n\"It has been shown in the present volume that the offspring from the union of two distinct individuals, especially if their progenitors have been subjected to very different conditions, have an immense advantage in height, weight, constitutional vigour and fertility over the self-fertilised offspring from one of the same parents. And this fact is amply sufficient to account for the development of the sexual elements, that is, for the genesis of the two sexes.\"\nIn addition, it is thought by some, that a long-term advantage of out-crossing in nature is increased genetic variability that promotes adaptation or avoidance of extinction (see Genetic variability).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46830", "revid": "49853791", "url": "https://en.wikipedia.org/wiki?curid=46830", "title": "Postmodernist", "text": ""}
{"id": "46832", "revid": "279219", "url": "https://en.wikipedia.org/wiki?curid=46832", "title": "United States Federalist Party", "text": ""}
{"id": "46833", "revid": "274535", "url": "https://en.wikipedia.org/wiki?curid=46833", "title": "The Federalist Papers", "text": "1788 essay collection by Alexander Hamilton, James Madison, and John Jay\nThe Federalist Papers is a collection of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay under the collective pseudonym \"Publius\" to promote the ratification of the Constitution of the United States. The collection was commonly known as The Federalist until the name \"The Federalist Papers\" emerged in the nineteenth century.\nThe first seventy-seven of these essays were published serially in the \"Independent Journal\", the \"New York Packet\", and the \"Daily Advertiser\" between October 1787 and April 1788. A compilation of these 77 essays and eight others were published in two volumes as The Federalist: A Collection of Essays, Written in Favour of the New Constitution, as Agreed upon by the Federal Convention, September 17, 1787, by publishing firm J. &amp; A. McLean in March and May 1788. The last eight papers (Nos. 78\u201385) were republished in the New York newspapers between June 14 and August 16, 1788.\nThe authors of \"The Federalist\" intended to influence the voters to ratify the Constitution. In Federalist No. 1, Hamilton explicitly sets that debate in broad political terms:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It has been frequently remarked, that it seems to have been reserved to the people of this country, by their conduct and example, to decide the important question, whether societies of men are really capable or not, of establishing good government from reflection and choice, or whether they are forever destined to depend, for their political constitutions, on accident and force.\nIn Federalist No. 10, Madison discusses the means of preventing rule by majority faction and advocates a large, commercial republic. This is complemented by Federalist No. 14, in which Madison takes the measure of the United States, declares it appropriate for an extended republic, and concludes with a memorable defense of the constitutional and political creativity of the Federal Convention.\nIn Federalist No. 84, Hamilton makes the case that there is no need to amend the Constitution by adding a Bill of Rights, insisting that the various provisions in the proposed Constitution protecting liberty amount to a \"bill of rights\". Federalist No. 78, also written by Hamilton, lays the groundwork for the doctrine of judicial review by federal courts of federal legislation or executive acts. Federalist No. 70 presents Hamilton's case for a one-man chief executive. In Federalist No. 39, Madison presents the clearest exposition of what has come to be called \"Federalism\". In Federalist No. 51, Madison distills arguments for checks and balances in an essay often cited for its justification of government as \"the greatest of all reflections on human nature.\" According to historian Richard B. Morris, the essays that make up \"The Federalist Papers\" are an \"incomparable exposition of the Constitution, a classic in political science unsurpassed in both breadth and depth by the product of any later American writer.\"\nOn June 21, 1788, the proposed Constitution was ratified by the minimum of nine states required under Article VII. In late July 1788, with eleven states having ratified the new Constitution, the process of organizing the new government began.\nHistory.\nOrigins.\nThe Federal Convention (Constitutional Convention) sent the proposed Constitution to the Confederation Congress, which in turn submitted it to the states for ratification at the end of September 1787. On September 27, 1787, \"Cato\" first appeared in the New York press criticizing the proposition; \"Brutus\" followed on October 18, 1787. These and other articles and public letters critical of the new Constitution would eventually become known as the \"Anti-Federalist Papers\". In response, Alexander Hamilton decided to launch a measured defense and extensive explanation of the proposed Constitution to the people of the state of New York. He wrote in Federalist No. 1 that the series would \"endeavor to give a satisfactory answer to all the objections which shall have made their appearance, that may seem to have any claim to your attention.\"\nHamilton recruited collaborators for the project. He enlisted John Jay, who after four essays (Federalist Nos. 2, 3, 4, and 5), fell ill and contributed only one more essay, Federalist No. 64, to the series. Jay also distilled his case into a pamphlet in the spring of 1788, \"An Address to the People of the State of New-York\"; Hamilton cited it approvingly in Federalist No. 85. James Madison, present in New York as a Virginia delegate to the Confederation Congress, was recruited by Hamilton and Jay and became Hamilton's primary collaborator. Gouverneur Morris and William Duer were also considered. However, Morris turned down the invitation, and Hamilton rejected three essays written by Duer. Duer later wrote in support of the three Federalist authors under the name \"Philo-Publius\", meaning either \"Friend of the People\" or \"Friend of Hamilton\" based on Hamilton's pen name \"Publius\".\nHamilton chose the pseudonymous name \"Publius\". While many other pieces representing both sides of the constitutional debate were written under Roman names, historian Albert Furtwangler contends that \"'Publius' was a cut above 'Caesar' or 'Brutus' or even 'Cato'. Publius Valerius helped found the ancient republic of Rome. His more famous name, Publicola, meant 'friend of the people'.\" Hamilton had applied this pseudonym to three letters in 1778, in which he attacked fellow Federalist Samuel Chase and revealed that Chase had taken advantage of knowledge gained in Congress to try to dominate the flour market.\nAuthorship.\nAt the time of publication, the authors of \"The Federalist Papers\" attempted to hide their identities due to Hamilton and Madison having attended the convention. Astute observers, however, correctly discerned the identities of Hamilton, Madison, and Jay. Establishing authorial authenticity of the essays that constitute \"The Federalist Papers\" has not always been clear. After Hamilton's death in 1804, a list emerged, claiming that he alone had written two-thirds of \"The Federalist\" essays. Some believe that several of these essays were written by Madison (Nos. 49\u201358 and 62\u201363). The scholarly detective work of Douglass Adair in 1944 postulated the following assignments of authorship, corroborated in 1964 by a statistical analysis of the text:\nIn six months, a total of 85 articles were written by the three men.\nHamilton, who had been a leading advocate of national constitutional reform throughout the 1780s and was one of the three representatives for New York at the Constitutional Convention, in 1789 became the first secretary of the treasury, a post he held until his resignation in 1795.\nMadison, who is now acknowledged as the father of the Constitution, despite his repeated rejection of this honor during his lifetime, became a leading member of the U.S. House of Representatives from Virginia (1789\u20131797), secretary of state (1801\u20131809), and ultimately the fourth president of the United States (1809\u20131817).\nJohn Jay, who had been secretary for foreign affairs under the Articles of Confederation from 1784 through their expiration in 1789, became the first Chief Justice of the United States in 1789, stepping down in 1795 to accept election as governor of New York, a post he held for two terms, retiring in 1801.\nPublication.\n\"The Federalist\" articles appeared in three New York newspapers: \"The Independent Journal\", the \"New-York Packet\", and the \"Daily Advertiser\", beginning on October 27, 1787. Although written and published with haste, \"The Federalist\" articles were widely read and greatly influenced the shape of American political institutions. Hamilton, Madison and Jay published the essays at a rapid pace. At times, three to four new essays by Publius appeared in the papers in a single week. Garry Wills observes that this fast pace of production \"overwhelmed\" any possible response: \"Who, given ample time could have answered such a battery of arguments? And no time was given.\" Hamilton also encouraged the reprinting of the essays in newspapers outside New York state, and indeed they were published in several other states where the ratification debate was taking place. However, they were only irregularly published outside New York, and in other parts of the country they were often overshadowed by local writers.\nBecause the essays were initially published in New York, most of them begin with the same salutation: \"To the People of the State of New York\".\nThe high demand for the essays led to their publication in a more permanent form. On January 1, 1788, the New York publishing firm J. &amp; A. McLean announced that they would publish the first 36 essays as a bound volume; that volume was released on March 22, 1788, and was titled \"The Federalist\" Volume 1. New essays continued to appear in the newspapers; Federalist No. 77 was the last number to appear first in that form, on April 2. A second bound volume was released on May 28, containing Federalist Nos. 37\u201377 and the previously unpublished Nos. 78\u201385. The last eight papers (Nos. 78\u201385) were republished in the New York newspapers between June 14 and August 16, 1788.\nA 1792 French edition ended the collective anonymity of Publius, announcing that the work had been written by \"Mm. Hamilton, Maddisson e Gay, citoyens de l'\u00c9tat de New York\". In 1802, George Hopkins published an American edition that similarly named the authors. Hopkins wished as well that \"the name of the writer should be prefixed to each number,\" but at this point Hamilton insisted that this was not to be, and the division of the essays among the three authors remained a secret.\nThe first publication to divide the papers in such a way was an 1810 edition that used a list left by Hamilton to associate the authors with their numbers; this edition appeared as two volumes of the compiled \"Works of Hamilton\". In 1818, Jacob Gideon published a new edition with a new listing of authors, based on a list provided by Madison. The difference between Hamilton's list and Madison's formed the basis for a dispute over the authorship of a dozen of the essays.\nBoth Hopkins's and Gideon's editions incorporated significant edits to the text of the papers themselves, generally with the approval of the authors. In 1863, Henry Dawson published containing the original text of the papers, arguing that they should be preserved as they were written in that particular historical moment, not as edited by the authors years later.\nModern scholars generally use the text prepared by Jacob E. Cooke for his 1961 edition of \"The Federalist\"; this edition used the newspaper texts for essay numbers 1\u201376 and the McLean edition for essay numbers 77\u201385.\nDisputed essays.\nWhile the authorship of 73 of \"The Federalist\" essays is fairly certain, the identities of those who wrote the twelve remaining essays are disputed by some scholars. The modern consensus is that Madison wrote essays Nos. 49\u201358, with Nos. 18\u201320 being products of a collaboration between him and Hamilton; No. 64 was by John Jay. The first open designation of which essay belonged to whom was provided by Hamilton who, in the days before his ultimately fatal gun duel with Aaron Burr, provided his lawyer with a list detailing the author of each number. This list credited Hamilton with a full 63 of the essays (three of those being jointly written with Madison), almost three-quarters of the whole, and was used as the basis for an 1810 printing that was the first to make specific attribution for the essays.\nMadison did not immediately dispute Hamilton's list, but provided his own list for the 1818 Gideon edition of \"The Federalist\". Madison claimed 29 essays for himself, and he suggested that the difference between the two lists was \"owing doubtless to the hurry in which [Hamilton's] memorandum was made out.\" A known error in Hamilton's list\u2014Hamilton incorrectly ascribed No. 54 to John Jay, when in fact, Jay wrote No. 64\u2014provided some evidence for Madison's suggestion.\nStatistical analysis has been undertaken on several occasions in attempts to accurately identify the author of each individual essay. After examining word choice and writing style, studies generally agree that the disputed essays were written by James Madison. However, there are notable exceptions maintaining that some of the essays which are now widely attributed to Madison were, in fact, collaborative efforts.\nInfluence on the ratification debates.\n\"The Federalist Papers\" were written to support the ratification of the Constitution, specifically in New York. Whether they succeeded in this mission is questionable. Separate ratification proceedings took place in each state, and the essays were not reliably reprinted outside of New York; furthermore, by the time the series was well underway, a number of important states had already ratified it, for instance Pennsylvania on December 12. New York held out until July 26; certainly \"The Federalist\" was more important there than anywhere else, but historian Albert Furtwangler argues that it \"could hardly rival other major forces in the ratification contests\"\u2014specifically, these forces included the personal influence of well-known Federalists, for instance Hamilton and Jay, and Anti-Federalists, including Governor George Clinton. Further, by the time New York came to a vote, ten states had already ratified the Constitution and it had thus already passed\u2014only nine states had to ratify it for the new government to be established among them; the ratification by Virginia, the tenth state, placed pressure on New York to ratify. In light of that, Furtwangler observes, \"New York's refusal would make that state an odd outsider.\"\nOnly 19 Federalists were elected to New York's ratification convention, compared to the Anti-Federalists' 46 delegates. While New York did indeed ratify the Constitution on July 26, the lack of public support for pro-Constitution Federalists has led historian John Kaminski to suggest that the impact of \"The Federalist\" on New York citizens was \"negligible\".\nAs for Virginia, which ratified the Constitution only at its convention on June 25, Hamilton writes in a letter to Madison that the collected edition of \"The Federalist\" had been sent to Virginia; Furtwangler presumes that it was to act as a \"debater's handbook for the convention there\", though he claims that this indirect influence would be a \"dubious distinction\". Probably of greater importance to the Virginia debate, in any case, were George Washington's support for the proposed Constitution and the presence of Madison and Edmund Randolph, the governor, at the convention arguing for ratification.\nStructure and content.\nIn Federalist No. 1, Hamilton listed six topics to be covered in the subsequent articles:\nFurtwangler notes that as the series grew, this plan was somewhat changed. The fourth topic expanded into detailed coverage of the individual articles of the Constitution and the institutions it mandated, while the two last topics were merely touched on in the last essay.\nThe papers can be broken down by author as well as by topic. At the start of the series, all three authors were contributing; the first 20 papers are broken down as 11 by Hamilton, five by Madison and four by Jay. The rest of the series, however, is dominated by three long segments by a single writer: Nos. 21\u201336 by Hamilton, Nos. 37\u201358 by Madison, written while Hamilton was in Albany, and No. 65 through the end by Hamilton, published after Madison had left for Virginia.\nOpposition to the Bill of Rights.\n\"The Federalist Papers\" (specifically Federalist No. 84) are notable for their opposition to what later became the United States Bill of Rights. The idea of adding a bill of rights to the Constitution was originally controversial because the Constitution, as written, did not specifically enumerate or protect the rights of the people; rather, it was intended to list the powers of the government and left all that remained to the states and the people. Alexander Hamilton, the author of Federalist No. 84, feared that such an enumeration, once written down explicitly, would later be interpreted as a list of the \"only\" rights that people had.\nHowever, Hamilton's opposition to a Bill of Rights was far from universal. Robert Yates, writing under the pseudonym \"Brutus\", articulated this view point in the so-called Anti-Federalist No. 84, asserting that a government unrestrained by such a bill could easily devolve into tyranny. References in \"The Federalist\" and in the ratification debates warn of demagogues of the variety who through divisive appeals would aim at tyranny. \"The Federalist\" begins and ends with this issue. In the final paper Hamilton offers \"a lesson of moderation to all sincere lovers of the Union, and ought to put them on their guard against hazarding anarchy, civil war, a perpetual alienation of the States from each other, and perhaps the military despotism of a successful demagogue\". The matter was further clarified by the Ninth Amendment.\nJudicial use.\nFederal judges, when interpreting the Constitution, frequently use \"The Federalist Papers\" as a contemporary account of the intentions of the framers and ratifiers. They have been applied on issues ranging from the power of the federal government in foreign affairs (in \"Hines v. Davidowitz\") to the validity of ex post facto laws (in the 1798 decision \"Calder v. Bull\", apparently the first decision to mention \"The Federalist\"). By 2000[ [update]], \"The Federalist\" had been quoted 291 times in Supreme Court decisions.\nThe amount of deference that should be given to \"The Federalist Papers\" in constitutional interpretation has always been somewhat controversial. As early as 1819, Chief Justice John Marshall noted in the famous case \"McCulloch v. Maryland\", that \"the opinions expressed by the authors of that work have been justly supposed to be entitled to great respect in expounding the Constitution. No tribute can be paid to them which exceeds their merit; but in applying their opinions to the cases which may arise in the progress of our government, a right to judge of their correctness must be retained.\" In a letter to Thomas Ritchie in 1821, James Madison stated of the Constitution that \"the legitimate meaning of the Instrument must be derived from the text itself; or if a key is to be sought elsewhere, it must be not in the opinions or intentions of the Body which planned &amp; proposed the Constitution, but in the sense attached to it by the people in their respective State Conventions where it recd. all the authority which it possesses.\"\nComplete list.\nThe colors used to highlight the rows correspond to the author of the paper.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Alexander Hamilton\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0John Jay\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0James Madison\nCalendar view.\n&lt;templatestyles src=\"Template:Calendar/styles.css\"/&gt;\nIn popular culture.\nThe purposes and authorship of \"The Federalist Papers\" were prominently highlighted in the lyrics of \"Non-Stop\", the finale of Act One in the 2015 Broadway musical \"Hamilton\", written by Lin-Manuel Miranda.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46834", "revid": "50317656", "url": "https://en.wikipedia.org/wiki?curid=46834", "title": "Cisco (disambiguation)", "text": "Cisco is an American technology conglomerate.\nCisco may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "46835", "revid": "45774152", "url": "https://en.wikipedia.org/wiki?curid=46835", "title": "Federalist", "text": "Set of political beliefs\nThe term federalist describes several political beliefs around the world. It may also refer to the concept of parties, whose members or supporters call themselves \"Federalists\".\nHistory.\nEurope federation.\nIn Europe, proponents of deeper European integration are sometimes called Federalists. A major European NGO and advocacy group campaigning for such a political union is the Union of European Federalists. Movements towards a peacefully unified European state have existed since the 1920s, notably the Paneuropean Union. A pan-European party with representation in the European Parliament fighting for the same cause is Volt Europa.\nIn the European Parliament the Spinelli Group brings together MEPs from different political groups to work together of ideas and projects of European federalism; taking their name from Italian politician and MEP Altiero Spinelli, who himself was a major proponent of European federalism, also meeting with fellow deputies in the Crocodile Club.\nNotable European Federalists are former European Commission president Jean-Claude Juncker, current EC president Ursula von der Leyen, leader of ALDE group Guy Verhofstadt, German Federal Minister for Economic Affairs and Energy of Germany Peter Altmaier, German MEP Elmar Brok and the former leader of the SPD Martin Schulz.\nLatin America.\nIn the Spanish-speaking parts of Latin America the term \"federalist\" is used in reference to the politics of 19th-century Argentina and Colombia. The Federalists opposed the Unitarians in Argentina and the Centralists in Colombia through the 19th century. Federalists fought for complete self-government and full provincial autonomy, as opposed to the centralized government that the Unitarians and Centralists favored. Furthermore, Federalists demanded tariff protection for their industries and, in Argentina, called for the end of the Buenos Aires customs as the only intermediary for foreign trade. During the Federal War (1859-1863) in Venezuela, liberal \"caudillos\" confronted conservatives, leading to the establishment of the modern federal States of Venezuela.\nArgentina.\nThe one Federalist leader in the Platine Region was Jos\u00e9 Gervasio Artigas, who opposed the centralist governments in Buenos Aires that followed the May Revolution, and created instead the Federal League in 1814 among several Argentine Provinces and the Banda Oriental (modern-day Uruguay). In 1819, the Federal armies rejected the \"centralist\" Constitution of the United Provinces of South America and defeated the forces of Supreme Director Jos\u00e9 Rondeau at the 1820 Battle of Cepeda, effectively ending the central government and securing Provinces' sovereignty through a series of inter-Provincial pacts (v.g. \"Treaty of Pilar\", \"Treaty of Benegas\", \"Quadrilateral Treaty\"). A new National Constitution was proposed only in 1826, during the Presidency of Unitarian Bernardino Rivadavia, but it was again rejected by the Provinces, leading to the dissolution of the National Government the following year.\nFederalist Buenos Aires Governor Manuel Dorrego took over the management of the foreign affairs of the United Provinces, but he was deposed and executed in 1828 by Unitarian General Juan Lavalle, who commanded troops dissatisfied with the negotiations that ended the War with Brazil. The following year, Juan Manuel de Rosas, leader of Buenos Aires Federalists, defeated Lavalle and secured his resignation. Rosas was elected Governor of Buenos Aires later that year by the Provincial Legislature. To counteract these developments, the Unitarian League was created by General Jos\u00e9 Mar\u00eda Paz in 1830, uniting nine Argentine Provinces. The 1831 Federal Pact between Buenos Aires, Entre R\u00edos and Santa Fe Provinces opposed a military alliance to the League and ultimately defeated it during 1832, its former members joining the Federal Pact into a loose confederation of Provinces known as the Argentine Confederation. Although the Unitarians were exiled in neighboring countries, the Civil War continued for two decades.\nBuenos Aires Governor Juan Manuel de Rosas exerted a growing hegemony over the rest of the country during his 1835-1852 Government and resisted several Unitarian uprisings, but was finally defeated in 1852 by a coalition Army gathered by Entre R\u00edos Federalist Governor Justo Jos\u00e9 de Urquiza, who accused Rosas of not complying with Federal Pact provisions for a National Constitution. In 1853, a Federal Constitution was enacted (the current Constitution of Argentina, through amendments) and Urquiza was elected President of the Argentine Confederation. However, on the aftermath of 1852 Battle of Caseros, the Province of Buenos Aires had seceded from the Confederation. In 1859, after the Battle of Cepeda the State of Buenos Aires rejoined the Confederation, although it was granted the right to make some amendments to its Constitution. Finally, after the 1861 Battle of Pav\u00f3n, Buenos Aires took over the Confederation.\nThe following federal governments fought the weaker Federalist and Autonomist resistances in the countryside until the 1870s. The last Autonomist rebellion in Buenos Aires was quelled in 1880, leading to the federalization of Buenos Aires city and the stabilization of the Argentine State and government through the National Autonomist Party.\nNorth America.\nQuebec.\n\"Federalism\", in regard to the National Question, refers to support for Quebec remaining within Canada, while either keeping the \"status quo\" or pursuing greater autonomy and constitutional recognition of a Quebec nation, with corresponding rights and powers for Quebec within the Canadian federation. This ideology is opposed to Quebec sovereigntism, proponents of Quebec independence, most often (but not for all followers) along with an economic union with Canada similar to the European Union.\nUnited States.\nIn the United States the term \"federalist\" usually applies to a member of one of the following groups:\nThe Federalist Society for Law and Public Policy Studies is an organization of conservative and libertarian lawyers and others dedicated to debate of these principles.\nGlobal federalism.\nThe World Federalist Movement is a global citizens movement that advocates for strengthened and democratic world institutions subjected to the federalist principles of subsidiarity, solidarity and democracy. It states that \"[w]orld federalists support the creation of democratic global structures accountable to the citizens of the world and call for the division of international authority among separate agencies\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46836", "revid": "1739907", "url": "https://en.wikipedia.org/wiki?curid=46836", "title": "David Lloyd George", "text": "Prime Minister of the United Kingdom from 1916 to 1922\nDavid Lloyd George, 1st Earl Lloyd-George of Dwyfor (17 January 1863 \u2013 26 March 1945) was Prime Minister of the United Kingdom from 1916 to 1922. A Liberal Party politician from Wales, he was known for leading the United Kingdom during the First World War, for social-reform policies, for his role in the Paris Peace Conference, and for negotiating the establishment of the Irish Free State.\nBorn in Chorlton-on-Medlock, Manchester, and raised in Llanystumdwy, Lloyd George gained a reputation as an orator and proponent of a Welsh blend of radical Liberal ideas that included support for Welsh devolution, the disestablishment of the Church of England in Wales, equality for labourers and tenant farmers, and reform of land ownership. He won an 1890 by-election to become the Member of Parliament for Caernarvon Boroughs, and was continuously re-elected to the role for 55 years. He served in Henry Campbell-Bannerman's cabinet from 1905. After H. H. Asquith succeeded to the premiership in 1908, Lloyd George replaced him as Chancellor of the Exchequer. To fund extensive welfare reforms, he proposed taxes on land ownership and high incomes in the 1909 People's Budget, which the Conservative-dominated House of Lords rejected. The resulting constitutional crisis was only resolved after elections in 1910 and passage of the Parliament Act 1911. His budget was enacted in 1910, with the National Insurance Act 1911 and other measures helping to establish the modern welfare state. He was embroiled in the 1913 Marconi scandal but remained in office and secured the disestablishment of the Church of England in Wales.\nIn 1915, Lloyd George became Minister of Munitions and expanded artillery shell production for the war. In 1916, he was appointed Secretary of State for War but was frustrated by his limited power and clashes with Army commanders over strategy. Asquith proved ineffective as prime minister and was replaced by Lloyd George in December 1916. He centralised authority by creating a smaller war cabinet. To combat food shortages caused by u-boats, he implemented the convoy system, established rationing, and stimulated farming. After supporting the disastrous French Nivelle offensive in 1917, he had to reluctantly approve Field Marshal Douglas Haig's plans for the Battle of Passchendaele, which resulted in huge casualties with little strategic benefit. Against British military commanders, he was finally able to see the Allies brought under one command in March 1918. The war effort turned in the Allies' favour and was won in November. Following the December 1918 \"Coupon\" election, he and the Conservatives maintained their coalition with popular support.\nLloyd George was a leading participant at the Paris Peace Conference of 1919, but the situation in Ireland worsened, erupting into the Irish War of Independence, which lasted until Lloyd George negotiated independence for the Irish Free State in 1921. At home, he initiated education and housing reforms, but trade-union militancy rose to record levels, the economy became depressed in 1920 and unemployment rose; spending cuts followed in 1921\u201322, and in 1922 he became embroiled in a scandal over the sale of honours and the Chanak Crisis. The Carlton Club meeting decided the Conservatives should end the coalition and contest the next election alone. Lloyd George resigned as prime minister, but continued as the leader of a Liberal faction. After an awkward reunion with Asquith's faction in 1923, Lloyd George led the weak Liberal Party from 1926 to 1931. He proposed innovative schemes for public works and other reforms, but made only modest gains in the 1929 election. After 1931, he was a mistrusted figure heading a small rump of breakaway Liberals opposed to the National Government. In 1940, he refused to serve in Churchill's War Cabinet. He was elevated to the peerage in 1945 but died before he could take his seat in the House of Lords.\nEarly life.\nDavid George was born on 17 January 1863 in Chorlton-on-Medlock, Manchester, to Welsh parents William George and Elizabeth Lloyd George. William died in June 1864 of pneumonia, aged 44. David was just over one year old. Elizabeth George moved with her children to her native Llanystumdwy in Caernarfonshire, where she lived in a cottage known as Highgate with her brother Richard, a shoemaker, lay minister and a strong Liberal. Richard Lloyd was a towering influence on his nephew and David adopted his uncle's surname to become \"Lloyd George\". Lloyd George was educated at the local Anglican school, Llanystumdwy National School, and later under tutors.\nHe was raised with Welsh as his first language; Roy Jenkins, another Welsh politician, notes that, \"Lloyd George was Welsh, that his whole culture, his whole outlook, his language was Welsh.\"\nThough raised a devout evangelical Protestant, Lloyd George privately lost his religious faith as a young man. Biographer Don Cregier says he became \"a Deist and perhaps an agnostic, though he remained a chapel-goer and connoisseur of good preaching all his life.\" He was nevertheless, according to Frank Owen, \"one of the foremost fighting leaders of a fanatical Welsh Nonconformity\" for a quarter of a century.\nLegal practice and early politics.\nLloyd George qualified as a solicitor in 1884 after being articled to a firm in Porthmadog and taking Honours in his final law examination. He set up his own practice in the back parlour of his uncle's house in 1885. Although many prime ministers have been barristers, Lloyd George is, as of 2025, the only solicitor to have held that office.\nAs a solicitor, Lloyd George was politically active from the start, campaigning for his uncle's Liberal Party in the 1885 election. He was attracted by Joseph Chamberlain's \"unauthorised programme\" of Radical reform. After the election, Chamberlain split with William Ewart Gladstone in opposition to Irish Home Rule, and Lloyd George moved to join the Liberal Unionists. Uncertain of which wing to follow, he moved a resolution in support of Chamberlain at a local Liberal club and travelled to Birmingham to attend the first meeting of Chamberlain's new National Radical Union, but arrived a week too early. In 1907 Lloyd George would tell Herbert Lewis that he had thought Chamberlain's plan for a federal solution to the Home Rule Question correct in 1886 and still thought so, and that \"If Henry Richmond, Osborne Morgan and the Welsh members had stood by Chamberlain on an agreement as regards the [Welsh] disestablishment, they would have carried Wales with them\"\nHis legal practice quickly flourished; he established branch offices in surrounding towns and took his brother William into partnership in 1887. Lloyd George's legal and political triumph came in the Llanfrothen burial case, which established the right of Nonconformists to be buried according to denominational rites in parish burial grounds, as given by the Burial Laws Amendment Act 1880 but theretofore ignored by the Anglican clergy. On Lloyd George's advice, a Baptist burial party broke open a gate to a cemetery that had been locked against them by the vicar. The vicar sued them for trespass and although the jury returned a verdict for the party, the local judge misrecorded the jury's verdict and found in the vicar's favour. Suspecting bias, Lloyd George's clients won on appeal to the Divisional Court of Queen's Bench in London, where Lord Chief Justice Coleridge found in their favour. The case was hailed as a great victory throughout Wales and led to Lloyd George's adoption as the Liberal candidate for Carnarvon Boroughs on 27 December 1888. The same year, he and other young Welsh Liberals founded a monthly paper, \"Udgorn Rhyddid\" (Bugle of Freedom).\nIn 1889, Lloyd George became an alderman on Carnarvonshire County Council (a new body which had been created by the Local Government Act 1888) and would remain so for the rest of his life. Lloyd George would also serve the county as a Justice of the Peace (1910), chairman of Quarter Sessions (1929\u201338), and Deputy Lieutenant in 1921.\nMarriage.\nLloyd George married Margaret Owen, the daughter of a well-to-do local farming family, on 24 January 1888. They had five children, see later.\nEarly years as a member of Parliament (1890\u20131905).\nLloyd George's career as a member of parliament began when he was returned as a Liberal MP for Caernarfon Boroughs (now Caernarfon), narrowly winning the by-election on 10 April 1890, following the death of the Conservative member Edmund Swetenham. He would remain an MP for the same constituency until 1945, 55 years later. Lloyd George's early beginnings in Westminster may have proven difficult for him as a radical liberal and \"a great outsider\". Backbench members of the House of Commons were not paid at that time, so Lloyd George supported himself and his growing family by continuing to practise as a solicitor. He opened an office in London under the name of \"Lloyd George and Co.\" and continued in partnership with William George in Criccieth. In 1897, he merged his growing London practice with that of Arthur Rhys Roberts (who was to become Official Solicitor) under the name of \"Lloyd George, Roberts and Co.\"\nWelsh affairs.\nKenneth O. Morgan describes Lloyd George as a \"lifelong Welsh nationalist\" and suggests that between 1880 and 1914 he was \"the symbol and tribune of the national reawakening of Wales\", although he is also clear that from the early 1900s his main focus gradually shifted to UK-wide issues. He also became an associate of Tom Ellis, MP for Merioneth, having previously told a Caernarfon friend in 1888 that he was a \"Welsh Nationalist of the Ellis type\".\nDecentralisation and Welsh disestablishment.\nOne of Lloyd George's first acts as an MP was to organise an informal grouping of Welsh Liberal members with a programme that included; disestablishing and disendowing the Church of England in Wales, temperance reform, and establishing Welsh home rule. He was keen on decentralisation and thus Welsh devolution, starting with the devolution of the Church in Wales saying in 1890: \"I am deeply impressed with the fact that Wales has wants and inspirations of her own which have too long been ignored, but which must no longer be neglected. First and foremost amongst these stands the cause of Religious Liberty and Equality in Wales. If returned to Parliament by you, it shall be my earnest endeavour to labour for the triumph of this great cause. I believe in a liberal extension of the principle of Decentralization.\"\nDuring the next decade, Lloyd George campaigned in Parliament largely on Welsh issues, in particular for disestablishment and disendowment of the Church of England. When Gladstone retired in 1894 after the defeat of the second Home Rule Bill, the Welsh Liberal members chose him to serve on a deputation to William Harcourt to press for specific assurances on Welsh issues. When those assurances were not provided, they resolved to take independent action if the government did not bring a bill for disestablishment. When a bill was not forthcoming, he and three other Welsh Liberals (D. A. Thomas, Herbert Lewis and Frank Edwards) refused the whip on 14 April 1894, but accepted Lord Rosebery's assurance and rejoined the official Liberals on 29 May.\nCymru Fydd and Welsh devolution.\nHistorian Emyr Price referred to Lloyd George as \"the first architect of Welsh devolution and its most famous advocate\" as well as \"the pioneering advocate of a powerful parliament for the Welsh people\". Lloyd George himself stated in 1890, \"the Imperial Parliament is so overweighted with the concerns of a large empire that it cannot possibly devote the time and trouble necessary to legislate for the peculiar and domestic requirements of each and every separate province\". These statements would later be used to advocate for a Welsh assembly in the 1979 Welsh devolution referendum. Lloyd George felt that disestablishment, land reform and other forms of Welsh devolution could only be achieved if Wales formed its own government within a federal imperial system. In 1895, in a failed Church in Wales Bill, Lloyd George added an amendment in a discreet attempt at forming a sort of Welsh home rule, a national council for appointment of the Welsh Church commissioners. Although not condemned by Tom Ellis MP, this was to the annoyance of J. Bryn Roberts MP and the Home Secretary H. H. Asquith MP.\nHe was also a co-leader of , a national Welsh party with liberal values with the goals of promoting a \"stronger Welsh identity\" and establishing a Welsh government. He hoped that would become a force like the Irish National Party. He abandoned this idea after being criticised in Welsh newspapers for bringing about the defeat of the Liberal Party in the 1895 election. In an AGM meeting in Newport on 16 January 1896 of the South Wales Liberal Federation, led by D. A. Thomas, a proposal was made to unite the North and South Liberal Federations with to form The Welsh National Federation. This was a proposal which the North Wales Liberal Federation had already agreed to. However, the South Wales Liberal Federation rejected this. According to Lloyd George, he was shouted down by \"Newport Englishmen\" in the meeting, although the \"South Wales Argus\" suggested the poor crowd behaviour came from Lloyd George's supporters. Following difficulty in uniting the Liberal federations along with in the South East and thus, difficulty in gaining support for Home Rule for Wales, Lloyd George shifted his focus to improving the socio-economic environment of Wales as part of the United Kingdom and the British Empire. Although Lloyd George considered himself a \"Welshman first\", he saw the opportunities for Wales within the UK.\nUniting Welsh Liberals.\nIn 1898, Lloyd George created the Welsh National Liberal Council, a loose umbrella organisation covering the two federations, but with very little power. In time, it became known as the Liberal Party of Wales.\nSupport of Welsh institutions.\nLloyd George had a connection to or promoted the establishment of the National Library of Wales, the National Museum of Wales and the Welsh Department of the Board of Education. He also showed considerable support for the University of Wales, that its establishment raised the status of Welsh people and that the university deserved greater funding by the UK government.\nOpposition to the Boer War.\nLloyd George had been impressed by his journey to Canada in 1899. Although sometimes wrongly supposed\u2014both at the time and subsequently\u2014to be a Little Englander, he was not an opponent of the British Empire \"per se\", but in a speech at Birkenhead (21 November 1901) he stressed that it needed to be based on freedom, including for India, not \"racial arrogance\". Consequently, he gained national fame by displaying vehement opposition to the Second Boer War.\nFollowing Rosebery's lead, he based his attack firstly on what were supposed to be Britain's war aims\u2014remedying the grievances of the and in particular the claim that they were wrongly denied the right to vote, saying \"I do not believe the war has any connection with the franchise. It is a question of 45% dividends\" and that England (which did not then have universal male suffrage) was more in need of franchise reform than the Boer republics. A second attack came on the cost of the war, which, he argued, prevented overdue social reform in England, such as old-age pensions and workmen's cottages. As the fighting continued his attacks moved to its conduct by the generals, who, he said (basing his words on reports by William Burdett-Coutts in \"The Times\"), were not providing for the sick or wounded soldiers and were starving Boer women and children in concentration camps. But his major thrusts were reserved for the Chamberlains, accusing them of war profiteering through the family company Kynoch Ltd, of which Joseph Chamberlain's brother was chairman. The firm had won tenders to the War Office, though its prices were higher than some of its competitors. After speaking at a meeting in Birmingham Lloyd George had to be smuggled out disguised as a policeman, as his life was in danger from the mob. At this time the Liberal Party was badly split as H. H. Asquith, R. B. Haldane and others were supporters of the war and formed the Liberal Imperial League.\nOpposition to the Education Act 1902.\nOn 24 March Arthur Balfour, just about to take office as prime minister, introduced a bill which was to become the Education Act 1902. Lloyd George supported the bill's proposals to bring voluntary schools (i.e. religious schools\u2014mainly Church of England, and some Roman Catholic schools in certain inner city areas) in England and Wales under the control of local school boards, who would conduct inspections and appoint two out of each school's six managers. However, other measures were more contentious: the majority-religious school managers would retain the power to employ or sack teachers on religious grounds and would receive money from the rates (local property taxes). This offended nonconformist opinion, then in a period of revival, as it seemed like a return to the hated church rates (which had been compulsory until 1868), and inspired a large grassroots campaign against the bill.\nWithin days of the bill's unveiling (27 March), Lloyd George denounced \"priestcraft\" in a speech to his constituents, and he began an active campaign of speaking against the bill, both in public in Wales (with a few speeches in England) and in the House of Commons. On 12 November, Balfour accepted an amendment (willingly, but a rare case of him doing so), ostensibly from Alfred Thomas, chairman of the Welsh Parliamentary Liberal Party, but in reality instigated by Lloyd George, transferring control of Welsh schools from appointed boards to the elected county councils. The Education Act became law on 20 December 1902.\nLloyd George now announced the real purpose of the amendment, described as a \"booby trap\" by his biographer John Grigg. The Welsh National Liberal Council soon adopted his proposal that county councils should refuse funding unless repairs were carried out to schools (many were in a poor state), and should also demand control of school governing bodies and a ban on religious tests for teachers; \"no control, no cash\" was Lloyd George's slogan. Lloyd George negotiated with A. G. Edwards, Anglican Bishop of St Asaph, and was prepared to settle on an \"agreed religious syllabus\" or even to allow Anglican teaching in schools, provided the county councils retained control of teacher appointments, but this compromise failed after opposition from other Anglican Welsh bishops. A well-attended meeting at Park Hall Cardiff (3 June 1903) passed a number of resolutions by acclamation: county council control of schools, withholding money from schools or even withholding rates from unsupportive county councils. The Liberals soon gained control of all thirteen Welsh County Councils. Lloyd George continued to speak in England against the bill, but the campaign there was less aggressively led, taking the form of passive resistance to rate paying.\nIn August 1904 the government brought in the Education (Local Authority Default) Act giving the Board of Education power to take charge of schools, which Lloyd George immediately nicknamed the \"Coercion of Wales Act\". He addressed another convention in Cardiff on 6 October 1904, during which he proclaimed that the Welsh flag was \"a dragon rampant, not a sheep recumbent\". Under his leadership, the convention pledged not to maintain elementary schools, or to withdraw children from elementary schools altogether so that they could be taught privately by the nonconformist churches. In Travis Crosbie's words, public resistance to the Education Act had caused a \"perfect impasse\". There was no progress between Welsh counties and Westminster until 1905.\nHaving already gained national recognition for his anti-Boer War campaigns, Lloyd George's leadership of the attacks on the Education Act gave him a strong parliamentary reputation and marked him as a likely future cabinet member. The Act served to reunify the Liberals after their divisions over the Boer War and to increase Nonconformist influence in the party, which then included educational reform as policy in the 1906 election, which resulted in a Liberal landslide. All 34 Welsh seats returned a Liberal, except for one Labour seat in Merthyr Tydfil.\nOther stances.\nLloyd George also supported the romantic nationalist idea of Pan-Celtic unity and gave a speech at the 1904 Pan-Celtic Congress in Caernarfon.\nDuring his second-ever speech in the House of Commons, Lloyd George criticised the grandeur of the monarchy.\nLloyd George wrote extensively for Liberal-supporting papers such as the \"Manchester Guardian\" and spoke on Liberal issues (particularly temperance\u2014the \"local option\"\u2014and national as opposed to denominational education) throughout England and Wales.\nHe served as the legal adviser of Theodor Herzl in his negotiations with the British government regarding the Uganda Scheme, proposed as an alternative homeland for the Jews due to Turkish refusal to grant a charter for Jewish settlement in Palestine.\nPresident of the Board of Trade (1905\u20131908).\nIn 1905, Lloyd George entered the new Liberal Cabinet of Sir Henry Campbell-Bannerman as President of the Board of Trade.\nThe first priority on taking office was the repeal of the Education Act 1902. Lloyd George took the lead along with Augustine Birrell, President of the Board of Education. Lloyd George appears to have been the dominant figure on the committee drawing up the bill in its later stages and insisted that the bill create a separate education committee for Wales. Birrell complained privately that the bill, introduced in the Commons on 9 April 1906, owed more to Lloyd George and that he himself had had little say in its contents. The bill passed the House of Commons greatly amended but was completely mangled by the House of Lords. For the rest of the year Lloyd George made numerous public speeches attacking the House of Lords for mutilating the bill with wrecking amendments, in defiance of the Liberals' electoral mandate to reform the 1902 Act. Lloyd George was rebuked by King Edward VII for these speeches: the Prime Minister defended him to the King's secretary Francis Knollys, stating that his behaviour in Parliament was more constructive but that in speeches to the public \"the combative spirit seems to get the better of him\". No compromise was possible and the bill was abandoned, allowing the 1902 Act to continue in effect. As a result of Lloyd George's lobbying, a separate department for Wales was created within the Board of Education.\nNonconformists were bitterly upset by the failure of the Liberal Party to reform the 1902 Education Act, its most important promise to them, and over time their support for the Liberal Party slowly fell away.\nAt the Board of Trade Lloyd George introduced legislation on many topics, from merchant shipping and the Port of London to companies and railway regulation. His main achievement was in stopping a proposed national strike of the railway unions by brokering an agreement between the unions and the railway companies. While almost all the companies refused to recognise the unions, Lloyd George persuaded the companies to recognise elected representatives of the workers who sat with the company representatives on conciliation boards\u2014one for each company. If those boards failed to agree then an arbitrator would be called upon.\nChancellor of the Exchequer (1908\u20131915).\nOn Campbell-Bannerman's death, he succeeded Asquith, who had become prime minister, as Chancellor of the Exchequer from 1908 to 1915. While he continued some work from the Board of Trade\u2014for example, legislation to establish the Port of London Authority and to pursue traditional Liberal programmes such as licensing law reforms\u2014his first major trial in this role was over the 1909\u20131910 Naval Estimates. The Liberal manifesto at the 1906 general election included a commitment to reduce military expenditure. Lloyd George strongly supported this, writing to Reginald McKenna, First Lord of the Admiralty, of \"the emphatic pledges given by all of us at the last general election to reduce the gigantic expenditure on armaments built up by the recklessness of our predecessors.\" He then proposed the programme be reduced from six to four dreadnoughts. This was adopted by the government, but there was a public storm when the Conservatives, with covert support from the First Sea Lord, Admiral Jackie Fisher, campaigned for more with the slogan \"We want eight and we won't wait\". This resulted in Lloyd George's defeat in Cabinet and the adoption of estimates including provision for eight dreadnoughts. During this period he was also a target of protest by the women's suffrage movement, for he professed personal support for extension of the suffrage but did not move for changes within the Parliament process.\nPeople's Budget, 1909.\nIn 1909, Lloyd George introduced his People's Budget, imposing a 20% tax on the unearned increase in the value of land, payable at the death of the owner or sale of the land, and &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442\u00a0d. on undeveloped land and minerals, increased death duties, a rise in income tax, and the introduction of Supertax on income over \u00a33,000. There were taxes also on luxuries, alcohol and tobacco, so that money could be made available for the new welfare programmes as well as new battleships. The nation's landowners (well represented in the House of Lords) were intensely angry at the new taxes, mostly at the proposed very high tax on land values, but also because the instrumental redistribution of wealth could be used to detract from an argument for protective tariffs.\nThe immediate consequences included the end of the Liberal League, and Rosebery breaking friendship with the Liberal Party, which in itself was for Lloyd George a triumph. He had won the case of social reform without losing the debate on Free Trade. Arthur Balfour denounced the budget as \"vindictive, inequitable, based on no principles, and injurious to the productive capacity of the country.\" Roy Jenkins described it as the most reverberating since Gladstone's in 1860.\nIn the House of Commons, Lloyd George gave a brilliant account of the budget, which was attacked by the Conservatives. On the stump, notably at his Limehouse speech in 1909, he denounced the Conservatives and the wealthy classes with all his very considerable oratorical power. Excoriating the House of Lords in another speech, Lloyd George said, \"should 500 men, ordinary men, chosen accidentally from among the unemployed, override the judgement\u2014the deliberate judgement\u2014of millions of people who are engaged in the industry which makes the wealth of the country?\". In a break with convention, the budget was defeated by the Conservative majority in the House of Lords. The January and December elections of 1910 narrowly upheld the Liberal government. The 1909 budget was passed on 28 April 1910 by the Lords and received the Royal Assent on the 29th. Subsequently, the Parliament Act 1911 removed the House of Lords' power to block money bills, and with a few exceptions replaced their veto power over most bills with a power to delay them for up to two years.\nAlthough old-age pensions had already been introduced by Asquith as Chancellor, Lloyd George was largely responsible for the introduction of state financial support for the sick and infirm (known colloquially as \"going on the Lloyd George\" for decades afterwards)\u2014legislation referred to as the Liberal Reforms. Lloyd George also succeeded in putting through Parliament his National Insurance Act 1911, making provision for sickness and invalidism, and a system of unemployment insurance. He was helped in his endeavours by forty or so backbenchers who regularly pushed for new social measures, often voted with Labour MPs. These social reforms in Britain were the beginnings of a welfare state and fulfilled the aim of dampening down the demands of the growing working class for rather more radical solutions to their impoverishment.\nUnder his leadership, after 1909 the Liberals extended minimum wages to farmworkers.\nMansion House Speech, 1911.\nLloyd George was an opponent of warfare but he paid little attention to foreign affairs until the Agadir Crisis of 1911. After consulting Edward Grey (the foreign minister) and H.H. Asquith (the prime minister) he gave a stirring and patriotic speech at Mansion House on 21 July 1911, during that year's annual white tie dinner at the official residence of the Lord Mayor of London, where the Chancellor of the Exchequer delivers a speech known as the \"Mansion House Speech\". He stated: But if a situation were to be forced upon us in which peace could only be preserved by the surrender of the great and beneficent position Britain has won by centuries of heroism and achievement, by allowing Britain to be treated where her interests were vitally affected as if she were of no account in the Cabinet of nations, then I say emphatically that peace at that price would be a humiliation intolerable for a great country like ours to endure. National honour is no party question. The security of our great international trade is no party question. \nHe was warning both France and Germany, but the public response cheered solidarity with France and hostility toward Germany. Berlin was outraged, blaming Lloyd George for doing \"untold harm both with regard to German public opinion and the negotiations.\" Count Metternich, Germany's ambassador in London, said, \"Mr Lloyd George's speech came upon us like a thunderbolt\".\nMarconi scandal 1913.\nIn 1913, Lloyd George, along with Rufus Isaacs, the Attorney General, was involved in the Marconi scandal. Accused of speculating in Marconi shares on the inside information that they were about to be awarded a key government contract (which would have caused them to increase in value), he told the House of Commons that he had not speculated in the shares of \"that company\". He had in fact bought shares in the American Marconi Company.\nWelsh disestablishment.\nLloyd George was instrumental in fulfilling a long-standing aspiration to disestablish the Anglican Church of Wales. As with Irish Home Rule, previous attempts to enact this had failed in the 1892\u20131895 Governments, and were now made possible by the removal of the Lords' veto in 1911, and as with Home Rule the initial bill (1912) was delayed for two years by the Lords, becoming law in 1914, only to be suspended for the duration of the war. After the Welsh Church (Temporalities) Act 1919 was passed, Welsh Disestablishment finally came into force in 1920. This Act also removed the right of the six Welsh Bishops in the new Church in Wales to sit in the House of Lords and removed (disendowed) certain pre-1662 property rights.\nFirst World War.\nLloyd George was as surprised as almost everyone else by the outbreak of the First World War. On 23 July 1914, almost a month after the assassination of Archduke Franz Ferdinand and on the eve of the Austro-Hungarian ultimatum to Serbia, he made a speech advocating \"economy\" in the House of Commons, saying that Britain's relations with Germany were better than for many years. On 27 July he told C. P. Scott of the \"Manchester Guardian\" that Britain would keep out of the impending war. With the Cabinet divided, and most ministers reluctant for Britain to get involved, he struck Asquith as \"statesmanlike\" at the Cabinet meeting on 1 August, favouring keeping Britain's options open. The next day he seemed likely to resign if Britain intervened, but he held back at Cabinet on Monday 3 August, moved by the news that Belgium would resist Germany's demand of passage for her army across her soil. He was seen as a key figure whose stance helped to persuade almost the entire Cabinet to support British intervention. He was able to give the more pacifist members of the cabinet and the Liberal Party a principle\u2014the rights of small nations\u2014which meant they could support the war and maintain united political and popular support.\nLloyd George remained in office as Chancellor of the Exchequer for the first year of the Great War. The budget of 17 November 1914 had to allow for lower taxation receipts because of the reduction in world trade. The Crimean and Boer Wars had largely been paid for out of taxation, but Lloyd George raised debt financing of \u00a3321\u00a0million. Large (but deferred) increases in Supertax and income tax rates were accompanied by increases in excise duties, and the budget produced a tax increase of \u00a363\u00a0million in a full year. His last budget, on 4 May 1915, showed a growing concern for the effects of alcohol on the war effort, with large increases in duties, and a scheme of state control of alcohol sales in specified areas. The excise proposals were opposed by the Irish Parliamentary Party and the Conservatives, and were abandoned.\nMinister of Munitions.\nLloyd George gained a heroic reputation with his energetic work as Minister of Munitions in 1915 and 1916, setting the stage for his move up to the height of power. After a long struggle with the War Office, he wrested responsibility for arms production away from the generals, making it a purely industrial department, with considerable expert assistance from Walter Runciman. The two men gained the respect of Liberal cabinet colleagues for improving administrative capabilities, and increasing outputs.\nWhen the Shell Crisis of 1915 dismayed public opinion with the news that the Army was running short of artillery shells, demands rose for a strong leader to take charge of munitions. In the first coalition ministry, formed in May 1915, Lloyd George was made Minister of Munitions, heading a new department. In this position, he won great acclaim, which formed the basis for his political ascent. All historians agree that he boosted national morale and focussed attention on the urgent need for greater output, but many also say the increase in munitions output in 1915\u201316 was due largely to reforms already underway, though not yet effective before he had even arrived. The Ministry broke through the cumbersome bureaucracy of the War Office, resolved labour problems, rationalised the supply system and dramatically increased production. Within a year it became the largest buyer, seller and employer in Britain.\nLloyd George was not at all satisfied with the progress of the war. He wanted to \"knock away the props\", by attacking Germany's allies\u2014from early in 1915 he argued for the sending of British troops to the Balkans to assist Serbia and bring Greece and other Balkan countries onto the side of the Allies (this was eventually done\u2014the Salonika expedition\u2014although not on the scale that Lloyd George had wanted, and mountain ranges made his suggestions of grand Balkan offensives impractical); in 1916, he wanted to send machine guns to Romania (insufficient amounts were available for this to be feasible). These suggestions began a period of poor relations with the Chief of the Imperial General Staff, General William Robertson, who was \"brusque to the point of rudeness\" and \"barely concealed his contempt for Lloyd George's military opinions\", to which he was in the habit of retorting \"I've 'eard different\".\nLloyd George persuaded Lord Kitchener, the Secretary of State for War, to raise a Welsh Division, and, despite Kitchener's threat of resignation, to recognise nonconformist chaplains in the Army.\nLate in 1915, Lloyd George became a strong supporter of general conscription, an issue that divided Liberals, and helped the passage of several conscription acts from January 1916 onwards. In spring 1916 Alfred Milner hoped Lloyd George could be persuaded to bring down the coalition government by resigning, but this did not happen.\nSecretary of State for War.\nIn June 1916 Lloyd George succeeded Lord Kitchener (who died when the ship HMS \"Hampshire\" was sunk taking him on a mission to Russia) as Secretary of State for War, although he had little control over strategy, as General Robertson had been given direct right of access to the Cabinet so as to bypass Kitchener. He did succeed in securing the appointment of Sir Eric Geddes to take charge of military railways behind British lines in France, with the honorary rank of major-general. Lloyd George told a journalist, Roy W. Howard, in late September that \"the fight must be to a finish\u2014to a knockout\", a rejection of President Woodrow Wilson's offer to mediate.\nLloyd George was increasingly frustrated at the limited gains of the Somme offensive, criticising General Douglas Haig to Ferdinand Foch on a visit to the Western Front in September (British casualty ratios were worse than those of the French, who were more experienced and had more artillery), proposing sending Robertson on a mission to Russia (he refused to go), and demanding that more troops be sent to Salonika to help Romania. Robertson eventually threatened to resign.\nMuch of the press still argued that the professional leadership of Haig and Robertson was preferable to civilian interference that had led to disasters like Gallipoli and Kut. Lord Northcliffe, owner of \"The Times\", stormed into Lloyd George's office and, finding him unavailable, told his secretary \"You can tell him that I hear he has been interfering with Strategy and that if he goes on I will break him\", and the same day (11 October) Lloyd George also received a warning letter from H. A. Gwynne, editor of the \"Morning Post\". He was obliged to give his \"word of honour\" to Asquith that he had complete confidence in Haig and Robertson and thought them irreplaceable, but he wrote to Robertson wanting to know how their differences had been leaked to the press (affecting to believe that Robertson had not personally \"authorised such a breach of confidence &amp; discipline\"). He asserted his right to express his opinions about strategy in November, by which time ministers had taken to holding meetings to which Robertson was not invited.\nThe weakness of Asquith as a planner and organiser was increasingly apparent to senior officials. After Asquith had refused, then agreed to, and then refused again Lloyd George's demand to be allowed to chair a small committee to manage the war, he resigned in December 1916. Grey was among leading Asquithians who had identified Lloyd George's intentions the previous month. Lloyd George became prime minister, with the nation demanding he take vigorous charge of the war.\nAlthough during the political crisis Robertson had advised Lloyd George to \"stick to it\" and form a small War Council, Lloyd George had planned if necessary to appeal to the country. His Military Secretary Colonel Arthur Lee prepared a memo blaming Robertson and the General Staff for the loss of Serbia and Romania. Lloyd George was restricted by his promise to the Unionists to keep Haig as Commander-in-Chief and the press support for the generals, although Milner and Curzon were also sympathetic to campaigns to increase British power in the Middle East. After Germany's offer (12 December 1916) of a negotiated peace, Lloyd George rebuffed President Wilson's request for the belligerents to state their war aims by demanding terms tantamount to German defeat.\nPrime Minister (1916\u20131922).\nLloyd George's tenure as wartime prime minister from 1916 to 1918, remains a subject of intense historical scrutiny, primarily due to its military success and his restructuring of the office along presidential lines. His ascent to the premiership marked a significant departure from traditional norms, as he was the first Welshman to hold the office. Once in office his leadership style diverged markedly from his predecessors, characterized by a more dynamic and interventionist approach to governance. Lloyd George relied on his political background: rooted in Liberalism, advocating for social reforms and challenging the established aristocratic order, he had made his mark through his persuasive oratory and political acumen.\nWar leader (1916\u20131918).\nForming a government.\nThe fall of Asquith as prime minister split the Liberal Party into two factions: those who supported him and those who supported the coalition government. In his \"War Memoirs\", Lloyd George compared himself with Asquith:\nThere are certain indispensable qualities essential to the Chief Minister of the Crown in a great war.\u00a0... Such a minister must have courage, composure, and judgment. All this Mr. Asquith possessed in a superlative degree.\u00a0... But a war minister must also have vision, imagination and initiative\u2014he must show untiring assiduity, must exercise constant oversight and supervision of every sphere of war activity, must possess driving force to energize this activity, must be in continuous consultation with experts, official and unofficial, as to the best means of using the resources of the country in conjunction with the Allies for the achievement of victory. If to this can be added a flair for conducting a great fight, then you have an ideal War Minister.\nAfter December 1916 Lloyd George relied on the support of Conservatives and of the press baron Lord Northcliffe (who owned both \"The Times\" and the \"Daily Mail\"). Besides the Prime Minister, the five-member War Cabinet contained three Conservatives (Lord President of the Council and Leader of the House of Lords Lord Curzon, Chancellor of the Exchequer and Leader of the House of Commons Bonar Law, and Minister without Portfolio Lord Milner) and Arthur Henderson, unofficially representing Labour. Edward Carson was appointed First Lord of the Admiralty, as had been widely touted during the intrigues of the previous month, but excluded from the War Cabinet. Amongst the few Liberal frontbenchers to support Lloyd George were Christopher Addison (who had played an important role in drumming up some backbench Liberal support for Lloyd George), H. A. L. Fisher, Lord Rhondda and Sir Albert Stanley. Edwin Montagu and Churchill joined the government in the summer of 1917.\nLloyd George's Secretariat, popularly known as Downing Street's \"Garden Suburb\", assisted him in discharging his responsibilities within the constraints of the war cabinet system. Its function was to maintain contact with the numerous departments of government, to collect information, and to report on matters of special concern. Its leading members were George Adams and Philip Kerr, and the other secretaries included David Davies, Joseph Davies, Waldorf Astor and, later, Cecil Harmsworth.\nLloyd George wanted to make the destruction of the Ottoman Empire a major British war aim, and two days after taking office told Robertson that he wanted a major victory, preferably the capture of Jerusalem, to impress British public opinion.\nAt the Rome Conference (5\u20136 January 1917) Lloyd George was discreetly quiet about plans to take Jerusalem, an object which advanced British interests rather than doing much to win the war. Lloyd George proposed sending heavy guns to Italy with a view to defeating Austria-Hungary, possibly to be balanced by a transfer of Italian troops to Salonika but was unable to obtain the support of the French or Italians, and Robertson talked of resigning.\nNivelle affair.\nLloyd George engaged almost constantly in intrigues calculated to reduce the power of the generals, including trying to subordinate British forces in France to the French General Robert Nivelle. He backed Nivelle because he thought he had \"proved himself to be a Man\" by his successful counterattacks at Verdun, and because of his promises that he could break the German lines in 48 hours. Nivelle increasingly complained of Haig's dragging his feet rather than cooperating with their plans for the offensive.\nThe plan was to put British forces under Nivelle's direct command for the great 1917 offensive. The British would attack first, thereby tying down the German reserves. Then the French would strike and score an overwhelming victory in two days. It was announced at a War Cabinet meeting on 24 February, to which neither Robertson nor Lord Derby (Secretary of State for War) had been invited. Ministers felt that the French generals and staff had shown themselves more skilful than the British in 1916, whilst politically Britain had to give wholehearted support to what would probably be the last major French effort of the war. The Nivelle proposal was then given to Robertson and Haig without warning on 26\u201327 February at the Calais Conference (minutes from the War Cabinet meeting were not sent to the King until 28 February, so that he did not have a prior chance to object). Robertson in particular protested vehemently. Finally, a compromise was reached whereby Haig would be under Nivelle's orders but would retain operational control of British forces and keep a right of appeal to London \"if he saw good reason\". After further argument the \"status quo\", that Haig was an ally of the French but was expected to defer to their wishes, was largely restored in mid-March.\nThe British attack at the Battle of Arras (9\u201314 April 1917) was partly successful but with much higher casualties than the Germans suffered. There had been many delays and the Germans, suspecting an attack, had shortened their lines to the strong Hindenburg Line. The French attack on the Aisne River in mid-April gained some tactically important high ground but failed to achieve the promised decisive breakthrough, pushing the French Army to the point of mutiny. While Haig gained prestige, Lloyd George lost credibility, and the affair further poisoned relations between himself and the \"Brasshats\".\nU-boat war.\nShipping.\nIn early 1917 the Germans had resumed unrestricted submarine warfare in a bid to achieve victory on the Western Approaches. Lloyd George set up a Ministry of Shipping under Sir Joseph Maclay, a Glasgow shipowner who was not, until after he left office, a member of either House of Parliament, and housed in a wooden building in a specially drained lake in St James's Park, within a few minutes' walk from the Admiralty. The Junior Minister and House of Commons spokesman was Leo Chiozza Money, with whom Maclay did not get on, but on whose appointment Lloyd George insisted, feeling that their qualities would complement one another. The Civil Service staff was headed by the highly able John Anderson (then only thirty-four years old) and included Arthur Salter. A number of shipping magnates were persuaded, like Maclay himself, to work unpaid for the ministry (as had a number of industrialists for the Ministry of Munitions), who were also able to obtain ideas privately from junior naval officers who were reluctant to argue with their superiors in meetings. The ministers heading the Board of Trade, for Munitions (Addison) and for Agriculture and Food (Lord Rhondda), were also expected to co-operate with Maclay.\nIn accordance with a pledge Lloyd George had given in December 1916 nearly 90% of Britain's merchant shipping tonnage was soon brought under state control (previously less than half had been controlled by the Admiralty), whilst remaining privately owned (similar measures were in force at the time for the railways). Merchant shipping was concentrated, largely on Chiozza Money's initiative, on the transatlantic route where it could more easily be protected, instead of being spread out all over the globe (this relied on imports coming first into North America). Maclay began the process of increasing ship construction, although he was hampered by shortages of steel and labour, and ships under construction in the United States were confiscated by the Americans when she entered the war. In May 1917 Eric Geddes, based at the Admiralty, was put in charge of shipbuilding, and in July he became First Lord of the Admiralty. Later the German U-boats were defeated in 1918.\nConvoys.\nLloyd George had raised the matter of convoys at the War Committee in November 1916, only to be told by the admirals present, including John Jellicoe, that convoys presented too large a target, and that merchant ship masters lacked the discipline to keep station in a convoy.\nIn February 1917 Maurice Hankey, the secretary of the War Cabinet, wrote a memorandum for Lloyd George calling for the introduction of \"scientifically organised convoys\", almost certainly after being persuaded by Commander Reginald Henderson and the Shipping Ministry officials with whom he was in contact. After a breakfast meeting (13 February 1917) with Lloyd George, Sir Edward Carson (First Lord of the Admiralty) and Admirals Jellicoe and Alexander Duff agreed to \"conduct experiments\"; however, convoys were not in general use until August, by which time the rate of shipping losses was already in decline after peaking in April.\nLloyd George later claimed in his \"War Memoirs\" that the delay in introducing convoys was because the Admiralty mishandled an experimental convoy between Britain and Norway and because Jellicoe obtained, behind Maclay's back, an unrepresentative sample of merchant skippers claiming that they lacked the skill to \"keep station\" in convoy. In fact, Hankey's diary shows that Lloyd George's interest in the matter was intermittent, whilst Frances Stevenson's diaries contain no mention of the topic. He may well have been reluctant, especially at a time when his relations with the generals were so poor, for a showdown with Carson, a weak administrator who was as much the mouthpiece of the admirals as Derby was of the generals, but who had played a key role in the fall of Asquith and who led a significant bloc of Conservative and Irish Unionist MPs.\nThe new Commander of the Grand Fleet Admiral David Beatty, whom Lloyd George visited at Invergordon on 15 April, was a supporter of convoys, as was the American Admiral William Sims (the USA had just entered the war). The War Cabinet on 25 April authorised Lloyd George to look into the anti-submarine campaign, and on 30 April he visited the Admiralty. Duff had already recommended to Jellicoe that the Admiralty adopt convoys after a recent successful convoy from Gibraltar.\nMost of the organisations Lloyd George created during the First World War were replicated with the outbreak of the Second World War. As Lord Beaverbrook wrote, \"There were no road signs on the journey he had to undertake.\" The latter's \"personal\" efforts to promote convoys were less consistent than he (and Churchill in \"The World Crisis\" and Beaverbrook in \"Men and Power\") later claimed; the idea that he, after a hard struggle, sat in the First Lord's chair (on his 30 April visit to the Admiralty) and imposed convoys on a hostile Board is a myth; however, in Grigg's view the credit goes largely to men and institutions which he set in place, and with a freer hand, and making fewer mistakes, than in his dealings with the generals, he and his appointees took decisions which can reasonably be said to have saved the country. \"It was a close-run thing\u00a0... failure would have been catastrophic.\"\nRussian Revolution.\nLloyd George welcomed the Fall of the Tsar, both in a private letter to his brother and in a message to the new Russian Prime Minister Prince Georgy Lvov, not least as the war could now be portrayed as a clash between liberal governments and the autocratic Central Powers. Like many observers, he had been taken by surprise by the exact timing of the revolution (it had not been predicted by Lord Milner or General Henry Hughes Wilson on their visit to Russia a few weeks earlier) and hoped\u2014albeit with some concerns\u2014that Russia's war effort would be invigorated like that of France in the early 1790s.\nLloyd George gave a cautious welcome to the suggestion (19 March on the western calendar) by the Russian Foreign Minister Pavel Milyukov that the toppled Tsar and his family be given sanctuary in Britain (although Lloyd George would have preferred that they go to a neutral country). From the very start, the King's adviser Lord Stamfordham raised objections, and in April the British government withdrew its consent under royal pressure. Eventually, the Russian royal family were moved to the Urals where they were executed in 1918. Lloyd George was often blamed for the refusal of asylum, and in his \"War Memoirs\" he did not mention King George V's role in the matter, which was not explicitly confirmed until Kenneth Rose's biography of the King was published in 1983.\nImperial War Cabinet.\nAn Imperial War Cabinet, including representatives from Canada, Newfoundland, Australia, New Zealand, South Africa and India, met 14 times from 20 March 1917 to 2 May 1917 (a crisis period of the war) and twice in 1918. The idea was not entirely without precedent as there had been Imperial Conferences in 1887, 1894, 1897, 1902, 1907 and 1911, whilst the Australian Prime Minister Billy Hughes had been invited to attend the Cabinet and War Committee on his visit to the UK in the spring of 1916. The South African Jan Smuts was appointed to the British War Cabinet in the early summer of 1917.\nPasschendaele.\nLloyd George set up a War Policy Committee (himself, Curzon, Milner, Law and Smuts, with Maurice Hankey as secretary) to discuss strategy, which held 16 meetings over the next six weeks. At the very first meeting (11 June) Lloyd George proposed helping the Italians to capture Trieste, explicitly telling the War Policy Committee (21 June 1917) that he wanted Italian soldiers to be killed rather than British.\nHaig believed that a Flanders offensive had a good chance of clearing the Belgian coast, from which German submarines and destroyers were operating (a popular goal with politicians), and that victory at Ypres \"might quite possibly lead to (German) collapse\". Robertson was less optimistic, but preferred Britain to keep her focus on defeating Germany on the Western Front, and had told Haig that the politicians would not \"dare\" overrule both soldiers if they gave the same advice. Haig promised he had no \"intention of entering into a tremendous offensive involving heavy losses\" (20 June) whilst Robertson wanted to avoid \"disproportionate loss\" (23 June).\nThe Flanders offensive was reluctantly sanctioned by the War Policy Committee on 18 July and the War Cabinet two days later, on condition it did not degenerate into a long drawn-out fight like the Somme. The War Cabinet promised to monitor progress and casualties and, if necessary call a halt, although in the event they made little effort to monitor progress until September. Frustrated at his inability to get his way, Lloyd George talked of resigning and taking his case to the public.\nThe Battle of Passchendaele began on 31 July, but soon became bogged down in unseasonably early wet weather, which turned much of the battlefield into a barely passable swamp in which men and animals sometimes drowned, whilst the mud and rain severely reduced the accuracy and effectiveness of artillery, the dominant weapon of the time. Lloyd George tried to enlist the King for diverting efforts against Austria-Hungary, telling Stamfordham (14 August) that the King and Prime Minister were \"joint trustees of the nation\" who had to avoid waste of manpower. A new Italian offensive began (18 August), but Robertson advised that it was \"false strategy\" to call off Passchendaele to send reinforcements to Italy, and despite being summoned to George Riddell's home in Sussex, where he was served apple pudding (his favourite dish), agreed only reluctantly. The Anglo-French leadership agreed in early September to send 100 heavy guns to Italy (50 of them French) rather than the 300 which Lloyd George wanted\u2014Lloyd George talked of ordering a halt to Passchendaele, but in Hankey's words \"funked it\" (4 September). Had he not done so his government might have fallen, for as soon as the guns reached Italy Luigi Cadorna called off his offensive (21 September).\nLloyd George attended the National Eisteddfod of Wales in Birkenhead on 6 September 1917 (the first UK prime minister to do so for 44 years). He attended the chairing of the Bard ceremony, where the chair was awarded to the poet Hedd Wyn ('Blessed Peace' in Welsh). It was announced that the poet had been killed in the Battle of Passchendaele six weeks earlier. The chair was draped with a black sheet, giving the Eisteddfod it's alternative name of the Eisteddfod of the Black Chair. \nAt a meeting at Boulogne on the 25th of September, Lloyd George broached with Paul Painlev\u00e9 the setting up of an Allied Supreme War Council then making Ferdinand Foch generalissimo. Law had written to Lloyd George that ministers must soon decide whether or not the offensive was to continue. Lloyd George and Robertson met Haig in France (26 September) to discuss the recent German peace feelers (which in the end were publicly repudiated by Chancellor Georg Michaelis) and the progress of the offensive. Haig preferred to continue, encouraged by Herbert Plumer's recent successful attacks in dry weather at Menin Road (20 September) and Polygon Wood (26 September), and stating that the Germans were \"very worn out\". In October the wet weather returned for the final attack towards Passchendaele. At the final meeting of the War Policy Committee on 11 October 1917, Lloyd George authorised the offensive to continue, but warning of failure in three weeks' time. Hankey (21 October) claimed in his diary that Lloyd George had deliberately allowed Passchendaele to continue to discredit Haig and Robertson and make it easier for him to forbid similar offensives in 1918.\nSupreme War Council.\nThe Italians suffered a disastrous defeat at Caporetto, requiring British and French reinforcements to be sent. Lloyd George said he \"wanted to take advantage of Caporetto to gain \"control of the War\". The Supreme War Council was inaugurated at the Rapallo Conference (6\u20137 November 1917). Lloyd George then gave a controversial speech in Paris (12 November) at which he criticised the high casualties of recent Allied \"victories\" (a word which he used with an element of sarcasm). These events led to an angry Commons debate (19 November), which Lloyd George survived.\nIn reply to Robertson's 19 November memo, which warned (correctly) that the Germans would use the opportunity of Russia's departure from the war to attack in 1918 before the Americans were present in strength, Lloyd George wrote (wrongly) that the Germans would not attack and would fail if they did. That autumn he declared that he was willing \"to risk his whole political reputation\" to avoid a repetition of the Somme or Passchendaele.\nIn December 1917 Lloyd George remarked to C. P. Scott that: \"If people really knew, the war would be stopped tomorrow. But of course, they don't know, and can't know.\"\nManpower crisis and the unions.\nA Manpower Committee was set up on 6 December 1917, consisting of the Prime Minister, Curzon, Carson, George Barnes and Smuts with Maurice Hankey as secretary, and Auckland Geddes (Minister of National Service\u2014in charge of Army recruitment) in regular attendance.\nThe first meeting of the Manpower Committee was on 10 December, and it met twice the next day and again on 15 December. Lloyd George questioned Generals Nevil Macready (Adjutant-General) and George Macdonogh (Chief of Military Intelligence), who advised that the Allied superiority of numbers on the Western Front would not survive the transfer of German reinforcements from the East now that Russia was dropping out of the war. Deeply concerned about the publicity attracted by the recent Lansdowne letter's mention of casualties, he suggested removing Haig and Robertson from office at this time, but this was met by a threat of resignation from Lord Derby. At this stage Lloyd George opposed extending conscription to Ireland\u2014Carson advised that extending conscription to Ulster alone would be impractical.\nWhen Hankey's report eventually emerged it reflected Lloyd George's wishes: it gave top priority to shipbuilding and merchant shipping (not least to ship US troops to Europe), and placed Army manpower below both weapons production and civilian industry. The size of the Army in Britain was to be reduced from eight divisions to four, freeing about 40,000 men for service in France. In the House of Commons (20 December) Lloyd George also argued that the collapse of Russia and defeat of Italy required further \"combing-out\" of men from industry, in breach of pledges given to the trade unions in 1916. Auckland Geddes was given increased powers to direct labour\u2014a new bill became law, despite the opposition of the Amalgamated Society of Engineers, in February 1918.\nWar goals.\nLloyd George outlined Allied war aims at a conference at Caxton Hall on 5 January 1918. Addressing an audience of trade unionists, he called for Germany to be stripped of her conquests (including her colonies, and Alsace-Lorraine, annexed in 1871) and democratised (although he was clear that this was not an Allied war aim, something which would help to ensure the future peace of Europe), and for the liberation of the subject peoples of Austria-Hungary and the Ottoman Empire. He also hinted at reparations (although it was suggested that these would not be on the scale imposed on France after 1871) and a new international order. Lloyd George explained to critics that he was hoping to detach Austria-Hungary and turn the German people against her rulers; the speech greatly increased his support amongst trade unions and the Labour Party. President Wilson at first considered abandoning his speech outlining US war aims\u2014the \"Fourteen Points\", many of which were similar to the aims outlined by Lloyd George\u2014but was persuaded by his adviser Colonel House to deliver it. Wilson's speech (8 January) overshadowed Lloyd George's and is better remembered by posterity.\nStrategic priorities.\nLloyd George told Edmund Allenby, who was appointed the new commander in Egypt in June, that his objective was \"Jerusalem before Christmas.\" Amidst months of argument throughout the autumn of 1917 Robertson was able to block Lloyd George's plan to make Palestine the main theatre of operations by having Allenby make the impossible demand that thirteen extra divisions be sent to him. Allenby captured Jerusalem in December 1917.\nIn the winter of 1917\u201318, Lloyd George secured the resignations of both the service chiefs. Removing the First Sea Lord Admiral Jellicoe earlier in 1917, as Lloyd George wanted, would have been politically impossible given Conservative anger at the return of Churchill (still blamed for the Dardanelles) to office as Minister of Munitions in July, and Lloyd George's preoccupations with Passchendaele, Caporetto and the Supreme War Council from July onward. By December it was clear that Lloyd George would have to sack Jellicoe or lose Eric Geddes (First Lord of the Admiralty), who wanted to return to his previous job in charge of military transport in France. The Christmas holiday, when Parliament was not sitting, provided a good opportunity. Before Jellicoe left for leave on Christmas Eve he received a letter from Geddes demanding his resignation. The other Sea Lords talked of resigning but did not do so, whilst Jellicoe's ally Carson remained a member of the War Cabinet until he resigned in January over Irish Home Rule.\nRelations with General Robertson had worsened further over the creation of the Supreme War Council at Versailles and he was eventually forced out over his insistence that the British delegate there be subordinate to Robertson as CIGS in London.\nBalfour Declaration.\nAs Prime Minister Lloyd George played a pivotal role in the establishment of a Jewish National Home in Palestine. His government issued the Balfour Declaration in 1917 announcing an official British commitment to support an eventual Jewish homeland in Palestine. Lloyd George led the war effort against the Ottomans, whose empire was broken up and taken over in large part by Britain and France. Britain took over Palestine. He presided over the Imperial Cabinet when it endorsed this policy, and he secured the backing of Britain's allies, especially the United States and France. He was also the principal delegate at the San Remo conference in 1920, where the League of Nations designed the Mandate for Palestine and conferred it upon Britain. Zionist leader Chaim Weizmann said Lloyd George initiated the Balfour Declaration and followed the development of the Zionist movement and the upbuilding of Palestine with keen interest in every stage.\nCrises of 1918.\nIn rapid succession in spring 1918 came a series of military and political crises. The Germans, having moved troops from the Eastern Front and retrained them in new tactics, now had more soldiers on the Western Front than the Allies. Germany launched the full-scale spring offensive starting on 21 March against the British and French lines, hoping for victory on the battlefield before the American troops arrived in numbers. The Allied armies fell back 40 miles in confusion, and, facing defeat, London realised it needed more troops to fight a mobile war. Lloyd George found half a million soldiers and rushed them to France, asked American President Wilson for immediate help, and agreed to the appointment of French General Foch as commander in chief on the Western Front. He considered taking on the role of War Minister himself, but was dissuaded by the King, and instead appointed Lord Milner.\nDespite strong warnings that it was a bad idea, the War Cabinet decided to impose conscription on Ireland. The main reason was that trade unions in Britain demanded it as the price for cutting back on conscription exemptions for certain workers. Labour wanted the principle established that no one was exempt, but it did not demand that conscription actually take place in Ireland. The proposal was enacted but never enforced. The Catholic bishops for the first time entered the fray and called for open resistance to conscription. Many Irish Catholics and nationalists moved into Sinn F\u00e9in, a decisive moment marking the dominance of Irish politics by a party committed to leaving the UK altogether.\nAt one point Lloyd George unknowingly misled the House of Commons in claiming that Haig's forces were stronger at the start of 1918 than they had been a year earlier\u2014in fact, the increase was in the number of labourers, most of them Chinese, Indians and black South Africans, and Haig had fewer infantry, holding a longer stretch of front. The Prime Minister had used incorrect information furnished by the War Department office headed by Major-General Sir Frederick Maurice. Maurice then made the spectacular public allegation that the War Cabinet had deliberately held soldiers back from the Western Front, and both Lloyd George and Law had lied to Parliament about it. Instead of going to the Prime Minister about the problem Maurice had waited and then broke King's Regulations by making a public attack. Asquith, still the Liberal Party leader, took up the allegations and called for a Parliamentary Inquiry. While Asquith's presentation was poorly done, Lloyd George vigorously defended his position, treating the debate as a vote of confidence. He won over the House with a powerful refutation of Maurice's allegations. The Liberal Party was openly split for the first time.\nMeanwhile, the German offensive stalled. By summer the Americans were sending 10,000 fresh men a day to the Western Front, a speedup made possible by leaving their equipment behind and using British and French munitions. The German army had used up its last reserves and was steadily shrinking in numbers, further weakening its resolve. The Allies launched an offensive that led to a series of successful battles and victory came on 11 November 1918.\nThat autumn Lloyd George was one of the many infected during the 1918 flu pandemic, but he survived.\nPostwar prime minister (1918\u20131922).\nAt the end of the war Lloyd George's reputation stood at its zenith. Law, who was also from a provincial background, said \"He can be Prime Minister for life if he likes.\" Headlines at this time declared a \"huge majority win\" and that \"pacifists, even 'shining lights' such as Arnold Lupton, had been completely overthrown by Ramsay MacDonald and Philip Snowden\".\nCoupon election of 1918.\nIn the \"Coupon election\" of December 1918 he led a coalition of Conservatives and his own faction of Liberals to a landslide victory. Coalition candidates received a \"coalition coupon\" (an endorsement letter signed by Lloyd George and Law). He did not say \"We shall squeeze the German lemon until the pips squeak\" (that was Sir Eric Geddes), but he did express that sentiment about reparations from Germany to pay the entire cost of the war, including pensions. He said that German industrial capacity \"will go a pretty long way\". We must have \"the uttermost farthing\", and \"shall search their pockets for it\". As the campaign closed, he summarised his programme:\nThe election was fought not so much on the peace issue and what to do with Germany, although those themes played a role. More important was the voters' evaluation of Lloyd George in terms of what he had accomplished so far and what he promised for the future. His supporters emphasised that he had won the Great War. Against his strong record in social legislation, he himself called for making \"a country fit for heroes to live in\".\nThe Coalition gained an overwhelming victory, winning 525 of the 707 seats contested; however, the Conservatives had more than two-thirds of the Coalition's seats.\nAsquith's independent Liberals were crushed, although they were still the official opposition as the two Liberal factions combined had more seats than Labour. Accounts vary about the factional allegiance of some MPs: by some accounts as few as 29 uncouponed Liberals had been elected, only 3 with any junior ministerial experience, and only 23 of them were actually opponents of the coalition. Until April 1919 the government whip was extended to \"all\" Liberal MPs and Lloyd George might easily have been elected chairman of the Liberal MPs (Asquith was still party leader but had lost his seat) had he been willing to antagonise his Conservative coalition partners by doing so.\nParis 1919.\nLloyd George represented Britain at the Paris Peace Conference, clashing with French Prime Minister Georges Clemenceau, US President Woodrow Wilson, and Italian Prime Minister Vittorio Orlando. Unlike Clemenceau and Orlando, Lloyd George on the whole stood on the side of generosity and moderation. He did not want to utterly destroy the German economy and political system\u2014as Clemenceau demanded\u2014with massive reparations. The economist John Maynard Keynes looked askance at Lloyd George's economic credentials in \"The Economic Consequences of the Peace\", and in \"Essays in Biography\" called the Prime Minister \"this goat-footed bard, this half-human visitor to our age from the hag-ridden magic and enchanted woods of Celtic antiquity\".\nLloyd George was also responsible for the pro-German shift in the peace conditions regarding the borders of Poland. Instead of handing over Upper Silesia (2,073,000 people), and the southern part of East Prussia (720,000 people) to Poland as was planned before, the plebiscite was organised. Danzig (366,000 people) was organised as the Free City of Danzig. The Poles were grateful that he had saved that country from the Bolsheviks but were annoyed by his comment that they were \"children who gave trouble\". Distrusting Foreign Office professionals, Lloyd George and his team at Paris instead relied on non-professional experts through informal networks below them. They consulted with James Headlam-Morley about Danzig. Several academic historians also were consulted. Their experiences were the basis for building up diplomatic history as a field of academic research and the emergence of the new academic discipline of international relations.\nAsked how he had done at the peace conference, Lloyd George retorted: \"I think I did as well as might be expected, seated as I was between Jesus Christ [Wilson] and Napoleon Bonaparte [Clemenceau].\" Historian Antony Lentin evaluated his role in Paris as a major success, saying:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nPostwar social reforms.\nA major programme of social reform was introduced under Lloyd George in the last months of the war, and in the post-war years. The Workmen's Compensation (Silicosis) Act 1918 (which was introduced a year later) allowed for compensation to be paid to men \"who could prove they had worked in rock which contained no less than 80% silica.\" The Education Act 1918 raised the school leaving age to 14, increased the powers and duties of the Board of Education (together with the money it could provide to Local Education Authorities), and introduced a system of compulsory part-time continuation schools for children between the ages of 14 and 16. The Blind Persons Act 1920 provided assistance for unemployed blind people and blind persons who were in low paid employment.\nThe Housing, Town Planning, &amp;c. Act 1919 provided subsidies for house building by local authorities, and 170,000 dwellings were built under it by the end of 1922. which established, according to A. J. P. Taylor, \"the principle that housing was a social service\". A further 30,000 houses were constructed by private enterprise with government subsidy under a second act.\nThe Land Settlement (Facilities) Act 1919 and Land Settlement (Scotland) Acts of 1919 encouraged local authorities to provide land for people to take up farming \"and also to provide allotments in urban areas.\"\nThe Rent Act 1920 was intended to safeguard working-class tenants against exorbitant rent increases, but it failed. Rent controls were continued after the war, and an \"out-of-work donation\" was introduced for ex-servicemen and civilians.\nElectoral changes: suffragism.\nThe Representation of the People Act 1918 greatly extended the franchise for men (by abolishing most property qualifications) and gave the vote to many women over 30, and the Parliament (Qualification of Women) Act 1918 enabled women to sit in the House of Commons. The Sex Disqualification (Removal) Act 1919 provided that \"A person shall not be disqualified by sex or marriage from the exercise of any public function, or from being appointed to or holding any civil or judicial office or post, or from entering or assuming or carrying on any civil profession or vocation, or for admission to any incorporated society\".\nWages for workers.\nThe Unemployment Insurance Act 1920 extended national insurance to 11 million additional workers. This was considered to be a revolutionary measure, in that it extended unemployment insurance to almost the entire labour force, whereas only certain categories of workers had been covered before. As a result of this legislation, roughly three-quarters of the British workforce were now covered by unemployment insurance.\nThe Agriculture Act 1920 provided for farm labourers to receive a minimum wage while the state continued to guarantee the prices of farm produce until 1921. It also provided tenant farmers with greater protection by granting them better security of tenure. In education, teachers' salaries were standardised, and more than doubled from pre-War levels, in 1921 by the Burnham Committee.\nThe Mining Industry Act 1920 placed a mandatory requirement to provide social welfare opportunities to mining communities, while the Public Health (Tuberculosis) Act 1921 increased the obligation of local authorities to treat and prevent TB.\nHealth reforms.\nIn 1919, the government set up the Ministry of Health, a development which led to major improvements in public health in the years that followed. Whilst the Unemployed Workers' Dependants (Temporary Provisions) Act 1921 provided payments for the wives and dependent children of unemployed workers. The Employment of Women, Young Persons, and Children Act 1920 prohibited the employment of children below the limit of compulsory school age in railways and transport undertakings, building and engineering construction works, factories, and mines. The legislation also prohibited the employment of children in ships at sea (except in certain circumstances, such as in respect of family members employed on the same vessel).\nThe National Health Insurance Act 1920 increased insurance benefits, and eligibility for pensions was extended to more people. The means limit for pensions was raised by about two-thirds, immigrants and their wives were allowed to receive pensions after living in Britain for ten years, and the imprisonment and \"failure to work\" disqualifications for receiving pensions were abolished. The Blind Persons Act 1920 reduced the pension age for blind people from 70 to 50.\nOld age pensions were nearly doubled (from \u00a326 5s to \u00a347 5s a year), efforts were made to help returning soldiers find employment, and the Whitley Councils of employees and employers set up.\nCost.\nThe reforming efforts of the coalition government were such that, according to the historian Kenneth O. Morgan, its achievements were greater than those of the pre-war Liberal governments. However, the reform programme was substantially rolled back by the Geddes Axe, which cut public expenditure by \u00a376\u00a0million, including substantial cuts to education, and abolished the Agricultural Wages Board.\nIreland.\nAs early as 1913 Lloyd George expressed interest in the issues surrounding the of Irish Home Rule movement. He stated that he supported \"...the principle of a referendum...each of the Ulster Counties is to have the option of exclusion from the Home Rule Bill\". Had a referendum occurred it is quite possible that only four of Ulster's nine Counties would have voted for exclusion (see List of MPs elected in the 1918 United Kingdom general election). During Asquith's premiership, the armed insurrection by Irish republicans, known as the Easter Rising, had taken place in Dublin during Easter Week, 1916. The government responded with harsh repression; key leaders were quickly executed. The mostly Catholic Irish nationalists then underwent a dramatic change of mood, and shifted to demand vengeance and independence.\nIn 1917, Lloyd George called the 1917\u201318 Irish Convention in an attempt to settle the outstanding Home Rule for Ireland issue; however, the upsurge in republican sympathies in Ireland following the Easter Rising coupled with Lloyd George's disastrous attempt to extend conscription to Ireland in April 1918 led to the landslide victory of Sinn F\u00e9in and the wipeout of the Irish Parliamentary Party at the December 1918 election. Replaced by Sinn F\u00e9in MPs, they immediately declared an Irish Republic.\nLloyd George presided over the Government of Ireland Act 1920 which partitioned Ireland into Southern Ireland and Northern Ireland in May 1921 during the Anglo-Irish War. Lloyd George famously declared of the Irish Republican Army that \"We have murder by the throat!\" However, he soon afterwards began negotiations with IRA leaders to recognise their authority and to end a bloody conflict. Lloyd George also invited the leader of northern Irish Unionists James Craig to the negotiations but he refused to attend. Lloyd George wrote to Craig on 14 November 1921 \"Your proposal to leave the six counties under the Northern Parliament would stereotype a frontier based neither upon natural features nor broad geographical considerations by giving it the character of an international boundary. Partition upon these lines the majority of the Irish people will never accept, nor could we conscientiously attempt to enforce it.\" (See The Troubles in Northern Ireland (1920\u20131922)). The Anglo-Irish Treaty was signed in December 1921 with Irish leaders. The Parliament of Northern Ireland exercised Article 12 of the Treaty to opt out of the Irish Free State. The Treaty established the Irish Boundary Commission to draw a border between Northern Ireland and the rest of Ireland \"in accordance with the wishes of the inhabitants, so far as may be compatible with economic and geographic conditions...\". Southern Ireland, representing over a fifth of the United Kingdom's territory, seceded in 1922 to form the Irish Free State. (See Partition of Ireland).\nForeign policy crises.\nIn 1921, Lloyd George successfully concluded the Anglo-Soviet Trade Agreement. Despite much effort he was unable to negotiate full diplomatic relations, as the Russians rejected all repayment of Tsarist era debts, and Conservatives in Britain grew exceedingly wary of the communist threat to European stability. Indeed, Henry Wilson, the Chief of the Imperial General Staff, worried that Lloyd George had become \"a traitor &amp; a Bolshevist\".\nLloyd George in 1922 decided to support Greece in a war against Turkey. This led to the Chanak Crisis when most of the Dominions rejected his policy and refused to support the proposed war.\nDomestic crises.\nThe more conservative wing of the Unionist Party had no intention of introducing reforms, which led to three years of frustrated fighting within the coalition both between the National Liberals and the Unionists and between factions within the Conservatives themselves. Many Conservatives were angered by the granting of independence to the Irish Free State and by Edwin Montagu's moves towards limited self-government for India, while a sharp economic downturn and wave of strikes in 1921 damaged Lloyd George's credibility. The \"cash for patronage\" scandal erupted in 1922 when it became known that Lloyd George had essentially sold peerages (from 1917 to 1922 more than 120 hereditary peers were created) and lesser honours such as knighthoods, with a \"price list for peerages\" (\u00a310,000 for a knighthood, \u00a340,000 for a baronetcy), to raise funds for his party, via Maundy Gregory. This was not illegal at the time. The practoce was in fact well-established, and seen as a safeguard against the kind of corruption seen in France and the USA. A major attack in the House of Lords on his corruption followed, resulting in the Honours (Prevention of Abuses) Act 1925. Other complaints were that the Cabinet contained too many Scots, too few men from Oxbridge and the great public schools, too many businessmen, and too few gentlemen.\nFall from power, 1922.\nThe coalition was dealt its final blow in October 1922. The Conservatives felt let down by France over the Chanak Crisis, with Law telling France, \"We cannot act alone as the policeman of the world.\" The Conservative leader, Austen Chamberlain, summoned a meeting of Conservative members of parliament at the Carlton Club to discuss their attitude to the Coalition in the forthcoming election. Chamberlain and most Conservative leaders supported Lloyd George; however, the rank and file rejected the coalition. The main attack came from Stanley Baldwin, then President of the Board of Trade, who spoke of Lloyd George as a \"dynamic force\" who would break the Conservative Party. They sealed Lloyd George's fate on 19 October 1922 by voting in favour of the motion to end the coalition and fight the election \"as an independent party, with its own leader and its own programme\". Lloyd George submitted his resignation to the King that afternoon.\nLater political career (1922\u20131945).\nLiberal reunion.\nThroughout the 1920s Lloyd George remained highly visible in politics; predictions that he would return to power were common, but it never happened. He still controlled a large fund, thought to have been between \u00a31m () and \u00a33m (), from his investments in newspaper ownership and from his sale of titles.\nBefore the 1923 election, he resolved his dispute with Asquith, allowing the Liberals to run a united ticket against Stanley Baldwin's policy of protective tariffs. Baldwin both feared and despised Lloyd George, and one of his aims was to keep him out of power. He later claimed that he had adopted tariffs, which cost the Conservatives their majority, out of concern that Lloyd George was about to do so on his return from a tour of North America. Although there was press speculation at the time that Lloyd George would do so (or adopt US-style Prohibition to appeal to newly enfranchised women voters), there is no evidence that this was his intent. Asquith and Lloyd George reached agreement on 13 November 1923 and issued a joint Free Trade manifesto, followed by a more general one. Lloyd George agreed to contribute \u00a3100,000 (in the event he claimed to have contributed \u00a3160,000 including help given to individual candidates; Liberal HQ put the figure at \u00a390,000).\nIn 1924, Lloyd George, realising that Liberal defeat was inevitable and keen to take control of the party himself, spent only \u00a360,000.\nAt the 1924 general election, Baldwin won a clear victory. Despite having a large majority, he appointed the leading coalitionists such as Austen Chamberlain and F. E. Smith, 1st Earl of Birkenhead (and former Liberal Winston Churchill) to senior cabinet places, to discourage any restoration of the 1916\u20131922 coalition.\nLiberal leader.\nThe disastrous election result in 1924 left the Liberals as a weak third party in British politics behind the ascendant Labour Party, with just over 40 MPs. Although Asquith, who had again lost his seat and was created an Earl, remained Liberal leader, Lloyd George was elected chairman of the Liberal MPs by 26 votes to 7. Sir John Simon and his followers were still loyal to Asquith (after 1931 Simon would lead a breakaway National Liberal Party, which eventually merged with the Conservatives) whilst Walter Runciman led a separate radical group within the Parliamentary Party.\nLloyd George was now mainly interested in the reform of land ownership, but had only been permitted to put a brief paragraph about it in the hastily drafted 1924 Liberal manifesto. In the autumn of 1925, despite the hostility of Charles Hobhouse, Runciman and Alfred Mond, he began an independent campaign, soon to become \"The Land and the Nation\" (the \"Green Book\", first of a series of policy papers produced by Lloyd George in the late 1920s). Asquith rebuked him, but was ignored; they reached an agreement in principle on 2 December, then together they presented Lloyd George's plans to the National Liberal Federation on 26 February 1926.\nThe Liberal Shadow Cabinet, including Lloyd George, unequivocally backed Baldwin's handling of the General Strike on 3 May 1926, but Lloyd George then wrote an article for the American press more sympathetic to the strikers, and did not attend the Shadow Cabinet on 10 May, sending his apologies on \"policy grounds\". Asquith sent him a public letter (20 May) rebuking him for not attending the meeting to discuss his opinions with colleagues in private. Lloyd George's letter of 10 May had not been published, making it appear that Asquith had fired the first shot, and Lloyd George sent a public reply, moderate in tone (the journalist C. P. Scott helped him draft it), on 25 May. In late May, the executive of the National Liberal Federation convened to plan the agenda for the following month's conference. 16 were pro-Asquith and 8 pro-Lloyd George; they planned a motion expressing confidence in Asquith, but another option was also proposed to seek Asquith's opinion first, and also general feeling of regret at having been forced to choose between Asquith and Lloyd George. Asquith then wrote another public letter (1 June) stating that he regarded Lloyd George's behaviour as tantamount to resignation, the same as if a Cabinet Minister had refused to abide by the principle of collective responsibility. Twelve leading Liberals wrote in Asquith's support to \"The Times\" (1 June); however, Lloyd George had more support in the wider party than among the grandees: the London Liberal Candidates' Association (3 June) defied its officers and expressed its dismay at the split, effectively supporting Lloyd George, and on 8 June the Liberal MPs voted 20:10 urging a reconciliation. Asquith had planned to launch a fightback at the National Liberal Federation in Weston-super-Mare, but on 12 June, five days before the conference was due to start, he suffered a stroke which put him out of action for three months. Lloyd George was given a rapturous welcome. Asquith resigned as party leader in October 1926, dying in 1928.\nAs Liberal leader at last, Lloyd George used his fund to finance candidates and put forward innovative ideas for public works to reduce unemployment, detailed in works such as \"Britain's Industrial Future\" (known as the \"Yellow Book\"), and \"We Can Conquer Unemployment\" (known as the \"Orange Book\"). Charles Masterman, a member of the commission which prepared \"Britain's Industrial Future\", wrote: \"When Lloyd George came back to the party, ideas came back to the party\". Lloyd George was helped by John Maynard Keynes to write \"We Can Conquer Unemployment\", setting out economic policies to solve unemployment. In 1927, Lloyd George gave \u00a3300,000 and an annual grant of between \u00a330,000 and \u00a340,000 for the operations of the Liberal headquarters. He also gave \u00a32,000 per annum to the parliamentary party until 1931. Even with the money, the results at the 1929 general election were disappointing. The Liberals increased their support only to 59 seats, while Labour became the largest party for the first time. Once again, the Liberals ended up supporting a minority Labour government. In 1929, Lloyd George became Father of the House (longest-serving member of the Commons), an honorific position without power.\nMarginalised.\nIn 1931, an illness prevented Lloyd George's joining the National Government when it was formed. When the National Government later called a general election he tried to pull the Liberal Party out of it, but succeeded in taking only a few followers, most of whom were related to him; the main Liberal Party remained in the coalition for a year longer, under the leadership of Sir Herbert Samuel. By the 1930s Lloyd George was on the margins of British politics, although still intermittently in the public eye and publishing his \"War Memoirs\".\nLloyd George's \"New Deal\".\nIn January 1935 Lloyd George announced a programme of economic reform, called \"Lloyd George's New Deal\" after the American New Deal. This Keynesian economic programme was essentially the same as that of 1929. Ramsay MacDonald requested that he put his case before the Cabinet. In March, Lloyd George submitted a 100-page memorandum (published as \"Organizing Prosperity: A Scheme of National Reconstruction\") that was cross-examined between April and June in ten meetings of the Cabinet's sub-committee; however, the programme did not find favour; two-thirds of Conservative MPs were against Lloyd George joining the National government, and some Cabinet members would have resigned if he had joined.\nSupport for Nazi Germany.\nLloyd George was consistently pro-German after 1923, in part due to his growing conviction that Germany had been treated unfairly at Versailles. He supported German demands for territorial concessions and recognition of its \"great power\" status; he paid much less attention to the security concerns of France, Poland, Czechoslovakia, and Belgium.\nIn a speech in 1933, he warned that if Adolf Hitler were overthrown, communism would replace him in Germany. In August 1934, he insisted Germany could not wage war and assured European nations that there would be no risk of war during the next ten years. In September 1936, he visited Germany to talk with Hitler. Hitler said he was pleased to have met \"the man who won the war\"; Lloyd George was moved, and called Hitler \"the greatest living German\".\nLloyd George also visited Germany's public works programmes and was impressed. On his return to Britain, he wrote an article for the \"Daily Express\" praising Hitler and stating: \"The Germans have definitely made up their minds never to quarrel with us again.\" He believed Hitler was \"the George Washington of Germany\"; that he was rearming Germany for defence and not for offensive war; that a war between Germany and the Soviet Union would not happen for at least ten years; that Hitler admired the British and wanted their friendship but that there was no British leadership to exploit this. However, by 1937, Lloyd George's distaste for Neville Chamberlain led him to disavow Chamberlain's appeasement policies.\nFinal years.\nIn the last important parliamentary intervention of his career, which occurred during the crucial Norway Debate of May 1940, Lloyd George made a powerful speech that helped to undermine Chamberlain as prime minister and to pave the way for the ascendancy of Churchill. Churchill offered Lloyd George the agriculture portfolio in his Cabinet, initially subject to Chamberlain's approval, but this condition and, once Chamberlain had withdrawn his opposition, Lloyd George's unwillingness to sit alongside Chamberlain, led him to refuse.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;As we approached No 10 LG said 'I won't go in with Neville' \nLloyd George also thought that Britain's chances in the war were dim, and he remarked to his secretary: \"I shall wait until Winston is bust.\"\nA pessimistic speech by Lloyd George on 7 May 1941 led Churchill to compare him with Philippe P\u00e9tain who had become a Nazi puppet. He cast his last vote in the Commons on 18 February 1943 as one of the 121 MPs (97 Labour) condemning the Government for its failure to back the Beveridge Report.\nHe continued to attend Castle Street Baptist Chapel in London, but by 1944 he was weakening rapidly and his voice failing. He was still an MP but, concerned about his health (he felt physically unable to campaign) and the wartime social changes in the constituency, he feared Carnarvon Boroughs might go Conservative at the next election.\nIt was announced in the 1945 New Year Honours that Lloyd George would be made an earl, which he was as Earl Lloyd-George of Dwyfor, and Viscount Gwynedd, of Dwyfor in the County of Caernarvonshire on 12 February 1945; however, he did not live long enough to take his seat in the House of Lords.\nDeath.\nLloyd George died of cancer at the age of 82 on 26 March 1945, with his wife Frances and his daughter Megan at his bedside. Four days later, on Good Friday, he was buried beside the river Dwyfor in Llanystumdwy. A boulder marks the grave; there is no inscription; however, a monument designed by the architect Sir Clough Williams-Ellis was subsequently erected around the grave, bearing an englyn (strict-metre stanza) engraved on slate in his memory composed by his nephew W. R. P. George. Nearby stands the Lloyd George Museum, also designed by Williams-Ellis and opened in 1963.\nAssessment.\nLloyd George has often been ranked highly among modern British prime ministers, but his legacy remains complicated and controversial. Scholars have praised his welfare reforms and his efforts to mobilise and lead Britain to victory during the First World War, but he has also been criticised for adopting a \"presidential\" style of leadership, for distrusting his own commanders during the war, and for his strategic failures and involvement in various scandals. His legacies over Ireland and the Treaty of Versailles are also controversial; he was an ardent Zionist, who expressed antisemitic views. In the post-war period he arguably alienated many of the workers he had earlier championed, helping to swell Labour's popular support at the Liberals' expense (not helped by his conflicts with Asquithian Liberals after 1916).\nHistorian Martin Pugh in \"The Oxford Companion to British History\" argues that:[Lloyd George] made a greater impact on British public life than any other 20th-cent. statesman. He laid the foundations of what later became the welfare state, and put a progressive income tax system at the centre of government finance. He also left his mark on the system of government by enlarging the scope of the prime minister's role. He was acclaimed, not without reason, as the 'Man Who Won the War'.\u00a0... he was blamed by many Liberals for destroying their party in 1918, hated in the Labour movement for his handling of industrial issues after 1918, and disparaged by Conservatives for his radicalism.\nGeorge Riddell, 1st Baron Riddell, a wealthy newspaper publisher, was a close confidant and financial supporter of Lloyd George from 1908 to 1922. During Lloyd George's first year as prime minister, in summer 1917, Riddell assessed his personality in his diary:His energy, capacity for work, and power of recuperation are remarkable. He has an extraordinary memory, imagination, and the art of getting at the root of a matter.\u00a0... He is not afraid of responsibility, and has no respect for tradition or convention. He is always ready to examine, scrap or revise established theories and practices. These qualities give him unlimited confidence in himself.\u00a0... He is one of the craftiest of men, and his extraordinary charm of manner not only wins him friends, but does much to soften the asperities of his opponents and enemies. He is full of humour and a born actor.\u00a0... He has an instinctive power of divining the thoughts and intentions of people with whom he is conversing\u00a0... His chief defects are: (1) Lack of appreciation of existing institutions, organisations, and stolid, dull people\u00a0... their ways are not his ways and their methods are not his methods. (2) Fondness for a grandiose scheme in preference to an attempt to improve existing machinery. (3) Disregard of difficulties in carrying out big projects\u00a0... he is not a man of detail.\nIn 2007, historian John Shepherd wrote in \"History Today\":\nIn any poll of modern historians Winston Churchill and David Lloyd George would emerge as the two most renowned prime ministers during the past century.\nFamily.\nMargaret and children.\nHe had five children by his first wife, Dame Margaret Lloyd George:\nDespite his long-term affair with Frances Stevenson, he remained married to Margaret, and remained fond of her until her death on 20 January 1941; Lloyd George was deeply upset by the fact that bad weather prevented him from being with her when she died.\nGwilym and Megan both followed their father into politics and were elected Members of Parliament. They were politically faithful to their father throughout his life, but after 1945 each drifted away from the Liberal Party, Gwilym finishing his career as Home Secretary under the Conservatives in the 1950s and Megan becoming a Labour MP in 1957.\nFrances.\nLloyd George met Frances Stevenson in 1910; she worked for him first as a teacher for Megan in 1911; she became his secretary and, from early 1913, his long-term mistress. Lloyd George may have been the father of Stevenson's daughter Jennifer (1929\u20132012), born long before they wed, but it is more likely that she was the daughter of Thomas Tweed, with whom Stevenson had had an affair. To the disapproval of his children he finally married Frances in October 1943; he was aged 80 at the time.\nFrances was the first Countess Lloyd-George, and is now largely remembered for her diaries, which dealt with the great issues, and statesmen, of Lloyd George's heyday. A volume of their letters, \"My Darling Pussy\", has also been published; Lloyd George's nickname for Frances referred to her gentle personality.\nWomanising.\nLloyd George had a considerable reputation as a womaniser, including an alleged long affair with the wife of a Parliamentary colleague in the 1890s. In a letter to his wife, Lloyd George wrote of his philandering, \"You say I have my weakness. So has anyone that ever lived &amp; the greater the man the greater the weakness. It is only insipid, wishy washy fellows that have no weaknesses\". His biographer Travis Crosbie comments that although he clearly enjoyed the company of women much of the information is based on hearsay rather than actual evidence and that his reputation may well be considerably exaggerated.\nDescendants.\nThe Canadian historian Margaret MacMillan, who detailed Lloyd George's role at the 1919 Peace Conference in her book, \"\", is his great-granddaughter. The British television historian and presenter Dan Snow is a great-great-grandson through his mother, Canadian-born Ann MacMillan (married Peter Snow), a long-time CBC reporter based in London and sister of Margaret MacMillan.\nHonours.\nFreedoms.\nLloyd George was made Honorary Freeman of the following cities and towns:\nNamesakes.\nLloyd George Avenue is an extension of the A470 road, connecting Central Cardiff to Cardiff Bay.\nMount Lloyd George in the Northern Rocky Mountains of British Columbia, Canada was named after Lloyd George during the First World War, and still retains the name.\nKibbutz Ramat David in the Jezreel Valley in northern Israel and the adjacent Ramat David Airbase are named after him.\nDavid Lloyd George Elementary School in Vancouver was named after Lloyd George in 1921.\n\"David Lloyd George\", a Fairlie locomotive built in 1992 on the Ffestiniog Railway, is named after him. Lloyd George was once a frequent passenger on the railway.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\nBiographical.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nSpecialised studies.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nPrimary sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46838", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=46838", "title": "Federalist party", "text": ""}
{"id": "46839", "revid": "50967015", "url": "https://en.wikipedia.org/wiki?curid=46839", "title": "Kinder Surprise", "text": "Chocolate egg candy\nKinder Surprise (Italian: \"Kinder Sorpresa\" or \"Ovetto Kinder\"), also known as Kinder Egg or Kinder Surprise Egg, is a milk chocolate consisting of a chocolate egg surrounding a yellow plastic capsule with a small toy inside. Manufactured by the Italian company Ferrero since 1974, it was co-created by Michele Ferrero and William Salice, and is one of several candies sold under the Kinder brand. \nKinder Surprise was originally created with children in mind, replicating an Italian Easter family tradition in which adults give children large chocolate eggs with toys inside. However, Kinder Surprise toys have become collectible for adults as well. As of 2016, 30 billion Kinder Surprise eggs have been sold worldwide since its launch in 1974.\nDescription.\nKinder Surprise is a milk chocolate egg lined with a layer of white chocolate. Inside each egg is a plastic capsule, which would be twisted open along its circumference, and that contains a small surprise toy, which sometimes requires assembly. The capsule case is colored yellow and sometimes orange, to resemble an egg's yolk. The chocolates have foil packaging with warning labels advising parents to avoid giving the eggs to children under three years old and encouraging supervision during consumption.\nKinder Surprise was originally created with children in mind, replicating an Italian Easter family tradition in which adults give children a large chocolate egg with a toy inside. However, Kinder Surprise toys have become collectible for adults as well. Collectors often try to acquire all toys within a themed set. Some even share their egg openings on social media, or create their own toys and re-wrap them in Kinder Surprise packaging. More than 100 new toys are distributed each year. Around 12,000 different toys had been included within Kinder Surprise as of 2016.\nAccording to CNNMoney, Kinder Surprise is most popular in Germany, Russia, and the United Kingdom. Michele Ferrero and William Salice have been credited as co-creators of the candy.\nHistory.\nIn 1968, Michele Ferrero raised the idea with his employees of a product that could be given to children so they could have a little \"surprise\" every day, based on the Italian tradition of large chocolate eggs given to children by their parents at Easter. Ferrero said that at first his attempt to follow through with this idea was unsuccessful after employees questioned the order he placed for a machine to make the chocolate eggs. They thought it would not make any money, since eggs are only for Easter. Ferrero also said that he wanted the product to have a higher milk content and make that a key part of its promotion; he believed mothers would respond well to the idea of giving their children more milk. Ferrero commissioned William Salice to realize the concept.\nThe Italian company Ferrero began manufacturing Kinder Surprises in 1974. Since then around 30 billion eggs have been sold worldwide.\nSalice, who has been credited as the inventor of Kinder Surprise but insisted he was just \"material executor\", died in Italy on 29 December 2016, at the age of 83.\nCollections and promotion.\nThe toys within Kinder Surprise have been themed for various popular licensed characters. Collections of Kinder Surprise toys have included Asterix, Fantomimi, Smurfs, and Minions. Ferrero and Kinder have also partnered with various companies, institutions, and people to promote Kinder Surprise, including The Walt Disney Company, Universal, and Smart.\nSafety concern.\nUnited Kingdom.\nIn 2000, three families who had lost children to choking on toys inside edible eggs campaigned for the products to be withdrawn from the European Union.\nDefenders of the chocolates said that these had been unfortunate fatalities. This was discussed in the House of Commons and also by the Department of Trade and Industry which said, \"The child\u2019s tragic death was caused by the ingestion of a small part of the egg\u2019s contents. Many other products and toys with small parts are available in the market place. If we were to start banning every product that could be swallowed by a child, there would be very few toys left in the market\".\nUnited States.\nA 1938 law, the Federal Food, Drug, and Cosmetic Act, prohibits confectionery products that contain a \"non-nutritive object\", unless the non-nutritive object has functional value. Essentially, the Act bans \"the sale of any candy that has embedded in it a toy or trinket\".\nIn 1997, the staff of the Consumer Product Safety Commission examined and issued a recall for some Kinder Surprise illegally brought into the US with foreign labels. The staff determined that the toys within the eggs had small parts. The staff presumed that Kinder Surprise, being a chocolate product, was intended for children of all ages, including those under three years of age. On this basis, the staff took the position that Kinder Surprise was in violation of the small parts regulation and should be banned from importation into the US.\nKinder Surprise eggs are legal in Canada and Mexico, but are illegal to import into the US. In January 2011, the US Customs and Border Protection (CBP) threatened a Manitoba resident with a 300 Canadian dollar fine for carrying one egg across the US border into Minnesota. In June 2012, CBP held two Seattle men for two and a half hours after discovering six Kinder Surprise eggs in their car upon returning to the US from a trip to Vancouver. According to Joseph Cummings of Seattle, Washington, one of the men detained, a border guard quoted the potential fine as \"$2,500 per egg\".\nIn 2012, the Food and Drug Administration (FDA) re-issued their import alert stating \"The embedded non-nutritive objects in these confectionery products may pose a public health risk as the consumer may unknowingly choke on the object\".\nKinder Surprise bears warnings advising the consumer that the toy is \"not suitable for children under three years, due to the presence of small parts\", and that \"adult supervision is recommended\".\nAs of 2017 Kinder Joy eggs, a similar product, are being sold in the United States. Instead of a toy being encased in a chocolate egg, it is in an egg-shaped plastic package with the toy and chocolate separated. Kinder Surprise eggs are still illegal in the US, but remain popular on the black market.\nThe chocolate content of the Kinder Surprise and Kinder Joy is what differentiates them. Kinder Joy has a spoon to eat a creme inside, while Kinder Surprise is two-layer chocolate\u2014milk chocolate on the outside and white chocolate on the inside.\nChile.\nIn 2016, new food labeling and packaging laws resulted in Chile banning the Kinder Surprise.\nBelgium.\nIn 2022, the Belgian food agency reported about 20 cases of salmonella in Belgium due to contaminated Kinder Surprise eggs.\nCanada.\nIn 2022, Ferrero Canada Ltd. recalled 23 Kinder brand chocolate products in Canada. The recall included Kinder Surprise 100g, and other products containing them. According to the Canadian Food Inspection Agency (CFIA), the recall was voluntary. No illnesses were associated due to the consumption of the product.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46840", "revid": "483858", "url": "https://en.wikipedia.org/wiki?curid=46840", "title": "Kinder Eggs", "text": ""}
{"id": "46842", "revid": "7196877", "url": "https://en.wikipedia.org/wiki?curid=46842", "title": "Boojum", "text": "Boojum may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "46843", "revid": "1313133326", "url": "https://en.wikipedia.org/wiki?curid=46843", "title": "Fouquieria columnaris", "text": "Species of flowering plant\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nFouquieria columnaris, the Boojum tree or cirio () is a tree in the ocotillo family, whose other members include the ocotillos. Some taxonomists place it in the separate genus \"Idria\". It is nearly endemic to the Baja California Peninsula (both the northern and southern states), with only a small population in the Sierra Bacha of Sonora, Mexico. The plant's English name, Boojum, was given by Godfrey Sykes of the Desert Laboratory in Tucson, Arizona, and is taken from Lewis Carroll's poem \"The Hunting of the Snark\".\nDescription.\nThis plant is a columniform, upwardly tapering tree. The central axis of the plant is homologous to the single stem of other species. It has a cortical water-storage network, a feature unique to the family.\nThe \"Fouquieria columnaris\" trunk is up to 24 inches (61 centimeters) thick, off-white in color, with few or no major branches and with numerous thin, twiggy branches sticking out at right angles, all covered with small leaves long. They can grow to a height of 20 meters (almost 70 feet), but the tallest, in Montevideo Valley between Mission San Borja and Bahia de los Angeles is in height, the second tallest succulent after \"Euphorbia ampliphylla\".\nThe flowers bloom in August and September regardless of rainfall; they occur in short racemes, and have a honey scent. The flowers have short, cream-yellow corollas, with the limb of the petals inflexing around the filaments of the stamens. The anthers and stamens protrude out, while the stigma is protected by the inflexed petal limbs. The flowers are visited by at least 15 species of bees in 11 genera, who pry open the inflexed corolla limbs to obtain the sweetened nectar and contact the protected stigma.\nIt is among the slowest growing trees. At fifty years of age, it may be only tall, and thereafter averages twelve inches (thirty centimeters) every ten years.\nTaxonomy.\nThis species is most similar to two \"Fouquieria\" native to south-central Mexico, \"Fouquieria fasciculata\" and \"Fouquieria purpusii\", as they share a succulent xylem, widely spaced decurrent leaf bases, and small decandrous (ten-stamen) flowers. In the two species, they are initially woody, and their succulent xylem develops only in the lower portion of their main stem. In contrast, the Boojum tree has a succulent xylem from its initiation, with the primary thickening occurring from the meristem. The chromosome number of this species is \"n\"=36.\nTaxonomic history.\nThis species was first described by Albert Kellogg based on specimens that were collected by a J. A. Veatch in Baja California. The type description was published twice and nearly identically, in the \"Proceedings of the California Academy of Natural Sciences\" and the San Francisco monthly periodical \"Hesperian\" in May 1860. Because the exact date of the publication in the \"Proceedings\" is unknown, but is likely to be after 1862, the type description in the \"Hesperian\" has the priority. The holotype specimen was destroyed in the San Francisco earthquake and fire of 1906.\nThis species was previously placed in the monotypic genus Idria. However, this taxonomic classification was created before the vegetative and floral structures of \"F. fasciculata\" and \"F. purpursii\" were understood, as these two species share intermediate characteristics between the genera \"Idria\" and \"Fouquieria\". As many other genera (\"Pachypodium\", \"Euphorbia\", \"Jatropha\" and \"Coreopsis\") contain both woody and succulent species, and because there are few diagnostic characteristics to separate major groups within the family, the genus \"Idria\" has been merged into \"Fouquieria\".\nDistribution and habitat.\nThis species occurs from sea level to up to 1450 meters in elevation on deep to shallow volcanic loams or clays to decomposed granite soils, on well-drained sites on hillsides, mesa, and alluvial plains. It is found from the vicinity of San Quint\u00edn in Baja California south to the Tres V\u00edrgenes complex of volcanoes in Baja California Sur; a notable concentration is the Boojum Forest. It is also found on Isla Angel de la Guarda in the Gulf of California. On mainland Mexico, it is also found in a small area south of Puerto Libertad in coastal Sonora. The annual rainfall within its region only averages about 73 to 140\u00a0mm, mainly from January to April and in lesser amounts in August through September. Rainfall is unreliable and sometimes years may pass without heavy rainfall.\nThe peculiar distribution pattern of the mainland boojums has led Mexican botanists to conclude that they were probably transplanted to the mainland by the indigenous Seri people, who lived in this area and still live on communal property south of this location. The Seri name for this plant is \"cototaj\" . In Seri belief, touching this plant will cause strong winds to blow (an undesirable state). Given this belief, the hypothesis that the Seri people transplanted it is doubtful.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46845", "revid": "1123", "url": "https://en.wikipedia.org/wiki?curid=46845", "title": "Four-Color Theorem", "text": ""}
{"id": "46846", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=46846", "title": "August Weismann", "text": "German evolutionary biologist (1834\u20131914)\nAugust Friedrich Leopold Weismann (; 17 January 1834\u00a0\u2013 5 November 1914) was a German evolutionary biologist. Fellow German Ernst Mayr ranked him as the second most notable evolutionary theorist of the 19th century, after Charles Darwin. Weismann became the Director of the Zoological Institute and the first Professor of Zoology at Freiburg.\nHis main contribution involved germ plasm theory, at one time also known as Weismannism, according to which inheritance (in a multicellular animal) only takes place by means of the germ cells\u2014the gametes such as egg cells and sperm cells. Other cells of the body\u2014somatic cells\u2014do not function as agents of heredity. The effect is one-way: germ cells produce somatic cells and are not affected by anything the somatic cells learn or therefore any ability an individual acquires during its life. Genetic information cannot pass from soma to germ plasm and on to the next generation. Biologists refer to this concept as the Weismann barrier. This idea, if true, rules out the inheritance of acquired characteristics as proposed by Jean-Baptiste Lamarck. However, a careful reading of Weismann's work over the span of his entire career shows that he had more nuanced views, insisting, like Darwin, that a variable environment was necessary to cause variation in the hereditary material.\nThe idea of the Weismann barrier is central to the modern synthesis of the early 20th century, though scholars do not express it today in the same terms. In Weismann's opinion the largely random process of mutation, which must occur in the gametes (or stem cells that make them) is the only source of change for natural selection to work on. Weismann became one of the first biologists to deny Lamarckism entirely. Weismann's ideas preceded the rediscovery of Gregor Mendel's work, and though Weismann was cagey about accepting Mendelism, younger workers soon made the connection.\nLife.\nYouth and studies.\nWeismann was born a son of high school teacher Johann (Jean) Konrad Weismann (1804\u20131880), a graduate of ancient languages and theology, and his wife Elise (1803\u20131850), n\u00e9e L\u00fcbbren, the daughter of the county councillor and mayor of Stade, on 17 January 1834 in Frankfurt am Main. He had a typical 19th century bourgeois education, receiving music lessons from the age of four, and drafting and painting lessons from Jakob Becker (1810\u20131872) at the Frankfurter St\u00e4delsche Institut from the age of 14. His piano teacher was a devoted butterfly collector and introduced him to the collecting of imagos and caterpillars. But studying natural sciences was out of the question due to the cost involved and limited job prospects. A friend of the family, chemist Friedrich W\u00f6hler (1800\u20131882), recommended studying medicine. A foundation from the inheritance of Weismann's mother allowed him to take up studies in G\u00f6ttingen. Following his graduation in 1856, he wrote his dissertation on the synthesis of hippuric acid in the human body.\nProfessional life.\nImmediately after university, Weismann took on a post as assistant at the St\u00e4dtische Klinik (city clinic) in Rostock. Weismann successfully submitted two manuscripts, one about hippuric acid in herbivores, and one about the salt content of the Baltic Sea, and won two prizes. The paper about the salt content dissuaded him from becoming a chemist, since he felt himself lacking in apothecarial accuracy.\nAfter a study visit to see Vienna's museums and clinics, he visited Italy (1859) and Paris (1860). He returned to Frankfurt as personal physician to the banished Archduke Stephen of Austria at Schaumburg Castle from 1861 to 1863. During the war between Austria, France and Italy in 1859, he became Chief Medical Officer in the military, and on a leave from duty he walked through Northern Italy and the County of Tyrol. After a sabbatical in Paris, he worked with Rudolf Leuckart at the University of Gie\u00dfen. \nFrom 1863, he was privatdozent in comparative anatomy and zoology; from 1866 extraordinary professor; and from 1873 to 1912 full professor, first holder of the chair in zoology and director of the zoological institute at Albert Ludwig University of Freiburg in Breisgau. He retired in 1912. His earlier work was largely concerned with purely zoological investigations, one of his earliest works dealing with the development of the Diptera. Microscopical work, however, became impossible to him owing to impaired eyesight, and he turned his attention to wider problems of biological inquiry.\nFamily.\nIn 1867 he married Mary Dorothea Gruber.\nTheir son, Julius Weismann (1879\u20131950), was a composer.\nContributions to evolutionary biology.\nAt the beginning of Weismann's preoccupation with evolutionary theory was his grappling with Christian creationism as a possible alternative. In his work \"\u00dcber die Berechtigung der Darwin'schen Theorie\" (\"On the justification of the Darwinian theory\") he compared creationism and evolutionary theory, and concluded that many biological facts can be seamlessly accommodated within evolutionary theory, but remain puzzling if considered the result of acts of creation.\nAfter this work, Weismann accepted evolution as a fact on a par with the fundamental assumptions of astronomy (e.g. Heliocentrism). Weismann's position towards the mechanism of inheritance and its role for evolution changed during his life. Three periods can be distinguished.\nGerman work on cells.\nWeismann's work on the demarcation between germ-line and soma can scarcely be appreciated without considering the work of (mostly) German biologists during the second half of the 19th century. This was the time that the mechanisms of cell division began to be understood. Eduard Strasburger, Walther Flemming, Heinrich von Waldeyer and the Belgian Edouard Van Beneden laid the basis for the cytology and cytogenetics of the 20th century. Strasburger, the outstanding botanical physiologist of that century, coined the terms nucleoplasm and cytoplasm. He said \"new cell nuclei can only arise from the division of other cell nuclei\". Van Beneden discovered how chromosomes combined at meiosis, during the production of gametes, and discovered and named chromatin. Walther Flemming, the founder of cytogenetics, named mitosis, and pronounced \"omnis nucleus e nucleo\" (which means the same as Strasburger's dictum). The discovery of mitosis, meiosis and chromosomes is regarded as one of the 100 most important scientific discoveries of all times, and one of the 10 most important discoveries in cell biology.\nMeiosis was discovered and described for the first time in sea urchin eggs in 1876, by Oscar Hertwig. It was described again in 1883, at the level of chromosomes, by Van Beneden in \"Ascaris\" eggs. The \"significance of meiosis for reproduction and inheritance\", however, was first described in 1890 by Weismann, who noted that two cell divisions were necessary to transform one diploid cell into four haploid cells if the number of chromosomes had to be maintained. Thus the work of the earlier cytologists laid the ground for Weismann, who turned his mind to the consequences for evolution, which was an aspect the cytologists had not addressed. All this took place before the rediscovery of the work of Mendel. \n1868\u20131881/82.\nWeismann started out believing, like many other 19th century scientists, among them Charles Darwin, that the observed variability of individuals of one species is due to the inheritance of \"sports\" (Darwin's term). He believed, as written in 1876, that transmutation of species is directly due to the influence of environment. He also wrote, \"if every variation is regarded as a reaction of the organism to external conditions, as a deviation of the inherited line of development, it follows that no evolution can occur without a change of the environment\". (This is close to the modern use of the concept that changes in the environment can mediate selective pressures on a population, so leading to evolutionary change.) Weismann also used the classic Lamarckian metaphor of use and disuse of an organ.\n1882\u20131895.\nWeismann's first rejection of the inheritance of acquired traits was in a lecture in 1883, titled \"On inheritance\" (\"\u00dcber die Vererbung\"). Again, as in his treatise on creation vs. evolution, he attempts to explain individual examples with either theory. For instance, the existence of non-reproductive castes of ants, such as workers and soldiers, cannot be explained by inheritance of acquired characters. Germ plasm theory, on the other hand, does so effortlessly. Weismann used this theory to explain Lamark's original examples for \"use and disuse\", such as the tendency to have degenerate wings and stronger feet in domesticated waterfowl.\n1896\u20131910.\nWeismann worked on the embryology of sea urchin eggs, and in the course of this observed different kinds of cell division, namely equatorial division and reductional division, terms he coined (\"\u00c4quatorialteilung\" and \"Reduktionsteilung\" respectively).\nHis \"germ plasm theory\" states that multicellular organisms consist of germ cells containing heritable information, and somatic cells that carry out ordinary bodily functions. The germ cells are influenced neither by environmental influences nor by learning or morphological changes that happen during the lifetime of an organism, which information is lost after each generation. The concept as he proposed it was referred to as \"Weismannism\" in his day, for example in the book \"An examination of Weismannism\" by George Romanes This idea was illuminated and explained by the rediscovery of Gregor Mendel's work in the early years of the 20th century (see Mendelian inheritance).\nExperiments on the inheritance of mutilation.\nThe idea that germline cells contain information that passes to each generation unaffected by experience and independent of the somatic (body) cells, came to be referred to as \"the Weismann barrier\", and is frequently quoted as putting a final end to the theory of Lamarck and the inheritance of acquired characteristics. What Lamarck claimed was the inheritance of characteristics acquired through effort, or will.\nWeismann conducted the experiment of removing the tails of 68 white mice, repeatedly over 5 generations, and reporting that no mice were born in consequence without a tail or even with a shorter tail. He stated that \"901 young were produced by five generations of artificially mutilated parents, and yet there was not a single example of a rudimentary tail or of any other abnormality in this organ.\" Weismann was aware of the limitations of this experiment, and made it clear that he embarked on the experiment precisely because, at the time, there were many claims of animals inheriting mutilations (he refers to a claim regarding a cat that had lost its tail having numerous tail-less offspring). There were also claims of Jews born without foreskins. None of these claims, he said, were backed up by reliable evidence that the parent had in fact been mutilated, leaving the perfectly plausible possibility that the modified offspring were the result of a mutated gene. The purpose of his experiment was to lay the claims of \"inherited mutilation\" to rest. The results were consistent with Weismann's germ plasm theory.\nAwards and honors.\nWeismann was elected to honorary membership of the Manchester Literary and Philosophical Society in 1894. He was nominated the Honorary Fellow of the University of Tartu in 1902. He was elected an International Member of the American Philosophical Society in 1906. He was awarded the Linnean Society of London's Darwin-Wallace Medal in 1908. He was elected an International Member of the United States National Academy of Sciences in 1913.\nErnst Mayr judged Weismann to be the most important evolutionary thinker between Darwin and the evolutionary synthesis around 1930\u20131940, and \"one of the great biologists of all time\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46847", "revid": "134766", "url": "https://en.wikipedia.org/wiki?curid=46847", "title": "Spoon", "text": "Eating utensil\nA spoon (, ) is a utensil consisting of a shallow bowl (also known as a head), oval or round, at the end of a handle. A type of cutlery (sometimes called flatware in the United States), especially as part of a place setting, it is used primarily for transferring food to the mouth (eating). Spoons are also used in food preparation to measure, mix, stir and toss ingredients and for serving food. Present day spoons are made from metal (notably stainless steel, flat silver or silverware, plated or solid), wood, porcelain or plastic. There are many different types of spoons made from different materials by different cultures for different purposes and food.\nTerminology.\nThe spoon consists of a \"bowl\" and a handle. A handle in the shape of a slender stick is frequently called a \"stem\". The stem can end in a sharp point or be crowned with a \"knop\", a decorative knob. The \"knop-top\" spoons with a variety of knop shapes described by colorful terms like \"acorn\", \"writhen-end\" (spiral ornament on a ball), \"maidenhead\" (a bust), \"diamond point,\" \"apostle\" were particularly popular in England in the 14th to 17th centuries. \nThe name \"spoon\" came from Old English \"spon\", 'chip'.\nHistory.\nPreserved examples of various forms of spoons used by the ancient Egyptians include those composed of ivory, flint, slate and wood, many of them carved with religious symbols. During the Neolithic Ozieri civilization in Sardinia, ceramic ladles and spoons were already in use. In Shang dynasty China, spoons were made of bone. Early bronze spoons in China were designed with a sharp point, and may have also been used as cutlery. The spoons of the Greeks and Romans were chiefly made of bronze and silver and the handle usually takes the form of a spike or pointed stem. There are many examples in the British Museum from which the forms of the various types can be ascertained, the chief points of difference being found in the junction of the bowl with the handle. The ancient Greeks called the spoon \"mystron\" (\u03bc\u03cd\u03c3\u03c4\u03c1\u03bf\u03bd), and they also used pieces of bread scooped out in the shape of a spoon, which they called, \"mystile\" (\u03bc\u03c5\u03c3\u03c4\u03af\u03bb\u03b7).\nA 2024 study by archaeologist Andrzej Kokowski and biologists from Maria Curie-Sk\u0142odowska University in Lublin, Poland, identified 241 small, spoon-shaped objects at 116 archaeological sites across Scandinavia, Germany, and Poland, dating back to the Roman era. These sites primarily consisted of marshes and graves. The study proposes that these objects, often found alongside items associated with warfare and featuring a small disk 10-20 millimeters in diameter, were likely used to administer drugs, especially stimulants, before battles. Germanic peoples of the era had access to various substances with potential medicinal or psychoactive properties, including poppy, hops, hemp, henbane, belladonna, and certain fungi.\nIn the early Muslim world, spoons were used for eating soup. Medieval spoons meant for domestic use were commonly made of cow horn or wood, but brass, pewter, and latten spoons appear to have been common in about the 15th century. The full descriptions and entries relating to silver spoons in the inventories of the royal and other households point to their special value and rarity. The earliest English reference appears to be in a will of 1259. In the wardrobe accounts of Edward I for the year 1300 some gold and silver spoons marked with the \"fleur-de-lis\", the Paris mark, are mentioned. One of the most interesting medieval spoons is the Coronation Spoon used in the anointing of the English and later British sovereign; this 12th-century object is the oldest surviving item in the British royal regalia.\nThe sets of Apostle Spoons, popular as christening presents in Tudor times, the handles of which terminate in heads or busts of the apostles, are a special form to which antiquarian interest attaches. The earlier English spoon-handles terminate in an acorn, plain knob or a diamond; at the end of the 16th century, the baluster and seal ending becomes common, the bowl being fig-shaped. During The Restoration, the handle becomes broad and flat, the bowl is broad and oval and the termination is cut into the shape known as the hind's foot.\nIn the first quarter of the 18th century, the bowl becomes narrow and elliptical, with a tongue or rat's tail down the back, and the handle is turned up at the end.\nThe modern form, with the tip of the bowl narrower than the base and the rounded end of the handle turned down, came into use about 1760.\nTypes and uses.\nSpoons are used primarily for eating liquid or semi-liquid foods, such as soup, stew or ice cream, and very small or powdery solid items which cannot be easily lifted with a fork, such as rice, sugar, cereals and green peas. In Southeast Asia, spoons are the primary utensil used for eating; forks are used to push foods such as rice onto the spoon as well as their western usage for piercing the food.\nSpoons are also widely used in cooking and serving. In baking, batter is usually thin enough to pour or drop from a spoon; a mixture of such consistency is sometimes called \"drop batter\". Rolled dough dropped from a spoon to a cookie sheet can be made into rock cakes and other cookies, while johnnycake may be prepared by dropping spoonfuls of cornmeal onto a hot greased griddle.\nA spoon is similarly useful in processing jelly, sugar and syrup. A test sample of jelly taken from a boiling mass may be allowed to slip from a spoon in a sheet, in a step called \"sheeting\". At the \"crack\" stage, syrup from boiling sugar may be dripped from a spoon, causing it to break with a snap when chilled. When boiled to 240\u00a0\u00b0F. and poured from a spoon, sugar forms a filament, or \"thread\". Hot syrup is said to \"pearl\" when it forms such a long thread without breaking when dropped from a spoon.\nUsed for stirring, a spoon is passed through a substance with a continued circular movement for the purpose of mixing, blending, dissolving, cooling, or preventing sticking of the ingredients. Mixed drinks may be \"muddled\" by working a spoon to crush and mix ingredients such as mint and sugar on the bottom of a glass or mixer. Spoons are employed for mixing certain kinds of powder into water to make a sweet or nutritious drink.\nA spoon may also be employed to toss ingredients by mixing them lightly until they are well coated with a dressing.\nFor storage, spoons and knives were sometimes placed in paired \"knife boxes\", which were often ornate wooden containers with sloping tops, used especially during the 18th century. On the table, an ornamental utensil called a \"nef\", shaped like a ship, might hold a napkin, knife and spoon.\nLanguage and culture.\nSpoons are mentioned in the Bible (KJV): God in the Book of Exodus tells Moses to make for Tabernacle, among other things, spoons of gold.\nThe expression \"born with a silver spoon in his mouth\" (born into privilege) formed due to the mediaeval custom of gifting a \"baptismal spoon\" to a child; well-to-do families were able to afford spoons made of precious metals. \nSpoons can be used as a musical instrument.\nTo \"spoon-feed\" oneself or another can simply mean to feed by means of a spoon. Metaphorically, however, it often means to present something to a person or group so thoroughly or wholeheartedly as to preclude the need for independent thought, initiative or self-reliance on the part of the recipient; or to present information in a slanted version, with the intent to preclude questioning or revision. Someone who accepts passively what has been offered in this way is said to have been spoon-fed.\nA \"spoonful\" is the amount of material a spoon contains or can contain. It is used as a standard unit of measure for volume in cooking, where it normally signifies a teaspoonful. It is abbreviated \"coch\" or \"cochl\", from , a small Roman spoon. \"Teaspoonful\" is often used in a similar way to describe the dosage for over the counter medicines. Dessert spoonful and tablespoonful may also be found in drink and food recipes. A teaspoon holds about 5 ml and a tablespoon about 15 ml.\nThe souvenir spoon generally exists solely as a decorative object commemorating an event, place, or special date.\nManufacture.\nFor machine-made spoons, the basic shape is cut out from a sheet of sterling silver, nickel silver alloy or stainless steel. The bowl is cross rolled between two pressurized rollers to produce a thinner section. The handle section is also rolled to produce the width required for the top end. The blank is then cropped to the required shape, and two dies are used to apply the pattern to the blank. The flash is then removed using a linisher, and the bowl is formed between two dies and bent.\nTo make a spoon the traditional way by way of hand forging, a bar of silver is marked up to the correct proportions for the bowl and handle.\nIt is then heated until red hot and held in tongs, and using the hammer and anvil, beaten into shape. The tip of the bar is pointed to form the tip of the bowl, then hammered to form the bowl. If a heel is to be added, a section down the centre is left thicker. The edges of the bowl and the tip of the spoon are left thicker as this is where most of the thickness is needed. The handle is then started and hammered out to length going from thick at the neck and gradually tapering down in thickness giving a balanced feel. During this process, the piece becomes very hard and has to be annealed several times, then worked again until the final shape is achieved.\nThe bowl is filed to shape, often using a metal template. The bowl is then formed using a tin cake and spoon stake. The molten tin is poured around the spoon stake and left to harden. The handle is then bent down to 45 degrees, and the spoon is hammered into the tin using the spoon stake and a heavy hammer, to form the bowl. The bend in the handle is then adjusted to match the other spoons in the set so that it sits correctly on the table. The bowl is then filed level, a process called striking off. The surfaces are filed, first with a rough file to remove the fire stain from the surface, then with a smooth file. It is then buffed to remove any file marks and fire stain from inside the bowl and is polished to the desired finish.\nDerivatives.\nBoth the spork and the sporf are derived from the spoon: they combine the bowl of the spoon with the tines of the fork and with both tines and the cutting edge of the knife, respectively.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46852", "revid": "28078513", "url": "https://en.wikipedia.org/wiki?curid=46852", "title": "George I of Great Britain", "text": "King of Great Britain and Ireland from 1714 to 1727\nGeorge I (George Louis; ; 28 May 1660 \u2013 11 June 1727) was King of Great Britain and Ireland from 1 August 1714 and ruler of the Electorate of Hanover within the Holy Roman Empire from 23 January 1698 until his death in 1727. He was the first British monarch of the House of Hanover.\nBorn in Hanover to Ernest Augustus and Sophia of Hanover, George inherited the titles and lands of the Duchy of Brunswick-L\u00fcneburg from his father and uncles. In 1682, he married his cousin Sophia Dorothea of Celle, with whom he had two children; he also had three daughters with his mistress Melusine von der Schulenburg. George and Sophia Dorothea divorced in 1694. A succession of European wars expanded George's German domains during his lifetime; he was ratified as prince-elector of Hanover in 1708. \nAs the senior Protestant descendant of his great-grandfather James VI and I, George inherited the British throne following the deaths in 1714 of his mother, Sophia, and his second cousin Anne, Queen of Great Britain. During his reign the powers of the monarchy diminished, and Britain began a transition to the modern system of cabinet government led by a prime minister. Jacobites attempted, but failed, to depose George and replace him with James Francis Edward Stuart, Anne's Catholic half-brother. Towards the end of his reign, actual political power was held by Robert Walpole, now recognised as Britain's first \"de facto\" prime minister. \nGeorge died in 1727 on a journey to his native Hanover, where he was buried. He is the most recent British monarch to be buried outside the United Kingdom.\nEarly life.\nGeorge was born on 28 May 1660 in the city of Hanover in the Duchy of Brunswick-L\u00fcneburg in the Holy Roman Empire. He was the eldest son of Ernest Augustus, Duke of Brunswick-L\u00fcneburg, and his wife, Sophia of the Palatinate. Sophia was the granddaughter of King James I of England, through her mother, Elizabeth Stuart, Queen of Bohemia.\nFor the first year of his life George was the only heir to the German territories of his father and three childless uncles. George's brother, Frederick Augustus, was born in 1661, and the two boys (known respectively by the family as \"G\u00f6rgen\" and \"Gustchen\") were brought up together. In 1662 the family moved to Osnabr\u00fcck when Ernest Augustus was appointed ruler of the Prince-Bishopric of Osnabr\u00fcck, while his older brother George William ruled in Hanover. They lived at Iburg Castle outside the city until 1673 when they moved to the newly completed . The parents were absent for almost a year (1664\u20131665) during a long convalescent holiday in Italy but Sophia corresponded regularly with her sons' governess and took a great interest in their upbringing, even more so upon her return. Sophia and Ernest Augustus had another four sons and a daughter. In her letters Sophia describes George as a responsible, conscientious child who set an example to his younger brothers and sisters.\nBy 1675 George's eldest uncle had died without issue, but his remaining two uncles had married, putting George's inheritance in jeopardy, for his uncles' estates might pass to their own sons, were they to have any, instead of to George. George's father took him hunting and riding and introduced him to military matters; mindful of his uncertain future, Ernest Augustus took the fifteen-year-old George on campaign in the Franco-Dutch War with the deliberate purpose of testing and training his son in battle.\nIn 1679 another uncle died unexpectedly without sons, and Ernest Augustus became reigning Duke of Calenberg-G\u00f6ttingen, with his capital at Hanover. George's surviving uncle, George William of Celle, had married his mistress in order to legitimise his only daughter, Sophia Dorothea, but looked unlikely to have any further children. Under Salic law, where inheritance of territory was restricted to the male line, the succession of George and his brothers to the territories of their father and uncle now seemed secure. In 1682 the family agreed to adopt the principle of primogeniture, meaning George would inherit all the territory and not have to share it with his brothers.\nMarriage.\nIn 1682, George married Sophia Dorothea of Celle, the daughter of his uncle George William, thereby securing additional incomes that would have been outside Salic laws. This marriage of state was arranged primarily to ensure a healthy annual income, and assisted the eventual unification of Hanover and Celle. His mother at first opposed the marriage because she looked down on Sophia Dorothea's mother, Eleonore (who came from lower French nobility), and because she was concerned by Sophia Dorothea's legitimated status. She was eventually won over by the advantages inherent in the marriage.\nIn 1683, George and his brother Frederick Augustus served in the Great Turkish War at the Battle of Vienna, and Sophia Dorothea bore George a son, George Augustus. The following year, Frederick Augustus was informed of the adoption of primogeniture, meaning he would no longer receive part of his father's territory as he had expected. This led to a breach between Frederick Augustus and his father, and between the brothers, that lasted until his death in battle in 1690. With the imminent formation of a single Hanoverian state, and the Hanoverians' continuing contributions to the Empire's wars, Ernest Augustus was made a prince-elector of the Holy Roman Empire in 1692, pending confirmation by the Imperial Diet. George's prospects were now better than ever as the sole heir to his father's electorate and his uncle's duchy.\nSophia Dorothea had a second child, a daughter named after her, in 1687, but there were no other pregnancies. The couple became estranged\u2014George preferred the company of his mistress, Melusine von der Schulenburg, and Sophia Dorothea had her own romance with the Swedish Count Philip Christoph von K\u00f6nigsmarck. Threatened with the scandal of an elopement, the Hanoverian court, including George's brothers and mother, urged the lovers to desist, but to no avail. According to diplomatic sources from Hanover's enemies, in July 1694, the Swedish count was killed, possibly with George's connivance, and his body thrown into the river Leine weighted with stones. The murder was claimed to have been committed by four of Ernest Augustus's courtiers, one of whom, Don Nicol\u00f2 Montalbano, was paid the enormous sum of 150,000 thalers, about one hundred times the annual salary of the highest-paid minister. Later rumours supposed that K\u00f6nigsmarck was hacked to pieces and buried beneath the Hanover palace floorboards. However, sources in Hanover itself, including Sophia, denied any knowledge of K\u00f6nigsmarck's whereabouts.\nGeorge's marriage to Sophia Dorothea was dissolved, not on the grounds that either of them had committed adultery, but on the grounds that Sophia Dorothea had abandoned her husband. With her father's agreement, George had Sophia Dorothea imprisoned in Ahlden House in her native Celle, where she stayed until she died more than thirty years later. She was denied access to her children and father, forbidden to remarry and only allowed to walk unaccompanied within the mansion courtyard. She was, however, endowed with an income, establishment, and servants, and allowed to ride in a carriage outside her castle under supervision. Melusine von der Schulenburg acted as George's hostess openly from 1698 until his death, and they had three daughters together, born in 1692, 1693 and 1701.\nElectoral reign.\nErnest Augustus died on 23 January 1698, leaving all of his territories to George with the exception of the Prince-Bishopric of Osnabr\u00fcck, an office he had held since 1661. George thus became Duke of Brunswick-L\u00fcneburg (also known as Hanover, after its capital) as well as Archbannerbearer and a prince-elector of the Holy Roman Empire. His court in Hanover was graced by many cultural icons such as the mathematician and philosopher Gottfried Leibniz and the composers George Frideric H\u00e4ndel and Agostino Steffani.\nShortly after George's accession to his paternal duchy, Prince William, Duke of Gloucester, who was second-in-line to the English and Scottish thrones, died. By the terms of the English Act of Settlement 1701, George's mother, Sophia, was designated as the heir to the English throne if the then reigning monarch, William III, and his sister-in-law, Anne, died without surviving issue. The succession was so designed because Sophia was the closest Protestant relative of the British royal family. Fifty-six Catholics with superior hereditary claims were bypassed. The likelihood of any of them converting to Protestantism for the sake of the succession was remote; some had already refused.\nIn August 1701, George was invested with the Order of the Garter and, within six weeks, the nearest Catholic claimant to the thrones, the former king James II, died. William III died the following March and was succeeded by Anne. Sophia became heiress presumptive to the new Queen of England. Sophia was in her seventy-first year, thirty-five years older than Anne, but she was very fit and healthy and invested time and energy in securing the succession either for herself or for her son. However, it was George who understood the complexities of English politics and constitutional law, which required further acts in 1705 to naturalise Sophia and her heirs as English subjects, and to detail arrangements for the transfer of power through a regency council. In the same year, George's surviving uncle died and he inherited further German dominions: the Principality of L\u00fcneburg-Grubenhagen, centred at Celle.\nShortly after George's accession in Hanover, the War of the Spanish Succession (1701\u20131714) broke out. At issue was the right of Philip, the grandson of King Louis XIV of France, to succeed to the Spanish throne under the terms of King Charles II of Spain's will. The Holy Roman Empire, the United Dutch Provinces, England, Hanover and many other German states opposed Philip's right to succeed because they feared that the French House of Bourbon would become too powerful if it also controlled Spain. As part of the war effort, George invaded his neighbouring state, Brunswick-Wolfenb\u00fcttel, which was pro-French, writing out some of the battle orders himself. The invasion succeeded with few lives lost. As a reward, the prior Hanoverian annexation of the Duchy of Saxe-Lauenburg by George's uncle was recognised by the British and Dutch.\nIn 1706, Maximilian II Emanuel, Elector of Bavaria was deprived of his offices and titles for siding with Louis against the Empire. The following year, George was invested as an Imperial Field Marshal with command of the Imperial army stationed along the Rhine. His tenure was not altogether successful, partly because he was deceived by his ally, John Churchill, 1st Duke of Marlborough, into a diversionary attack, and partly because Emperor Joseph I appropriated the funds necessary for George's campaign for his own use. Despite this, the German princes thought he had acquitted himself well. In 1708, they formally confirmed George's position as a prince-elector in recognition of, or because of, his service. George did not hold Marlborough's actions against him; he understood they were part of a plan to lure French forces away from the main attack.\nIn 1709, George resigned as field marshal, never to go on active service again. In 1710, he was granted the dignity of Arch-Treasurer of the Empire, an office formerly held by the Elector Palatine; the absence of the Elector of Bavaria allowed a reshuffling of offices. The Emperor's death in 1711 threatened to destroy the balance of power in the opposite direction, so the war ended in 1713 with the ratification of the Treaty of Utrecht. Philip was allowed to succeed to the Spanish throne but removed from the French line of succession, and the Elector of Bavaria was restored.\nAccession in Great Britain and Ireland.\nThough both England and Scotland recognised Anne as their queen, only the Parliament of England had settled on Sophia, Electress of Hanover, as the heir presumptive. The Parliament of Scotland (the Estates) had not formally settled the succession question for the Scottish throne. In 1703, the Estates passed a bill declaring that their selection for Queen Anne's successor would not be the same individual as the successor to the English throne, unless England granted full freedom of trade to Scottish merchants in England and its colonies. At first Royal Assent was withheld, but the following year Anne capitulated to the wishes of the Estates and assent was granted to the bill, which became the Act of Security 1704. In response the English Parliament passed the Alien Act 1705, which threatened to restrict Anglo-Scottish trade and cripple the Scottish economy if the Estates did not agree to the Hanoverian succession. Eventually, in 1707, both Parliaments agreed on a Treaty of Union, which united England and Scotland into a single political entity, the Kingdom of Great Britain, and established the rules of succession as laid down by the Act of Settlement 1701. The union created the largest free trade area in 18th-century Europe.\nWhig politicians believed Parliament had the right to determine the succession, and to bestow it on the nearest Protestant relative of the Queen, while many Tories were more inclined to believe in the hereditary right of the Catholic Stuarts, who were nearer relations. In 1710, George announced that he would succeed in Britain by hereditary right, as the right had been removed from the Stuarts, and he retained it. \"This declaration was meant to scotch any Whig interpretation that parliament had given him the kingdom\u00a0[and] ...\u00a0convince the Tories that he was no usurper.\"\nGeorge's mother, the Electress Sophia, died on 28 May 1714 at the age of 83. She had collapsed in the gardens at Herrenhausen after rushing to shelter from a shower of rain. George was now Queen Anne's heir presumptive. He swiftly revised the membership of the regency council that would take power after Anne's death, as it was known that Anne's health was failing and politicians in Britain were jostling for power. She suffered a stroke, which left her unable to speak, and died on 1 August 1714. The list of regents was opened, the members sworn in, and George was proclaimed King of Great Britain and King of Ireland. Partly due to contrary winds, which kept him in The Hague awaiting passage, he did not arrive in Britain until 18 September. George was crowned at Westminster Abbey on 20 October. His coronation was accompanied by rioting in over twenty towns in England.\nGeorge mainly lived in Great Britain after 1714, though he visited his home in Hanover in 1716, 1719, 1720, 1723 and 1725. In total, George spent about one fifth of his reign as king in Germany. A clause in the Act of Settlement that forbade the British monarch from leaving the country without Parliament's permission was unanimously repealed in 1716. During all but the first of the King's absences, power was vested in a regency council rather than in his son, George Augustus, Prince of Wales.\nWars and rebellions.\nWithin a year of George's accession the Whigs won an overwhelming victory in the general election of 1715. Several members of the defeated Tory Party sympathised with the Jacobites, who sought to replace George with Anne's Catholic half-brother, James Francis Edward Stuart (called \"James III and VIII\" by his supporters and \"the Pretender\" by his opponents). Some disgruntled Tories sided with a Jacobite rebellion, which became known as \"The Fifteen\". James's supporters, led by John Erskine, Earl of Mar, a Scottish nobleman who had previously served as a secretary of state, instigated rebellion in Scotland where support for Jacobitism was stronger than in England. \"The Fifteen\", however, was a dismal failure; Lord Mar's battle plans were poor, and James arrived late with too little money and too few arms. By the end of the year the rebellion had all but collapsed. In February 1716, facing defeat, James and Lord Mar fled to France. After the rebellion was defeated, although there were some executions and forfeitures, George acted to moderate the Government's response, showed leniency, and spent the income from the forfeited estates on schools for Scotland and paying off part of the national debt.\nGeorge's distrust of the Tories aided the passing of power to the Whigs. Whig dominance grew to be so great under George that the Tories did not return to power for another half-century. After the election, the Whig-dominated Parliament passed the Septennial Act 1715, which extended the maximum duration of Parliament to seven years (although it could be dissolved earlier by the Sovereign). Thus Whigs already in power could remain in such a position for a greater period of time.\nAfter his accession in Great Britain, George's relationship with his son (which had always been poor) worsened. Prince George Augustus encouraged opposition to his father's policies, including measures designed to increase religious freedom in Britain and expand Hanover's German territories at Sweden's expense. In 1717, the birth of a grandson led to a major quarrel between George and the Prince of Wales. The King, supposedly following custom, appointed the Lord Chamberlain (Thomas Pelham-Holles, 1st Duke of Newcastle) as one of the baptismal sponsors of the child. The King was angered when the Prince of Wales, disliking Newcastle, verbally insulted the Duke at the christening, which the Duke misunderstood as a challenge to a duel. The Prince was told to leave the royal residence, St. James's Palace. The Prince's new home, Leicester House, became a meeting place for the King's political opponents. The King and his son were later reconciled at the insistence of Robert Walpole and the desire of the Princess of Wales, who had moved out with her husband but missed her children, who had been left in the King's care. Nevertheless, father and son were never again on cordial terms.\nGeorge was active in directing British foreign policy during his early reign. In 1717, he contributed to the creation of the Triple Alliance, an anti-Spanish league composed of Great Britain, France and the Dutch Republic. In 1718, Austria was added to the body, which became known as the Quadruple Alliance. The subsequent War of the Quadruple Alliance involved the same issue as the War of the Spanish Succession. The 1713 Treaty of Utrecht had recognised the grandson of Louis XIV of France, Philip V, as king of Spain on the condition that he gave up his rights to succeed to the French throne. But upon Louis XIV's 1715 death, Philip sought to overturn the treaty.\nSpain supported a Jacobite-led invasion of Scotland in 1719, but stormy seas allowed only about three hundred Spanish troops to reach Scotland. A base was established at Eilean Donan Castle on the west Scottish coast in April, only to be destroyed by British ships a month later. Jacobite attempts to recruit Scottish clansmen yielded a fighting force of only about a thousand men. The Jacobites were poorly equipped and were easily defeated by British artillery at the Battle of Glen Shiel. The clansmen dispersed into the Highlands, and the Spaniards surrendered. The invasion never posed any serious threat to George's government. With the French now fighting against him, Philip's armies fared poorly. As a result, the Spanish and French thrones remained separate. Simultaneously, Hanover gained from the resolution of the Great Northern War, which had been caused by rivalry between Sweden and Russia for control of the Baltic. The Swedish territories of Bremen-Verden were ceded to Hanover in 1719, with Hanover paying Sweden monetary compensation for the loss of territory.\nMinistries.\nIn Hanover, George was an absolute monarch, albeit within the laws of the Holy Roman Empire. All government expenditure above 50 thalers (between 12 and 13 British pounds), and the appointment of all army officers, all ministers, and even government officials above the level of copyist, was in his personal control. By contrast in Great Britain, George had to govern through Parliament.\nIn 1715 when the Whigs came to power, George's chief ministers included Robert Walpole, Lord Townshend (Walpole's brother-in-law), Lord Stanhope and Lord Sunderland. In 1717 Townshend was dismissed, and Walpole resigned from the Cabinet over disagreements with their colleagues; Stanhope became supreme in foreign affairs, and Sunderland the same in domestic matters.\nLord Sunderland's power began to wane in 1719. He introduced a Peerage Bill that attempted to limit the size of the House of Lords by restricting new creations. The measure would have solidified Sunderland's control of the House by preventing the creation of opposition peers, but it was defeated after Walpole led the opposition to the bill by delivering what was considered \"the most brilliant speech of his career\". Walpole and Townshend were reappointed as ministers the following year and a new, supposedly unified, Whig government formed.\nGreater problems arose over financial speculation and the management of the national debt. Certain government bonds could not be redeemed without the consent of the bondholder and had been issued when interest rates were high; consequently each bond represented a long-term drain on public finances, as bonds were hardly ever redeemed. In 1719, the South Sea Company proposed to take over \u00a331\u00a0million (three fifths) of the British national debt by exchanging government securities for stock in the company. The Company bribed Lord Sunderland, George's mistress Melusine von der Schulenburg, Duchess of Kendal, and Lord Stanhope's cousin, Secretary of the Treasury Charles Stanhope, to support their plan. The Company enticed bondholders to convert their high-interest, irredeemable bonds to low-interest, easily tradeable stocks by offering apparently preferential financial gains. Company prices rose rapidly; the shares had cost \u00a3128 on 1 January 1720, but were valued at \u00a3500 when the conversion scheme opened in May. On 24 June the price reached a peak of \u00a31,050. The company's success led to the speculative flotation of other companies, some of a bogus nature, and the Government, in an attempt to suppress these schemes and with the support of the company, passed the Bubble Act. With the rise in the market now halted, uncontrolled selling began in August, which caused the stock to plummet to \u00a3150 by the end of September. Many individuals\u2014including aristocrats\u2014lost vast sums and some were completely ruined. George, who had been in Hanover since June, returned to London in November\u2014sooner than he wanted or was usual\u2014at the request of the ministry.\nThe economic crisis, known as the South Sea Bubble, made George and his ministers extremely unpopular. In 1721, Lord Stanhope, though personally innocent, collapsed and died after a stressful debate in the House of Lords, and Lord Sunderland resigned from public office.\nSunderland, however, retained a degree of personal influence with George until his sudden death in 1722 allowed the rise of Robert Walpole. Walpole became \"de facto\" Prime Minister, although the title was not formally applied to him (officially, he was First Lord of the Treasury and Chancellor of the Exchequer). His management of the South Sea crisis, by rescheduling the debts and arranging some compensation, helped the return to financial stability. Through Walpole's skilful management of Parliament, George managed to avoid direct implication in the company's fraudulent actions. Claims that George had received free stock as a bribe are not supported by evidence; indeed receipts in the Royal Archives show that he paid for his subscriptions and that he lost money in the crash.\nLater years.\nAs requested by Walpole, George revived the Order of the Bath in 1725, which enabled Walpole to reward or gain political supporters by offering them the honour. Walpole became extremely powerful and was largely able to appoint ministers of his own choosing. Unlike his predecessor, Queen Anne, George rarely attended meetings of the cabinet; most of his communications were in private, and he only exercised substantial influence with respect to British foreign policy. With the aid of Lord Townshend, he arranged for the ratification by Great Britain, France and Prussia of the Treaty of Hanover, which was designed to counterbalance the Austro-Spanish Treaty of Vienna and protect British trade.\nGeorge, although increasingly reliant on Walpole, could still have replaced his ministers at will. Walpole was actually afraid of being removed from office towards the end of George I's reign, but such fears were put to an end when George died during his sixth trip to his native Hanover since his accession as king. He suffered a stroke on the road between Delden and Nordhorn on 9 June 1727, and was taken by carriage about 55 miles to the east, to the palace of his younger brother, Ernest Augustus, Prince-Bishop of Osnabr\u00fcck, where he died two days after arrival in the early hours before dawn on 11 June 1727. George I was buried in the chapel of Leine Palace in Hanover, but his remains were moved to the chapel at Herrenhausen Gardens after World War II. Leine Palace was entirely burnt out as a result of Allied air raids and the King's remains, along with his parents', were moved to the 19th-century mausoleum of King Ernest Augustus in the Berggarten.\nGeorge was succeeded by his son, George Augustus, who took the throne as George II. It was widely assumed, even by Walpole for a time, that George II planned to remove Walpole from office but was dissuaded from doing so by his wife, Caroline of Ansbach. However, Walpole commanded a substantial majority in Parliament and George II had little choice but to retain him or risk ministerial instability.\nEvaluations and legacy.\nGeorge I's accession secured the Protestant succession and prevented a Catholic restoration under the \"Old Pretender\" (James Stuart). The result was constitutional continuity after the turbulent Stuart period. The king's limited English and disinterest in day-to-day British affairs left a vacuum that was filled by the rise of modern parliamentary democracy. This enabled Robert Walpole to seize the initiative and emerge as Britain's first de facto Prime Minister; his new style of cabinet governance filled the royal vacuum. The British economy tripled in size in the 18th century and prosperity marked George's reign. There was continued expansion of trade and commerce, building on the financial innovations of the previous decades. The South Sea Bubble of 1720 was a brief setback but it also led to improved financial regulation. The consolidation of Whig party dominance brought political stability and supported policies favouring commercial interests and religious toleration for Protestant dissenters.\nHowever, the king was widely disliked as a foreigner who was more interested in Hanover than in Britain. This fuelled dissent and strengthened Jacobite plans to overthrow the Hanoverians. Corruption was widespread, as exemplified by the South Sea Bubble scandal that financially ruined many investors\u2014the king himself lost heavily in it as did Walpole. The king's bitter relationship with his son (the future George II) created political factions and instability at court, weakening the monarchy's prestige. George was ridiculed by his British subjects as unintelligent and wooden. His treatment of his wife, Sophia Dorothea, became something of a scandal. His Lutheran faith, his overseeing both the Lutheran churches in Hanover and the Church of England, and the presence of Lutheran preachers in his court caused some consternation among his Anglican subjects.\nThe British distrusted George as too German, and spread false rumours about supposed German mistresses. However, in mainland Europe, he was seen as a progressive ruler supportive of the Enlightenment who permitted his critics to publish without risk of severe censorship, and provided sanctuary to Voltaire when the philosopher was exiled from Paris in 1726. European and British sources agree that George was reserved, temperate and financially prudent; he disliked being in the public light at social events, avoided the royal box at the opera and often travelled incognito to the homes of friends to play cards. Despite some unpopularity, the Protestant George I was seen by most of his subjects and Parliament as infinitely preferable to the Catholic pretender James. William Makepeace Thackeray indicates such ambivalent feelings as he wrote: \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;His heart was in Hanover\u00a0... He was more than fifty years of age when he came amongst us: we took him because we wanted him, because he served our turn; we laughed at his uncouth German ways, and sneered at him. He took our loyalty for what it was worth; laid hands on what money he could; kept us assuredly from Popery\u00a0... I, for one, would have been on his side in those days. Cynical and selfish, as he was, he was better than a king out of St. Germains [James, the Stuart Pretender] with the French king's orders in his pocket, and a swarm of Jesuits in his train.\nWriters of the nineteenth century, such as Thackeray, Walter Scott and Lord Mahon, were reliant on biased first-hand accounts published in the previous century such as Lord Hervey's memoirs, and looked back on the Jacobite cause with romantic, even sympathetic, eyes. They in turn, influenced British authors of the first half of the twentieth century such as G. K. Chesterton, who introduced further anti-German and anti-Protestant bias into the interpretation of George's reign. However, in the wake of World War II continental European archives were opened to historians of the later twentieth century and nationalistic anti-German feeling subsided. George's life and reign were re-explored by scholars such as J. M. Beattie and Ragnhild Hatton, and his character, abilities and motives re-assessed in a more generous light. For example, though he was unpopular in Britain due to his supposed inability to speak English, documents from later in his reign show that he understood, spoke and wrote English. He spoke fluent German and French, good Latin, and some Italian and Dutch. John H. Plumb noted that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Some historians have exaggerated the king's indifference to English affairs and made his ignorance of the English language seem more important than it was. He had little difficulty in communicating with his ministers in French, and his interest in all matters affecting both foreign policy and the court was profound.\nThe character of George I remains elusive; he was in turn genial and affectionate in private letters to his daughter, and then dull and awkward in public. Perhaps his own mother summed him up when \"explaining to those who regarded him as cold and overserious that he could be jolly, that he took things to heart, that he felt deeply and sincerely and was more sensitive than he cared to show.\" Whatever his true character, he ascended a precarious throne, and either by political wisdom and guile, or through accident and indifference, he left it secure in the hands of the Hanoverians and of Parliament.\nArms.\nAs king, his arms were: Quarterly, I, Gules three lions passant guardant in pale Or (for England) impaling Or a lion rampant within a tressure flory-counter-flory Gules (for Scotland); II, Azure three fleurs-de-lis Or (for France); III, Azure a harp Or stringed Argent (for Ireland); IV, tierced per pale and per chevron (for Hanover), I Gules two lions passant guardant Or (for Brunswick), II Or a semy of hearts Gules a lion rampant Azure (for L\u00fcneburg), III Gules a horse courant Argent (for Westphalia), overall an escutcheon Gules charged with the crown of Charlemagne Or (for the dignity of Archtreasurer of the Holy Roman Empire).\nIssue and mistresses.\nMistresses.\nIn addition to Melusine von der Schulenburg, three other women were said to be George's mistresses:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46853", "revid": "50345263", "url": "https://en.wikipedia.org/wiki?curid=46853", "title": "Indus Valley Civilisation", "text": "Bronze Age civilisation in South Asia\nThe Indus Valley Civilisation (IVC), also known as the Indus Civilisation, was a Bronze Age civilisation in the northwestern regions of South Asia, lasting from 3300\u00a0BCE to 1300\u00a0BCE, and in its mature form from 2600\u00a0BCE to 1900\u00a0BCE. Together with ancient Egypt and Mesopotamia, it was one of three early civilisations of the Near East and South Asia. Of the three, it was the most widespread: it spanned much of Pakistan, northwestern India, and northeast Afghanistan. The civilisation flourished both in the alluvial plain of the Indus River, which flows through the length of Pakistan, and along a system of perennial monsoon-fed rivers that once coursed in the vicinity of the Ghaggar-Hakra, a seasonal river in northwest India and eastern Pakistan.\nThe term \"Harappan\" is also applied to the Indus Civilisation, after its type site Harappa, the first to be excavated early in the 20th\u00a0century in what was then the Punjab province of British India and is now Punjab, Pakistan. The discovery of Harappa and soon afterwards Mohenjo-daro was the culmination of work that had begun after the founding of the Archaeological Survey of India in the British Raj in 1861. There were earlier and later cultures called Early Harappan and Late Harappan in the same area. The early Harappan cultures were populated from Neolithic cultures, the earliest and best-known of which is named after Mehrgarh, in Balochistan, Pakistan. Harappan civilisation is sometimes called \"Mature Harappan\" to distinguish it from the earlier cultures.\nThe cities of the ancient Indus were noted for their urban planning, baked brick houses, elaborate drainage systems, water supply systems, clusters of large non-residential buildings, and techniques of handicraft and metallurgy. Mohenjo-daro and Harappa very likely grew to contain between 30,000 and 60,000 individuals, and the civilisation may have contained between one and five million individuals during its florescence. A gradual drying of the region during the 3rd\u00a0millennium BCE may have been the initial stimulus for its urbanisation. Eventually it also reduced the water supply enough to cause the civilisation's demise and to disperse its population to the east.\nAlthough over a thousand Mature Harappan sites have been reported and nearly a hundred excavated, there are only five\u00a0major urban centres: Mohenjo-daro in the lower Indus Valley (declared a UNESCO World Heritage Site in 1980 as \"\"Archaeological Ruins at Moenjodaro\"), Harappa in the western Punjab region, Ganeriwala in the Cholistan Desert, Dholavira in western Gujarat (declared a UNESCO World Heritage Site in 2021 as \"Dholavira: A Harappan City\"\"), and Rakhigarhi in Haryana. The Harappan language is not directly attested, and its affiliations are uncertain, as the Indus script has remained undeciphered. A relationship with the Dravidian or Elamo-Dravidian language family is favoured by a section of scholars.\nEtymology.\nThe Indus civilisation is \nnamed after the Indus River system in whose alluvial plains the early sites of the civilisation were identified and excavated.\nFollowing a tradition in archaeology, the civilisation is sometimes referred to as the \"Harappan,\" after its type site, Harappa, the first site to be excavated in the 1920s; this is notably true of usage employed by the Archaeological Survey of India after India's independence in 1947.\nThe term \"Ghaggar-Hakra\" figures prominently in modern labels applied to the Indus civilisation on account of a good number of sites having been found along the Ghaggar-Hakra River in northwest India and eastern Pakistan. The terms \"Indus-Sarasvati Civilisation\" and \"Sindhu-Saraswati Civilisation\" have also been employed in the literature by supporters of Indigenous Aryanism, after a posited identification of the Ghaggar-Hakra with the river Sarasvati described in the early chapters of the \"Rigveda\", a collection of hymns in archaic Sanskrit composed in the second-millennium BCE, which are unrelated to the mature phase of the Indus Valley Civilization.\nRecent geophysical research suggests that, unlike the Sarasvati described in the Rigveda as a snow-fed river the Ghaggar-Hakra was a system of perennial monsoon-fed rivers, which became seasonal around the time the civilisation diminished, approximately 4,000 years ago.\nExtent.\nThe Indus Valley Civilisation was roughly contemporary with the other riverine civilisations of the ancient world: Ancient Egypt along the Nile, Mesopotamia in the lands watered by the Euphrates and the Tigris, and China in the drainage basin of the Yellow River and the Yangtze. By the time of its mature phase, the civilisation had spread over an area larger than the others, which included a core of up the alluvial plain of the Indus and its tributaries. In addition, there was a region with disparate flora, fauna, and habitats, up to ten times as large, which had been shaped culturally and economically by the Indus.\nAround 6500\u00a0BCE, agriculture emerged in Balochistan, on the margins of the Indus alluvium. In the following millennia, settled life made inroads into the Indus plains, setting the stage for the growth of rural and urban settlements. The more organised sedentary life, in turn, led to a net increase in the birth rate. The large urban centres of Mohenjo-daro and Harappa very likely grew to containing between 30,000 and 60,000\u00a0individuals, and during the civilisation's florescence, the population of the subcontinent grew to between 4\u20136\u00a0million people. During this period the death rate increased, as the close living conditions of humans and domesticated animals led to an increase in contagious diseases. According to one estimate, the population of the Indus civilisation at its peak may have been between one and five million.\nDuring its height the civilisation extended from Balochistan in the west to western Uttar Pradesh in the east, from northeastern Afghanistan in the north to Gujarat state in the south. The largest number of sites are in the Punjab region, Gujarat, Haryana, Rajasthan, Uttar Pradesh, Jammu and Kashmir states, Sindh, and Balochistan. Coastal settlements extended from Sutkagan Dor in Western Baluchistan to Lothal in Gujarat. An Indus Valley site has been found on the Oxus River at Shortugai in Afghanistan which is the northernmost site of the Indus Valley Civilisation, in the Gomal River valley in northwestern Pakistan, at Manda, Jammu on the Beas River near Jammu, and at Alamgirpur on the Hindon River, only from Delhi. The southernmost site of the Indus Valley Civilisation is Daimabad in Maharashtra. Indus Valley sites have been found most often on rivers, but also on the ancient seacoast, for example, Balakot (Kot Bala), and on islands, for example, Dholavira.\nDiscovery and history of excavation.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n \"Three other scholars whose names I cannot pass over in silence, are the late Mr. R. D. Banerji, to whom belongs the credit of having discovered, if not Mohenjo-daro itself, at any rate its high antiquity, and his immediate successors in the task of excavation, Messrs. M.S. Vats and K.N. Dikshit. ... no one probably except myself can fully appreciate the difficulties and hardships which they had to face in the three first seasons at Mohenjo-daro.\"\n\u00a0\u2014 From, John Marshall (ed), \"Mohenjo-daro and the Indus Civilization\", London: Arthur Probsthain, 1931.\nThe first modern accounts of the ruins of the Indus civilisation are those of Charles Masson, a deserter from the East India Company's army. In 1829, Masson traveled through the princely state of Punjab, gathering useful intelligence for the Company in return for a promise of clemency. An aspect of this arrangement was the additional requirement to hand over to the Company any historical artifacts acquired during his travels. Masson, who had versed himself in the classics, especially in the military campaigns of Alexander the Great, chose for his wanderings some of the same towns that had featured in Alexander's campaigns, and whose archaeological sites had been noted by the campaign's chroniclers. Masson's major archaeological discovery in the Punjab was Harappa, a metropolis of the Indus civilisation in the valley of Indus's tributary, the Ravi river. Masson made copious notes and illustrations of Harappa's rich historical artifacts, many lying half-buried. In 1842, Masson included his observations of Harappa in the book \"Narrative of Various Journeys in Baluchistan, Afghanistan, and the Punjab\". He dated the Harappa ruins to a period of recorded history, erroneously mistaking it to have been described earlier during Alexander's campaign. Masson was impressed by the site's extraordinary size and by several large mounds formed from long-existing erosion.\nTwo years later, the Company contracted Alexander Burnes to sail up the Indus to assess the feasibility of water travel for its army. Burnes, who also stopped in Harappa, noted the baked bricks employed in the site's ancient masonry, but noted also the haphazard plundering of these bricks by the local population.\nDespite these reports, Harappa was raided even more perilously for its bricks after the British annexation of the Punjab in 1848\u201349. A considerable number were carted away as track ballast for the railway lines being laid in the Punjab. Nearly of railway track between Multan and Lahore, laid in the mid-1850s, was supported by Harappan bricks.\nIn 1861, three years after the dissolution of the East India Company and the establishment of Crown rule in India, archaeology on the subcontinent became more formally organised with the founding of the Archaeological Survey of India (ASI). Alexander Cunningham, the Survey's first director-general, who had visited Harappa in 1853 and had noted the imposing brick walls, visited again to carry out a survey, but this time of a site whose entire upper layer had been stripped in the interim. Although his original goal of demonstrating Harappa to be a lost Buddhist city mentioned in the seventh century CE travels of the Chinese visitor, Xuanzang, proved elusive, Cunningham did publish his findings in 1875. For the first time, he interpreted a Harappan stamp seal, with its unknown script, which he concluded to be of an origin foreign to India.\nArchaeological work in Harappa thereafter lagged until a new viceroy of India, Lord Curzon, pushed through the Ancient Monuments Preservation Act 1904, and appointed John Marshall to lead the ASI. Several years later, Hiranand Sastri, who had been assigned by Marshall to survey Harappa, reported it to be of non-Buddhist origin, and by implication more ancient. Expropriating Harappa for the ASI under the Act, Marshall directed ASI archaeologist Daya Ram Sahni to excavate the site's two mounds.\nFarther south, along the main stem of the Indus in Sind province, the largely undisturbed site of Mohenjo-daro had attracted notice. Marshall deputed a succession of ASI officers to survey the site. These included D. R. Bhandarkar (1911), R. D. Banerji (1919, 1922\u20131923), and M. S. Vats (1924). In 1923, on his second visit to Mohenjo-daro, Baneriji wrote to Marshall about the site, postulating an origin in \"remote antiquity\", and noting a congruence of some of its artifacts with those of Harappa. Later in 1923, Vats, also in correspondence with Marshall, noted the same more specifically about the seals and the script found at both sites. On the weight of these opinions, Marshall ordered crucial data from the two sites to be brought to one location and invited Banerji and Sahni to a joint discussion. By 1924, Marshall had become convinced of the significance of the finds, and on 24 September 1924, made a tentative but conspicuous public intimation in the \"Illustrated London News\": \"Not often has it been given to archaeologists, as it was given to Schliemann at Tiryns and Mycenae, or to Stein in the deserts of Turkestan, to light upon the remains of a long forgotten civilisation. It looks, however, at this moment, as if we were on the threshold of such a discovery in the plains of the Indus.\"\nIn the next issue, a week later, the British Assyriologist Archibald Sayce was able to point to very similar seals found in Bronze Age levels in Mesopotamia and Iran, giving the first strong indication of their date; confirmations from other archaeologists followed. Systematic excavations began in Mohenjo-daro in 1924\u201325 with that of K. N. Dikshit, continuing with those of H. Hargreaves (1925\u20131926), and Ernest J. H. Mackay (1927\u20131931). By 1931, much of Mohenjo-daro had been excavated, but occasional excavations continued, such as the one led by Mortimer Wheeler, a new director-general of the ASI appointed in 1944, and including Ahmad Hasan Dani.\nAfter the partition of India in 1947, when most excavated sites of the Indus Valley Civilisation lay in territory awarded to Pakistan, the Archaeological Survey of India, its area of authority reduced, carried out large numbers of surveys and excavations along the Ghaggar-Hakra system in India. Some speculated that the Ghaggar-Hakra system might yield more sites than the Indus river basin. According to archaeologist Ratnagar, many Ghaggar-Hakra sites in India and Indus Valley sites in Pakistan are actually those of local cultures; some sites display contact with Harappan civilisation, but only a few are fully developed Harappan ones. As of 1977, about 90% of the Indus script seals and inscribed objects discovered were found at sites in Pakistan along the Indus river, while other sites accounts only for the remaining 10%. By 2002, over 1,000 Mature Harappan cities and settlements had been reported, of which just under a hundred had been excavated, mainly in the general region of the Indus and Ghaggar-Hakra rivers and their tributaries; however, there are only five major urban sites: Harappa, Mohenjo-daro, Dholavira, Ganeriwala and Rakhigarhi. As of 2008, about 616\u00a0sites have been reported in India, whereas 406\u00a0sites have been reported in Pakistan.\nUnlike India, in which after 1947, the ASI attempted to \"Indianise\" archaeological work in keeping with the new nation's goals of national unity and historical continuity, in Pakistan the national imperative was the promotion of Islamic heritage, and consequently archaeological work on early sites was left to foreign archaeologists. After the partition, Mortimer Wheeler, the Director of ASI from 1944, oversaw the establishment of archaeological institutions in Pakistan, later joining a UNESCO effort tasked to conserve the site at Mohenjo-daro. Other international efforts at Mohenjo-daro and Harappa have included the German \"Aachen Research Project Mohenjo-daro\", the \"Italian Mission to Mohenjo-daro\", and the US \"Harappa Archaeological Research Project (HARP)\" founded by George F. Dales. Following a chance flash flood which exposed a portion of an archaeological site at the foot of the Bolan Pass in Balochistan, excavations were carried out in Mehrgarh by French archaeologist Jean-Fran\u00e7ois Jarrige and his team in the early 1970s.\nChronology.\nThe cities of the ancient Indus had \"social hierarchies, their writing system, their large planned cities and their long-distance trade [which] mark them to archaeologists as a full-fledged 'civilisation.' The mature phase of the Harappan civilisation lasted from c.\u20092600\u20131900\u00a0BCE. With the inclusion of the predecessor and successor cultures \u2013 Early Harappan and Late Harappan, respectively \u2013 the entire Indus Valley Civilisation may be taken to have lasted from the 33rd to the 14th\u00a0centuries BCE. It is part of the Indus Valley Tradition, which also includes the pre-Harappan occupation of Mehrgarh, the earliest farming site of the Indus Valley.\nSeveral periodisations are employed for the IVC. The most commonly used classifies the Indus Valley Civilisation into Early, Mature and Late Harappan Phase. An alternative approach by Shaffer divides the broader Indus Valley Tradition into four eras, the pre-Harappan \"Early Food Producing Era\", and the Regionalisation, Integration, and Localisation eras, which correspond roughly with the Early Harappan, Mature Harappan, and Late Harappan phases.\nPre-Harappan era: Mehrgarh.\nMehrgarh is a Neolithic (7000\u00a0BCE to c.\u20092500\u00a0BCE) mountain site in the Balochistan province of Pakistan, which gave new insights on the emergence of the Indus Valley Civilisation. Mehrgarh is one of the earliest sites with evidence of farming and herding in South Asia. Mehrgarh was influenced by the Near Eastern Neolithic, with similarities between \"domesticated wheat varieties, early phases of farming, pottery, other archaeological artefacts, some domesticated plants and herd animals\".\nJean-Francois Jarrige argues for an independent origin of Mehrgarh. Jarrige notes \"the assumption that farming economy was introduced full-fledged from Near-East to South Asia\", and the similarities between Neolithic sites from eastern Mesopotamia and the western Indus valley, which are evidence of a \"cultural continuum\" between those sites. But given the originality of Mehrgarh, Jarrige concludes that Mehrgarh has an earlier local background, and is not a \"'backwater' of the Neolithic culture of the Near East\".\nLukacs and Hemphill suggest an initial local development of Mehrgarh, with a continuity in cultural development but a change in population. According to Lukacs and Hemphill, while there is a strong continuity between the Neolithic and Chalcolithic (Copper Age) cultures of Mehrgarh, dental evidence shows that the Chalcolithic population did not descend from the Neolithic population of Mehrgarh, which \"suggests moderate levels of gene flow\". Mascarenhas et al. (2015) note that \"new, possibly West Asian, body types are reported from the graves of Mehrgarh beginning in the Togau phase (3800\u00a0BCE).\"\nGallego Romero et al. (2011) state that their research on lactose tolerance in India suggests that \"the west Eurasian genetic contribution identified by Reich et al. (2009) principally reflects gene flow from Iran and the Middle East.\" They further note that \"[t]he earliest evidence of cattle herding in South Asia comes from the Indus River Valley site of Mehrgarh and is dated to 7,000\u00a0YBP.\"\nEarly Harappan.\nThe Early Harappan Ravi Phase, named after the nearby Ravi River, lasted from c.\u20093300 BCE until 2800\u00a0BCE. It started when farmers from the mountains gradually moved between their mountain homes and the lowland river valleys, and is related to the Hakra Phase, identified in the Ghaggar-Hakra River Valley to the west, and predates the Kot Diji Phase (2800\u20132600\u00a0BCE, Harappan\u00a02), named after a site in northern Sindh, Pakistan, near Mohenjo-daro. The earliest examples of the Indus script date to the 3rd\u00a0millennium BCE.\nThe mature phase of earlier village cultures is represented by Rehman Dheri and Amri in Pakistan. Kot Diji represents the phase leading up to Mature Harappan, with the citadel representing centralised authority and an increasingly urban quality of life. Another town of this stage was found at Kalibangan in India on the Hakra River.\nTrade networks linked this culture with related regional cultures and distant sources of raw materials, including lapis lazuli and other materials for bead-making. By this time, villagers had domesticated numerous crops, including peas, sesame seeds, dates, and cotton, as well as animals, including the water buffalo. Early Harappan communities turned to large urban centres by 2600 BCE, from where the mature Harappan phase started. The latest research shows that Indus Valley people migrated from villages to cities.\nThe final stages of the Early Harappan period are characterised by the building of large walled settlements, the expansion of trade networks, and the increasing integration of regional communities into a \"relatively uniform\" material culture in terms of pottery styles, ornaments, and stamp seals with Indus script, leading into the transition to the Mature Harappan phase.\nMature Harappan.\nAccording to Giosan et al. (2012), the slow southward migration of the monsoons across Asia initially allowed the Indus Valley villages to develop by taming the floods of the Indus and its tributaries. Flood-supported farming led to large agricultural surpluses, which in turn supported the development of cities. The IVC residents did not develop irrigation capabilities, relying mainly on the seasonal monsoons leading to summer floods. Brooke further notes that the development of advanced cities coincides with a reduction in rainfall, which may have triggered a reorganisation into larger urban centres.\nAccording to J.G. Shaffer and D.A. Lichtenstein, the Mature Harappan civilisation was \"a fusion of the Bagor, Hakra, and Kot Diji traditions or 'ethnic groups' in the Ghaggar-Hakra valley on the borders of India and Pakistan\".\nAlso, according to a more recent summary by Maisels (2003), \"The Harappan oecumene formed from a Kot Dijian/Amri-Nal synthesis\". He also says that, in the development of complexity, the site of Mohenjo-daro has priority, along with the Hakra-Ghaggar cluster of sites, \"where Hakra wares actually precede the Kot Diji related material\". He sees these areas as \"catalytic in producing the fusion from Hakra, Kot Dijian and Amri-Nal cultural elements that resulted in the gestalt we recognize as Early Harappan (Early Indus).\"\nBy 2600 BCE, the Early Harappan communities turned into large urban centres. Such urban centres include Harappa, Ganeriwala, Mohenjo-daro in modern-day Pakistan, and Dholavira, Kalibangan, Rakhigarhi, Rupar, and Lothal in modern-day India. In total, more than 1,000\u00a0settlements have been found, mainly in the general region of the Indus and Ghaggar-Hakra Rivers and their tributaries.\nCities.\nA sophisticated and technologically advanced urban culture is evident in the Indus Valley Civilisation, making them the first urban centre in the region. The high degree of forward-looking urban planning demonstrates the existence of well-organised local governments capable of formulating and executing a large-scale forward-looking development program, and which placed a high value on public health and hygiene, or, alternatively, accessibility to the means of religious ritual.\nAs seen in Harappa, Mohenjo-daro and the recently partially excavated Rakhigarhi, this urban plan included the world's first known city sanitation systems. Within the city, individual homes or groups of homes obtained water from wells. From a room that appears to have been set aside for bathing, waste water was directed to covered drains, which lined the major streets. Houses opened only to inner courtyards and smaller lanes. The housebuilding in some villages in the region still resembles in some respects the housebuilding of the Harappans.\nThe Indus Valley cities developed elaborate drainage and sewerage systems, described by archaeologists as well-planned and advanced compared with many contemporary societies. Their urban architecture included dockyards, granaries, warehouses, brick platforms, and massive protective walls, which were likely intended both as flood defenses and as fortifications.\nThe purpose of the citadel remains debated. In sharp contrast to this civilisation's contemporaries, Mesopotamia and ancient Egypt, no large monumental structures were built. There is no conclusive evidence of palaces or temples. Some structures are thought to have been granaries. Found at one city is an enormous well-built bath (the \"Great Bath\"), which may have been a public bath. Although the citadels were walled, it is far from clear that these structures were defensive; many may have been flood defenses.\nMost city dwellers appear to have been traders or artisans, who lived with others pursuing the same occupation in well-defined neighbourhoods. Materials from distant regions were used in the cities for constructing seals, beads and other objects. Among the artefacts discovered were beautiful glazed fa\u00efence beads. Steatite seals have images of animals, people (perhaps gods), and other types of inscriptions, including the yet un-deciphered writing system of the Indus Valley Civilisation. Some of the seals were used to stamp clay on trade goods.\nAlthough some houses were larger than others, Indus civilisation cities were remarkable for their apparent, if relative, egalitarianism. All the houses had access to water and drainage facilities. This gives the impression of a society with relatively low wealth concentration.\nAuthority and governance.\nArchaeological records provide no immediate answers for the question of who ruled Harappan cities and how. Nonetheless, there are indications of complex decisions being taken and large-scale mobilisation of resources. For instance, the majority of the cities were constructed in a highly uniform and well-planned grid pattern, divided into two levels from ground making one part slightly higher than the other; such complex urban planning, combined with the construction of large public works projects, demonstrates the existence of some sort of planning authority. The remarkable consistency of Harappan weights and measures, as evident in pottery, seals, weights and bricks, also indicates the existence of a central authority able to make definitive regulations.\nThese are some major theories:\nMetallurgy.\nHarappans evolved some new techniques in metallurgy and produced copper, bronze, lead, and tin.\nA touchstone bearing gold streaks was found in Banawali, which was probably used for testing the purity of gold (such a technique is still used in some parts of India).\nMetrology.\nThe people of the Indus civilisation achieved great accuracy in measuring length, mass, and time. They were among the first to develop a system of uniform weights and measures. A comparison of available objects indicates large scale variation across the Indus territories. Their smallest division, which is marked on an ivory scale found in Lothal in Gujarat, was approximately 1.704\u00a0mm, the smallest division ever recorded on a scale of the Bronze Age. Harappan engineers followed the decimal division of measurement for all practical purposes, including the measurement of mass as revealed by their hexahedron weights.\nThese chert weights were in a ratio of 5:2:1 with weights of 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, and 500\u00a0units, with each unit weighing approximately 28\u00a0grams, similar to the English Imperial ounce or Greek uncia, and smaller objects were weighed in similar ratios with the units of 0.871. However, as in other cultures, actual weights were not uniform throughout the area. The weights and measures later used in Kautilya's \"Arthashastra\" (4th\u00a0century BCE) are the same as those used in Lothal.\nArts and crafts.\nMany Indus Valley seals and items in pottery and terracotta have been found, along with a very few stone sculptures and some gold jewellery and bronze vessels. Some anatomically detailed figurines in terracotta, bronze, and steatite have been found at excavation sites, the former probably mostly toys. The Harappans also made various toys and games, among them cubical dice (with one to six holes on the faces), which were found in sites like Mohenjo-daro.\nThe terracotta figurines included cows, bears, monkeys, and dogs. The animal depicted on a majority of seals at sites of the mature period has not been clearly identified. Part bull, part zebra, with a majestic horn, it has been a source of speculation. As yet, there is insufficient evidence to substantiate claims that the image had religious or cultic significance, but the prevalence of the image raises the question of whether or not the animals in images of the IVC are religious symbols.\nMany crafts including, \"shell working, ceramics, and agate and glazed steatite bead making\" were practised and the pieces were used in the making of necklaces, bangles, and other ornaments from all phases of Harappan culture. Some of these crafts are still practised in the subcontinent today. Some make-up and toiletry items (a special kind of combs (kakai), the use of collyrium and a special three-in-one toiletry gadget) that were found in Harappan contexts still have similar counterparts in modern India. Terracotta female figurines were found (c.\u20092800\u20132600\u00a0BCE) which had red colour applied to the \"manga\" (line of partition of the hair).\nArcheological remains from 2000 to 3000 BC have been found from the city of Lothal of pieces on a board that resemble chess.\nThe finds from Mohenjo-daro were initially deposited in the Lahore Museum, but later moved to the ASI headquarters at New Delhi, where a new \"Central Imperial Museum\" was being planned for the new capital of the British Raj, in which at least a selection would be displayed. It became apparent that Indian independence was approaching, but the Partition of India was not anticipated until late in the process. The new Pakistani authorities requested the return of the Mohenjo-daro pieces excavated on their territory, but the Indian authorities refused. Eventually an agreement was reached, whereby the finds, totalling some 12,000 objects (most sherds of pottery), were split equally between the countries; in some cases this was taken very literally, with some necklaces and girdles having their beads separated into two piles. In the case of the \"two most celebrated sculpted figures\", Pakistan asked for and received the so-called \"Priest-King\" figure, while India retained the much smaller \"Dancing Girl\".\nThough written considerably later, the arts treatise \"Natya Shastra\" (c.\u2009200 BCE \u2013 200 CE) classifies musical instruments into four groups based on their means of acoustical production\u2014strings, membranes, solid materials and air\u2014and it is probable that such instruments had existed since the IVC. Archeological evidence indicates the use of simple rattles and vessel flutes, while iconographical evidence suggests early harps and drums were also used. An ideogram in the IVC contains the earliest known depiction of an arched harp, dated sometime before 1800 BCE.\nHuman statuettes.\nA handful of realistic statuettes have been found at IVC sites, of which much the most famous is the lost-wax casting bronze statuette of a slender-limbed \"Dancing Girl\" adorned with bangles, found in Mohenjo-daro. Two other realistic incomplete statuettes have been found in Harappa in proper stratified excavations, which display near-Classical treatment of the human shape: the who seems to be male, and the \"Harappa Torso\", a , both now in the Delhi National Museum. Sir John Marshall reacted with surprise when he saw these two statuettes from Harappa:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;When I first saw them I found it difficult to believe that they were prehistoric; they seemed to completely upset all established ideas about early art, and culture. Modelling such as this was unknown in the ancient world up to the Hellenistic age of Greece, and I thought, therefore, that some mistake must surely have been made; that these figures had found their way into levels some 3000\u00a0years older than those to which they properly belonged ... Now, in these statuettes, it is just this anatomical truth which is so startling; that makes us wonder whether, in this all-important matter, Greek artistry could possibly have been anticipated by the sculptors of a far-off age on the banks of the Indus.\nThese statuettes remain controversial, due to their advanced style in representing the human body. Regarding the red jasper torso, the discoverer, Vats, claims a Harappan date, but Marshall considered this statuette is probably historical, dating to the Gupta period, comparing it to the much later Lohanipur torso. A second rather similar grey stone torso of a dancing male was also found about 150\u00a0meters away in a secure Mature Harappan stratum. Overall, anthropologist Gregory Possehl tends to consider that these statuettes probably form the pinnacle of Indus art during the Mature Harappan period.\nSeals.\nThousands of steatite seals have been recovered, and their physical character is fairly consistent. In size they range from squares of side . In most cases they have a pierced boss at the back to accommodate a cord for handling or for use as personal adornment. In addition a large number of sealings have survived, of which only a few can be matched to the seals. The great majority of examples of the Indus script are short groups of signs on seals.\nSeals have been found at Mohenjo-daro depicting a figure standing on its head, and another, on the Pashupati seal, sitting cross-legged in what some call a yoga-like pose (see image, the so-called \"Pashupati\", below). This figure has been variously identified. Sir John Marshall identified a resemblance to the Hindu god, Shiva.\nA also appears in the seals, in particular in a fighting scene with a horned tiger-like beast. This deity has been compared to the Mesopotamian bull-man Enkidu. Several seals also show a man fighting two lions or tigers, a \"Master of Animals\" motif common to civilisations in Western and South Asia.\nTrade and transportation.\nThe Indus Valley civilisation may have had bullock carts identical to those seen throughout South Asia today, as well as boats. Most of these boats were probably small, flat-bottomed craft, perhaps driven by sail, similar to those one can see on the Indus River today. An extensive canal network, used for irrigation, has however also been discovered by H.-P. Francfort.\nDuring 4300\u20133200\u00a0BCE of the chalcolithic period (copper age), the Indus Valley Civilisation area shows ceramic similarities with southern Turkmenistan and northern Iran which suggest considerable mobility and trade. During the Early Harappan period (about 3200\u20132600\u00a0BCE), similarities in pottery, seals, figurines, ornaments, etc. document intensive caravan trade with Central Asia and the Iranian plateau.\nJudging from the dispersal of Indus civilisation artefacts, the trade networks economically integrated a huge area, including portions of Afghanistan, the coastal regions of Persia connected by the Gulf of Oman from the Arabian Sea, northern and western India, and Mesopotamia, leading to the development of Indus-Mesopotamia relations. Studies of tooth enamel from individuals buried at Harappa suggest that some residents had migrated to the city from beyond the Indus Valley. Ancient DNA studies of graves at Bronze Age sites at Gonur Depe, Turkmenistan, and Shahr-e Sukhteh, Iran, have identified 11 individuals of South Asian descent, who are presumed to be of mature Indus Valley origin.\nThere was an extensive maritime trade network operating between the Harappan and Mesopotamian civilisations as early as the middle Harappan Phase, with much commerce being handled by \"middlemen merchants from Dilmun\" (modern Bahrain, Eastern Arabia and Failaka located in the Persian Gulf). Such long-distance sea trade became feasible with the development of plank-built watercraft, equipped with a single central mast supporting a sail of woven rushes or cloth.\nHowever, the evidence of sea-borne trade involving the Harappan civilisation is not firm. In their book \"Rise of Civilization in India and Pakistan\" archaeologists Bridget Allchin and Raymond Allchin write: ... (p. 173) the settlement at Lothal ... along the east side was a brick basin. It is claimed by its excavator to have been a dockyard, connected by channels to a neighbouring estuary. ... On its edge the excavator discovered several heavily-pierced stones, similar to modern anchor stones employed by traditional seafaring communities of Western India. This interpretation, however, has been challenged, and indeed the published levels of the basin and its entrance relative to the modern sea level seem to argue against it. Leshnik has cogently suggested that it was a tank for the reception of sweet water, channelled from higher ground inland to an area where the local water supplies were anciently, as still today, saline. We regard either interpretation as still unproven, but favour the latter. ... (p. 188\u2013189) The discussion of trade focuses attention upon methods of transport. Several representations of ships are found on seals and graffiti at Harappa, Mohenjo-daro (Figs. 7.15\u20137.16], etc, and a terracotta model of a ship, with a stick impressed socket for the mast and eyeholes for fixing rigging comes from Lothal. We have already seen above that the great brick tank, interpreted by Rao as a dock at Lothal, cannot yet be certainly identified. The evidence of sea trade and contact during the Harappan period is largely circumstantial, or derived from inferences from the Mesopotamian texts, as detailed above. (Figure 7. 15 had caption: Mohenjo-daro: representation of ship on a stone seal (length 4.3 cm) (after Mackay). Figure 7.16 Mohenjo-daro: representation of ship on terracotta amulet (length 4.5 cm) after Dales)\nDaniel T. Potts writes:\n It is generally assumed that most trade between the Indus Valley (ancient Meluhha?) and western neighbors proceeded up the Persian Gulf rather than overland. Although there is no incontrovertible proof that this was indeed the case, the distribution of Indus-type artifacts on the Oman peninsula, on Bahrain and in southern Mesopotamia makes it plausible that a series of maritime stages linked the Indus Valley and the Gulf region. If this is accepted, then the presence of etched carnelian beads, a Harappan-style cubical stone weight, and a Harappan-style cylinder seal at Susa (Amiet 1986a, Figs. 92-94) may be evidence of maritime trade between Susa and the Indus Valley in the late 3rd millennium BCE. On the other hand, given that similar finds, particularly etched carnelian beads, are attested at landlocked sites including Tepe Hissar (Tappe He\u1e63\u0101r), Shah Tepe (\u0160\u0101h-Tappe), Kalleh Nisar (Kalla Nis\u0101r), Jalalabad (Jal\u0101l\u0101b\u0101d), Marlik (M\u0101rlik) and Tepe Yahya (Tappe Ya\u1e25y\u0101) (Possehl 1996, pp. 153-54), other mechanisms, including overland traffic by peddlers or caravans, may account for their presence at Susa.\nIn the 1980s, important archaeological discoveries were made at Ras al-Jinz (Oman), demonstrating maritime Indus Valley connections with the Arabian Peninsula.\nDennys Frenez recently regards that:\nIndus-type and Indus-related artifacts were found over a large and differentiated ecumene, encompassing Central Asia, the Iranian Plateau, Mesopotamia and the northern Levant, the Persian Gulf, and the Oman Peninsula. The discovery of Indus trade tools (seals, weights, and containers) across the entire Middle Asia, complemented by information from Mesopotamian cuneiform texts, shows that entrepreneurs from the Indus Valley regularly ventured into these regions to transact with the local socioeconomic and political entities. However, Indus artifacts were also exchanged beyond this core region, eventually reaching as far [as] the Nile River valley, Anatolia, and the Caucasus. On the contrary, only a handful of exotic trade tools and commodities have been found at sites in the Greater Indus Valley. The success of Indus trade in Central and Western Asia did not only rely on the dynamic entrepreneurialism of Indus merchants and the exotic commodities they offered. Specific products were proactively designed and manufactured in the Indus Valley to fulfill the particular needs of foreign markets, and Indus craftspeople moved beyond their native cultural sphere adapting their distinctive productions to the taste of foreign elites or reworking indigenous models. The adoption of specific seals and iconographies to regulate external trade activities suggests a conscious attempt at implementing a coordinated supraregional marketing strategy[...]\nAgriculture.\nAccording to Gangal et al. (2014), there is strong archeological and geographical evidence that neolithic farming spread from the Near East into north-west India, but there is also \"good evidence for the local domestication of barley and the zebu cattle at Mehrgarh.\"\nAccording to Jean-Francois Jarrige, farming had an independent local origin at Mehrgarh, which he argues is not merely a \"'backwater' of the Neolithic culture of the Near East\", despite similarities between Neolithic sites from eastern Mesopotamia and the western Indus valley which are evidence of a \"cultural continuum\" between those sites. Archaeologist Jim G. Shaffer writes that the Mehrgarh site \"demonstrates that food production was an indigenous South Asian phenomenon\" and that the data support interpretation of \"the prehistoric urbanisation and complex social organisation in South Asia as based on indigenous, but not isolated, cultural developments\".\nJarrige notes that the people of Mehrgarh used domesticated wheats and barley, while Shaffer and Liechtenstein note that the major cultivated cereal crop was naked six-row barley, a crop derived from two-row barley. Gangal agrees that \"Neolithic domesticated crops in Mehrgarh include more than 90% barley,\" noting that \"there is good evidence for the local domestication of barley.\" Yet, Gangal also notes that the crop also included \"a small amount of wheat,\" which \"are suggested to be of Near-Eastern origin, as the modern distribution of wild varieties of wheat is limited to Northern Levant and Southern Turkey.\"\nThe cattle that are often portrayed on Indus seals are humped Indian aurochs (\"Bos primigenius namadicus\"), which are similar to Zebu cattle. Zebu cattle are still common in India, and in Africa. They are different from European cattle (\"Bos primigenius taurus\"), and are believed to have been independently domesticated on the Indian subcontinent, probably in the Baluchistan region of Pakistan.\nResearch by J. Bates et al. (2016) confirms that Indus populations were the earliest people to use complex multi-cropping strategies across both seasons, growing foods during summer (rice, millets and beans) and winter (wheat, barley and pulses), which required different watering regimes. Bates et al. (2016) also found evidence for an entirely separate domestication process of rice in ancient South Asia, based around the wild species \"Oryza nivara\". This led to the local development of a mix of \"wetland\" and \"dryland\" agriculture of local \"Oryza sativa indica\" rice agriculture, before the truly \"wetland\" rice \"Oryza sativa japonica\" arrived around 2000 BCE.\nFood.\nAccording to archeological finds, the Indus Valley civilisation had a diet dominated by meats of animals such as cattle, buffalo, goat, pig and chicken. Remnants of dairy products were also discovered. According to Akshyeta Suryanarayan et al., available evidence indicates culinary practices to be common over the region; food-constituents were dairy products (in low proportion), ruminant carcass meat, and either non-ruminant adipose fats, plants, or mixtures of these products. The dietary pattern remained the same throughout the decline.\nSeven food-balls (\"laddus\") were found in intact form, along with two figurines of bulls and a hand-held copper adze, during excavations in 2017 from western Rajasthan. Dated to about 2600 BCE, they were likely composed of legumes, primarily mung, and cereals. The authors speculated the food-balls to be of a ritualistic significance, given the finds of bull figurines, adze and a seal in immediate vicinity.\nLanguage.\nThe Harappan language is the unknown language (or languages) of the Indus Valley civilization. The Harappan script is yet undeciphered, indeed it has not even been demonstrated to be a writing system, and therefore the language remains unknown. The language being yet unattested in readable contemporary sources, hypotheses regarding its nature are based on possible loanwords, the substratum in Vedic Sanskrit, and some terms recorded in Sumerian cuneiform (such as \"Meluhha\"), in conjunction with analyses of the Harappan script.\nThere are some possible loanwords from the language of the Indus Valley civilization. ' or ' ( ) is the Sumerian name of a prominent trading partner of Sumer during the Middle Bronze Age. Its identification remains an open question, but most scholars associate it with the Indus Valley Civilisation. Of the substratum in Vedic Sanskrit, the bulk have no proven basis in any of the known families, suggesting a source in one or more lost languages. One of these lost languages could have been the Harappan language, which Witzel labelled as the Kubh\u0101-Vip\u0101\u015b substrate.\nOne hypothesis has been suggested that the bearers of the IVC corresponded to proto-Dravidians linguistically, the break-up of proto-Dravidian corresponding to the break-up of the Late Harappan culture. Finnish Indologist Asko Parpola concludes that the uniformity of the Indus inscriptions precludes any possibility of widely different languages being used, and that an early form of Dravidian language must have been the language of the Indus people. Today, the Dravidian language family is concentrated mostly in southern India and northern and eastern Sri Lanka, but pockets of it still remain throughout the rest of India and Pakistan (the Brahui language), which lends credence to the theory.\nPossible writing system.\nBetween 400 and as many as 600\u00a0distinct Indus symbols have been found on stamp seals, small tablets, ceramic pots and more than a dozen other materials, including a \"signboard\" that apparently once hung over the gate of the inner citadel of the Indus city of Dholavira. Typical Indus inscriptions are around five characters in length, most of which (aside from the Dholavira \"signboard\") are tiny; the longest on any single object (inscribed on a copper plate) has a length of 34\u00a0symbols.\nWhile the Indus Valley Civilisation is generally characterised as a literate society on the evidence of these inscriptions, this description has been challenged by Farmer, Sproat, and Witzel (2004) who argue that the Indus system did not encode language, but was instead similar to a variety of non-linguistic sign systems used extensively in the Near East and other societies, to symbolise families, clans, gods, and religious concepts. Others have claimed on occasion that the symbols were exclusively used for economic transactions, but this claim leaves unexplained the appearance of Indus symbols on many ritual objects, many of which were mass-produced in moulds. No parallels to these mass-produced inscriptions are known in any other early ancient civilisations.\nIn a 2009 study by P.N. Rao et al. published in \"Science\", computer scientists, comparing the pattern of symbols to various linguistic scripts and non-linguistic systems, including DNA and a computer programming language, found that the Indus script's pattern is closer to that of spoken words, supporting the hypothesis that it codes for an as-yet-unknown language.\nFarmer, Sproat, and Witzel have disputed this finding, pointing out that Rao et al. did not actually compare the Indus signs with \"real-world non-linguistic systems\" but rather with \"two wholly artificial systems invented by the authors, one consisting of 200,000\u00a0randomly ordered signs and another of 200,000\u00a0fully ordered signs, that they spuriously claim represent the structures of all real-world non-linguistic sign systems\". Farmer et al. have also demonstrated that a comparison of a non-linguistic system like medieval heraldic signs with natural languages yields results similar to those that Rao et al. obtained with Indus signs. They conclude that the method used by Rao et al. cannot distinguish linguistic systems from non-linguistic ones.\nThe messages on the seals have proved to be too short to be decoded by a computer. Each seal has a distinctive combination of symbols and there are too few examples of each sequence to provide a sufficient context. The symbols that accompany the images vary from seal to seal, making it impossible to derive a meaning for the symbols from the images. There have, nonetheless, been a number of interpretations offered for the meaning of the seals. These interpretations have been marked by ambiguity and subjectivity.\nPhotos of many of the thousands of extant inscriptions are published in the \"Corpus of Indus Seals and Inscriptions\" (1987, 1991, 2010), edited by Asko Parpola and his colleagues. The most recent volume republished photos taken in the 1920s and 1930s of hundreds of lost or stolen inscriptions, along with many discovered in the last few decades; formerly, researchers had to supplement the materials in the \"Corpus\" by study of the tiny photos in the excavation reports of Marshall (1931), MacKay (1938, 1943), Wheeler (1947), or reproductions in more recent scattered sources.\nReligion.\nThe religion and belief system of the Indus Valley people has received considerable attention, especially from the view of identifying precursors to deities and religious practices of Indian religions that later developed in the area. However, due to the sparsity of evidence, which is open to varying interpretations, and the fact that the Indus script remains undeciphered, the conclusions are partly speculative and largely based on a retrospective view from a much later Hindu perspective.\nEarly and influential work in the area that set the trend for Hindu interpretations of archaeological evidence from the Harappan sites was that of John Marshall, who in 1931 identified the following as prominent features of the Indus religion: a Great Male God and a Mother Goddess; deification or veneration of animals and plants; a symbolic representation of the phallus (linga) and vulva (yoni); and, use of baths and water in religious practice. Marshall's interpretations have been much debated, and sometimes disputed over the following decades.\nOne Indus Valley seal shows a seated figure with a horned headdress, possibly tricephalic and possibly ithyphallic, surrounded by animals. Marshall identified the figure as an early form of the Hindu god Shiva (or Rudra), who is associated with asceticism, yoga, and linga; regarded as a lord of animals, and often depicted as having three eyes. The seal has hence come to be known as the Pashupati Seal, after \"Pashupati\" (lord of all animals), an epithet of Shiva. While Marshall's work has earned some support, many critics and even supporters have raised several objections. Doris Srinivasan has argued that the figure does not have three faces or yogic posture and that in Vedic literature Rudra was not a protector of wild animals. Herbert Sullivan and Alf Hiltebeitel also rejected Marshall's conclusions, with the former claiming that the figure was female, while the latter associated the figure with \"Mahisha\", the Buffalo God and the surrounding animals with vahanas (vehicles) of deities for the four cardinal directions. Writing in 2002, Gregory L. Possehl concluded that while it would be appropriate to recognise the figure as a deity, its association with the water buffalo, and its posture as one of ritual discipline, regarding it as a proto-Shiva would be going too far. Despite the criticisms of Marshall's association of the seal with a proto-Shiva icon, it has been interpreted as the Tirthankara Rishabhanatha by some scholars of Jainism like Vilas Sangave. Historians such as Heinrich Zimmer and Thomas McEvilley believe that there is a connection between first Jain Tirthankara Rishabhanatha and the Indus Valley Civilisation.\nMarshall hypothesised the existence of a cult of Mother Goddess worship based upon excavation of several female figurines and thought that this was a precursor of the Hindu sect of Shaktism. However the function of the female figurines in the life of Indus Valley people remains unclear, and Possehl does not regard the evidence for Marshall's hypothesis to be \"terribly robust\". Some of the baetyls interpreted by Marshall to be sacred phallic representations are now thought to have been used as pestles or game counters instead, while the ring stones that were thought to symbolise \"yoni\" were determined to be architectural features used to stand pillars, although the possibility of their religious symbolism cannot be eliminated. Many Indus Valley seals show animals, with some depicting them being carried in processions, while others show chimeric creations. One seal from Mohenjo-daro shows a half-human, a half-buffalo monster attacking a tiger, which may be a reference to the Sumerian myth of such a monster created by goddess Aruru to fight Gilgamesh.\nIn contrast to contemporary Egyptian and Mesopotamian civilisations, Indus Valley lacks any monumental palaces, even though excavated cities indicate that the society possessed the requisite engineering knowledge. This may suggest that religious ceremonies if any, may have been largely confined to individual homes, small temples, or the open air. Several sites have been proposed by Marshall and later scholars as possibly devoted to religious purposes, but at present only the Great Bath at Mohenjo-daro is widely thought to have been so used, as a place for ritual purification. The funerary practices of the Harappan civilisation are marked by fractional burial (in which the body is reduced to skeletal remains by exposure to the elements before final interment), and even cremation.\nLate Harappan.\nAround 1900\u00a0BCE signs of a gradual decline began to emerge, and by around 1700 BCE most of the cities had been abandoned. Examination of human skeletons from the site of Harappa in the 2010s demonstrated that the end of the Indus civilisation saw an increase in inter-personal violence and in infectious diseases like leprosy and tuberculosis.\nAccording to historian Upinder Singh, \"the general picture presented by the late Harappan phase is one of a breakdown of urban networks and an expansion of rural ones.\"\nDuring the period of approximately 1900 to 1700 BCE, multiple regional cultures emerged within the area of the Indus civilisation. The Cemetery H culture was in Punjab, Haryana, and Western Uttar Pradesh, the Jhukar culture was in Sindh, and the Rangpur culture (characterised by Lustrous Red Ware pottery) was in Gujarat. Other sites associated with the Late phase of the Harappan culture are Pirak in Balochistan, Pakistan, and Daimabad in Maharashtra, India.\nThe largest Late Harappan sites are Kudwala in Cholistan in Punjab, Bet Dwarka in Gujarat, and Daimabad in Maharashtra, which can be considered as urban, but they are smaller and few in number compared with the Mature Harappan cities. Bet Dwarka was fortified and continued to have contacts with the Persian Gulf region, but there was a general decrease of long-distance trade. On the other hand, the period also saw a diversification of the agricultural base, with a diversity of crops and the advent of double-cropping, as well as a shift of rural settlement towards the east and the south.\nThe pottery of the Late Harappan period is described as \"showing some continuity with mature Harappan pottery traditions\", but also distinctive differences. Many sites continued to be occupied for some centuries, although their urban features declined and disappeared. Formerly typical artifacts such as stone weights and female figurines became rare. There are some circular stamp seals with geometric designs, but lacking the Indus script which characterised the mature phase of the civilisation. Script is rare and confined to potsherd inscriptions. There was also a decline in long-distance trade, although the local cultures show new innovations in faience and glass making, and carving of stone beads. Urban amenities such as drains and the public bath were no longer maintained, and newer buildings were \"poorly constructed\". Stone sculptures were deliberately vandalised, valuables were sometimes concealed in hoards, suggesting unrest, and the corpses of animals and even humans were left unburied in the streets and in abandoned buildings.\nDuring the later half of the 2nd\u00a0millennium BCE, most of the post-urban Late Harappan settlements were abandoned altogether. Subsequent material culture was typically characterised by temporary occupation, \"the campsites of a population which was nomadic and mainly pastoralist\" and which used \"crude handmade pottery\". However, there is greater continuity and overlap between Late Harappan and subsequent cultural phases at sites in Punjab, Haryana, and western Uttar Pradesh, primarily small rural settlements.\nAryan migration.\nIn 1953 Sir Mortimer Wheeler proposed that the invasion of an Indo-European tribe from Central Asia, the \"Aryans\", caused the decline of the Indus civilisation. As evidence, he cited a group of 37 skeletons found in various parts of Mohenjo-daro, and passages in the Vedas referring to battles and forts. However, scholars soon started to reject Wheeler's theory, since the skeletons belonged to a period after the city's abandonment and none were found near the citadel. Subsequent examinations of the skeletons by Kenneth Kennedy in 1994 showed that the marks on the skulls were caused by erosion, and not by violence.\nIn the Cemetery H culture (the late Harappan phase in the Punjab region), some of the designs painted on the funerary urns have been interpreted through the lens of Vedic literature: for instance, peacocks with hollow bodies and a small human form inside, which has been interpreted as the souls of the dead, and a hound that can be seen as the hound of Yama, the god of death. This may indicate the introduction of new religious beliefs during this period, but the archaeological evidence does not support the hypothesis that the Cemetery H people were the destroyers of the Harappan cities.\nClimate change and drought.\nSuggested contributory causes for the localisation of the IVC include changes in the course of the river, and climate change that is also signalled for the neighbouring areas of the Middle East. As of 2016[ [update]] many scholars believe that drought, and a decline in trade with Egypt and Mesopotamia, caused the collapse of the Indus civilisation. The climate change which caused the collapse of the Indus Valley Civilisation was possibly due to \"an abrupt and critical mega-drought and cooling 4,200\u00a0years ago\", which marks the onset of the Meghalayan Age, the present stage of the Holocene.\nThe Ghaggar-Hakra system was rain-fed, and water-supply depended on the monsoons. The Indus Valley climate grew significantly cooler and drier from about 1800\u00a0BCE, linked to a general weakening of the monsoon at that time. The Indian monsoon declined and aridity increased, with the Ghaggar-Hakra retracting its reach towards the foothills of the Himalaya, leading to erratic and less extensive floods that made inundation agriculture less sustainable.\nAridification reduced the water supply enough to cause the civilisation's demise, and scatter its population eastward. According to Giosan et al. (2012), the IVC residents did not develop irrigation capabilities, relying mainly on the seasonal monsoons leading to summer floods. As the monsoons kept shifting south, the floods grew too erratic for sustainable agricultural activities. The residents then migrated towards the Ganges basin in the east, where they established smaller villages and isolated farms. The small surplus produced in these small communities did not allow the development of trade, and the cities died out.\nContinuity and coexistence.\nArchaeological excavations indicate that the decline of Harappa drove people eastward. According to Possehl, after 1900\u00a0BCE the number of sites in today's India increased from 218 to 853. According to Andrew Lawler, \"excavations along the Gangetic plain show that cities began to arise there starting about 1200\u00a0BCE, just a few centuries after Harappa was deserted and much earlier than once suspected.\" According to Jim Shaffer there was a continuous series of cultural developments, just as in most areas of the world. These link \"the so-called two major phases of urbanisation in South Asia\".\nAt sites such as Bhagwanpura (in Haryana), archaeological excavations have discovered an overlap between the final phase of Late Harappan pottery and the earliest phase of Painted Grey Ware pottery, the latter being associated with the Vedic culture and dating from around 1200\u00a0BCE. This site provides evidence of multiple social groups occupying the same village but using different pottery and living in different types of houses: \"over time the Late Harappan pottery was gradually replaced by Painted Grey ware pottery,\" and other cultural changes indicated by archaeology include the introduction of the horse, iron tools, and new religious practices.\nThere is also a Harappan site called Rojdi in Rajkot district of Saurashtra. Its excavation started under an archaeological team from Gujarat State Department of Archaeology and the Museum of the University of Pennsylvania in 1982\u201383. In their report on archaeological excavations at Rojdi, Gregory Possehl and M.H. Raval write that although there are \"obvious signs of cultural continuity\" between the Harappan civilisation and later South Asian cultures, many aspects of the Harappan \"sociocultural system\" and \"integrated civilization\" were \"lost forever,\" while the Second Urbanisation of India (beginning with the Northern Black Polished Ware culture, c.\u2009600\u00a0BCE) \"lies well outside this sociocultural environment\".\nPost-Harappan.\nPreviously, scholars believed that the decline of the Harappan civilisation led to an interruption of urban life in the Indian subcontinent. However, the Indus Valley Civilisation did not disappear suddenly, and many elements of the Indus civilisation appear in later cultures. The Cemetery H culture may be the manifestation of the Late Harappan over a large area in the region of Punjab, Haryana and western Uttar Pradesh, and the Ochre Coloured Pottery culture its successor. David Gordon White cites three other mainstream scholars who \"have emphatically demonstrated\" that Vedic religion derives partially from the Indus Valley Civilisations.\nAs of 2016[ [update]], archaeological data suggests that the material culture classified as Late Harappan may have persisted until at least c.\u20091000\u2013900\u00a0BCE and was partially contemporaneous with the Painted Grey Ware culture. Harvard archaeologist Richard Meadow points to the late Harappan settlement of Pirak, which thrived continuously from 1800 BCE to the time of the invasion of Alexander the Great in 325\u00a0BCE.\nIn the aftermath of the Indus civilisation's localisation, regional cultures emerged, to varying degrees showing the influence of the Indus civilisation. In the formerly great city of Harappa, burials have been found that correspond to a regional culture called the Cemetery H culture. At the same time, the Ochre Coloured Pottery culture expanded from Rajasthan into the Gangetic Plain. The Cemetery H culture has the earliest evidence for cremation; a practice dominant in Hinduism today.\nThe inhabitants of the Indus Valley Civilisation migrated from the river valleys of Indus and Ghaggar-Hakra, towards the Himalayan foothills of the Ganga-Yamuna basin.\nGenetics.\nIn 2019, a study was published by Shinde et al. on a reconstructed genome obtained from a female skeleton found in an IVC-related cemetery in Rakhigarhi, Haryana, India, dating to around 2,800\u20132,300 BCE. The analysis suggested that the majority of the genome was closely related to Mesolithic Iranian hunter-gatherers. The remaining portion of the genome was from an indigenous East Eurasian source, termed Ancient Ancestral South Indian (AASI), thought to represent indigenous South Asian hunter-gatherer ancestry. The genome completely lacked the Western Steppe Herder-related ancestry that is found in modern (particularly in northern) South Asians, or any Anatolian Neolithic farmer ancestry. The ancestry of the IVC-related individual is similar to the majority of the ancestry of modern South Asians.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46854", "revid": "28078513", "url": "https://en.wikipedia.org/wiki?curid=46854", "title": "Wallis Simpson", "text": "Wife of Prince Edward, Duke of Windsor (1896\u20131986)\nWallis, Duchess of Windsor (born Bessie Wallis Warfield, later Spencer and then Simpson; June 19, 1896\u00a0\u2013 April 24, 1986), was an American socialite and the wife of Prince Edward, Duke of Windsor (former King Edward VIII). Their intention to marry and her status as a divorc\u00e9e caused a constitutional crisis that led to Edward's abdication.\nWallis grew up in Baltimore, Maryland. Her father died shortly after her birth, and she and her widowed mother were partly supported by their wealthier relatives. Her first marriage, to United States Navy officer Win Spencer, was punctuated by periods of separation and eventually ended in divorce. In 1931, while married to her second husband Ernest Simpson, she met Edward, the heir apparent to the British throne. Five years later, after Edward's accession as King of the United Kingdom, Wallis divorced Ernest to marry Edward.\nThe King's desire to marry a woman who had two living ex-husbands threatened to cause a constitutional crisis in the United Kingdom and the Dominions, ultimately leading to his abdication in December 1936 to marry \"the woman I love\". After abdicating, Edward was made Duke of Windsor by his brother and successor, George\u00a0VI. Wallis married Edward six months later, after which she was formally known as the Duchess of Windsor, but was not allowed to share her husband's style of \"Royal Highness\".\nBefore, during, and after the Second World War, Wallis and Edward were suspected by many in government and society of being Nazi sympathizers. In 1937, without government approval, they visited Germany and met Adolf Hitler. In 1940, Edward was appointed governor of the Bahamas, and the couple moved to the islands until he relinquished the office in 1945. In the 1950s and 1960s, they shuttled between Europe and the United States, living a life of leisure as society celebrities. After Edward's death in 1972, Wallis lived in seclusion and was rarely seen in public. Her private life has been a source of much speculation, and she remains a controversial figure in British history.\nEarly life and education.\nAn only child, Bessie Wallis (sometimes written \"Bessiewallis\") Warfield was born on June 19, 1896, in Square Cottage at Monterey Inn, a hotel directly across the road from the Monterey Country Club, in Blue Ridge Summit, Pennsylvania. A summer resort close to the Maryland\u2013Pennsylvania border, Blue Ridge Summit was popular with Baltimoreans escaping the season's heat, and Monterey Inn, which had a central building, as well as individual wooden cottages, was the town's largest hotel.\nWallis's father was Teackle Wallis Warfield (named after Severn Teackle Wallis), the fifth and youngest son of Henry Mactier Warfield, a prominent merchant, described as \"one of the best known and personally one of the most popular citizens of Baltimore\", who ran for mayor in 1875. Her mother was Alice Montague, a daughter of stockbroker William Latane Montague. Wallis was named in honor of her father (who was known as Wallis) and her mother's elder sister, Bessie (Mrs. D. Buchanan Merryman), and was called Bessie Wallis until, at some time in her youth, the name Bessie was dropped.\nAccording to a wedding announcement published in \"The Baltimore Sun\" on November 20, 1895, Wallis's parents were married by C. Ernest Smith at Baltimore's Saint Michael and All Angels' Protestant Episcopal Church on November 19, 1895, which suggests she was conceived out of wedlock. Wallis said that her parents were married in June 1895. Her father died of tuberculosis on November 15, 1896. For her first few years, Wallis and her mother were dependent upon the charity of her father's wealthy bachelor brother Solomon Davies Warfield, postmaster of Baltimore and later president of the Continental Trust Company and the Seaboard Air Line Railway. Initially, they lived with him at the four-story row house, 34 East Preston Street, that he shared with his mother.\nIn 1901, Wallis's aunt Bessie Merryman was widowed, and the following year Alice and Wallis moved into her four-bedroom house on West Chase Street, Baltimore, where they lived for at least a year until they settled in an apartment, and then a house, of their own. In 1908, Wallis's mother married her second husband, John Freeman Rasin, son of prominent Democratic party boss Isaac Freeman Rasin.\nOn April 17, 1910, Wallis was confirmed at Christ Episcopal Church, Baltimore, and between 1912 and 1914 her uncle paid for her to attend Oldfields School, the most expensive girls' school in Maryland. There she became a friend of heiress Ren\u00e9e du Pont, a daughter of Senator T. Coleman du Pont of the du Pont family, and Mary Kirk, whose family founded Kirk Silverware. A fellow pupil at one of Wallis's schools recalled, \"She was bright, brighter than all of us. She made up her mind to go to the head of the class, and she \"did\".\" Wallis was always immaculately dressed and pushed herself hard to do well. A later biographer wrote of her, \"Though Wallis's jaw was too heavy for her to be counted beautiful, her fine violet-blue eyes and petite figure, quick wits, vitality, and capacity for total concentration on her interlocutor ensured that she had many admirers.\"\nFirst marriage.\nIn April 1916, Wallis met Earl Winfield Spencer Jr., a US Navy aviator, in Pensacola, Florida, while visiting her cousin Corinne Mustin, spouse of Henry C. Mustin. It was at this time that Wallis witnessed two airplane crashes about two weeks apart, resulting in a lifelong fear of flying. The couple married on November 8, 1916, at Christ Episcopal Church in Baltimore, which had been Wallis's parish. Win, as her husband was known, was a heavy drinker. He drank even before flying and once crashed into the sea, but escaped almost unharmed. After the United States entered the First World War in 1917, Spencer was posted to San Diego as the first commanding officer of a training base in Coronado, known as Naval Air Station North Island; they remained there until 1921.\nIn 1920, Edward, Prince of Wales, visited San Diego, but he and Wallis did not meet. Later that year, Spencer left his wife for a period of four months, but in the spring of 1921 they were reunited in Washington, D.C., where Spencer had been posted. They soon separated again, and in 1922, when Spencer was posted to the Far East as commander of the , Wallis remained behind, continuing an affair with an Argentine diplomat, Felipe de Espil. In January 1924, she visited Paris with her recently widowed cousin Corinne Mustin, before sailing to the Far East aboard a troop carrier, . The Spencers were briefly reunited until she fell ill, after which she returned to Hong Kong.\nWallis toured China, and while in Beijing stayed with Katherine and Herman Rogers, who were to remain her longterm friends. According to the wife of one of Win's fellow officers, Mrs. Milton E. Miles, in Beijing Wallis met Count Galeazzo Ciano, later Mussolini's son-in-law and foreign minister, had an affair with him, and became pregnant, leading to a botched abortion that left her infertile. The rumor was later widespread but never substantiated and Ciano's wife, Edda Mussolini, denied it. The existence of an official \"China dossier\" (detailing the supposed sexual and criminal exploits of Wallis in China) is denied by historians and biographers. Wallis spent over a year in China, during which time\u2014according to the socialite Madame Wellington Koo\u2014she managed to master only one Chinese phrase: \"Boy, pass me the champagne\". By September 1925, she and her husband were back in the United States, though living apart. Their divorce was finalized on December 10, 1927.\nSecond marriage.\nBy the time her marriage to Spencer was dissolved, Wallis had become involved with Ernest Aldrich Simpson, an Anglo-American shipping executive and former officer in the Coldstream Guards. He divorced his first wife, Dorothea (by whom he had a daughter, Audrey), to marry Wallis on July 21, 1928, at the Register Office in Chelsea, London. Wallis had telegraphed her acceptance of his proposal from Cannes, where she was staying with her friends, Mr. and Mrs. Rogers.\nThe Simpsons temporarily set up home in a furnished house with four servants in Mayfair. In 1929, Wallis sailed back to the United States to visit her sick mother, who had married legal clerk Charles Gordon Allen after the death of Rasin. During the trip, Wallis's investments were wiped out in the Wall Street Crash, and her mother died penniless on November 2, 1929. Wallis returned to England and with the shipping business still buoyant, the Simpsons moved into a large flat with a staff of servants.\nThrough a friend, Consuelo Thaw, Wallis met Consuelo's sister Thelma, Viscountess Furness, at the time the mistress of Edward, Prince of Wales. On January 10, 1931, Lady Furness introduced Wallis to Edward at Burrough Court, near Melton Mowbray. Edward was the eldest son of King George\u00a0V and Queen Mary, and heir apparent to the British throne. Between 1931 and 1934, he met the Simpsons at various house parties, and Wallis was presented at court. Ernest was beginning to encounter financial difficulties, as the Simpsons were living beyond their means, and they had to fire a succession of staff.\nRelationship with Edward, Prince of Wales.\nIn January 1934, while Lady Furness was away in New York City, Wallis allegedly became Edward's mistress. Edward denied this to his father, despite his staff seeing them in bed together as well as \"evidence of a physical sexual act\". Wallis soon ousted Furness, and Edward distanced himself from a former lover and confidante, the Anglo-American textile heiress Freda Dudley Ward.\nBy the end of 1934, Edward was irretrievably besotted with Wallis, finding her domineering manner and abrasive irreverence toward his position appealing; in the words of his official biographer, he became \"slavishly dependent\" on her. According to Wallis, it was during a cruise on Lord Moyne's private yacht \"Rosaura\" in August 1934 that she fell in love with Edward. At an evening party in Buckingham Palace, he introduced her to his mother; his father was outraged, primarily on account of her marital history, as divorced people were generally excluded from court. Edward showered Wallis with money and jewels, and in February 1935, and again later in the year, he holidayed with her in Europe. His courtiers became increasingly alarmed as the affair began to interfere with his official duties.\nIn 1935, the head of the Metropolitan Police Special Branch told the Metropolitan Police Commissioner that Wallis was also having an affair with Guy Marcus Trundle, who was \"said to be employed by the Ford Motor Company\". Rumors of an affair were doubted, however, by Captain Val Bailey, who knew Trundle well and whose mother had an affair with Trundle for nearly two decades, and by historian Susan Williams.\nAbdication crisis.\nOn January 20, 1936, George\u00a0V died at Sandringham and Edward ascended the throne as EdwardVIII. The next day, he broke royal protocol by watching the proclamation of his accession from a window of St James's Palace, in the company of the still-married Wallis. It was becoming apparent to court and government circles that the new king meant to marry her. Edward's behaviour and his relationship with Wallis made him unpopular with the Conservative-led British government, as well as distressing his mother and his brother the Duke of York. The British media remained deferential to the monarchy, and no stories of the affair were reported in the domestic press, but foreign media widely reported their relationship. After the death of GeorgeV, before her divorce from her second husband, Wallis reportedly said, \"Soon I shall be Queen of England.\"\nThe monarch of the United Kingdom is Supreme Governor of the Church of England. At the time of the proposed marriage (and until 2002), the Church of England disapproved of, and would not perform, the remarriage of divorced people if their former spouse was still alive. Constitutionally, the King was required to be in communion with the Church of England, but his proposed marriage conflicted with the Church's teachings. Additionally, at the time both the Church and English law only recognized adultery as a legitimate ground for divorce. Since she had divorced her first husband on grounds of \"mutual incompatibility\", there was a possibility that her second marriage, as well as her prospective marriage to Edward, would be considered bigamous if her first divorce had been challenged in court.\nThe British and Dominion governments believed that a twice-divorced woman was politically, socially, and morally unsuitable as a prospective consort. Wallis was perceived by many in the British Empire as a woman of \"limitless ambition\" who was pursuing the King because of his wealth and position.\nWallis had already filed for divorce from her second husband on the grounds that he had committed adultery with her childhood friend Mary Kirk and the decree nisi was granted on October 27, 1936. In November, the King consulted with the British prime minister, Stanley Baldwin, on a way to marry Wallis and keep the throne. Edward suggested a morganatic marriage, where he would remain king but Wallis would not be queen, but this was rejected by Baldwin and the prime ministers of Australia, Canada, and the Union of South Africa. If Edward were to marry Wallis against Baldwin's advice, the government would be required to resign, causing a constitutional crisis.\nWallis's relationship with Edward had become public knowledge in the United Kingdom by early December. She decided to flee the country as the scandal broke, and was driven to the south of France in a dramatic race to outrun the press. For the next three months, she was under siege by the media at the Villa Lou Viei, near Cannes, the home of her close friends Herman and Katherine Rogers, whom she later thanked effusively in her ghost-written memoirs. According to Andrew Morton, who relied on an interview with the stepdaughter-in-law of Herman Rogers conducted 80 years later, Simpson confessed during the writing of her memoirs that Rogers was the love of her life. However, at her instruction, the ghostwriter omitted this revelation from the final memoirs. At her hideaway, Wallis was pressured by Lord Brownlow, the King's lord-in-waiting, to renounce Edward. On December 7, 1936, Brownlow read to the press Wallis's statement, which he had helped her draft, indicating her readiness to give up Edward. However, Edward was determined to marry Wallis. John Theodore Goddard, Wallis's solicitor, stated: \"[his] client was ready to do anything to ease the situation but the other end of the wicket [Edward VIII] was determined.\" This seemingly indicated that Edward had decided he had no option but to abdicate if he wished to marry Wallis.\nEdward signed the Instrument of Abdication on December 10, 1936, in the presence of his three surviving brothers, the Dukes of York, Gloucester and Kent. Special laws passed by the Parliaments of the Dominions finalized Edward's abdication the following day, or in Ireland's case one day later. The Duke of York then became King GeorgeVI. On December 11, Edward said in a radio broadcast, \"I have found it impossible to carry the heavy burden of responsibility, and to discharge my duties as King as I would wish to do, without the help and support of the woman I love.\"\nEdward left Britain for Austria, where he stayed at Schloss Enzesfeld, the home of Baron Eug\u00e8ne and Baroness Kitty de Rothschild. Edward had to remain apart from Wallis until there was no danger of compromising the granting of a decree absolute in her divorce proceedings. Upon her divorce being made final in May 1937, she changed her name by deed poll to Wallis Warfield, resuming her maiden name. The couple were reunited at the Ch\u00e2teau de Cand\u00e9, Monts, France, on May 4, 1937.\nThird marriage: Duchess of Windsor.\nWallis and Edward married one month later on June 3, 1937, at the Ch\u00e2teau de Cand\u00e9, lent to them by French millionaire Charles Bedaux. The date would have been King George V's 72nd birthday; Queen Mary thought the wedding had been scheduled for then as a deliberate slight. No member of Edward's family attended. Wallis wore a \"Wallis blue\" Mainbocher wedding dress. Edward presented her with an engagement ring that consisted of an emerald mount in yellow gold set with diamonds, and the sentence \"We are ours now\" was engraved on it. While the Church of England refused to sanction the wedding, Robert Anderson Jardine, Vicar of St Paul's, Darlington, offered to perform the service, an offer that was accepted by the couple. Guests included Randolph Churchill, Baron Eug\u00e8ne Daniel von Rothschild, and the best man, Major Edward Dudley \"Fruity\" Metcalfe. The marriage produced no children. In November, Ernest Simpson married Mary Kirk.\nEdward was created Duke of Windsor by his brother King GeorgeVI prior to the marriage. However, letters patent, issued by the new king and unanimously supported by the Dominion governments, prevented Wallis, now Duchess of Windsor, from sharing her husband's style of \"Royal Highness\". GeorgeVI's firm view that the Duchess should not be given a royal title was shared by his mother, Queen Mary, and his wife, Queen Elizabeth (later the Queen Mother). At first, the British royal family did not accept Wallis and would not receive her formally, although the former king sometimes met his mother and siblings after his abdication. Some biographers have suggested that Wallis's sister-in-law Queen Elizabeth remained bitter towards her for her role in bringing GeorgeVI to the throne (which she may have seen as a factor in his early death) and for prematurely behaving as Edward's consort when she was his mistress. These claims were denied by Elizabeth's close friends, such as the Duke of Grafton, who wrote that she \"never said anything nasty about the Duchess of Windsor, except to say she really hadn't got a clue what she was dealing with.\" Elizabeth was said to have referred to Wallis as \"that woman\", while Wallis and Edward referred to Queen Elizabeth as \"Mrs. Temple\" and \"Cookie\", alluding to her solid figure and fondness for food, and to her daughter Princess Elizabeth (later Queen Elizabeth\u00a0II) as \"Shirley\", as in Shirley Temple. Wallis bitterly resented the denial of the royal title and the refusal of Edward's relatives to accept her as part of the family. Within the household of the Duke and Duchess, the style \"Her Royal Highness\" was used by those who were close to the couple.\nAccording to Diana Mosley, who knew both Queen Elizabeth and the Duchess of Windsor but was only friendly with the latter, Elizabeth's antipathy toward Wallis may have resulted from jealousy. Lady Mosley wrote to her sister, the Duchess of Devonshire, after the death of the Duke of Windsor, \"probably the theory of their [the Windsors'] contemporaries that Cake [a Mitford nickname for the Queen Mother] was rather in love with him [the Duke] (as a girl) &amp; took second best, may account for much.\"\nWallis and Edward lived in France in the pre-war years. In 1937, they made a high-profile visit to Germany and met Adolf Hitler at the Berghof, his Berchtesgaden retreat. After the visit, Hitler said of Wallis, \"she would have made a good queen\". The visit tended to corroborate the strong suspicions of many in government and society that Wallis was a German agent, a claim that she ridiculed in her letters to Edward. US FBI files compiled in the 1930s also portray her as a possible Nazi sympathizer. Duke Carl Alexander of W\u00fcrttemberg told the FBI that Wallis and leading Nazi Joachim von Ribbentrop, who served as Ambassador of Germany to the United Kingdom during the 1930s, had been lovers in London. There were even rather improbable reports during the Second World War that she kept a signed photograph of Ribbentrop on her bedside table.\nEdward wrote in the New York \"Daily News\" of December 13, 1966: \"In a roundabout way [Hitler] encouraged me to infer that Red Russia was the only enemy and that it was in Britain's interest and in Europe's too, that Germany be encouraged to strike east and smash Communism forever\u00a0... I confess frankly that he took me in. ... I thought the rest of us could be fence-sitters while the Nazis and the Reds slogged it out.\"\nSecond World War.\nAs the German troops advanced into France in 1940, the Windsors fled south from their Paris home, first to Biarritz then to Spain in June. Wallis told United States ambassador to Spain Alexander W. Weddell that France had lost because it was \"internally diseased\". The couple moved to Portugal in July. They stayed in Cascais, at Casa de Santa Maria, the home of Ricardo do Esp\u00edrito Santo e Silva, a banker who was suspected of being a German agent.\nIn August 1940, the Duke and Duchess traveled by commercial liner to the Bahamas, where Edward was installed as governor. Wallis performed her role as the governor's consort competently for five years; she worked actively for the Red Cross and in the improvement of infant welfare, as well as overseeing renovations of Government House. However, she hated Nassau, calling it \"our St Helena\" in a reference to Napoleon's final place of exile, and sarcastically commenting on the government surveillance. She was heavily criticized in the British press for her extravagant shopping in the United States, undertaken when Britain was enduring privations such as rationing and blackout. She referred to the local population as \"lazy, thriving niggers\" in letters to her aunt, which reflected her upbringing in Jim Crow Baltimore. Prime Minister Winston Churchill strenuously objected in 1941 when she and her husband planned to tour the Caribbean aboard a yacht belonging to Swedish magnate Axel Wenner-Gren, who Churchill said was \"pro-German\", and Churchill complained again when the Duke gave a \"defeatist\" interview. Another of their acquaintances, Charles Bedaux, who had hosted their wedding, was arrested on charges of treason in 1943 but committed suicide in jail in Miami before the case was brought to trial. The British establishment distrusted Wallis; Sir Alexander Hardinge wrote that her suspected anti-British activities were motivated by a desire for revenge against a country that rejected her as its queen. The couple returned to France and retirement after the defeat of Nazi Germany.\nLater life.\nIn 1946, when Wallis was staying at Ednam Lodge, the home of the Earl of Dudley, some of her jewels were stolen. The stolen pieces were only a small portion of the Windsor jewels, which were either bought privately, inherited by the Duke, or given to him when he was Prince of Wales. There were rumors that the theft had been masterminded by the royal family as an attempt to regain jewels taken from the Royal Collection by Edward, or by the Windsors themselves as part of an insurance fraud (they made a large deposit of loose stones at Cartier the following year). However, in 1960, career criminal Richard Dunphie confessed to the crime.\nIn 1952, the Windsors were offered the use of a house by the Paris municipal authorities. The couple lived at 4 route du Champ d'Entra\u00eenement in the Bois de Boulogne, near Neuilly-sur-Seine, for most of the remainder of their lives, essentially living a life of easy retirement. They traveled frequently between Europe and America aboard ocean liners. They bought a second house in a far suburb of Paris, \"Moulin de la Tuilerie\" or \"The Mill\" in Gif-sur-Yvette, where they soon became close friends with their neighbors, Oswald and Diana Mosley. Years later, Diana Mosley said that Wallis and Edward shared her and her husband's views that Hitler should have been given a free hand to destroy Communism.\nIn 1965, the Duke and Duchess visited London as Edward required eye surgery for a detached retina; Edward's niece Queen ElizabethII and sister-in-law Princess Marina, Duchess of Kent, visited them. Edward's sister, the Princess Royal, also visited them just 10 days before her death. Wallis and Edward attended her memorial service in Westminster Abbey. Later, in 1967, they joined the royal family in London for the unveiling of a plaque by ElizabethII to commemorate the centenary of Queen Mary's birth. The couple spoke to Kenneth Harris for an extensive BBC television interview in 1970. Both Queen Elizabeth\u00a0II and her son Charles, Prince of Wales, visited the Windsors in Paris in Edward's later years, the Queen's visit being shortly before Edward's death. For much of their later lives, Wallis and Edward were served by their valet and footman Sydney Johnson.\nWidowhood.\nUpon Edward's death from throat cancer in 1972, Wallis traveled to the United Kingdom to attend his funeral, staying at Buckingham Palace during her visit. She became increasingly frail and eventually succumbed to dementia, living the final years of her life as a recluse, supported by both her husband's estate and an allowance from Elizabeth II. She suffered several falls and broke her hip twice.\nAfter Edward's death, Wallis's French lawyer, Suzanne Blum, assumed power of attorney. Blum sold items belonging to the Duchess to her own friends at lower than market value and was accused of exploiting her client in Caroline Blackwood's \"The Last of the Duchess\", written in 1980 but not published until 1995, after Blum's death. Later, royal biographer Hugo Vickers called Blum a \"Satanic figure\u00a0... wearing the mantle of good intention to disguise her inner malevolence\".\nIn 1980, Wallis lost her ability to speak. Towards the end of her life, she was bedridden and did not receive any visitors, apart from her doctor and nurses.\nDeath.\nWallis died on April 24, 1986, at her home in the Bois de Boulogne, Paris, at the age of 89 from bronchial pneumonia. Her funeral was held on April 29 at St George's Chapel, Windsor Castle, attended by her two surviving sisters-in-law\u00a0\u2013 Queen Elizabeth The Queen Mother and Princess Alice, Duchess of Gloucester\u00a0\u2013 and other members of the royal family. Queen Elizabeth II and her husband, Prince Philip, attended both the funeral ceremony and the burial, as did their son Charles and daughter-in-law Diana. Diana said afterwards that it was the only time she had seen the Queen weep.\nWallis was buried next to Edward in the Royal Burial Ground near Windsor Castle, as \"Wallis, Duchess of Windsor\". Prior to an agreement with ElizabethII in the 1960s, Wallis and Edward had previously planned for a burial in a purchased cemetery plot at Green Mount Cemetery in Baltimore, where Wallis's father was interred.\nIn recognition of the help France gave to the Windsors in providing them with a home, and in lieu of death duties, Wallis's collection of Louis XVI style furniture, some porcelain, and paintings were made over to the French state. The British royal family received no major bequests. Most of her estate went to the Pasteur Institute medical research foundation, on the instructions of Suzanne Blum. The decision took the royal family and Wallis's friends by surprise, as she had shown little interest in charity during her life.\nIn a Sotheby's auction in Geneva, in April 1987, Wallis's jewelry collection raised $45\u00a0million for the institute, approximately seven times its pre-sale estimate. Blum later said that Egyptian entrepreneur Mohamed Al-Fayed tried to purchase the jewels for a \"rock bottom price\". Al-Fayed bought much of the non-financial estate, including the lease of the Paris mansion. An auction of his collection was announced in July 1997 for later that year in New York. Delayed by his son's death in the car crash that also claimed the life of Diana, Princess of Wales, the sale raised more than \u00a314\u00a0million for charity in 1998.\nLegacy.\nWallis was plagued by rumors of other lovers. The gay American Jimmy Donahue, an heir to the Woolworth fortune, said he had a liaison with her in the 1950s, but Donahue was notorious for his inventive pranks and rumor-mongering. Wallis's memoir \"The Heart Has Its Reasons\" was published in 1956, and biographer Charles Higham said that \"facts were remorselessly rearranged in what amounted to a self-performed face-lift\". He describes Wallis as \"charismatic, electric and compulsively ambitious\".\nFictional depictions of the Duchess include the novel \"Famous Last Words\" (1981) by Canadian author Timothy Findley, which portrays her as a manipulative conspirator, and Rose Tremain's short story \"The Darkness of Wallis Simpson\" (2006), which depicts her more sympathetically in her final years of ill health. Hearsay and conjecture have clouded assessment of Wallis's life, not helped by her own manipulation of the truth. But, in the opinion of her biographers, there is no document that proves directly that she was anything other than a victim of her own ambition, who lived out a great romance that became a great tragedy. In the words of one, \"she experienced the ultimate fairy tale, becoming the adored favorite of the most glamorous bachelor of his time. The idyll went wrong when, ignoring her pleas, he threw up his position to spend the rest of his life with her.\" Wallis herself is reported to have summed up her life in a sentence: \"You have no idea how hard it is to live out a great romance.\"\nTitles and styles.\nWallis resumed her maiden name Wallis Warfield by deed poll on May 7, 1937, but continued to use the title \"Mrs\".\nThe Duchess of Windsor was unofficially styled \"Her Royal Highness\" within her own household.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46858", "revid": "31541467", "url": "https://en.wikipedia.org/wiki?curid=46858", "title": "Henry VI of England", "text": "King of England (1422\u201361, 1470\u201371)\nHenry VI (6 December 1421 \u2013 21 May 1471) was King of England from 1422 to 1461 and 1470 to 1471, and disputed King of France from 1422 to 1453. The only child of Henry V, he succeeded to the English throne at the age of eight months, upon his father's death, and to the French throne on the death of his maternal grandfather, Charles VI, shortly afterwards.\nHenry was born during the Hundred Years' War (1337\u20131453). He is the only English monarch to have been crowned King of France, following his coronation at Notre-Dame de Paris in 1431 as Henry II. His early reign, when England was ruled by a regency government, saw the pinnacle of English power in France. However, setbacks followed once he assumed full control in 1437. The young king faced military reversals in France, as well as political and financial crises in England, where divisions among the nobility in his government began to widen. His reign saw the near total loss of English lands in France.\nIn contrast to his father, Henry VI was described as timid, passive, benevolent and averse to warfare and violence. In 1445, Henry married Charles VII's niece Margaret of Anjou in the hope of achieving peace. However, the peace policy failed and war recommenced. By 1453, Calais was the only English-governed territory on the continent. Henry's domestic popularity declined in the 1440s, and political unrest in England grew as a result. Because of military defeats and political crises, Henry suffered a mental breakdown in 1453, triggering a power struggle between the royal family: Richard, 3rd Duke of York; Edmund Beaufort, 2nd Duke of Somerset; and Queen Margaret. Civil war broke out in 1455, leading to a long period of dynastic conflict known as the Wars of the Roses (1455\u20131487).\nHenry was deposed in March 1461 by York's eldest son, who took the throne as Edward IV. Henry was captured by Edward's forces in 1465 and imprisoned in the Tower of London. Henry was restored to the throne by Richard Neville (\"Warwick the Kingmaker\") in 1470. However in 1471, Edward retook power, killing Henry's only son, Edward of Westminster, and imprisoning Henry once again. Henry died in the Tower in May 1471, possibly killed on the orders of King Edward. Henry may have been bludgeoned to death: his corpse was found much later to have light brown hair matted with what appeared to be blood. He was buried at Chertsey Abbey and moved to Windsor Castle in 1484. He left a legacy of educational institutions, having founded Eton College, King's College, Cambridge, and All Souls College, Oxford. William Shakespeare wrote a trilogy of plays about his life, depicting him as weak-willed and easily influenced by his wife.\nChild king.\nHenry was born on 6 December 1421 at Windsor Castle, the only child and heir-apparent of King Henry V. He succeeded to the throne as King of England at the age of eight months on 1 September 1422, the day after his father's death; he remains the youngest person ever to succeed to the English throne. On 21 October 1422, in accordance with the Treaty of Troyes of 1420, he became titular King of France upon his grandfather Charles VI's death. His mother, the 20-year-old Catherine of Valois, was viewed with considerable suspicion by English nobles as Charles VI's daughter. She was prevented from playing a full role in her son's upbringing.\nOn 28 September 1423, the nobles swore loyalty to Henry VI, who was not yet two years old. They summoned Parliament in the King's name and established a regency council to govern until the King should come of age. One of Henry V's surviving brothers, John, Duke of Bedford, was appointed senior regent of the realm and was in charge of the ongoing war in France. During Bedford's absence, the government of England was headed by Henry V's other surviving brother, Humphrey, Duke of Gloucester, who was appointed Lord Protector and Defender of the Realm. His duties were limited to keeping the peace and summoning Parliament. Henry V's uncle Henry Beaufort, Bishop of Winchester (after 1426 also Cardinal), had an important place on the Council. After the Duke of Bedford died in 1435, the Duke of Gloucester claimed the Regency himself but was contested by the other members of the Council.\nFrom 1428, Henry's tutor was Richard de Beauchamp, Earl of Warwick, whose father had been instrumental in the opposition to Richard II's reign. For the period 1430\u20131432, Henry was also tutored by the physician John Somerset. Somerset's duties were to \"tutor the young king as well as preserv[e] his health\". Somerset remained within the royal household until early 1451 after the English House of Commons petitioned for his removal because of his \"dangerous and subversive influence over Henry VI\".\nHenry's mother Catherine remarried to Owen Tudor and had two sons by him, Edmund and Jasper. Henry later gave his half-brothers earldoms. Edmund then fathered the future King Henry VII of England.\nIn reaction to the coronation of Charles VII of France in Reims Cathedral on 17 July 1429, Henry was soon crowned King of England at Westminster Abbey on 6 November 1429, aged 7, followed by his own coronation as King of France at Notre-Dame de Paris on 16 December 1431, aged 10. He was the only English king to be crowned king in both England and France. It was shortly after his crowning ceremony at Merton Priory on All Saints' Day, 1 November 1437, shortly before his 16th birthday, that he obtained some measure of independent authority. This was confirmed on 13 November 1437, but his growing willingness to involve himself in administration had already become apparent in 1434, when the place named on writs temporarily changed from Westminster (where the Privy Council met) to Cirencester (where the King resided). He finally assumed full royal powers when he came of age at the end of the year 1437, when he turned 16 years old. Henry's assumption of full royal powers occurred during the Great Bullion Famine and the beginning of the Great Slump in England.\nAssumption of government.\nHenry, who was by nature shy, pious, and averse to deceit and bloodshed, immediately allowed his court to be dominated by a few noble favourites who clashed on the matter of the French war when he assumed the reins of government in 1437. After the death of King Henry V, England had lost momentum in the Hundred Years' War, whereas the House of Valois had gained ground beginning with Joan of Arc's military victories in the year 1429. The young King came to favour a policy of peace in France and thus favoured the faction around Cardinal Beaufort and William de la Pole, Earl of Suffolk, who thought likewise; the Duke of Gloucester and Richard, Duke of York, who argued for a continuation of the war, were ignored.\nMarriage.\nAs the English military situation in France deteriorated, talks emerged in England about arranging a marriage for the king to strengthen England's foreign connections and facilitate a peace between the warring parties. In 1434, the English council suggested that peace with the Scots could best be effected by the wedding of Henry to one of the daughters of King James I of Scotland; the proposal came to nothing. During the Congress of Arras in 1435, the English put forth the idea of a union between Henry and a daughter of King Charles VII of France, but the Armagnacs refused even to contemplate the suggestion unless Henry renounced his claim to the French throne. Another proposal in 1438 to a daughter of King Albert II of Germany likewise failed.\nBetter prospects for England arose amid a growing effort by French lords to resist the growing power of the French monarchy, a conflict which culminated in the Praguerie revolt of 1440. Though the English failed to take advantage of the Praguerie itself, the prospect of gaining the allegiance of one of Charles VII's more rebellious nobles was attractive from a military perspective. In about 1441, the recently ransomed Charles, Duke of Orl\u00e9ans, in an attempt to force Charles VII to make peace with the English, suggested a marriage between Henry VI and Isabella of Armagnac, daughter of John IV, Count of Armagnac, a powerful noble in southwestern France who was at odds with the Valois crown. An alliance with Armagnac would have helped to protect English Gascony from increasing French threats in the region, especially in the face of defections to the enemy by local English vassals, and might have helped to wean some other French nobles to the English party. The proposal was seriously entertained between 1441 and 1443, but a massive French campaign in 1442 against Gascony disrupted the work of the ambassadors and frightened the Count of Armagnac into reluctance. The deal fell through due to problems in commissioning portraits of the Count's daughters and the Count's imprisonment by Charles VII's men in 1443.\nCardinal Beaufort and the Duke of Suffolk persuaded Henry that the best way to pursue peace with France was through a marriage with Margaret of Anjou, the niece of King Charles VII. Henry agreed, especially when he heard reports of Margaret's stunning beauty, and sent Suffolk to negotiate with Charles, who consented to the marriage on condition that he would not have to provide the customary dowry and instead would receive the province of Maine from the English. These conditions were agreed upon in the Treaty of Tours in 1444, but the cession of Maine was kept secret from Parliament, as it was known that this would be hugely unpopular with the English populace. The marriage took place at Titchfield Abbey on 23 April 1445, one month after Margaret's 15th birthday. She had arrived with an established household, composed primarily not of Angevins, but of members of Henry's royal servants; this increase in the size of the royal household, and a concomitant increase on the birth of their son, Edward of Westminster, in 1453, led to proportionately greater expense but also to greater patronage opportunities at Court.\nHenry had wavered in yielding Maine to Charles, knowing that the move was unpopular and would be opposed by the Dukes of Gloucester and York, and also because Maine was vital to the defence of Normandy. However, Margaret was determined that he should see it through. As the treaty became public knowledge in 1446, public anger focused on the Earl of Suffolk, but Henry and Margaret were determined to protect him.\nAscendancy of Suffolk and Somerset.\nIn 1447, the king and queen summoned the Duke of Gloucester to appear before parliament on the charge of treason. Queen Margaret had no tolerance for any sign of disloyalty toward her husband and kingdom, thus any suspicion of this was immediately brought to her attention. This move was instigated by Gloucester's enemies, the earl of Suffolk, whom Margaret held in great esteem, and the ageing Cardinal Beaufort and his nephew, Edmund Beaufort, Earl of Somerset. Gloucester was put in custody in Bury St Edmunds, where he died, probably of a heart attack (although contemporary rumours spoke of poisoning) before he could be tried.\nThe Duke of York, being the most powerful duke in the realm and also being both an agnate and the heir general of Edward III (thus having, according to some, a better claim to the throne than Henry VI himself), probably had the best chances to succeed to the throne after Gloucester. However, he was excluded from the court circle and sent to govern Ireland, while his opponents, the earls of Suffolk and Somerset, were promoted to dukes, a title at that time still normally reserved for immediate relatives of the monarch. The new duke of Somerset was sent to France to assume the command of the English forces; this prestigious position was previously held by the duke of York himself, who was dismayed at his term not being renewed and at seeing his enemy take control of it.\nIn the later years of Henry's reign, the monarchy became increasingly unpopular, due to a breakdown in law and order, corruption, the distribution of royal land to the king's court favourites, the troubled state of the crown's finances, and the steady loss of territories in France. In 1447, this unpopularity took the form of a Commons campaign against William de la Pole, 1st Duke of Suffolk, who was the most unpopular of all the king's entourage and widely seen as a traitor. He was impeached by Parliament to a background that has been called \"the baying for Suffolk's blood [by] a London mob\", to the extent that Suffolk admitted his alarm to Henry. Ultimately, Henry was forced to send him into exile, but Suffolk's ship was intercepted in the English Channel. His murdered body was found on the beach at Dover.\nHenry's mental health began to deteriorate in the late 1440s. He exhibited possible signs of paranoia (the arrest of Duke Humphrey in 1447) and grandiosity (the scale of his plans of expansion for Eton Chapel in 1449 and King's College in 1446). By 1449, Henry had many critics questioning his ability to rule due to his mental health.\nIn 1449, the Duke of Somerset, leading the campaign in France, reopened hostilities in Normandy (although he had previously been one of the main advocates for peace), but by the autumn he had been pushed back to Caen. By 1450, the French had retaken the whole province, so hard won by Henry V. Returning troops, who had often not been paid, added to the lawlessness in the southern counties of England. Jack Cade led a rebellion in Kent in 1450, calling himself \"John Mortimer\", apparently in sympathy with York, and setting up residence at the White Hart Inn in Southwark (the white hart had been the symbol of the deposed Richard II). Henry came to London with an army to crush the rebellion, but on finding that Cade had fled kept most of his troops behind while a small force followed the rebels and met them at Sevenoaks. The flight proved to have been tactical: Cade successfully ambushed the force in the Battle of Solefields (near Sevenoaks) and returned to occupy London. In the end, the rebellion achieved nothing, and London was retaken after a few days of disorder; but this was principally because of the efforts of its own residents rather than those of the army. At any rate, the rebellion showed that feelings of discontent were running high.\nIn 1451, the Duchy of Aquitaine, held by England since Henry II's time, was also lost. In October 1452, an English advance in Aquitaine retook Bordeaux and was having some success, but by 1453 Bordeaux was lost again, leaving Calais as England's only remaining territory on the continent.\nIllness and the ascendancy of York.\nIn 1452, the Duke of York was persuaded to return from Ireland, claim his rightful place on the council, and put an end to bad government. His cause was a popular one and he soon raised an army at Shrewsbury. The court party, meanwhile, raised their own similar-sized force in London. A stand-off took place south of London, with the Duke of York presenting a list of grievances and demands to the court circle, including the arrest of Edmund Beaufort, Duke of Somerset. The king initially agreed, but Margaret intervened to prevent the arrest of Beaufort. By 1453, Somerset's influence had been restored, and York was again isolated. The court party was also strengthened by the announcement that the queen was pregnant.\nHowever, in August 1453, Henry received the bad news that his army had been routed in the decisive Battle of Castillon. Shortly thereafter, Henry experienced a mental breakdown. He became completely unresponsive to everything that was going on around him for more than a year. At the age of 31, he \"fell by a sudden and accidental fright into such a weak state of health that for a whole year and a half he had neither sense nor reason capable of carrying on the government and neither physician nor medicine could cure that infirmity...\" and he was, \"...smitten with a frenzy and his wit and reason withdrawn.\" Henry even failed to respond to the birth of his son Edward six months into the illness.\nThe Duke of York, meanwhile, had gained a very important ally, Richard Neville, 16th Earl of Warwick, one of the most influential magnates and possibly richer than York himself. York was named regent as Protector of The Realm in 1454. The queen was excluded completely, and Edmund Beaufort was detained in the Tower of London, while many of York's supporters spread rumours that Edward was not the king's son, but Beaufort's. Other than that, York's months as regent were spent tackling the problem of government overspending.\nWars of the Roses.\nAround Christmas Day 1454, King Henry regained his senses. Disaffected nobles who had grown in power during Henry's reign, most importantly the Earls of Warwick and Salisbury, took matters into their own hands. They backed the claims of the rival House of York, first to the control of government, and then to the throne itself (from 1460), pointing to York's better descent from Edward III. It was agreed that York would become Henry's successor, despite York being older. In 1457, Henry created the Council of Wales and the Marches for his son Prince Edward, and in 1458, he attempted to unite the warring factions by staging the Loveday in London as an arbitration event.\nDespite such attempts at reconciliation, tensions between the houses of Lancaster and York eventually broke out in open war. Their forces engaged at the Battle of Northampton, 10 July 1460, where the king was captured and taken into captivity under the Yorkists. Queen Margaret, who also had been on the field, managed to escape with her son, the prince, fleeing through Wales to Scotland where she found refuge in the court of the queen regent, Mary of Guelders, recent widow of James II. Here she set about eliciting support for her husband from that kingdom.\nRe-entering England at the end of the year, the English queen in force engaged with the Duke of York at the Battle of Wakefield, 30 December 1460, where York fell. A few weeks later, at the Second Battle of St Albans, 17 February 1461, her forces engaged with the Earl of Warwick, under whose custody her husband was being held. She defeated Warwick and liberated the king. Henry's mental state at the time was such that he had reputedly laughed and sung as the battle raged around him.\nExile.\nThe victory however was short-lived. Within six weeks, the king and queen's forces were once more defeated at the Battle of Towton, 29 March 1461, by the Duke of York's son, Edward. Henry and Margaret together evaded capture by Edward and this time they both escaped into exile in Scotland. With Scottish aid, Margaret now travelled to the continent to elicit further support for her husband's cause.\nMainly under her leadership, Lancastrian resistance continued in the north of England during the first period of Edward IV's reign but met with little luck on the field. At the same time as Henry's cause was beginning to look increasingly desperate in military terms, an English embassy to Scotland, through the Earl of Warwick on behalf of Edward, served to further weaken his interests at the Scottish Court in political terms. After the queen mother's death on 1 December 1463, Scotland now actively sued for peace with England and the exiled king passed back across the border to try his fortune with those nobles in the north of England and Wales who were still loyal.\nFollowing defeat in the Battle of Hexham, 15 May 1464, Henry, as a fugitive in his own land, continued to be afforded safety in various Lancastrian houses across the north of England. Sir John Pennington provided refuge to Henry VI of England in Muncaster Castle following the battle. Legend has it that Henry VI left behind a Venetian glass bowl as a token of gratitude, known as the \"Luck of Muncaster\", ensuring the prosperity of the Pennington family as long as it remained intact.\nNonetheless, while he was in hiding at Waddington Hall, in Waddington, Lancashire, the home of Sir Richard Tempest, he was betrayed by \"a black monk of Addington\" and on 13 July 1464, a party of Yorkist men, including Sir Richard's brother John, entered the house for his arrest. Henry fled into nearby woods but was soon captured at Brungerley Hippings (stepping stones) over the River Ribble. He was subsequently held captive in the Tower of London.\nThe following poem has long been attributed to Henry, allegedly having been written during his imprisonment. However, a largely identical verse appears in William Baldwin's 1559 work \"The Mirror for Magistrates\", a collection of poems written from the perspective of historical figures.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nReturn to the throne.\nQueen Margaret, exiled in Scotland and later in France, was determined to win back the throne on behalf of her husband and her son, Edward of Westminster. By herself, there was little she could do. However, eventually, Edward IV fell out with two of his main supporters: Richard Neville, Earl of Warwick, and his own younger brother George, Duke of Clarence. At the urging of King Louis XI of France they formed a secret alliance with Margaret. After marrying his daughter Anne Neville to Henry and Margaret's son, Warwick returned to England, forced Edward IV into exile, and restored Henry VI to the throne on 3\u00a0October 1470; the term \"readeption\" is still sometimes used for this event. However, by this time, years in hiding followed by years in captivity had taken their toll on Henry. Warwick and Clarence effectively ruled in his name.\nHenry's return to the throne lasted less than six months. Warwick soon overreached himself by declaring war on Burgundy, whose ruler Charles the Bold responded by giving Edward IV the assistance he needed to win back his throne by force. Edward returned to England in early 1471 and was reconciled with Clarence. Warwick was killed at the Battle of Barnet on 14 April and the Yorkists won a final decisive victory at the Battle of Tewkesbury on 4 May, where Henry's son Edward of Westminster was killed.\nImprisonment and death.\nHenry was imprisoned in the Tower of London again and when the royal party arrived in London, he was reported dead. Official chronicles and documents state that the deposed king died on the night of 21 May 1471. In all likelihood, his opponents had kept him alive up to that point, rather than leave the Lancastrians with a far more formidable leader in Henry's son, Edward. However, once the last of the most prominent Lancastrian supporters had been either killed or exiled, it became clear that Henry VI would be a burden on Edward IV's reign. The common fear was the possibility of another noble using the mentally unstable king to further their own agenda.\nAccording to the \"Historie of the arrivall of Edward IV\", an official chronicle favourable to Edward IV, Henry died of melancholia, but it is widely suspected, however, that Edward IV, who was re-crowned the morning following Henry's death, had ordered his murder.\nSir Thomas More's \"History of Richard III\" explicitly states that Richard, who was then the Duke of Gloucester, killed Henry. More might have derived his opinion from Philippe de Commines' \"M\u00e9moires\".\nModern tradition places his death in Wakefield Tower, part of the Tower of London, but that is not supported by evidence, and is unlikely, since the tower was used for record storage at the time. Henry's actual place of death is unknown, though he was imprisoned within the Tower of London.\nKing Henry VI was originally buried in Chertsey Abbey in Surrey, but in 1484 Richard III had his body moved to St George's Chapel, Windsor Castle. When the body was exhumed in 1910, it was found to be tall with a damaged, abnormally thin skull and the fore-leg bone of a pig substituting his missing right arm. It was initially thought the damage to the skull indicated a violent death, however due to the difficult nature in identifying cause of death from bones alone, as well as the previous redisposition of his body, such evidence is inconclusive.\nLegacy.\nOverall, Henry VI is largely seen as a weak, inept king, whose inability to rule effectively led to the Wars of the Roses. He favoured diplomacy, rather than all-out war in the Hundred Years' War, in stark contrast to his father, Henry V, who led the victory at Agincourt. This allowed Henry to be heavily influenced by many nobles, such as William de la Pole, who oversaw significant English losses in France, such as the Siege of Orl\u00e9ans. On the other hand, many historians see Henry as a pious, generous king, who was victim of an unstable crown, caused by the deposition of Richard II. John Blacman, personal chaplain of Henry, described the king as a man without \"any crook or uncouth.\"\nArchitecture and education.\nHenry's one lasting achievement was his fostering of education: he founded Eton College; King's College, Cambridge; and All Souls College, Oxford. He continued a career of architectural patronage started by his father: King's College Chapel and Eton College Chapel and most of his other architectural commissions (such as his completion of his father's foundation of Syon Abbey) consisted of a late Gothic or Perpendicular-style church with a monastic or educational foundation attached. Each year on the anniversary of Henry VI's death, the Provosts of Eton and King's lay white lilies and roses, the respective floral emblems of those colleges, on the spot in the Wakefield Tower at the Tower of London where the imprisoned Henry VI was, according to tradition, murdered as he knelt at prayer. There is a similar ceremony at his resting place, St George's Chapel.\nPosthumous cult.\nMiracles were attributed to Henry, and he was informally regarded as a saint and martyr, addressed particularly in cases of adversity. The anti-Yorkist cult was encouraged by Henry VII of England as dynastic propaganda. A volume was compiled of the miracles attributed to him at St George's Chapel, Windsor, where Richard III had reinterred him, and Henry VII began building a chapel at Westminster Abbey to house Henry VI's relics. A number of Henry VI's miracles possessed a political dimension, such as his cure of a young girl afflicted with the King's evil, whose parents refused to bring her to the usurper, Richard III. By the time of Henry VIII's break with Rome, canonisation proceedings were under way. Hymns to him still exist, and until the Reformation his hat was kept by his tomb at Windsor, where pilgrims would put it on to enlist Henry's aid against migraines.\nNumerous miracles were credited to the dead king, including his raising the plague victim Alice Newnett from the dead and appearing to her as she was being stitched in her shroud. He also intervened in the attempted hanging of a man who had been unjustly condemned to death, accused of stealing some sheep. Henry placed his hand between the rope and the man's windpipe, thus keeping him alive, after which he was revived in the cart as it was taking him away for burial. He was also capable of inflicting harm, such as when he struck John Robyns blind after Robyns cursed \"Saint Henry\". Robyns was healed only after he went on a pilgrimage to the shrine of King Henry. A particular devotional act that was closely associated with the cult of Henry VI was the bending of a silver coin as an offering to the \"saint\" so that he might perform a miracle. One story had a woman, Katherine Bailey, who was blind in one eye. As she was kneeling at mass, a stranger told her to bend a coin to King Henry. She promised to do so, and as the priest was raising the communion host, her partial blindness was cured.\nAlthough Henry VI's shrine was enormously popular as a pilgrimage destination during the early decades of the 16th century, over time, with the lessened need to legitimise Tudor rule, his cult faded.\nIn culture.\nWilliam Shakespeare and possibly others completed the Henry VI trilogy around 1593, roughly 121 years after the real monarch's death. The period of history covered in the plays was between the funeral of Henry V (1422) to the Battle of Tewkesbury (1471).\nThough modern scholars are more interested in the context that the Henry VI trilogy paved for the more popular play \"Richard III\", it was very popular during Elizabethan times. Rather than being representative of the historical events or the actual life and temperament of Henry VI himself, the Shakespearean plays are more representative of the pivotal political situation in England at that time: international war in the form of the Hundred Years' War, and civil strife in the form of the War of the Roses.\nShakespeare's portrayal of Henry is notable in that it does not mention the King's madness. This is considered to have been a politically advisable move to not risk offending Elizabeth I whose family was descended from Henry's Lancastrian family. Instead, Henry is portrayed as a pious and peaceful man ill-suited to the crown. He spends most of his time in contemplation of the Bible and expressing his wish to be anyone other than a king. Shakespeare's Henry is weak-willed and easily influenced allowing his policies to be led by Margaret and her allies, and being unable to defend himself against York's claim to the throne. He takes an act of his own volition only just before his death when he curses Richard of Gloucester just before he is murdered. (Shakespeare, William: Henry VI, Part III Act 5, scene 6)\nThere have been , which include the bulk of Henry VI's cultural appearances in modern times. In screen adaptations of these plays Henry has been portrayed by: James Berry in the 1911 silent short \"Richard III\"; Miles Mander portrayed Henry VI in \"Tower of London\", a 1939 historical film loosely dramatising the rise to power of Richard III; Terry Scully in the 1960 BBC series \"An Age of Kings\" which contained all the history plays from \"Richard II\" to \"Richard III\"; Carl Wery in the 1964 West German TV version \"K\u00f6nig Richard III\"; David Warner in \"The Wars of the Roses\", a 1965\u201366 filmed version of the Royal Shakespeare Company performing the three parts of \"Henry VI\" (condensed and edited into two plays, \"Henry VI\" and \"Edward IV\") and \"Richard III\"; Peter Benson in the 1983 BBC versions of \"Henry VI\" part 1, 2, and 3 as well as \"Richard III\"; Paul Brennen in the 1989 film version of the full cycle of consecutive history plays performed, for several years, by the English Shakespeare Company; Edward Jewesbury in the 1995 film version of \"Richard III\" with Ian McKellen as Richard; James Dalesandro as Henry in the 2007 modern-day film version of \"Richard III\"; and Tom Sturridge as Henry to Benedict Cumberbatch's Richard III in the 2016 second season of the BBC series \"The Hollow Crown\", an adaptation of \"Henry VI\" (condensed into two parts) and \"Richard III\".\nHenry VI's marriage to Margaret of Anjou is the subject of the historical novel \"A Stormy Life\" (1867) by Lady Georgiana Fullerton. The novel \"The Triple Crown\" (1912) by Rose Schuster focuses on Henry's insanity. The novel \"London Bridge Is Falling\" (1934) by Philip Lindsay depicts Henry's response to Jack Cade's Rebellion. Henry VI also features in the short story \"The Duchess and the Doll\" (1950) by Edith Pargeter.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46860", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=46860", "title": "Catalan's constant", "text": "Number, approximately 0.916\nIn mathematics, Catalan's constant G, is the alternating sum of the reciprocals of the odd square numbers, being defined by:\n formula_1\nwhere \u03b2 is the Dirichlet beta function. Its numerical value is approximately (sequence in the OEIS)\n \"G\" = \u2026\nCatalan's constant was named after Eug\u00e8ne Charles Catalan, who found quickly-converging series for its calculation and published a memoir on it in 1865.\nUses.\nIn low-dimensional topology, Catalan's constant is 1/4 of the volume of an ideal hyperbolic octahedron, and therefore 1/4 of the hyperbolic volume of the complement of the Whitehead link. It is 1/8 of the volume of the complement of the Borromean rings.\nIn combinatorics and statistical mechanics, it arises in connection with counting domino tilings, spanning trees, and Hamiltonian cycles of grid graphs.\nIn number theory, Catalan's constant appears in a conjectured formula for the asymptotic number of primes of the form formula_2 according to Hardy and Littlewood's Conjecture F. However, it is an unsolved problem (one of Landau's problems) whether there are even infinitely many primes of this form.\nCatalan's constant also appears in the calculation of the mass distribution of spiral galaxies.\nProperties.\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in mathematics\nIs Catalan's constant irrational? If so, is it transcendental?\nMore unsolved problems in mathematics\nIt is not known whether G is irrational, let alone transcendental. G has been called \"arguably the most basic constant whose irrationality and transcendence (though strongly\nsuspected) remain unproven\".\nThere exist however partial results. It is known that infinitely many of the numbers \"\u03b2\"(2\"n\") are irrational, where \"\u03b2(s)\" is the Dirichlet beta function. In particular at least one of \"\u03b2\"(2), \"\u03b2\"(4), \"\u03b2\"(6), \"\u03b2\"(8), \"\u03b2\"(10) and \"\u03b2\"(12) must be irrational, where \"\u03b2\"(2) is Catalan's constant. These results by Wadim Zudilin and Tanguy Rivoal are related to similar ones given for the odd zeta constants \u03b6(2\"n+1\").\nCatalan's constant is known to be an algebraic period, which follows from some of the double integrals given below.\nSeries representations.\nCatalan's constant appears in the evaluation of several rational series including:formula_3formula_4\nThe following two formulas involve quickly converging series, and are thus appropriate for numerical computation:\nformula_5\nand\nformula_6\nThe theoretical foundations for such series are given by Broadhurst, for the first formula, and Ramanujan, for the second formula. The algorithms for fast evaluation of the Catalan constant were constructed by E. Karatsuba. Using these series, calculating Catalan's constant is now about as fast as calculating Ap\u00e9ry's constant, formula_7.\nOther quickly converging series, due to Guillera and Pilehrood and employed by the y-cruncher software, include:\nformula_8\nformula_9\nformula_10\nAll of these series have time complexity formula_11.\nIntegral identities.\nAs Se\u00e1n Stewart writes, \"There is a rich and seemingly endless source of definite integrals that\ncan be equated to or expressed in terms of Catalan's constant.\" Some of these expressions include:\nformula_12\nwhere the last three formulas are related to Malmsten's integrals.\nIf K(\"k\") is the complete elliptic integral of the first kind, as a function of the elliptic modulus \"k\", then\nformula_13\nIf E(\"k\") is the complete elliptic integral of the second kind, as a function of the elliptic modulus \"k\", then\nformula_14\nWith the gamma function \u0393(\"x\" + 1) = \"x\"!\nformula_15\nThe integral\nformula_16\nis a known special function, called the inverse tangent integral, and was extensively studied by Srinivasa Ramanujan.\nRelation to special functions.\nG appears in values of the second polygamma function, also called the trigamma function, at fractional arguments:\nformula_17\nSimon Plouffe gives an infinite collection of identities between the trigamma function, \u03c02 and Catalan's constant; these are expressible as paths on a graph (see External links below).\nCatalan's constant occurs frequently in relation to the Clausen function, the inverse tangent integral, the inverse sine integral, the Barnes G-function, as well as integrals and series summable in terms of the aforementioned functions.\nAs a particular example, by first expressing the inverse tangent integral in its closed form \u2013 in terms of Clausen functions \u2013 and then expressing those Clausen functions in terms of the Barnes G-function, the following expression is obtained (see Clausen function for more):\nformula_18\nIf one defines the Lerch transcendent \u03a6(\"z\",\"s\",\"\u03b1\") by\nformula_19\nthen\nformula_20\nContinued fraction.\nG can be expressed in the following form:\nformula_21\nThe simple continued fraction is given by:\nformula_22\nThis continued fraction would have infinite terms if and only if formula_23 is irrational, which is still unresolved.\nThe following continued fraction representation gives (asymptotically) 2.08 new correct decimal places per cycle:\nformula_24\nwith\nformula_25\nformula_26\nKnown digits.\nThe number of known digits of Catalan's constant G has increased dramatically during the last decades. This is due both to the increase of performance of computers as well as to algorithmic improvements.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46861", "revid": "7131173", "url": "https://en.wikipedia.org/wiki?curid=46861", "title": "Henry IV of England", "text": "King of England from 1399 to 1413\nHenry IV (c.\u2009April 1367 \u2013 20 March 1413), also known as Henry Bolingbroke, was King of England from 1399 to 1413, Lord of Ireland and duke of Aquitaine. Henry was the son of John of Gaunt, Duke of Lancaster and a grandson of King Edward III.\nWhen Henry came of age, he was involved in the 1388 revolt of the Lords Appellant against his first cousin, King Richard II. Henry was later exiled from England in 1397. After Henry's father died in 1399, Richard blocked Henry's inheritance to the holdings of the House of Lancaster. So, Henry rallied Lancastrian supporters, imprisoned Richard II and formally deposed him, usurping the throne. These actions later contributed to dynastic disputes in the Wars of the Roses (1455\u20131487).\nHenry was the first English ruler whose mother tongue was English (rather than French) since the Norman Conquest, over 300 years earlier. He was also the first monarch to come from the House of Lancaster. As king, he faced a number of rebellions, most seriously those of Owain Glynd\u0175r, the last Welshman to claim the title of Prince of Wales, and the English knight Henry Percy (Hotspur), who was killed in the Battle of Shrewsbury in 1403. Henry IV had six children from his first marriage to Mary de Bohun, while his second marriage to Joan of Navarre produced no surviving children. Henry and Mary's eldest son, Henry of Monmouth, assumed the reins of government in 1410 as the king's health worsened. Henry IV died in 1413, and his son succeeded him as Henry V.\nEarly life.\nHenry was born at Bolingbroke Castle, in Lincolnshire, to John of Gaunt and Blanche of Lancaster. His epithet \"Bolingbroke\" was derived from his birthplace. Gaunt was the third son of King Edward III. Blanche was the daughter of the wealthy royal politician and nobleman Henry, Duke of Lancaster. Gaunt enjoyed a position of considerable influence during much of the reign of his own nephew, King Richard II. Henry's elder sisters were Philippa, Queen of Portugal, and Elizabeth, Duchess of Exeter. His younger half-sister Katherine, Queen of Castile, was Gaunt's daughter with his second wife, Constance of Castile. Henry also had four half-siblings born to Katherine Swynford, originally his sisters' governess, then his father's longstanding mistress and later third wife. These illegitimate (but later legitimized) children were given the surname Beaufort.\nHenry's relationship with his stepmother Katherine Swynford was amicable, but his relationship with the Beauforts varied. In his youth, he seems to have been close to all of them, but rivalries with Henry and Thomas Beaufort caused trouble after 1406. Ralph Neville, 4th Baron Neville, married Henry's half-sister Joan Beaufort. Neville remained one of his strongest supporters, and so did his eldest half-brother John Beaufort, even though Henry revoked Richard II's grant to John of a marquessate. Katherine Swynford's son from her first marriage, Thomas, was another loyal companion. Thomas Swynford was Constable of Pontefract Castle, where Richard II is said to have died.\nOne of Gaunt's esquires, Thomas Burton, was appointed Henry's tutor in December 1374, and a Gascon named William Montendre was made his instructor in military matters in 1376. It was likely Hugh Herle who taught Henry to read and write in English and French and some Latin as well; Herle served as Henry's chaplain for many years. By 1381\u20131382, Henry is recorded riding, hunting, and travelling with his father, as well as jousting and observing official events. He later became an active and highly successful jouster. He was besieged with King Richard and others in the Tower of London during the Peasants' Revolt in 1381 and narrowly escaped being murdered when the rebels stormed the tower. He was saved by a man named John Ferrour of Southwark, who participated in the Epiphany Rising against Henry, by then king of England, nearly twenty years later and was pardoned.\nHenry had a close relationship with his father, but he did not participate much in public affairs while Gaunt was present in England. He accompanied Gaunt to negotiations with the French in Calais in November 1383. He took part in Richard's Scottish campaign with his father's forces in 1385, and he may have also served in an earlier incursion into Scotland by Gaunt. He was summoned to parliament for the first time in October 1385.\nIn his youth, Henry joined crusading expeditions; contemporaries remarked that he \"never lost a battle\", and John Gower described him as \"full of knighthood and all grace\".\nConflict at court.\nRelationship with Richard II.\nHenry experienced a more inconsistent relationship with King Richard II than his father had. First cousins and childhood playmates, they were admitted together as knights of the Order of the Garter in 1377, but Henry participated in the Lords Appellants' rebellion against the king in 1387. After regaining power, Richard did not punish Henry, although he did execute or exile many of the other rebellious barons. In fact, Richard elevated Henry from Earl of Derby to Duke of Hereford.\nHenry spent all of 1390 supporting the unsuccessful siege of Vilnius (capital of the Grand Duchy of Lithuania) by Teutonic Knights with 70 to 80 household knights. During this campaign, he bought captured Lithuanian women and children and took them back to K\u00f6nigsberg to be converted, even though Lithuanians had already been baptised by Polish priests for a decade by then.\nHenry's second expedition to Lithuania in 1392 illustrates the financial benefits to the Order of these guest crusaders. His small army consisted of over 100 men, including longbow archers and six minstrels, at a total cost to the Lancastrian purse of \u00a34,360. Despite the efforts of Henry and his English crusaders, two years of attacks on Vilnius proved fruitless. In 1392\u201393 Henry undertook a pilgrimage to Jerusalem, where he made offerings at the Holy Sepulchre and at the Mount of Olives. Later he vowed to lead a crusade to \"free Jerusalem from the infidel\", but he died before this could be accomplished.\nThe relationship between Henry and Richard had a second crisis. In 1398, a remark about Richard's rule by Thomas de Mowbray, 1st Duke of Norfolk, was interpreted as treason by Henry, who reported it to the king. The two dukes agreed to undergo a duel of honour (called by Richard) at Gosford Green near Caludon Castle, Mowbray's home in Coventry. Yet before the duel could take place, Richard decided to banish Henry from the kingdom (with the approval of Henry's father, John of Gaunt), although it is unknown where he spent his exile, to avoid further bloodshed. Mowbray was exiled for life.\nJohn of Gaunt died in February 1399. Without explanation, Richard cancelled the legal documents that would have allowed Henry to inherit Gaunt's land automatically. Instead, Henry would be required to ask Richard for the lands.\nAccession.\nAfter some hesitation, Henry met the exiled Thomas Arundel, former archbishop of Canterbury, who had lost his position because of his involvement with the Lords Appellant. Sailing from Boulogne, Henry and Arundel returned to England while Richard was on a military campaign in Ireland. With Arundel as his advisor, Henry began a military campaign, confiscating land from those who opposed him and ordering his soldiers to destroy much of Cheshire. Henry initially announced that he intended to reclaim his rights as Duke of Lancaster, though he quickly gained enough power and support to have himself declared King Henry IV, imprison Richard (who died in prison, most probably forcibly starved to death), and bypass Richard's heir-presumptive, Edmund de Mortimer, 5th Earl of March.\nHenry's 13 October 1399 coronation at Westminster Abbey may have been the first time since the Norman Conquest that the monarch made an address in English. Henry was also the first king to be anointed with the Virgin Mary's sacred oils.\nIn January 1400, Henry quashed the Epiphany Rising, a rebellion by Richard's supporters who plotted to assassinate him. Henry was forewarned and raised an army in London, at which the conspirators fled. They were apprehended and executed without trial.\nIn August 1400, urgently wanting to defend the Anglo-Scottish border, and to overcome his predecessor's legacy of failed military campaigns, Henry invaded Scotland. A large army was assembled slowly and marched into Scotland. Not only was no pitched battle ever attempted, but the King did not try and besiege Scotland's capital, Edinburgh. Henry's army left at the end of the summer after only a brief stay, mostly camped near Leith (near Edinburgh) where it could maintain contact with its supply fleet. The campaign ultimately accomplished little except to deplete further the king's coffers, and is historically notable only for being the last one led by an English king on Scottish soil.\nReign.\nHenry consulted with Parliament frequently, but was sometimes at odds with the members, especially over ecclesiastical matters. In January 1401, Arundel convened a convocation at St. Paul's cathedral to address Lollardy. Henry dispatched a group to implore the clergy to address the heresies that were causing turmoil in England and confusion among Christians, and to impose penalties on those responsible. A short time later the convocation along with the House of Commons petitioned Henry to take action against the Lollards. On this advice, Henry obtained from Parliament the enactment of \"De heretico comburendo\" in 1401, which prescribed the burning of heretics, an act done mainly to suppress the Lollard movement. In 1404 and 1410, Parliament suggested confiscating church land, in which both attempts failed to gain support.\nRebellions.\nHenry spent much of his reign defending himself against plots, rebellions, and assassination attempts. Henry's first major problem as monarch was what to do with the deposed Richard. After the early assassination plot was foiled in January 1400, Richard died in prison aged 33, probably of starvation on Henry's order. Some chroniclers claimed that the despondent Richard had starved himself, which would not have been out of place with what is known of Richard's character. Though council records indicate that provisions were made for the transportation of the deposed king's body as early as 17 February, there is no reason to believe that he did not die on 14 February, as several chronicles stated. It can be positively said that he did not suffer a violent death, for his skeleton, upon examination, bore no signs of violence; whether he did indeed starve himself or whether that starvation was forced upon him are matters for lively historical speculation.\nAfter his death, Richard's body was put on public display in Old St Paul's Cathedral, both to prove to his supporters that he was truly dead and also to prove that he had not suffered a violent death. This did not stop rumours from circulating for years after that he was still alive and waiting to take back his throne, and that the body displayed was that of Richard's chaplain, a priest named Maudelain, who greatly resembled him. Henry had the body discreetly buried in the Dominican Priory at Kings Langley, Hertfordshire, where it remained until King Henry V brought the body back to London and buried it in the tomb that Richard had commissioned for himself in Westminster Abbey.\nRebellions continued throughout the first 10 years of Henry's reign, including the revolt of Owain Glynd\u0175r, who declared himself Prince of Wales in 1400, and the rebellions led by Henry Percy, 1st Earl of Northumberland, from 1403. The first Percy rebellion ended in the Battle of Shrewsbury in 1403 with the death of the earl's son Henry, a renowned military figure known as \"Hotspur\" for his speed in advance and readiness to attack. Also in this battle, Henry IV's eldest son, Henry of Monmouth, later King Henry V, was wounded by an arrow in his face. He was cared for by royal physician John Bradmore. Despite this, the Battle of Shrewsbury was a royalist victory. Monmouth's military ability contributed to the king's victory (though Monmouth seized much effective power from his father in 1410).\nIn the last year of Henry's reign, the rebellions picked up speed. \"The old fable of a living Richard was revived\", notes one account, \"and emissaries from Scotland traversed the villages of England, in the last year of Henry's reign, declaring that Richard was residing at the Scottish Court, awaiting only a signal from his friends to repair to London and recover his throne.\"\nA suitable-looking impostor was found and King Richard's old groom circulated word in the city that his master was alive in Scotland. \"Southwark was incited to insurrection\" by Sir Elias Lyvet (Levett) and his associate Thomas Clark, who promised Scottish aid in carrying out the insurrection. Ultimately, the rebellion came to nought. Lyvet was released and Clark thrown into the Tower of London.\nForeign relations.\nEarly in his reign, Henry hosted the visit of Manuel II Palaiologos, the only Byzantine emperor ever to visit England, from December 1400 to February 1401 at Eltham Palace, with a joust being given in his honour. Henry also sent monetary support with Manuel upon his departure to aid him against the Ottoman Empire.\nIn 1406, English pirates captured the future James I of Scotland, aged eleven, off the coast of Flamborough Head as he was sailing to France. James was delivered to Henry IV and remained a prisoner until after the death of Henry's son, Henry V.\nFinal illness and death.\nThe later years of Henry's reign were marked by serious health problems. He had a disfiguring skin disease and, more seriously, suffered acute attacks of a grave illness in June 1405; April 1406; June 1408; during the winter of 1408\u201309; December 1412; and finally a fatal bout in March 1413. In 1410, Henry had provided his royal surgeon Thomas Morstede with an annuity of \u00a340 p.a. which was confirmed by Henry V immediately after his succession. This was so that Morstede would \"not be retained by anyone else\". Medical historians have long debated the nature of this affliction or afflictions. The skin disease might have been leprosy (which did not necessarily mean precisely the same thing in the 15th century as it does to modern medicine), perhaps psoriasis, or a different disease. The acute attacks have been given a wide range of explanations, from epilepsy to a form of cardiovascular disease. Some medieval writers felt that he was struck with leprosy as a punishment for his treatment of Richard le Scrope, Archbishop of York, who was executed in June 1405 on Henry's orders after a failed coup.\nAccording to Holinshed, it was predicted that Henry would die in Jerusalem, and Shakespeare's play repeats this prophecy. Henry took this to mean that he would die on crusade. In reality, he died in the Jerusalem Chamber in the abbot's house of Westminster Abbey, on 20 March 1413 during a convocation of Parliament. His executor, Thomas Langley, was at his side.\nBurial.\nDespite the example set by most of his recent predecessors, Henry and his second wife, Joan, were not buried at Westminster Abbey but at Canterbury Cathedral, on the north side of Trinity Chapel and directly adjacent to the shrine of St Thomas Becket. Becket's cult was then still thriving, as evidenced in the monastic accounts and in literary works such as \"The Canterbury Tales\", and Henry seemed particularly devoted to it, or at least keen to be associated with it. The reasons for his interment in Canterbury are debatable, but it is highly likely that Henry deliberately associated himself with the martyr saint for reasons of political expediency, namely, the legitimisation of his dynasty after seizing the throne from Richard II. Significantly, at his coronation, he was anointed with holy oil that had reportedly been given to Becket by the Virgin Mary shortly before his death in 1170; this oil was placed inside a distinct eagle-shaped container of gold. According to one version of the tale, the oil had then passed to Henry's maternal grandfather, Henry of Grosmont, Duke of Lancaster.\nProof of Henry's deliberate connection to Becket lies partially in the structure of the tomb itself. The wooden panel at the western end of his tomb bears a painting of the martyrdom of Becket, and the tester, or wooden canopy, above the tomb is painted with Henry's personal motto, 'Soverayne', alternated by crowned golden eagles. Likewise, the three large coats of arms that dominate the tester painting are surrounded by collars of SS, a golden eagle enclosed in each tiret. The presence of such eagle motifs points directly to Henry's coronation oil and his ideological association with Becket. Sometime after Henry's death, an imposing tomb was built for him and his queen, probably commissioned and paid for by Queen Joan herself. Atop the tomb chest lie detailed alabaster effigies of Henry and Joan, crowned and dressed in their ceremonial robes. Henry's body was evidently well embalmed, as an exhumation in 1832 established, allowing historians to state with reasonable certainty that the effigies do represent accurate portraiture.\nTitles and arms.\nTitles.\nDuke of Hereford was a title in the Peerage of England. It was created in 1397 for Richard II's cousin, Henry Bolingbroke, due to his support for the King in his struggle against their uncle Thomas of Woodstock, 1st Duke of Gloucester. As such it was a \"duketti\" (\"little dukes\") title. It merged in the crown on Henry's usurpation two years later, and has never since been recreated.\nArms.\nBefore his father's death in 1399, Henry bore the arms of the kingdom, differenced by a \"label of five points ermine\". After his father's death, the difference changed to a \"label of five points per pale ermine and France\".\nMarriages and issue.\nFirst marriage: Mary de Bohun.\nHenry married Mary de Bohun at an unknown date, but her marriage licence, purchased by Henry's father John of Gaunt in June 1380, is preserved at the National Archives. The accepted date of the ceremony is 5 February 1381, at Mary's family home of Rochford Hall, Essex. The near-contemporary chronicler Jean Froissart reports a rumour that Mary's sister Eleanor de Bohun kidnapped Mary from Pleshey Castle and held her at Arundel Castle, where she was kept as a novice nun; Eleanor's intention was to control Mary's half of the Bohun inheritance (or to allow her husband, Thomas, Duke of Gloucester, to control it). There Mary was persuaded to marry Henry. She died in 1394 and therefore never became queen after Henry claimed the English throne in 1399. They had six children:\nHenry had four sons from his first marriage, which was undoubtedly a clinching factor in his acceptability for the throne. By contrast, Richard II had no children and Richard's heir-presumptive Edmund Mortimer was only seven years old. The only two of Henry's six children who produced legitimate children to survive to adulthood were Henry V and Blanche, whose son, Rupert, was the heir to the Electorate of the Palatinate until his death at 20. All three of his other sons produced illegitimate children. Henry IV's male Lancaster line ended in 1471 during the War of the Roses, between the Lancastrians and the Yorkists, with the deaths of his grandson Henry VI and Henry VI's son Edward, Prince of Wales. Mary de Bohun died giving birth to her daughter Philippa in 1394.\nSecond marriage: Joan of Navarre.\nOn 7 February 1403, nine years after the death of his first wife, Henry married Joan, the daughter of Charles II of Navarre, at Winchester. She was the widow of John IV, Duke of Brittany (known in traditional English sources as John V), with whom she had 9 children; however, her marriage to King Henry produced no surviving children. In 1403, Joan of Navarre gave birth to stillborn twins fathered by King Henry IV, which was the last pregnancy of her life. Joan was 35 years old at the time.\nMistresses.\nBy an unknown mistress, Henry IV had one illegitimate child:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46863", "revid": "25046916", "url": "https://en.wikipedia.org/wiki?curid=46863", "title": "Asymmetric warfare", "text": "War between belligerents whose relative military power differs significantly\nAsymmetric warfare (or asymmetric engagement) is a type of war between belligerents whose relative military power, strategy or tactics differ significantly. This type of warfare often, but not necessarily, involves insurgents, terrorist groups, or resistance militias operating within territory mostly controlled by the superior force.\n\"Asymmetrical warfare\" can also describe a conflict in which belligerents' material resources are uneven, and consequently, each may attempt to exploit each other's relative weaknesses. Such struggles often involve unconventional warfare, with the weaker side attempting to use strategy to offset deficiencies in the quantity or quality of their forces and equipment. Such strategies may not necessarily be militarized. This is in contrast to \"symmetrical warfare\", where two powers have comparable military power, resources, and rely on similar tactics and victory standards.\nConventional militaries consider asymmetric warfare a form of irregular warfare \u2013 conflicts in which nominally weaker adversaries are not regular military forces of nation-states. For military analysts the term is most often used as an umbrella term to describe what is also called \"guerrilla warfare\", \"insurgency\", \"counterinsurgency\", \"rebellion\", \"terrorism\", and \"counterterrorism\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDefinition and differences.\nThe popularity of the term dates from Andrew J. R. Mack's 1975 article \"Why Big Nations Lose Small Wars\" in \"World Politics\", in which \"asymmetric\" referred to a significant disparity in relative power between opposing actors in a conflict. \"Power,\" in this sense, is broadly understood to mean material power, such as a large army, sophisticated weapons, an advanced economy, and so on. Mack's analysis was largely ignored in its day, but the end of the Cold War sparked renewed interest among academics. By the late 1990s, new research building on Mack's work was beginning to mature; after 9/11, the U.S. military began once again to grapple with asymmetric warfare strategy.\nSince 2004, the discussion of asymmetric warfare has been complicated by the tendency of academic and military officials to use the term in different ways, as well as by its close association with guerrilla warfare, insurgency, terrorism, counterinsurgency, and counterterrorism.\nAcademic authors tend to focus on explaining two puzzles in asymmetric conflict. First, if \"power\" determines victory, there must be reasons why weaker actors decide to fight more powerful actors. Key explanations include:\nSecond, if \"power,\" as generally understood, leads to victory in war, then there must be an explanation for why the \"weak\" can defeat the \"strong.\" Key explanations include:\nAsymmetric conflicts include interstate and civil wars, and over the past two hundred years, have generally been won by strong actors. Since 1950, however, weak actors have won the majority of asymmetric conflicts. In asymmetric conflicts conflict escalation can be rational for one side.\nStrategic basis.\nIn most conventional warfare, the belligerents deploy forces of a similar type, and the outcome can be predicted by the quantity or quality of the opposing forces, for example, better command and control of theirs (c2). There are times when this is the case, and conventional forces are not easily compared, making it difficult for opposing sides to engage. An example of this is the standoff between the continental land forces of the French Army and the maritime forces of the United Kingdom's Royal Navy during the French Revolutionary and Napoleonic Wars. In the words of Admiral Jervis during the campaigns of 1801, \"I do not say, my Lords, that the French will not come. I say only they will not come by sea\", and a confrontation that Napoleon Bonaparte described as that between the elephant and the whale.\nTactical basis.\nThe tactical success of asymmetric warfare is dependent on at least some of the following assumptions:\nTerrorism.\nThere are two opposing viewpoints on the relationship between asymmetric warfare and terrorism. In the modern context, asymmetric warfare is increasingly considered a component of fourth generation warfare. When practiced outside the laws of war, it is often defined as terrorism, though rarely by its practitioners or their supporters. The other view is that asymmetric warfare does not coincide with terrorism.\nUse of terrain.\nTerrain that limits mobility, such as forests and mountains, can be used as a force multiplier by the smaller force and as a force inhibitor against the larger one, especially one operating far from its logistical base. Such terrain is called \"difficult terrain\". Urban areas, though generally having good transport access, provide innumerable ready-made defensible positions with simple escape routes and can also become rough terrain if prolonged combat fills the streets with rubble:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The contour of the land is an aid to the army, sizing up opponents to determine victory and assessing dangers and distance. \"Those who do battle without knowing these will lose.\"\u2014\u200a\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The guerrillas must move amongst the people as a fish swims in the sea.\u2014\u200a\nIn the 12th century, irregulars known as the Assassins were successful in the Nizari Ismaili state. The \"state\" consisted of fortresses (such as the Alamut Castle) built on strategic mountaintops and highlands with difficult access, surrounded by hostile lands. The Assassins developed tactics to eliminate high-value targets, threatening their security, including the Crusaders.\nIn the American Revolutionary War, Patriot Lieutenant Colonel Francis Marion, known as the \"Swamp Fox,\" took advantage of irregular tactics, interior lines, and the wilderness of colonial South Carolina to hinder larger British regular forces.\nYugoslav Partisans, starting as small detachments around mountain villages in 1941, fought the German and other Axis occupation forces, successfully taking advantage of the rough terrain to survive despite their small numbers. Over the next four years, they slowly forced their enemies back, recovering population centers and resources, eventually growing into the regular Yugoslav Army.\nThe Vietnam war is a classical example of the use of terrain to fight an asymmetrical war, The North Vietnamese army (NVA) and Viet Cong (VC) used the forested and mountainous terrain of Vietnam to allow for effective concealment of troop movements in spite of superior enemy air power. This allowed supplying troops to be possible without incurring heavy losses from American airstrikes, who could not effectively identify or track their movements from the air. This was true to such an extent that the US employed defoliation methods such as the use of Agent Orange and extensive Napalm use to make forested areas visible from the air. The NVA and VC also used intricate tunnel systems, such as the C\u1ee7 Chi tunnels, which enabled them to move undetected, store supplies, and evade U.S. search-and-destroy missions.\nRole of civilians.\nCivilians can play a vital role in determining the outcome of an asymmetric war. In such conflicts, when it is easy for insurgents to assimilate into the population quickly after an attack, tips on the timing or location of insurgent activity can severely undermine the resistance. An information-central framework, in which civilians are seen primarily as sources of strategic information rather than resources, provides a paradigm to understand better the dynamics of such conflicts where civilian information-sharing is vital. The framework assumes that:\nGiven the additional assumption that the larger or dominant force is the government, the framework suggests the following implications:\nA survey of the empirical literature on conflict, does not provide conclusive evidence on the claims. But the framework gives a starting point to explore the role of civilian information sharing in asymmetric warfare.\nWar by proxy.\nWhere asymmetric warfare is carried out (generally covertly) by allegedly non-governmental actors who are connected to or sympathetic to a particular nation's (the \"state actor's\") interest, it may be deemed \"war by proxy.\" This is typically done to give the state actor \"deniability\". The deniability can be crucial to keep the state actor from being tainted by the actions, to allow the state actor to negotiate in apparent good faith by claiming they are not responsible for the actions of parties who are merely sympathizers, or to avoid being accused of belligerent actions or war crimes. If proof emerges of the true extent of the state actor's involvement, this strategy can backfire; for example, see Iran-contra affair and Philip Agee.\nExamples.\nAmerican Indian Wars.\nBenjamin Church designed his force primarily to emulate Native American patterns of war. Toward this end, Church endeavored to learn to fight like Native Americans from Native Americans. Americans became rangers exclusively under the tutelage of the Native American allies. (Until the end of the colonial period, rangers depended on Native Americans as both allies and teachers.)\nChurch developed a special full-time unit mixing white colonists selected for frontier skills with friendly Native Americans to carry out offensive strikes against hostile Native Americans in terrain where normal militia units were ineffective. Church paid special care to outfitting, supplying and instructing his troops in ways inspired by indigenous methods of warfare and ways of living. He emphasized the adoption of indigenous techniques, which prioritized small, mobile and flexible units which used the countryside for cover, in lieu of massed frontal assaults by large formations. Benjamin Church is sometimes referred to as the father of Unconventional warfare. \nAmerican Revolutionary War.\nFrom its initiation, the American Revolutionary War was, necessarily, a showcase for asymmetric techniques. In the 1920s, Harold Murdock of Boston attempted to solve the puzzle of the first shots fired at Lexington, Massachusetts and came to the suspicion that the few dozen American minutemen who gathered before sunrise to await the arrival of a column of British regulars were sent to provoke an incident which could be used for Patriot propaganda purposes. The return of the British column to Boston following search operations at Concord, Massachusetts was subject to constant skirmishing attacks by minutemen gathered from communities all along the route, making maximum use of the terrain (particularly, trees and stone field walls) to overcome the limitations of their weapons \u2013 muskets with an effective range of only about 50\u201370 meters. Throughout the war, skirmishing tactics against enemy forces on the move continued to be a key factor in American victories, particularly in the western theater of the American Revolutionary War.\nDuring the Revolutionary War, both British and American warships engaged in asymmetric warfare tactics, primarily against civilian targets such as merchant vessels and coastal communities. Small Continental Navy ships such as USS \"Reprisal\", USS \"Ranger\", and USS \"Surprise\" made incursions into British and Irish waters during the war, attacking British-flagged merchantmen and forcing Britain to deploy more warships to the area along with increasing coastal defences. Although these incursions only inflicted minor losses on the Royal Navy and Britain's merchant fleet, they had a disproportionate psychological effect on the British public, which responded by demanding greater protection from American naval attacks. The Royal Navy also used asymmetric tactics to target American coastal communities, such as the burning of Falmouth by a force under Captain Henry Mowat. These attacks, carried out using small, mobile forces, spread terror among the Americans and destroyed vast quantities of buildings and material in exchange for minor losses.\nIn 1778, France entered the Revolutionary War on the American side, transforming the conflict into a global one and sharply reducing the asymmetric nature of the war. However, in the backcountry of North America, particularly in the American South, engagements between British and American forces often remained mostly asymmetric in nature.\nAmerican Civil War.\nThe American Civil War saw the rise of asymmetric warfare in the Border States, and in particular on the US Western Territorial Border after the Kansas-Nebraska Act of 1854 opened the territories to vote on the expansion of slavery beyond the Missouri Compromise lines. Political implications of this broken 1820's compromise were nothing less than the potential expansion of slavery all across the North American continent, including the northern reaches of the annexed Mexican territories to California and Oregon. So the stakes were high, and it caused a flood of immigration to the border: some to grab land and expand slavery west, others to grab land and vote down the expansion of slavery. The pro-slavery land grabbers began asymmetric, violent attacks against the more pacifist abolitionists who had settled Lawrence and other territorial towns to suppress slavery. John Brown, the abolitionist, travelled to Osawatomie in the Kansas Territory expressly to foment retaliatory attacks back against the pro-slavery guerrillas who, by 1858, had twice ransacked both Lawrence and Osawatomie (where one of Brown's sons was shot dead).\nThe abolitionists would not return the attacks and Brown theorized that a violent spark set off on \"the Border\" would be a way to finally ignite his long hoped-for slave rebellion. Brown had broad-sworded slave owners at Potawatomi Creek, so the bloody civilian violence was initially symmetrical; however, once the American Civil War ignited in 1861, and when the state of Missouri voted overwhelmingly not to secede from the Union, the pro-slavers on the MO-KS border were driven either south to Arkansas and Texas, or underground\u2014where they became guerrilla fighters and \"Bushwhackers\" living in the bushy ravines throughout northwest Missouri across the (now) state line from Kansas. The bloody \"Border War\" lasted all during the Civil War (and long after with guerrilla partisans like the James brothers cynically robbing and murdering, aided and abetted by lingering lost causers). Tragically the Western Border War was an asymmetric war: pro-slavery guerrillas and paramilitary partisans on the pro-Confederate side attacked pro-Union townspeople and commissioned Union military units, with the Union army trying to keep both in check: blocking Kansans and pro-Union Missourians from organizing militarily against the marauding Bushwhackers.\nThe worst act of domestic terror in U.S. history came in August 1863 when paramilitary guerrillas amassed 350 strong and rode all night 50 miles across eastern Kansas to the abolitionist stronghold of Lawrence (a political target) and destroyed the town, gunning down 150 civilians. The Confederate officer whose company had joined Quantrill's Raiders that day witnessed the civilian slaughter and forbade his soldiers from participating in the carnage. The commissioned officer refused to participate in Quantrill's asymmetric warfare on civilians.\nPhilippine\u2013American War.\nThe Philippine\u2013American War (1899\u20131902) was an armed conflict between the United States and Filipino revolutionaries. Estimates of the Filipino forces vary between 100,000 and 1,000,000, with tens of thousands of auxiliaries. Lack of weapons and ammunition was a significant impediment to the Filipinos, so most of the forces were only armed with bolo knives, bows and arrows, spears and other primitive weapons that, in practice, proved vastly inferior to U.S. firepower.\nThe goal, or end-state, sought by the First Philippine Republic was a sovereign, independent, socially stable Philippines led by the \"ilustrado\" (intellectual) oligarchy. Local chieftains, landowners, and businessmen were the \"principales\" who controlled local politics. The war was strongest when \"illustrados\", \"principales\", and peasants were unified in opposition to annexation. The peasants, who provided the bulk of guerrilla forces, had interests different from their \"illustrado\" leaders and the \"principales\" of their villages. Coupled with the ethnic and geographic fragmentation, unity was a daunting task. The challenge for Aguinaldo and his generals was to sustain unified Filipino public opposition; this was the revolutionaries' strategic centre of gravity. The Filipino operational center of gravity was the ability to sustain its force of 100,000 irregulars in the field. The Filipino General Francisco Macabulos described the Filipinos' war aim as \"not to vanquish the U.S. Army but to inflict on them constant losses.\" They initially sought to use conventional tactics and an increasing toll of U.S. casualties to contribute to McKinley's defeat in the 1900 presidential election. Their hope was that as president the avowedly anti-imperialist future Secretary of state William Jennings Bryan would withdraw from the Philippines. They pursued this short-term goal with guerrilla tactics better suited to a protracted struggle. While targeting McKinley motivated the revolutionaries in the short term, his victory demoralized them and convinced many undecided Filipinos that the United States would not depart precipitously. For most of 1899, the revolutionary leadership had viewed guerrilla warfare strategically only as a tactical option of final recourse, not as a means of operation which better suited their disadvantaged situation. On 13 November 1899, Emilio Aguinaldo decreed that guerrilla war would henceforth be the strategy. This made the American occupation of the Philippine archipelago more difficult over the next few years. In fact, during just the first four months of the guerrilla war, the Americans had nearly 500 casualties. The Philippine Revolutionary Army began staging bloody ambushes and raids, such as the guerrilla victories at Paye, Catubig, Makahambus, Pulang Lupa, Balangiga and Mabitac. At first, it seemed like the Filipinos would fight the Americans to a stalemate and force them to withdraw. President McKinley even considered this at the beginning of the phase. The shift to guerrilla warfare drove the U.S. Army to adopt counterinsurgency tactics.\n20th century.\nSecond Boer War.\nAsymmetric warfare featured prominently during the Second Boer War. After an initial phase, which was fought by both sides as a conventional war, the British captured Johannesburg, the Boers' largest city, and captured the capitals of the two Boer Republics. The British then expected the Boers to accept peace as dictated in the traditional European manner. However, the Boers fought a protracted guerrilla war instead of capitulating. 20,000-30,000 Boer guerrillas were only defeated after the British brought to bear 450,000 imperial troops, about ten times as many as were used in the conventional phase of the war. The British began constructing blockhouses built within machine gun range of one another and flanked by barbed wire to slow the Boers' movement across the countryside and block paths to valuable targets. Such tactics eventually evolved into today's counterinsurgency tactics.\nThe Boer commando raids deep into the Cape Colony, which were organized and commanded by Jan Smuts, resonated throughout the century as the British adopted and adapted the tactics first used against them by the Boers.\nAfter World War II.\nCold War (1945\u20131992).\nThe end of World War II established the two strongest victors, the United States of America (the United States, or just the U.S.) and the Union of Soviet Socialist Republics (USSR, or just the Soviet Union) as the two dominant global superpowers.\nCold War examples of proxy wars.\nIn Southeast Asia, specifically Vietnam, the Viet Minh, NLF and other insurgencies engaged in asymmetrical guerrilla warfare with France. The war between the Mujahideen and the Soviet Armed Forces during the Soviet\u2013Afghan War of 1979 to 1989, though claimed as a source of the term \"asymmetric warfare,\" occurred years after Mack wrote of \"asymmetric conflict.\" (Note that the term \"asymmetric warfare\" became well-known in the West only in the 1990s.) The aid given by the U.S. to the Mujahideen during the war was only covert at the tactical level; the Reagan Administration told the world that it was helping the \"freedom-loving people of Afghanistan.\" Many countries, including the U.S., participated in this proxy war against the USSR during the Cold War.\nPost-Cold War.\nThe Kosovo War, which pitted Yugoslav security forces (Serbian police and Yugoslav army) against Albanian separatists of the guerrilla Kosovo Liberation Army, is an example of asymmetric warfare, due to Yugoslav forces' superior firepower and manpower, and due to the nature of insurgency/counter-insurgency operations. The NATO bombing of Yugoslavia (1999), which pitted NATO air power against the Yugoslav armed forces during the Kosovo war, can also be classified as asymmetric, exemplifying international conflict with asymmetry in weapons and strategy/tactics.\n21st century.\nIsrael/Palestine.\nThe ongoing conflict between Israel and some Palestinian organizations (such as Hamas and PIJ) is a classic case of asymmetric warfare. Israel has a powerful army, air force and navy, while the Palestinian organizations have no access to large-scale military equipment with which to conduct operations; instead, they utilize asymmetric tactics, such as taking hostages, paragliding, small gunfights, cross-border sniping, indiscriminate mortar/rocket attacks, and others.\nSri Lanka.\nThe Sri Lankan Civil War, which raged on and off from 1983 to 2009, between the Sri Lankan government and the Liberation Tigers of Tamil Eelam (LTTE) saw large-scale asymmetric warfare. The war started as an insurgency and progressed to a large-scale conflict with the mixture of guerrilla and conventional warfare, seeing the LTTE use suicide bombing (male/female suicide bombers) both on and off the battlefield use of explosive-filled boats for suicide attacks on military shipping; and use of light aircraft targeting military and economic infrastructure.\nIraq.\nThe victory by the US-led coalition forces in the 1991 Persian Gulf War and the 2003 invasion of Iraq demonstrated that training, tactics and technology could provide overwhelming victories in the field of battle during modern conventional warfare. After Saddam Hussein's regime was removed from power, the Iraq campaign moved into a different type of asymmetric warfare where the coalition's use of superior conventional warfare training, tactics and technology was of much less use against continued opposition from the various partisan groups operating inside Iraq.\nSyria.\nMuch of the 2012\u2013present Syrian Civil War has been asymmetrical. The Syrian National Coalition, Mujahideen, and Kurdish Democratic Union Party have been engaging with the forces of the Syrian government through asymmetric means. The conflict has seen large-scale asymmetric warfare across the country, with the forces opposed to the government unable to engage symmetrically with the Syrian government and resorting instead to other asymmetric tactics such as suicide bombings and targeted assassinations.\nUkraine.\nThe 2022 Russian invasion of Ukraine has resulted in what could be described in some respects as an asymmetrical warfare scenario. Russia has a much larger economy, population, and has superior military might to Ukraine. The use of MAGURA V5 unmanned surface vehicles (USVs) to attack Russian Black Sea Fleet ships such as the \"Tsezar Kunikov\" has been cited as example of asymmetrical warfare by analysts.\nSemi-symmetric warfare.\nA new understanding of warfare has emerged amidst the 2022 Russian invasion of Ukraine. Although this type of warfare does not oppose an insurgency to a counter-insurgency force, it does involve two actors with substantially asymmetrical means of waging war. Notably, as technology has improved war-fighting capabilities, it has also made them more complex, thus requiring greater expertise, training, flexibility and decentralization. The nominally weaker military can exploit those complexities and seek to eliminate the asymmetry. This has been observed in Ukraine, as defending forces used a rich arsenal of anti-tank and anti-air missiles to negate the invading forces' apparent mechanized and aerial superiority, thus denying their ability to conduct combined arms operations. The success of this strategy will be compounded by access to real-time intelligence and the adversary's inability to utilize its forces to the maximum of their potential due to factors such as the inability to plan, brief and execute complex, full-spectrum operations.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;*Civilian casualty ratio\nU.S. organisations:\nWars\nDocuments:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46864", "revid": "136874", "url": "https://en.wikipedia.org/wiki?curid=46864", "title": "N-sequence", "text": ""}
{"id": "46865", "revid": "49150429", "url": "https://en.wikipedia.org/wiki?curid=46865", "title": "Manzanar", "text": "World War II internment camp in California, US\nManzanar is the site of one of ten American concentration camps, where more than 120,000 Japanese Americans were incarcerated during World War II, from March 1942 to November 1945. Although it had over 10,000 inmates at its peak, Manzanar was one of the smaller internment camps. It is located in California's Owens Valley, on the eastern side of the Sierra Nevada mountains, between the towns of Lone Pine to the south and Independence to the north, approximately north of Los Angeles. Manzanar means \"apple orchard\" in Spanish. The Manzanar National Historic Site, which preserves and interprets the legacy of Japanese American incarceration in the United States, was identified by the United States National Park Service as the best-preserved of the ten former camp sites.\nThe first Japanese Americans arrived at Manzanar in March 1942, just one month after President Franklin D. Roosevelt signed Executive Order 9066, to build the camp their families would be staying in. Manzanar was in operation as an internment camp from 1942 until 1945. Since the last of those incarcerated left in 1945, former detainees and others have worked to protect Manzanar and to establish it as a National Historic Site to ensure that the history of the site, along with the stories of those who were incarcerated there, is recorded for current and future generations. The primary focus is the Japanese American incarceration era, as specified in the legislation that created the Manzanar National Historic Site. The site also interprets the former town of Manzanar, the ranch days, the settlement by the Owens Valley Paiute, and the role that water played in shaping the history of the Owens Valley.\nBackground.\nManzanar was first inhabited by Indigenous Americans nearly 10,000 years ago. Approximately 1,500 years ago, the area was settled by the Owens Valley Paiute, who ranged across the Owens Valley from Long Valley on the north to Owens Lake on the south, and from the crest of the Sierra Nevada on the west to the Inyo Mountains on the east. When European American settlers first arrived in the Owens Valley in the mid-19th century, they found a number of large Paiute villages in the Manzanar area. John Shepherd, one of the first of the new settlers, homesteaded of land north of Georges Creek in 1864. With the help of Owens Valley Paiute field workers and laborers, he expanded his ranch to .\nIn 1905, George Chaffey, an agricultural developer from Southern California, purchased Shepherd's ranch and subdivided it, along with other adjacent ranches. He founded the town of Manzanar in 1910, along the main line of the Southern Pacific. By August 1911, the town's population was approaching 200. The company built an irrigation system over an area of and planted about 20,000 fruit trees. By 1920, the town had more than 25\u00a0homes, a two-room school, a town hall, and a general store. Also at that time, nearly of apple, pear, and peach trees were under cultivation; along with crops of grapes, prunes, potatoes, corn and alfalfa; and large vegetable and flower gardens.\nAs early as March 1905, the City of Los Angeles began acquiring water rights in the Owens Valley. In 1913, it completed construction of its Los Angeles Aqueduct, In dry years, Los Angeles pumped ground water and drained all surface water, diverting all of it into its aqueduct and leaving Owens Valley ranchers without water. Without water for irrigation, the holdout ranchers were forced off their ranches and out of their communities; that included the town of Manzanar, which was abandoned by 1929. Manzanar remained uninhabited until the United States Army leased from the City of Los Angeles for the Manzanar War Relocation Center.\nEstablishment.\nAfter the December 7, 1941, attack on Pearl Harbor, the United States Government swiftly moved to begin solving the \"Japanese Problem\" on the West Coast of the United States. In the evening hours of that same day, the Federal Bureau of Investigation (FBI) arrested selected \"enemy aliens\", including more than 5,500 Issei men. Many citizens in California were alarmed about potential activities by people of Japanese descent even if the families have been in America for generations.\nOn February 19, 1942, President Franklin D. Roosevelt signed Executive Order 9066, which authorized the Secretary of War to designate military commanders to prescribe military areas and to exclude \"any or all persons\" from such areas. The order also authorized the construction of what were later called \"relocation centers\" by the War Relocation Authority (WRA), to house those who were to be excluded. This order resulted in the forced relocation of more than 120,000 Japanese Americans, two-thirds of whom were native-born American citizens; the rest had been prevented from becoming citizens by federal law. Over 110,000 were incarcerated in ten concentration camps located far inland and away from the coast.\nManzanar was the first of the ten concentration camps to be established, and began accepting detainees in March 1942. Initially, it was a temporary \"reception center\", known as the \"Owens Valley Reception Center\" from March 21, 1942, to May 31, 1942. At that time, it was operated by the US Army's Wartime Civilian Control Administration (WCCA). \nThe first director of the camp was Calvin E. Triggs, a longtime veteran of the Works Progress Administration (WPA), a signature program of the Second New Deal. Many of his fellow employees had worked in that agency. Manzanar, according to one insider, was \"manned just about 100% by the WPA.\" Drawing on experiences derived from New Deal era road building, Triggs, funded primarily through the WPA, supervised the installation of such features as guard towers and spotlights.\nThe Owens Valley Reception Center was transferred to the WRA on June 1, 1942, and officially became the \"Manzanar War Relocation Center\". The first Japanese Americans to arrive at Manzanar were volunteers who helped build the camp. By mid\u2013April, up to 1,000 Japanese Americans were arriving daily, and by July, the population of the camp neared 10,000. About 90 percent of the incarcerated were from the Los Angeles area, with the rest coming from Stockton, California; and Bainbridge Island, Washington. Many were farmers and fishermen. Manzanar held 10,046 adults and children at its peak, and a total of 11,070 were incarcerated there.\nCamp conditions and facilities.\nClimate and location.\nThe Manzanar facility was located between Lone Pine and Independence. The weather at Manzanar caused suffering for the inmates, few of whom were accustomed to the extremes of the area's climate. While the majority of people were from the Los Angeles area, some were from places with much different climates (such as Bainbridge Island in Washington). The temporary buildings were inadequate to shield people from the weather. The Owens Valley lies at an elevation of about .\nSummers on the desert floor of the Owens Valley are generally hot, with temperatures often exceeding . Winters bring occasional snowfall and daytime temperatures that often drop into the range. At night, temperatures are generally lower than the daytime highs, and high winds are common day or night.\nThe area's mean annual precipitation is barely . The ever-present dust was a continual problem due to the frequent high winds; so much so that people usually woke up in the morning covered from head to toe with a fine layer of dust, and they constantly had to sweep dirt out of the barracks.\n\"In the summer, the heat was unbearable,\" said former Manzanar inmate Ralph Lazo. \"In the winter, the sparsely rationed oil didn't adequately heat the tar paper-covered pine barracks with knotholes in the floor. The wind would blow so hard, it would toss rocks around.\"\nCamp layout and facilities.\nThe camp site was situated on at Manzanar, leased from the City of Los Angeles, with the developed portion covering approximately . Eight guard towers equipped with machine guns were located at intervals around the perimeter fence, which was topped by barbed wire. The grid layout used in the camp was standard, and a similar layout was used in all of the relocation centers.\nThe residential area was about one square mile (2.6\u00a0km2), and consisted of 36 blocks of hastily constructed, tarpaper barracks, with each family (up to eight people) living in a single \"apartment\" in the barracks.\nJeanne Wakatsuki Houston, a Manzanar survivor, described the living conditions in her book: \"After dinner we were taken to Block 16, a cluster of fifteen barracks that had just been finished a day or so earlier\u2014although finished was hardly a word for it. The shacks were built of pine planking covered with tarpaper. They sat on concrete footings, with about two feet of open space between the floorboards and the ground. Gaps showed between the planks, and as the weeks passed and the green wood dried out, the gaps widened. Knotholes gaped in the uncovered floor.\"18 In the book, she goes on to explain the size and layout of the barracks. They were divided into six units that were sixteen long by twenty feet wide, and a single light bulb hung from the ceiling. They had an oil stove for heat as well as two army blankets each, some mattress covers and steel army cots.\nThese apartments consisted of partitions with no ceilings, eliminating any chance of privacy. Lack of privacy was a major problem, especially since the camp had communal men's and women's latrines. Former Manzanar inmate Rosie Kakuuchi said that the communal facilities were \"[o]ne of the hardest things to endure\", adding that neither the latrines nor showers had partitions or stalls.\nEach residential block also had a communal mess hall (large enough to serve 300 people at one time), a laundry room, a recreation hall, an ironing room, and a heating oil storage tank, although Block\u00a033 lacked a recreation hall. In addition to the residential blocks, Manzanar had 34 blocks that had staff housing, camp administration offices, two warehouses, a garage, a camp hospital, and 24 firebreaks.\nThe camp had school facilities, a high-school auditorium (that was also used as a theatre), staff housing, chicken and hog farms, churches, a cemetery, a post office, a hospital, an orphanage, two community latrines, an outdoor theater, and other necessary amenities that one would expect to find in most American cities. Some of the facilities were not built until after the camp had been operating for a while. The camp perimeter had eight watchtowers manned by armed military police, and it was enclosed by five-strand barbed wire. There were sentry posts at the main entrance. Many of the camp administration staff lived inside the fence at the camp, though the military police lived outside the fence.\nCommercial facilities.\nTypical businesses such as a cooperative store and other shops and a camp newspaper were operated by the internees. A camouflage net factory, to provide the nets to various military units, was operated on the site. An experimental plantation for producing natural rubber from the Guayule plant was built and operated.\nBefore a hospital was built, doctors in the camp faced many difficulties, including treating internees for diseases such as measles, chickenpox, whooping cough, and diarrhea. Treatment facilities were often the barracks, which did not include running water or heating. Once the Manzanar Hospital was built, it included a kitchen, operating rooms, treatment wards, laboratories, and other facilities. All medical treatment in Manzanar was provided at no charge.\nManzanar Children's Village, an orphanage housing 101 Japanese-American orphans from June 1942 to September 1945, operated within the camp. Children incarcerated there were from multiple orphanages in the Los Angeles area as well as locations in Washington, Oregon, and Alaska. Infants born to unmarried mothers in other WRA camps were also sent to Children's Village over the next three years.\nThe 61 remaining children in Maryknoll, Shonien and the Salvation Army Home were slated for removal. On June 23, 1942, they were bused, under armed guard, with several adult caretakers, from Los Angeles to Manzanar. Over the next few months, approximately thirty more children from Washington, Oregon and Alaska, mostly orphans who had been living with non-Japanese foster families, would arrive in Manzanar.\nLife in camp.\nAfter being uprooted from their homes and communities, the incarcerated people had to endure primitive, sub-standard conditions and lack of privacy. They had to wait in line for meals, at latrines, and at the laundry room. Each camp was intended to be self-sufficient, and Manzanar was no exception. Cooperatives operated various services, such as the camp newspaper, beauty salons and barber shops, shoe repair, libraries, and more. In addition, there were some who raised chickens, hogs, and vegetables, and cultivated the existing orchards for fruit. During the time Manzanar was in operation, 188 weddings were held, 541 children were born in the camp, and between 135 and 146 individuals died.\nLife in the camp became more difficult as sickness spread throughout it. The housing sector of the camp was just 500 acres and held more than ten thousand prisoners at its peak. The compactness of the camp led many people to fall ill even though they were given vaccines upon arrival to the camp. The water at Manzanar was unclean, and caused many inmates to suffer from dysentery.\nSome of those interned at the camp supported the policies implemented by the War Relocation Authority, causing them to be targeted by others in the camp. On December 6, 1942, a riot broke out and two internees were killed. Togo Tanaka was one of those targeted, but he escaped by disguising himself and mingling into the crowd that was searching for him. Others were outraged that their patriotism was being questioned simply because of their ethnic heritage. Despite the hardships endured, the internees gradually \"turned [the] concentration camp into a community\" by \"[spending] their days creating beautiful things\".\nFood.\nThe barracks at Manzanar had no cooking areas, and all meals were served at block mess halls. The mess hall lines were long and stretched outside regardless of weather. The cafeteria-style eating was named by the 1980s Congressional Committee on the Wartime Relocation and Internment of Civilians (CWRIC) as a cause of the deterioration of the family due to children wanting to eat with their friends instead of their families, and families not always being able to eat together. There was a strict meal schedule, with one young detainee noting \"We eat from 7:00 AM to 8:00 AM o'clock in the morning 12:00 PM-1:00 PM in afternoon and 5:00 PM-6:00 in night and on Sunday we eat 8:00 AM-9:00.\" Food at Manzanar was based on military requirements. Meals usually consisted of hot rice and vegetables, since meat was scarce due to rationing.\nIn 1944, a chicken ranch and a hog farm began operation, providing the camp with meat. As many of the internees were farmers, they used their knowledge of fertilizers, irrigation, land reclamation, and cultivation to successfully grow productive gardens. They made their own soy sauce and tofu. Many families had small gardens outside their barracks.\nThe food varied in quality, but was mostly substandard compared to the food the internees ate prior to incarceration. Togo Tanaka described how people \"got sick from eating ill-prepared food.\" Aiko Herzig-Yoshinaga described trying to take care of her newborn daughter, saying that the child was so sick that, while \"[m]ost infants double their weight, birth weight, at six months\", her daughter \"had not doubled her weight in a year\".\nThe food in Manzanar was heavily starchy and low quality, including Vienna sausages, canned string beans, hot dogs, and apple sauce. Outside of the sausages and hot dogs, meat was rare, usually consisting of chicken or mutton that was heavily breaded and fried. Frank Kikuchi, an internee at Manzanar, stated that some of the newspapers lied to the American public by telling them that the \"Japs [in the camps] are getting steaks, chops, eggs, or eating high off the hog.\" Camp, school, and individual gardens eventually helped supplement the menu in the mess halls. Internees also snuck out of the camp to go fishing, often bringing back their catches to the camp.\nHarry Ueno accused camp administrators and leaders in the Japanese American Citizens League (JACL) of stealing food meant for the internees and then selling it on the black market. During the December 1942 camp riot, Ueno was arrested for allegedly beating another internee who was a member of the JACL.\nEmployment.\nMost of the adults were employed at Manzanar to keep the camp running. In order for the camps to be self-sufficient, the adults were employed in a variety of jobs to supply the camp and the military. Jobs included clothing and furniture manufacturing, farming and tending orchards, military manufacturing such as camouflage netting and experimental rubber, teaching, civil service jobs such as police, fire fighters, and nursing, and general service jobs operating stores, beauty parlors, and a bank.\nA farm and orchards provided vegetables and fruits for use by the camp, and people of all ages worked to maintain them. By the summer of 1943, camp gardens and farms were producing potatoes, onions, cucumbers, Chinese cabbage, watermelon, eggplant, tomatoes, aster, red radishes, and peppers. Eventually, there were more than 400 acres of farms producing more than 80 percent of the produce used by the camp. In early 1944, a chicken ranch began operation, and in late April of the same year, the camp opened a hog farm. Both operations provided welcome meat supplements to the diet.\nShortly after being interned, Togo Tanaka and Joe Masaoka were hired by anthropologist Robert Redfield as documentary historians for the camp. In addition to his work at the \"Manzanar Free Press\", he filed hundreds of reports to the WRA that often criticized those in charge at the camp and the living conditions in the camp.\nUnskilled workers earned US$8 per month ($ per month as of 2024), semi-skilled workers earned $12 per month ($ per month as of 2024), skilled workers made $16 per month ($ per month as of 2024), and professionals earned $19 per month ($ per month as of 2024). In addition, everybody received $3.60 per month ($ per month as of 2024) as a clothing allowance.\n\"Manzanar Free Press\".\nThe \"Manzanar Free Press\" was first published April 11, 1942, and was published through the October 19, 1945, issue. It was published with both Japanese and English sections, with the Japanese section added on July 14, 1942. Between the first issue and the May 31, 1942, issue, it was published at the Manzanar Assembly Center, which was operated by the Wartime Civil Control Administration. After that, it was published at the Manzanar Relocation Center until it ceased publication.\nThe paper was originally published as four pages biweekly which were hand-typed and mimeographed. The circulation increased as the number of people in the camp grew, the release increased to three issues weekly, and a printing press was acquired, allowing the paper to be typeset beginning on July 22, 1942. The page count also increased to six.\nJournalists who reported for the newspaper include Togo Tanaka, who was the English section editor of the \"Rafu Shimpo\" before being incarcerated. Tanaka also delivered the \"Free Press\" before becoming a journalist for them. While working as a reporter for the \"Free Press\", Tanaka wrote hundreds of articles documenting the everyday life in the camp. Beginning on July 22, 1942, Chiye Mori, poet and journalist, was listed as an editor.\nDespite the name of the newspaper, the War Relocation Authority (WRA) controlled the content of the paper and used it to publish announcements from the camp administration, news from other camps, orders, rules and guidelines from the WRA, and upcoming camp events, in addition to the regular content. Some content was not allowed to be published. The standard content included articles about life in the camps, sports scores and coverage, coverage of the war, and so on.\nRecreation.\nPeople made life at Manzanar more tolerable through recreation. They participated in sports, including baseball, football, basketball, soccer, volleyball, softball, and martial arts. A nine-hole golf course was built at the camp. Lou Frizzell served as the musical director, and under his mentorship Mary Nomura became known as the \"songbird of Manzanar\" for her performances at dances and other camp events. Theatre performances\u2014for internees, camp administration and WRA staff, and even for some members of the surrounding communities\u2014included original productions by internees as well as traditional Japanese works of kabuki and noh.\nInternees, many of whom were relocated from their landscaping businesses in the Los Angeles area, personalized and beautified their barren surroundings by building elaborate gardens and parks, which often included pools, waterfalls, and rock ornaments. Competitions were often held between landscapers as they created gardens in the public spaces of the camp (such as between barracks). The camp administration even allowed some gardens to be created outside the camp. These helped create a sense of community and gave the internees a place to heal. Remnants of some of the gardens, pools, and rock ornaments are still present at Manzanar, and there are plans to restore at least some of them.\nOne of the most popular pastimes for those incarcerated at Manzanar was baseball. The men there formed almost 100 baseball teams, and the women formed 14. Regular seasons were established, teams were divided into leagues, and championship games were held. The teams included both professional and amateur players. Some of the players viewed playing baseball as a way to prove their loyalty to America, treating it like wearing an American flag. Photographer Ansel Adams took his photo (right) as part of his effort to show how those incarcerated at Manzanar \"overcome [their] sense of defeat and despair.\"\nMany Japanese cultural celebrations were continued, though the official photos allowed out by the WRA rarely showed them. The New Year tradition of \"mochitsuki\"\u2014pounding glutinous rice into mochi\u2014was regularly covered by the camp newspaper. Craftsman in the camp carved \"geta\" for many of the residents, though the official photography only pointed out that they were useful for keeping above the dusty ground.\nManzanar Riot.\nAlthough most quietly accepted their fate during World War II, there was some resistance in the camps. Poston, Heart Mountain, Topaz, and Tule Lake each had civil disturbances about wage differences, black marketing of sugar, food shortages, intergenerational friction, rumors of \"informers\" reporting to the camp administration or the FBI, and other issues. The most serious incident occurred at Manzanar on December 5\u20136, 1942 (with some of the actions on both sides carrying over into the following days), and became known as the \"Manzanar Revolt\" or \"Manzanar Riot\".\nSome of the tension that precipitated the riot was related to work availability and the pay of those jobs, with \"Nisei\" and members of the Japanese American Citizens League (JACL) getting preferential treatment. After several months of tension between those who supported the JACL and a group of \"Kibei\" (Japanese Americans educated in Japan), rumors spread that sugar and meat shortages were the result of black marketing by camp administrators. To make matters worse, JACL leader Fred Tayama was beaten by six masked men on the evening of December 5. Harry Ueno, the leader of the Kitchen Workers Union, and two others suspected of involvement, were arrested. The other two suspects were questioned and released, but Ueno was removed from Manzanar.\nAbout 200 internees met on the morning of December 6 in the gardens at the Block\u00a022 mess hall to discuss what they should do, and another meeting was scheduled for a few hours later. Between two and four thousand people gathered at the meeting where they listened to speeches and chose five people to present their grievances to the camp director. The crowd decided to follow the five representatives, which caused the camp director to tell the military police to muster in order to be available to control the crowd. The five representatives demanded that Ueno be released, but the camp director did not immediately agree.\nAfter the crowd began getting more unruly, the director finally agreed to release Ueno if the crowd agreed he should still stand trial, no one attempted to break him out of the camp jail, the five representatives would discuss any further wants with the director, the protesting crowds would disperse and not reassemble, and the five would work to dispel and quiet the protesters. Ueno was then returned to the camp jail in the early evening.\nWhen the five representatives went to verify that Ueno was in the jail, the crowd again returned to protest. Instead of dispersing as asked, they broke into groups to try to find Tayama and kill him. When they were unable to find him in the hospital, they began searching all through the camp for Tayama as well as Tokie Slocum and Togo Tanaka, two other suspected collaborators. When they were unable to find any of them, the searchers began returning toward the jail.\nWhile the smaller search parties were searching the camp, the camp director had been trying to negotiate with the five representatives. This appeared to work initially, but the crowd gradually became more angry and started throwing bottles and rocks at the soldiers. The military police responded with tear gas to disperse them. As people ran to avoid the tear gas, some in the crowd pushed a driverless truck toward the jail. At that moment, the military police fired into the crowd, killing a 17-year-old boy instantly. A 21-year-old man who was shot in the abdomen died a few days later. At least nine to ten other prisoners were wounded, and a military police corporal was wounded by a ricocheting bullet.\nThat night, some inmates continued attacking suspected collaborators and meeting in small groups while avoiding military police patrols. Over the next several days, internees marked as suspected collaborators were quietly removed from the camp with their families in order to protect them from being beaten or killed by the protesters.\n100th Infantry Battalion and the 442nd Regimental Combat Team.\nThe vanguard of Japanese American (JA) combat units was the legendary 100th Infantry Battalion (Separate) made up of soldiers in the Hawaii National Guard that was formed in June 1942. The training record of the 100th Battalion at Camp McCoy WI from June to December 1942 convinced the War Department to authorize the formation of the 442nd Regimental Combat Team (RCT) on February 1, 1943. On August 21, 1943, the 100th Battalion was deployed to Oran in North Africa. This unit became the War Department's test on whether JA soldiers could be trusted in combat when it landed in Italy in September 1943 as part of the 34th Infantry Division. The unparalleled bravery of the 100th Battalion in the first weeks of combat forever answered this question of trust, paving the way for the 442nd RCT to join them in June 1944.\nBecause of the 100th Battalion's sterling training record and the Varsity Victory Volunteers, a group of University of Hawaii ROTC students who received positive publicity for their volunteer civilian labor for the U.S. Army, along with many organizations and leaders in Hawaii and on the mainland lobbying the government to allow Japanese Americans to serve in the armed forces, President Roosevelt authorized the formation of the 442nd Regimental Combat Team (RCT) on February 1, 1943. When the announcement about the new unit was made, 10,000 young men in Hawaii signed up from which 2,686 were selected, and along with 1,182 from the mainland, they were sent to Camp Shelby in Mississippi for basic training in April 1943. Along with the cadre of those already in the Army, roughly 2/3 of the 442nd RCT were from Hawaii and 1/3 from the mainland.\nOf the nearly 160,000 people of Japanese descent living in Hawaii in 1940, fewer than 2,000 were incarcerated compared to the mass incarceration of those on the West Coast; thus, less than 2% of the soldiers from the islands had families in the camps.\nClosure.\nThe WRA closed Manzanar when the final internee left at 11:00\u00a0a.m. on November 21, 1945. It was the sixth camp to be closed. Although the Japanese Americans had been brought to the Owens Valley by the United States Government, they had to leave the camp and travel to their next destinations on their own. The WRA gave each person $25 ($ today), one-way train or bus fare, and meals to those who had less than $600 ($ today).\nWhile many left the camp voluntarily, a significant number refused to leave because they had no place to go after having lost everything when they were forcibly uprooted and removed from their homes. As such, they had to be forcibly removed once again, this time from Manzanar. Indeed, those who refused to leave were generally removed from their barracks, sometimes by force, even if they had no place to go.\nBetween 135 and 146 Japanese Americans died at Manzanar. Fifteen were buried there, but only five graves remain, as most were reburied elsewhere by their families. The Manzanar cemetery site is marked by a monument that was built by stonemason Ryozo Kado in 1943. An inscription in Japanese on the front (east side) of the monument reads ('Soul Consoling Tower': ireit\u014d=Mandarin: w\u00e8i-l\u00edng-t\u01ce 'consoling-soul monument' ). The inscription on the back (west side) reads \"Erected by the Manzanar Japanese\" on the left-hand column, and \"August 1943\" on the right-hand column.\nAfter the camp was closed, the site eventually returned to its original state. Within a couple of years, all the structures had been removed, with the exception of the two sentry posts at the entrance, the cemetery monument, and the former Manzanar High School auditorium, which was purchased by the County of Inyo. The County leased the auditorium to the Independence Veterans of Foreign Wars, who used it as a meeting facility and community theater until 1951. After that, the building was used as a maintenance facility by the Inyo County Road Department.\nThe site also retained numerous building foundations, portions of the water and sewer systems, the outline of the road grid, some landscaping, and much more. Despite four years of use, the site also retains evidence of the ranches and of the town of Manzanar, as well as artifacts from the days of the Owens Valley Paiute settlement.\nPreservation and remembrance.\nDuring the war, the War Relocation Authority hired photographers Ansel Adams and Dorothea Lange to document through pictures the Japanese-Americans impacted by the forced relocation, including Manzanar. Togo Tanaka and Joe Masaoka were hired by anthropologist Robert Redfield as documentary historians for the camp on behalf of the WRA.\nManzanar Pilgrimage.\nOn December 21, 1969, about 150 people departed Los Angeles by car and bus, headed for Manzanar. It was the first official annual Manzanar Pilgrimage, though two ministers\u2014the Reverend Sentoku Mayeda and the Reverend Shoichi Wakahiro\u2014had been making annual pilgrimages to Manzanar since the camp closed in 1945.\nThe non-profit Manzanar Committee, formerly led by Sue Kunitomi Embrey, has sponsored the Pilgrimage since 1969. The event is held annually on the last Saturday of April with hundreds of visitors of all ages and backgrounds, including former inmates, gathering at the Manzanar cemetery to remember the incarceration. The hope is that participants can learn about it and help ensure that what is generally accepted to be a tragic chapter in American history is neither forgotten nor repeated. The program traditionally consists of speakers, cultural performances, an interfaith service to memorialize those who died at Manzanar, and \"Ondo\" dancing.\nIn 1997, the Manzanar At Dusk program became a part of the Pilgrimage. The program attracts local area residents, as well as descendants of Manzanar's ranch days and the town of Manzanar. Through small-group discussions, the event gives participants the opportunity to hear directly from those who had been there and to talk about the relevance of what had happened at Manzanar to their own lives.\nSince the September 11 attacks, American Muslims have participated in the Pilgrimage to promote and increase awareness of civil rights protections in the wake of widespread suspicions harbored against them post-9/11. A group of 150 Muslims visited in 2017, in part to compare treatment of Japanese-Americans during World War II with how Muslims are treated following the 9/11 attacks. Over 2,000 people visited the site on April 27, 2019, for the 50th anniversary of the first pilgrimage, including a number of Muslim speakers, and a group of Muslims held afternoon prayers at the monument.\nDesignations.\nThe Manzanar Committee's efforts resulted in the State of California naming Manzanar as California Historical Landmark #850 in 1972, with an historical marker being placed at the sentry post on April 14, 1973. Manzanar, which had been historically owned by the City of Los Angeles, was registered as a Los Angeles Historic-Cultural Monument in 1976.\nThe Manzanar Committee also spearheaded efforts for Manzanar to be listed in the National Register of Historic Places, and in February 1985, Manzanar was designated a National Historic Landmark. Embrey and the committee, along with California representative Mel Levine, led the effort to have Manzanar designated a National Historic Site, and on March 3, 1992, President George H. W. Bush signed House Resolution 543 into law. This act of Congress established the Manzanar National Historic Site \"to provide for the protection and interpretation of the historical, cultural, and natural resources associated with the relocation of Japanese Americans during World War II.\" Five years later, the National Park Service acquired of land at Manzanar from the City of Los Angeles. It was the first of the camps to be designated as a National Historical Site.\nAfter Congress named Manzanar a National Historic Site and gave the National Park Service the job of restoring the site in 1992, protests against its creation emerged. Letters were sent to the National Park Service included statements that Manzanar should be portrayed as a guest housing center, with others stating that calling the site a concentration camp is \"treason\", threatening dismissal campaigns against National Park Service employees and other related individuals, threatening to destroy buildings, and objecting to the use of the phrase \"concentration camp\" on signage at the site. The California State historical marker was hacked and stained, with the first \"C\" of \"concentration camp\" ground off. A man describing himself as a World War II veteran stated that he had driven 200 miles to urinate on the marker.\nMonument facilities and setting.\nThe site features a visitor center with a gift shop, housed in the historically restored Manzanar High School Auditorium with a reconstructed stage proscenium. The auditorium and the two sentry posts at the entrance are the only original structures from the time the camp was operating during World War II. Permanent exhibits tell the stories of the internee transportation to Manzanar, the Owens Valley Paiute, the ranchers, the town of Manzanar, the role that water played in shaping the history of the Owens Valley, and one that plays a video of Ronald Reagan signing the Civil Liberties Act.\nAn \"interpretive center\" helps visitors gain an understanding of some of the internees' experiences. The exhibits in the center are constructed with materials that would have been used\u2014or are similar to those used\u2014when the camp was in operation. Details of camp experiences are from all ten of the relocation centers. A driving tour with 27 points of interest takes visitors around the site.\nA mess hall, salvaged from a closing military facility, was added to the site in 2002. The replica guard tower was built in 2005. The Manzanar cemetery, where some of the internees who died at the camp were buried, also contains the memorial obelisk, which was built by masons in the camp in August 1943. All of the remains have been removed to other locations.\nThe site features restored sentry posts at the camp entrance, a replica of a camp guard tower built in 2005, a self-guided tour road, and wayside exhibits. Staff offer guided tours and other educational programs, including a Junior Ranger educational program for children between four and fifteen years of age.\nReconstruction.\nUnder most circumstances, the National Park Service discourages the reconstruction of structures and artifacts that are no longer extant, but allows for exceptions when \"there is no alternative that would accomplish the park's interpretive mission, there is sufficient data to enable an accurate reconstruction,\" and \"the reconstruction occurs on the original location.\" On the basis that these criteria were met, and after extensive discussion with the Japanese-American community, the NPS decided to proceed with a reconstruction of some elements of the original site alongside preservation of those remnants that survive.\nThe National Park Service is reconstructing one of the 36 residential blocks as a demonstration block (Block\u00a014, adjacent to and west of the Visitor Center). One barrack appears as it would have when Japanese Americans first arrived at Manzanar in 1942, while another has been reconstructed to represent barracks life in 1945. Exhibits in these barracks opened on April 16, 2015. A restored World War II mess hall, moved to the site from Bishop Airport in 2002, was opened to visitors in late 2010. The Manzanar National Historic Site also unveiled its virtual museum on May 17, 2010.\nNational Park Service staff have continued to uncover artifacts from throughout Manzanar's history, the result of archaeological digs that have also excavated several of the gardens designed and built there, including the noted Merritt Park (also known as Pleasure Park). In progress is a classroom exhibit that will be housed in the Block\u00a09 barracks and an historic replica of the Block\u00a09 women's latrine (opened in October 2016, but with no interpretive exhibit materials at this time).\nReception of and discussion regarding Manzanar.\nThe Manzanar site had 1,275,195 people visit from 2000 through December 2016. The National Park Service's interpretation of events and experiences has been described as both \"[willing] to memorialize a shameful, unconstitutional policy\" and \"providing a shortcut around the unjust suffering and often insurmountable adversity imposed by the internment\". Congressman Mel Levine said the site should \"serve as a reminder of the grievous errors and inhumane policies we pursued domestically during World War II and a reminder that we must never again allow such actions to occur in this country.\"\nAcademics have criticized those who initiated and implemented the WRA relocation policy and members of the JACL for supporting the WRA policies. They have also pointed out that the majority of accounts of the relocation published within the first few decades following the closure of the camps have been from the perspective of the WRA and the JACL.\nFollowing President Trump's Executive Order 14253, and by order of Secretary of the Interior, Manzanar was one of the National Park Service sites to post a notice directing visitors to report \"signs or other information that are negative about either past or living Americans or that fail to emphasize the beauty, grandeur, and abundance of landscapes and other natural features.\" In an interview for NBC News Dr. Nicholas Baham, professor of ethnic studies, was pessimistic about the feasibility of the Manzanar site avoiding such information.\nTerminology.\nSince the end of World War II, there has been debate over the terminology used to refer to Manzanar and the other camps in which Americans of Japanese ancestry and their immigrant parents were incarcerated by the United States Government during the war. Manzanar has been referred to as a \"War Relocation Authority center\", \"War Relocation Center\", \"relocation camp\", \"relocation center\", \"internment camp\", \"incarceration camp\", \"prison camp\", and \"concentration camp\".\nPrior to the opening of an exhibit about the American camps at Ellis Island, the American Jewish Committee (AJC) and the National Park Service, which manages Ellis Island, expressed concern regarding the use of the term \"concentration camp\" in the exhibit. At a meeting held at the offices of the AJC in New York City, leaders representing Japanese Americans and Jewish Americans reached an understanding about the use of the term, and the Japanese American National Museum and the AJC issued a joint statement:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A concentration camp is a place where people are imprisoned not because of any crimes they have committed, but simply because of who they are. Although many groups have been singled out for such persecution throughout history, the term 'concentration camp' was first used at the turn of the [20th] century in the Spanish American and Boer Wars. During World War II, America's concentration camps were clearly distinguishable from Nazi Germany's. Nazi camps were places of torture, barbarous medical experiments and summary executions; some were extermination centers with gas chambers.\nIn popular culture.\nFilms and television.\nA made-for-television movie, \"Farewell to Manzanar\", aired on NBC in 1976. It was based on the 1973 memoir of the same name, written by Jeanne Wakatsuki Houston, who was incarcerated at Manzanar as a child, and her husband James D. Houston. In 2011, the Japanese American National Museum (JANM) announced that they had negotiated the rights to the movie, and that they would make it available for purchase on DVD.\nThe 1990 feature film \"Come See the Paradise\" detailed the forced removal and incarceration at Manzanar of a Japanese American family from Los Angeles.\nIn the 1984 film \"The Karate Kid\", Mr. Miyagi opens up to his student Daniel about the dual loss of his wife and son in childbirth at the Manzanar internment camp; the actor who played Mr. Miyagi, Pat Morita, was interned for two years at Manzanar with his parents.\nIn \"Die\" \"Hard\", Hans Gruber says that Joseph Takagi was interred in Manzanar from 1942 to 1943.\nThe short film, \"A Song for Manzanar\", depicts the true story of a detainee and her struggle to remain hopeful for her son and stay in contact with her family in Hiroshima.\nIn \"Baku\", the 2018 season three episode of \"The Man in the High Castle\" TV series, Frank Frink is executed for his resistance against the Japanese occupation by Kenpeitai inspector Kido on the site of the former camp.\nMusic.\nFolk/country musician Tom Russell wrote \"Manzanar\", a song about the Japanese American incarceration, that was released on his album \"Box of Visions\" (1993). Laurie Lewis covered the song on her album \"Seeing Things\" (1998), adding the koto to her performance. The Asian American jazz fusion band Hiroshima has a song entitled \"Manzanar\", inspired by the incarceration, on its album \"The Bridge\" (2003). Hiroshima's song \"Living in America\", on its album titled \"East\" (1990), contains the phrase \"I still remember Manzanar\". Fort Minor's song \"Kenji\", from the album \"The Rising Tied\" (2005), tells the true story of Mike Shinoda's family including their experiences during their imprisonment at Manzanar. The band Channel 3 recorded a song titled \"Manzanar\" about the incarceration.\nAmerican composer Steve Heitzeg's work \"Green Hope After Black Rain (Symphony for the Survivors of Hiroshima, Nagasaki and the Manzanar Concentration Camp)\" is a memorial to the victims of the 1945 atomic bombings of the two Japanese cities as well as the Manzanar camp. It includes percussion elements made from Hiroshima and Nagasaki trees as well as stones from Manzanar. It was premiered in 2022 by the Saint Paul Civic Symphony in Minnesota.\nLiterature.\nThe 1994 novel \"Snow Falling on Cedars\" by David Guterson contains scenes and details relating to Japanese Americans from the state of Washington and their incarceration experiences at Manzanar. A 1999 film of the same name was based on the book. Paper Wishes, a book published in 2016 by Lois Sepahban is a book about a girl named Manami who goes to Manzanar with her family and loses her dog Yujiin, on the way.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46866", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=46866", "title": "Manazanar War Relocation Center", "text": ""}
{"id": "46867", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=46867", "title": "Manzanar National Historic Landmark", "text": ""}
{"id": "46869", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=46869", "title": "Nise Japanese", "text": ""}
{"id": "46870", "revid": "6959733", "url": "https://en.wikipedia.org/wiki?curid=46870", "title": "Raspberry and cream tart", "text": ""}
{"id": "46872", "revid": "170", "url": "https://en.wikipedia.org/wiki?curid=46872", "title": "Viet Nam War", "text": ""}
{"id": "46873", "revid": "483374", "url": "https://en.wikipedia.org/wiki?curid=46873", "title": "Pie crust", "text": ""}
{"id": "46875", "revid": "577301", "url": "https://en.wikipedia.org/wiki?curid=46875", "title": "Puff pastry", "text": "Light, flaky pastry\nPuff pastry, also known as , is a light, flaky pastry, its base dough () composed of wheat flour and water. Butter or other solid fat () is then layered into the dough. The dough is repeatedly rolled and folded, rested, re-rolled and folded, encasing solid butter between each resulting layer. \nThis produces a laminated dough. During baking, gaps form between the layers left by the fat melting; the pastry is leavened by steam from the water content of the fat as it expands, puffing the separate layers. The pastry layers crisp as the heated fat is in contact with its surfaces.\nHistory.\nWhile modern puff pastry was developed in France in the 17th century, related laminated and air-leavened pastry has a long history. In Spain, likely built upon Arab or Moorish culinary traditions, the first known recipe for pastry using butter or lard following the Arab technique of making each layer separately, appears in the Spanish recipe book ('book on the art of cooking') by Domingo Hern\u00e1ndez de Maceras, published in 1607. Hern\u00e1ndez, the head cook of a college of the University of Salamanca, already distinguished between filled puff pastry recipes and puff pastry tarts, and even mentions leavened preparations. Francisco Mart\u00ednez Moti\u00f1o, head chef to Philip II of Spain (1527\u20131598), also gave several recipes of puff pastry in his published in 1611. In this book, puff pastry is abundantly used, particularly to make savoury game pies.\nThe oldest known documented recipe for puff pastry in France was included in a charter by Robert, bishop of Amiens in 1311. The first recipe to explicitly use the technique of (the action of encasing solid butter within dough layers, keeping the fat intact and separate, by folding several times) was published in 1651 by Fran\u00e7ois Pierre La Varenne in . Modern French puff pastry was then developed and improved by the chef M. Feuillet and Antonin Car\u00eame.\nThe method is sometimes considered the idea of the famous painter Claude Gell\u00e9e when he was an apprentice baker in 1612. Historical evidence for this is negligible, but it is retained as culinary lore. The story goes that Lorrain was making a type of very buttery bread for his sick father, and the process of rolling the butter into the bread dough created a croissant-like finished product.\nProduction.\nThe production of puff pastry dough can be time-consuming, because it must be kept at a temperature of approximately 16\u00a0\u00b0C (60\u00a0\u00b0F) to keep the shortening from melting and the layers melding; it must rest in between folds to allow gluten strands time to link up and thus retain layering. Therefore, between each step the dough is rested and chilled. Before re-rolling, the dough is rotated ninety degrees, so that it is rolled at right angle relative to the previous \"turn\" (as each step is usually referred to). After rolling, another thin layer of butter is applied, the folding and resting are repeated.\nThe chef Julia Child's method has 72 layers for rough-puff pastry () and 729 layers for .\nThe number of layers in puff pastry is calculated with the formula:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nwhere formula_1 is the number of finished layers, formula_2 the number of folds in a single folding move, and formula_3 is how many times the folding move is repeated. For example, twice-folding (i.e. in three), repeated four times gives formula_4 layers.\nCommercially made puff pastry is available in grocery stores. Common types of fat used include butter, vegetable shortenings, lard and margarine. Butter is the most common type used because it provides a richer taste and superior mouthfeel. Shortenings and lard have a higher melting point therefore puff pastry made with either will rise more than pastry made with butter, if made correctly. Puff pastry made in this manner will, however, often have a waxy mouthfeel and more bland flavor. Specialized margarine formulated for high plasticity (the ability to spread very thin without breaking apart) is used for industrial production of puff pastry.\nVariants and distinctions.\nSince the process of making puff pastry is generally laborious and time-intensive, faster recipes are fairly common: known as \"blitz\",490 \"rough puff\", or \"flaky pastry\". Some of these recipes combine the butter into the \"\" rather than adding it in the folding process and are thus similar to a folded short crust. Many retain the layering process, but the number of steps (\"turns\" or \"\") is reduced. Alternatively, or in addition, the butter is scattered over the dough-layer surface in small pieces, or grated, rather than in a single mass or block. Time and effort to evenly distribute the fat in a single mass is thus avoided, and chilling time may be reduced as less handling of the butter generally keeps it at a lower temperature. \nThis process makes rough-puff more similar to another laminated pastry, phyllo (or filo). The dough for phyllo is stretched and rolled to its final pre-baking size. Layering is done immediately before baking, with a small amount of oil or melted fat (usually butter) brushed on one layer of dough, which is then topped with another layer that is also brushed with the fat; the layering is repeated as often as desired. When the phyllo bakes it becomes crispy, but since it contains somewhat less water, it does not expand to the same degree as puff pastry. Puff pastry also differs from Austrian strudel dough, or , which more closely resembles phyllo, in that is stretched (and rolled) into a very thin sheet. Most of the fat is incorporated into , rather than applied to sheets. For strudel, pastry layers are achieved by rolling the (lightly fat-coated) dough around the filling multiple times; some phyllo pastry dishes also use this method.\nSimilar to Puff Pastry, Viennoiserie, is a baker's yeast leavened dough, is used to create croissants, Danish pastry or pain au chocolat. Viennoiserie is considered a separate category of dough and is neither a pastry or a bread.\nUses.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46876", "revid": "1100050", "url": "https://en.wikipedia.org/wiki?curid=46876", "title": "Mince pie", "text": "Sweet pie with dried fruits and spices\nA mince pie (also mincemeat pie in North America, and fruit mince pie in Australia and New Zealand) is a sweet pie of English origin filled with mincemeat, being a mixture of fruit, spices, and suet. The pies are traditionally served during the Christmas season in much of the English-speaking world. Its ingredients are traceable to the 13th century, when returning European crusaders brought with them Middle Eastern recipes containing meats, fruits, and spices; these contained the Christian symbolism of representing the gifts delivered to Jesus by the Biblical Magi. Mince pies, at Christmas time, were traditionally shaped in an oblong shape, to resemble a manger and were often topped with a depiction of the Christ Child.\nThe early mince pie was known by several names, including \"mutton pie\", \"shrid pie\", and \"Christmas pie\". Typically, its ingredients were a mixture of minced meat, suet, a range of fruits, and spices, such as cinnamon, cloves, and nutmeg. Served around Christmas, the savoury Christmas pie (as it became known) was associated with supposed Catholic \"idolatry\", and during the English Civil War was frowned on by the Puritan authorities. Nevertheless, the tradition of eating Christmas pie in December continued through to the Victorian era, although by then its recipe had become sweeter and its size markedly reduced from its once large oblong shape. Today, the mince pie, usually made without meat (but often including suet or other animal fat), remains a popular seasonal treat enjoyed by many across the United Kingdom and Ireland.\nHistory.\nBritain.\nThe ingredients for the modern mince pie can be traced to the return of European crusaders from the Holy Land. Middle Eastern methods of cooking, which sometimes combined meats, fruits and spices, were popular at the time. Pies were created from such mixtures of sweet and savoury foods; in Tudor England, shrid pies (as they were known then) were formed from shredded meat, suet and dried fruit. The addition of spices such as cinnamon, cloves and nutmeg was, according to the English antiquary John Timbs, \"in token of the offerings of the Eastern Magi.\" Several authors, including Timbs, viewed the pie as being derived from an old Roman custom practised during Saturnalia, where Roman fathers in the Vatican were presented with sweetmeats. Early pies were much larger than those consumed today, and oblong shaped; the jurist John Selden presumed that \"the coffin of our \"Christmas\"-Pies, in shape long, is in Imitation of the Cratch [Jesus's crib]\", although writer T. F. Thistleton-Dyer thought Selden's explanation unlikely, as \"in old English cookery books the crust of a pie is generally called 'the coffin'.\"\nThe modern mince pie's precursor was known by several names. The antiquary John Brand claimed that in Elizabethan and Jacobean-era England they were known as minched pies, but other names include mutton pie, and starting in the following century, Christmas pie. Gervase Markham's 1615 recipe recommends taking \"a leg of mutton\", and cutting \"the best of the flesh from the bone\", before adding mutton suet, pepper, salt, cloves, mace, currants, raisins, prunes, dates and orange peel. He also suggested that beef or veal might be used in place of mutton. In the north of England, goose was used in the pie's filling, but more generally beef tongue was also used; a North American filling recipe published in 1854 includes chopped neat's tongue, beef suet, bloom raisins, currants, mace, cloves, nutmeg, brown sugar, apples, lemons, brandy and orange peel. During the English Civil War, along with the censure of other Catholic customs, they were banned: \"Nay, the poor rosemary and bays, and \"Christmas pie\", is made an abomination.\" Puritans were opposed to the Christmas pie, on account of its connection with Catholicism. In his \"History of the Rebellion\", Marchamont Needham wrote \"All Plums the Prophets Sons defy, And Spice-broths are too hot; Treason's in a \"December\"-Pye, And Death within the Pot.\" Some considered them unfit to occupy the plate of a clergyman, causing Philo-Clericus to comment:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIn his essay \"The Life of Samuel Butler\", Samuel Johnson wrote of \"an old Puritan, who was alive in my childhood\u00a0... would have none of his superstitious meats and drinks.\" Another essay, published in the December 1733 issue of \"The Gentleman's Magazine\", explained the popularity of \"Christmas Pye\" as perhaps \"owing to the Barrenness of the Season, and the Scarcity of Fruit and Milk, to make Tarts, Custards, and other Desserts\", but also possibly bearing \"a religious kind of Relation to the Festivity from which it takes its Name.\" The author also mentions the Quakers' objection to the treat, \"who distinguish their Feasts by an heretical Sort of Pudding, known by their Names, and inveigh against Christmas Pye, as an Invention of the Scarlet Whore of \"Babylon\", an Hodge-Podge of Superstition, Popery, the Devil and all his Works.\" Nevertheless, the Christmas pie remained a popular treat at Christmas, although smaller and sweeter, and lacking in post-Reformation England any sign of supposed Catholic idolatry. People began to prepare the fruit and spice filling months before it was required, storing it in jars, and as Britain entered the Victorian age, the addition of meat had, for many, become an afterthought (although the use of suet remains). Its taste then was broadly similar to that experienced today, although some 20th-century writers continued to advocate the inclusion of meat.\nAlthough the modern recipe is no longer the same list of 13\u00a0ingredients once used (representative of Christ and his 12\u00a0Apostles according to author Margaret Baker), the mince pie remains a popular Christmas treat. Bakers Greggs reported sales of 7.5\u00a0million mince pies during Christmas 2011. The popular claim that the consumption of mince pies on Christmas Day is illegal is an urban myth.\nNew England.\nMincemeat pie was brought to New England by English settlers in the 17th century. While it was originally a Christmas pie, as in Britain, the Puritans did not celebrate Christmas, causing the pie's associations in the region to shift toward the American holiday of Thanksgiving. The ingredients for New England mincemeat pie are similar to the British one, with a mixture of apples, raisins, spices, and minced beef serving as the filling. Later recipes sometimes omit the beef, though \"None Such\" (now owned by The J.M. Smucker Company), the major brand of condensed American mincemeat, still contains beef. New England mincemeat pies are usually full-sized pies, as opposed to the individual-sized pies now common in Britain.\nReferences.\nFootnotes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46877", "revid": "50437522", "url": "https://en.wikipedia.org/wiki?curid=46877", "title": "Shortcrust pastry", "text": "Base used for a tart, quiche or pie\nShortcrust is a type of pastry often used for the base of a tart, quiche, pie, or (in the British English sense) flan. Shortcrust pastry can be used to make both sweet and savory pies such as apple pie, quiche, lemon meringue or chicken pie.\nA sweetened version \u2013 using butter \u2013 is used in making spritz cookies. \nShortcrust pastry recipes usually call for twice as much flour as fat by weight. Fat (as lard, shortening, butter or traditional margarine) is rubbed into plain flour to create a loose mixture that is then bound using a small amount of ice water, rolled out, then shaped and placed to create the top or bottom of a pie. Often, equal amounts of butter and lard are used to make the pastry, ensuring that the combined weight of the two fat products is still half that of the flour. The butter is employed to give the pastry a rich flavor, while the lard ensures optimum texture.\nTechniques.\nIn preparing a shortcrust, the fat and flour are \"cut\" into each other, rather than blended, and the ingredients are kept cold. This ensures that the fat remains distinct in the crust, and when it heats during baking, steam is released, resulting in the pockets that make a flaky crust. Water is only added once the fat and flour are thoroughly combined. This ensures that the flour granules are adequately coated with fat and are less likely to develop gluten. This may be achieved with the use of a food processor, a specialized kitchen utensil called a pastry blender, or through various alternatives, like a pair of table knives held in one hand, or smearing the flour and fat together using the heel of the hand in a method known as .\nIn addition to over-warming the dough, overworking it is also a hazard. Overworking elongates the gluten strands, creating a product that is tough, rather than light and crumbly or flaky. Flour made from low protein soft wheat, like cake flour, is used for pastry making because it does not become overworked and tough as easily as bread flour.\nDecorative techniques.\nA pie crust edge is often crimped to provide visual interest, and in the case of a two-crust pie in order to seal the top and bottom crusts together to prevent the filling from leaking. Crimping can be done by hand, pinching the two crusts together to create a ruffled edge, or with a tool. \nA pie's top crust is often pierced to allow steam to escape and to provide visual interest. Piercing can be done with a knife or by using one of several techniques such as latticing. Latticing involves interweaving strips of pastry. It can also be achieved by cutting horizontal rows of slits into a whole pie crust and pulling gently to open the slits , a technique known as a \"peekaboo\" lattice.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46878", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=46878", "title": "La Caletta", "text": "La Caletta is a small town, a harbour and a tourist destination in Sardinia, Italy. Caletta means a small bay or little harbour.\nLocation.\nThe town is located approximately 50\u00a0km south of Olbia, in the administrative territory of Siniscola (province of Nuoro), on the Tyrrhenian coast of the island.\nIt borders, at its northern side, with San Giovanni, the coastal fraction of Posada.\nLa Caletta is in front of a well known beach (appr. 10\u00a0km of pure white sand) that ends at the small town of Santa Lucia.\nHistory.\nAn ancient village of fishermen, its small gulf has been transformed in the 1970s into a touristic harbour, and recently renewed and enlarged. The town (est. 1,000 inhabitants, that become more than 10,000 in Summer) is today deeply dependent on tourism. \nThe local church, dedicated to Nostra Signora di Fatima, was built in 1954.\nProposed ferry service.\nLocal population requires that La Caletta can have in its port a ferry line for \"the Continent\" (Italian mainland), and some experiments were practiced a few years ago, that confirmed the potential success of such an eventual initiative, but (also due to the particular administrative competence of an external organ and being part of the port under the authority of the bordering territory of Posada) administrative problems and local rivalries actually stop any further evolution in this sense."}
{"id": "46879", "revid": "19569748", "url": "https://en.wikipedia.org/wiki?curid=46879", "title": "Nisei Japanese", "text": ""}
{"id": "46880", "revid": "44141905", "url": "https://en.wikipedia.org/wiki?curid=46880", "title": "Japanese-American service in World War II", "text": "Nisei serving in the United States military\nDuring the early years of World War II, Japanese Americans were forcibly relocated from their homes on the West Coast because military leaders and public opinion combined to fan unproven fears of sabotage. As the war progressed, many of the young \"Nisei\", Japanese immigrants' children who were born with American citizenship, volunteered or were drafted to serve in the United States military. Japanese Americans served in all the branches of the United States Armed Forces, including the United States Merchant Marine. An estimated 33,000 Japanese Americans served in the U.S. military during World War II, of which 20,000 joined the Army. Approximately 800 were killed in action.\nThe 100th Battalion and the 442nd Infantry Regiment became the most decorated unit in U.S. military history. The related 522nd Field Artillery Battalion liberated one or more subcamps of the infamous Dachau concentration camp. Other Japanese-American units also included the 100th Infantry Battalion, the 1399th Engineer Construction Battalion, and the Military Intelligence Service.\nServicemen in the U.S. Army.\nThe majority of Japanese Americans serving in the American Armed Forces during World War II enlisted in the army.\n100th Infantry Battalion.\nThe 100th Infantry Battalion was engaged in heavy action during the war taking part in multiple campaigns. The 100th was made up of \"Nisei\" who were originally members of the Hawaii National Guard. Sent to the mainland as the Hawaii Provisional Infantry Battalion on June 5, 1942, the 1,432 original members of the 100th were stationed first at Camp McCoy and later at Camp Shelby for combat training. Their exemplary military record, and the patriotic activities of the Varsity Victory Volunteers, paved the way for the creation of the 442nd Regimental Combat Team in January 1943. The Battalion shipped out in August 1943, landing in North Africa before fighting in Italy, eventually participating in the liberation of Rome.\n442nd Regimental Combat Team.\nMeanwhile, an earlier decision to demote \"Nisei\" soldiers to 4-C class (enemy aliens ineligible for military service because of nationality) was reversed, and the Army in January 1943 issued a call for Japanese-American volunteers. Most of the initial recruits came from Hawaii, as those on the mainland were reluctant to volunteer while they and their families remained in camp. The 2,686 accepted Hawaiians (out of 10,000 volunteers) and about 1,000 mainlanders were sent to Camp Shelby. The U.S. Army regiment served in Europe during World War II. Japanese Americans already in training at the start of the war had been removed from active duty shortly after Pearl Harbor, and the Army stopped accepting new \"Nisei\" recruits in early 1942. However, community leaders in Hawaii as well as Japanese-American leaders like Mike Masaoka along with War Department officials like John J. McCloy soon began to push the Roosevelt administration to allow \"Nisei\" to serve in combat. A military board was convened in June 1942 to address the issue, but their final report opposed forming a \"Nisei\" unit, citing \"the universal distrust in which they [Japanese Americans] are held.\" Despite resistance from military and War Relocation Authority leaders, the President eventually sided with the War Department, and on February 1, 1943, Roosevelt announced the creation of a segregated unit composed of \"Nisei\" soldiers and commanded by white officers. The 100th Infantry Battalion composed of men from Hawaii entered combat in Italy in September 1943 and suffered horrific casualties and became known as the Purple Heart Battalion. As a result, the 1st Battalion of the 442nd began sending replacement troops to join the 100th in early 1944. The 2nd and 3rd Battalions shipped out on May 1, 1944, joining the 100th in Italy in June 1944. These men arrived in Europe after the 100th Infantry Battalion had already established its reputation as a fighting unit, and in time, the 100th/442nd became, for its size and length of service, the most decorated unit in U.S. military history.\n522nd Field Artillery Battalion.\nThe \"Nisei\" 522nd Field Artillery Battalion was organized as part of the 442nd Regimental Combat Team; but towards the end of the war, the 522nd became a roving battalion, shifting to whatever command most needed the unit. The 522nd had the distinction of liberating survivors of the Dachau concentration camp system from the Nazis on April 29, 1945. \"Nisei\" scouts west of Munich near the small Bavarian town of Lager Lechfeld encountered some barracks encircled by barbed wire. Technician Fourth Grade Ichiro Imamura described it in his diary:\n\"I watched as one of the scouts used his carbine to shoot off the chain that held the prison gates shut ... They weren\u2019t dead, as he had first thought. When the gates swung open, we got our first good look at the prisoners. Many of them were Jews. They were wearing striped prison suits and round caps. It was cold and the snow was two feet deep in some places. There were no German guards. The prisoners struggled to their feet ... They shuffled weakly out of the compound. They were like skeletons \u2013 all skin and bones ...\"\nHolocaust historians have clarified the \"Nisei\" 522nd liberated about 3,000 prisoners at Kaufering IV in Hurlach. Hurlach was one of 169 subordinate slave labor camps of Dachau. Dachau, like Auschwitz, Buchenwald, Mauthausen and Ravensbr\u00fcck, was surrounded by hundreds of sub-camps. Only three days later, the survivors of a death march southwards from Dachau towards the Austrian border were found by troops of the 522nd just west of the village of Waakirchen, and cared for them until dedicated medical personnel took over.\nPierre Moulin in his recent book 'Dachau, Holocaust and US Samurais' writes that the first \"Nisei\" arrived at Dachau's gate not on April 29, the date of the liberation of the camp, but on April 28, 1945. Two jeeps of forward observers with 522nd Field Artillery Battalion Captain Charles Feibleman, Kelly Nakamura (Driver), George Oide, Kenzo Okubo, Mike Hara, arrived first at the gates of Dachau but were told to wait for back up since the SS were still in the towers.\nMilitary Intelligence Service.\nApproximately 6,000 Japanese Americans served in the Military Intelligence Service (MIS). The first class received their training at the Presidio in San Francisco, but in June 1942 the MIS Language School was moved to Camp Savage, Minnesota, which offered larger facilities, removed the complications of training Japanese-American students in an area they were technically prohibited from entering, and had less anti-Japanese prejudice. In August 1944, the language school was moved again to Fort Snelling.\n Most of the MIS Language School graduates were attached to the Allied Translator and Interpreter Section (ATIS) as linguists and in other non-combatant roles, interpreting captured enemy documents and interrogating prisoners of war. Graduates from the MISLS included Japanese-American women translators as well. (At the end of the war, MIS linguists had translated 18,000 enemy documents, created 16,000 propaganda leaflets and interrogated over 10,000 Japanese POWs.) However, MIS servicemen were present at every major battle against Japanese forces, and those who served in combat faced extremely dangerous and difficult conditions, sometimes coming under friendly fire from U.S. soldiers unable to distinguish them from the Japanese and often encountering former friends on the battlefield.\nJapanese-American MIS linguists translated Japanese documents known as the \"Z Plan\", which contained Japan's counterattack strategy in the Central Pacific. This information led to Allied victories at the Battle of the Philippine Sea, in which the Japanese lost most of their aircraft carrier planes, and the Battle of Leyte Gulf. An MIS radio operator intercepted a message describing Admiral Isoroku Yamamoto's flight plans, which led to P-38 Lightning fighter planes shooting down his plane over the Solomon Islands.\nWhen Merrill's Marauders were organized to conduct long range penetration special operations jungle warfare deep behind Japanese lines in the China-Burma-India Theater in January 1944, fourteen MIS linguists were assigned to the unit, including Army Rangers and Military Intelligence Hall of Fame inductee Roy Matsumoto.\nThe Nisei under Merrill's command proved themselves particularly intrepid and helpful, venturing into the enemy lines and translating audible commands to counterattacks, and shouting conflicting commands to the Japanese, throwing them into confusion. They soon became the best known Nisei in the war against Japan. The War Relocation Authority used their story to impress other Americans with Nisei valor and loyalty, even placing stories in local newspapers as the war waned in 1945 and the WRA prepared to release the Japanese-Americans back into their communities.\nOver 5,000 Japanese Americans served in the occupation of Japan. Dozens of MIS graduates served as translators, interpreters, and investigators in the International Military Tribunal for the Far East. Thomas Sakamoto served as press escort during the occupation of Japan. He escorted American correspondents to Hiroshima, and the USS \"Missouri\" in Tokyo Bay. Sakamoto was one of three Japanese Americans to be on board the USS \"Missouri\" when the Japanese formally surrendered. Arthur S. Komori served as personal interpreter for Brig. Gen. Elliot R. Thorpe. Kay Kitagawa served as personal interpreter of Fleet Admiral William Halsey Jr. Kan Tagami served as personal interpreter-aide for General Douglas MacArthur. Journalist Don Caswell was accompanied by a Nisei interpreter to Fuch\u016b Prison, where the Japanese government imprisoned communists Tokuda Kyuichi, Yoshio Shiga, and Shiro Mitamura.\nServicemen in the Army Air Forces.\nJapanese Americans were generally forbidden to fight a combat role in the Pacific theatre ; although no such limitations were placed on Americans of German or Italian ancestry who fought against the Axis powers. Up to this point, the United States government has only been able to find records of five Japanese Americans who were members of the Army Air Forces during World War II, one of them being Kenje Ogata. There was at least one \"Nisei\", U.S. Army Air Forces Technical Sergeant Ben Kuroki, who participated initially in 35 missions as a dorsal turret gunner over Europe, followed by 28 bombing missions over mainland Japan and other locations in the Pacific Theater.\n\"Nisei\" Herbert Seijin Ginoza flew combat missions over Europe as a waist-tail gunner in the 483rd Bomb Group. He spent 3 months as a German prisoner-of-war after his B17 was shot down on a bombing mission near Vienna, Austria.\nWomen's Army Corps.\nLike their male counterparts, \"Nisei\" women were at first prohibited from serving in the U.S. military; this changed in November 1943, and 142 young women volunteered to join the WAC. Because their number was relatively small, the \"Nisei\" WACs were not restricted to a segregated corps, but instead were spread out and served alongside other ethnic groups. The idea of female auxiliary service was still new at this time (the Women's Army Corps was only nine months old when it opened its ranks to \"Nisei\" volunteers), and these women were most often assigned to clerical duties or other \"women's work.\" Additionally, WACs were often portrayed in media and propaganda as highly sexualized and were encouraged by male supervisors to play into this role. The \"Nisei\" WACs faced another difficulty in that they were expected to translate Japanese military documents; even those who were fluent in Japanese struggled to understand the military language, and eventually some were sent to the Military Intelligence Language School for training.\nAfter the war.\nThe 442nd marched down Constitution Avenue to the Ellipse south of the White House on July 15, 1946, where President Truman honored the regiment with a Presidential Unit Citation saying, \"You fought not only the enemy, but you fought prejudice--and you have won.\" However, the unit's service and decorations still did not change the attitudes of the general population in much of the U.S. towards people of Japanese ancestry. Veterans came home to signs that read \"No Japs Allowed\" and \"No Japs Wanted\", the denial of service in shops and restaurants, and the vandalism of their homes and property.\nInitially, many veterans' organizations such as the VFW and the American Legion refused to allow Nisei veterans into existing posts and some even removed Japanese-American soldiers from their honor rolls. White officers from the 442nd including Col Virgil R. Miller advocated on the behalf of the Nisei in Chicago to be allowed to form their own American Legion post 1183 in 1946, while Alva Fleming, a Navy veteran in Sacramento district leadership approved the charter for Nisei VFW Post 8985 in 1947. Fleming would go on to become the VFW State Commander for California and was instrumental in founding a total of 14 segregated Nisei VFW posts in the state. Veterans in the Pacific Northwest were unable to find any post willing to accept them, and eventually formed their own independent \"Nisei Veterans Committee\". Although VFW National leadership condemned the actions of local posts, their bylaws promoted autonomy in individual posts and were powerless to prevent the discrimination. Smaller organizations such as American Veterans Committee and Military Order of the Purple Heart invited Nisei into their ranks, however they did not offer the same facilities and benefits as the larger organizations.\nRecognition.\nSome of the first memorials to the Nisei were created by 442nd and MIS veterans themselves, in the creation of the many Nisei American Legion, VFW, and independent memorial posts around the country, dedicated to their fallen brothers in arms.\nThe nation's highest award for combat valor, the Medal of Honor, was conferred upon one \"Nisei\" during the war, Sadao Munemori, after he sacrificed his life to save his fellow soldiers. Nineteen members of the 100th Infantry Battalion and the 442nd Regimental Combat Team received Distinguished Service Crosses during or immediately after their World War II service, but in the 1990s, after a study revealed that racial discrimination had caused them to be overlooked, their awards were upgraded to Medals of Honor. In addition, one soldier who had received the Silver Star had his award upgraded to the Medal of Honor. \nOn October 5, 2010, Congress approved the granting of the Congressional Gold Medal to the 442nd Regimental Combat Team and the 100th Infantry Battalion, as well as the 6,000 Japanese Americans who served in the Military Intelligence Service during the war. The Nisei Soldiers of World War II Congressional Gold Medal was collectively presented on November 2, 2011.\nThe Japanese American Memorial to Patriotism During World War II in Washington, D.C. is a National Park Service site to commemorate the experience of American citizens of Japanese ancestry and their parents who patriotically supported the United States despite unjust treatment during World War II.\nThe Go for Broke Monument in Little Tokyo, Los Angeles, California, commemorates the Japanese Americans who served in the United States Army during World War II.\nThe National Japanese American Veterans Memorial Court in Los Angeles lists the names of all the Japanese Americans killed in service to the country in World War II as well as in Korea, Vietnam, Iraq, and Afghanistan.\nCalifornia has given four state highway segments honorary designations for Japanese American soldiers:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46881", "revid": "279219", "url": "https://en.wikipedia.org/wiki?curid=46881", "title": "Issei Japanese", "text": ""}
{"id": "46883", "revid": "18186449", "url": "https://en.wikipedia.org/wiki?curid=46883", "title": "Babylonia", "text": "Ancient Amorite-Akkadian state in Mesopotamia\nBabylonia (; , ) was an ancient Akkadian-speaking state and cultural area based on the city of Babylon in central-southern Mesopotamia (present-day Iraq and parts of Syria). It emerged as an Akkadian-populated but Amorite-ruled state c.\u20091894 BC. During the reign of Hammurabi and afterwards, Babylonia was retrospectively called \"the country of Akkad\" ( in Akkadian), a deliberate archaism in reference to the previous glory of the Akkadian Empire. It was often involved in rivalry with the linguistically related state of Assyria in Upper Mesopotamia, and with Elam to the east. Babylonia briefly became the major power in the region after Hammurabi (fl. c.\u20091792\u20131752 BC middle chronology, or c.\u20091696\u20131654 BC, short chronology) created a short-lived empire, succeeding the earlier Akkadian Empire, Third Dynasty of Ur, and Old Assyrian Empire. The Babylonian Empire rapidly fell apart after the death of Hammurabi and reverted to a small kingdom centered around the city of Babylon.\nLike Assyria, the Babylonian state retained the written Akkadian language for official use, despite its Northwest Semitic-speaking Amorite founders and Kassite successors, who spoke a language isolate. The state retained the Sumerian language in sacred texts for the Babylonian religion, but already by the time Babylon was founded, this was no longer a spoken language, having been replaced by Akkadian. The earlier Akkadian and Sumerian traditions played a major role in the descendant Babylonian culture, and the region would remain an important cultural center, even under its protracted periods of outside rule.\nHistory.\nPre-Babylonian Sumero-Akkadian period.\nMesopotamia had already enjoyed a long history before the emergence of Babylon, with Sumerian civilization emerging in the region c.\u20095400 BC, and the Akkadian-speakers who would go on to form Akkad, Assyria and Babylonia appearing somewhere between the 35th and 30th century BC.\nDuring the 3rd millennium BC, an intimate cultural symbiosis occurred between Sumerian and Akkadian-speakers, which included widespread bilingualism. The influence of Sumerian on Akkadian and vice versa is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the third millennium as a \"sprachbund\".\nAkkadian gradually replaced Sumerian as the spoken language of Mesopotamia somewhere around the turn of the third and the second millennium BC (the precise timeframe being a matter of debate). From c.\u20095400 BC until the rise of the Akkadian Empire in the 24th century BC, Mesopotamia had been dominated by largely Sumerian cities and city states, such as Ur, Lagash, Uruk, Kish, Isin, Larsa, Adab, Eridu, Gasur, Assur, Hamazi, Akshak, Arbela and Umma, although Semitic Akkadian names began to appear on the king lists of some of these states (such as Eshnunna and Assyria) between the 29th and 25th centuries BC. Traditionally, the major religious center of all Mesopotamia was the city of Nippur where the god Enlil was supreme, and it would remain so until replaced by Babylon during the reign of Hammurabi in the mid-18th century BC. The Akkadian Empire (2334\u20132154 BC) saw the Akkadian Semites and Sumerians of Mesopotamia unite under one rule, and the Akkadians fully attain ascendancy over the Sumerians and indeed come to dominate much of the ancient Near East. The empire eventually disintegrated due to economic decline, climate change, and civil war, followed by attacks by the language isolate speaking Gutians from the Zagros Mountains to the northeast. Sumer rose up again with the Third Dynasty of Ur (Neo-Sumerian Empire) in the late 22nd century BC, and ejected the Gutians from southern Mesopotamia in 2161 BC as suggested by surviving tablets and astronomy simulations. They also seem to have gained ascendancy over much of the territory of the Akkadian speaking kings of Assyria in northern Mesopotamia for a time.\nFollowed by the collapse of the Sumerian \"Ur-III\" dynasty at the hands of the Elamites in 2002 BC, the Amorites (\"Westerners\"), a foreign Northwest Semitic-speaking people, began to migrate into southern Mesopotamia from the northern Levant, gradually gaining control over most of southern Mesopotamia, where they formed a series of small kingdoms, while the Assyrians reasserted their independence in the north. The states of the south were unable to stem the Amorite advance, and for a time may have relied on their fellow Akkadians in Assyria for protection.\nKing Ilu-shuma (c.\u20092008\u20131975 BC) of the Old Assyrian period (2025\u20131750 BC) in a known inscription describes his exploits to the south as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt; Past scholars originally extrapolated from this text that it means he defeated the invading Amorites to the south and Elamites to the east, but there is no explicit record of that, and some scholars believe the Assyrian kings were merely giving preferential trade agreements to the south.\nThese policies, whether military, economic or both, were continued by his successors Erishum I and Ikunum.\nHowever, when Sargon I (1920\u20131881 BC) succeeded as king in Assyria in 1920 BC, he eventually withdrew Assyria from the region, preferring to concentrate on continuing the vigorous expansion of Assyrian colonies in Anatolia at the expense of the Hurrians and Hattians and the Amorite inhabited Levant, and eventually southern Mesopotamia fell to the Amorites. During the first centuries of what is called the \"Amorite period\", the most powerful city-states in the south were Isin, Eshnunna and Larsa, together with Assyria in the north.\nFirst Babylonian dynasty \u2013 Amorite dynasty, 1894\u20131595 BC.\nAround 1894 BC, an Amorite chieftain named Sumu-abum appropriated a tract of land which included the then relatively small city of Babylon from the neighbouring minor city-state of Kazallu, of which it had initially been a territory, turning his newly acquired lands into a state in its own right. His reign was concerned with establishing statehood amongst a sea of other minor city-states and kingdoms in the region. However, Sumu-abum appears never to have bothered to give himself the title of \"King of Babylon\", suggesting that Babylon itself was still only a minor town or city, and not worthy of kingship.\nHe was followed by Sumu-la-El, Sabium, and Apil-Sin, each of whom ruled in the same vague manner as Sumu-abum, with no reference to kingship of Babylon itself being made in any written records of the time. Sin-Muballit was the first of these Amorite rulers to be regarded officially as a \"king of Babylon\", and then on only one single clay tablet. Under these kings, Babylonia remained a small nation which controlled very little territory, and was overshadowed by neighbouring kingdoms that were both older, larger, and more powerful, such as; Isin, Larsa, Assyria to the north and Elam to the east in ancient Iran. The Elamites occupied huge swathes of southern Mesopotamia, and the early Amorite rulers were largely held in vassalage to Elam.\nEmpire of Hammurabi.\nBabylon remained a minor town in a small state until the reign of its sixth Amorite ruler, Hammurabi, during 1792\u20131750 BC (or c.\u20091728\u20131686 BC in the short chronology). He conducted major building work in Babylon, expanding it from a small town into a great city worthy of kingship. A very efficient ruler, he established a bureaucracy, with taxation and centralized government. Hammurabi freed Babylon from Elamite dominance, and indeed drove the Elamites from southern Mesopotamia entirely, invading Elam itself. He then systematically conquered southern Mesopotamia, including the cities of Isin, Larsa, Eshnunna, Kish, Lagash, Nippur, Borsippa, Ur, Uruk, Umma, Adab, Sippar, Rapiqum, and Eridu. His conquests gave the region stability after turbulent times, and coalesced the patchwork of small states into a single nation; it is only from the time of Hammurabi that southern Mesopotamia acquired the name \"Babylonia\".\nHammurabi turned his disciplined armies eastwards and invaded the region which a thousand years later became Iran, conquering Elam, Gutium, Lullubi, Turukku and Kassites. To the west, he conquered the Amorite states of the Levant (modern Syria and Jordan) including the powerful kingdoms of Mari and Yamhad.\nHammurabi then entered into a protracted war with the Old Assyrian Empire for control of Mesopotamia and dominance of the Near East. Assyria had extended control over much of the Hurrian and Hattian parts of southeast Anatolia from the 21st century BC, and from the latter part of the 20th century BC had asserted itself over the northeast Levant and central Mesopotamia. After a protracted struggle over decades with the powerful Assyrian kings Shamshi-Adad I and Ishme-Dagan I, Hammurabi forced their successor Mut-Ashkur to pay tribute to Babylon c.\u20091751 BC, giving Babylonia control over Assyria's centuries-old Hattian and Hurrian colonies in Anatolia.\nOne of Hammurabi's most important and lasting works was the compilation of the Babylonian law code, which improved the much earlier codes of Sumer, Akkad and Assyria. This was made by order of Hammurabi after the expulsion of the Elamites and the settlement of his kingdom. In 1901, a copy of the Code of Hammurabi was discovered on a stele by Jacques de Morgan and Jean-Vincent Scheil at Susa in Elam, where it had later been taken as plunder. That copy is now in the Louvre.\nFrom before 3000 BC until the reign of Hammurabi, the major cultural and religious center of southern Mesopotamia had been the ancient city of Nippur, where the god Enlil was supreme. Hammurabi transferred this dominance to Babylon, making Marduk supreme in the pantheon of southern Mesopotamia (with the god Ashur, and to some degree Ishtar, remaining the long-dominant deity in northern Mesopotamian Assyria). The city of Babylon became known as a \"holy city\" where any legitimate ruler of southern Mesopotamia had to be crowned, and the city was also revered by Assyria for these religious reasons. Hammurabi turned what had previously been a minor administrative town into a large, powerful and influential city, extended its rule over the entirety of southern Mesopotamia, and erected a number of buildings.\nThe Amorite-ruled Babylonians, like their predecessor states, engaged in regular trade with the Amorite and Canaanite city-states to the west, with Babylonian officials or troops sometimes passing to the Levant and Canaan, and Amorite merchants operating freely throughout Mesopotamia. The Babylonian monarchy's western connections remained strong for quite some time. Ammi-Ditana, great-grandson of Hammurabi, still titled himself \"king of the land of the Amorites\". Ammi-Ditana's father and son also bore Amorite names: Abi-Eshuh and Ammi-Saduqa.\nDecline.\nSouthern Mesopotamia had no natural, defensible boundaries, making it vulnerable to attack. After the death of Hammurabi, his empire began to disintegrate rapidly. Under his successor Samsu-iluna (1749\u20131712 BC) the far south of Mesopotamia was lost to a native Akkadian-speaking king Ilum-ma-ili who ejected the Amorite-ruled Babylonians. The south became the native Sealand Dynasty, remaining free of Babylon for the next 272 years.\nBoth the Babylonians and their Amorite rulers were driven from Assyria to the north by an Assyrian-Akkadian governor named Puzur-Sin c.\u20091740 BC, who regarded king Mut-Ashkur as both a foreign Amorite and a former lackey of Babylon. After six years of civil war in Assyria, a native king named Adasi seized power c.\u20091735 BC, and went on to appropriate former Babylonian and Amorite territory in central Mesopotamia, as did his successor Bel-bani.\nAmorite rule survived in a much reduced Babylon, Samshu-iluna's successor Abi-Eshuh made a vain attempt to recapture the Sealand Dynasty for Babylon, but met defeat at the hands of king Damqi-ilishu II. By the end of his reign Babylonia had shrunk to the small and relatively weak nation it had been upon its foundation, although the city itself was far larger and opulent than the small town it had been prior to the rise of Hammurabi.\nHe was followed by Ammi-Ditana and then Ammi-Saduqa, both of whom were in too weak a position to make any attempt to regain the many territories lost after the death of Hammurabi, contenting themselves with peaceful building projects in Babylon itself.\nSamsu-Ditana was to be the last Amorite ruler of Babylon. Early in his reign he came under pressure from the Kassites, a people speaking an apparent language isolate originating in the mountains of what is today northwest Iran. Babylon was then attacked by the Indo-European-speaking, Anatolia-based Hittites in 1595 BC. Shamshu-Ditana was overthrown following the \"sack of Babylon\" by the Hittite king Mursili I. The Hittites did not remain for long, but the destruction wrought by them finally enabled their Kassite allies to gain control.\nSack of Babylon and ancient Near East chronology.\nThe date of the sack of Babylon by the Hittites under king Mursili I is considered crucial to the various calculations of the early chronology of the ancient Near East, as it is taken as a fixed point in the discussion. Suggestions for its precise date vary by as much as 230 years, corresponding to the uncertainty regarding the length of the \"Dark Age\" of the much later Late Bronze Age collapse, resulting in the shift of the entire Bronze Age chronology of Mesopotamia with regard to the Egyptian chronology. Possible dates for the sack of Babylon are:\nMursili I, the Hittite king, first conquered Aleppo, capital of Yamhad kingdom, to avenge the death of his father, but his main geopolitical target was Babylon. The Mesopotamian Chronicle 40, written after 1500 BC, mentions briefly the sack of Babylon as: \"During the time of Samsu-Ditana, the Hittites marched on Akkad.\" More details can be found in another source, the Telepinu Proclamation, a Hittite text from around 1520 BC, which states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And then he [Mursili I] marched to Aleppo, and he destroyed Aleppo and brought captives and possessions of Aleppo to \u1e2aattu\u0161a. Then, however, he marched to Babylon, and he destroyed Babylon, and he defeated the Hurrian troops, and he brought captives and possessions of Babylon to \u1e2aattu\u0161a.\nThe movement of Mursili's troops was around 800\u00a0km from the conquered Aleppo to reach the Euphrates, located to the east, skirting around Assyria, and then to the south along the course of the river to reach finally Babylon. His conquest of Babylon brought to an end the dynasty of Hammurabi, and although the Hittite text, Telipinu Proclamation, does not mention Samsu-ditana, and the Babylonian Chronicle 20 does not mention a specific Hittite king either, Trevor Bryce concludes that there is no doubt that both sources refer to Mursili I and Samsu-ditana.\nThe Hittites, when sacking Babylon, removed the images of the gods Marduk and his consort Zarpanitu from the Esagil temple and they took them to their kingdom. The later inscription of Agum-kakrime, the Kassite king, claims he returned the images; and another later text, the Marduk Prophesy, written long after the events, mentions that the image of Marduk was in exile around twenty-four years.\nAfter the conquest, Mursili I did not attempt to convert the whole region he had occupied from Aleppo to Babylon as a part of his kingdom; he instead made an alliance with the Kassites, and then a Kassite dynasty was established in Babylonia.\nKassite dynasty, 1595\u20131155 BC.\nThe Kassite dynasty was founded by Gandash of Mari. The Kassites, like the Amorite rulers who had preceded them, were not originally native to Mesopotamia. Rather, they had first appeared in the Zagros Mountains of what is today northwestern Iran.\nThe ethnic affiliation of the Kassites is unclear. Still, their language was not Semitic or Indo-European, and is thought to have been either a language isolate or possibly related to the Hurro-Urartian language family of Anatolia, although the evidence for its genetic affiliation is meager due to the scarcity of extant texts. That said, several Kassite leaders may have borne Indo-European names, and they may have had an Indo-European elite similar to the Mitanni elite that later ruled over the Hurrians of central and eastern Anatolia, while others had Semitic names.\nThe Kassites renamed Babylon Kardunia\u0161 and their rule lasted for 576 years, the longest dynasty in Babylonian history.\nThis new foreign dominion offers a striking analogy to the roughly contemporary rule of the Semitic Hyksos in ancient Egypt. Most divine attributes ascribed to the Amorite kings of Babylonia disappeared at this time; the title \"god\" was never given to a Kassite sovereign. Babylon continued to be the capital of the kingdom and one of the \"holy cities\" of western Asia, where the priests of the ancient Mesopotamian religion were all-powerful, and the only place where the right to inheritance of the short lived old Babylonian empire could be conferred.\nBabylonia experienced short periods of relative power, but in general proved to be relatively weak under the long rule of the Kassites, and spent long periods under Assyrian and Elamite domination and interference.\nIt is not clear precisely when Kassite rule of Babylon began, but the Indo-European Hittites from Anatolia did not remain in Babylonia for long after the sacking of the city, and it is likely the Kassites moved in soon afterwards. Agum II took the throne for the Kassites in 1595 BC, and ruled a state that extended from Iran to the middle Euphrates; The new king retained peaceful relations with Erishum III, the native Mesopotamian king of Assyria, but successfully went to war with the Hittite Empire, and twenty-four years after, the Hittites took the sacred statue of Marduk, he recovered it and declared the god equal to the Kassite deity Shuqamuna.\nBurnaburiash I succeeded him and drew up a peace treaty with the Assyrian king Puzur-Ashur III, and had a largely uneventful reign, as did his successor Kashtiliash III.\nThe Sealand Dynasty of southern Mesopotamia remained independent of Babylonia and like Assyria was in native Akkadian-speaking hands. Ulamburiash managed to attack it and conquered parts of the land from \"Ea-gamil\", a king with a distinctly Sumerian name, around 1450 BC, whereupon Ea-Gamil fled to his allies in Elam. The Sealand Dynasty region still remained independent, and the Kassite king seems to have been unable to finally conquer it. Ulamburiash began making treaties with ancient Egypt, which then was ruling southern Canaan, and Assyria to the north. Agum III also campaigned against the Sealand Dynasty, finally wholly conquering the far south of Mesopotamia for Babylon, destroying its capital Dur-Enlil in the process. From there Agum III extended farther south still, invading what was many centuries later to be called the Arabian Peninsula or Arabia, and conquering the \"pre-Arab\" state of Dilmun (in modern Bahrain).\nKaraindash built a bas-relief temple in Uruk and Kurigalzu I (1415\u20131390 BC) built a new capital Dur-Kurigalzu named after himself, transferring administrative rule from Babylon. Both of these kings continued to struggle unsuccessfully against the Sealand Dynasty. Karaindash also strengthened diplomatic ties with the Assyrian king Ashur-bel-nisheshu and the Egyptian Pharaoh Thutmose III and protected Babylonian borders with Elam.\nKadashman-Harbe I succeeded Karaindash, and briefly invaded Elam before being eventually defeated and ejected by its king Tepti Ahar. He then had to contend with the Suteans, ancient Semitic-speaking peoples from the southeastern Levant who invaded Babylonia and sacked Uruk. He describes having \"annihilated their extensive forces\", then constructed fortresses in a mountain region called \u1e2ai\u1e2bi, in the desert to the west (modern Syria) as security outposts, and \"he dug wells and settled people on fertile lands, to strengthen the guard\".\nKurigalzu I succeeded the throne, and soon came into conflict with Elam, to the east. When \u1e2aur-batila, the successor of Tepti Ahar took the throne of Elam, he began raiding Babylonia, taunting Kurigalzu to do battle with him at D\u016br-\u0160ulgi. Kurigalzu launched a campaign which resulted in the abject defeat and capture of \u1e2aur-batila, who appears in no other inscriptions. He went on to conquer the eastern lands of Elam. This took his army to the Elamite capital, the city of Susa, which was sacked. After this a puppet ruler was placed on the Elamite throne, subject to Babylonia. Kurigalzu I maintained friendly relations with Assyria, Egypt and the Hittites throughout his reign. Kadashman-Enlil I (1374\u20131360 BC) succeeded him, and continued his diplomatic policies.\nBurna-Buriash II ascended to the throne in 1359 BC, he retained friendly relations with Egypt, but the resurgent Middle Assyrian Empire (1365\u20131050 BC) to the north was now encroaching into northern Babylonia, and as a symbol of peace, the Babylonian king took the daughter of the powerful Assyrian king Ashur-uballit I in marriage. He also maintained friendly relations with Suppiluliuma I, ruler of the Hittite Empire.\nHe was succeeded by Kara-\u1e2barda\u0161 (who was half Assyrian, and the grandson of the Assyrian king) in 1333 BC, a usurper named Nazi-Buga\u0161 deposed him, enraging Ashur-uballit I, who invaded and sacked Babylon, slew Nazi-Buga\u0161, annexed Babylonian territory for the Middle Assyrian Empire, and installed Kurigalzu II (1345\u20131324 BC) as his vassal ruler of Babylonia.\nSoon after Arik-den-ili succeeded the throne of Assyria in 1327 BC, Kurigalzu II attacked Assyria in an attempt to reassert Babylonian power. After some impressive initial successes he was ultimately defeated, and lost yet more territory to Assyria. Between 1307 BC and 1232 BC his successors, such as Nazi-Maruttash, Kadashman-Turgu, Kadashman-Enlil II, Kudur-Enlil and Shagarakti-Shuriash, allied with the empires of the Hittites and the Mitanni (who were both also losing swathes of territory to the resurgent Assyrians), in a failed attempt to stop Assyrian expansion. This expansion, nevertheless, continued unchecked.\nKashtiliash IV's (1242\u20131235 BC) reign ended catastrophically as the Assyrian king Tukulti-Ninurta I (1243\u20131207 BC) routed his armies, sacked and burned Babylon and set himself up as king, ironically becoming the first \"native\" Mesopotamian to rule the Mesopotamian populated state, its previous rulers having all been \"non-Mesopotamian\" Amorites and Kassites. Kashtiliash himself was taken to Ashur as a prisoner of war.\nAn Assyrian governor/king named Enlil-nadin-shumi was placed on the throne to rule as viceroy to Tukulti-Ninurta I, and Kadashman-Harbe II and Adad-shuma-iddina succeeded as Assyrian governor/kings, also subject to Tukulti-Ninurta I until 1216 BC.\nBabylon did not begin to recover until late in the reign of Adad-shuma-usur (1216\u20131189 BC), as he too remained a vassal of Assyria until 1193 BC. However, he was able to prevent the Assyrian king Enlil-kudurri-usur from retaking Babylonia, which, apart from its northern reaches, had mostly shrugged off Assyrian domination during a short period of civil war in the Assyrian empire, in the years after the death of Tukulti-Ninurta.\nMeli-Shipak II (1188\u20131172 BC) seems to have had a peaceful reign. Despite not being able to regain northern Babylonia from Assyria, no further territory was lost, Elam did not threaten, and the Late Bronze Age collapse now affecting the Levant, Canaan, Egypt, the Caucasus, Anatolia, Mediterranean, North Africa, northern Iran and Balkans seemed (initially) to have little impact on Babylonia (or indeed Assyria and Elam).\nWar resumed under subsequent kings such as Marduk-apla-iddina I (1171\u20131159 BC) and Zababa-shuma-iddin (1158 BC). The long reigning Assyrian king Ashur-dan I (1179\u20131133 BC) resumed expansionist policies and conquered further parts of northern Babylonia from both kings, and the Elamite ruler Shutruk-Nakhunte eventually conquered most of eastern Babylonia. Enlil-nadin-ahhe (1157\u20131155 BC) was finally overthrown and the Kassite dynasty ended after Ashur-dan I conquered yet more of northern and central Babylonia, and the equally powerful Shutruk-Nahhunte pushed deep into the heart of Babylonia itself, sacking the city and slaying the king. Poetical works have been found lamenting this disaster.\nDespite the loss of territory, general military weakness, and evident reduction in literacy and culture, the Kassite dynasty was the longest-lived dynasty of Babylon, lasting until 1155 BC, when Babylon was conquered by Shutruk-Nakhunte of Elam, and reconquered a few years later by the Nebuchadnezzar I, part of the larger Late Bronze Age collapse.\nEarly Iron Age \u2013 Native rule, second dynasty of Isin, 1155\u20131026 BC.\nThe Elamites did not remain in control of Babylonia long, instead entering into an ultimately unsuccessful war with Assyria, allowing Marduk-kabit-ahheshu (1155\u20131139 BC) to establish the Dynasty IV of Babylon, from Isin, with the first native Akkadian-speaking south Mesopotamian dynasty to rule Babylonia, with Marduk-kabit-ahheshu becoming only the second native Mesopotamian to sit on the throne of Babylon, after the Assyrian king Tukulti-Ninurta I. His dynasty was to remain in power for some 125 years. The new king successfully drove out the Elamites and prevented any possible Kassite revival. Later in his reign he went to war with Assyria, and had some initial success, briefly capturing the south Assyrian city of Ekallatum before ultimately suffering defeat at the hands of Ashur-Dan I.\nItti-Marduk-balatu succeeded his father in 1138 BC, and successfully repelled Elamite attacks on Babylonia during his 8-year reign. He too made attempts to attack Assyria, but also met with failure at the hands of the still reigning Ashur-Dan I.\nNinurta-nadin-shumi took the throne in 1127 BC, and also attempted an invasion of Assyria, his armies seem to have skirted through eastern Aramea (modern Syria) and then made an attempt to attack the Assyrian city of Arbela (modern Erbil) from the west. However, this bold move met with defeat at the hands of Ashur-resh-ishi I who then forced a treaty in his favour upon the Babylonian king.\nNebuchadnezzar I (1124\u20131103 BC) was the most famous ruler of this dynasty. He fought and defeated the Elamites and drove them from Babylonian territory, invading Elam itself, sacking the Elamite capital Susa, and recovering the sacred statue of Marduk that had been carried off from Babylon during the fall of the Kassites. Shortly afterwards, the king of Elam was assassinated and his kingdom disintegrated into civil war. However, Nebuchadnezzar failed to extend Babylonian territory further, being defeated a number of times by Ashur-resh-ishi I (1133\u20131115 BC), king of the Middle Assyrian Empire, for control of formerly Hittite-controlled territories in Aram and Anatolia. The Hittite Empire of the northern and western Levant and eastern Anatolia had been largely annexed by the Middle Assyrian Empire, and its heartland finally overrun by invading Phrygians from the Balkans. In the later years of his reign, Nebuchadnezzar I devoted himself to peaceful building projects and securing Babylonia's borders against the Assyrians, Elamites and Arameans.\nNebuchadnezzar was succeeded by his two sons, firstly Enlil-nadin-apli (1103\u20131100 BC), who lost territory to Assyria. The second of them, Marduk-nadin-ahhe (1098\u20131081 BC) also went to war with Assyria. Some initial success in these conflicts gave way to a catastrophic defeat at the hands of the powerful Assyrian king Tiglath-Pileser I (1115\u20131076 BC), who annexed huge swathes of Babylonian territory, thus further expanding the Assyrian Empire. Following this a terrible famine gripped Babylon, inviting attacks and migrations from the northwest Semitic tribes of Aramaeans and Suteans from the Levant.\nIn 1072 BC Marduk-shapik-zeri signed a peace treaty with Ashur-bel-kala (1075\u20131056 BC) of Assyria, however, his successor Kada\u0161man-Buria\u0161 was not so friendly to Assyria, prompting the Assyrian king to invade Babylonia and depose him, placing Adad-apla-iddina on the throne as his vassal. Assyrian domination continued until c.\u20091050 BC, with Marduk-ahhe-eriba and Marduk-zer-X regarded as vassals of Assyria. After 1050 BC the Middle Assyrian Empire descended into a period of civil war, followed by constant warfare with the Arameans, Phrygians, Neo-Hittite states and Hurrians, allowing Babylonia to once more largely free itself from the Assyrian yoke for a few decades.\nHowever, East Semitic-speaking Babylonia soon began to suffer further repeated incursions from West Semitic nomadic peoples migrating from the Levant during the Bronze Age collapse, and during the 11th century BC large swathes of the Babylonian countryside was appropriated and occupied by these newly arrived Arameans and Suteans. Arameans settled much of the countryside in eastern and central Babylonia and the Suteans in the western deserts, with the weak Babylonian kings being unable to stem these migrations.\nPeriod of chaos, 1026\u2013911 BC.\nThe ruling Babylonian dynasty of Nabu-shum-libur was deposed by marauding Arameans in 1026 BC, and the heart of Babylonia, including the capital city itself descended into anarchic state, and no king was to rule Babylon for over 20 years.\nHowever, in southern Mesopotamia (a region corresponding with the old Dynasty of the Sealand), Dynasty V (1025\u20131004 BC) arose, this was ruled by Simbar-shipak, leader of a Kassite clan, and was in effect a separate state from Babylon. The state of anarchy allowed the Assyrian ruler Ashur-nirari IV (1019\u20131013 BC) the opportunity to attack Babylonia in 1018 BC, and he invaded and captured the Babylonian city of Atlila and some south central regions of Mesopotamia for Assyria.\nThe south Mesopotamian dynasty was replaced by another Kassite Dynasty (Dynasty VI; 1003\u2013984 BC) which also seems to have regained control over Babylon itself. The Elamites deposed this brief Kassite revival, with king Mar-biti-apla-usur founding Dynasty VII (984\u2013977 BC). However, this dynasty too fell, when the Arameans once more ravaged Babylon.\nBabylonian rule was restored by Nab\u00fb-mukin-apli in 977 BC, ushering in Dynasty VIII. Dynasty IX begins with Ninurta-kudurri-usur II, who ruled from 941 BC. Babylonia remained weak during this period, with whole areas of Babylonia now under firm Aramean and Sutean control. Babylonian rulers were often forced to bow to pressure from Assyria and Elam, both of which had appropriated Babylonian territory.\nAssyrian rule, 911\u2013619 BC.\nBabylonia remained in a state of chaos as the 10th century BC drew to a close. A further migration of nomads from the Levant occurred in the early 9th century BC with the arrival of the Chaldeans, another nomadic Northwest Semitic-speaking people described in Assyrian annals as the \"Kaldu\". The Chaldeans settled in the far southeast of Babylonia, joining the already long extant Arameans and Suteans. By 850 BC the migrant Chaldeans had established a small territory in the extreme southeast of Mesopotamia.\nFrom 911 BC with the founding of the Neo-Assyrian Empire (911\u2013605 BC) by Adad-nirari II, Babylon found itself once again under the domination and rule of its fellow Mesopotamian state for the next three centuries. Adad-nirari II twice attacked and defeated Shamash-mudammiq of Babylonia, annexing a large area of land north of the Diyala River and the towns of H\u012bt and Zanqu in mid Mesopotamia. He made further gains over Babylonia under Nabu-shuma-ukin I later in his reign. Tukulti-Ninurta II and Ashurnasirpal II also forced Babylonia into vassalage, and Shalmaneser III (859\u2013824 BC) sacked Babylon itself, slew king Nabu-apla-iddina, subjugated the Aramean, Sutean and Chaldean tribes settled within Babylonia, and installed Marduk-zakir-shumi I (855\u2013819 BC) followed by Marduk-balassu-iqbi (819\u2013813 BC) as his vassals. It was during the late 850s BC, in the annals of Shalmaneser III, that the Chaldeans and Arabs dwelling in some northern regions of the Arabian Peninsula are first mentioned in the pages of written recorded history.\nUpon the death of Shalmaneser II, Baba-aha-iddina was reduced to vassalage by the Assyrian queen Shammuramat (known as Semiramis to the Persians, Armenians and Greeks), acting as regent to his successor Adad-nirari III who was merely a boy. Adad-nirari III eventually killed Baba-aha-iddina and ruled there directly until 800 BC until Ninurta-apla-X was crowned. However, he too was subjugated by Adad-Nirari III. The next Assyrian king, Shamshi-Adad V then made a vassal of Marduk-bel-zeri.\nBabylonia briefly fell to another foreign ruler when Marduk-apla-usur ascended the throne in 780 BC, taking advantage of a period of civil war in Assyria. He was a member of the Chaldean tribe who had a century or so earlier settled in a small region in the far southeastern corner of Mesopotamia, bordering the Persian Gulf and southwestern Elam. Shalmaneser IV attacked him and retook northern Babylonia, forcing a border treaty in Assyria's favour upon him. However, he was allowed to remain on the throne, and successfully stabilised the part of Babylonia he controlled. Eriba-Marduk, another Chaldean, succeeded him in 769 BC and his son, Nabu-shuma-ishkun in 761 BC. Babylonia appears to have been in a state of chaos during this time, with the north occupied by Assyria, its throne occupied by foreign Chaldeans, and civil unrest prominent throughout the land.\nThe Babylonian king Nabonassar overthrew the Chaldean usurpers in 748 BC, and successfully stabilised Babylonia, remaining untroubled by Ashur-nirari V of Assyria. However, with the accession of Tiglath-Pileser III (745\u2013727 BC) Babylonia came under renewed attack. Babylon was invaded and sacked and Nabonassar reduced to vassalage. His successors Nabu-nadin-zeri, Nabu-suma-ukin II and Nabu-mukin-zeri were also in servitude to Tiglath-Pileser III, until in 729 BC the Assyrian king decided to rule Babylon directly as its king instead of allowing Babylonian kings to remain as vassals of Assyria as his predecessors had done for two hundred years.\nIt was during this period that Eastern Aramaic was introduced by the Assyrians as the lingua franca of the Neo-Assyrian Empire, and the still spoken (by Assyrians and Mandeans) Mesopotamian Aramaic began to slowly overlay and supplant Akkadian as the spoken language of the general populace of both Assyria and Babylonia.\nThe Assyrian king Shalmaneser V was declared king of Babylon in 727 BC, but died whilst besieging Samaria in 722 BC.\nMarduk-apla-iddina II, a Chaldean malka (chieftain) of the far southeast of Mesopotamia, then fomented revolt against Assyrian domination, assisted by strong Elamite support. Marduk-apla-iddina managed to take the throne of Babylon itself between 721 and 710 BC whilst the Assyrian king Sargon II (722\u2013705 BC) were otherwise occupied in defeating the Scythians and Cimmerians who had attacked Assyria's Persian and Median vassal colonies in ancient Iran. Marduk-apla-iddina II was eventually defeated and ejected by Sargon II of Assyria, and fled to his protectors in Elam. Sargon II was then declared king in Babylon.\nDestruction of Babylon.\nSennacherib (705\u2013681 BC) succeeded Sargon II, and after ruling directly for a while, he placed his son Ashur-nadin-shumi on the throne. However, Merodach-Baladan and his Elamite protectors continued to unsuccessfully agitate against Assyrian rule. Nergal-ushezib, an Elamite, murdered the Assyrian prince and briefly took the throne. This led the infuriated Assyrian king Sennacherib to invade and subjugate Elam and to sack Babylon, laying waste to the region and largely destroying the city. While praying to the god Nisroch in Nineveh in 681 BC, Sennacherib was soon murdered by his own sons. The new Assyrian king Esarhaddon placed a puppet king Marduk-zakir-shumi II on the throne in Babylon. However, Marduk-apla-iddina returned from exile in Elam, and briefly deposed Marduk-zakir-shumi, whereupon Esarhaddon was forced to attack and defeat him. Marduk-apla-iddina once more fled to his masters in Elam, where he died in exile.\nRestoration and rebuilding.\nEsarhaddon (681\u2013669 BC) ruled Babylon personally, he completely rebuilt the city, bringing rejuvenation and peace to the region. Upon his death, and in an effort to maintain harmony within his vast empire (which stretched from the Caucasus to Egypt and Nubia and from Cyprus to Persia and the Caspian Sea), he installed his eldest son Shamash-shum-ukin as a subject king in Babylon, and his youngest, the highly educated Ashurbanipal (669\u2013627 BC), in the more senior position as king of Assyria and overlord of Shamash-shum-ukin.\nBabylonian revolt.\nDespite being an Assyrian himself, Shamash-shum-ukin, after decades subject to his brother Ashurbanipal, declared that the city of Babylon (and not the Assyrian city of Nineveh) should be the seat of the immense empire. He raised a major revolt against his brother, Ashurbanipal. He led a powerful coalition of peoples also resentful of Assyrian subjugation and rule, including Elam, the Persians, Medes, the Babylonians, Chaldeans and Suteans of southern Mesopotamia, the Arameans of the Levant and southwest Mesopotamia, the Arabs and Dilmunites of the Arabian Peninsula and the Canaanites-Phoenicians. After a bitter struggle Babylon was sacked and its allies vanquished, Shamash-shum-ukim being killed in the process. Elam was destroyed once and for all, and the Babylonians, Persians, Chaldeans, Arabs, Medes, Elamites, Arameans, Suteans and Canaanites were violently subjugated, with Assyrian troops exacting savage revenge on the rebelling peoples. An Assyrian governor named Kandalanu was placed on the throne to rule on behalf of the Assyrian king. Upon Ashurbanipal's death in 627 BC, his son Ashur-etil-ilani (627\u2013623 BC) became ruler of Babylon and Assyria.\nHowever, Assyria soon descended into a series of brutal internal civil wars which were to cause its downfall. Ashur-etil-ilani was deposed by one of his own generals, named Sin-shumu-lishir in 623 BC, who also set himself up as king in Babylon. After only one year on the throne amidst continual civil war, Sinsharishkun (622\u2013612 BC) ousted him as ruler of Assyria and Babylonia in 622 BC. However, he too was beset by constant unremitting civil war in the Assyrian heartland. Babylonia took advantage of this and rebelled under Nabopolassar, a previously unknown \"malka\" (chieftain) of the Chaldeans, who had settled in southeastern Mesopotamia by c. 850 BC.\nIt was during the reign of Sin-shar-ishkun that Assyria's vast empire began to unravel, and many of its former subject peoples ceased to pay tribute, most significantly for the Assyrians; the Babylonians, Chaldeans, Medes, Persians, Scythians, Arameans and Cimmerians.\nNeo-Babylonian Empire (Chaldean Empire).\nIn 620 BC Nabopolassar seized control over much of Babylonia with the support of most of the inhabitants, with only the city of Nippur and some northern regions showing any loyalty to the beleaguered Assyrian king. Nabopolassar was unable to utterly secure Babylonia, and for the next four years he was forced to contend with an occupying Assyrian army encamped in Babylonia trying to unseat him. However, the Assyrian king, Sin-shar-ishkun was plagued by constant revolts among his people in Nineveh, and was thus prevented from ejecting Nabopolassar.\nThe stalemate ended in 615 BC, when Nabopolassar entered the Babylonians and Chaldeans into alliance with Cyaxares, an erstwhile vassal of Assyria, and king of the Iranian peoples; the Medes, Persians, Sagartians and Parthians. Cyaxares had also taken advantage of the Assyrian destruction of the formerly regionally dominant pre-Iranian Elamite and Mannean nations and the subsequent anarchy in Assyria to free the Iranic peoples from three centuries of the Assyrian yoke and regional Elamite domination. The Scythians from north of the Caucasus, and the Cimmerians from the Black Sea who had both also been subjugated by Assyria, joined the alliance, as did regional Aramean tribes.\nIn 615 BC, while the Assyrian king was fully occupied fighting rebels in both Babylonia and Assyria itself, Cyaxares launched a surprise attack on the Assyrian heartlands, sacking the cities of Kalhu (the Biblical Calah, Nimrud) and Arrapkha (modern Kirkuk), Nabopolassar was still pinned down in southern Mesopotamia and thus not involved in this breakthrough.\nFrom this point on the coalition of Babylonians, Chaldeans, Medes, Persians, Scythians, Cimmerians and Sagartians fought in unison against a civil war ravaged Assyria. Major Assyrian cities such as Ashur, Arbela (modern Irbil), Guzana, Dur Sharrukin (modern Khorsabad), Imgur-Enlil, Nibarti-Ashur, Gasur, Kanesh, Kar Ashurnasipal and Tushhan fell to the alliance during 614 BC. Sin-shar-ishkun somehow managed to rally against the odds during 613 BC, and drove back the combined forces ranged against him.\nThe alliance launched a renewed combined attack the following year, and after five years of fierce fighting Nineveh was sacked in late 612 BC after a prolonged siege, in which Sin-shar-ishkun was killed defending his capital.\nHouse to house fighting continued in Nineveh, and an Assyrian general and member of the royal household, took the throne as Ashur-uballit II (612\u2013605 BC). He was offered the chance of accepting a position of vassalage by the leaders of the alliance according to the Babylonian Chronicle. He refused and managed to successfully fight his way out of Nineveh and to the northern Assyrian city of Harran in Upper Mesopotamia where he founded a new capital. The fighting continued, as the Assyrian king held out against the alliance until 607 BC, when he was eventually ejected by the Medes, Babylonians, Scythians and their allies, and prevented in an attempt to regain the city the same year.\nThe Egyptian Pharaoh Necho II, whose dynasty had been installed as vassals of Assyria in 671 BC, belatedly tried to aid Egypt's former Assyrian masters, possibly out of fear that Egypt would be next to succumb to the new powers without Assyria to protect them, having already been ravaged by the Scythians. The Assyrians fought on with Egyptian aid until what was probably a final decisive victory was achieved against them at Carchemish in northwestern Assyria in 605 BC. The seat of empire was thus transferred to Babylonia for the first time since Hammurabi over a thousand years before.\nNabopolassar was followed by his son Nebuchadnezzar II (605\u2013562 BC), whose reign of 43 years made Babylon once more the ruler of much of the civilized world, taking over portions of the former Assyrian Empire, with the eastern and northeastern portion being taken by the Medes and the far north by the Scythians.\nNebuchadnezzar II may have also had to contend with remnants of the Assyrian resistance. Some sections of the Assyrian army and administration may have still continued in and around Dur-Katlimmu in northwest Assyria for a time, however, by 599 BC Assyrian imperial records from this region also fell silent. The fate of Ashur-uballit II remains unknown, and he may have been killed attempting to regain Harran, at Carchemish, or continued to fight on, eventually disappearing into obscurity.\nThe Scythians and Cimmerians, erstwhile allies of Babylonia under Nabopolassar, now became a threat, and Nebuchadnezzar II was forced to march into Anatolia and rout their forces, ending the northern threat to his Empire.\nThe Egyptians attempted to remain in the Near East, possibly in an effort to aid in restoring Assyria as a secure buffer against Babylonia and the Medes and Persians, or to carve out an empire of their own. Nebuchadnezzar II campaigned against the Egyptians and drove them back over the Sinai. However, an attempt to take Egypt itself as his Assyrian predecessors had succeeded in doing failed, mainly due to a series of rebellions from the Israelites of Judah and the former kingdom of Ephraim, the Phoenicians of Caanan and the Arameans of the Levant. The Babylonian king crushed these rebellions, deposed Jehoiakim, the king of Judah, and deported a sizeable part of the population to Babylonia. Cities like Tyre, Sidon and Damascus were also subjugated. The Arabs and other South Arabian peoples who dwelt in the deserts to the south of the borders of Mesopotamia were then also subjugated.\nIn 567 BC he went to war with Pharaoh Amasis, and briefly invaded Egypt itself. After securing his empire, which included marrying a Median princess, he devoted himself to maintaining the empire and conducting numerous impressive building projects in Babylon. He is credited with building the fabled Hanging Gardens of Babylon.\nAmel-Marduk succeeded to the throne and reigned for only two years. Little contemporary record of his rule survives, though Berosus later stated that he was deposed and murdered in 560 BC by his successor Neriglissar for conducting himself in an \"improper manner\".\nNeriglissar (560\u2013556 BC) also had a short reign. He was the son in law of Nebuchadnezzar II, and it is unclear if he was a Chaldean or native Babylonian who married into the dynasty. He campaigned in Aram and Phoenicia, successfully maintaining Babylonian rule in these regions. Neriglissar died young however, and was succeeded by his son Labashi-Marduk (556 BC), who was still a boy. He was deposed and killed during the same year in a palace conspiracy.\nOf the reign of the last Babylonian king, Nabonidus (\"Nabu-na'id\", 556\u2013539 BC) who is the son of the Assyrian priestess Adda-Guppi and who managed to kill the last Chaldean king, Labashi-Marduk, and took the reign, there is a fair amount of information available. Nabonidus (hence his son, the regent Belshazzar) was, at least from the mother's side, neither Chaldean nor Babylonian, but ironically Assyrian, hailing from its final capital of Harran (Kharranu). His father's origins remain unknown. Information regarding Nabonidus is chiefly derived from a chronological tablet containing the annals of Nabonidus, supplemented by another inscription of Nabonidus where he recounts his restoration of the temple of the Moon-god Sin at Harran; as well as by a proclamation of Cyrus issued shortly after his formal recognition as king of Babylonia.\nA number of factors arose which would ultimately lead to the fall of Babylon. The population of Babylonia became restive and increasingly disaffected under Nabonidus. He excited a strong feeling against himself by attempting to centralize the polytheistic religion of Babylonia in the temple of Marduk at Babylon, and while he had thus alienated the local priesthoods, the military party also despised him on account of his antiquarian tastes. He seemed to have left the defense of his kingdom to his son Belshazzar (a capable soldier but poor diplomat who alienated the political elite), occupying himself with the more congenial work of excavating the foundation records of the temples and determining the dates of their builders. He also spent time outside Babylonia, rebuilding temples in the Assyrian city of Harran, and also among his Arab subjects in the deserts to the south of Mesopotamia. Nabonidus and Belshazzar's Assyrian heritage is also likely to have added to this resentment. In addition, Mesopotamian military might had usually been concentrated in the martial state of Assyria. Babylonia had always been more vulnerable to conquest and invasion than its northern neighbour, and without the might of Assyria to keep foreign powers in check and Mesopotamia dominant, Babylonia was exposed.\nIt was in the sixth year of Nabonidus (549 BC) that Cyrus the Great, the Achaemenid Persian \"king of Anshan\" in Elam, revolted against his suzerain Astyages, \"king of the Manda\" or Medes, at Ecbatana. Astyages' army betrayed him to his enemy, and Cyrus established himself at Ecbatana, thus putting an end to the empire of the Medes and making the Persian faction dominant among the Iranic peoples. Three years later Cyrus had become king of all Persia, and was engaged in a campaign to put down a revolt among the Assyrians. Meanwhile, Nabonidus had established a camp in the desert of his colony of Arabia, near the southern frontier of his kingdom, leaving his son Belshazzar (\"Belsharutsur\") in command of the army.\nIn 539 BC Cyrus invaded Babylonia. A battle was fought at Opis in the month of June, where the Babylonians were defeated; and immediately afterwards Sippar surrendered to the invader. Nabonidus fled to Babylon, where he was pursued by Gobryas, and on the 16th day of Tammuz, two days after the capture of Sippar, \"the soldiers of Cyrus entered Babylon without fighting\". Nabonidus was dragged from his hiding place, where the services continued without interruption. Cyrus did not arrive until the 3rd of \"Marchesvan\" (October), Gobryas having acted for him in his absence. Gobryas was now made governor of the province of Babylon, and a few days afterwards Belshazzar the son of Nabonidus died in battle. A public mourning followed, lasting six days, and Cyrus' son Cambyses accompanied the corpse to the tomb.\nOne of the first acts of Cyrus accordingly was to allow the Jewish exiles to return to their own homes, carrying with them their sacred temple vessels. The permission to do so was embodied in a proclamation, whereby the conqueror endeavored to justify his claim to the Babylonian throne.\nCyrus now claimed to be the legitimate successor of the ancient Babylonian kings and the avenger of Bel-Marduk, who was assumed to be wrathful at the impiety of Nabonidus in removing the images of the local gods from their ancestral shrines to his capital Babylon.\nThe Chaldean tribe had lost control of Babylonia decades before the end of the era that sometimes bears their name, and they appear to have blended into the general populace of Babylonia even before this (for example, Nabopolassar, Nebuchadnezzar II and their successors always referred to themselves as \"Shar Akkad\" and never as \"Shar Kaldu\" on inscriptions), and during the Persian Achaemenid Empire the term \"Chaldean\" ceased to refer to a race of people, and instead specifically to a social class of priests educated in classical Babylonian literature, particularly Astronomy and Astrology. By the mid Seleucid Empire (312\u2013150 BC) period this term too had fallen from use.\nFall of Babylon.\nBabylonia was absorbed into the Achaemenid Empire in 539 BC, becoming the satrapy of Babirush ().\nA year before Cyrus' death, in 529 BC, he elevated his son Cambyses II in the government, making him king of Babylon. He reserved for himself the fuller title of \"king of the (other) provinces\" of the empire. It was only when Darius I acquired the Persian throne and ruled it as a representative of the Zoroastrian religion that the old tradition was broken and the claim of Babylon to confer legitimacy on the rulers of West Asia ceased to be acknowledged.\nImmediately after Darius seized Persia, Babylonia briefly recovered its independence under a native ruler, Nidinta-Bel, who took the name of Nebuchadnezzar III, and reigned from October 522 BC to August 520 BC, when Darius took the city by storm. During this period Assyria to the north also rebelled. A few years later, probably 514 BC, Babylon again revolted under the Urartian king Nebuchadnezzar IV; on this occasion, after its capture by the Persians, the walls were partly destroyed. The Esagila, the great temple of Marduk, however, still continued to be kept in repair and to be a center of Babylonian religious feelings.\nAlexander the Great conquered Babylon in 333 BC for the Macedonians, and died there in 323 BC. Babylonia and Assyria then became part of the Greek Seleucid Empire. It has long been maintained that the foundation of Seleucia diverted the population to the new capital of Lower Mesopotamia and that the ruins of the old city became a quarry for the builders of the new seat of government, but the recent publication of the \"Babylonian Chronicles\" has shown that urban life was still very much the same well into the Parthian Empire (150 BC to 226 AD). The Parthian king Mithridates conquered the region into the Parthian Empire in 150 BC, and the region became something of a battleground between Greeks and Parthians.\nThere was a brief interlude of Roman conquest (the provinces of Assyria and Mesopotamia; 116\u2013118 AD) under Trajan, after which the Parthians reasserted control.\nThe satrapy of Babylonia was absorbed into As\u014drist\u0101n (Middle Persian for \"the land of Assyria\") in the Sasanian Empire, which began in 226 AD, and by this time East Syriac Rite Christianity, which emerged in Assyria and Upper Mesopotamia the first century, had become the dominant religion among the Assyrian people, who had never adopted the Zoroastrianism or Hellenistic religion or the languages of their rulers.\nApart from the small 2nd century BC to 3rd century AD independent Neo-Assyrian states of Adiabene, Osroene, Assur, Beth Garmai, Beth Nuhadra and Hatra in the north, Mesopotamia remained under largely Persian control until the Arab Muslim conquest of Persia in the seventh century AD. As\u014drist\u0101n was dissolved as a geopolitical entity in 637, and the native Eastern Aramaic-speaking and largely Christian populace of southern and central Mesopotamia (with the exception of the Mandeans) gradually underwent Arabization and Islamization, in contrast to northern Mesopotamia where an Assyrian continuity endures to the present day.\nCulture.\nBronze Age to Early Iron Age Mesopotamian culture is sometimes summarized as \"Assyro-Babylonian\", because of the close ethnic, linguistic and cultural interdependence of the two political centers. The term \"Babylonia\", especially in writings from around the early 20th century, was formerly used to also include Southern Mesopotamia's earliest \"pre-Babylonian\" history, and not only in reference to the later city-state of Babylon proper. This geographic usage of the name \"Babylonia\" has generally been replaced by the more accurate term \"Sumer\" or \"Sumero-Akkadian\" in more recent writing, referring to the pre-Assyro-Babylonian Mesopotamian civilization.\nBabylonian culture.\nArt and architecture.\nIn Babylonia, an abundance of clay, and lack of stone, led to greater use of mudbrick; Babylonian, Sumerian and Assyrian temples were massive structures of crude brick which were supported by buttresses, the rain being carried off by drains. One such drain at Ur was made of lead. The use of brick led to the early development of the pilaster and column, and of frescoes and enameled tiles. The walls were brilliantly coloured, and sometimes plated with zinc or gold, as well as with tiles. Painted terracotta cones for torches were also embedded in the plaster. In Babylonia, in place of the relief, there was greater use of three-dimensional figures\u2014the earliest examples being the Statues of Gudea, that are realistic if somewhat clumsy. The paucity of stone in Babylonia made every pebble precious, and led to a high perfection in the art of gem-cutting.\nAstronomy.\nTablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform script tablets known as the 'En\u016bma Anu Enlil'. The oldest significant astronomical text that we possess is Tablet 63 of 'En\u016bma Anu Enlil', the Venus tablet of Ammi-Saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The oldest rectangular astrolabe dates back to Babylonia c.\u20091100 BC. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.\nMedicine.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We find [medical semiotics] in a whole constellation of disciplines. ... There was a real common ground among these [Babylonian] forms of knowledge ... an approach involving analysis of particular cases, constructed only through traces, symptoms, hints. ... In short, we can speak about a symptomatic or divinatory [or conjectural] paradigm which could be oriented toward past present or future, depending on the form of knowledge called upon. Toward future ... that was the medical science of symptoms, with its double character, diagnostic, explaining past and present, and prognostic, suggesting likely future. ...\u2014\u200a\nThe oldest Babylonian (i.e., Akkadian) texts on medicine date back to the First Babylonian dynasty in the first half of the 2nd millennium BC although the earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur period. The most extensive Babylonian medical text, however, is the \"Diagnostic Handbook\" written by the \"umm\u00e2n\u016b\", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069\u20131046 BC).\nAlong with contemporary ancient Egyptian medicine, the Babylonians introduced the concepts of diagnosis, prognosis, physical examination, and prescriptions. In addition, the \"Diagnostic Handbook\" introduced the methods of therapy and aetiology and the use of empiricism, logic and rationality in diagnosis, prognosis and therapy. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.\nThe symptoms and diseases of a patient were treated through therapeutic means such as bandages, creams and pills. If a patient could not be cured physically, the Babylonian physicians often relied on exorcism to cleanse the patient from any curses. Esagil-kin-apli's \"Diagnostic Handbook\" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its aetiology and future development, and the chances of the patient's recovery.\nEsagil-kin-apli discovered a variety of illnesses and diseases and described their symptoms in his \"Diagnostic Handbook\". These include the symptoms for many varieties of epilepsy and related ailments along with their diagnosis and prognosis. Later Babylonian medicine resembles early Greek medicine in many ways. In particular, the early treatises of the Hippocratic Corpus show the influence of late Babylonian medicine in terms of both content and form.\nLiterature.\nThere were libraries and temples in most towns; an old Sumerian proverb averred that \"he who would excel in the school of the scribes must rise with the dawn\". Women as well as men learned to read and write, and in Semitic times, this involved knowledge of the extinct Sumerian language, and a complicated and extensive syllabary.\nA considerable amount of Babylonian literature was translated from Sumerian originals, and the language of religion and law long continued to be written in the old agglutinative language of Sumer. Vocabularies, grammars, and interlinear translations were compiled for the use of students, as well as commentaries on the older texts and explanations of obscure words and phrases. The characters of the syllabary were all arranged and named, and elaborate lists of them were drawn up.\nThere are many Babylonian literary works whose titles have come down to us. One of the most famous of these was the Epic of Gilgamesh, in twelve books, translated from the original Sumerian by a certain Sin-liqi-unninni, and arranged upon an astronomical principle. Each division contains the story of a single adventure in the career of Gilgamesh. The whole story is a composite product, and it is probable that some of the stories are artificially attached to the central figure.\nNeo-Babylonian culture.\nThe brief resurgence of Babylonian culture in the 7th to 6th centuries BC was accompanied by a number of important cultural developments.\nAstronomy.\nAmong the sciences, astronomy and astrology still occupied a conspicuous place in Babylonian society. Astronomy was of old standing in Babylonia. The zodiac was a Babylonian invention of great antiquity; and eclipses of the sun and moon could be foretold. There are dozens of cuneiform records of original Mesopotamian eclipse observations.\nBabylonian astronomy was the basis for much of what was done in ancient Greek astronomy, in classical, in Sasanian, Byzantine and Syrian astronomy, astronomy in the medieval Islamic world, and in Central Asian and Western European astronomy. Neo-Babylonian astronomy can thus be considered the direct predecessor of much of ancient Greek mathematics and astronomy, which in turn is the historical predecessor of the European (Western) Scientific Revolution.\nDuring the 8th and 7th centuries BC, Babylonian astronomers developed a new approach to astronomy. They began studying philosophy dealing with the ideal nature of the early universe and began employing an internal logic within their predictive planetary systems. This was an important contribution to astronomy and the philosophy of science and some scholars have thus referred to this new approach as the first scientific revolution. This new approach to astronomy was adopted and further developed in Greek and Hellenistic astronomy.\nIn Seleucid and Parthian times, the astronomical reports were of a thoroughly scientific character; how much earlier their advanced knowledge and methods were developed is uncertain. The Babylonian development of methods for predicting the motions of the planets is considered to be a major episode in the history of astronomy.\nThe only Babylonian astronomer known to have supported a heliocentric model of planetary motion was Seleucus of Seleucia (b. 190 BC). Seleucus is known from the writings of Plutarch. He supported the heliocentric theory where the Earth rotated around its own axis which in turn revolved around the Sun. According to Plutarch, Seleucus even proved the heliocentric system, but it is not known what arguments he used.\nMathematics.\nBabylonian mathematical texts are plentiful and well edited. In respect of time they fall in two distinct groups: one from the First Babylonian dynasty period (1830\u20131531 BC), the other mainly Seleucid from the last three or four centuries BC. In respect of content there is scarcely any difference between the two groups of texts. Thus Babylonian mathematics remained stale in character and content, with very little progress or innovation, for nearly two millennia.\nThe Babylonian system of mathematics was sexagesimal, or a base 60 numeral system. From this we derive the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 \u00d7 6) degrees in a circle. The Babylonians were able to make great advances in mathematics for two reasons. First, the number 60 has many divisors (2, 3, 4, 5, 6, 10, 12, 15, 20, and 30), making calculations easier. Additionally, unlike the Egyptians and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values (much as in our base-ten system: 734 = 7\u00d7100 + 3\u00d710 + 4\u00d71). Among the Babylonians' mathematical accomplishments were the determination of the square root of two correctly to seven places (YBC 7289). They also demonstrated knowledge of the Pythagorean theorem well before Pythagoras.\nThe \"ner\" of 600 and the \"sar\" of 3600 were formed from the unit of 60, corresponding with a degree of the equator. Tablets of squares and cubes, calculated from 1 to 60, have been found at Senkera, and a people acquainted with the sun-dial, the clepsydra, the lever and the pulley, must have had no mean knowledge of mechanics. A crystal lens, turned on the lathe, was discovered by Austen Henry Layard at Nimrud along with glass vases bearing the name of Sargon; this could explain the excessive minuteness of some of the writing on the Assyrian tablets, and a lens may also have been used in the observation of the heavens.\nThe Babylonians might have been familiar with the general rules for measuring area. They are also known for the Babylonian mile, which was a measure of distance equal to about 11 kilometres (7\u00a0mi) today. This measurement for distances eventually was converted to a time-mile used for measuring the travel of the Sun, therefore, representing time. (Eves, Chapter 2) The Babylonians used also space time graphs to calculate the velocity of Jupiter. This is an idea that is considered highly modern, traced to the 14th century England and France and anticipating integral calculus.\nPhilosophy.\nThe origins of Babylonian philosophy can be traced back to early Mesopotamian wisdom literature, which embodied certain philosophies of life, particularly ethics, in the forms of dialectic, dialogs, epic poetry, folklore, hymns, lyrics, prose, and proverbs. Babylonian reasoning and rationality developed beyond empirical observation.\nIt is possible that Babylonian philosophy had an influence on Greek philosophy, particularly Hellenistic philosophy. The Babylonian text \"Dialogue of Pessimism\" contains similarities to the Agnostic thought of the sophists, the Heraclitean doctrine of contrasts, and the dialogs of Plato, as well as a precursor to the maieutic Socratic method of Socrates. The Milesian philosopher Thales is also known to have studied philosophy in Mesopotamia.\nAccording to the assyriologist Marc Van de Mieroop, Babylonian philosophy was a highly developed system of thought with a unique approach to knowledge and a focus on writing, lexicography, divination, and law. It was also a bilingual intellectual culture, based on Sumerian and Akkadian.\nLegacy.\nBabylonia, and particularly its capital city Babylon, has long held a place in the Abrahamic religions as a symbol of excess and dissolute power. Many references are made to Babylon in the Bible, both literally (historical) and allegorically. The mentions in the Tanakh tend to be historical or prophetic, while New Testament apocalyptic references to the Whore of Babylon are more likely figurative, or cryptic references possibly to pagan Rome, or some other archetype. The legendary Hanging Gardens of Babylon and the Tower of Babel are seen as symbols of luxurious and arrogant power respectively.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nstyle=\"width:100%\""}
{"id": "46884", "revid": "51035645", "url": "https://en.wikipedia.org/wiki?curid=46884", "title": "Internment of Japanese Americans", "text": "Mass incarceration in the U.S. during WWII\nDuring World War II, the United States forcibly relocated and incarcerated about 120,000 people of Japanese descent in ten concentration camps operated by the War Relocation Authority (WRA), mostly in the western interior of the country. About two-thirds were U.S. citizens. These actions were initiated by Executive Order 9066, issued by President Franklin D. Roosevelt on February 19, 1942, following Imperial Japan's attack on Pearl Harbor on December 7, 1941. About 127,000 Japanese Americans then lived in the continental U.S., of which about 112,000 lived on the West Coast. About 80,000 were \"Nisei\" ('second generation'; American-born Japanese with U.S. citizenship) and \"Sansei\" ('third generation', the children of \"Nisei\"). The rest were \"Issei\" ('first generation') immigrants born in Japan, who were ineligible for citizenship. In Hawaii, where more than 150,000 Japanese Americans comprised more than one-third of the territory's population, only 1,200 to 1,800 were incarcerated.\nInternment was intended to mitigate a security risk which Japanese Americans were believed to pose. The scale of the incarceration in proportion to the size of the Japanese American population far surpassed similar measures undertaken against German and Italian Americans who numbered in the millions and of whom some thousands were interned, most of these non-citizens. Following the executive order, the entire West Coast was designated a military exclusion area, and all Japanese Americans living there were taken to assembly centers before being sent to concentration camps in California, Arizona, Wyoming, Colorado, Utah, Idaho, and Arkansas. Similar actions were taken against individuals of Japanese descent in Canada. Internees were prohibited from taking more than they could carry into the camps, and many were forced to sell some or all of their property, including their homes and businesses. At the camps, which were surrounded by barbed wire fences and patrolled by armed guards, internees often lived in overcrowded barracks with minimal furnishing.\nIn its 1944 decision \"Korematsu v. United States\", the U.S. Supreme Court upheld the constitutionality of the removals under the Due Process Clause of the Fifth Amendment to the United States Constitution. The Court limited its decision to the validity of the exclusion orders, avoiding the issue of the incarceration of U.S. citizens without due process, but ruled on the same day in \"Ex parte Endo\" that a loyal citizen could not be detained, which began their release. On December 17, 1944, the exclusion orders were rescinded, and nine of the ten camps were shut down by the end of 1945. Japanese Americans were initially barred from U.S. military service, but by 1943, they were allowed to join, with 20,000 serving during the war. Over 4,000 students were allowed to leave the camps to attend college. Hospitals in the camps recorded 5,981 births and 1,862 deaths during incarceration.\nIn the 1970s, under mounting pressure from the Japanese American Citizens League (JACL) and redress organizations, President Jimmy Carter appointed the Commission on Wartime Relocation and Internment of Civilians (CWRIC) to investigate whether the internment had been justified. In 1983, the commission's report, \"Personal Justice Denied,\" found little evidence of Japanese disloyalty and concluded that internment had been the product of racism. It recommended that the government pay reparations to the detainees. In 1988, President Ronald Reagan signed the Civil Liberties Act of 1988, which officially apologized and authorized a payment of $20,000 () to each former detainee who was still alive when the act was passed. The legislation admitted that the government's actions were based on \"race prejudice, war hysteria, and a failure of political leadership.\" By 1992, the U.S. government eventually disbursed more than $1.6\u00a0billion (equivalent to $\u00a0billion in 2024) in reparations to 82,219 Japanese Americans who had been incarcerated.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nBackground.\nPrior use of internment camps in the United States.\nThe United States Government had previously employed civilian internment policies in a variety of circumstances. During the 1830s, civilians of the indigenous Cherokee nation were evicted from their homes and detained in \"emigration depots\" in Alabama and Tennessee prior to the deportation to Oklahoma following the passage of the Indian Removal Act in 1830. Similar internment policies were carried out by U.S. territorial authorities against the Dakota and Navajo peoples during the American Indian Wars in the 1860s. In 1901, during the Philippine\u2013American War, General J. Franklin Bell ordered the detainment of Filipino civilians in the provinces of Batangas and Laguna into U.S. Army-run concentration camps in order to prevent them from collaborating with Filipino General Miguel Malvar's guerrillas; over 11,000 people died in the camps from malnutrition and disease.\nAfter the United States entered World War I in 1917, roughly 6,300 German-born residents of the United States were arrested, with 2,048 of those residents being incarcerated at two U.S. Army bases, where they remained interned until 1920. However, these policies only targeted a small fraction of German-born Americans and did not apply to German-American U.S. citizens.\nJapanese Americans before World War II.\nDue in large part to socio-political changes which stemmed from the Meiji Restoration\u2014and a recession which was caused by the abrupt opening of Japan's economy to the world economy\u2014people emigrated from the Empire of Japan after 1868 in search of employment. From 1869 to 1924, approximately 200,000 Japanese immigrated to the islands of Hawaii, mostly laborers expecting to work on the islands' sugar plantations. Some 180,000 went to the U.S. mainland, with the majority of them settling on the West Coast and establishing farms or small businesses. Most arrived before 1908, when the Gentlemen's Agreement between Japan and the United States banned the immigration of unskilled laborers. A loophole allowed the wives of men who were already living in the US to join their husbands. The practice of women marrying by proxy and immigrating to the U.S. resulted in a large increase in the number of \"picture brides.\"\nAs the Japanese American population continued to grow, European Americans who lived on the West Coast resisted the arrival of this ethnic group, fearing competition, and making the exaggerated claim that hordes of Asians would take over white-owned farmland and businesses. Groups such as the Asiatic Exclusion League, the California Joint Immigration Committee, and the Native Sons of the Golden West organized in response to the rise of this \"Yellow Peril.\" They successfully lobbied to restrict the property and citizenship rights of Japanese immigrants, just as similar groups had previously organized against Chinese immigrants. Beginning in the late 19th century, several laws and treaties which attempted to slow immigration from Japan were introduced. The Immigration Act of 1924, which followed the example of the 1882 Chinese Exclusion Act, effectively banned all immigration from Japan and other \"undesirable\" Asian countries.\nThe 1924 ban on immigration produced unusually well-defined generational groups within the Japanese American community. The \"Issei\" were exclusively those Japanese who had immigrated before 1924; some of them desired to return to their homeland. Because no more immigrants were permitted, all Japanese Americans who were born after 1924 were, by definition, born in the U.S. and by law, they were automatically considered U.S. citizens. The members of this \"Nisei\" generation constituted a cohort which was distinct from the cohort which their parents belonged to. In addition to the usual generational differences, Issei men were typically ten to fifteen years older than their wives, making them significantly older than the younger children in their often large families. U.S. law prohibited Japanese immigrants from becoming naturalized citizens, making them dependent on their children whenever they rented or purchased property. Communication between English-speaking children and parents who mostly or completely spoke in Japanese was often difficult. A significant number of older Nisei, many of whom were born prior to the immigration ban, had married and already started families of their own by the time the US entered World War II.\nDespite racist legislation which prevented Issei from becoming naturalized citizens (or owning property, voting, or running for political office), these Japanese immigrants established communities in their new hometowns. Japanese Americans contributed to the agriculture of California and other Western states, by introducing irrigation methods which enabled them to cultivate fruits, vegetables, and flowers on previously inhospitable land.\nIn both rural and urban areas, \"kenjinkai,\" community groups for immigrants from the same Japanese prefecture, and \"fujinkai,\" Buddhist women's associations, organized community events and did charitable work, provided loans and financial assistance and built Japanese language schools for their children. Excluded from setting up shop in white neighborhoods, nikkei-owned small businesses thrived in the \"Nihonmachi,\" or Japantowns of urban centers, such as Los Angeles, San Francisco, and Seattle.\nIn the 1930s, the Office of Naval Intelligence (ONI), concerned as a result of Imperial Japan's rising military power in Asia, began to conduct surveillance in Japanese American communities in Hawaii. Starting in 1936, at the behest of President Roosevelt, the ONI began to compile a \"special list of those Japanese Americans who would be the first to be placed in a concentration camp in the event of trouble\" between Japan and the United States. In 1939, again by order of the President, the ONI, Military Intelligence Division, and FBI began working together to compile a larger Custodial Detention Index. Early in 1941, Roosevelt commissioned Curtis Munson to conduct an investigation on Japanese Americans living on the West Coast and in Hawaii. After working with FBI and ONI officials and interviewing Japanese Americans and those familiar with them, Munson determined that the \"Japanese problem\" was nonexistent. His final report to the President, submitted November 7, 1941, \"certified a remarkable, even extraordinary degree of loyalty among this generally suspect ethnic group.\" A subsequent report by Kenneth Ringle (ONI), delivered to the President in January 1942, also found little evidence to support claims of Japanese American disloyalty and argued against mass incarceration.\nRoosevelt's racial attitudes toward Japanese Americans.\nDuring the 1920s, Roosevelt wrote articles in the Macon Telegraph opposing white-Japanese intermarriage for fostering \"the mingling of Asiatic blood with European or American blood\" and praising California's ban on land ownership by the first-generation Japanese. In 1936, while president, he wrote privately, with regards to contacts between Japanese sailors and the local Japanese American population in the event of war, that \"every Japanese citizen or non-citizen on the Island of Oahu who meets these Japanese ships or has any connection with their officers or men should be secretly but definitely identified and his or her name placed on a special list of those who would be the first to be placed in a concentration camp.\"\nAfter Pearl Harbor.\nIn the weeks immediately following the attack on Pearl Harbor the president disregarded the advice of advisors, notably John Franklin Carter, who urged him to speak out in defense of the rights of Japanese Americans.\nThe surprise attack on Pearl Harbor on December 7, 1941, led military and political leaders to suspect that Imperial Japan was preparing a full-scale invasion of the United States West Coast. Due to Japan's rapid military conquest of a large portion of Asia and the Pacific including a small portion of the U.S. West Coast (i.e., Aleutian Islands Campaign) between 1937 and 1942, some Americans feared that its military forces were unstoppable.\nAmerican public opinion initially stood by the large population of Japanese Americans living on the West Coast, with the \"Los Angeles Times\" characterizing them as \"good Americans, born and educated as such.\" Many Americans believed that their loyalty to the United States was unquestionable. Though some in the administration (including Attorney General Francis Biddle and FBI Director J. Edgar Hoover) dismissed all rumors of Japanese American espionage on behalf of the Japanese war effort, pressure mounted upon the administration as the tide of public opinion turned against Japanese Americans.\nA survey of the Office of Facts and Figures on February 4 (two weeks prior to the president's order) reported that a majority of Americans expressed satisfaction with existing governmental controls on Japanese Americans. Moreover, in his autobiography in 1962, Biddle, who opposed incarceration, downplayed the influence of public opinion in prompting the president's decision. He even considered it doubtful \"whether, political and special group press aside, public opinion even on the West Coast supported evacuation.\" Support for harsher measures toward Japanese Americans increased over time, however, in part since Roosevelt did little to use his office to calm attitudes. According to a March 1942 poll conducted by the American Institute of Public Opinion, after incarceration was becoming inevitable, 93% of Americans supported the relocation of Japanese non-citizens from the Pacific Coast while only 1% opposed it. According to the same poll, 59% supported the relocation of Japanese people who were born in the country and were United States citizens, while 25% opposed it.\nThe incarceration and imprisonment measures taken against Japanese Americans after the attack falls into a broader trend of anti-Japanese attitudes on the West Coast of the United States. To this end, preparations had already been made in the collection of names of Japanese American individuals and organizations, along with other foreign nationals such as Germans and Italians, that were to be removed from society in the event of a conflict. The December 7th attack on Pearl Harbor, bringing the United States into the Second World War, enabled the implementation of the dedicated government policy of incarceration, with the action and methodology having been extensively prepared before war broke out despite multiple reports that had been consulted by Roosevelt expressing the notion that Japanese Americans posed little threat.\nAdditionally, the forced removal and incarceration of Japanese Americans during World War II led to severe economic consequences. Numerous Japanese Americans had to leave their homes, businesses, and possessions since they were relocated to the internment camps. This also led to the collapse of many family-owned businesses, real estate, and their savings since they had been escorted to the camps. \"Camp residents lost some $400 million in property during their incarceration. Congress provided $38 million in reparations in 1948 and, forty years later, paid an additional $20,000 to each surviving individual who had been detained in the camps\". Additionally, Japanese American farmers suffered greatly due to their forced relocation. In 1942, the managing secretary of the Western Growers Protective Association reported that the removal of Japanese Americans led to significant profits for growers and shippers. These losses were tragic, which ended up affecting a multitude of Japanese Americans and resulted in many losses of properties, businesses, and more. This also resulted in limited compensation, and far less than what they had originally lost. The economic outcomes of the Japanese imprisonments were disastrous and serve as a reminder of the lasting cost of cultural discrimination.\nNiihau incident.\nAlthough the impact on US authorities is controversial, the Niihau incident immediately followed the attack on Pearl Harbor, when Ishimatsu Shintani, an Issei, and Yoshio Harada, a Nisei, and his Issei wife Irene Harada on the island of Ni'ihau violently freed a downed and captured Japanese naval airman, attacking their fellow Ni'ihau islanders in the process.\nRoberts Commission.\nSeveral concerns over the loyalty of ethnic Japanese seemed to stem from racial prejudice rather than any evidence of malfeasance. The Roberts Commission report, which investigated the Pearl Harbor attack, was released on January 25 and accused persons of Japanese ancestry of espionage leading up to the attack. Although the report's key finding was that General Walter Short and Admiral Husband E. Kimmel had been derelict in their duties during the attack on Pearl Harbor, one passage made vague reference to \"Japanese consular agents and other... persons having no open relations with the Japanese foreign service\" transmitting information to Japan. It was unlikely that these \"spies\" were Japanese American, as Japanese intelligence agents were distrustful of their American counterparts and preferred to recruit \"white persons and Negroes.\" However, despite the fact that the report made no mention of Americans of Japanese ancestry, national and West Coast media nevertheless used the report to vilify Japanese Americans and inflame public opinion against them.\nQuestioning loyalty.\nMajor Karl Bendetsen and Lieutenant General John L. DeWitt, head of the Western Defense Command, questioned Japanese American loyalty. DeWitt said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nHe further stated in a conversation with California's governor, Culbert L. Olson:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There's a tremendous volume of public opinion now developing against the Japanese of all classes, that is aliens and non-aliens, to get them off the land, and in Southern California around Los Angeles\u2014in that area too\u2014they want and they are bringing pressure on the government to move all the Japanese out. As a matter of fact, it's not being instigated or developed by people who are not thinking but by the best people of California. Since the publication of the Roberts Report they feel that they are living in the midst of a lot of enemies. They don't trust the Japanese, none of them.\n\"A Jap's a Jap\".\nDeWitt, who administered the incarceration program, repeatedly told newspapers that \"A Jap's a Jap\" and testified to Congress:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I don't want any of them [persons of Japanese ancestry] here. They are a dangerous element. There is no way to determine their loyalty... It makes no difference whether he is an American citizen, he is still a Japanese. American citizenship does not necessarily determine loyalty... But we must worry about the Japanese all the time until he is wiped off the map.\nDeWitt also sought approval to conduct search and seizure operations which were aimed at preventing alien Japanese from making radio transmissions to Japanese ships. The Justice Department declined, stating that there was no probable cause to support DeWitt's assertion, as the FBI concluded that there was no security threat. On January 2, the Joint Immigration Committee of the California Legislature sent a manifesto to California newspapers which attacked \"the ethnic Japanese,\" who it alleged were \"totally unassimilable.\" This manifesto further argued that all people of Japanese heritage were loyal subjects of the Emperor of Japan; the manifesto contended that Japanese language schools were bastions of racism which advanced doctrines of Japanese racial superiority.\nThe manifesto was backed by the Native Sons and Daughters of the Golden West and the California Department of the American Legion, which in January demanded that all Japanese with dual citizenship be placed in concentration camps. By February, Earl Warren, the Attorney General of California (and a future Chief Justice of the United States), had begun his efforts to persuade the federal government to remove all people of Japanese ethnicity from the West Coast.\nThose who were as little as &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204416 Japanese were placed in incarceration camps. Bendetsen, promoted to colonel, said in 1942, \"I am determined that if they have one drop of Japanese blood in them, they must go to camp.\"\nPresidential Proclamations.\nUpon the bombing of Pearl Harbor and pursuant to the Alien Enemies Act, Presidential Proclamations 2525, 2526 and 2527 were issued designating Japanese, German and Italian nationals as enemy aliens. Information gathered by US officials over the previous decade was used to locate and incarcerate thousands of Japanese American community leaders in the days immediately following Pearl Harbor (see section elsewhere in this article \"Other concentration camps\"). In Hawaii, under the auspices of martial law, both \"enemy aliens\" and citizens of Japanese and \"German\" descent were arrested and interned (incarcerated if they were a US citizen).\nPresidential Proclamation 2537 (codified at https://) was issued on January 14, 1942, requiring \"alien enemies\" to obtain a certificate of identification and carry it \"at all times\". Enemy aliens were not allowed to enter restricted areas. Violators of these regulations were subject to \"arrest, detention and incarceration for the duration of the war.\"\nOn February 13, the Pacific Coast Congressional subcommittee on aliens and sabotage recommended to the President immediate evacuation of \"all persons of Japanese lineage and all others, aliens and citizens alike\" who were thought to be dangerous from \"strategic areas,\" further specifying that these included the entire \"strategic area\" of California, Oregon, Washington, and Alaska. On February 16 the President tasked Secretary of War Henry L. Stimson with replying. A conference on February 17 of Secretary Stimson with assistant secretary John J. McCloy, Provost Marshal General Allen W. Gullion, Deputy chief of Army Ground Forces Mark W. Clark, and Colonel Bendetsen decided that General DeWitt should be directed to commence evacuations \"to the extent he deemed necessary\" to protect vital installations. Throughout the war, interned Japanese Americans protested against their treatment and insisted that they be recognized as loyal Americans. Many sought to demonstrate their patriotism by trying to enlist in the armed forces. Although early in the war Japanese Americans were barred from military service, by 1943 the army had begun actively recruiting Nisei to join new all-Japanese American units.\nDevelopment.\nExecutive Order 9066 and related actions.\nExecutive Order 9066, signed by Roosevelt on February 19, 1942, authorized military commanders to designate \"military areas\" at their discretion, \"from which any or all persons may be excluded.\" These \"exclusion zones,\" unlike the \"alien enemy\" roundups, were applicable to anyone that an authorized military commander might choose, whether citizen or non-citizen. Eventually such zones would include parts of both the East and West Coasts, totaling about 1/3 of the country by area. Unlike the subsequent deportation and incarceration programs that would come to be applied to large numbers of Japanese Americans, detentions and restrictions directly under this Individual Exclusion Program were placed primarily on individuals of German or Italian ancestry, including American citizens. The order allowed regional military commanders to designate \"military areas\" from which \"any or all persons may be excluded.\" Although the executive order did not mention Japanese Americans, this authority was used to declare that all people of Japanese ancestry were required to leave Alaska and the military exclusion zones from all of California and parts of Oregon, Washington, and Arizona, with the exception of those inmates who were being held in government camps. The detainees were not only people of Japanese ancestry, they also included a relatively small number\u2014though still totaling well over ten thousand\u2014of people of German and Italian ancestry as well as Germans who were expelled from Latin America and deported to the U.S. Approximately 5,000 Japanese Americans relocated outside the exclusion zone before March 1942, while some 5,500 community leaders had been arrested immediately after the Pearl Harbor attack and thus were already in custody.\nOn March 2, 1942, General John DeWitt, commanding general of the Western Defense Command, publicly announced the creation of two military restricted zones. Military Area No. 1 consisted of the southern half of Arizona and the western half of California, Oregon, and Washington, as well as all of California south of Los Angeles. Military Area No. 2 covered the rest of those states. DeWitt's proclamation informed Japanese Americans they would be required to leave Military Area 1, but stated that they could remain in the second restricted zone. Removal from Military Area No. 1 initially occurred through \"voluntary evacuation.\" Japanese Americans were free to go anywhere outside of the exclusion zone or inside Area 2, with arrangements and costs of relocation to be borne by the individuals. The policy was short-lived; DeWitt issued another proclamation on March 27 that prohibited Japanese Americans from leaving Area 1. A night-time curfew, also initiated on March 27, 1942, placed further restrictions on the movements and daily lives of Japanese Americans.\nIncluded in the forced removal was Alaska, which, like Hawaii, was an incorporated U.S. territory located in the northwest extremity of the continental United States. Unlike the contiguous West Coast, Alaska was not subject to any exclusion zones due to its small Japanese population. Nevertheless, the Western Defense Command announced in April 1942 that all Japanese people and Americans of Japanese ancestry were to leave the territory for incarceration camps inland. By the end of the month, over 200 Japanese residents regardless of citizenship were exiled from Alaska, most of them ended up at the Minidoka War Relocation Center in Southern Idaho.\nEviction from the West Coast began on March 24, 1942, with Civilian Exclusion Order No. 1, which gave the 227 Japanese American residents of Bainbridge Island, Washington six days to prepare for their \"evacuation\" directly to Manzanar. Colorado governor Ralph Lawrence Carr was the only elected official to publicly denounce the incarceration of American citizens (an act that cost his reelection, but gained him the gratitude of the Japanese American community, such that a statue of him was erected in the Denver Japantown's Sakura Square). A total of 108 exclusion orders issued by the Western Defense Command over the next five months completed the removal of Japanese Americans from the West Coast in August 1942.\nIn addition to imprisoning those of Japanese descent in the US, the US also interned people of Japanese (and German and Italian) descent deported from Latin America. Thirteen Latin American countries\u2014Bolivia, Colombia, Costa Rica, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Mexico, Nicaragua, Panama, and Peru\u2014cooperated with the US by apprehending, detaining and deporting to the US 2,264 Japanese Latin American citizens and permanent residents of Japanese ancestry.\nSupport and opposition.\nNon-military advocates of exclusion, removal, and detention.\nThe deportation and incarceration of Japanese Americans was popular among many white farmers who resented the Japanese American farmers. \"White American farmers admitted that their self-interest required the removal of the Japanese.\" These individuals saw incarceration as a convenient means of uprooting their Japanese American competitors. Austin E. Anson, managing secretary of the Salinas Vegetable Grower-Shipper Association, told \"The Saturday Evening Post\" in 1942:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nWe're charged with wanting to get rid of the Japs for selfish reasons. We do. It's a question of whether the White man lives on the Pacific Coast or the brown men. They came into this valley to work, and they stayed to take over... If all the Japs were removed tomorrow, we'd never miss them in two weeks because the White farmers can take over and produce everything the Jap grows. And we do not want them back when the war ends, either.\nThe Leadership of the Japanese American Citizens League did not question the constitutionality of the exclusion of Japanese Americans from the West Coast. Instead, arguing it would better serve the community to follow government orders without protest, the organization advised the approximately 120,000 affected to go peacefully.\nThe Roberts Commission Report, prepared at President Franklin D. Roosevelt's request, has been cited as an example of the fear and prejudice informing the thinking behind the incarceration program. The Report sought to link Japanese Americans with espionage activity, and to associate them with the bombing of Pearl Harbor. Columnist Henry McLemore, who wrote for the Hearst newspapers, reflected the growing public sentiment that was fueled by this report:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I am for the immediate removal of every Japanese on the West Coast to a point deep in the interior. I don't mean a nice part of the interior either. Herd 'em up, pack 'em off, and give 'em the inside room in the badlands... Personally, I hate the Japanese. And that goes for all of them.\nOther California newspapers also embraced this view. According to a \"Los Angeles Times\" editorial,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A viper is nonetheless a viper wherever the egg is hatched... So, a Japanese American born of Japanese parents, nurtured upon Japanese traditions, living in a transplanted Japanese atmosphere...notwithstanding his nominal brand of accidental citizenship almost inevitably and with the rarest exceptions grows up to be a Japanese, and not an American... Thus, while it might cause injustice to a few to treat them all as potential enemies, I cannot escape the conclusion...that such treatment...should be accorded to each and all of them while we are at war with their race.\nU.S. Representative Leland Ford (R-CA) of Los Angeles joined the bandwagon, who demanded that \"all Japanese, whether citizens or not, be placed in [inland] concentration camps.\"\nIncarceration of Japanese Americans, who provided critical agricultural labor on the West Coast, created a labor shortage which was exacerbated by the induction of many white American laborers into the Armed Forces. This vacuum precipitated a mass immigration of Mexican workers into the United States to fill these jobs, under the banner of what became known as the Bracero Program. Many Japanese detainees were temporarily released from their camps \u2013 for instance, to harvest Western beet crops \u2013 to address this wartime labor shortage.\nNon-military advocates who opposed exclusion, removal, and detention.\nLike many white American farmers, the white businessmen of Hawaii had their own motives for determining how to deal with the Japanese Americans, but they opposed their incarceration. Instead, these individuals gained the passage of legislation which enabled them to retain the freedom of the nearly 150,000 Japanese Americans who would have otherwise been sent to concentration camps which were located in Hawaii. As a result, only 1,200 to 1,800 Japanese Americans in Hawaii were incarcerated.\nThe powerful businessmen of Hawaii concluded that the imprisonment of such a large proportion of the islands' population would adversely affect the economic prosperity of the territory. The Japanese represented \"over 90 percent of the carpenters, nearly all of the transportation workers, and a significant portion of the agricultural laborers\" on the islands. General Delos Carleton Emmons, the military governor of Hawaii, also argued that Japanese labor was \"'absolutely essential' for rebuilding the defenses destroyed at Pearl Harbor.\" Recognizing the Japanese American community's contribution to the affluence of the Hawaiian economy, General Emmons fought against the incarceration of the Japanese Americans and had the support of most of the businessmen of Hawaii. By comparison, Idaho governor Chase A. Clark, in a Lions Club speech on May 22, 1942, said \"Japs live like rats, breed like rats and act like rats. We don't want them ... permanently located in our state.\"\nInitially, Oregon's governor Charles A. Sprague opposed the incarceration, and as a result, he decided not to enforce it in the state and he also discouraged residents from harassing their fellow citizens, the Nisei. He turned against the Japanese by mid-February 1942, days before the executive order was issued, but he later regretted this decision and he attempted to atone for it for the rest of his life.\nEven though the incarceration was a generally popular policy in California, it was not universally supported. R.C. Hoiles, publisher of the \"Orange County Register\", argued during the war that the incarceration was unethical and unconstitutional:\nIt would seem that convicting people of disloyalty to our country without having specific evidence against them is too foreign to our way of life and too close akin to the kind of government we are fighting... We must realize, as Henry Emerson Fosdick so wisely said, 'Liberty is always dangerous, but it is the safest thing we have.'\nMembers of some Christian religious groups (such as Presbyterians), particularly those who had formerly sent missionaries to Japan, were among opponents of the incarceration policy. Some Baptist and Methodist churches, among others, also organized relief efforts to the camps, supplying inmates with supplies and information.\nStatement of military necessity as a justification of incarceration.\nNiihau Incident.\nThe Niihau Incident occurred in December 1941, just after the Imperial Japanese Navy's attack on Pearl Harbor. The Imperial Japanese Navy had designated the Hawaiian island of Niihau as an uninhabited island for damaged aircraft to land and await rescue. Three Japanese Americans on Niihau assisted a Japanese pilot, Shigenori Nishikaichi, who crashed there. Despite the incident, the Territorial Governor of Hawaii Joseph Poindexter rejected calls for the mass incarceration of the Japanese Americans living there.\nCryptography.\nIn \"Magic: The Untold Story of U.S. Intelligence and the Evacuation of Japanese Residents from the West Coast During World War II\", David Lowman, a former National Security Agency operative, argues that Magic (the code-name for American code-breaking efforts) intercepts posed \"frightening specter of massive espionage nets\", thus justifying incarceration. Lowman contended that incarceration served to ensure the secrecy of U.S. code-breaking efforts, because effective prosecution of Japanese Americans might necessitate disclosure of secret information. If U.S. code-breaking technology was revealed in the context of trials of individual spies, the Japanese Imperial Navy would change its codes, thus undermining U.S. strategic wartime advantage.\nSome scholars have criticized or dismissed Lowman's reasoning that \"disloyalty\" among some individual Japanese Americans could legitimize \"incarcerating 120,000 people, including infants, the elderly, and the mentally ill\". Lowman's reading of the contents of the \"Magic\" cables has also been challenged, as some scholars contend that the cables demonstrate that Japanese Americans were not heeding the overtures of Imperial Japan to spy against the United States. According to one critic, Lowman's book has long since been \"refuted and discredited\".\nThe controversial conclusions drawn by Lowman were defended by conservative commentator Michelle Malkin in her book \"In Defense of Internment: The Case for 'Racial Profiling' in World War II and the War on Terror\" (2004). Malkin's defense of Japanese incarceration was due in part to reaction to what she describes as the \"constant alarmism from Bush-bashers who argue that every counter-terror measure in America is tantamount to the internment\". She criticized academia's treatment of the subject, and suggested that academics critical of Japanese incarceration had ulterior motives. Her book was widely criticized, particularly with regard to her reading of the Magic cables. Daniel Pipes, also drawing on Lowman, has defended Malkin, and said that Japanese American incarceration was \"a good idea\" which offers \"lessons for today\".\nBlack and Jewish reactions to the Japanese American incarceration.\nThe American public overwhelmingly approved of the Japanese American incarceration measures and as a result, they were seldom opposed, particularly by members of minority groups who felt that they were also being chastised within America. Morton Grodzins writes that \"The sentiment against the Japanese was not far removed from (and it was interchangeable with) sentiments against Negroes and Jews.\"\nOccasionally, the NAACP and the NCJW spoke out but few were more vocal in opposition to incarceration than George S. Schuyler, an associate editor of the Pittsburgh Courier, perhaps the leading black newspaper in the U.S., who was increasingly critical of the domestic and foreign policy of the Roosevelt administration. He dismissed accusations that Japanese Americans presented any genuine national security threat. Schuyler warned African Americans that \"if the Government can do this to American citizens of Japanese ancestry, then it can do this to American citizens of ANY ancestry...Their fight is our fight.\"\nThe shared experience of racial discrimination has led some modern Japanese American leaders to come out in support of HR 40, a bill which calls for reparations to be paid to African-Americans because they are affected by slavery and subsequent discrimination. Cheryl Greenberg adds \"Not all Americans endorsed such racism. Two similarly oppressed groups, African Americans and Jewish Americans, had already organized to fight discrimination and bigotry.\" However, due to the justification of concentration camps by the US government, \"few seemed tactile to endorse the evacuation; most did not even discuss it.\" Greenberg argues that at the time, the incarceration was not discussed because the government's rhetoric hid the motivations for it behind a guise of military necessity, and a fear of seeming \"un-American\" led to the silencing of most civil rights groups until years into the policy.\nUnited States District Court's opinions.\nA letter by DeWitt and Bendetsen expressing racist bias against Japanese Americans was circulated and then hastily redacted in 1943\u20131944. DeWitt's final report stated that, because of their race, it was impossible to determine the loyalty of Japanese Americans, thus necessitating incarceration. The original version was so offensive \u2013 even in the atmosphere of the wartime 1940s \u2013 that Bendetsen ordered all copies to be destroyed.\nIn 1980, a copy of the original \"Final Report: Japanese Evacuation from the West Coast \u2013 1942\" was found in the National Archives, along with notes which show the numerous differences which exist between the original version and the redacted version. This earlier, racist and inflammatory version, as well as the FBI and Office of Naval Intelligence (ONI) reports, led to the \"coram nobis\" retrials which overturned the convictions of Fred Korematsu, Gordon Hirabayashi and Minoru Yasui on all charges related to their refusal to submit to exclusion and incarceration. The courts found that the government had intentionally withheld these reports and other critical evidence, at trials all the way up to the Supreme Court, which proved that there was no military necessity for the exclusion and incarceration of Japanese Americans. In the words of Department of Justice officials writing during the war, the justifications were based on \"willful historical inaccuracies and intentional falsehoods\".\nThe Ringle Report.\nIn May 2011, U.S. Solicitor General Neal Katyal, after a year of investigation, found Charles Fahy had intentionally withheld \"The Ringle Report\" drafted by the Office of Naval Intelligence, in order to justify the Roosevelt administration's actions in the cases of \"Hirabayashi v. United States\" and \"Korematsu v. United States\". The report would have undermined the administration's position of the military necessity for such action, as it concluded that most Japanese Americans were not a national security threat, and that allegations of communication espionage had been found to be without basis by the FBI and Federal Communications Commission.\nNewspaper editorials.\nEditorials from major newspapers at the time were generally supportive of the incarceration of the Japanese by the United States.\nA \"Los Angeles Times\" editorial dated February 19, 1942, stated that:\nSince Dec. 7 there has existed an obvious menace to the safety of this region in the presence of potential saboteurs and fifth columnists close to oil refineries and storage tanks, airplane factories, Army posts, Navy facilities, ports and communications systems. Under normal sensible procedure not one day would have elapsed after Pearl Harbor before the government had proceeded to round up and send to interior points all Japanese aliens and their immediate descendants for classification and possible incarceration.\nThis dealt with aliens, and the unassimilated. Going even farther, an \"Atlanta Constitution\" editorial dated February 20, 1942, stated that:\nThe time to stop taking chances with Japanese aliens and Japanese-Americans has come. . . . While Americans have an inate [\"sic\"] distaste for stringent measures, every one must realize this is a total war, that there are no Americans running loose in Japan or Germany or Italy and there is absolutely no sense in this country running even the slightest risk of a major disaster from enemy groups within the nation.\nA \"Washington Post\" editorial dated February 22, 1942, stated that:\nThere is but one way in which to regard the Presidential order empowering the Army to establish \"military areas\" from which citizens or aliens may be excluded. That is to accept the order as a necessary accompaniment of total defense.\nA \"Los Angeles Times\" editorial dated February 28, 1942, stated that:\nAs to a considerable number of Japanese, no matter where born, there is unfortunately no doubt whatever. They are for Japan; they will aid Japan in every way possible by espionage, sabotage and other activity; and they need to be restrained for the safety of California and the United States. And since there is no sure test for loyalty to the United States, all must be restrained. Those truly loyal will understand and make no objection.\nA \"Los Angeles Times\" editorial dated December 8, 1942, stated that:\nThe Japs in these centers in the United States have been afforded the very best of treatment, together with food and living quarters far better than many of them ever knew before, and a minimum amount of restraint. They have been as well fed as the Army and as well as or better housed. . . . The American people can go without milk and butter, but the Japs will be supplied.\nA \"Los Angeles Times\" editorial dated April 22, 1943, stated that:\nAs a race, the Japanese have made for themselves a record for conscienceless treachery unsurpassed in history. Whatever small theoretical advantages there might be in releasing those under restraint in this country would be enormously outweighed by the risks involved.\nFacilities.\nThe Works Projects Administration (WPA) played a key role in the construction and staffing of the camps in the initial period. From March to \nthe end of November 1942, that agency spent $4.47 million on removal \nand incarceration, which was even more than the Army devoted to that purpose during that period. The WPA was instrumental in creating such features of the camps as guard towers and barbed wire fencing.\nThe government operated several different types of camps holding Japanese Americans. The best known facilities were the military-run Wartime Civil Control Administration (WCCA) \"Assembly Centers\" and the civilian-run War Relocation Authority (WRA) \"Relocation Centers,\" which are generally (but unofficially) referred to as \"internment camps\". Many employees of the WRA had earlier worked for the WPA during the initial period of removal and construction. Scholars have urged dropping such euphemisms and refer to them as concentration camps and the people as incarcerated. Another argument for using the label \"concentration camps\" is that President Roosevelt himself applied that terminology to them, including at a press conference in November 1944.\nThe Department of Justice (DOJ) operated camps officially called \"Internment Camps\", which were used to detain those suspected of crimes or of \"enemy sympathies\". The government also operated camps for a number of German Americans and Italian Americans, who sometimes were assigned to share facilities with the Japanese Americans. The WCCA and WRA facilities were the largest and the most public. The WCCA Assembly Centers were temporary facilities that were first set up in horse racing tracks, fairgrounds, and other large public meeting places to assemble and organize inmates before they were transported to WRA Relocation Centers by truck, bus, or train. The WRA Relocation Centers were semi-permanent camps that housed persons removed from the exclusion zone after March 1942, or until they were able to relocate elsewhere in the United States outside the exclusion zone.\nDOJ and Army incarceration camps.\nEight U.S. Department of Justice Camps (in Texas, Idaho, North Dakota, New Mexico, and Montana) held Japanese Americans, primarily non-citizens and their families. The camps were run by the Immigration and Naturalization Service, under the umbrella of the DOJ, and guarded by Border Patrol agents rather than military police. The population of these camps included approximately 3,800 of the 5,500 Buddhist and Christian ministers, school instructors, newspaper workers, fishermen, and community leaders who had been accused of fifth column activity and arrested by the FBI after Pearl Harbor. (The remaining 1,700 were released to WRA relocation centers.) Immigrants and nationals of German and Italian ancestry were also held in these facilities, often in the same camps as Japanese Americans. Approximately 7,000 German Americans and 3,000 Italian Americans from Hawai\u02bbi and the U.S. mainland were interned in DOJ camps, along with 500 German seamen already in custody after being rescued from the \"SS Columbus\" in 1939. In addition 2,264 ethnic Japanese, 4,058 ethnic Germans, and 288 ethnic Italians were deported from 19 Latin American countries for a later-abandoned hostage exchange program with Axis countries or confinement in DOJ camps.\nSeveral U.S. Army incarceration camps held Japanese, Italian, and German American men considered \"potentially dangerous\". Camp Lordsburg, in New Mexico, was the only site built specifically to confine Japanese Americans. In May 1943, the Army was given responsibility for the detention of prisoners of war and all civilian internees were transferred to DOJ camps.\nWCCA Civilian Assembly Centers.\nExecutive Order 9066 authorized the removal of all persons of Japanese ancestry from the West Coast; however, it was signed before there were any facilities completed to house the displaced Japanese Americans. After the voluntary evacuation program failed to result in many families leaving the exclusion zone, the military took charge of the now-mandatory evacuation. On April 9, 1942, the Wartime Civil Control Administration (WCCA) was established by the Western Defense Command to coordinate the forced removal of Japanese Americans to inland concentration camps.\nThe relocation centers faced opposition from inland communities near the proposed sites who disliked the idea of their new \"Jap\" neighbors. In addition, government forces were struggling to build what would essentially be self-sufficient towns in very isolated, undeveloped, and harsh regions of the country; they were not prepared to house the influx of over 110,000 inmates. Since Japanese Americans living in the restricted zone were considered too dangerous to conduct their daily business, the military decided it had to house them in temporary centers until the relocation centers were completed.\nUnder the direction of Colonel Karl Bendetsen, existing facilities had been designated for conversion to WCCA use in March 1942, and the Army Corps of Engineers finished construction on these sites on April 21, 1942. All but four of the 15 confinement sites (12 in California, and one each in Washington, Oregon, and Arizona) had previously been racetracks or fairgrounds. The stables and livestock areas were cleaned out and hastily converted to living quarters for families of up to six, while wood and tar-paper barracks were constructed for additional housing, as well as communal latrines, laundry facilities, and mess halls. A total of 92,193 Japanese Americans were transferred to these temporary detention centers from March to August 1942. (18,026 more had been taken directly to two \"reception centers\" that were developed as the Manzanar and Poston WRA camps.) The WCCA was dissolved on March 15, 1943, when it became the War Relocation Authority and turned its attentions to the more permanent relocation centers.\nWRA Relocation Centers.\nThe War Relocation Authority (WRA) was the U.S. civilian agency responsible for the relocation and detention. The WRA was created by President Roosevelt on March 18, 1942, with Executive Order 9102 and it officially ceased to exist on June 30, 1946. Milton S. Eisenhower, then an official of the Department of Agriculture, was chosen to head the WRA. In the 1943 US Government film \"Japanese Relocation\" he said, \"This picture tells how the mass migration was accomplished. Neither the Army, not the War Relocation Authority relish the idea of taking men, women and children from their homes, their shops and their farms. So, the military and civilian agencies alike, determined to do the job as a democracy should\u2014with real consideration for the people involved.\" Dillon S. Myer replaced Eisenhower three months later on June 17, 1942. Myer served as Director of the WRA until the centers were closed. Within nine months, the WRA had opened ten facilities in seven states and transferred over 100,000 people from the WCCA facilities.\nThe WRA camp at Tule Lake was integral to food production in its own camp, as well as other camps. Almost 30 crops were harvested at this site by farmworkers. Despite this, Tule Lake's camp was eventually used as a detention center for people believed to pose a security risk. Tule Lake also served as a \"segregation center\" for individuals and families who were deemed \"disloyal\", and for those who were to be deported to Japan.\nList of camps.\nThere were three types of camps. \"Civilian Assembly Centers\" were temporary camps, frequently located at horse tracks, where Japanese Americans were sent after they were removed from their communities. Eventually, most of the Japanese Americans were sent to \"Relocation Centers,\" also known as \"internment camps\". \"Detention camps\" housed Nikkei who the government considered disruptive as well as Nikkei who the government believed were of special interest. When most of the Assembly Centers closed, they became training camps for US troops.\nJustice Department detention camps.\nThese camps often held German-American and Italian-American detainees in addition to Japanese Americans:\nCitizen Isolation Centers.\nThe Citizen Isolation Centers were for those considered to be problem inmates.\nSome consider these camps illegal because they were not authorized by Executive Order 9066.\nFederal Bureau of Prisons.\nDetainees convicted of crimes, usually draft resistance, were sent to these sites, mostly federal prisons:\nU.S. Army facilities.\nThese camps often held German and Italian detainees in addition to Japanese Americans:\nImmigration and Naturalization Service facilities.\nThese immigration detention stations held the roughly 5,500 men arrested immediately after Pearl Harbor, in addition to several thousand German and Italian detainees, and served as processing centers from which the men were transferred to DOJ or Army camps:\nExclusion, removal, and detention.\nSomewhere between 110,000 and 120,000 people of Japanese ancestry were subject to this mass exclusion program, of whom about 80,000 \"Nisei\" (second generation) and \"Sansei\" (third generation) were U.S. citizens. The rest were \"Issei\" (first generation) who were subject to internment under the Alien Enemies Act; many of these \"resident aliens\" had been inhabitants of the United States for decades, but had been deprived by law of being able to become naturalized citizens. Also part of the West Coast removal were 101 orphaned children of Japanese descent taken from orphanages and foster homes within the exclusion zone.\nDetainees of Japanese descent were first sent to one of 17 temporary \"Civilian Assembly Centers\", where most awaited transfer to more permanent relocation centers being constructed by the newly formed War Relocation Authority (WRA). Some of those who reported to the civilian assembly centers were not sent to relocation centers, but were released under the condition that they remain outside the prohibited zone until the military orders were modified or lifted. Almost 120,000 Japanese Americans and resident Japanese aliens were eventually removed from their homes on the West Coast and Southern Arizona as part of one of the largest forced relocations in U.S. history.\nMost of these camps/residences, gardens, and stock areas were placed on Native American reservations, for which the Native Americans were formally compensated. The Native American councils disputed the amounts negotiated in absentia by US government authorities. They later sued to gain relief and additional compensation for some items of dispute.\nUnder the National Student Council Relocation Program (supported primarily by the American Friends Service Committee), students of college age were permitted to leave the camps to attend institutions willing to accept students of Japanese ancestry. Although the program initially granted leave permits to a very small number of students, this eventually included 2,263 students by December 31, 1943.\nConditions in the camps.\nIn 1943, Secretary of the Interior Harold L. Ickes wrote \"the situation in at least some of the Japanese internment camps is bad and is becoming worse rapidly.\" The quality of life in the camps was heavily influenced by which government entity was responsible for them. INS Camps were regulated by international treaty. The legal difference between \"interned\" and relocated had significant effects on those who were imprisoned.\nAccording to a 1943 War Relocation Authority report, inmates were housed in \"tar paper-covered barracks of simple frame construction without plumbing or cooking facilities of any kind\". The spartan facilities met international laws, but left much to be desired. Many camps were built quickly by civilian contractors during the summer of 1942 based on designs for military barracks, making the buildings poorly equipped for cramped family living. Throughout many camps, twenty-five people were forced to live in space built to contain four, leaving no room for privacy.\nThe Heart Mountain War Relocation Center in northwestern Wyoming was a barbed-wire-surrounded enclave with unpartitioned toilets, cots for beds, and a budget of 45 cents daily per capita for food rations.\nArmed guards were posted at the camps, which were all in remote, desolate areas far from population centers. Inmates were typically allowed to stay with their families. There are documented instances of guards shooting inmates who reportedly attempted to walk outside the fences. One such shooting, that of James Wakasa at Topaz, led to a re-evaluation of the security measures in the camps. Some camp administrations eventually allowed relatively free movement outside the marked boundaries of the camps. Nearly a quarter of the inmates left the camps to live and work elsewhere in the United States, outside the exclusion zone. Eventually, some were authorized to return to their hometowns in the exclusion zone under supervision of a sponsoring American family or agency whose loyalty had been assured.\nThe phrase \"shikata ga nai\" (loosely translated as \"it cannot be helped\") was commonly used to summarize the incarcerated families' resignation to their helplessness throughout these conditions. This was noticed by their children, as mentioned in the well-known memoir \"Farewell to Manzanar\" by Jeanne Wakatsuki Houston and James D. Houston. Further, it is noted that parents may have internalized these emotions to withhold their disappointment and anguish from affecting their children. Nevertheless, children still were cognizant of this emotional repression.\nMedical care.\nBefore the war, 87 physicians and surgeons, 137 nurses, 105 dentists, 132 pharmacists, 35 optometrists, and 92 lab technicians provided healthcare to the Japanese American population, with most practicing in urban centers like Los Angeles, San Francisco, and Seattle. As the eviction from the West Coast was carried out, the Wartime Civilian Control Administration worked with the United States Public Health Service (USPHS) and many of these professionals to establish infirmaries within the temporary assembly centers. An Issei doctor was appointed to manage each facility, and additional healthcare staff worked under his supervision, although the USPHS recommendation of one physician for every 1,000 inmates and one nurse to 200 inmates was not met. Overcrowded and unsanitary conditions forced assembly center infirmaries to prioritize inoculations over general care, obstetrics, and surgeries; at Manzanar, for example, hospital staff performed over 40,000 immunizations against typhoid and smallpox. Food poisoning was common and also demanded significant attention. Those who were detained in Topaz, Minidoka, and Jerome experienced outbreaks of dysentery.\nFacilities in the more permanent \"relocation centers\" eventually surpassed the makeshift assembly center infirmaries, but in many cases, these hospitals were incomplete when inmates began to arrive and were not fully functional for several months. Additionally, vital medical supplies such as medications and surgical and sterilization equipment were limited. The staff shortages suffered in the assembly centers continued in the WRA camps. The administration's decision to invert the management structure and demote Japanese American medical workers to positions below white employees, while capping their pay rate at $20/month, further exacerbated this problem. (At Heart Mountain, for example, Japanese American doctors received $19/month compared to white nurses' $150/month.) The war had caused a shortage of healthcare professionals across the country, and the camps often lost potential recruits to outside hospitals that offered better pay and living conditions. When the WRA began to allow some Japanese Americans to leave camp, many Nikkei medical professionals resettled outside the camp. Those who remained had little authority in the administration of the hospitals. Combined with the inequitable payment of salaries between white and Japanese American employees, conflicts arose at several hospitals, and there were two Japanese American walk-outs at Heart Mountain in 1943.\nDespite a shortage of healthcare workers, limited access to equipment, and tension between white administrators and Japanese American staff, these hospitals provided much-needed medical care in camp. The extreme climates of the remote incarceration sites were hard on infants and elderly prisoners. The frequent dust storms of the high desert locations led to increased cases of asthma and coccidioidomycosis, while the swampy, mosquito-infested Arkansas camps exposed residents to malaria, all of which were treated in camp. Almost 6,000 live deliveries were performed in these hospitals, and all mothers received pre- and postnatal care. The WRA recorded 1,862 deaths across the ten camps, with cancer, heart disease, tuberculosis, and vascular disease accounting for the majority.\nEducation.\nOf the 110,000 Japanese Americans detained by the United States government during World War II, 30,000 were children. Most were school-age children, so educational facilities were set up in the camps. The government had not adequately planned for the camps, and no real budget or plan was set aside for the new camp educational facilities. Camp schoolhouses were crowded and had insufficient materials, books, notebooks, and desks for students. Books were only issued a month after the opening. In the Southwest, the schoolhouses were extremely hot in summertime. Class sizes were very large. At the height of its attendance, the Rohwer Camp of Arkansas reached 2,339, with only 45 certified teachers. The student to teacher ratio in the camps was 48:1 in elementary schools and 35:1 for secondary schools, compared to the national average of 28:1. There was a general teacher shortage in the US at the time, and the teachers were required to live in the camps themselves. Although the salary in the camps was triple that for regular teaching jobs, authorities were still unable to fill all the teaching positions with certified personnel, and so some non-certified teacher detainees were hired as assistants. Students of all ages graduated and obtained diplomas while in the camp. Graduation celebrations, such as one that took place at Santa Anita in 1942, were well attended. The camp's jazz band, The Starlight Serenaders, played music for over three hundred students spanning from grammar school graduates to those obtaining university degrees.\nAs early as spring 1942, inmates in WCCA assembly camps began the orchestration of informal music education programs, such as in the Santa Anita Detention Center. By June of that year, the music department employed four teachers who instructed over three hundred students and had obtained six pianos for classroom instruction. Ruth Watanabe, a student in musicology at USC before the evacuation, organized much of Santa Anita\u2019s music education programming, including coordinating a \u201crecord-loan\u201d system with friends outside the camp to provide students with a variety of European classical musicians. During the incarceration at permanent relocation camps, often times those with experience in classical Japanese song and dance were employed directly by the WRA. Bando Misa, an inmate at Tule Lake and an instructor of Japanese classical dance, earned a salary of nineteen dollars a month through the camp\u2019s Recreation Department. At Poston, Haruo \u201cFoozie\u201d Fujisawa, a member of The Music Makers, one of the many jazz bands spread throughout 9 of 10 relocation camps, transported his drum kit from home and gave lessons through the camp\u2019s Music Department. Class sizes varied across camps; generally, enrollment was in the double digits but could reach as many as 140 students in one class. The ages of students extended beyond the typical classroom demographics of young Nisei and Sansei; the inclusion of Issei adults and seniors was not unheard of.\nSports.\nAlthough life in the camps was very difficult, Japanese Americans formed many different sports teams, including baseball and football teams. In January 1942, President Franklin D. Roosevelt issued what came to be known as the \"Green Light Letter\" to MLB Commissioner Kenesaw Mountain Landis, which urged him to continue playing Major League Baseball games despite the ongoing war. In it Roosevelt said that \"baseball provides a recreation\", and this was true for Japanese American incarcerees as well. Over 100 baseball teams were formed in the Manzanar camp so that Japanese Americans could have some recreation, and some of the team names were carry-overs from teams formed before the incarceration.\nBoth men and women participated in the sports. In some cases, the Japanese American baseball teams from the camps traveled to outside communities to play other teams. Incarcerees from Idaho competed in the state tournament in 1943, and there were games between the prison guards and the Japanese American teams. Branch Rickey, who would be responsible for bringing Jackie Robinson into Major League Baseball in 1947, sent a letter to all of the WRA camps expressing interest in scouting some of the Nisei players. In the fall of 1943, three players tried out for the Brooklyn Dodgers in front of MLB scout George Sisler, but none of them made the team.\nTule Lake Agricultural Program.\nThe Tule Lake agricultural program was constructed with the purpose of growing crops in order to feed both detainees in their camp and in the other camps. It is said that any extras would be sold on the open market. The agricultural program was a way for inmates to be employed while at the center, as well as a way for some to learn farming skills. A 4-H program was established to pave a way for children to help the agricultural process at the center. From 1942 through 1945, Tule Lake produced 29 different crops, including Japanese vegetables like daikon, gobo, and nappa.\nStudent leave to attend Eastern colleges.\nJapanese American students were no longer allowed to attend college in the West during the incarceration, and many found ways to transfer or attend schools in the Midwest and East in order to continue their education.\nMost Nisei college students followed their families into camp, but a small number arranged for transfers to schools outside the exclusion zone. Their initial efforts expanded as sympathetic college administrators and the American Friends Service Committee began to coordinate a larger student relocation program. The Friends petitioned WRA Director Milton Eisenhower to place college students in Eastern and Midwestern academic institutions.\nThe National Japanese American Student Relocation Council was formed on May 29, 1942, and the AFSC administered the program. The acceptance process vetted college students and graduating high school students through academic achievement and a questionnaire centering on their relationship with American culture. Some high school students were also able to leave the incarceration camps through boarding schools. 39 percent of the Nisei students were women. The student's tuition, book costs, and living expenses were absorbed by the U.S. government, private foundations (such as the Columbia Foundation and the Carnegie Corporation) and church scholarships, in addition to significant fundraising efforts led by Issei parents in camp.\nOutside camp, the students took on the role of \"ambassadors of good will\", and the NJASRC and WRA promoted this image to soften anti-Japanese prejudice and prepare the public for the resettlement of Japanese Americans in their communities. Some students worked as domestic workers in nearby communities during the school year.\nAt Earlham College, President William Dennis helped institute a program that enrolled several dozen Japanese American students in order to spare them from incarceration. While this action was controversial in Richmond, Indiana, it helped strengthen the college's ties to Japan and the Japanese American community. At Park College in Missouri, Dr. William Lindsay Young attempted to get Nisei students enrolled despite backlash from the greater Parkville city.\nAt Oberlin College, about 40 evacuated Nisei students were enrolled. One of them, Kenji Okuda, was elected as student council president. Three Nisei students were enrolled at Mount Holyoke College during World War 2.\nIn total, over 500 institutions east of the exclusion zone opened their doors to more than 3,000 college-age youth who had been placed behind barbed wire, many of whom were enrolled in West Coast schools prior to their removal. These included a variety of schools, from small liberal arts colleges to large public universities.\nThe NJASRC ceased operations on June 7, 1946. After the incarceration camps had been shut down, releasing many Issei parents with little belongings, many families followed the college students to the eastern cities where they attended school. In 1980, former Nisei students formed the NSRC Nisei Student Relocation Commemorative Fund. In 2021, The University of Southern California apologized for discriminating against Nisei students. It issued posthumous degrees to the students whose educations were cut short or illegitimated, having already issued degrees to those surviving.\nLoyalty questions and segregation.\nIn early 1943, War Relocation Authority officials, working with the War Department and the Office of Naval Intelligence, circulated a questionnaire in an attempt to determine the loyalty of incarcerated Nisei men they hoped to recruit into military service. The \"Statement of United States Citizen of Japanese Ancestry\" was initially given only to Nisei who were eligible for service (or would have been, but for the 4-C classification imposed on them at the start of the war). Authorities soon revised the questionnaire and required all adults in camp to complete the form. Most of the 28 questions were designed to assess the \"Americanness\" of the respondent \u2014 had they been educated in Japan or the U.S.? were they Buddhist or Christian? did they practice \"judo\" or play on a baseball team? The final two questions on the form, which soon came to be known as the \"loyalty questionnaire\", were more direct:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Question 27: Are you willing to serve in the armed forces of the United States on combat duty, wherever ordered?\nQuestion 28: Will you swear unqualified allegiances to the United States of America and faithfully defend the United States from any and all attack by foreign or domestic forces, and forswear any form of allegiance or obedience to the Japanese emperor, or other foreign government, power or organization?\nAcross the camps, people who answered No to both questions became known as \"No Nos\".\nWhile most camp inmates simply answered \"yes\" to both questions, several thousand \u2014 17 percent of the total respondents, 20 percent of the Nisei \u2014 gave negative or qualified replies out of confusion, fear or anger at the wording and implications of the questionnaire. In regard to Question 27, many worried that expressing a willingness to serve would be equated with volunteering for combat, while others felt insulted at being asked to risk their lives for a country that had imprisoned them and their families. An affirmative answer to Question 28 brought up other issues. Some believed that renouncing their loyalty to Japan would suggest that they had at some point been loyal to Japan and disloyal to the United States. Many believed they were to be deported to Japan no matter how they answered, they feared an explicit disavowal of the Emperor would become known and make such resettlement extremely difficult.\nOn July 15, 1943, Tule Lake, the site with the highest number of \"no\" responses to the questionnaire, was designated to house inmates whose answers suggested they were \"disloyal\". During the remainder of 1943 and into early 1944, more than 12,000 men, women and children were transferred from other camps to the maximum-security Tule Lake Segregation Center.\nAfterward, the government passed the Renunciation Act of 1944, a law that made it possible for Nisei and Kibei to renounce their American citizenship. A total of 5,589 detainees opted to do so; 5,461 of these were sent to Tule Lake. Of those who renounced US citizenship, 1,327 were repatriated to Japan. Those persons who stayed in the US faced discrimination from the Japanese American community, both during and after the war, for having made that choice of renunciation. At the time, they feared what their futures held were they to remain American and remain incarcerated.\nThese renunciations of American citizenship have been highly controversial, for a number of reasons. Some apologists for incarceration have cited the renunciations as evidence that \"disloyalty\" or anti-Americanism was well represented among the incarcerated peoples, thereby justifying the incarceration. Many historians have dismissed the latter argument, for its failure to consider that the small number of individuals in question had been mistreated and persecuted by their own government at the time of the \"renunciation\":\n[T]he renunciations had little to do with \"loyalty\" or \"disloyalty\" to the United States, but were instead the result of a series of complex conditions and factors that were beyond the control of those involved. Prior to discarding citizenship, most or all of the renunciants had experienced the following misfortunes: forced removal from homes; loss of jobs; government and public assumption of disloyalty to the land of their birth based on race alone; and incarceration in a \"segregation center\" for \"disloyal\" ISSEI or NISEI...\nMinoru Kiyota, who was among those who renounced his citizenship and soon came to regret the decision, has said that he wanted only \"to express my fury toward the government of the United States\", for his incarceration and for the mental and physical duress, as well as the intimidation, he was made to face.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[M]y renunciation had been an expression of momentary emotional defiance in reaction to years of persecution suffered by myself and other Japanese Americans and, in particular, to the degrading interrogation by the FBI agent at Topaz and being terrorized by the guards and gangs at Tule Lake.\nCivil rights attorney Wayne M. Collins successfully challenged most of these renunciations as invalid, owing to the conditions of duress and intimidation under which the government obtained them. Many of the deportees were Issei (first generation) or Kibei, who often had difficulty with English and often did not understand the questions they were asked. Even among those Issei who had a clear understanding, Question 28 posed an awkward dilemma: Japanese immigrants were denied U.S. citizenship at the time, so when asked to renounce their Japanese citizenship, answering \"Yes\" would have made them stateless persons.\nWhen the government began seeking army volunteers from among the camps, only 6% of military-aged male inmates volunteered to serve in the U.S. Armed Forces. Most of those who refused tempered that refusal with statements of willingness to fight if they were restored their rights as American citizens. Eventually 33,000 Japanese American men and many Japanese American women served in the U.S. military during World War II, of which 20,000 served in the U.S. Army.\nThe 100th Infantry Battalion, which was formed in June 1942 with 1,432 men of Japanese descent from the Hawaii National Guard, was sent to Camps McCoy and Shelby for advanced training. Because of the 100th's superior training record, the War Department authorized the formation of the 442nd Regimental Combat Team. When the call was made, 10,000 young men from Hawaii volunteered with eventually 2,686 being chosen along with 1,500 from the continental U.S. The 100th Infantry Battalion landed in Salerno, Italy in September 1943 and became known as the Purple Heart Battalion. This legendary outfit was joined by the 442nd RCT in June 1944, and this combined unit became the most highly decorated U.S. military unit of its size and duration in U.S. military history. The 442nd's Nisei segregated field artillery battalion, then on detached service within the U.S. Army in Bavaria, liberated at least one of the satellite labor camps of the Nazis' original Dachau concentration camp on April 29, 1945, and only days later, on May 2, halted a death march in southern Bavaria.\nProving commitment to the United States.\nMany Nisei worked to prove themselves as loyal American citizens. Of the 20,000 Japanese Americans who served in the Army during World War II, \"many Japanese American soldiers had gone to war to fight racism at home\" and they were \"proving with their blood, their limbs, and their bodies that they were truly American\". Some one hundred Nisei women volunteered for the WAC (Women's Army Corps), where, after undergoing rigorous basic training, they had assignments as typists, clerks, and drivers. A smaller number of women also volunteered to serve as nurses for the ANC (Army Nurse Corps). Satoshi Ito, an incarceration camp inmate, reinforces the idea of the immigrants' children striving to demonstrate their patriotism to the United States. He notes that his mother would tell him, \"'you're here in the United States, you need to do well in school, you need to prepare yourself to get a good job when you get out into the larger society'\". He said she would tell him, \"'don't be a dumb farmer like me, like us'\" to encourage Ito to successfully assimilate into American society. As a result, he worked exceptionally hard to excel in school and later became a professor at the College of William &amp; Mary. His story, along with the countless Japanese Americans willing to risk their lives in war, demonstrate the lengths many in their community went to prove their American patriotism.\nOther concentration camps.\nAs early as September 1931, after the Japanese invasion of Manchuria, US officials began to compile lists of individuals, lists which were particularly focused on the Issei. This data was eventually included in the Custodial Detention index (CDI). Agents in the Department of Justice's Special Defense Unit classified the subjects into three groups: A, B, and C, with A being the \"most dangerous\", and C being \"possibly dangerous\".\nAfter the attack on Pearl Harbor, Roosevelt authorized his attorney general to put into motion a plan for the arrest of thousands of individuals whose names were on the potential enemy alien lists. Most of these individuals were Japanese American community leaders. Armed with a blanket arrest warrant, the FBI seized these men on the eve of December 8, 1941. These men were held in municipal jails and prisons until they were moved to Department of Justice detention camps, these camps were separate from the camps which were operated by the Wartime Relocation Authority (WRA). These camps were operated under far more stringent conditions and they were also patrolled by heightened criminal-style guards, despite the absence of criminal proceedings. Memoirs about the camps include those by Keiho Soga and Toru Matsumoto.\nCrystal City, Texas, was one such camp where Japanese Americans, German Americans, and Italian Americans were interned along with a large number of Axis-descended nationals who were seized from several Latin-American countries by the U.S.\nThe Canadian government also confined its citizens with Japanese ancestry during World War II (see Internment of Japanese Canadians), for many reasons which were also based on fear and prejudice. Some Latin American countries on the Pacific Coast, such as Peru, interned ethnic Japanese or sent them to the United States for incarceration. Brazil also imposed restrictions on its ethnic Japanese population.\nHawaii.\nAlthough Japanese Americans in Hawaii comprised more than one-third of Hawaii's entire population, businessmen prevented their incarceration or deportation to the concentration camps which were located on the mainland because they recognized their contributions to Hawaii's economy. In the hysteria of the time, some mainland Congressmen (Hawaii was only an incorporated U.S. territory at the time, and despite being fully part of the U.S., did not have a voting representative or senator in Congress) promoted that all Japanese Americans and Japanese immigrants should be removed from Hawaii but were unsuccessful. An estimated 1,200 to 1,800 Japanese nationals and American-born Japanese from Hawaii were interned or incarcerated, either in five camps on the islands or in one of the mainland concentration camps, but this represented well-under two percent of the total Japanese American residents in the islands. \"No serious explanations were offered as to why ... the internment of individuals of Japanese descent was necessary on the mainland, but not in Hawaii, where the large Japanese-Hawaiian population went largely unmolested.\"\nThe vast majority of Japanese Americans and their immigrant parents in Hawaii were not incarcerated because the government had already declared martial law in Hawaii, a legal measure which allowed it to significantly reduce the supposed risks of espionage and sabotage by residents of Hawaii who had Japanese ancestry. Also, Japanese Americans comprised over 35% of the territory's entire population: they numbered 157,905 out of a total population of 423,330 at the time of the 1940 census, making them the largest ethnic group at that time; detaining so many people would have been enormously challenging in terms of logistics. Additionally, the whole of Hawaiian society was dependent on their productivity. According to intelligence reports which were published at the time, \"the Japanese, through a concentration of effort in select industries, had achieved a virtual stranglehold on several key sectors of the economy in Hawaii,\" and they \"had access to virtually all jobs in the economy, including high-status, high-paying jobs (e.g., professional and managerial jobs)\". To imprison such a large percentage of the islands' work force would have crippled the Hawaiian economy. Thus, the unfounded fear of Japanese Americans turning against the United States was overcome by the reality-based fear of massive economic loss.\nDespite the financial and logistical obstacles, President Roosevelt persisted for quite some time in urging incarceration of Japanese Americans in Hawaii. As late as February 26, 1942, he informed Secretary of the Navy Knox that he had \"long felt that most of the Japanese should be removed from Oahu to one of the other Islands.\" While Roosevelt conceded that such an undertaking involved \"much planning, much temporary construction, and \ncareful supervision of them when they get to the new location,\" he did not \"worry about the constitutional question\u2014first, because of my recent order and, second, because Hawaii is under martial law.\" He called for Knox to work with Stimson and \"go ahead and do it as a military project.\" Eventually, he too gave up the project.\nLieutenant General Delos C. Emmons, the commander of the Hawaii Department, promised that the local Japanese American community would be treated fairly as long as it remained loyal to the United States. He succeeded in blocking efforts to relocate it to the outer islands or the mainland by pointing out the logistical difficulties of such a move. Among the small number incarcerated were community leaders and prominent politicians, including territorial legislators Thomas Sakakihara and Sanji Abe.\nFive concentration camps were operated in the territory of Hawaii, referred to as the \"Hawaiian Island Detention Camps\". One camp was located on Sand Island, at the mouth of Honolulu Harbor. This camp was constructed before the outbreak of the war. All of the prisoners who were held there were \"detained under military custody... because of the imposition of martial law throughout the Islands\". It was replaced by the Honouliuli Internment Camp, near Ewa, on the southwestern shore of Oahu in 1943. Another was located in Haiku, Maui, in addition to the Kilauea Detention Center on Hawaii and Camp Kalaheo on Kauai.\nJapanese Latin Americans.\nDuring World War II, over 2,200 Japanese from Latin America were held in concentration camps run by the Immigration and Naturalization Service, part of the Department of Justice. Beginning in 1942, Latin Americans of Japanese ancestry were rounded up and transported to American concentration camps run by the INS and the U.S. Justice Department. Most of these internees, approximately 1,800, came from Peru. An additional 250 were from Panama, Bolivia, Colombia, Costa Rica, Cuba, Ecuador, El Salvador, Mexico, Nicaragua, and Venezuela.\nThe first group of Japanese Latin Americans arrived in San Francisco on April 20, 1942, on board the \"Etolin\" along with 360 ethnic Germans and 14 ethnic Italians from Peru, Ecuador, and Colombia. The 151 men \u2014 ten from Ecuador, the rest from Peru \u2014 had volunteered for deportation believing they were to be repatriated to Japan. They were denied visas by U.S. Immigration authorities and then detained on the grounds they had tried to enter the country illegally, without a visa or passport. Subsequent transports brought additional \"volunteers\", including the wives and children of men who had been deported earlier. A total of 2,264 Japanese Latin Americans, about two-thirds of them from Peru, were interned in facilities on the U.S. mainland during the war.\nThe United States originally intended to trade these Latin American internees as part of a hostage exchange program with Japan and other Axis nations; at least one trade occurred. Over 1,300 persons of Japanese ancestry were exchanged for a like number of non-official Americans in October 1943, at the port of Marmagao, India. Over half were Japanese Latin Americans (the rest being ethnic Germans and Italians) and of that number one-third were Japanese Peruvians.\nOn September 2, 1943, the Swedish ship \"MS Gripsholm\" departed the U.S. with just over 1,300 Japanese nationals (including nearly a hundred from Canada and Mexico) en route for the exchange location, Marmagao, the main port of the Portuguese colony of Goa on the west coast of India. After two more stops in South America to take on additional Japanese nationals, the passenger manifest reached 1,340. Of that number, Latin American Japanese numbered 55 percent of the Gripsholm's travelers, 30 percent of whom were Japanese Peruvian. Arriving in Marmagao on October 16, 1943, the Gripsholm's passengers disembarked and then boarded the Japanese ship \"Teia Maru.\" In return, \"non-official\" Americans (secretaries, butlers, cooks, embassy staff workers, etc.) previously held by the Japanese Army boarded the \"Gripsholm\" while the \"Teia Maru\" headed for Tokyo. Because this exchange was done with those of Japanese ancestry officially described as \"volunteering\" to return to Japan, no legal challenges were encountered. The U.S. Department of State was pleased with the first trade and immediately began to arrange a second exchange of non-officials for February 1944. This exchange would involve 1,500 non-volunteer Japanese who were to be exchanged for 1,500 Americans. The US was busy with Pacific Naval activity and future trading plans stalled. Further slowing the program were legal and political \"turf\" battles between the State Department, the Roosevelt administration, and the DOJ, whose officials were not convinced of the legality of the program.\nThe completed October 1943 trade took place at the height of the Enemy Alien Deportation Program. Japanese Peruvians were still being \"rounded up\" for shipment to the U.S. in previously unseen numbers. Despite logistical challenges facing the floundering prisoner exchange program, deportation plans were moving ahead. This is partly explained by an early-in-the-war revelation of the overall goal for Latin Americans of Japanese ancestry under the Enemy Alien Deportation Program. Secretary of State Cordell Hull wrote to an agreeing President Roosevelt, \"[that the US must] continue our efforts to remove all the Japanese from these American Republics for internment in the United States.\"\n\"Native\" Peruvians expressed extreme animosity toward their Japanese citizens and expatriates, and Peru refused to accept the post-war return of Japanese Peruvians from the US. Although a small number asserting special circumstances, such as marriage to a non-Japanese Peruvian, did return, the majority were trapped. Their home country refused to take them back (a political stance Peru maintained until 1950), they were generally Spanish speakers in the Anglo US, and in the postwar U.S., the Department of State started expatriating them to Japan. Civil rights attorney Wayne Collins filed injunctions on behalf of the remaining internees, helping them obtain \"parole\" relocation to the labor-starved Seabrook Farms in New Jersey. He started a legal battle that was not resolved until 1953, when, after working as undocumented immigrants for almost ten years, those Japanese Peruvians remaining in the U.S. were finally offered citizenship.\nIncarceration ends.\nOn December 18, 1944, the Supreme Court handed down two decisions on the legality of the incarceration under Executive Order 9066. \"Korematsu v. United States\", a 6\u20133 decision upholding a Nisei's conviction for violating the military exclusion order, stated that, in general, the removal of Japanese Americans from the West Coast was constitutional. However, \"Ex parte Endo\" unanimously declared on that same day that loyal citizens of the United States, regardless of cultural descent, could not be detained without cause. In effect, the two rulings held that, while the eviction of American citizens in the name of military necessity was legal, the subsequent incarceration was not\u2014thus paving the way for their release.\nHaving been alerted to the Court's decision, the Roosevelt administration issued Public Proclamation No. 21 the day before the \"Korematsu\" and \"Endo\" rulings were made public, on December 17, 1944, rescinding the exclusion orders and declaring that Japanese Americans could return to the West Coast the next month.\nAlthough War Relocation Authority (WRA) Director Dillon Myer and others had pushed for an earlier end to the incarceration, the Japanese Americans were not allowed to return to the West Coast until January 2, 1945, after the November 1944 election, so as not to impede Roosevelt's reelection campaign. Many younger detainees had already been sent to Midwest or Eastern cities to pursue work or educational opportunities. For example, 20,000 were sent to Lake View, Chicago. The remaining population began to leave the camps to try to rebuild their lives at home. Former inmates were given $25 and a train ticket to wherever they wanted to go, but many had little or nothing to return to, having lost their homes and businesses. When Japanese Americans were sent to the camps they could only take a few items with them and while incarcerated could only work for menial jobs with a small monthly salary of $12\u2013$19. When incarceration ended, they therefore had few savings to survive on. Some emigrated to Japan, although many of these were repatriated against their will. The camps remained open for residents who were not ready to return (mostly elderly Issei and families with young children), but the WRA pressured stragglers to leave by gradually eliminating services in camp. Those who had not left by each camp's close date were forcibly removed and sent back to the West Coast.\nNine of the ten WRA camps were shut down by the end of 1945, although Tule Lake, which held \"renunciants\" slated for deportation to Japan, was not closed until March 20, 1946. Japanese Latin Americans brought to the U.S. from Peru and other countries, who were still being held in the DOJ camps at Santa Fe and Crystal City, took legal action in April 1946 in an attempt to avoid deportation to Japan.\nAftermath.\nHardship and material loss.\nMany detainees lost irreplaceable personal property due to restrictions that prohibited them from taking more than they could carry into the camps. These losses were compounded by theft and destruction of items placed in governmental storage. Leading up to their incarceration, Nikkei were prohibited from leaving the Military Zones or traveling more than from home, forcing those who had to travel for work, like truck farmers and residents of rural towns, to quit their jobs. Many others were simply fired for their Japanese heritage.\nMany Japanese Americans encountered continued housing injustice after the war. Alien land laws in California, Oregon, and Washington barred the Issei from owning their pre-war homes and farms. Many had cultivated land for decades as tenant farmers, but they lost their rights to farm those lands when they were forced to leave. Other Issei (and Nisei who were renting or had not completed payments on their property) had found families willing to occupy their homes or tend their farms during their incarceration. However, those unable to strike a deal with caretakers had to sell their property, often in a matter of days and at great financial loss to predatory land speculators, who made huge profits.\nIn addition to these monetary and property losses, there were seven who were shot and killed by sentries: Kanesaburo Oshima, 58, during an escape attempt from Fort Sill, Oklahoma; Toshio Kobata, 58, and Hirota Isomura, 59, during transfer to Lordsburg, New Mexico; James Ito, 17, and Katsuji James Kanegawa, 21, during the December 1942 Manzanar Riot; James Hatsuaki Wakasa, 65, while walking near the perimeter wire of Topaz; and Shoichi James Okamoto, 30, during a verbal altercation with a sentry at the Tule Lake Segregation Center.\nHatano Farm is located south of Los Angeles. It was shut down in 2022 by city officials, but recently gained status as a point of historical interest by vote from the California State Resources Commission. Upon returning from incarceration, U.S. Army veteran James Hatano settled and began to grow flowers. His land was located in Rancho Palos Verdes and was acquired through a federal lease in 1953. Hatano was one of the only returning Japanese who decided to farm again.\nPsychological injury was observed by Dillon S. Myer, director of the WRA camps. In June 1945, Myer described how the Japanese Americans had grown increasingly depressed and overcome with feelings of helplessness and personal insecurity. Author Betty Furuta explains that the Japanese used \"gaman,\" loosely meaning \"perseverance\", to overcome hardships; this was mistaken by non-Japanese as being introverted and lacking initiative.\nJapanese Americans also encountered hostility and even violence when they returned to the West Coast. Concentrated largely in rural areas of Central California, there were dozens of reports of gunshots, fires, and explosions aimed at Japanese American homes, businesses, and places of worship, in addition to non-violent crimes like vandalism and the defacing of Japanese graves. In one of the few cases that went to trial, four men were accused of attacking the Doi family of Placer County, California, setting off an explosion, and starting a fire on the family's farm in January 1945. Despite a confession from one of the men that implicated the others, the jury accepted their defense attorney's framing of the attack as a justifiable attempt to keep California \"a white man's country\" and acquitted all four defendants.\nTo compensate former detainees for their property losses, Congress passed the Japanese-American Claims Act on July 2, 1948, allowing Japanese Americans to apply for compensation for property losses which occurred as \"a reasonable and natural consequence of the evacuation or exclusion\". By the time the Act was passed, the IRS had already destroyed most of the detainees' 1939\u201342 tax records. Due to the time pressure and strict limits on how much they could take to the camps, few were able to preserve detailed tax and financial records during the evacuation process. Therefore, it was extremely difficult for claimants to establish that their claims were valid. Under the Act, Japanese American families filed 26,568 claims totaling $148\u00a0million in requests; about $37\u00a0million was approved and disbursed.\nThe different placement for the detainees had significant consequences for their lifetime outcomes. A 2016 study finds, using the random dispersal of detainees into camps in seven different states, that the people assigned to richer locations did better in terms of income, education, socioeconomic status, house prices, and housing quality roughly fifty years later.\nReparations and redress.\nBeginning in the 1960s, a younger generation of Japanese Americans, inspired by the civil rights movement, began what is known as the \"Redress Movement\", an effort to obtain an official apology and reparations from the federal government for incarcerating their parents and grandparents during the war. They focused not on documented property losses but on the broader injustice and mental suffering caused by the incarceration. The movement's first success was in 1976, when President Gerald Ford proclaimed that the incarceration was \"wrong\", and a \"national mistake\" which \"shall never again be repeated\". President Ford signed a proclamation formally terminating Executive Order 9066 and apologized for the incarceration, stating: \"We now know what we should have known then\u2014not only was that evacuation wrong but Japanese-Americans were and are loyal Americans. On the battlefield and at home the names of Japanese-Americans have been and continue to be written in history for the sacrifices and the contributions they have made to the well-being and to the security of this, our common Nation.\"\nThe campaign for redress was launched by Japanese Americans in 1978. The Japanese American Citizens League (JACL), which had cooperated with the administration during the war, became part of the movement. It asked for three measures: $25,000 to be awarded to each person who was detained, an apology from Congress acknowledging publicly that the U.S. government had been wrong, and the release of funds to set up an educational foundation for the children of Japanese American families.\nIn 1980, under the Carter administration, Congress established the Commission on Wartime Relocation and Internment of Civilians (CWRIC) to study the matter. On February 24, 1983, the commission issued a report entitled \"Personal Justice Denied\", condemning the incarceration as unjust and motivated by racism and xenophobic ideas rather than factual military necessity. Concentration camp survivors sued the federal government for $24\u00a0million in property loss, but lost the case. However, the Commission recommended that $20,000 in reparations be paid to those Japanese Americans who had suffered incarceration.\nThe Civil Liberties Act of 1988 exemplified the Japanese American redress movement that impacted the large debate about the reparation bill. There was question over whether the bill would pass during the 1980s due to the poor state of the federal budget and the low support of Japanese Americans covering 1% of the United States. However, four powerful Japanese American Democrats and Republicans who had war experience, with the support of Democratic congressmen Barney Frank, sponsored the bill and pushed for its passage as their top priority.\nOn August 10, 1988, U.S. President Ronald Reagan signed the Civil Liberties Act of 1988, which had been sponsored by several representatives including Barney Frank, Norman Mineta, and Bob Matsui in the House and by Spark Matsunaga who got 75 co-sponsors in the Senate, provided financial redress of $20,000 for each former detainee who was still alive when the act was passed, totaling $1.2\u00a0billion. The question of to whom reparations should be given, how much, and even whether monetary reparations were appropriate were subjects of sometimes contentious debate within the Japanese American community and Congress.\nOn September 27, 1992, the Civil Liberties Act Amendments of 1992, appropriating an additional $400\u00a0million to ensure all remaining detainees received their $20,000 redress payments, was signed into law by President George H. W. Bush. He issued another formal apology from the U.S. government on December 7, 1991, on the 50th anniversary of the Pearl Harbor attack, saying:\nIn remembering, it is important to come to grips with the past. No nation can fully understand itself or find its place in the world if it does not look with clear eyes at all the glories and disgraces of its past. We in the United States acknowledge such an injustice in our history. The internment of Americans of Japanese ancestry was a great injustice, and it will never be repeated.\nOver 81,800 people qualified by 1998 and $1.6\u00a0billion was distributed among them.\nUnder the 2001 budget of the United States, Congress authorized the preservation of ten detention sites as historical landmarks: \"places like Manzanar, Tule Lake, Heart Mountain, Topaz, Amache, Jerome, and Rohwer will forever stand as reminders that this nation failed in its most sacred duty to protect its citizens against prejudice, greed, and political expediency\".\nPresident Bill Clinton awarded the Presidential Medal of Freedom, the highest civilian honor in the United States, to Korematsu in 1998, saying, \"In the long history of our country's constant search for justice, some names of ordinary citizens stand for millions of souls: Plessy, Brown, Parks\u00a0... to that distinguished list, today we add the name of Fred Korematsu.\" That year, Korematsu served as the Grand Marshal of San Francisco's annual Cherry Blossom Festival parade. On January 30, 2011, California first observed an annual \"Fred Korematsu Day of Civil Liberties and the Constitution\", the first such ceremony ever to be held in commemoration of an Asian American in the United States. On June 14, 2011, Peruvian President Alan Garc\u00eda apologized for his country's internment of Japanese immigrants during World War II, most of whom were transferred to the U.S.\nSocial impact and legacy.\nAfter the war, and once the process of internment came to its conclusion, Japanese Americans became socially affected by the war and their experiences of United States government policy. Japanese Americans rejected their racial identity as a prerequisite to various organizations that had existed prior to their internment, in order to assimilate back into American society, with both the Japanese Association and the Japanese Chamber of Commerce slipping into non-existence in the post-war years. The distancing of Japanese Americans from any collective, racially labelled establishments was something they saw necessary in order to preserve their status in the United States in the wake of their experiences.\nIn addition, Japanese Americans were also impacted socially by a changing religious structure in which ethnic churches were terminated, with church membership dropping from 25% of the Japanese American population in 1942, to 6% in 1962.\nTerminology debate.\nMisuse of the term \"internment\".\nThe legal term \"internment\" has been used in regards to the mass incarceration of Japanese Americans. This term, however, derives from international conventions regarding the treatment of enemy nationals during wartime and specifically limits internment to those (noncitizen) enemy nationals who threaten the security of the detaining power. The internment of selected enemy alien belligerents, as opposed to mass incarceration, is legal both under US and international law. UCLA Asian American studies professor Lane Hirabayashi pointed out that the history of the term internment, to mean the arrest and holding of non-citizens, could only be correctly applied to Issei, Japanese people who were not legal citizens. These people were a minority during Japanese incarceration and thus Roger Daniels, emeritus professor of history at the University of Cincinnati, has concluded that this terminology is wrongfully used by any government that wishes to include groups other than the Issei.\nOn April 22, 2022, the Associated Press edited its entry for \"Japanese internment\", changing the entry heading to \"Japanese internment, incarceration,\" and adding the following wording:Though internment has been applied historically to all detainments of Japanese Americans and Japanese nationals during World War II, the broader use of the term is inaccurate\u2014about two-thirds of those who were relocated were US citizens, and thus could not be considered interns\u2014and many Japanese-Americans find it objectionable.\n&lt;br&gt;\nIt is better to say that they were \"incarcerated\" or \"detained\" and to define the larger event as the \"incarceration of Japanese Americans\". \nWhich term to use.\nDuring World War II, the camps were referred to both as relocation centers and concentration camps by government officials and in the press. Roosevelt himself referred to the camps as concentration camps on different occasions, including at a press conference held on October 20, 1942. In 1943, his attorney general Francis Biddle lamented that \"The present practice of keeping loyal American citizens in concentration camps for longer than is necessary is dangerous and repugnant to the principles of our government.\"\nFollowing World War II, other government officials made statements suggesting that the use of the term \"relocation center\" had been largely euphemistic. In 1946, former Secretary of the Interior Harold Ickes wrote \"We gave the fancy name of 'relocation centers' to these dust bowls, but they were concentration camps nonetheless.\" In a 1961 interview, Harry S. Truman stated \"They were\nconcentration camps. They called it relocation but they put them in concentration camps, and I was against it. We were in a period of emergency, but it was still the wrong thing to do.\"\nIn subsequent decades, debate has arisen over the terminology used to refer to camps in which Americans of Japanese ancestry and their immigrant parents, were incarcerated by the US government during the war. These camps have been referred to as \"war relocation centers\", \"relocation camps\", \"relocation centers\", \"internment camps\", and \"concentration camps\", and the controversy over which term is the most accurate and appropriate continues.\nTowards a consensus.\nIn 1998, the use of the term \"concentration camps\" gained greater credibility prior to the opening of an exhibit about the American camps at Ellis Island. Initially, the American Jewish Committee (AJC) and the National Park Service, which manages Ellis Island, objected to the use of the term in the exhibit. However, during a subsequent meeting held at the offices of the AJC in New York City, leaders representing Japanese Americans and Jewish Americans reached an understanding about the use of the term.\nAfter the meeting, the Japanese American National Museum and the AJC issued a joint statement (which was included in the exhibit) that read in part:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A concentration camp is a place where people are imprisoned not because of any crimes they have committed, but simply because of who they are. Although many groups have been singled out for such persecution throughout history, the term 'concentration camp' was first used at the turn of the [20th] century in the Spanish American and Boer Wars. During World War II, America's concentration camps were clearly distinguishable from Nazi Germany's. Nazi camps were places of torture, barbarous medical experiments and summary executions; some were extermination centers with gas chambers. Six million Jews were slaughtered in the Holocaust. Many others, including Gypsies, Poles, homosexuals and political dissidents were also victims of the Nazi concentration camps. In recent years, concentration camps have existed in the former Soviet Union, Cambodia and Bosnia. Despite differences, all had one thing in common: the people in power removed a minority group from the general population and the rest of society let it happen.\"The New York Times\" published an unsigned editorial supporting the use of \"concentration camp\" in the exhibit. An article quoted Jonathan Mark, a columnist for \"The Jewish Week\", who wrote, \"Can no one else speak of slavery, gas, trains, camps? It's Jewish malpractice to monopolize pain and minimize victims.\" AJC Executive Director David A. Harris stated during the controversy, \"We have not claimed Jewish exclusivity for the term 'concentration camps.'\", while also stating \"Since the Second World War, these terms have taken on a specificity and a new level of meaning that deserves protection. A certain care needs to be exercised.\"\nDeborah Schiffrin has written that, at the opening of the exhibition, entitled \"America's Concentration Camps: Remembering the Japanese-American Experience\", \"some Jewish groups\" had been offended by the use of the term. However, Schiffrin also notes that a compromise was reached when an appropriate footnote was added to the exhibit brochure.\nOn the rejection of euphemisms.\nOn July 7, 2012, at its annual convention, the National Council of the Japanese American Citizens League unanimously ratified the \"Power of Words Handbook,\" calling for the use of \"...truthful and accurate terms, and retiring the misleading euphemisms created by the government to cover up the denial of Constitutional and human rights, the force, oppressive conditions, and racism against 120,000 innocent people of Japanese ancestry locked up in America's World War II concentration camps.\" Moreover, Roosevelt himself publicly used the term \"concentration camps\" without any qualifiers to describe Japanese American incarceration in a press conference in November 1944.\nComparisons.\nThe incarceration of Japanese Americans has been compared to the internal deportation of Ethnically Volga German Soviet citizens from the western USSR to Soviet Central Asia. As well as the persecutions, expulsions, and dislocations of other ethnic minority groups which also occurred during World War II, both in Europe and Asia.\nLegacy.\nCultural legacy.\nExhibitions and collections.\nSculpture.\nNina Akamu, a Sansei, created the sculpture entitled \"Golden Cranes\" of two red-crowned cranes, which became the center feature of the Japanese American Memorial to Patriotism During World War II. The U.S. Department of Defense described the November 9, 2000, dedication of the Memorial: \"Drizzling rain was mixed with tears streaming down the faces of Japanese American World War II heroes and those who spent the war years imprisoned in isolated internment camps.\" Akamu's family's connection to the concentration camps based on the experience of her maternal grandfather, who was interned and later died in a concentration camp in Hawaii\u2014combined with the fact that she grew up in Hawaii for a time, where she fished with her father at Pearl Harbor\u2014and the erection of a Japanese American war memorial near her home in Massa, Italy, inspired a strong connection to the Memorial and its creation.\nUnited States Attorney General Janet Reno also spoke at the dedication of the Memorial, where she shared a letter from President Clinton stating: \"We are diminished when any American is targeted unfairly because of his or her heritage. This Memorial and the internment sites are powerful reminders that stereotyping, discrimination, hatred and racism have no place in this country.\"\nAccording to the National Japanese American Memorial Foundation, the memorial:\n...is symbolic not only of the Japanese American experience, but of the extrication of anyone from deeply painful and restrictive circumstances. It reminds us of the battles we've fought to overcome our ignorance and prejudice and the meaning of an integrated culture, once pained and torn, now healed and unified. Finally, the monument presents the Japanese American experience as a symbol for all peoples.\nFilms.\nDozens of movies were filmed about and in the concentration camps; these relate the experiences of inmates or were made by former camp inmates. Examples follow.\nLiterature.\nMany books and novels were written by and about Japanese Americans' experience during and after their residence in concentration camps among them can be mentioned the followed:\nLegal legacy.\nSeveral significant legal decisions arose out of Japanese American incarceration, relating to the powers of the government to detain citizens in wartime. Among the cases which reached the US Supreme Court were \"Ozawa v. United States\" (1922), \"Yasui v. United States\" (1943), \"Hirabayashi v. United States\" (1943), \"ex parte Endo\" (1944), and \"Korematsu v. United States\" (1944). In \"Ozawa\", the court established that peoples defined as 'white' were specifically of Caucasian descent; In \"Yasui\" and \"Hirabayashi,\" the court upheld the constitutionality of curfews based on Japanese ancestry; in \"Korematsu,\" the court upheld the constitutionality of the exclusion order. In \"Endo\", the court accepted a petition for a writ of habeas corpus and ruled that the WRA had no authority to subject a loyal citizen to its procedures.\nKorematsu's and Hirabayashi's convictions were vacated in a series of \"coram nobis\" cases in the early 1980s. In the \"coram nobis\" cases, federal district and appellate courts ruled that newly uncovered evidence revealed an unfairness which, had it been known at the time, would likely have changed the Supreme Court's decisions in the Yasui, Hirabayashi, and Korematsu cases.\nThese new court decisions rested on a series of documents recovered from the National Archives showing that the government had altered, suppressed, and withheld important and relevant information from the Supreme Court, including the Final Report by General DeWitt justifying the incarceration program. The Army had destroyed documents in an effort to hide alterations that had been made to the report to reduce their racist content. The \"coram nobis\" cases vacated the convictions of Korematsu and Hirabayashi (Yasui died before his case was heard, rendering it moot), and are regarded as part of the impetus to gain passage of the Civil Liberties Act of 1988.\nThe ruling of the US Supreme Court in the Korematsu case was overturned in the 2018 majority opinion of \"Trump v. Hawaii\" (upholding a ban on immigration of nationals from several Muslim majority countries). Regarding the Korematsu case, Chief Justice Roberts wrote: \"The forcible relocation of U.S. citizens to concentration camps, solely and explicitly on the basis of race, is objectively unlawful and outside the scope of Presidential authority.\"\nFormer Supreme Court Justice Tom C. Clark, who represented the US Department of Justice in the \"relocation\", writes in the epilogue to the book \"Executive Order 9066: The Internment of 110,000 Japanese Americans\" (1992):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe truth is\u2014as this deplorable experience proves\u2014that constitutions and laws are not sufficient of themselves... Despite the unequivocal language of the Constitution of the United States that the writ of habeas corpus shall not be suspended, and despite the Fifth Amendment's command that no person shall be deprived of life, liberty or property without due process of law, both of these constitutional safeguards were denied by military action under Executive Order 9066.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n\u00a0This article incorporates https:// from \nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "46886", "revid": "20585603", "url": "https://en.wikipedia.org/wiki?curid=46886", "title": "Japanese relocation", "text": "Japanese relocation may refer to World War II events:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "46887", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=46887", "title": "Japanese-American relocation", "text": ""}
{"id": "46888", "revid": "22444", "url": "https://en.wikipedia.org/wiki?curid=46888", "title": "Franklin Deleno Roosevelt", "text": ""}
{"id": "46889", "revid": "44217690", "url": "https://en.wikipedia.org/wiki?curid=46889", "title": "Leaning Tower of Pisa", "text": "Bell tower in Pisa, Italy\nThe Leaning Tower of Pisa ( ), or simply the Tower of Pisa (), is the , or freestanding bell tower, of Pisa Cathedral. It is known for its nearly four-degree lean, the result of an unstable foundation. The tower is one of three structures in Pisa's Cathedral Square (), which includes the cathedral and Pisa Baptistry. Over time, the tower has become one of the most visited tourist attractions in the world as well as an architectural icon of Italy, receiving over 5 million visitors each year.\nThe height of the tower is from the ground on the low side and on the high side. The width of the walls at the base is . Its weight is estimated at . The tower has 296 or 294 steps; the seventh floor has two fewer steps on the north-facing staircase.\nThe tower began to lean during construction in the 12th century, due to soft ground which could not properly support the structure's weight. It worsened through the completion of construction in the 14th century. By 1990, the tilt had reached 5.5 degrees. The structure was stabilized by remedial work between 1993 and 2001, which reduced the tilt to 3.97 degrees.\nArchitect.\nThe identity of the architect of the tower is a subject of controversy. The design had long been attributed to a man named Guglielmo and to Bonanno Pisano, the latter a well-known 12th-century resident artist of Pisa known for his bronze casting, particularly in the Pisa Duomo. Pisano left Pisa in 1185 for Monreale, Sicily, only to return and die in his home town. A piece of cast bearing his name was discovered at the foot of the tower in 1820, but this may be related to the bronze door in the fa\u00e7ade of the cathedral that was destroyed in 1595. A 2001 study seems to indicate Diotisalvi was the original architect, due to the time of construction and affinity with other Diotisalvi works, notably the bell tower of San Nicola and the Baptistery, both in Pisa.\nConstruction.\nConstruction of the tower occurred in three stages over 199 years. On 5 January 1172, Donna Berta di Bernardo, a widow and resident of the house of dell'Opera di Santa Maria, bequeathed sixty soldi to the . The sum was then used toward the purchase of a few stones which still form the base of the bell tower. On 9 August 1173, the foundations of the tower were laid. Work on the ground floor of the white marble campanile began on 14 August of the same year during a period of military success and prosperity. This ground floor is a blind arcade articulated by engaged columns with classical Corinthian capitals. Nearly four centuries later Giorgio Vasari wrote: \"Guglielmo, according to what is being said, in the year 1174, together with sculptor Bonanno, laid the foundations of the bell tower of the cathedral in Pisa\".\nThe tower began to sink after construction had progressed to the second floor in 1178. This was due to a mere three-metre foundation, set in weak, unstable subsoil, a design that was flawed from the beginning. Construction was subsequently halted for the better part of a century, as the Republic of Pisa was almost continually engaged in battles with Genoa, Lucca, and Florence. This allowed time for the underlying soil to settle. Otherwise, the tower would almost certainly have toppled. On 27 December 1233, the worker Benenato, son of Gerardo Bottici, oversaw the continuation of the tower's construction.\nOn 23 February 1260, Guido Speziale, son of Giovanni Pisano, was elected to oversee the building of the tower. On 12 April 1264, the master builder Giovanni di Simone, architect of the Camposanto, and 23 workers went to the mountains close to Pisa to cut the required marble. The cut stones were given to Rainaldo Speziale, worker of St. Francesco. In 1272, construction resumed under Di Simone. In an effort to compensate for the tilt, the engineers built upper floors with one side taller than the other. Because of this, the tower is curved. Construction was halted again in 1284 when the Pisans were defeated by the Genoese in the Battle of Meloria.\nThe seventh floor was completed in 1319. The bell-chamber was finally added in 1372. It was built by Tommaso di Andrea Pisano, who succeeded in harmonizing the Gothic elements of the belfry with the Romanesque style of the tower. There are seven bells, one for each note of the musical major scale. The largest one was installed in 1655.\nHistory following construction.\nBetween 1589 and 1592, Galileo Galilei, who lived in Pisa at the time, is said to have dropped two cannonballs of different masses from the tower to demonstrate that their speed of descent was independent of their mass, in keeping with the scientific law of free fall. The primary source for this is the biography \"Racconto istorico della vita di Galileo Galilei (Historical Account of the Life of Galileo Galilei)\", written by Galileo's pupil and secretary Vincenzo Viviani in 1654, but only published in 1717, long after his death.\nDuring World War II, the Allies suspected that the Germans were using the tower as an observation post. Leon Weckstein, a U.S. Army sergeant sent to confirm the presence of German troops in the tower, was impressed by the beauty of the cathedral and its campanile, and thus refrained from ordering an artillery strike, sparing it from destruction.\nNumerous efforts have been made to restore the tower to a vertical orientation or at least keep it from falling over. Most of these efforts failed; some worsened the tilt. On 27 February 1964, the government of Italy requested aid in preventing the tower from toppling. It was, however, considered important to retain the current tilt, due to the role that this element played in promoting the tourism industry of Pisa.\nStarting in 1993, 870 tonnes of lead counterweights were added, which straightened the tower slightly.\nThe tower and the neighbouring cathedral, baptistery, and cemetery are included in the Piazza del Duomo UNESCO World Heritage Site, which was declared in 1987.\nThe tower was closed to the public on 7 January 1990, after more than two decades of stabilisation studies and spurred by the abrupt collapse of the Civic Tower of Pavia in 1989. The bells were removed to relieve some weight, and cables were cinched around the third level and anchored several hundred meters away, and residences in the path of a potential collapse were vacated. The selected method for preventing the collapse of the tower was to slightly reduce its tilt to a safer angle by removing of soil from underneath the raised end. The tower's tilt was reduced by , returning to its 1838 position. After a decade of corrective reconstruction and stabilization efforts, the tower was reopened to the public on 15 December 2001, and was declared stable for at least another 300 years. In total, of soil were removed.\nAfter a phase (1990\u20132001) of structural strengthening, the tower has been undergoing gradual surface restoration to repair visible damage, mostly corrosion and blackening. These are particularly pronounced due to the tower's age and its exposure to wind and rain. In May 2008, engineers announced that the tower had been stabilized such that it had stopped moving for the first time in its history. They stated that it would be stable for at least 200 years.\nA ceremony for the 850th anniversary of the laying of the foundation stone was held on 9 August 2023.\nEarthquake survival.\nThe tower has survived at least four strong earthquakes since 1280. A 2018 engineering investigation concluded that the tower withstood the tremors because of dynamic soil-structure interaction: the height and stiffness of the tower combined with the softness of the foundation soil influences the tower's vibrational characteristics in such a way that it does not resonate with earthquake ground motion. The same soft soil that caused the leaning and brought the tower to the verge of collapse helped to prevent significant destruction in the event of an earthquake.\nTechnical information.\nAbout the 5th bell: The name \"Pasquareccia\" comes from \"Easter\", because it used to ring on Easter day. However, this bell is older than the bell-chamber itself, and comes from the tower Vergata in \"Palazzo Pretorio\" in Pisa, where it was called \"La Giustizia\" (The Justice). The bell was tolled to announce executions of criminals and traitors, including Count Ugolino in 1289. A new bell was installed in the bell tower at the end of the 18th century to replace the broken \"Pasquareccia\".\nThe circular shape and great height of the campanile were unusual for their time, and the crowning belfry is stylistically distinct from the rest of the construction. This belfry incorporates a correction for the inclined axis below. The siting of the campanile within the Piazza del Duomo diverges from the axial alignment of the cathedral and baptistery of the Piazza del Duomo.\n\"Guinness World Records\".\nTwo German churches have challenged the tower's status as the world's most lopsided building: the 15th-century square Leaning Tower of Suurhusen and the 14th-century bell tower of the Oberkirche in the town of Bad Frankenhausen. \"Guinness World Records\" measured the Pisa and Suurhusen towers, finding the former's tilt to be 3.97 degrees. In June 2010, \"Guinness World Records\" certified the Capital Gate building in Abu Dhabi, UAE as the \"World's Furthest Leaning Man-made Tower\"; it has an 18-degree slope, almost five times more than the Tower of Pisa, but was deliberately engineered to slant. The Leaning Tower of Wanaka in W\u0101naka, New Zealand, also deliberately built, leans at 53 degrees to the ground.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46890", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=46890", "title": "Frequency-hopping spread spectrum", "text": "Radio signal transmission method\nFrequency-hopping spread spectrum (FHSS) is a method of transmitting radio signals by rapidly changing the carrier frequency among many frequencies occupying a large spectral band. The changes are controlled by a code known to both transmitter and receiver. FHSS is used to avoid interference, to prevent eavesdropping, and to enable code-division multiple access (CDMA) communications.\nThe frequency band is divided into smaller sub-bands. Signals rapidly change (\"hop\") their carrier frequencies among the center frequencies of these sub-bands in a determined order. Interference at a specific frequency will affect the signal only during a short interval.\nFHSS offers four main advantages over a fixed-frequency transmission:\nUsage.\nMilitary.\nSpread-spectrum signals are highly resistant to deliberate jamming unless the adversary has knowledge of the frequency-hopping pattern. Military radios generate the frequency-hopping pattern under the control of a secret Transmission Security Key (TRANSEC) that the sender and receiver share in advance. This key is generated by devices such as the KY-57 Speech Security Equipment. United States military radios that use frequency hopping include the JTIDS/MIDS family, the HAVE QUICK Aeronautical Mobile communications system, and the SINCGARS Combat Net Radio, Link-16.\nCivilian.\nIn the US, since the Federal Communications Commission (FCC) amended rules to allow FHSS systems in the unregulated 2.4\u00a0GHz band, many consumer devices in that band have employed various FHSS modes. eFCC CFR 47 part 15.247 covers the regulations in the US for 902\u2013928\u00a0MHz, 2400\u20132483.5\u00a0MHz, and 5725\u20135850\u00a0MHz bands, and the requirements for frequency hopping.\nSome walkie-talkies that employ FHSS technology have been developed for unlicensed use on the 900\u00a0MHz band. FHSS technology is also used in many hobby transmitters and receivers used for radio-controlled model cars, airplanes, and drones. A type of multiple access is achieved allowing hundreds of transmitter/receiver pairs to be operated simultaneously on the same band, in contrast to previous FM or AM radio-controlled systems that had limited simultaneous channels.\nTechnical considerations.\nThe overall bandwidth required for frequency hopping is much wider than that required to transmit the same information using only one carrier frequency. But because transmission occurs only on a small portion of this bandwidth at any given time, the instantaneous interference bandwidth is really the same. While providing no extra protection against wideband thermal noise, the frequency-hopping approach reduces the degradation caused by narrowband interference sources. \nOne of the challenges of frequency-hopping systems is to synchronize the transmitter and receiver. One approach is to have a guarantee that the transmitter will use all the channels in a fixed period of time. The receiver can then find the transmitter by picking a random channel and listening for valid data on that channel. The transmitter's data is identified by a special sequence of data that is unlikely to occur over the segment of data for this channel, and the segment can also have a checksum for integrity checking and further identification. The transmitter and receiver can use fixed tables of frequency-hopping patterns, so that once synchronized they can maintain communication by following the table.\nIn the US, FCC part 15 on unlicensed spread spectrum systems in the 902\u2013928\u00a0MHz and 2.4\u00a0GHz bands permits more power than is allowed for non-spread-spectrum systems. Both FHSS and direct-sequence spread-spectrum (DSSS) systems can transmit at 1 watt, a thousandfold increase from the 1 milliwatt limit on non-spread-spectrum systems. The FCC also prescribes a minimum number of frequency channels and a maximum dwell time for each channel.\nOrigins.\nIn 1899, Guglielmo Marconi experimented with frequency-selective reception in an attempt to minimise interference.\nThe earliest mentions of frequency hopping in open literature are in https://, awarded to Nikola Tesla on March 17, 1903, and in radio pioneer Jonathan Zenneck's book \"Wireless Telegraphy\" (German, 1908, English translation McGraw Hill, 1915), although Zenneck writes that Telefunken had already tried it. Nikola Tesla doesn't mention the phrase \"frequency hopping\" directly, but certainly alludes to it. Entitled \"Method of Signaling\", the patent describes a system that would enable radio communication \"without any danger of the signals or messages being disturbed, intercepted, interfered with in any way\".\nThe German military made limited use of frequency hopping for communication between fixed command points in World War I to prevent eavesdropping by British forces, who did not have the technology to follow the sequence. Jonathan Zenneck's book \"Wireless Telegraphy\" was originally published in German in 1908, but was translated into English in 1915 as the enemy started using frequency hopping on the front line.\nIn 1920, Otto B. Blackwell, De Loss K. Martin, and Gilbert S. Vernam filed a patent application for a \"Secrecy Communication System\", granted as https:// in 1926. This patent described a method of transmitting signals on multiple frequencies in a random manner for secrecy, anticipating key features of later frequency hopping systems.\nA Polish engineer and inventor, Leonard Danilewicz, claimed to have suggested the concept of frequency hopping in 1929 to the Polish General Staff, but it was rejected.\nIn 1932, https:// was awarded to Willem Broertjes, named \"Method of maintaining secrecy in the transmission of wireless telegraphic messages\", which describes a system where \"messages are transmitted by means of a group of frequencies... known to the sender and receiver alone, and alternated at will during transmission of the messages\".\nDuring World War II, the US Army Signal Corps was inventing a communication system called SIGSALY, which incorporated spread spectrum in a single frequency context. However, SIGSALY was a top-secret communications system, so its existence was not known until the 1980s.\nIn 1942, actress Hedy Lamarr and composer George Antheil received https:// for their \"Secret Communications System\", an early version of frequency hopping using a piano-roll to switch among 88 frequencies to make radio-guided torpedoes harder for enemies to detect or jam. They then donated the patent to the U.S. Navy.\nFrequency-hopping ideas may have been rediscovered in the 1950s during patent searches when private companies were independently developing direct-sequence Code Division Multiple Access, a non-frequency-hopping form of spread-spectrum. In 1957, engineers at Sylvania Electronic Systems Division adopted a similar idea, using the recently invented transistor instead of Lamarr's and Antheil's clockwork technology. In 1962, the US Navy utilized Sylvania Electronic Systems Division's work during the Cuban Missile Crisis.\nA practical application of frequency hopping was developed by Ray Zinn, co-founder of Micrel Corporation. Zinn developed a method allowing radio devices to operate without the need to synchronize a receiver with a transmitter. Using frequency hopping and sweep modes, Zinn's method is primarily applied in low data rate wireless applications such as utility metering, machine and equipment monitoring and metering, and remote control. In 2006 Zinn received https:// for his \"Wireless device and method using frequency hopping and sweep modes.\"\nVariations.\nAdaptive frequency-hopping spread spectrum (AFH) as used in Bluetooth improves resistance to radio frequency interference by avoiding crowded frequencies in the hopping sequence. This sort of adaptive transmission is easier to implement with FHSS than with DSSS.\nThe key idea behind AFH is to use only the \"good\" frequencies and avoid the \"bad\" ones\u2014those experiencing frequency selective fading, those on which a third party is trying to communicate, or those being actively jammed. Therefore, AFH should be complemented by a mechanism for detecting good and bad channels.\nBut if the radio frequency interference is itself dynamic, then AFH's strategy of \"bad channel removal\" may not work well. For example, if there are several colocated frequency-hopping networks (as Bluetooth Piconet), they are mutually interfering and AFH's strategy fails to avoid this interference.\nThe problem of dynamic interference, gradual reduction of available hopping channels and backward compatibility with legacy Bluetooth devices was resolved in version 1.2 of the Bluetooth Standard (2003). Such a situation can often happen in the scenarios that use unlicensed spectrum.\nIn addition, dynamic radio frequency interference is expected to occur in the scenarios related to cognitive radio, where the networks and the devices should exhibit frequency-agile operation.\nChirp modulation can be seen as a form of frequency-hopping that simply scans through the available frequencies in consecutive order to communicate.\nFrequency hopping can be superimposed on other modulations or waveforms to enhance the system performance.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46891", "revid": "3987679", "url": "https://en.wikipedia.org/wiki?curid=46891", "title": "Sansei Japanese", "text": ""}
{"id": "46892", "revid": "14650386", "url": "https://en.wikipedia.org/wiki?curid=46892", "title": "Issei", "text": "First generation of Japanese people who immigrated to the Americas\n are Japanese immigrants to countries in North America and South America. The term is used mostly by ethnic Japanese. are born in Japan; their children born in the new country are (, \"two\", plus , \"generation\"); and their grandchildren are (, \"three\", plus , \"generation\").\nThe character and uniqueness of the is recognized in their social history.\nHistory.\nThe earliest organized group of Japanese emigrants settled in Mexico in 1897. In the 21st century, the four largest populations of diaspora Japanese and descendants of Japanese immigrants in the Western Hemisphere live in Brazil, the United States, Canada, and Peru.\nBrazilian.\nBrazil is home to the largest ethnic Japanese population outside Japan, numbering an estimated more than 1.5\u00a0million (including those of mixed-race or mixed-ethnicity), more than that of the 1.2\u00a0million in the United States. The Japanese Brazilians are an important part of Asian ethnic minorities in Brazil.\nAmerican.\nThe first members of the emigrated not directly to the mainland United States, but to Hawaii. These emigrants\u2014the first of whom arrived on board the steamship \"City of Tokio\" in February 1885\u2014were common laborers escaping hard times in Japan to work in Hawai'i. Their immigration was subsidized by the Hawaiian government, as cheap labor was needed for important commodity crops, especially its sugar plantations. Numerous Japanese eventually settled in Hawaii.\nEmigration of Japanese directly to the mainland began in 1885, when \"student-laborers\" landed on the West Coast of the United States. The earliest of these emigrated to San Francisco. Their numbers continually increased in the late 1880s and early 1890s. Their purpose in moving to America was to gain advanced knowledge and experience to develop the modern society at home. Both students and laborers were attracted by the image of the United States as a country that welcomed foreigners. When they first arrived in the U.S., they had not intended to live there permanently, but rather to learn from Americans and to take that knowledge back home. While they encountered discrimination, they also made opportunities, and many settled in California, and later in Washington and Oregon as well as Alaska (to a lesser degree).\nCanadian.\nWithin Japanese-Canadian communities across Canada, like their American counterparts, three distinct subgroups developed, each with different socio-cultural referents, generational identity, and wartime experiences. The narrative of Japanese-Canadians include post-Pearl Harbor experiences of uprooting, incarceration, and dispersal of the pre-war Japanese-Canadian communities.\nPeruvian.\nAmong the approximately 100,000 (2021) Peruvians of Japanese descent living in Peru, the Japanese Peruvians comprise a small number.\nCultural profile.\nGenerations.\nJapanese-Americans and Japanese-Canadians have specific names for each of their generations in North America. These are formed by combining one of the Japanese numbers corresponding to the generation with the Japanese word for . The Japanese-American and Japanese-Canadian communities have themselves distinguished their members with terms like , , and , which describe the first, second and third generation of immigrants. The fourth generation is called and the fifth is called .\n is a Japanese-language term used by ethnic Japanese in countries in North America and South America to specify the Japanese people who were the first generation to immigrate there.\nOriginally, as mentioned above, these words were themselves common nouns in Japan referred to generations or reigns. So they are also still used in Japanese terms for personal names, such as means Queen Elizabeth II. Within the ethnic Japanese immigrant community they had come to characterize their own generations.\nThe , , and generations reflect distinctly different attitudes to authority, gender, involvement with non-Japanese, religious belief and practice, and other matters. The age when individuals faced the wartime evacuation and internment during World War II has been found to be the most significant factor that explains such variations in attitudes and behaviour patterns.\nThe term encompasses all of the world's Japanese immigrants across generations. The collective memory of the and older was an image of Meiji Japan from 1870 through 1911. Newer immigrants carry very different memories of more recent Japan. These differing attitudes, social values and associations with Japan were often incompatible with each other. The significant differences in post-war experiences and opportunities did nothing to mitigate the gaps which separated generational perspectives.\nIn North America, since the redress victory in 1988, a significant evolutionary change has occurred. The , their parents and their children are changing the way they look at themselves and their pattern of accommodation to the non-Japanese majority.\nThere are just over one hundred thousand British Japanese, mostly in London. Unlike other communities in the world, these Britons do not identify themselves in such generational terms as , , or .\nIssei.\nThe first generation of immigrants, born in Japan before emigrating, is called \"Issei\" (\u4e00\u4e16). In the 1930s, the term \"Issei\" came into common use, replacing the term \"immigrant\" (\"ijusha\"). This new term illustrated a changed way of looking at themselves. The term \"Issei\" represented the idea of beginning, a psychological transformation relating to being settled, having a distinctive community, and the idea of belonging to the new country.\n\"Issei\" settled in close ethnic communities, and therefore did not learn English. They endured great economic and social losses during the early years of World War II, and they were unable to rebuild their lost businesses and savings. The external circumstances tended to reinforce the pattern of \"Issei\" being predominantly friends with other \"Issei.\"\nUnlike their children, they tend to rely primarily on Japanese-language media (newspapers, television, movies), and in some senses, they tend to think of themselves as more Japanese than Canadian or American.\n\"Issei\" women.\n\"Issei\" women's lives were somewhat similar, despite differences in context, because they were structured within interlocking webs of patriarchal relationships, and that consistent subordination was experienced both as oppressive and as a source of happiness. The \"Issei\" women lived lives of transition which were affected by three common factors: the dominant ideology of late \"Meiji\" Japan, which advanced the economic objectives of the Japanese state; the patriarchal traditions of the agricultural village, which arose partly as a form of adjustment to national objectives and the adjustment to changes imposed by modernization; and the constraints which arose within a Canadian or American society dominated by racist ideology. Substantive evidence of the working lives of \"Issei\" women is very difficult to find, partly for lack of data and partly because the data that do exist are influenced by their implicit ideological definition of women. In Hawai\u2018i, \"Issei\" women worked as washerwomen, midwives, and barbers, providing essential services to the growing immigrant population. \"Issei\" women were instrumental in fostering social cohesion and preserving Japanese culture through the establishment of community organizations. Shizue Iwatsuki founded the Japanese Women\u2019s Society in Hood River, Oregon, which provided a vital social network for Japanese immigrant women while ensuring the continuation of cultural traditions. \n\"Issei\" women divided their time between working and keeping house. Many described their lives as a constant cycle of labor, balancing agricultural work with domestic responsibilities. They frequently referred to their husbands as \"Meiji men,\" describing them as embodying the patriarchal ideals of late Meiji Japan. These men often avoided household or childcare duties, leaving Issei women to shoulder most of the physical and emotional labor.\nAging.\nThe \"kanreki\" (\u9084\u66a6), a traditional, pre-modern Japanese rite of passage to old age at 60, was sometimes celebrated by the \"Issei\" and is now being celebrated by increasing numbers of \"Nisei.\" Rituals are enactments of shared meanings, norms, and values; and this Japanese rite of passage highlights a collective response among the Nisei to the conventional dilemmas of growing older.\nJapanese-American photographer Mary Koga documented elderly first generation immigrants in her \"Portrait of the Issei in Illinois\", taken between 1986 and 1989.\nHistory.\nThe experience of emigrants is inevitably affected by a range of factors directly related to the Japanese society they left behind. As immigrants, the conflicts between the old country and the new played out in unique ways for each individual, and yet common elements do begin to appear in the history of the Japanese Canadian and Japanese American communities.\nEmigrants from Japan.\nJapan was a closed country for more than two centuries, 1636 to 1853, since military rulers from the Tokugawa family wanted to keep foreigners away from Japanese society. The only exceptions were Chinese and some Dutch, but even they were discouraged from associating with Japanese citizens. Also, it was strictly prohibited by law for ordinary Japanese citizens to go abroad. Change came around the early 19th century when the visit of an American fleet commanded by Commodore Perry caused the new Japanese government to replace the Tokugawa system of economics and politics during the Meiji era to open its door to trade and contact with the outside world.\nAfter 1866, the new Japanese government decided to send students and laborers to the U.S. to bring back the knowledge and experience necessary for the nation to grow strong.\nAfter 1884, emigration of working classes was permitted; and the first issei began to arrive in North and South America soon after. For example, in 1890, only 25 Issei lived in Oregon. By 1891, 1,000 Japanese lived in Oregon. In 1900, 2,051 Japanese had come to live in Oregon. By 1915, Japanese men with savings of $800 were considered eligible to summon wives from Japan.\nImmigrants in the United States.\nFew Japanese workers came to North America intending to become immigrants. Initially, most of them came with vague plans for gaining new experiences and for making some money before returning to homes in Japan. This group of workers was overwhelmingly male. Many \"Issei\" arrived as laborers. They worked in employment sectors such as agriculture, mining, and railroad construction.\nThe Issei were born in Japan, and their cultural perspective was primarily Japanese; but they were in America by choice. Despite a certain nostalgia for the old country, they had created homes in a country far from Japan. If they had not been prohibited from becoming citizens, many would have become citizens of the United States.\nIn 1913, California's Alien Land Law prohibited non-citizens from owning land in the state, and several other states soon after passed their own restrictive alien land laws. This included the \"Issei\", Japanese residents born in Japan, but not their children, the Nisei, who were born in United States or Hawaii, and who therefore were American citizens by birth. Many of the Issei responded to the law by transferring title to their land to their \"Nisei\" children.\nAmericans' first impression of Issei.\nAmericans generally viewed the \"Issei\" as a crude, ill-educated lot. Possible reasons for this may be the fact that most Japanese were forced to work in menial jobs in the U.S., such as farming. Many Issei were in fact better educated than either the Japanese or American public. Sixty percent had completed middle school, and 21 percent were high school graduates. \nWhether Christian, Buddhists, or nonbelievers, the \"Issei\" almost never caused trouble in the civil authority. The arrest rate for the \"Issei\" from 1902 to the 1960s was relatively lower than for any other major ethnic group in California. The only exceptions were that some young \"Issei\" committed crimes relating to gambling and prostitution, which stemmed from different cultural morals in Japan.\nRacial segregation and immigration law.\nThe post-1900 cause to renew the Chinese Exclusion Act became generalized protests against all Asian immigrants, including the Issei. Since Chinese immigration to the U.S. was largely limited, hostility fell on the \"Issei.\" American labor organizations took an initiative in spreading anti-Japanese sentiment. White Americans wanted to exclude them since they did not want any Asians to take their jobs away. As a result, they formed the Asiatic Exclusion League that viewed Japanese and Chinese as a threat of American workers. The protest of the league involved picketing and beatings of the Issei. In October 1906, amid this anti-Japanese milieu, the San Francisco School Board, carrying out a campaign promise of the mayor, ordered all Japanese and Korean pupils to join the Chinese students at a segregated school. The \"Issei\" were displeased with the situation and some reported to Japanese newspapers. This caused the Japanese government to protest against the former president, Theodore Roosevelt, and as a result, they signed the Gentlemen's Agreement of 1907. This agreement led the period of settling and family building to come.\nBy 1911, almost half of the Japanese immigrants were women who landed in the U.S. to reunite with their husbands. After the Gentleman's agreement, a number of \"Nisei\", the second-generation Japanese, were born in California. Yet, it did not stop some white Americans from segregating Japanese immigrants. The \"Issei\" were a role model of American citizens by being hardworking, law-abiding, devoted to family and the community. However, some Americans did not want to admit the virtues of the \"Issei.\"\nThe Immigration Act of 1924 represented the Issei's failed struggle against the segregation. The experiences of the Issei extend from well before the period before 1 July 1924, when the Japanese Exclusion Act came into effect.\nThe \"Issei,\" however, were very good at enhancing rice farming on \"unusable\" land. Japanese Californian farmers made rice a major crop of the state. The largest \"Issei\" community settled around Vacaville, California, near San Francisco.\nInternment.\nWhen the Canadian and American governments interned West Coast Japanese in 1942, neither distinguished between those who were citizens (\"Nisei\") and their non-citizen parents (\"Issei\"). When the apology and redress for injustices were enacted by the American Congress and the Canadian Parliament in 1988, most of the \"Issei\" were dead, or too old for it to make any significant difference in lives that had been disrupted.\nNotable individuals.\nThe number of \"issei\" who have earned some degree of public recognition has continued to increase over time; but the quiet lives of those whose names are known only to family and friends are no less important in understanding the broader narrative of the \"nikkei.\" Although the names highlighted here are over-represented by \"issei\" from North America, the Latin American member countries of the Pan American Nikkei Association (PANA) include Argentina, Bolivia, Brazil, Chile, Colombia, Mexico, Paraguay, Peru and Uruguay, in addition to the English-speaking United States and Canada.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46893", "revid": "204692", "url": "https://en.wikipedia.org/wiki?curid=46893", "title": "Sansei", "text": "Grandchildren of Japanese-born emigrants\n is a Japanese and North American English term used in parts of the world (mainly in South America and North America) to refer to the children of children born to ethnically Japanese emigrants (\"Issei\") in a new country of residence, outside of Japan. The \"nisei\" are considered the second generation, while grandchildren of the Japanese-born emigrants are called \"Sansei\". The fourth generation is referred to as \"yonsei\". The children of at least one \"nisei\" parent are called \"Sansei\"; they are usually the first generation of whom a high percentage are mixed-race, given that their parents were (usually), themselves, born and raised in America.\nThe character and uniqueness of the \"sansei\" is recognized in its social history.\nBy country.\nAlthough the earliest organized group of Japanese emigrants settled in Mexico in 1897, the four largest populations of Japanese and their descendants are in Brazil, the United States, Canada, and Peru.\nBrazilian \"Sansei\".\nBrazil is home to the largest Japanese population outside of Japan, with an estimate of more than 1.5 million people (including those of mixed-race or mixed-ethnicity), more than that of the 1.2 million in the United States. The \"Sansei\" Japanese of Brazil are an important ethnic minority in the South American nation.\nAmerican \"Sansei\".\nMost American \"Sansei\" were born during the Baby Boom after the end of World War II; older \"Sansei\", who were living in the western United States during the war, were forcibly incarcerated with their parents (\"Nisei\") and grandparents (\"Issei\") after Executive Order 9066 was promulgated to exclude everyone of Japanese descent from the West Coast and from Southern Arizona. The \"Sansei\" were forceful activists in the redress movement of the 1980s, which resulted in an official apology to the internees. In some senses, the \"Sansei\" seem to feel they are caught in a dilemma between their \"quiet\" Nisei parents and their other identity model of \"verbal\" and outspoken Americans.\nIn the United States, an iconic \"Sansei\" is General Eric Shinseki (born November 28, 1942, 34th Chief of Staff of the United States Army (1999\u20132003) and former United States Secretary of Veterans Affairs. He is the first Asian American in U.S. history to be a four-star general, and the first to lead one of the four U.S. military services.\nCanadian \"Sansei\".\nWithin Japanese-Canadian communities across Canada, three distinct subgroups developed, each with different sociocultural referents, generational identities, and wartime experiences.\nPeruvian \"Sansei\".\nAmong the approximately 80,000 Peruvians of Japanese descent, the \"Sansei\" Japanese Peruvians comprise the largest number. Former Peruvian President Alberto Fujimori, who was in office from 28 July 1990 until 22 November 2000, was the \"nisei\" son of \"Issei\" emigrants from Kumamoto City, Kumamoto Prefecture, Japan.\nCultural profile.\nGenerations.\nJapanese-Americans and Japanese-Canadians have special names for each of their generations in North America. These are formed by combining one of the Japanese numbers corresponding to the generation with the Japanese word for generation (\"sei\" \u4e16). The Japanese-American and Japanese-Canadian communities have themselves distinguished their members with terms like \"Issei\", \"Nisei\" and \"Sansei\" which describe the first, second and third generation of immigrants. The fourth generation is called \"Yonsei\" (\u56db\u4e16) and the fifth is called \"Gosei\" (\u4e94\u4e16). The \"Issei\", \"Nisei\" and \"Sansei\" generations reflect distinctly different attitudes to authority, gender, non-Japanese involvement, religious belief and practice and other matters. The age when individuals faced the wartime evacuation and internment is the single, most significant factor which explains these variations in their experiences, attitudes and behaviour patterns. \nThe term \"Nikkei\" (\u65e5\u7cfb) encompasses all of the world's Japanese immigrants across generations. The collective memory of the \"Issei\" and older \"Nisei\" was an image of Meiji Japan from 1870 through 1911, which contrasted sharply with the Japan that newer immigrants had more recently left. These differing attitudes, social values and associations with Japan were often incompatible with each other. In this context, the significant differences in post-war experiences and opportunities did nothing to mitigate the gaps which separated generational perspectives.\nIn North America since the redress victory in 1988, a significant evolutionary change has occurred. The \"Sansei\", their parents, their grandparents, and their children are changing the way they look at themselves and their pattern of accommodation to the non-Japanese majority.\nThere are currently just over one hundred thousand British Japanese, mostly in London; but unlike other \"Nikkei\" communities elsewhere in the world, these Britons do not conventionally parse their communities in generational terms as \"Issei\", \"Nisei\" or \"Sansei\".\nSansei.\nThe third generation of immigrants, born in the United States or Canada to parents born in the United States or Canada, is called \"Sansei\" (\u4e09\u4e16). Children born to the \"Nisei\" were generally born after 1945. They speak English as their first language and are completely acculturized in the contexts of Canadian or American society. They tend to identify with Canadian or American values, norms and expectations. Few speak Japanese and most tend to express their identity as Canadian or American rather than Japanese. Among the \"Sansei\" there is an overwhelming percentage of marriages to persons of non-Japanese ancestry.\nAging.\nThe \"kanreki\" (\u9084\u66a6), a traditional, pre-modern Japanese rite of passage to old age at 60, was sometimes celebrated by the \"Issei\" and is now being celebrated by increasing numbers of \"Nisei\" and a few \"Sansei\". Rituals are enactments of shared meanings, norms, and values and this Japanese rite of passage highlights a collective response among the Nisei to the conventional dilemmas of growing older.\nHistory.\nInternment and redress.\nSome responded to internment with lawsuits and political action; and for others, poetry became an unplanned consequence:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nPolitics.\nThe \"sansei\" became known as the \"activist generation\" because of their large hand in the redress movement and individuals that have become a part of the American mainstream political landscape.\nNotable individuals.\nThe numbers of \"sansei\" who have earned some degree of public recognition has continued to increase over time; but the quiet lives of those whose names are known only to family and friends are no less important in understanding the broader narrative of the \"Nikkei.\" Although the names highlighted here are over-represented by \"sansei\" from North America, the Latin American member countries of the Pan American Nikkei Association (PANA) include Argentina, Bolivia, Brazil, Chile, Colombia, Mexico, Paraguay, Peru, Uruguay, in addition to the English-speaking United States and Canada.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46894", "revid": "346308", "url": "https://en.wikipedia.org/wiki?curid=46894", "title": "Pseudonoise", "text": ""}
{"id": "46895", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=46895", "title": "Euler numbers", "text": "Integers occurring in the coefficients of the Taylor series of 1/cosh t\nIn mathematics, the Euler numbers are a sequence \"En\" of integers (sequence in the OEIS) defined by the Taylor series expansion\nformula_1\nwhere formula_2 is the hyperbolic cosine function. The Euler numbers are related to a special value of the Euler polynomials, namely\nformula_3\nThe Euler numbers appear in the Taylor series expansions of the secant and hyperbolic secant functions. The latter is the function in the definition. They also occur in combinatorics, specifically when counting the number of alternating permutations of a set with an even number of elements.\nExamples.\nThe odd-indexed Euler numbers are all zero. The even-indexed ones (sequence in the OEIS) have alternating signs. Some values are:\nSome authors re-index the sequence in order to omit the odd-numbered Euler numbers with value zero, or change all signs to positive (sequence in the OEIS). This article adheres to the convention adopted above.\nExplicit formulas.\nIn terms of Stirling numbers of the second kind.\nThe following two formulas express the Euler numbers in terms of Stirling numbers of the second kind:\nformula_4\nformula_5\nwhere formula_6 denotes the Stirling numbers of the second kind, and formula_7 denotes the rising factorial.\nAs a recursion.\nThe Euler numbers can be defined by the recursion\nformula_8\nor equivalently\nformula_9\nBoth of these recursions can be found by using the fact that\nformula_10\nAs a double sum.\nThe following two formulas express the Euler numbers as double sums\nformula_11\nformula_12\nAs an iterated sum.\nAn explicit formula for Euler numbers is\nformula_13\nwhere i denotes the imaginary unit with \"i\"2 \n \u22121.\nAs a sum over partitions.\nThe Euler number \"E\"2\"n\" can be expressed as a sum over the even partitions of 2\"n\",\nformula_14\nas well as a sum over the odd partitions of 2\"n\" \u2212 1,\nformula_15\nwhere in both cases \"K\" \n \"k\"1 + \u00b7\u00b7\u00b7 + \"kn\" and\nformula_16\nis a multinomial coefficient. The Kronecker deltas in the above formulas restrict the sums over the ks to 2\"k\"1 + 4\"k\"2 + \u00b7\u00b7\u00b7 + 2\"nkn\" \n 2\"n\" and to \"k\"1 + 3\"k\"2 + \u00b7\u00b7\u00b7 + (2\"n\" \u2212 1)\"kn\" \n 2\"n\" \u2212 1, respectively. \nAs an example,\nformula_17\nAs a determinant.\n\"E\"2\"n\" is given by the determinant\nformula_18\nAs an integral.\n\"E\"2\"n\" is also given by the following integrals:\nformula_19\nCongruences.\nW. Zhang obtained the following combinational identities concerning the Euler numbers. For any prime formula_20, we have\nformula_21\nW. Zhang and Z. Xu proved that, for any prime formula_22 and integer formula_23, we have\nformula_24\nwhere formula_25 is the Euler's totient function.\nLower bound.\nThe Euler numbers grow quite rapidly for large indices, as they have the lower bound\n formula_26\nEuler zigzag numbers.\nThe Taylor series of formula_27 is\nformula_28\nwhere An is the Euler zigzag numbers, beginning with\n1, 1, 1, 2, 5, 16, 61, 272, 1385, 7936, 50521, 353792, 2702765, 22368256, 199360981, 1903757312, 19391512145, 209865342976, 2404879675441, 29088885112832, ... (sequence in the OEIS)\nFor all even n,\nformula_29\nwhere \"En\" is the Euler number, and for all odd n,\nformula_30\nwhere \"Bn\" is the Bernoulli number.\nFor every \"n\",\nformula_31\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46905", "revid": "6174461", "url": "https://en.wikipedia.org/wiki?curid=46905", "title": "Maclaurin series", "text": ""}
{"id": "46916", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=46916", "title": "Otter", "text": "Subfamily of mammals (Lutrinae)\nOtters are carnivorous mammals in the subfamily Lutrinae. The 14 extant otter species are all semiaquatic, both freshwater and marine. Lutrinae is a branch of the Mustelidae family, which includes weasels, badgers, mink, and wolverines, among other animals.\nOtters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones.\nOtters exhibit a varied life cycle with a gestation period of about 60\u201386 days, and offspring typically stay with their family for a year. They can live up to 16 years, with their diet mainly consisting of fish and sometimes frogs, birds, or shellfish, depending on the species. \nThere are 14 known species of otters, ranging in size and habitat preferences, with some species adapted to cold waters requiring a high metabolic rate for warmth. Otter-human interactions have varied over time, with otters being hunted for their pelts, used in fishing practices in southern Bangladesh, and occasionally attacking humans, though such incidents are rare and often a result of provocation. Otters hold a place in various cultures' mythology and religion, symbolizing different attributes and stories, from Norse mythology to Native American totems and Asian folklore, where they are sometimes believed to possess shapeshifting abilities.\nEtymology.\nThe word \"otter\" derives from the Old English word or . This and cognate words in other Indo-European languages ultimately stem from the Proto-Indo-European word , which also gave rise to the English word \"water\".\nTerminology.\nAn otter's den is called a holt, or couch. Male otters are called dogs or boars; females are called bitches or sows; and their offspring are called pups or cubs. The collective nouns for otters are bevy, family, lodge, romp (being descriptive of their often playful nature), or, when in water, raft.\nThe feces of otters are typically identified by their distinctive aroma, the smell of which has been described as ranging from freshly mown hay to putrefied fish; these are known as spraints.\nLife cycle.\nThe gestation period in otters is about 60 to 86 days. The newborn pup is cared for by the bitch, dog, and older offspring. Female otters reach sexual maturity at approximately two years of age and males at approximately three years. The holt is built under tree roots or a rocky cairn, more common in Scotland. It is lined with moss and grass.\nAfter one month, the pup can leave the holt and after two months, it is able to swim. The pup lives with its family for approximately one year. Otters live up to 16 years; they are by nature playful, and frolic in the water with their pups. Its usual source of food is fish, and further downriver, eels, but it may sample frogs and birds.\nDescription.\nOtters have long, slim bodies and relatively short limbs. Their most striking anatomical features are the powerful webbed feet used to swim, and their seal-like abilities for holding breath underwater. Most have sharp claws on their feet and all except the sea otter have long, muscular tails. The 13 species range in adult size from in length and in weight. The Asian small-clawed otter is the smallest otter species and the giant otter and sea otter are the largest. They have very soft, insulated underfur, which is protected by an outer layer of long guard hairs. This traps a layer of air which keeps them dry, warm, and somewhat buoyant under water.\nSeveral otter species live in cold waters and have high metabolic rates to help keep them warm. Eurasian otters must eat 15% of their body weight each day, and sea otters 20 to 25%, depending on the temperature. In water as warm as , an otter needs to catch of fish per hour to survive. Most species hunt for three to five hours each day and nursing mothers up to eight hours each day.\nFeeding.\nFor most otters, fish is the staple of their diet. This is often supplemented by frogs, crayfish and crabs. Some otters are experts at opening shellfish, and others will feed on available small mammals or birds. Prey-dependence leaves otters very vulnerable to prey depletion. Sea otters are hunters of clams, sea urchins and other shelled creatures. They are notable for their ability to use stones to break open shellfish on their bellies. This skill must be learned by the young.\nOtters are active hunters, chasing prey in the water or searching the beds of rivers, lakes or the seas. Most species live beside water, but river otters usually enter it only to hunt or travel, otherwise spending much of their time on land to prevent their fur becoming waterlogged. Sea otters are considerably more aquatic and live in the ocean for most of their lives.\nOtters are playful animals and appear to engage in various behaviors for sheer enjoyment, such as making waterslides and sliding on them into the water. They may also find and play with small stones. Different species vary in their social structure, some being largely solitary, while others live in groups\u00a0\u2013 in a few species these groups may be fairly large.\nSpecies.\nExtinct taxa.\nSubfamily Lutrinae\nRelation with humans.\nHunting.\nOtters have been hunted for their pelts from at least the 1700s, although it may have begun well before then. Early hunting methods included darts, arrows, nets and snares but later, traps were set on land and guns used.\nThere has been a long history of otter pelts being worn around the world. In China it was standard for the royalty to wear robes made from them. People that were financially high in status also wore them. The tails of otters were often made into items for men to wear. These included hats and belts. Even some types of mittens for children have been made from the fur of otters.\nOtters have also been hunted using dogs, especially the otterhound. From 1958 to 1963, the 11 otter hunts in England and Wales killed 1,065 otters between them. In such hunts, the hunters notched their poles after every kill. The prized trophy that hunters would take from the otters was the baculum, which would be worn as a tie-pin.\nTraffic (the wildlife trade monitoring network) reported that otters are at serious risk in Southeast Asia and have disappeared from parts of their former range. This decline in populations is due to hunting to supply the demand for skins.\nFishing for humans.\nFor many generations, fishermen in southern Bangladesh have bred smooth-coated otters and used them to chase fish into their nets. Once a widespread practice, passed down from father to son throughout many communities in Asia, this traditional use of domesticated wild animals is still in practice in the district of Narail, Bangladesh.\nAttacks on humans.\nA 2011 review by the IUCN/SSC Otter Specialist Group showed that otter attacks reported between 1875 and 2010 occurred most often in Florida, where human and otter populations have substantially increased since 2000, with the majority involving the North American river otter. At least 42 instances of attack were found, including one resulting in death and another case of serious injury. Attacking otters had rabies in 36% of anecdotal reports. 80% of otter bite victims do not seek medical treatment.\nAnimal welfare groups say that, unless threatened, otters rarely attack humans. In November 2021, about 20 river otters ambushed a British man in his 60s during an early morning walk in Singapore Botanic Gardens. Despite weighing over 200 pounds, he was trampled and bitten and could not stand up without help from a nearby rescuer. The man speculated that another runner might have stepped on one of the animals earlier, and wished that there could be more lighting installed at that location.\nReligion and mythology.\nNorse mythology tells of the dwarf \u00d3tr habitually taking the form of an otter. The myth of \"Otter's Ransom\" is the starting point of the Volsunga saga.\nIn Irish mythology, the character L\u00ed Ban was turned from a woman into a mermaid, half human and half salmon, and given three hundred years of life to roam the oceans. Her lapdog assumed the form of an otter and shared her prolonged lifetime and her extensive wanderings.\nIn some Native American cultures, otters are considered totem animals.\nThe otter is held to be a clean animal belonging to Ahura Mazda in Zoroastrian belief, and taboo to kill.\nIn popular Korean mythology, it is told that people who see an otter (\"soodal\") will attract 'rain clouds' for the rest of their lives.\nIn the Buddhist Jataka tales, The Otters and The Wolf, two otters agreed to let a wolf settle their dispute in dividing their caught fish but it was taken away by the cunning wolf.\nJapanese folklore.\nIn Japanese, otters are called \"kawauso\" (). In Japanese folklore, they fool humans in the same way as foxes (kitsune) and tanuki.\nIn the Noto region, Ishikawa Prefecture, there are stories where they shapeshift into beautiful women or children wearing checker-patterned clothing. If a human attempts to speak to one, they will answer \"oraya\" and then answer \"araya,\" and if anybody asks them anything, they say cryptic things like \"kawai.\" There are darker stories, such as one from Kaga Province (now Ishikawa Prefecture) in which an otter that lives in the castle's moat shapeshifts into a woman, invites males, and then kills and eats them.\nIn the kaidan, essays, and legends of the Edo period like the \"Urami Kanawa\" (), \"Taihei Hyaku Monogatari\" (), and the \"Shifu Goroku\" (), there are tales about strange occurrences like otters that shapeshift into beautiful women and kill men.\nIn the town of Numatachi, Asa District, Hiroshima Prefecture (now Hiroshima), they are called \"tomo no kawauso\" () and \"ato no kawauso\" (). It is said that they shapeshift into b\u014dzu (a kind of monk) and appear before passers-by, and if the passer-by tries to get close and look up, its height steadily increases until it becomes a large b\u014dzu.\nIn the Tsugaru region, Aomori Prefecture, they are said to possess humans. It is said that those possessed by otters lose their stamina as if their soul has been extracted. They are also said to shapeshift into severed heads and get caught in fishing nets.\nIn the Kashima District and the Hakui District in Ishikawa Prefecture, they are seen as a y\u014dkai under the name \"kabuso\" or \"kawaso\". They perform pranks like extinguishing the fire of the paper lanterns of people who walk on roads at night, shapeshifting into a beautiful woman of 18 or 19 years of age and fooling people, or tricking people and making them try to engage in sumo against a rock or a tree stump. It is said that they speak human words, and sometimes people are called and stopped while walking on roads.\nIn the Ishikawa and Kochi Prefectures, they are said to be a type of kappa, and there are stories told about how they engage in sumo with otters. In places like the Hokuriku region, Kii, and Shikoku, the otters are seen as a type of kappa. In the Kagakush\u016b, a dictionary from the Muromachi period, an otter that grew old becomes a kappa.\nIn an Ainu folktale, in Urashibetsu (in Abashiri, Hokkaido), there are stories where monster otters shapeshift into humans, go into homes where there are beautiful girls, and try to kill the girl and make her its wife.\nIn China, like in Japan, there are stories where otters shapeshift into beautiful women in old books like \"In Search of the Supernatural\" and the \"Zhenyizhi\" ().\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "46918", "revid": "50565511", "url": "https://en.wikipedia.org/wiki?curid=46918", "title": "Egyptian language", "text": "Extinct language in Egypt\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nThe Egyptian language, or Ancient Egyptian (; 'speech of Egypt'), is an extinct branch of the Afro-Asiatic language family that was spoken in ancient Egypt. It is known today from a large corpus of surviving texts, which were made accessible to the modern world following the decipherment of the ancient Egyptian scripts in the early 19th century.\nEgyptian is one of the earliest known written languages, first recorded in the hieroglyphic script in the late 4th millennium BC. It is also the longest-attested human language, with a written record spanning over 4,000 years. Its classical form, known as \"Middle Egyptian,\" served as the vernacular of the Middle Kingdom of Egypt and remained the literary language of Egypt until the Roman period.\nBy the time of classical antiquity, the spoken language had evolved into Demotic: its formation and development as a separate language from the Old Egyptian was strongly influenced by Aramaic and Ancient Greek.\nBy the Roman and Byzantine eras, the language later further diversified into various Coptic dialects written in the Greek alphabet. These were eventually supplanted by Arabic after the Muslim conquest of Egypt, although Bohairic Coptic remains in use as the liturgical language of the Coptic Church. and Semitic languages, particularly Arabic (which is spoken in Egypt today) and Hebrew. However, other scholars have argued that the Egyptian language shared closer linguistic ties with northeastern African regions.\nThere are two theories that seek to establish the cognate sets between Egyptian and Afroasiatic, the traditional theory and the \"neuere Komparatistik\", founded by Semiticist Otto R\u00f6ssler.\nAccording to the , in Egyptian, the Proto-Afroasiatic voiced consonants developed into pharyngeal \u27e8\ua725\u27e9 : Egyptian 'portal', Semitic 'door'. The traditional theory instead disputes the values given to those consonants by the , instead connecting \u27e8\ua725\u27e9 with Semitic and . Both schools agree that Afroasiatic merged with Egyptian \u27e8n\u27e9, \u27e8r\u27e9, \u27e8\ua723\u27e9, and \u27e8j\u27e9 in the dialect on which the written language was based, but it was preserved in other Egyptian varieties. They also agree that original palatalise to \u27e8\u1e6f j \u1e0f\u27e9 in some environments and are preserved as \u27e8k g q\u27e9 in others.\nThe Egyptian language has many biradical and perhaps monoradical roots, in contrast to the Semitic preference for triradical roots. Egyptian is probably more conservative, and Semitic likely underwent later regularizations converting roots into the triradical pattern.\nAlthough Egyptian is the oldest Afroasiatic language documented in written form, its morphological repertoire is very different from that of the rest of the Afroasiatic languages in general, and Semitic languages in particular. There are multiple possibilities: perhaps Egyptian had already undergone radical changes from Proto-Afroasiatic before it was recorded; or the Afroasiatic family has so far been studied with an excessively Semitocentric approach; or, as G. W. Tsereteli suggests, Afroasiatic is a sprachbund, rather than a true genetic language family.\nHistory.\nThe Egyptian language can be grouped thus:\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nThe Egyptian language is conventionally grouped into six major chronological divisions:\nOld, Middle, and Late Egyptian were all written using both the hieroglyphic and hieratic scripts. \nDemotic is the name of the script derived from the hieratic beginning in the 7th century BC.\nThe Coptic alphabet was derived from the Greek alphabet, with adaptations for Egyptian phonology. It was first developed in the Ptolemaic period, and gradually replaced the Demotic script in about the 4th to 5th centuries of the Christian era.\nOld Egyptian.\nThe term \"Archaic Egyptian\" is sometimes reserved for the earliest use of hieroglyphs, from the late fourth through the early third millennia BC. At the earliest stage, around 3300 BC, hieroglyphs were not a fully developed writing system, being at a transitional stage of proto-writing; over the time leading up to the 27th century BC, grammatical features such as nisba formation can be seen to occur.\nOld Egyptian is dated from the oldest known complete sentence, including a finite verb, which has been found. Discovered in the tomb of Seth-Peribsen (dated c.\u20092690 BC), the seal impression reads:\n\nExtensive texts appear from about 2600 BC. An early example is the Diary of Merer. The Pyramid Texts are the largest body of literature written in this phase of the language. One of its distinguishing characteristics is the tripling of ideograms, phonograms, and determinatives to indicate the plural. Overall, it does not differ significantly from Middle Egyptian, the classical stage of the language, though it is based on a different dialect.\nIn the period of the 3rd dynasty (c.\u20092650\u00a0\u2013 c.\u20092575 BC), many of the principles of hieroglyphic writing were regularized. From that time on, until the script was supplanted by an early version of Coptic (about the third and fourth centuries), the system remained virtually unchanged. Even the number of signs used remained constant at about 700 for more than 2,000 years.\nMiddle Egyptian.\nMiddle Egyptian was spoken for about 700 years, beginning around 2000 BC, during the Middle Kingdom and the subsequent Second Intermediate Period. As the classical variant of Egyptian, Middle Egyptian is the best-documented variety of the language, and has attracted the most attention by far from Egyptology. While most Middle Egyptian is seen written on monuments by hieroglyphs, it was also written using a cursive variant, and the related hieratic.\nMiddle Egyptian first became available to modern scholarship with the decipherment of hieroglyphs in the early 19th century. The first grammar of Middle Egyptian was published by Adolf Erman in 1894, surpassed in 1927 by Alan Gardiner's work. Middle Egyptian has been well-understood since then, although certain points of the verbal inflection remained open to revision until the mid-20th century, notably due to the contributions of Hans Jakob Polotsky.\nThe Middle Egyptian stage is taken to have ended around the 14th century BC, giving rise to Late Egyptian. This transition was taking place in the later period of the Eighteenth Dynasty of Egypt (known as the Amarna Period).\nEgyptien de tradition.\nOriginal Old Egyptian and Middle Egyptian texts were still used after the 14th century BCE. And an emulation of predominately Middle Egyptian, but also with characteristics of Old Egyptian, Late Egyptian and Demotic, called \"\" or \"Neo-Middle Egyptian\" by scholars, was used as a literary language for new texts since the later New Kingdom in official and religious hieroglyphic and hieratic texts in preference to Late Egyptian or Demotic. \"\u00c9gyptien de tradition\" as a religious language survived until the Christianisation of Roman Egypt in the 4th century.\nLate Egyptian.\nLate Egyptian was spoken for about 650 years, beginning around 1350 BC, during the New Kingdom of Egypt. Late Egyptian succeeded but did not fully supplant Middle Egyptian as a literary language, and was also the language of the New Kingdom administration.\nTexts written wholly in Late Egyptian date to the Twentieth Dynasty of Egypt and later. Late Egyptian is represented by a large body of religious and secular literature, comprising such examples as the \"Story of Wenamun\", the love poems of the Chester\u2013Beatty I papyrus, and the \"Instruction of Any\". Instructions became a popular literary genre of the New Kingdom, which took the form of advice on proper behavior. Late Egyptian was also the language of New Kingdom administration.\nLate Egyptian is not completely distinct from Middle Egyptian, as many \"classicisms\" appear in historical and literary documents of this phase. However, the difference between Middle and Late Egyptian is greater than the difference between Middle and Old Egyptian. Originally a synthetic language, Egyptian by the Late Egyptian phase had become an analytic language. The relationship between Middle Egyptian and Late Egyptian has been described as being similar to that between Latin and Italian. \nThe Late Egyptian stage is taken to have ended around the 8th century BC, giving rise to Demotic.\nDemotic.\nDemotic is a later development of the Egyptian language written in the Demotic script, following Late Egyptian and preceding Coptic, the latter of which it shares much with. In the earlier stages of Demotic, such as those texts written in the early Demotic script, it probably represented the spoken idiom of the time. However, as its use became increasingly confined to literary and religious purposes, the written language diverged more and more from the spoken form, leading to significant diglossia between the late Demotic texts and the spoken language of the time, similar to the use of classical Middle Egyptian during the Ptolemaic Period.\nCoptic.\nCoptic is the name given to the late Egyptian vernacular when it was written in a Greek-based alphabet, the Coptic alphabet; it flourished from the time of Early Christianity (c. 31/33\u2013324), but Egyptian phrases written in the Greek alphabet first appeared during the Hellenistic period c.\u20093rd century BC, with the first known Coptic text, still pagan (Old Coptic), from the 1st century AD.\nCoptic survived into the medieval period, but by the 16th century was dwindling rapidly due to the persecution of Coptic Christians under the Mamluks. It probably survived in the Egyptian countryside as a spoken language for several centuries after that. Coptic survives as the liturgical language of the Coptic Orthodox Church and the Coptic Catholic Church.\nDialects.\nSome evidence of dialectal variation in Egyptian is found in as early as the 3rd millennium BC. However, because the hieroglyphic scripts inherent conservatism and most hieroglyphic Egyptian texts are written in a literary prestige register rather than the vernacular speech variety of their author, the dialectical differences are not apparent in written Egyptian until the adoption of the Coptic alphabet. Nevertheless, it is clear that these differences existed before the Coptic period. In one Late Egyptian letter (dated c.\u20091200 BC), a scribe jokes that his colleague's writing is incoherent like \"the speech of a Delta man with a man of Elephantine.\"\nRecently, some evidence of internal dialects has been found in pairs of similar words in Egyptian that, based on similarities with later dialects of Coptic, may be derived from northern and southern dialects of Egyptian. Written Coptic has five major dialects, which differ mainly in graphic conventions, most notably the southern Saidic dialect, the main classical dialect, and the northern Bohairic dialect, currently used in Coptic Church services.\nPhonology.\nWhile the consonantal phonology of the Egyptian language may be reconstructed, the exact phonetics is unknown, and there are varying opinions on how to classify the individual phonemes. In addition, because Egyptian is recorded over a full 2,000 years, the Archaic and Late stages being separated by the amount of time that separates Old Latin from Modern Italian, significant phonetic changes must have occurred during that lengthy time frame.\nPhonologically, Egyptian contrasted labial, alveolar, palatal, velar, uvular, pharyngeal, and glottal consonants. Egyptian also contrasted voiceless and emphatic consonants, as with other Afroasiatic languages, but exactly how the emphatic consonants were realised is unknown. Early research had assumed that the opposition in stops was one of voicing, but it is now thought to be either one of tenuis and emphatic consonants, as in many Semitic languages, or one of aspirated and ejective consonants, as in many Cushitic languages.\nSince vowels were not written until Coptic, reconstructions of the Egyptian vowel system are much more uncertain and rely mainly on evidence from Coptic and records of Egyptian words, especially proper nouns, in other languages/writing systems.\nThe actual pronunciations reconstructed by such means are used only by a few specialists in the language. For all other purposes, the Egyptological pronunciation is used, but it often bears little resemblance to what is known of how Egyptian was pronounced.\nOld Egyptian.\nConsonants.\nThe following consonants are reconstructed for Archaic (before 2600 BC) and Old Egyptian (2686\u20132181 BC), with IPA equivalents in square brackets if they differ from the usual transcription scheme:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n has no independent representation in the hieroglyphic orthography, and it is frequently written as if it were or . That is probably because the standard for written Egyptian is based on a dialect in which had merged with other sonorants. Also, the rare cases of occurring are not represented. The phoneme is written as \u27e8\u27e9 in the initial position (\u27e8\u27e9 = 'father') and immediately after a stressed vowel (\u27e8\u27e9 = 'bad') and as \u27e8\u27e9 word-medially immediately before a stressed vowel (\u27e8\u1e2b\ua725jjk\u27e9 = 'you will appear') and are unmarked word-finally (\u27e8\u27e9 = 'father').\nMiddle Egyptian.\nIn Middle Egyptian (2055\u20131650 BC), a number of consonantal shifts take place. By the beginning of the Middle Kingdom period, and had merged, and the graphemes \u27e8s\u27e9 and \u27e8z\u27e9 are used interchangeably. In addition, had become word-initially in an unstressed syllable (\u27e8\u27e9 &gt; \"colour\") and after a stressed vowel (\u27e8\u1e25jpw\u27e9 &gt; '[the god] Apis').\nLate Egyptian.\nIn Late Egyptian (1069\u2013700 BC), the phonemes \"d \u1e0f g\" gradually merge with their counterparts \"t \u1e6f k\" (\u27e8dbn\u27e9 &gt; Akkadian transcription 'dbn-weight'). Also, \"\u1e6f \u1e0f\" often become , but they are retained in many lexemes; \"\ua723\" becomes ; and become at the end of a stressed syllable and eventually null word-finally: \u27e8p\u1e0f.t\u27e9 &gt; Akkadian transcription 'bow'.\nDemotic.\nPhonology.\nThe most important source of information about Demotic phonology is Coptic. The consonant inventory of Demotic can be reconstructed on the basis of evidence from the Coptic dialects. Demotic orthography is relatively opaque. The Demotic \"alphabetical\" signs are mostly inherited from the hieroglyphic script, and due to historical sound changes they do not always map neatly onto Demotic phonemes. However, the Demotic script does feature certain orthographic innovations, such as the use of the sign for //, which allow it to represent sounds that were not present in earlier forms of Egyptian.\nThe Demotic consonants can be divided into two primary classes: obstruents (stops, affricates and fricatives) and sonorants (approximants, nasals, and semivowels). Voice is not a contrastive feature; all obstruents are voiceless and all sonorants are voiced. Stops may be either aspirated or tenuis (unaspirated), although there is evidence that aspirates merged with their tenuis counterparts in certain environments.\nThe following table presents the consonants of Demotic Egyptian. The reconstructed value of a phoneme is given in IPA transcription, followed by a transliteration of the corresponding Demotic \"alphabetical\" sign(s) in angle brackets \u27e8\u27e9.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCoptic.\nMore changes occur in the 1st millennium BC and the first centuries AD, leading to Coptic (1st or 3rd \u2013 c. 19th centuries AD). In Sahidic \"\u1e96 \u1e2b \u1e25\" had merged into \u03e3 \"\u0161\" (most often from \"\u1e2b\") and \u03e9 (most often \"\u1e96 \u1e25\"). Bohairic and Akhmimic are more conservative and have a velar fricative (\u03e7 in Bohairic, \u2cc9 in Akhmimic). Pharyngeal \"*\ua725\" had merged into glottal after it had affected the quality of the surrounding vowels. is not indicated orthographically unless it follows a stressed vowel; then, it is marked by doubling the vowel letter (except in Bohairic): Akhmimic \u2cc9\u2c9f\u2c9f\u2ca1 , Sahidic and Lycopolitan \u03e3\u2c9f\u2c9f\u2ca1 \"\u0161o\u0294p\", Bohairic \u03e3\u2c9f\u2ca1 \"\u0161o\u0294p\" 'to be' &lt; \"\u1e2bpr.w\" * 'has become'. The phoneme \u2c83 was probably pronounced as a fricative , becoming \u2ca1 after a stressed vowel in syllables that had been closed in earlier Egyptian (compare \u2c9b\u2c9f\u2ca9\u2c83 &lt; 'gold' and \u2ca7\u2c81\u2ca1 &lt; * 'horn'). The phonemes occur only in Greek loanwords, with rare exceptions triggered by a nearby : \u2c81\u2c9b\u2c8d\u2c8f\u2c83\u2c89/\u2c81\u2c9b\u2ca5\u2c8f\u2c83\u2c89 &lt; \"\ua725.t n.t sb\ua723.w\" 'school'.\nEarlier \"*d \u1e0f g q\" are preserved as ejective \"t' c' k' k'\" before vowels in Coptic. Although the same graphemes are used for the pulmonic stops (\u27e8\u27e9), the existence of the former may be inferred because the stops \u27e8\u27e9 are allophonically aspirated before stressed vowels and sonorant consonants. In Bohairic, the allophones are written with the special graphemes \u27e8\u27e9, but other dialects did not mark aspiration: Sahidic \u2ca1\u2ca3\u2c8f, Bohairic \u2cab\u2ca3\u2c8f 'the sun'.\nThus, Bohairic does not mark aspiration for reflexes of older \"*d \u1e0f g q\": Sahidic and Bohairic \u2ca7\u2c81\u2ca1 'horn'. Also, the definite article \u2ca1 is unaspirated when the next word begins with a glottal stop: Bohairic \u2ca1 + \u2cb1\u2ca1 &gt; \u2ca1\u2cb1\u2ca1 'the account'.\nThe consonant system of Coptic is as follows:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nVowels.\nHere is the vowel system reconstructed for earlier Egyptian:\nVowels are always short in unstressed syllables (\u27e8tpj\u27e9 = 'first') and long in open stressed syllables (\u27e8rm\u1e6f\u27e9 = 'man'), but they can be either short or long in closed stressed syllables (\u27e8jnn\u27e9 = 'we', \u27e8mn\u27e9 = 'to stay').\nIn the Late New Kingdom, after Ramses II, around 1200 BC, changes to (like the Canaanite shift), \u27e8\u1e25rw\u27e9 '(the god) Horus' &gt; (Akkadian transcription: ). , therefore, changes to : \u27e8\u0161nj\u27e9 'tree' &gt; (Akkadian transcription: ).\nIn the Early New Kingdom, short stressed changes to : \u27e8mnj\u27e9 \"Menes\" &gt; (Akkadian transcription: ). Later, probably 1000\u2013800 BC, a short stressed changes to : \u27e8\u1e0f\ua725n.t\u27e9 \"Tanis\" was borrowed into Hebrew as *\u1e63u\u0295n but would become transcribed as \u27e8\u1e63e-e'-nu/\u1e63a-a'-nu\u27e9 during the Neo-Assyrian Empire.\nUnstressed vowels, especially after a stress, become : \u27e8nfr\u27e9 'good' &gt; (Akkadian transcription ). changes to next to and : \u27e8w\ua725w\u27e9 'soldier' &gt; (earlier Akkadian transcription: , later: ).\nIn Sahidic and Bohairic Coptic, Late Egyptian stressed becomes and becomes , but are unchanged in the other dialects:\nHowever, in the presence of guttural fricatives, Sahidic and Bohairic preserve , and Fayyumic renders it as \u27e8e\u27e9:\nIn Akhmimic and Lycopolitan, becomes before etymological :\nSimilarly, the diphthongs , , which normally have reflexes , in Sahidic and are preserved in other dialects, are in Bohairic \u27e8\u00f4i\u27e9 (in non-final position) and \u27e8\u00f4ou\u27e9 respectively:\nSahidic and Bohairic preserve before (etymological or from lenited or tonic-syllable coda ),: Sahidic and Bohairic \u27e8ne\u27e9 'to you (fem.)' &lt; &lt; . may also have different reflexes before sonorants, near sibilants and in diphthongs.\nOld surfaces as after nasals and occasionally other consonants: \u27e8n\u1e6fr\u27e9 'god' &gt; \u27e8noute\u27e9 has acquired phonemic status, as is evidenced by minimal pairs like 'to approach' \u27e8h\u00f4n\u27e9 &lt; \u1e96nn vs. 'inside' \u27e8houn\u27e9 &lt; \u1e96nw. An etymological &gt; often surfaces as next to and after etymological pharyngeals: \u27e8hir\u27e9 &lt; 'street' (Semitic loan).\nMost Coptic dialects have two phonemic vowels in unstressed position. Unstressed vowels generally became , written as \u27e8e\u27e9 or null (\u27e8i\u27e9 in Bohairic and Fayyumic word-finally), but pretonic unstressed /a/ occurs as a reflex of earlier unstressed near an etymological pharyngeal, velar or sonorant ('to become many' \u27e8a\u0161ai\u27e9 &lt; \ua725\u0161\ua723 ) or an unstressed . Pretonic [i] is underlyingly : Sahidic 'ibis' \u27e8hib\u00f4i\u27e9 &lt; h(j)bj.w .\nThus, the following is the Sahidic vowel system c. AD 400:\nPhonotactics.\nEarlier Egyptian has the syllable structure CV(\u02d0)(C) in which V is long in open stressed syllables and short elsewhere. In addition, CV\u02d0C or CVCC can occur in word-final, stressed position. However, CV\u02d0C occurs only in the infinitive of biconsonantal verbal roots, CVCC only in some plurals.\nIn later Egyptian, stressed CV\u02d0C, CVCC, and CV become much more common because of the loss of final dentals and glides.\nStress.\nEarlier Egyptian stresses one of the last two syllables. According to some scholars, that is a development from a stage in Proto-Egyptian in which the third-last syllable could be stressed, which was lost as open posttonic syllables lost their vowels: &gt; 'transformation'.\nEgyptological pronunciation.\nAs a convention, Egyptologists make use of an \"Egyptological pronunciation\" in English: the consonants are given fixed values, and vowels are inserted according to essentially arbitrary rules. Two of these consonants known as alef and ayin are generally pronounced as the vowel . Yodh is pronounced , \"w\" . Between other consonants, is then inserted. Thus, for example, the Egyptian name Ramesses is most accurately transliterated as (\"Ra is the one who bore him\") and pronounced as .\nIn transcription, \u27e8a\u27e9, \u27e8i\u27e9, and \u27e8u\u27e9 all represent consonants. For example, the name Tutankhamun (1341\u20131323 BC) was written in Egyptian as (\"living image of Amun\"). Experts have assigned generic sounds to these values as a matter of convenience, which is an artificial pronunciation and should not be mistaken for how Egyptian was ever pronounced at any time. So although is pronounced in modern Egyptological pronunciation, in his lifetime, it was likely to be pronounced something like *, transliterable as .\nWriting systems.\nMost surviving texts in the Egyptian language are written on stone in hieroglyphs. The native name for Egyptian hieroglyphic writing is (\"writing of the gods' words\").\nIn antiquity, most texts were written on the quite perishable medium of papyrus though a few have survived that were written in hieratic and (later) demotic. There was also a form of cursive hieroglyphs, used for religious documents on papyrus, such as the \"Book of the Dead\" of the Twentieth Dynasty; it was simpler to write than the hieroglyphs in stone inscriptions, but it was not as cursive as hieratic and lacked the wide use of ligatures. Additionally, there was a variety of stone-cut hieratic, known as \"lapidary hieratic\".\nIn the language's final stage of development, the Coptic alphabet replaced the older writing system.\nHieroglyphs are employed in two ways in Egyptian texts: as ideograms to represent the idea depicted by the pictures and, more commonly, as phonograms to represent their phonetic value.\nAs the phonetic realization of Egyptian cannot be known with certainty, Egyptologists use a system of transliteration to denote each sound that could be represented by a uniliteral hieroglyph.\nEgyptian scholar Gamal Mokhtar noted that the inventory of hieroglyphic symbols derived from \"fauna and flora used in the signs [which] are essentially African\", reflecting the local wildlife of North Africa, the Levant and southern Mediterranean. In \"regards to writing, we have seen that a purely Nilotic, hence [North] African origin not only is not excluded, but probably reflects the reality\" that the geographical location of Egypt is, of course, in Africa.\nMorphology.\nEgyptian is fairly typical for an Afroasiatic language in that most of its vocabulary is built around roots of three consonants, though there are sometimes only two consonants in the root: (, \"sun\"\u2014the is thought to have been something like a voiced pharyngeal fricative). Larger roots are also common and can have up to five consonants: (\"be upside-down\").\nVowels and other consonants are added to the root to derive different meanings, as Arabic, Hebrew, and other Afroasiatic languages still do. However, because vowels and sometimes glides are not written in any Egyptian script except Coptic, reconstructing the actual forms of words can be difficult. Thus, orthographic (\"to choose\"), for example, can represent the stative (whose endings can be left unexpressed), the imperfective forms or even a verbal noun (\"a choosing\").\nNouns.\nEgyptian nouns can be masculine or feminine (the latter is indicated, as with other Afroasiatic languages, by adding a ) and singular or plural ( / ), or dual ( / ).\nArticles, both definite and indefinite, do not occur until Late Egyptian but are used widely thereafter.\nPronouns.\nEgyptian has three different types of personal pronouns: suffix, enclitic (called \"dependent\" by Egyptologists) and independent pronouns. A number of verbal endings can also be added to the infinitive to form the stative and are regarded by some linguists as a \"fourth\" set of personal pronouns. They bear close resemblance to their Semitic counterparts. The three main sets of personal pronouns are as follows:\nDemonstrative pronouns have separate masculine and feminine singular forms and common plural forms for both genders:\nFinally, interrogative pronouns bear a close resemblance to their Semitic and Berber counterparts:\nVerbs.\nEgyptian verbs have finite and non-finite forms.\nFinite verbs convey person, tense/aspect, mood and voice. Each is indicated by a set of affixal morphemes attached to the verb: For example, the basic conjugation is (\"to hear\") is (\"he hears\").\nNon-finite verbs occur without a subject and are the infinitive, the participles and the negative infinitive, which \"\" calls \"negatival complement\". There are two main tenses/aspects in Egyptian: past and temporally-unmarked imperfective and aorist forms. The latter are determined from their syntactic context.\nAdjectives.\nAdjectives agree in gender and number with the nouns they modify:\n&lt;templatestyles src=\"Column/styles.css\"/&gt;\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\n&lt;templatestyles src = \"Column/styles.css\" /&gt;\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\nAttributive adjectives in phrases are after the nouns they modify: (\"[the] great god\").\nHowever, when they are used independently as a predicate in an adjectival phrase, as (\"[the] god [is] great\", lit. \"great [is the] god\"), adjectives precede the nouns they modify.\nPrepositions.\nEgyptian makes use of prepositions.\nAdverbs.\nAdverbs, in Egyptian, may appear at the end of a sentence. For example:\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\nAdverbs may also modify prepositions, in which case they precede the preposition they modify:\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\nAdverbs may also appear after adjectives to modify them:\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\nHere are some common Egyptian adverbs:\nSyntax.\nOld Egyptian, Classical Egyptian, and Middle Egyptian have verb-subject-object as the basic word order. For example, the equivalent of \"he opens the door\" would be (\"opens he [the] door\"). The so-called construct state combines two or more nouns to express the genitive, as in Semitic and Berber languages. However, that changed in the later stages of the language, including Late Egyptian, Demotic and Coptic.\nThe early stages of Egyptian have no articles, but the later forms use , and .\nAs with other Afroasiatic languages, Egyptian uses two grammatical genders: masculine and feminine. It also uses three grammatical numbers: singular, dual and plural. However, later Egyptian has a tendency to lose the dual as a productive form.\nLegacy.\nThe Egyptian language survived through the Middle Ages and into the early modern period in the form of the Coptic language. Coptic survived past the 16th century only as an isolated vernacular and as a liturgical language for the Coptic Orthodox and Coptic Catholic Churches. Coptic also had an enduring effect on Egyptian Arabic, which replaced Coptic as the main daily language in Egypt; the Coptic substratum in Egyptian Arabic appears in certain aspects of syntax and to a lesser degree in vocabulary and phonology.\nIn antiquity, Egyptian exerted some influence on Classical Greek, so that a number of Egyptian loanwords into Greek survive into modern usage. Examples include:\nThe Hebrew Bible also contains some words, terms, and names that are thought by scholars to be Egyptian in origin. An example of this is Zaphnath-Paaneah, the Egyptian name given to Joseph.\nThe etymological root of \"Egypt\" is the same as \"Copts\", ultimately from the Late Egyptian name of Memphis, \"Hikuptah\", a continuation of Middle Egyptian (lit. \"temple of the ka (soul) of Ptah\").\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nLiterature.\nOnline dictionaries.\nImportant Note: The old grammars and dictionaries of E. A. Wallis Budge have long been considered obsolete by Egyptologists, even though these books are still available for purchase.\nMore book information is available at Glyphs and Grammars."}
{"id": "46919", "revid": "41438237", "url": "https://en.wikipedia.org/wiki?curid=46919", "title": "Coptic", "text": "Coptic may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "46922", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=46922", "title": "Algore", "text": ""}
{"id": "46923", "revid": "1290264313", "url": "https://en.wikipedia.org/wiki?curid=46923", "title": "Nubian", "text": "Nubian may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
