{"id": "15033", "revid": "277086", "url": "https://en.wikipedia.org/wiki?curid=15033", "title": "Counties of Ireland", "text": "Administrative division of Ireland\n |subheaderstyle = font-weight: bold;\n |data1 = \n |data2 = Ireland (Republic of Ireland and Northern Ireland)\n |data3 = Provinces\n |data4 = {{br separated entries|||||\n |data5 = {{unbulleted list|||||\n |data6 = {{unbulleted list|||||\n |data7 = {{unbulleted list||||||\n |data9 = {{unbulleted list||||||\n |data10 = {{unbulleted list||||||\n |data11 = {{unbulleted list||||||\n |data12 = \n |data13 = \n |data14 = \n |data15 = {{unbulleted list|County councils (Republic of Ireland)|||||\n |data16 = {{unbulleted list||||||\nThe counties of Ireland (Irish: ) are historic administrative divisions of the island. They began as Norman structures, and as the powers exercised by the Cambro-Norman barons and the Old English nobility waned over time, new offices of political control were established at a county level. The number of counties varied depending on the time period, however thirty-two is the traditionally accepted and used number.\nIn 1921, upon the partition of Ireland, six of the traditional counties became part of Northern Ireland. In Northern Ireland, counties ceased to be used for local government in 1973. Districts are instead used. In the Republic of Ireland, some counties have been split, resulting in the creation of new counties: there are currently 26 counties, 3 cities and 2 cities and counties that demarcate areas of local government in the Republic.\nTerminology.\nThe word \"county\" has come to be used in different senses for different purposes. In common usage, it can mean the 32 counties that existed prior to 1838 \u2013 the so-called traditional counties, 26 of which are in the Republic of Ireland. The Local Government Acts define counties to include separate counties within the traditional county of Dublin.\nIn Ireland, the word \"county\" nearly always precedes the county name; thus \"\"County\" Roscommon\" in Ireland as opposed to \"Roscommon \"County\"\" in Michigan, United States. The former \"King's County\" and \"Queen's County\" were exceptions. These are now County Offaly and County Laois, respectively. The abbreviation Co. is used, as in \"Co. Roscommon\".\nThe counties in Dublin created in 1994 often drop the word \"county\" entirely, or use it after the name; thus, for example, internet search engines show many more uses, on Irish sites, of \"Fingal\" than of either \"County Fingal\" or \"Fingal County\". Although official guidance does not use the term county as part of its name, the local council uses all three forms. \nIn informal use, the word \"county\" is often dropped except where necessary to distinguish between county and town or city; thus \"Offaly\" rather than \"County Offaly\", but \"County Antrim\" to distinguish it from Antrim town. The synonym \"shire\" is not used for Irish counties, although the Marquessate of Downshire was named in 1789 after County Down.\nParts of some towns and cities were exempt from the jurisdiction of the counties that surrounded them. These towns and cities had the status of a county corporate, often granted by royal charter, which had all the judicial, administrative and revenue-raising powers of the regular counties.\nHistory.\nPre-Norman divisions of Ireland.\nThe political geography of Ireland can be traced with some accuracy from the 6th century. At that time Ireland was divided into a patchwork of petty kingdoms with a fluid political hierarchy which, in general, had three traditional grades of king. The lowest level of political control existed at the level of the (pl. ). A was an autonomous group of people of independent political jurisdiction under a r\u00ed t\u00faaithe, that is, a local petty king. About 150 such units of government existed. Each \"r\u00ed t\u00faaithe\" was in turn subject to a regional or \"over-king\" (). There may have been as many as 20 genuine ruiri in Ireland at any time.\nA \"king of over-kings\" () was often a provincial () or semi-provincial king to whom several ruiri were subordinate. No more than six genuine r\u00ed ruirech were ever contemporary. Usually, only five such \"king of over-kings\" existed contemporaneously and so are described in the Irish annals as \"fifths\" (). The areas under the control of these kings were: Ulster (), Leinster (), Connacht (), Munster () and Mide (). Later record-makers dubbed them \"provinces\", in imitation of Roman provinces. In the Norman period, the historic fifths of Leinster and Meath gradually merged, mainly due to the impact of the Pale, which straddled both, thereby forming the present-day province of Leinster.\nThe use of provinces as divisions of political power was supplanted by the system of counties after the Norman invasion. In modern times clusters of counties have been attributed to certain provinces but these clusters have no legal status. They are today seen mainly in a sporting context, as Ireland's four professional rugby teams play under the names of the provinces, and the Gaelic Athletic Association has separate Provincial councils and Provincial championships.\nPlantagenet era.\nLordships.\nWith the arrival of Cambro-Norman knights in 1169, the Anglo-Norman invasion of Ireland commenced. This was followed in 1172 by the invasion of King Henry II of England, commencing English royal involvement.\nAfter his intervention in Ireland, Henry II effectively divided the English colony into liberties also known as lordships. These were effectively palatine counties and differed from ordinary counties in that they were disjoined from the crown and that whoever they were granted to essentially had the same authority as the king and that the king's writ had no effect except a writ of error. This covered all land within the county that was not church land. The reason for the creation of such powerful entities in Ireland was due to the lack of authority the English crown had there. \nThe same process occurred after the Norman conquest of England where despite there being a strong central government, county palatines were needed in border areas with Wales and Scotland. In Ireland this meant that the land was divided and granted to Richard de Clare and his followers who became lords (and sometimes called earls), with the only land which the English crown had any direct control over being the sea-coast towns and territories immediately adjacent.\nOf Henry II's grants, at least three of them\u2014Leinster to Richard de Clare; Meath to Walter de Lacy; Ulster to John de Courcy\u2014were equivalent to palatine counties in their bestowing of royal jurisdiction to the grantees. Other grants include the liberties of Connaught and Tipperary.\nDivision of lordships.\nThese initial lordships were later subdivided into smaller \"liberties\", which appear to have enjoyed the same privileges as their predecessors. The division of Leinster and Munster into smaller counties is commonly attributed to King John, mostly due to a lack of prior documentary evidence, which has been destroyed. However, they may have had an earlier origin. These counties were: in Leinster: Carlow (also known as Catherlogh), Dublin, Kildare, Kilkenny, Louth (also known as Uriel), Meath, Wexford, Waterford; in Munster: Cork, Limerick, Kerry and Tipperary. It is thought that these counties did not have the administrative purpose later attached to them until late in the reign of King John and that no new counties were created until the Tudor dynasty.\nThe most important office in those that were palatine was that of seneschal. In those liberties that came under Crown control this office was held by a sheriff. The sovereign could appoint sheriffs in palatines. Their power was confined to the church lands, and they became known as sheriffs of a County of the Cross, of which there seem to have been as many in Ireland as there were counties palatine.\nThe exact boundaries of the liberties and shrievalties appear to have been in constant flux throughout the Plantagenet period, seemingly in line with the extent of English control. For example, in 1297 it is recorded that Kildare had extended to include the lands that now comprise the modern-day counties of Offaly, Laois (Leix) and Wicklow (Arklow). Some attempts had also been made to extend the county system to Ulster.\nThe Bruce Invasion of Ireland in 1315 resulted in the collapse of effective English rule in Ireland, with the land controlled by the crown continually shrinking to encompass Dublin, and parts of Meath, Louth and Kildare. Throughout the rest of Ireland, English rule was upheld by the earls of Desmond, Ormond, and Kildare (all created in the 14th-century), with the extension of the county system all but impossible. During the reign of Edward III (1327\u201377) all franchises, grants and liberties had been temporarily revoked with power passed to the king's sheriffs over the seneschals. This may have been due to the disorganisation caused by the Bruce invasion as well as the renouncing of the Connaught Burkes of their allegiance to the crown.\nThe Earls of Ulster divided their territory up into counties. These are not considered part of the Crown's shiring of Ireland. In 1333, the Earldom of Ulster is recorded as consisting of seven counties: Antrim, Blathewyc, Cragferus, Coulrath, del Art, Dun (also known as Ladcathel), and Twescard.\nPassage to the Crown.\nOf the original lordships or palatine counties:\nWith the passing of liberties to the Crown, the number of Counties of the Cross declined, and only one, Tipperary, survived into the Stuart era; the others had ceased to exist by the reign of Henry VIII.\nTudor era.\nUnder the Tudors, specifically the reign of Henry VIII (1509\u201347), crown control started to again extend throughout Ireland. Having declared himself King of Ireland in 1541, Henry VIII went about converting Irish chiefs into feudal subjects of the crown with land divided into districts, which were eventually amalgamated into the modern counties. County boundaries were still ill-defined; however, in 1543 Meath was split into Meath and Westmeath. Around 1545, the Byrnes and O'Tooles, both native septs who had constantly been a pain for the English administration of the Pale, petitioned the Lord Deputy of Ireland to turn their district into its own county, Wicklow. This was ignored.\nDuring the reigns of the last two Tudor monarchs, Mary I (1553\u201358) and Elizabeth I (1558\u20131603), the majority of the work for the foundation of the modern counties was carried out under the auspices of three Lord Deputies: Thomas Radclyffe, 3rd Earl of Sussex, Sir Henry Sydney, and Sir John Perrot.\nMary's reign saw the first addition of actual new counties since the reign of King John. Radclyffe had conquered the districts of Glenmaliry, Irry, Leix, Offaly, and Slewmargy from the O'Moores and O'Connors, and in 1556 a statute decreed that Offaly and part of Glenmaliry would be made into the county of King's County, whilst the rest of Glenmarliry along with Irry, Leix and Slewmargy was formed into Queen's County. Radclyffe brought forth legislation to shire all land as yet unshired throughout Ireland and sought to divide the island into six parts\u2014Connaught, Leinster, Meath, Nether Munster, Ulster, and Upper Munster. His administrative reign in Ireland was cut short, and it was not until the reign of Mary's successor, Elizabeth, that this legislation was re-adopted. Under Elizabeth, Radclyffe was brought back to implement it.\nSydney during his three tenures as Lord Deputy created two presidencies to administer Connaught and Munster. He shired Connaught into the counties of Galway, Mayo, Roscommon, and Sligo. In 1565, the territory of the O'Rourkes within Roscommon was made into the county of Leitrim. In 1569, in an attempt to reduce the importance of the province of Munster, Sydney, using the River Shannon as a natural boundary took the former kingdom of Thomond (North Munster) and made it into the county of Clare as part of the presidency of Connaught. \nIn 1569, the Irish Parliament passed \"An Act for turning of Countries that be not yet Shire Grounds into Shire Grounds\". In 1571, a commission headed by Perrot and others declared that the territory of Desmond in Munster was to be made a county of itself, and it had its own sheriff appointed. In 1606, it was merged with the county of Kerry. In 1575, Sydney made an expedition to Ulster to plan its shiring. Nothing came of the plans.\nIn 1578, the go-ahead was given for turning the districts of the Byrnes and O'Tooles into the county of Wicklow. With the outbreak of war in Munster and then Ulster, they resumed their independence. Sydney also sought to split Wexford into two smaller counties, the northern half of which was to be called Ferns, but the matter was dropped as it was considered impossible to properly administer. In 1583, the territory of the O'Farrells of Annaly, which was in Westmeath, was formed into the county of Longford and transferred to Connaught. The Desmond rebellion (1579\u201383) that was taking place in Munster stopped Sydney's work. By the time it had been defeated, Sir John Perrot was Lord Deputy, being appointed in 1584.\nPerrot was most remembered for shiring Ulster, the only province of Ireland that remained effectively outside of English control Prior to his tenancy the only proper county in Ulster was Louth, which had been part of the Pale. There were two other long recognised entities north of Louth\u2014Antrim and Down\u2014that had at one time been \"counties\" of the Earldom of Ulster and were regarded as apart from the unreformed parts of the province. The date Antrim and Down became constituted is unknown. \nIn 1588, Perrot was recalled and for two decades the shiring of Ulster basically existed on paper, as the territory affected remained firmly outside of English control until the defeat of Hugh O'Neill, Earl of Tyrone in the Nine Years' War. These counties were: Armagh, Cavan, Coleraine, Donegal, Fermanagh, Monaghan, and Tyrone. Cavan was formed from the territory of the O'Reilly's of East Breifne and had been transferred from Connaught to Ulster. After O'Neill and his allies fled Ireland in 1607 in the Flight of the Earls, their lands became escheated to the Crown. The county divisions designed by Perrot were used as the basis for the grants of the 1609 Plantation of Ulster effected by King James I..\nAround 1600, near the end of Elizabeth's reign, Clare was made an entirely distinct presidency of its own under the Earls of Thomond. It returned to being part of Munster after the Restoration in 1660.\nIn 1606, Wicklow was shired after the subjugation of the Byrnes and O'Tooles by Lord Deputy Sir Arthur Chichester. This county was one of the last to be created, yet was the closest to the centre of English power in Ireland.\nIn 1613, County Londonderry was incorporated by the merger of County Coleraine with the barony of Loughinsholin (in County Tyrone), the North West Liberties of Londonderry (in County Donegal), and the North East Liberties of Coleraine (in County Antrim).\nDemarcation of counties and Tipperary.\nThroughout the Elizabethan era and the reign of her successor James I, the exact boundaries of the provinces and the counties they consisted of remained uncertain. In 1598 Meath is considered a province in Hayne's \"Description of Ireland\", and included the counties of Cavan, East Meath, Longford, and Westmeath. This contrasts to George Carew's 1602 survey where there were only four provinces with Longford part of Connaught and Cavan not mentioned at all with only three counties mentioned for Ulster. During Perrot's tenure as Lord President of Munster before he became Lord Deputy, Munster contained as many as eight counties rather than the six it later consisted of. These eight counties were: the five English counties of Cork, Limerick, Kerry, Tipperary, and Waterford; and the three Irish counties of Desmond, Ormond, and Thomond.\nPerrot's divisions in Ulster were for the main confirmed by a series of inquisitions between 1606 and 1610 that settled the demarcation of the counties of Connaught and Ulster. John Speed's \"Description of the Kingdom of Ireland\" in 1610 showed that there was still a vagueness over what counties constituted the provinces, however, Meath was no longer reckoned a province. By 1616 when the Attorney General for Ireland Sir John Davies departed Ireland, almost all counties had been delimited. The only exception was the county of Tipperary, which still belonged to the palatinate of Ormond.\nTipperary would remain an anomaly being in effect two counties, one palatine, the other of the Cross until 1715 during the reign of King George I when an act abolished the \"royalties and liberties of the County of Tipperary\" and \"that whatsoever hath been denominated or called Tipperary or Cross Tipperary, shall henceforth be and remain one county forever, under the name of the County of Tipperary.\" Between 1838 and 2014, County Tipperary was divided into two ridings/counties, North Tipperary and South Tipperary.\nSub-divisions of counties.\nTo correspond with the subdivisions of the English shires into honours or baronies, Irish counties were granted out to the Anglo-Norman noblemen in cantreds, later known as baronies, which were subdivided, as in England, into parishes. Parishes were composed of townlands. In many cases, these divisions correspond to earlier, pre-Norman, divisions. While there are 331 baronies in Ireland, and more than a thousand civil parishes, there are around sixty thousand townlands that range in size from one to several thousand hectares. Townlands were often traditionally divided into smaller units called \"quarters\", but these subdivisions are not legally defined.\nCounties corporate.\nThe following towns/cities had charters specifically granting them the status of a county corporate:\nThe only entirely new counties created in 1898 were the county boroughs of Londonderry and Belfast. Carrickfergus, Drogheda and Kilkenny were abolished. Galway was also abolished, but recreated in 1986.\nExceptions to the county system of control.\nRegional presidencies of Connacht and Munster remained in existence until 1672, with special powers over their subsidiary counties. Tipperary remained a county palatine until the passing of the County Palatine of Tipperary Act 1715, with different officials and procedures from other counties. At the same time, Dublin, until the 19th century, had ecclesiastical liberties with rules outside those applying to the rest of Dublin city and county. Exclaves of the county of Dublin existed in counties Kildare and Wicklow. At least eight other enclaves of one county inside another, or between two others, existed. The enclaves and exclaves were merged into neighbouring and surrounding counties, primarily in the mid-19th century under a series of Orders in Council.\nEvolution of functions.\nThe Church of Ireland exercised functions at the level of a civil parish that were later exercised by county authorities. Vestigial feudal power structures of major old estates remained well into the 18th century. Urban corporations operated individual royal charters. Management of counties came to be exercised by grand juries. Members of grand juries were the local payers of rates who historically held judicial functions, taking maintenance roles in regard to roads and bridges, and the collection of \"county cess\" taxes. They were usually composed of wealthy \"country gentlemen\" (i.e. landowners, farmers and merchants):A country gentleman as a member of a Grand Jury...levied the local taxes, appointed the nephews of his old friends to collect them, and spent them when they were gathered in. He controlled the boards of guardians and appointed the dispensary doctors, regulated the diet of paupers, inflicted fines and administered the law at petty sessions. The counties were initially used for judicial purposes, but began to take on some governmental functions in the 17th century, notably with grand juries.\n19th and 20th centuries.\nIn 1836, the use of counties as local government units was further developed, with grand-jury powers extended under the Grand Jury (Ireland) Act 1836. The traditional county of Tipperary was split into two judicial counties (or ridings) following the establishment of assize courts in 1838. Also in 1838, local poor law boards, with a mix of magistrates and elected \"guardians\" took over the health and social welfare functions of the grand juries.\nIn 1898, a more radical reorganisation of local government took place with the passage of the Local Government (Ireland) Act 1898. This Act established a county council for each of the thirty-three Irish administrative counties. Elected county councils took over the powers of the grand juries. The boundaries of the traditional counties changed on a number of occasions. The 1898 Act changed the boundaries of Counties Galway, Clare, Mayo, Roscommon, Sligo, Waterford, Kilkenny, Meath and Louth, and others. County Tipperary was divided into two regions: North Riding and South Riding. Areas of the cities of Belfast, Cork, Dublin, Limerick, Derry and Waterford were carved from their surrounding counties to become county boroughs in their own right and given powers equivalent to those of administrative counties.\nUnder the Government of Ireland Act 1920, the island was partitioned between Southern Ireland and Northern Ireland. For the purposes of the Act, ... Northern Ireland shall consist of the parliamentary counties of Antrim, Armagh, Down, Fermanagh, Londonderry and Tyrone, and the parliamentary boroughs of Belfast and Londonderry, and Southern Ireland shall consist of so much of Ireland as is not comprised within the said parliamentary counties and boroughs.\nThe county and county borough borders were used to determine the line of partition. Southern Ireland shortly afterwards became the Irish Free State. This partition was entrenched in the Anglo-Irish Treaty, which was ratified in 1922, by which the Irish Free State left the United Kingdom with Northern Ireland making the decision to not separate two days later.\nHistoric and traditional counties.\nAreas that were shired by 1607 and continued as counties until the local government reforms of 1836, 1898 and 2001 are sometimes referred to as \"traditional\" or \"historic\" counties. These were distinct from the counties corporate that existed in some of the larger towns and cities, although linked to the county at large for other purposes. From 1898 to 2001, areas with county councils were known as administrative counties. The counties corporate were designated as county boroughs. From 2001, local government areas were divided between counties and cities. From 2014, they were divided into counties, cities, and cities and counties.\nCurrent usage.\nIn the Republic of Ireland.\nIn the Republic of Ireland, the traditional counties are, in general, the basis for local government, planning and community development purposes and are still generally respected for other purposes. They are governed by county councils. Administrative borders have been altered to allocate various towns exclusively into one county having been originally split between two counties.\nAt the establishment of the Irish Free State in 1922, there were 27 administrative counties (with County Tipperary divided into the administrative counties of North Tipperary and South Tipperary) and 4 county boroughs, Dublin, Cork, Limerick and Waterford.\nRural districts were abolished by the Local Government Act 1925 and the Local Government (Dublin) Act 1930 amidst widespread allegations of corruption.\nUnder the Local Government Provisional Order Confirmation Act 1976, part of the urban area of Drogheda, which lay in County Meath, was transferred to County Louth in January 1977. This resulted in the land area of County Louth increasing slightly at the expense of County Meath. The possibility of a similar action with regard to Waterford City has been raised in recent years, though opposition from Kilkenny has been strong.\nIn 1985, Galway became a county borough.\nCounty Dublin was abolished as an administrative county in 1994 and divided into three administrative counties: D\u00fan Laoghaire\u2013Rathdown, Fingal, and South Dublin.\nUnder the Local Government Act 2001, the county boroughs of Dublin, Cork, Galway, Limerick and Waterford were re-styled as cities, with the same status in law as counties. The term administrative county was replaced with the term \"county\".\nThe cities of Limerick and Waterford were merged with their respective counties by the Local Government Reform Act 2014, to form new \"cities and counties\". The same Act abolished North Tipperary and South Tipperary and re-established County Tipperary as an administrative unit.\nThere are now 31 local government areas: 26 counties, three cities, and two cities and counties.\nSince 2014, local authorities send representatives to Regional Assemblies overseeing three regions for the purposes of European Structural and Investment Funds: Southern Region, the Eastern and Midland Region, and the Northern and Western Region. From 1994 to 2014, there were eight Regional Authorities, dissolved under the Local Government Reform Act 2014.\nAs placenames, there is a distinction between the traditional counties, listed as \"counties\", and those created as local government areas, listed as \"administrative counties\".\nEducation.\nIn 2013 Education and Training Boards (ETBs) were formed throughout the Republic of Ireland, replacing the system of Vocational Education Committees (VECs) created in 1930. Originally, VECs were formed for each administrative county and county borough, and also in a number of larger towns, and were legally sub-committees of the relevant authorities. In 1997 the majority of town VECs were absorbed by the surrounding county authorities. The 33 VEC areas were reduced to 16 ETB areas, with each consisting of one or more local government county or city areas.\nThe Institute of technology system was organised by committee areas or \"functional areas\". These areas retain their legal basis but are not as important as originally envisioned as the institutes are now more national in character. The functional areas are only of significance today when selecting governing councils; similarly, Dublin Institute of Technology was originally a group of several colleges within the aegis of the City of Dublin VEC.\nElections.\nWhere possible, D\u00e1il constituencies follow county boundaries. Under the Electoral Act 1997, as amended, a Constituency Commission is established following the publication of preliminary census figures every five years. The commission is charged with defining constituency boundaries, and the 1997 Act provides that \"the breaching of county boundaries shall be avoided as far as practicable\". This provision does not apply to the boundaries between cities and counties, or between the three counties in the Dublin area.\nThis system usually results in more populated counties having several constituencies: Dublin, including Dublin city, is subdivided into twelve constituencies, Cork into five. Smaller counties such as Carlow and Kilkenny or Laois and Offaly may be paired to form constituencies. Leitrim, Ireland's least populated county, was divided between the constituencies of Sligo\u2013North Leitrim and Roscommon\u2013South Leitrim from 2007 to 2016.\nEach county, city, and city and county is divided into local electoral areas for the election of councillors. The boundaries of the areas and the number of councillors assigned are fixed from time to time by order of the Minister for Housing, Local Government and Heritage, following a report by the Local Government Commission, and based on population changes recorded in the census.\nIn Northern Ireland.\nIn Northern Ireland, a major reorganisation of local government in 1973 replaced the six traditional counties and two county boroughs (Belfast and Derry) with 26 single-tier districts for local government purposes. In 2015, as a result of a reform process that started in 2005, these districts were merged to form 11 new single-tier \"super districts\".\nThe six traditional counties remain in use for some purposes, including the three-letter coding of vehicle number plates, the Royal Mail Postcode Address File (which records counties in all addresses although they are no longer required for postcoded mail) and Lord Lieutenancies (for which the former county boroughs are also used). There are no longer official 'county towns'. However, the counties are still very widely acknowledged, for example as administrative divisions for sporting and cultural organisations.\nOther uses.\nThe administrative division of the island along the lines of the traditional 32 counties was also adopted by non-governmental and cultural organisations. In particular, the Gaelic Athletic Association (GAA) continues to organise its activities on the basis of its own system of counties that, throughout the island, correspond almost exactly to the 32 traditional counties in use at the time of the foundation of that organisation in 1884. The GAA also uses the term \"county\" for some of its organisational units in Britain and further afield. Legal adjustments to county bounds since 1884 have not been reflected in GAA county boards (e.g. Ballaghaderreen GAA which is located in County Roscommon but affiliated to Mayo GAA county board).\nList of counties.\nThe 35 divisions listed below include the traditional counties of Ireland as well as three created in Dublin in 1994. Twenty-four counties still delimit the remit of local government areas in the Republic of Ireland, in some cases with slightly redrawn boundaries. County Dublin, which was abolished as a distinct administrative entity in 1994, is included, as are the three new administrative counties which took over the functions of the former County Dublin. In Northern Ireland, the counties listed no longer serve this purpose. The Irish-language names of counties in the Republic of Ireland are prescribed by ministerial order, which in the case of three newer counties, omits the word (county). Irish names form the basis for all English-language county names except Waterford, Wexford, and Wicklow, which are of Norse origin.\nThe \"Region\" column of the table below, except for the six Northern Ireland counties, indicates Regions as defined under the Local Government Act 1991. These are NUTS 2 statistical regions of Ireland. \"County town\" is the current or former administrative capital of the county.\nThe cities of Cork, Dublin, and Galway, which are separate local government areas with the same legal status as counties, are not shown separately. Also omitted are the former county boroughs of Londonderry and Belfast which in Northern Ireland had the same legal status as the six counties until the reorganisation of local government in 1973.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15034", "revid": "20468248", "url": "https://en.wikipedia.org/wiki?curid=15034", "title": "Information Sciences Institute", "text": "University of Southern California research institute\nThe USC Information Sciences Institute (ISI) is a component of the University of Southern California (USC) Viterbi School of Engineering, and specializes in research and development in information processing, computing, and communications technologies. It is located in Marina del Rey, California.\nISI actively participated in the information revolution, and it played a leading role in developing and managing the early Internet and its predecessor ARPAnet. The Institute conducts basic and applied research supported by more than 20 U.S. government agencies involved in defense, science, health, homeland security, energy and other areas. Annual funding is about $100 million.\nISI employs about 400 research scientists, research programmers, graduate students and administrative staff at its Marina del Rey, California headquarters, in Arlington, Virginia, and in Boston, Massachusetts. About half of the research staff hold PhD degrees, and about 40 are research faculty who teach at USC and advise graduate students. Several senior researchers are tenured USC faculty in the Viterbi School.\nResearch and sponsors.\nISI research spans artificial intelligence (AI), cybersecurity, grid computing, cloud computing, quantum computing, microelectronics, supercomputing, nano-satellites and many other areas. AI expertise includes natural language processing, in which ISI has an international reputation, reconfigurable robotics, information integration, motion analysis and social media analysis. Hardware/software expertise includes cyber-physical system security, data mining, reconfigurable computing and cloud computing. In networking, ISI explores Internet resilience, Internet traffic analysis and photonics, among other areas. Researchers also work in scientific data management, wireless technologies, biomimetics and electrical smart grid, in which ISI is advising the Los Angeles Department of Water and Power on a major demonstration project. Another current initiative involves big data brain imaging jointly with the Keck School of Medicine of USC.\nFederal agency sponsors include the Air Force Research Laboratory, DARPA, Department of Education, Department of Energy, Department of Homeland Security, National Institutes of Health, National Science Foundation, and other scientific, technical, and defense-related agencies.\nCorporate partners include Chevron Corporation in the Center for Interactive Smart Oilfield Technologies (CiSoft), Lockheed Martin in the USC-Lockheed Martin Quantum Computing Center, and Sparta Inc., a subsidiary of Parsons Corporation in the DETER Project, a cybersecurity research initiative and international testbed. ISI also has partnered with businesses including IBM, Samsung Electronics, Raytheon, GlobalFoundries, Northrop Grumman and Carl Zeiss AG, and currently is working with Micron Technology, Inc., Altera Corporation and Fujitsu Ltd.\nISI also operates MOSIS, a multi-project electronic circuit wafer service that has prototyped more than 60,000 chips since 1981. MOSIS provides design tools and pools circuit designs to produce specialty and low-volume chips for corporations, universities and other research entities worldwide. The Institute also has given rise to several startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.\nHistory.\nISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation in the 1960s and early 1970s. Uncapher decided to leave RAND after his group's funding was cut in 1971. He approached the University of California, Los Angeles about creating an off-campus technology institute, but was told that a decision would take 15 months. He then presented the concept to USC, which approved the proposal in five days. ISI was launched with three employees in 1972. Its first proposal was funded by the Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.\nISI became one of the earliest nodes on ARPANET, the predecessor to the Internet, and in 1977 figured prominently in a demonstration of its international viability. ISI also helped refine the TCP/IP communications protocols fundamental to Net operations, and researcher Paul Mockapetris developed the now-familiar Domain Name System characterized by .com, .org, .net, .gov, and .edu on which the Net still operates. (The names .com, .org et al. were invented at SRI International, an ongoing collaborator.) Steve Crocker originated the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and shaped the emerging Internet. Another ISI researcher, Danny Cohen, became first to implement packet voice and packet video over ARPANET, demonstrating the viability of packet switching for real-time applications.\nJonathan Postel collaborated in development of TCP/IP, DNS and the SMTP protocol that supports email. He also edited the RFC for nearly three decades until his sudden death in 1998, when ISI colleagues assumed responsibility. The Institute retained that role until 2009. Postel simultaneously directed the Internet Assigned Numbers Authority (IANA) and its predecessor, which assign Internet addresses. IANA was administered from ISI until a nonprofit organization, ICANN, was created for that purpose in 1998.\nOther achievements.\nSome of the first Net security applications, and one of the world's first portable computers, also originated at ISI.\nISI researchers also created or co-created the:\nIn 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the \"Jeopardy!\" TV show. In 2012, ISI's Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained unreadable for 250 years. Also in 2012, the USC-Lockheed Martin Quantum Computing Center (QCC) became the first organization to operate a quantum annealing system outside of its manufacturer, D-Wave Systems, Inc. USC, ISI and Lockheed Martin now are performing basic and applied research into quantum computing. A second quantum annealing system is located at NASA Ames Research Center, and is operated jointly by NASA and Google.\nThe USC Andrew and Erna Viterbi School of Engineering was ranked among the nation's top 10 engineering graduate schools by \"US News &amp; World Report\" in 2015. Including ISI, USC is ranked first nationally in federal computer science research and development expenditures.\nOrganizational structure.\nISI is organized into seven divisions focused on differing areas of research expertise:\nSmaller, specialized research groups operate within almost all divisions.\nISI is led by Executive Director Craig Knoblock, the previous director to the AI division.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15036", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=15036", "title": "Information security", "text": "Protecting information by mitigating risk\nInformation security (infosec) is the practice of protecting information by mitigating information risks. It is part of information risk management. It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge). Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (known as the CIA triad, unrelated to the US government organization) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a structured risk management process. \nTo standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed. \nWhile paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized, with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). \nIT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses. They are responsible for keeping all of the technology within the company secure from malicious attacks that often attempt to acquire critical private information or gain control of the internal systems.\nThere are many specialist roles in Information Security including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics.\nDefinitions.\nInformation security standards are techniques generally outlined in published materials that attempt to protect the information of a user or organization. This environment includes users themselves, networks, devices, all software, processes, information in storage or transit, applications, services, and systems that can be connected directly or indirectly to networks.\nThe principal objective is to reduce the risks, including preventing or mitigating attacks. These published materials consist of tools, policies, security concepts, security safeguards, guidelines, risk management approaches, actions, training, best practices, assurance and technologies.\nVarious definitions of information security are suggested below, summarized from different sources:\nThreats.\nInformation security threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, theft of identity, theft of equipment or information, sabotage, and information extortion. Viruses, worms, phishing attacks, and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering. Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers. Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware. One of the most functional precautions against these attacks is to conduct periodical user awareness.\nGovernments, military, corporations, financial institutions, hospitals, non-profit organizations, and private businesses amass a great deal of confidential information about their employees, customers, products, research, and financial status. Should confidential information about a business's customers or finances or new product line fall into the hands of a competitor or hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.\nFor the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.\nHistory.\nSince the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands. However, for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read, and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653).\nIn the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. Section 1 of the law concerned espionage and unlawful disclosures of information, while Section 2 dealt with breaches of official trust. A public interest defense was soon added to defend disclosures in the interest of the state. A similar law was passed in India in 1889, The Indian Official Secrets Act, which was associated with the British colonial era and used to crack down on newspapers that opposed the Raj's policies. A newer version was passed in 1923 that extended to all matters of confidential or secret information for governance. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information.\nThe establishment of computer security inaugurated the history of information security. The need for such appeared during World War II. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed. The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information. Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570).\nVarious mainframe computers were connected online during the Cold War to complete more sophisticated tasks, in a communication process easier than mailing magnetic tapes back and forth by computer centers. As such, the Advanced Research Projects Agency (ARPA), of the United States Department of Defense, started researching the feasibility of a networked system of communication to trade information within the United States Armed Forces. In 1968, the ARPANET project was formulated by Larry Roberts, which would later evolve into what is known as the internet.\nIn 1973, important elements of ARPANET security were found by internet pioneer Robert Metcalfe to have many flaws such as the: \"vulnerability of password structure and formats; lack of safety procedures for dial-up connections; and nonexistent user identification and authorizations\", aside from the lack of controls and safeguards to keep data safe from unauthorized access. Hackers had effortless access to ARPANET, as phone numbers were known by the public. Due to these problems, coupled with the constant violation of computer security, as well as the exponential increase in the number of hosts and users of the system, \"network security\" was often alluded to as \"network insecurity\".\nThe end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful, and less expensive computing equipment made electronic data processing within the reach of small business and home users. The establishment of Transfer Control Protocol/Internetwork Protocol (TCP/IP) in the early 1980s enabled different types of computers to communicate. These computers quickly became interconnected through the internet.\nThe rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process, and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.\nSecurity Goals.\nCIA triad.\nThe \"CIA triad\" of \"confidentiality, integrity, and availability\" is at the heart of information security. The concept was introduced in the Anderson Report in 1972 and later repeated in \"The Protection of Information in Computer Systems.\" The abbreviation was coined by Steve Lipner around 1986. \nDebate continues about whether or not this triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy. Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.\nConfidentiality.\nIn information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\" While similar to \"privacy\", the two words are not interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers. Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.\nIntegrity.\nIn IT security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically incorporate controls to ensure their own integrity, in particular protecting the kernel or core functions against both deliberate and accidental threats. Multi-purpose and multi-user computer systems aim to compartmentalize the data and processing such that no user or process can adversely impact another: the controls may not succeed however, as we see in incidents such as malware infections, hacks, data theft, fraud, and privacy breaches.\nMore broadly, integrity is an information security principle that involves human/social, process, and commercial integrity, as well as data integrity. As such it touches on aspects such as credibility, consistency, truthfulness, completeness, accuracy, timeliness, and assurance.\nAvailability.\nFor any information system to serve its purpose, the information must be available when it is needed. This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.\nIn the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program. Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect. This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails. Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response, and policy/change management. A successful information security team involves many different key roles to mesh and align for the \"CIA\" triad to be provided effectively.\nAdditional security goals.\nIn addition to the classic CIA triad of security goals, some organisations may want to include security goals like authenticity, accountability, non-repudiation, and reliability. \nNon-repudiation.\nIn law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.\nIt is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity). The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).\nOther models.\nIn 1992 and revised in 2002, the OECD's \"Guidelines for the Security of Information Systems and Networks\" proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment. Building upon those, in 2004 the NIST's \"Engineering Principles for Information Technology Security\" proposed 33 principles. \nIn 1998, Donn Parker proposed an alternative model for the classic \"CIA\" triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.\nIn 2011, The Open Group published the information security management standard O-ISM3. This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance, and technical (4).\nRisk management.\nRisk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm. The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property).\nThe \"Certified Information Systems Auditor (CISA) Review Manual 2006\" defines risk management as \"the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"\nThere are two things in this definition that may need some clarification. First, the \"process\" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected. Furthermore, these processes have limitations as security breaches are generally rare and emerge in a specific context which may not be easily duplicated. Thus, any process and countermeasure should itself be evaluated for vulnerabilities. It is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk\".\nA risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.\nResearch has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:\nIn broad terms, the risk management process consists of:\nFor any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.\nSecurity controls.\nSelecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas. Organizations can implement additional controls according to requirement of the organization. ISO/IEC 27002 offers a guideline for organizational information security standards.\nDefense in depth.\nDefense in depth is a fundamental security philosophy that relies on overlapping security systems designed to maintain protection even if individual components fail. Rather than depending on a single security measure, it combines multiple layers of security controls both in the cloud and at network endpoints. This approach includes combinations like firewalls with intrusion-detection systems, email filtering services with desktop anti-virus, and cloud-based security alongside traditional network defenses.\nThe concept can be implemented through three distinct layers of administrative, logical, and physical controls, or visualized as an onion model with data at the core, surrounded by people, network security, host-based security, and application security layers. The strategy emphasizes that security involves not just technology, but also people and processes working together, with real-time monitoring and response being crucial components.\nClassification.\nAn important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification. The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.\nSome factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information. The Information Systems Audit and Control Association (ISACA) and its \"Business Model for Information Security\" also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.\nThe type of information security classification labels selected and used will depend on the nature of the organization, with examples being:\nAll employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.\nAccess control.\nAccess to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.\nAccess control is generally considered in three steps: identification, authentication, and authorization.\nIdentification.\nIdentification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".\nAuthentication.\nAuthentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.\nThere are three different types of information that can be used for authentication:\nStrong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose, but they are increasingly inadequate. Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as time-based one-time password algorithms.\nAuthorization.\nAfter a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.\nThe non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.\nExamples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems; Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.\nTo be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.\nAlso, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions. This principle is used in the government when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privilege to prevent employees from accessing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.\nCryptography.\nInformation security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.\nCryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU\u2011T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and email.\nCryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction, and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.\nProcess.\nU.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.\nIn the field of information security, Harris\noffers the following definitions of due care and due diligence:\n\"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees\"\".\" And, [Due diligence are the] \"continual activities that make sure the protection mechanisms are continually maintained and operational.\"\nAttention should be made to two important points in these definitions. First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.\nOrganizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA) provides principles and practices for evaluating risk. It considers all parties that could be affected by those risks. DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden. With increased data breach litigation, companies must balance security controls, compliance, and its mission.\nIncident response plans.\nComputer security incident management is a specialized form of incident management focused on monitoring, detecting, and responding to security events on computers and networks in a predictable way. \nOrganizations implement this through incident response plans (IRPs) that are activated when security breaches are detected. These plans typically involve an incident response team (IRT) with specialized skills in areas like penetration testing, computer forensics, and network security.\nChange management.\nChange management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers, and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.\nAny change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.\nNot every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.\nChange management is usually overseen by a change review board composed of representatives from key business areas, security, networking, systems administrators, database administration, application developers, desktop support, and the help desk. The tasks of the change review board can be facilitated with the use of automated work flow application. The responsibility of the change review board is to ensure the organization's documented change management procedures are followed. The change management process is as follows\nChange management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation, and communication.\nISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and ITIL all provide valuable guidance on implementing an efficient and effective change management program information security.\nBusiness continuity.\nBusiness continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects. BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual. The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.\nIt encompasses:\nWhereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster. A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.\nLaws and regulations.\nBelow is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security.\nThe US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.\nCulture.\nDescribing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer &amp; Petric (2017) identify seven core dimensions of information security culture in organizations:\nAndersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests. Research shows information security culture needs to be improved continuously. In \"Information Security Culture from Analysis to Change\", authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "15037", "revid": "50967404", "url": "https://en.wikipedia.org/wiki?curid=15037", "title": "Income", "text": "Wealth gained over a given time period\nIncome is the consumption and saving opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. Income is difficult to define conceptually and the definition may be different across fields. For example, a person's income in an economic sense may be different from their income as defined by law.\nAn extremely important definition of income is Haig\u2013Simons income, which defines income as \"Consumption + Change in net worth\" and is widely used in economics.\nFor households and individuals in the United States, income is defined by tax law as a sum that includes any wage, salary, profit, interest payment, rent, or other form of earnings received in a calendar year. Discretionary income is often defined as gross income minus taxes and other deductions (such as mandatory pension contributions), and is widely used as a basis to compare the welfare of taxpayers.\nIn the field of public economics, the concept may comprise the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income.\nFor a firm, gross income can be defined as sum of all revenue minus the cost of goods sold. Net income nets out expenses: net income equals revenue minus cost of goods sold, expenses, depreciation, interest, and taxes.\nEconomic definitions.\nFull and Haig\u2013Simons income.\n\"Full income\" refers to the accumulation of both the monetary and the non-monetary consumption-ability of any given entity, such as a person or a household. According to what the economist Nicholas Barr describes as the \"classical definition of income\" (the 1938 Haig\u2013Simons definition): \"income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights...\" Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, \"i.e.\" failing to accurately reflect affluence (and thus the consumption opportunities) of any given agent. \nIt omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, \"in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full income prevents a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income.\nFactor income.\nIn economics, factor income is the return accruing for a person, or a nation, derived from the \"factors of production\": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.\nIn consumer theory 'income' is another name for the \"budget constraint\", an amount formula_1 to be spent on different goods x and y in quantities formula_2 and formula_3 at prices formula_4 and formula_5. The basic equation for this is\nformula_6\nThis equation implies two things. First buying one more unit of good x implies buying formula_7 less units of good y. So, formula_7 is the \"relative\" price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed formula_1 and fixed formula_10 then its relative price falls. The usual hypothesis, the law of demand, is that the quantity demanded of x would increase at the lower price. The analysis can be generalized to more than two goods.\nThe theoretical generalization to more than one period is a multi-period wealth and income constraint. For example, the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis.\nLegal definitions.\nDefinitions under the Internal Revenue Code.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Except as otherwise provided in this subtitle, gross income means all income from whatever source derived, including (but not limited to) the following items: (1) Compensation for services, including fees, commissions, fringe benefits, and similar items; (2) Gross income derived from business; (3) Gains derived from dealings in property; (4) Interest; (5) Rents; (6) Royalties; (7) Dividends; (8) Annuities; (9) Income from life insurance and endowment contracts; (10) Pensions; (11) Income from discharge of indebtedness; (12) Distributive share of partnership gross income; (13) Income in respect of a decedent; and (14) Income from an interest in an estate or trust.\n26 U.S. Code \u00a7 61 - Gross income defined. There are also some statutory exclusions from income.\nDefinition under US Case law.\nIncome is an \"undeniable accessions to wealth, clearly realized, and over which the taxpayer has complete dominion.\" Commentators say that this is a pretty good definition of income.\nTaxable income is usually lower than Haig-Simons income. This is because unrealized appreciation (e.g., the increase in the value of stock over the course of a year) is economic income but not taxable income, and because there are many statutory exclusions from taxable income, including workman's compensation, SSI, gifts, child support, and in-kind government transfers.\nAccounting definitions.\nThe International Accounting Standards Board (IASB) uses the following definition: \"Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants.\" [F.70] (IFRS Framework).\nPreviously the IFRS conceptual framework (4.29) stated: \"The definition of income encompasses both revenue and gains. Revenue arises in the course of the ordinary activities of an entity and is referred to by a variety of different names including sales, fees, interest, dividends, royalties and rent. 4.30: Gains represent other items that meet the definition of income and may, or may not, arise in the course of the ordinary activities of an entity. Gains represent increases in economic benefits and as such are no different in nature from revenue. Hence, they are not regarded as constituting a separate element in this Conceptual Framework.\"\nThe current IFRS conceptual framework (4.68) no longer draws a distinction between revenue and gains. Nevertheless, the distinction continues to be drawn at the standard and reporting levels. For example, IFRS 9.5.7.1 states: \"A gain or loss on a financial asset or financial liability that is measured at fair value shall be recognised in profit or loss ...\" while the IASB defined IFRS XBRL taxonomy includes OtherGainsLosses, GainsLossesOnNetMonetaryPosition and similar items.\nUS GAAP does not define income but does define comprehensive income (CON 8.4.E75): Comprehensive income is the change in equity of a business entity during a period from transactions and other events and circumstances from nonowner sources. It includes all changes in equity during a period except those resulting from investments by owners and distributions to owners.\nAccording to John Hicks' definitions, income \"is the maximum amount which can be spent during a period if there is to be an expectation of maintaining intact, the capital value of prospective receipts (in money terms)\".\n\"Nonincome\".\nDebt.\nBorrowing or repaying money is not income under any definition, for either the borrower or the lender. Interest and forgiveness of debt are income.\nPsychic income.\n\"Non-monetary joy,\" such as watching a sunset or having sex, simply is not income. Similarly, nonmonetary suffering, such as heartbreak or labor, are not negative income. This may seem trivial, but the non-inclusion of psychic income has important effects on economics and tax policy. It encourages people to find happiness in nonmonetary, nontaxable ways and means that reported income may overstate or understate the well-being of a given individual.\nIncome growth.\nIncome per capita has been increasing steadily in most countries. Many factors contribute to people having a higher income, including education, globalisation and favorable political circumstances such as economic freedom and peace. Increases in income also tend to lead to people choosing to work fewer hours.\nDeveloped countries (defined as countries with a \"developed economy\") have higher incomes as opposed to developing countries tending to have lower incomes.\nFactors contributing to higher income.\nEducation has a positive effect on the level of income. Education increases the skills of the workforce, which in turn increases its productivity (and thus higher wages). Gary Becker developed a Human Capital Theory, which emphasizes that investment in education and training lead to efficiency gains, and by extension to economic growth.\nGlobalization can increase incomes by integrating markets, and allowing individuals greater possibilities of income increases through efficient allocation of resources and expanding existing wealth.\nGenerally, countries more open to trade have higher incomes. And while globalization tends to increase average income in a country, it does so unequally. Sachs and Warner claim, that \"countries with open economies will converge to the same level of income, although admittedly it will take a long time.\"\nIncome inequality.\nIncome inequality is the extent to which income is distributed in an uneven manner. It can be measured by various methods, including the Lorenz curve and the Gini coefficient. Many economists argue that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice. Thereby necessitating initiatives like the United Nations Sustainable Development Goal 10 aimed at reducing inequality.\nNational Income.\nNational income, measured by statistics such as net national income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see Measures of national income and output.\nThe total output of an economy equals its total income. From this viewpoint, GDP can be an indicator and measurement of national income since it measures a nation's total production of goods and services produced within the borders of one country and its total income simultaneously. GDP is measured through factors of production (inputs) and the production function (the ability to turn inputs into outputs). One important note in this is income distribution working through the factor market and how national income is divided among these factors. For this examination, the Neoclassical theory of distribution and factor prices is the modern theory to look into.\nBasic income.\nBasic income models advocate for a regular, and usually unconditional, receipt of money from the public institution. There are many basic income models, with the most famous being Universal Basic Income.\nUniversal Basic Income.\nUniversal Basic Income is a periodic receival of cash given to individuals on universal and unconditional basis. Unlike other programs like the Food Stamp Program, UBI provides eligible recipients with cash instead of coupons. Instead of households, it is paid to all individuals without requiring means test and regardless of employment status.\nThe proponents of UBI argue, that basic income is needed for social protection, mitigating automation and labour market disruptions. Opponents argue that UBI, in addition to being costly, will distort incentives for individuals to work. They might argue that there are other and more cost-effective policies that can tackle problems raised by the proponents of UBI. These policies include for example negative income tax.\nIncome in philosophy and ethics.\nThroughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' ( (ASV)).\nSome scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both the individual and the national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his \"Theory of Moral Sentiments\", and has more recently been developed by Harvard economist Benjamin Friedman in his book \"The Moral Consequences of Economic Growth\".\nIncome and health.\nA landmark systematic review from Harvard University researchers in the Cochrane Collaboration found that income given in the form of unconditional cash transfers leads to reductions in disease, improvements in food security and dietary diversity, increases in children's school attendance, decreases in extreme poverty, and higher health care spending.&lt;ref name=\"doi10.1002/14651858.CD011135.pub2\"&gt;&lt;/ref&gt;&lt;ref name=\"doi10.1002/14651858.CD011135.pub3\"&gt;&lt;/ref&gt;\nThe Health Foundation published an analysis where people on the lower income spectrum were more likely to describe their health negatively. Higher income was associated with self-reported better health. Another study found that \"an increase in household income of \u00a31,000 is associated with a 3.6 month increase in life expectancy for both men and women.\"\nA study by a Professor of Epidemiology Michael G Marmot found argues that there are two ways which could explain a positive correlation between income and health: the ability to afford goods and services necessary for biological survival, and the ability to influence life circumstances.\nRussell Ecob and George Davey Smith found that there is a relationship between income and a number of health measures. Greater household equivalised income is associated with better health indicators such as height, waist\u2013hip ratio, respiratory function, malaise, limiting long-term illness.\nHistory.\nIncome is conventionally denoted by \"Y\" in economics. John Hicks used \"I\" for income, but Keynes wrote to him in 1937, \"after trying both, I believe it is easier to use Y for income and I for investment.\" Some consider Y as an alternative letter for the phoneme I in languages like Spanish, although Y as the \"Greek I\" was actually pronounced like the modern German \u00fc or the phonetic /y/.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15039", "revid": "5839510", "url": "https://en.wikipedia.org/wiki?curid=15039", "title": "Iona", "text": "Island off the west coast of Scotland\nIona (; , sometimes simply \"\u00cc\") is an island in the Inner Hebrides, off the Ross of Mull on the western coast of Scotland. It is mainly known for Iona Abbey, though there are other buildings on the island. Iona Abbey was a centre of Gaelic monasticism for three centuries and is today known for its relative tranquility and natural environment. It is a tourist destination and a place for spiritual retreats. Its modern Scottish Gaelic name means \"Iona of (Saint) Columba\" (formerly anglicised as \"Icolmkill\").\nIona's resident population is about 180. In March 1980, the Hugh Fraser Foundation donated much of the main island (and its off-lying islands) to the current owner, the National Trust for Scotland. The abbey and some church buildings are owned by the Iona Cathedral Trust.\nOne publication, describing the religious significance of the island, says that the island is \"known as the birthplace of Celtic Christianity in Scotland,\u201d and notes that \u201cSt Columba came here in the year 563 to establish the Abbey, which still stands\".\nEtymology.\nBecause the Hebrides have been successively occupied by speakers of several languages since the Iron Age, many of its islands' names have more than one possible meaning. Nonetheless, few, if any, have accumulated as many different names over the centuries as the island now known in English as \"Iona\".\nThe place-name scholar William J. Watson has shown that the earliest recorded names of the island meant something like \"yew-place\". The element \"Ivo-\", denoting \"yew\", occurs in inscriptions in the ogham alphabet (\"Iva-cattos\" [genitive], \"Iva-geni\" [genitive]) and in Gaulish names (\"Ivo-rix\", \"Ivo-magus\"); it may also be the basis of early Gaelic names like \"E\u00f3gan\" (ogham: \"Ivo-genos\"). The island's name may also be related to the name of a mythological figure, \"Fer h\u00cd mac Eogabail\", the foster-son of Manann\u00e1n, whose forename meaning \"man of the yew\".\nCoates (2006) disputes the \"yew\" interpretation due to a lack of archeological evidence for yew on the island. Coates instead compares the Punic term \"\u2019y\" (\"island, isolated place\").\nMac an T\u00e0illeir (2003) has analyzed the more recent Gaelic names of \"\u00cc\", \"\u00cc Chaluim Chille\" and \"Eilean Idhe\". He notes that the name \"\u00cc\" was \"generally lengthened to avoid confusion\" to \"\u00cc Chaluim Chille\", which means \"Calum's Iona\" or \"island of Calum's monastery\". (\"Calum\"'s latinized form is \"Columba\".) This confusion would have arisen because \"\u00ec\", the original name of the island, would have been confused with the now-obsolete Gaelic noun \"\u00ec\", meaning \"island\", which was derived from the Old Norse word for island (\"ey\"). \"Eilean Idhe\" means \"the isle of Iona\", also known as \"\u00cc nam ban b\u00f2idheach\" (\"the isle of beautiful women\"). The modern English name comes from yet another variant, \"Ioua\", which arose either from Adomn\u00e1n's 7th century attempt to make the Gaelic name fit Latin grammar, or spontaneously, as a derivative of \"Ivova\" (\"yew place\"). The change in the island's name from \"Ioua\"' to \"Iona\", which is attested from c. 1274, resulted from a transcription error due to the similarity of \"n\" and \"u\" in Insular Minuscule script.\nDespite the continuity of forms in Gaelic from the pre-Norse to the post-Norse era, Haswell-Smith (2004) speculates that the island's name may be connected with the Norse word \"Hi\u014de\", meaning \"island of the den of the brown bear\". The medieval English-language version of the name was \"Icolmkill\" (and variants thereof).\nFolk etymology.\nMurray (1966) claims that the \"ancient\" Gaelic name was \"Innis nan Druinich\" (\"the isle of Druidic hermits\"), but there is no evidence for the \"ancient\" use of such a name before the nineteenth century when it appears in the \"New Statistical Account\" and it may arise from a misunderstanding of the name \"Cladh nan Druineach\", which means 'burial ground of the embroideresses or artificers' \u2013 a cemetery on the east shore of the island. He also repeats a Gaelic story (which he admits is apocryphal) that as Columba's coracle first drew close to the island one of his companions cried out \"Ch\u00ec mi i\" meaning \"I see her\" and that Columba's response was \"Henceforth we shall call her \u00cc\".\nGeology.\nThe geology of Iona is quite complex given the island's size and is quite distinct from that of nearby Mull. About half of the island's bedrock is Scourian gneiss assigned to the Lewisian complex and dating from the Archaean eon making it some of the oldest rock in Britain and indeed Europe. Closely associated with these gneisses are mylonite and meta-anorthosite and melagabbro. Along the eastern coast facing Mull are steeply dipping Neoproterozoic age metaconglomerates, metasandstones, metamudstones and hornfelsed metasiltstones ascribed to the Iona Group, described traditionally as Torridonian. In the southwest and on parts of the west coast are pelites and semipelites of Archaean to Proterozoic age. There are small outcrops of Silurian age pink granite on southeastern beaches, similar to those of the Ross of Mull pluton cross the sound to the east. Numerous geological faults cross the island, many in an E-W or NW-SE alignment. Devonian aged microdiorite dykes are found in places and some of these are themselves cut by Palaeocene age camptonite and monchiquite dykes ascribed to the \"Iona-Ross of Mull dyke swarm\". More recent sedimentary deposits of Quaternary age include both present day beach deposits and raised marine deposits around Iona as well as some restricted areas of blown sand.\nGeography.\nIona lies about from the coast of Mull. It is about wide and long with a resident population of 125. Like other places swept by ocean breezes, there are few trees; most of them are near the parish church.\nIona's highest point is D\u00f9n \u00cc, . Iona's geographical features include the Bay at the Back of the Ocean and \"C\u00e0rn C\u00f9l ri \u00c9irinn\" (the Hill/Cairn of [turning the] Back to Ireland), said to be adjacent to the beach where St. Columba first landed.\nThe main settlement, located at St. Ronan's Bay on the eastern side of the island, is called \"Baile M\u00f2r\" and is also known locally as \"The Village\". The primary school, post office, the island's two hotels, the Bishop's House and the ruins of the Nunnery are here. The Abbey and MacLeod Centre are a short walk to the north. Port B\u00e0n (white port) beach on the west side of the island is home to the Iona Beach Party.\nThere are numerous offshore islets and skerries: Eilean Annraidh (island of storm) and Eilean Chalbha (calf island) to the north, R\u00e8idh Eilean and Stac MhicMhurchaidh to the west and Eilean M\u00f9simul (mouse holm island) and Soa Island to the south are amongst the largest. The steamer \"Cathcart Park\" carrying a cargo of salt from Runcorn to Wick ran aground on Soa on 15 April 1912, the crew of 11 escaping in two boats.\nSubdivision.\nOn a map of 1874, the following territorial subdivision is indicated (from north to south):\nHistory.\nD\u00e1l Riata.\nIn the early Historic Period Iona lay within the Gaelic kingdom of D\u00e1l Riata, in the region controlled by the Cen\u00e9l Loairn (i.e. Lorn, as it was then). The island was the site of a highly important monastery (see Iona Abbey) during the Early Middle Ages. The monastery was founded in 563 by the monk Columba, also known as Colm Cille, who sailed here from Ireland to live the monastic life. Much later legends (a thousand years later, and without any good evidence) said that he had been exiled from his native Ireland as a result of his involvement in the Battle of Cul Dreimhne. Columba and twelve companions went into exile on Iona and founded a monastery there. The monastery was hugely successful and may have played a role in the conversion to Christianity of the Picts and Gaels of present-day Scotland in the late 6th century, and was certainly central to the conversion of the Anglo-Saxon kingdom of Northumbria in 635. Many satellite institutions were founded, and Iona became the centre of one of the most important monastic systems in Great Britain and Ireland.\nIona became a renowned centre of learning, and its scriptorium produced highly important documents, probably including the original texts of the Iona Chronicle, thought to be the source for the early Irish annals. The monastery is often associated with the distinctive practices and traditions known as Celtic Christianity. In particular, Iona was a major supporter of the \"Celtic\" system for calculating the date of Easter at the time of the Easter controversy, which pitted supporters of the Celtic system against those favoring the \"Roman\" system used elsewhere in Western Christianity. The controversy weakened Iona's ties to Northumbria, which adopted the Roman system at the Synod of Whitby in 664, and to Pictland, which followed suit in the early 8th century. Iona itself did not adopt the Roman system until 715, according to the Anglo-Saxon historian Bede. Iona's prominence was further diminished over the next centuries as a result of Viking raids and the rise of other powerful monasteries in the system, such as the Abbey of Kells.\nThe Book of Kells may have been produced or begun on Iona towards the end of the 8th century. Around this time the island's exemplary high crosses were sculpted; these may be the first such crosses to contain the ring around the intersection that became characteristic of the \"Celtic cross\". The series of Viking raids on Iona began in 794 and, after its treasures had been plundered many times, Columba's relics were removed and divided two ways between Scotland and Ireland in 849 as the monastery was abandoned.\nKingdom of the Isles.\nAs the Norse domination of the west coast of Scotland advanced, Iona became part of the Kingdom of the Isles. The Norse \"Rex plurimarum insularum\" Amla\u00edb Cuar\u00e1n died in 980 or 981 whilst in \"religious retirement\" on Iona. Nonetheless, the island was sacked twice by his successors, on Christmas night 986 and again in 987. Although Iona was never again important to Ireland, it rose to prominence once more in Scotland following the establishment of the Kingdom of Scotland in the later 9th century; the ruling dynasty of Scotland traced its origin to Iona, and the island thus became an important spiritual centre for the new kingdom, with many of its early kings buried there. However, a campaign by Magnus Barelegs led to the formal acknowledgement of Norwegian control of Argyll, in 1098.\nSomerled, the brother-in-law of Norway's governor of the region (the \"King of the Isles\"), launched a revolt and made the kingdom independent. A convent for Augustinian nuns was established in about 1208, with Beth\u00f3c, Somerled's daughter, as first prioress. The present buildings are of the Benedictine abbey, Iona Abbey, from about 1203, dissolved at the Reformation.\nOn Somerled's death, nominal Norwegian overlordship of the Kingdom was re-established, but de facto control was split between Somerled's sons, and his brother-in-law.\nKingdom of Scotland.\nFollowing the 1266 Treaty of Perth the Hebrides were transferred from Norwegian to Scottish overlordship. At the end of the century, King John Balliol was challenged for the throne by Robert the Bruce. By this point, Somerled's descendants had split into three groups, the MacRory, MacDougalls, and MacDonalds. The MacDougalls backed Balliol, so when he was defeated by Bruce, the latter exiled the MacDougalls and transferred their island territories to the MacDonalds; by marrying the heir of the MacRorys, the heir of the MacDonalds re-unified most of Somerled's realm, creating the Lordship of the Isles, under nominal Scottish authority. Iona, which had been a MacDougall territory (together with the rest of Lorn), was given to the Campbells, where it remained for half a century.\nIn 1354, though in exile and without control of his ancestral lands, John, the MacDougall heir, quitclaimed any rights he had over Mull and Iona to the Lord of the Isles (though this had no meaningful effect at the time). When Robert's son, David II, became king, he spent some time in English captivity; following his release, in 1357, he restored MacDougall authority over Lorn. The 1354 quitclaim, which seems to have been an attempt to ensure peace in just such an eventuality, took automatic effect, splitting Mull and Iona from Lorn, and making it subject to the Lordship of the Isles. Iona remained part of the Lordship of the Isles for the next century and a half.\nFollowing the 1491 Raid on Ross, the Lordship of the Isles was dismantled, and Scotland gained full control of Iona for the second time. The monastery and nunnery continued to be active until the Reformation, when buildings were demolished and all but three of the 360 carved crosses destroyed. The Augustine nunnery now only survives as a number of 13th century ruins, including a church and cloister. By the 1760s little more of the nunnery remained standing than at present, though it is the most complete remnant of a medieval nunnery in Scotland.\nPost-Union.\nAfter a visit in 1773, the English writer Samuel Johnson remarked:\nThe island, which was once the metropolis of learning and piety, now has no school for education, nor temple for worship.\nHe estimated the population of the village at 70 families or perhaps 350 inhabitants.\nIn the 19th century, green-streaked marble was commercially mined in the south-east of Iona; the quarry and machinery survive, see \"Marble Quarry remains\" below.\nIona Abbey.\nIona Abbey, now an ecumenical church, is of particular historical and religious interest to pilgrims and visitors alike. It is the most elaborate and best-preserved ecclesiastical building surviving from the Middle Ages in the Western Isles of Scotland. Though modest in scale in comparison to medieval abbeys elsewhere in Western Europe, it has a wealth of fine architectural detail and monuments of many periods. The enabling endowments, the core economic strength and life-blood of the Abbey came from successive Clan Donald Lords of the Isles and for 300 years were regularly confirmed, honoured, protected, increased and expanded. Endowments had \"carta confirmations\" and additional ones made by them during the 14th and 15th centuries as late as 1440 and 1485. Donald of Harlaw (1386-1421): \"gave lands to the monastery of Iona, and every immunity which the monastery of Iona had from his ancestors before him\" \u2013 MacVurich. It might be expressed that Iona Abbey had been acting as a \"(holy owned) land trust\" for Clan Donald from Somerled who built St. Oran's Chapel.\nThe 8th Duke of Argyll presented the sacred buildings and sites of the island to the Iona Cathedral trust in 1899. Historic Environment Scotland also recommends visiting the Augustinian nunnery, \"the most complete nunnery complex to survive in Scotland\". The nunnery was founded by Somerled's son, Reginald, as was Iona Abbey and Saddell Abbey. Reginald's sister, Beathag, was the first Prioress of the Iona Nunnery. The nunnery declined after the Scottish Reformation but was still used as a burial place for women.\nIn front of the Abbey stands the 9th century St Martin's Cross, one of the best-preserved Celtic crosses in the British Isles, and a replica of the 8th century St John's Cross (original fragments in the Abbey museum).\nThe ancient burial ground, called the R\u00e8ilig Odhrain (Eng: Oran's \"burial place\" or \"cemetery\"), contains the 12th century chapel built by Somerled where the Lords of the Isles were buried and named after St Odhr\u00e1n (said to be Columba's uncle). It was restored at the same time as the Abbey itself. It contains a number of medieval grave monuments. The abbey graveyard is said to contain the graves of many early Scottish Kings, as well as Norse kings from Ireland and Norway. Iona became the burial site for the kings of D\u00e1l Riata and their successors. Notable burials there include:\nIn 1549 an inventory of 48 Scottish, 8 Norwegian and 4 Irish kings was recorded. None of these graves are now identifiable (their inscriptions were reported to have worn away at the end of the 17th century). Saint Baithin and Saint Failbhe may also be buried on the island. The Abbey graveyard is also the final resting place of John Smith, the former Labour Party leader, who loved Iona. His grave is marked with an epitaph quoting Alexander Pope: \"An honest man's the noblest work of God\".\nLimited archaeological investigations commissioned by the National Trust for Scotland found some evidence for ancient burials in 2013. The excavations, conducted in the area of Martyrs Bay, revealed burials from the 6th\u20138th centuries, probably jumbled up and reburied in the 13\u201315th centuries.\nOther early Christian and medieval monuments have been removed for preservation to the cloister arcade of the Abbey, and the Abbey museum (in the medieval infirmary). The ancient buildings of Iona Abbey are now cared for by Historic Environment Scotland (there is an entrance charge to visit them).\nMarble quarry remains.\nThe remains of a marble quarrying enterprise are present in a small bay on the south-east shore of Iona. The quarry is the source of \"Iona Marble\", a translucent green and white stone, much used in brooches and other jewellery. The stone has been known of for centuries and was credited with healing and other powers. While the quarry had been used in a small way, it was not until around the end of the 18th century when it was opened up on a more industrial scale by the Duke of Argyle. The difficulties of extracting the hard stone and transporting it meant that the scheme was short lived. Another attempt was started in 1907, this time more successful with considerable quantities of stone extracted and indeed exported. The First World War impacted the quarry, with little quarrying after 1914 and the operation finally closed in 1919. A painting showing the quarry in operation, \"The Marble Quarry, Iona\" (1909) by David Young Cameron, is in the collection of Cartwright Hall art gallery in Bradford. The site has been designated as a Scheduled Ancient Monument.\nPresent day.\nThe island, other than the land owned by the Iona Cathedral Trust, was purchased from the Duke of Argyll by Hugh Fraser in 1979 and donated to the National Trust for Scotland. In 2001 Iona's population was 125 and by the time of the 2011 census this had grown to 177 usual residents. During the same period Scottish island populations as a whole grew by 4% to 103,702. The resident population in 2022 was stable at 178.\nThe island's tourism bureau estimated that roughly 130,000 visitors arrived each year. Many tourists come to visit the Abbey and other ecclesiastical properties and the marble quarry or to enjoy the nine beaches that are within walking distance of the main area.\nIona Community.\nNot to be confused with the local island community, Iona (Abbey) Community is based within Iona Abbey.\nIn 1938 George MacLeod founded the Iona Community, an ecumenical Christian community of men and women from different walks of life and different traditions in the Christian church committed to seeking new ways of living the Gospel of Jesus in today's world. This community is a leading force in the present Celtic Christian revival.\nThe Iona Community runs three residential centres on the Isle of Iona and on Mull, where one can live together in community with people of every background from all over the world. Weeks at the centres often follow a programme related to the concerns of the Iona Community.\nThe 8 tonne \"Fallen Christ\" sculpture by Ronald Rae was permanently situated outside the MacLeod Centre in February 2008.\nTransport.\nVisitors can reach Iona by the 10 minute ferry trip across the Sound of Iona from Fionnphort on Mull. The most common route from the mainland is via Oban in Argyll and Bute, where regular ferries connect to Craignure on Mull, from where the scenic road runs to Fionnphort. Tourist coaches and local bus services meet the ferries.\nCar ownership is lightly regulated, with no requirement for a MOT Certificate or payment of Road Tax for cars kept permanently on the island, but vehicular access is restricted to permanent residents and there are few cars. Visitors are not allowed to bring vehicles onto the island although \"blue badge holders with restricted mobility ... may apply for a permit under certain exemptions\". Visitors will find the village, the shops, the post office, the cafe, the hotels and the abbey are all within walking distance. Bike hire is available at the pier, and on Mull. Taxi service is also available.\n&lt;templatestyles src=\"Adjacent stations/styles.css\"/&gt;\nTourism.\nConde Nast Traveller recommends the island for its \"peaceful atmosphere ... a popular place for spiritual retreats\" but also recommends the \"sandy beaches, cliffs, rocks, fields and bogs ... \"wildflowers and birds such as the rare corncrake and puffins\" as well as the \"abundance of sea life\".\nThe Iona Council advises visitors that they can find a campsite (at Cnoc Oran), a hostel (at Lagandorain), family run bed and breakfasts and two hotels on the island in addition to several self-catering houses. The agency also mentions that distances are short, with the Abbey a mere 10 minutes walk from the pier. Tourists can rent bikes or use the local taxi.\nIona in Scottish painting.\nThe island of Iona has played an important role in Scottish landscape painting, especially during the Twentieth Century. As travel to north and west Scotland became easier from the mid-eighteenth century onwards, artists' visits to the island steadily increased. The Abbey remains in particular became frequently recorded during this early period. Many of the artists are listed and illustrated in the valuable book, \"Iona Portrayed \u2013 The Island through Artists' Eyes 1760\u20131960\", which lists over 170 artists known to have painted on the island.\nThe twentieth century, however, saw the greatest period of influence on landscape painting, in particular through the many paintings of the island produced by F. C. B. Cadell and S. J. Peploe, two of the \"Scottish Colourists\". As with many artists, both professional and amateur, they were attracted by the unique quality of light, the white sandy beaches, the aquamarine colours of the sea and the landscape of rich greens and rocky outcrops. While Cadell and Peploe are perhaps best known, many major Scottish painters of the C20 worked on Iona and visited many times \u2013 for example George Houston, D. Y. Cameron, James Shearer, John Duncan and John Maclauchlan Milne, among many.\nMedia and the arts.\nSamuel Johnson wrote \"That man is little to be envied whose patriotism would not gain force upon the plains of Marathon or whose piety would not grow warmer amid the ruins of Iona.\"\nIn Jules Verne's novel \"The Green Ray\", the heroes visit Iona in chapters 13 to 16. The inspiration is romantic, the ruins of the island are conducive to daydreaming. The young heroine, Helena Campbell, argues that Scotland in general and Iona in particular are the scene of the appearance of goblins and other familiar demons.\nIn Jean Raspail's novel \"The Fisherman's Ring\" (1995), his cardinal is one of the last to support the antipope Benedict XIII and his successors.\nIn the novel \"The Carved Stone\" (by Guillaume Pr\u00e9vost), the young Samuel Faulkner is projected in time as he searches for his father and lands on Iona in the year 800, then threatened by the Vikings.\n\"Peace of Iona\" is a song written by Mike Scott that appears on the studio album \"Universal Hall\" and on the live recording \"Karma to Burn\" by The Waterboys. Iona is the setting for the song \"Oran\" on the 1997 Steve McDonald album \"Stone of Destiny\".\nIona is featured prominently in the first episode (\"By the Skin of Our Teeth\") of the celebrated arts series \"\" (1969).\nThe Academy Award\u2013nominated Irish animated film \"The Secret of Kells\" is about the creation of the Book of Kells. One of the characters, Brother Aidan, is a master illuminator from Iona Abbey who had helped to illustrate the Book, but had to escape the island with it during a Viking invasion.\nNeil Gaiman's poem \"In Relig Odhrain\", published in \"Trigger Warning: Short Fictions and Disturbances (2015)\", retells the story of Oran's death, and the creation of the chapel on Iona. This poem was made into a short stop-motion animated film, released in 2019.\nThe island is the setting for David Greig\u2019s novel \u2018\u2019The Book of I,\u2019\u2019 2025, Europa Editions. The novel takes place in 825.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15040", "revid": "50471797", "url": "https://en.wikipedia.org/wiki?curid=15040", "title": "Ido", "text": "Constructed international auxiliary language\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nIdo () is a constructed language derived from a reformed version of Esperanto, and designed similarly with the goal of being a universal second language for people of diverse languages. To function as an effective \"international auxiliary language\", Ido was designed specifically to be grammatically, orthographically, and lexicographically regular (and, above all, easy to learn and use). It is the most successful of the many Esperanto derivatives, known as \"Esperantidoj.\"\nIdo was created in 1907 due to a desire to reform the perceived flaws of Esperanto, a language that had been created 20 years earlier to facilitate international communication. The name comes from the Esperanto word \"\", meaning \"offspring\", since the language is a derivative of Esperanto. After its inception, Ido was endorsed by some of the Esperanto community. A setback occurred with the sudden death in 1914 of one of its most influential proponents, Louis Couturat. In 1928, promoter Otto Jespersen quit the movement for his own language Novial.\nThe popularity of Ido decreased for two reasons: the emergence of further schisms developing from competing reform projects, and a general lack of awareness of Ido as a candidate for an international language. It was not until the spread of the Internet that it began to regain popularity.\nIdo uses the same 26 letters as the English (Latin) alphabet, with no diacritics. It draws its vocabulary from English, French, German, Italian, Latin, Russian, Spanish and Portuguese, and is largely intelligible to those who have studied Esperanto.\nSeveral works of literature have been translated into Ido, including \"The Little Prince\", the Book of Psalms, and the Gospel of Luke. As of the year 2000, there were approximately 100\u2013200 Ido speakers in the world. As of 2022, Ido has 26 speakers in Finland, according to Statistics Finland.\nHistory.\nThe idea of a universal second language is not new, and constructed languages are not a recent phenomenon. During the 12th century Hildegard of Bingen invented a set of words known as the Lingua Ignota. The concept did not attract significant interest until the language Volap\u00fck was created in 1879. Volap\u00fck was popular for some time and apparently had a few thousand users, but was later eclipsed by the popularity of Esperanto, which was created in 1887. Several other languages, such as Latino sine Flexione and Idiom Neutral were also proposed. It was during this time that French mathematician Louis Couturat formed the \"Delegation for the Adoption of an International Auxiliary Language\".\nThis delegation made a formal request to the International Association of Academies in Vienna to select and endorse an international language; the request was rejected in May 1907. The Delegation then met as a Committee in Paris in October 1907 to discuss the adoption of a standard international language. Among the languages considered was a new language submitted anonymously after the Committee's deadline by someone using the name \"Ido\". In the end the committee, always without plenary sessions and consisting of only 12 members, concluded the last day with 4 votes for and 1 abstention. They concluded that no language was completely acceptable, but that Esperanto could be accepted \"on condition of several modifications to be realized by the permanent Commission in the direction defined by the conclusions of the Report of the Secretaries [Louis Couturat and L\u00e9opold Leau] and by the Ido project\".\nEsperanto's inventor, L. L. Zamenhof, having heard a number of complaints, had suggested in 1894 a proposal for a reformed Esperanto with several changes that Ido adopted: eliminating the accented letters and the accusative case, changing the plural to an Italianesque \"-i\", and replacing the table of correlatives with more Latinate words. However, the Esperanto community voted and rejected Zamenhof's reformed Esperanto, and likewise most rejected the recommendations of the 1907 Committee composed nominally of 12 members. Zamenhof, undoubtedly reminiscent of his experience of the 1894 reforms, strongly supported the Esperanto Committee majority decision. Furthermore, controversy ensued when the \"Ido project\" was found to have been devised mainly by Louis de Beaufront, whom Zamenhof had chosen to represent Esperanto to the committee (Zamenhof himself could not represent Esperanto as the committee's rules dictated that the creator of a submitted language could not defend it). The Committee's meetings were performed mainly in French, with occasional German. When the president of the Committee asked who was the author of Ido's project, Couturat, de Beaufront and Leau answered that they were not. De Beaufront presented Ido's project and gave a description of it as a better, richer version of Esperanto. Couturat, Leau, de Beaufront and Jespersen were finally the only members who voted, all of them for Ido's project. A month later, Couturat accidentally forwarded Jespersen a copy of a letter in which he acknowledged that de Beaufront was the author of the Ido project. Jespersen was angered by this and asked for a public confession. De Beaufront procrastinated for four months before making a public confession.\nIt is estimated that some 20% of Esperanto's major promoters and 3\u20134% of ordinary Esperantists switched to Ido, which from then on suffered constant modifications seeking to perfect it, but which ultimately had the effect of causing many Ido speakers to abandon trying to learn it. Although it divided the Esperanto movement, the schism gave the remaining Esperantists the freedom to concentrate on using and promoting their language as it was. At the same time, it gave the Idists freedom to continue working on their own language for several more years before actively promoting it. The \"Uniono di la Amiki di la Linguo Internaciona\" (\"Union of Friends of the International Language\") was established along with an Ido Academy to develop the details of the new language.\nCouturat, who was the main proponent of Ido, was killed in an automobile accident in 1914. This, along with World War I, practically suspended the activities of the Ido Academy from 1914 to 1920. In 1928 Ido's major intellectual promoter, the Danish linguist Otto Jespersen, published his own planned language, Novial and ended his promotion of Ido.\nDigital era.\nThe language still has active speakers, numbering about 500. The Internet has caused a renewal of interest in the language during recent years. A sample of 24 Idists on the Yahoo! group \"Idolisto\" during November 2005 showed that 57% had begun their studies of the language during the preceding three years, 32% from the mid-1990s to 2002, and 8% had known the language from before.\nChanges.\nFew changes have been made to Ido since 1922.\nCamiel de Cock was named secretary of linguistic issues in 1990, succeeding Roger Moureaux. He resigned after the creation of a linguistic committee in 1991. De Cock was succeeded by Robert C. Carnaghan, who had the title from 1992 to 2008. No new words were adopted between 2001 and 2006. After the 2008\u20132011 elections of ULI's direction committee, Gon\u00e7alo Neves replaced Carnaghan as secretary of linguistic issues during February 2008. Neves resigned during August 2008. A new linguistic committee was formed in 2010. In April 2010, Tiberio Madonna was appointed as secretary of linguistic issues, succeeding Neves. \nIn January 2011, ULI approved eight new words. This was the first addition of words in many years. After a series of severe conflicts with the Directing Committee of ULI, Tiberio Madonna was revoked as secretary of linguistic issues on the 26th of May 2013 by official announcement from Lo\u00efs Landais, the secretary of ULI.\nDuring January 2022, ULI approved a set of new words (34)\nPhonology.\nIdo has five vowel phonemes. The values and are interchangeable depending on speaker preference, as are and . The orthographic sequences \u27e8au\u27e9 and \u27e8eu\u27e9 indicate diphthongs in word roots but not when created by affixing.\nAll polysyllabic words are stressed on the second-to-last syllable except for verb infinitives, which are stressed on the last syllable\u00a0\u2013 skolo, kafeo and lernas for \"school\", \"coffee\" and the present tense of \"to learn\", but irar, savar and drinkar for \"to go\", \"to know\" and \"to drink\". If an i or u precedes another vowel, the pair is considered part of the same syllable when applying the accent rule\u00a0\u2013 thus radio, familio and manuo for \"radio\", \"family\" and \"hand\", unless the two vowels are the only ones in the word, in which case the \"i\" or \"u\" is stressed: dio, frua for \"day\" and \"early\".\nOrthography.\nIdo uses the same 26 letters as the English alphabet and ISO Basic Latin alphabet with three digraphs and no ligatures or diacritics. Where the table below lists two pronunciations, either is perfectly acceptable.\nThe digraphs are:\nGrammar.\nThe definite article is \"la\" and is invariable. The indefinite article (a/an) does not exist in Ido. Each word in the Ido vocabulary is built from a root word. A word consists of a root and a grammatical ending. Other words can be formed from that word by removing the grammatical ending and adding a new one, or by inserting certain affixes between the root and the grammatical ending.\nSome of the grammatical endings are defined as follows:\nThese are the same as in Esperanto except for \"-i\", \"-ir\", \"-ar\", \"-or\" and \"-ez\". Esperanto marks noun plurals by an \"agglutinative\" ending \"-j\" (so plural nouns end in \"-oj\"), uses \"-i\" for verb infinitives (Esperanto infinitives are tenseless), and uses \"-u\" for the imperative. Verbs in Ido, as in Esperanto, do not conjugate depending on person, number or gender; the -as, -is, and -os endings suffice whether the subject is I, you, he, she, they, or anything else. For the word \"to be,\" Ido allows either \"esas\" or \"es\" in the present tense; however, the full forms must be used for the past tense \"esis\" and future tense \"esos\".\" Adjectives and adverbs are compared in Ido by means of the words \"plu\" = more, \"maxim\" = most, \"min\" = less, \"minim\" = least, \"kam\" = than/as. There exist in Ido three categories of adverbs: the simple, the derived, and the composed. The simple adverbs do not need special endings, for example: \"tre\" = very, \"tro\" = too, \"olim\" = formerly, \"nun\" = now, \"nur\" = only. The derived and composed adverbs, not being originally adverbs but derived from nouns, adjectives and verbs, have the ending -e.\nSyntax.\nIdo word order is generally the same as English (subject\u2013verb\u2013object), so the sentence \"Me havas la blua libro\" is the same as the English \"I have the blue book\", both in meaning and word order. There are a few differences, however:\nIdo generally does not impose rules of grammatical agreement between grammatical categories within a sentence. For example, the verb in a sentence is invariable regardless of the number and person of the subject. Nor must the adjectives be pluralized as well the nouns\u00a0\u2013 in Ido \"the large books\" would be \"la granda libri\" as opposed to the Esperanto \"la grandaj libroj\".\nNegation occurs in Ido by simply adding ne before a verb: Me ne havas libro means \"I do not have a book\". This as well does not vary, and thus the \"I do not\", \"He does not\", \"They do not\" before a verb are simply Me ne, Il ne, and Li ne. In the same way, past tense and future tense negatives are formed by ne before the conjugated verb. \"I will not go\" and \"I did not go\" become Me ne iros and Me ne iris respectively.\nYes/no questions are formed by the particle ka in front of the question. \"I have a book\" (me havas libro) becomes Ka me havas libro? (do I have a book?). Ka can also be placed in front of a noun without a verb to make a simple question, corresponding to the English \"is it?\" Ka Mark? can mean, \"Are you Mark?\", \"Is it Mark?\", \"Do you mean Mark?\" depending on the context.\nPronouns.\nThe pronouns of Ido were revised to make them more distinct acoustically than those of Esperanto, which all end in \"i\". Especially the singular and plural first-person pronouns \"mi\" and \"ni\" may be difficult to distinguish in a noisy environment, so Ido has \"me\" and \"ni\" instead. Ido also distinguishes between intimate (\"tu\") and formal (\"vu\") second-person singular pronouns as well as plural second-person pronouns (\"vi\") not marked for intimacy. Furthermore, Ido has a pan-gender third-person pronoun \"lu\" (it can mean \"he\", \"she\", or \"it\", depending on the context) in addition to its masculine (\"il\"), feminine (\"el\"), and neuter (\"ol\") third-person pronouns.\n\"ol\", like English \"it\" and Esperanto \"\u011di\", is not limited to inanimate objects, but can be used \"for entities whose sex is indeterminate: \"babies, children, humans, youths, elders, people, individuals, horses, [cattle], cats,\" etc.\"\n\"Lu\" is often mistakenly labeled an epicene pronoun, that is, one that refers to both masculine and feminine beings, but in fact, \"lu\" is more properly a \"pan-gender\" pronoun, as it is also used for referring to inanimate objects. From \"Kompleta Gramatiko Detaloza di la Linguo Internaciona Ido\" by Beaufront:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Lu\" (like \"li\") is used \"for all three genders.\" That \"lu\" does duty for the three genders at will in the singular is not in itself any more astonishing than seeing \"li\" serve the three genders at will in the plural ... By a decision (1558) the Idist Academy rejected every restriction concerning the use of \"lu.\" One may thus use that pronoun in exactly the same way for a thing and a person of obvious sex as for animals of unknown sex and a person that has a genderless name, like \"baby, child, human,\" etc., these being as truly masculine as feminine.\nThe motives for this decision were given in \"Mondo\", XI, 68: \"Lu\" for the singular is exactly the same as \"li\" for the plural. Logic, symmetry and ease demand this. Consequently, just as \"li\" may be used for people, animals, and objects whenever nothing obliges one to express the gender, so \"lu\" may be used for people, animals, and objects by the same condition. The proposed distinction would be a bothersome subtlety\u00a0...\nTable of correlatives.\nIdo makes correlatives by combining entire words together and changing the word ending, with some irregularities to show distinction.\nCompound formation.\nComposition in Ido obeys stricter rules than in Esperanto, especially formation of nouns, adjectives and verbs from a radical of a different class. The reversibility principle assumes that for each composition rule (affix addition), the corresponding decomposition rule (affix removal) is valid.\nHence, while in Esperanto an adjective (for instance ), formed on the noun radical , can mean an attribute ( \"paper-made encyclopedia\") and a relation ( \"paper-making factory\"), Ido will distinguish the attribute (\"paper\" or \"of paper\" (not \"paper-made\" exactly)) from the relation (\"paper-making\").\nSimilarly, means in both Esperanto and Ido the noun \"crown\"; where Esperanto allows formation of \"to crown\" by simply changing the ending from noun to verb (\"crowning\" is ), Ido requires an affix so the composition is reversible: (\"the act of crowning\" is ).\nAccording to Claude Piron, some modifications brought by Ido are in practice impossible to use and ruin spontaneous expression: \nIdo displays, on linguistic level, other drawbacks Esperanto succeeded to avoid, but I don't have at hand documents which would allow me to go further in detail. For instance, if I remember correctly, where Esperanto only has the suffix *, Ido has several: **, **, **, which match subtleties which were meant to make language clearer, but that, in practice, inhibit natural expression.\nVocabulary.\nVocabulary in Ido is derived from French, Italian, Spanish, English, German, and Russian. Basing the vocabulary on various widespread languages was intended to make Ido as easy as possible for the greatest number of people possible. Early on, the first 5,371 Ido word roots were analyzed compared to the vocabulary of the six source languages, and the following result was found:\nAnother analysis showed that:\nVocabulary in Ido is often created through a number of official prefixes and suffixes that alter the meaning of the word. This allows a user to take existing words and modify them to create neologisms when necessary, and allows for a wide range of expression without the need to learn new vocabulary each time. Though their number is too large to be included in one article, some examples include:\nNew vocabulary is generally created through an analysis of the word, its etymology, and reference to the six source languages. If a word can be created through vocabulary already existing in the language then it will usually be adopted without need for a new radical (such as wikipedio for \"Wikipedia\", which consists of wiki + enciklopedio for \"encyclopedia\"), and if not then an entirely new word will be created. The word alternatoro for example was adopted in 1926, likely because five of the six source languages used largely the same orthography for the word, and because it was long enough to avoid being mistaken for other words in the existing vocabulary. Adoption of a word is done through consensus, after which the word will be made official by the union. Care must also be taken to avoid homonyms if possible, and usually a new word undergoes some discussion before being adopted. Foreign words that have a restricted sense and are not likely to be used in everyday life (such as the word \"intifada\" to refer to the conflict between Israel and Palestine) are left untouched, and often written in italics.\nIdo, unlike Esperanto, does not assume the male sex by default. For example, Ido does not derive the word for \"waitress\" by adding a feminine suffix to \"waiter\", as Esperanto does. Instead, Ido words are defined as sex-neutral, and two different suffixes derive masculine and feminine words from the root: ' for a waiter of either sex, ' for a male waiter, and ' for a waitress. There are only two exceptions to this rule: First, ' for \"father\", ' for \"mother\", and ' for \"parent\", and second, ' for \"man\", ' for \"woman\", and \"\" for \"adult\".\nSample.\nThe Lord's Prayer:\nLiterature and publications.\nIdo has a number of publications that can be subscribed to or downloaded for free in most cases. is a magazine produced in France every few months with a range of topics. is a magazine produced by the Spanish Ido Society every two months that has a range of topics, as well as a few dozen pages of work translated from other languages. is the official organ for the language and has existed since the beginning of Idoism in 1908. Other sites can be found with various stories, fables or proverbs along with a few books of the Bible translated into Ido on a smaller scale. The site has a few podcasts in Ido along with various songs and other recorded material.\nWikipedia includes an http:// (known in Ido as ); in 2018 it was the 93rd most visited Wikipedia, and is second most viewed Wikipedia edition in artificial language (after Esperanto).\nSymbols of Ido.\nThe or Jank\u00f3 star is the main symbol of Ido. It is a six pointed star, with the points representing Ido's six source languages: English, French, Italian, German, Spanish and Russian. Alternatively, the six points represent the six continents (excluding Antarctica). The emblem was originally a six pointed white star on a circular blue background, consisting of two concentric, equilateral triangles, with one vertically flipped. However, this was soon changed due to the similarity it presented with the Star of David, since a true international auxiliary language should not have religious affiliations.\nAfter a search to find an appropriate new symbol, the Ido-Akademio decided on the current Ido symbol, created by their secretary, Paul von Jank\u00f3 (hence the alternative name the Jank\u00f3 star). The current Ido Star is a concave isotoxal hexagon, with a vertically flipped equilateral triangle overlaid on top.\nInternational Ido conventions.\nULI organises Ido conventions yearly, and the conventions include a mix of tourism and work.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAdditional notes"}
{"id": "15041", "revid": "50770307", "url": "https://en.wikipedia.org/wiki?curid=15041", "title": "Improvisational theatre", "text": "Theatrical genre featuring unscripted performance\nImprovisational theatre, often called improvisation, improv or impro in British English, is the form of theatre, often comedy, in which most or all of what is performed is unplanned or unscripted, created spontaneously by the performers. In its purest form, the dialogue, action, story, and characters are created collaboratively by the players as the improvisation unfolds in present time, without use of an already prepared, written script.\nImprovisational theatre exists in performance as a range of styles of improvisational comedy as well as some non-comedic theatrical performances. It is sometimes used in film and television, both to develop characters and scripts and occasionally as part of the final product.\nImprovisational techniques are often used extensively in drama programs to train actors for stage, film, and television and can be an important part of the rehearsal process. However, the skills and processes of improvisation are also used outside the context of performing arts. This practice, known as applied improvisation, is used in classrooms as an educational tool and in businesses as a way to develop communication skills, creative problem solving, and supportive team-work abilities that are used by improvisational, ensemble players. It is sometimes used in psychotherapy as a tool to gain insight into a person's thoughts, feelings, and relationships.\nHistory.\nThe earliest well-documented use of improvisational theatre in Western history is found in the Atellan Farce of 391 BC. From the 16th to the 18th centuries, \"commedia dell'arte\" performers improvised based on a broad outline in the streets of Italy. In the 1890s, theatrical theorists and directors such as the Russian Konstantin Stanislavski and the French Jacques Copeau, founders of two major streams of acting theory, both heavily utilized improvisation in acting training and rehearsal.\nModern.\nModern theatrical improvisation games began as drama exercises for children, which were a staple of drama education in the early 20th century thanks in part to the progressive education movement initiated by John Dewey in 1916. Some people credit American Dudley Riggs as the first vaudevillian to use audience suggestions to create improvised sketches on stage. Improvisation exercises were developed further by Viola Spolin in the 1940s, 50s, and 60s, and codified in her book \"Improvisation For The Theater\", the first book that gave specific techniques for learning to do and teach improvisational theatre. In 1977, Clive Barker's book \"Theatre Games\" (several translations and editions) spread the ideas of improv internationally. British playwright and director Keith Johnstone wrote \"\", a book outlining his ideas on improvisation, and invented Theatresports, which has become a staple of modern improvisational comedy and is the inspiration for the popular television show \"Whose Line Is It Anyway?\"\nViola Spolin influenced the first generation of modern American improvisers at The Compass Players in Chicago, which led to The Second City. Her son, Paul Sills, along with David Shepherd, started The Compass Players. Following the demise of the Compass Players, Paul Sills began The Second City. They were the first organized improv troupes in Chicago, and the modern Chicago improvisational comedy movement grew from their success.\nMany of the current \"rules\" of comedic improv were first formalized in Chicago in the late 1950s and early 1960s, initially among The Compass Players troupe, which was directed by Paul Sills. From most accounts, David Shepherd provided the philosophical vision of the Compass Players, while Elaine May was central to the development of the premises for its improvisations. Mike Nichols, Ted Flicker, and Del Close were her most frequent collaborators in this regard. When The Second City opened its doors on December 16, 1959, directed by Paul Sills, his mother Viola Spolin began training new improvisers through a series of classes and exercises which became the cornerstone of modern improv training. By the mid-1960s, Viola Spolin's classes were handed over to her prot\u00e9g\u00e9, Jo Forsberg, who further developed Spolin's methods into a one-year course, which eventually became The Players Workshop, the first official school of improvisation in the United States. During this time, Forsberg trained many of the performers who went on to star on The Second City stage.\nMany of the original cast of \"Saturday Night Live\" came from The Second City, and the franchise has produced such comedy stars as Mike Myers, Tina Fey, Bob Odenkirk, Amy Sedaris, Stephen Colbert, Eugene Levy, Jack McBrayer, Steve Carell, Chris Farley, Dan Aykroyd, and John Belushi.\nSimultaneously, Keith Johnstone's group The Theatre Machine, which originated in London, was touring Europe. This work gave birth to Theatresports, at first secretly in Johnstone's workshops, and eventually in public when he moved to Calgary. Toronto has been home to a rich improv tradition.\nIn 1984, Dick Chudnow (Kentucky Fried Theater) founded ComedySportz in Milwaukee, WI. Expansion began with the addition of ComedySportz-Madison (WI), in 1985. The first Comedy League of America National Tournament was held in 1988, with 10 teams participating. The league is now known as CSz Worldwide and boasts a roster of 29 international cities.\nIn San Francisco, The Committee theater was active in North Beach during the 1960s. It was founded by alumni of Chicago's Second City, Alan Myerson and his wife Jessica. When The Committee disbanded in 1972, three major companies were formed: The Pitchell Players, The Wing, and Improvisation Inc. The only company that continued to perform Close's Harold was the latter one. Its two former members, Michael Bossier and John Elk, formed Spaghetti Jam in San Francisco's Old Spaghetti Factory Cafe in 1976, where shortform improv and Harolds were performed through 1983. Stand-up comedians performing down the street at the Intersection for the Arts would drop by and sit in. In 1979, Elk brought shortform to England, teaching workshops at Jacksons Lane Theatre, and he was the first American to perform at The Comedy Store, London, above a Soho strip club.\nModern political improvisation's roots include Jerzy Grotowski's work in Poland during the late 1950s and early 1960s, Peter Brook's \"happenings\" in England during the late 1960s, Augusto Boal's \"Forum Theatre\" in South America in the early 1970s, and San Francisco's The Diggers' work in the 1960s. Some of this work led to pure improvisational performance styles, while others simply added to the theatrical vocabulary and were, on the whole, avant-garde experiments.\nJoan Littlewood, an English actress and director who was active from the 1950s to 1960s, made extensive use of improv in developing plays for performance. However, she was successfully prosecuted twice for allowing her actors to improvise in performance. Until 1968, British law required scripts to be approved by the Lord Chamberlain's Office. The department also sent inspectors to some performances to check that the approved script was performed exactly as approved.\nIn 1987, Annoyance Theatre began as a club in Chicago that emphasizes longform improvisation. The Annoyance Theatre has grown into multiple locations in Chicago and New York City. It is the home of the longest running musical improv show in history at 11 years.\nIn 2012, Lebanese writer and director Lucien Bourjeily used improvisational theatre techniques to create a multi-sensory play entitled \"66 Minutes in Damascus\". This play premiered at the London International Festival of Theatre, and is considered one of the most extreme kinds of interactive improvised theatre put on stage. The audience play the part of kidnapped tourists in today's Syria in a hyperreal sensory environment.\nRob Wittig and Mark C. Marino have developed a form of improv for online theatrical improvisation called netprov. The form relies on social media to engage audiences in the creation of dynamic fictional scenarios that evolve in real-time.\nComedy.\nModern improvisational comedy, as it is practiced in the West, falls generally into two categories: shortform and longform.\nShortform improv consists of short scenes usually constructed from a predetermined game, structure, or idea and driven by an audience suggestion. Many shortform exercises were first created by Viola Spolin, who called them theatre games, influenced by her training from recreational games expert Neva Boyd. One example of a game would be performers creating a scene and a \"director\" performer pointing to one performer after a line repeatedly. Each time, the performer being pointed to will rewind and say a new line. The shortform improv comedy television series \"Whose Line Is It Anyway?\" has familiarized American and British viewers with shortform.\nLongform improv performers create shows in which short scenes are often interrelated by story, characters, or themes. Longform shows may take the form of an existing type of theatre, for example a full-length play or Broadway-style musical such as Spontaneous Broadway. One of the better-known longform structures is the Harold, developed by ImprovOlympic co-founder Del Close. Many such longform structures now exist. Actors such as Will Ferrell, Tina Fey, and Steve Carell found their start in longform improv. \nLongform improvisation is especially performed in Chicago, New York City, Los Angeles, Austin, Atlanta, Dallas, Boston, Minneapolis, Phoenix, Philadelphia, San Francisco, Seattle, Detroit, Toronto, Vancouver, and Washington, D.C., and is building a growing following in Baltimore, Denver, Kansas City, Montreal, Columbus, New Orleans, Omaha, Rochester, NY, and Hawaii. Outside the United States, longform improv has a growing presence in the United Kingdom, especially in cities such as London, Bristol, Glasgow, and at the Edinburgh Festival Fringe.\nNon-comedic, experimental, and dramatic, narrative-based improvisational theatre.\nOther forms of improvisational theatre training and performance techniques are experimental and avant-garde in nature and not necessarily intended to be comedic. These include Playback Theatre and Theatre of the Oppressed, the Poor Theatre, The Open Theater, to name only a few.\nThe Open Theater was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. This avant-garde theatre group explored political, artistic, and social issues. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as \"sound and movement\" and \"transformations\", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's \"poor theatre\" in Poland. During the sixties, Chaikin and the Open Theatre developed full theatrical productions with nothing but the actors, a few chairs, and a bare stage, creating character, time, and place through a series of transformations the actors physicalized and discovered through improvisations.\nOn the west coast, Ruth Zaporah developed Action Theatre, a physically based improvisation form that treats language, movement and voice equally. Action Theatre performances have no scripts, no preplanned ideas and create full-length shows or shorter performances. Longform, dramatic, and narrative-based improvisation is well-established on the west coast with companies such as San Francisco's BATS Improv. This format allows for full-length plays and musicals to be created improvisationally.\nApplying improv principles in life.\nBeginning in the late 90's, the field of applied improvisation was born as a way to use improv as a tool outside of the performative space and many people who have studied improv have noted that the guiding principles of improv are useful, not just on stage, but in everyday life. For example, Stephen Colbert in a commencement address said,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Well, you are about to start the greatest improvisation of all. With no script. No idea what's going to happen, often with people and places you have never seen before. And you are not in control. So say \"yes.\" And if you're lucky, you'll find people who will say \"yes\" back.\nTina Fey, in her book \"Bossypants\", lists several rules of improv that apply in the workplace. There has been much interest in bringing lessons from improv into the corporate world. In a \"New York Times\" article titled https://, Stanford professor and author, Patricia Ryan Madson notes, \"executives and engineers and people in transition are looking for support in saying yes to their own voice. Often, the systems we put in place to keep us secure are keeping us from our more creative selves.\"\nIn film and television.\nMany directors have made use of improvisation in the creation of both mainstream and experimental films. Many silent filmmakers such as Charlie Chaplin and Buster Keaton used improvisation in the making of their films, developing their gags while filming and altering the plot to fit. The Marx Brothers were notorious for deviating from the script they were given, their ad libs often becoming part of the standard routine and making their way into their films. Many people, however, make a distinction between ad-libbing and improvising.\nThe British director Mike Leigh makes extensive use of improvisation in the creation of his films, including improvising important moments in the characters' lives that will not even appear in the film. \"This Is Spinal Tap\" and other mockumentary films of director Christopher Guest were created with a mix of scripted and unscripted material. \"Blue in the Face\" is a 1995 comedy directed by Wayne Wang and Paul Auster created in part by the improvisations during the filming of \"Smoke\".\nSome of the best known American film directors who used improvisation in their work with actors are John Cassavetes, Robert Altman, Christopher Guest, and Rob Reiner.\nImprov comedy techniques have also been used in hit television shows such as HBO's \"Curb Your Enthusiasm\" created by Larry David, the UK Channel 4 and ABC television series \"Whose Line Is It Anyway\" (and its spinoffs \"Drew Carey's Green Screen Show\" and \"Drew Carey's Improv-A-Ganza\"), Nick Cannon's improv comedy show \"Wild 'n Out\", and \"Thank God You're Here\". A very early American improv television program was the weekly half-hour \"What Happens Now?\" which premiered on New York's WOR-TV on October 15, 1949, and ran for 22 episodes. \"The Improvisers\" were six actors (including Larry Blyden, Ross Martin, and Jean Alexander \u2013 Jean Pugsley at the time) who improvised skits based on situations suggested by viewers. In Canada, the series \"Train 48\" was improvised from scripts which contained a minimal outline of each scene, and the comedy series \"This Sitcom Is...Not to Be Repeated\" incorporated dialogue drawn from a hat during the course of an episode. The American show \"Reno 911!\" also contained improvised dialogue based on a plot outline. \"Fast and Loose\" is an improvisational game show, much like \"Whose Line Is It Anyway?\" The BBC sitcoms \"Outnumbered\" and \"The Thick of It\" also had some improvised elements in them.\nPsychology.\nIn the field of the psychology of consciousness, Eberhard Scheiffele explored the altered state of consciousness experienced by actors and improvisers in his scholarly paper \"Acting: an altered state of consciousness\". According to G. William Farthing in \"The Psychology of Consciousness\" comparative study, actors routinely enter into an altered state of consciousness (ASC). Acting is seen as altering most of the 14 dimensions of changed subjective experience which characterize ASCs according to Farthing, namely: attention, perception, imagery and fantasy, inner speech, memory, higher-level thought processes, meaning or significance of experiences, time experience, emotional feeling and expression, level of arousal, self-control, suggestibility, body image, and sense of personal identity.\nIn the field of cognitive psychology and neuropsychology, improv seems to involve a lot of cognitive processes, especially attention. Improv can improve creativity, dealing with uncertainty, narrative skills and decrease anxiety with teenagers, young and older adults.\nIn the growing field of drama therapy, psychodramatic improvisation, along with other techniques developed for drama therapy, are used extensively. The \"Yes, and...\" rule has been compared to Milton H. Erickson's \"utilization\" process and to a variety of acceptance-based psychotherapies. Improv training has been recommended for couples therapy and therapist training, and it has been speculated that improv training may be helpful in some cases of social anxiety disorder.\nStructure and process.\nImprovisational theatre often allows an interactive relationship with the audience. Improv groups frequently solicit suggestions from the audience as a source of inspiration, a way of getting the audience involved, and as a means of proving that the performance is not scripted. That charge is sometimes aimed at the masters of the art, whose performances can seem so detailed that viewers may suspect the scenes are planned.\nIn order for an improvised scene to be successful, the improvisers involved must work together responsively to define the parameters and action of the scene, in a process of co-creation. With each spoken word or action in the scene, an improviser makes an \"offer\", meaning that he or she defines some element of the reality of the scene. This might include giving another character a name, identifying a relationship, location, or using mime to define the physical environment. These activities are also known as \"endowment\". It is the responsibility of the other improvisers to accept the offers that their fellow performers make; to not do so is known as blocking, negation, or denial, which usually prevents the scene from developing. Some performers may deliberately block (or otherwise break out of character) for comedic effect\u2014this is known as \"gagging\"\u2014but this generally prevents the scene from advancing and is frowned upon by many improvisers. Accepting an offer is usually accompanied by adding a new offer, often building on the earlier one; this is a process improvisers refer to as \"Yes, and...\" and is considered the cornerstone of improvisational technique. Every new piece of information added helps the improvisers to refine their characters and progress the action of the scene. The \"Yes, and...\" rule, however, applies to a scene's early stage since it is in this stage that a \"base (or shared) reality\" is established in order to be later redefined by applying the \"if (this is true), then (what else can also be true)\" practice progressing the scene into comedy, as explained in the 2013 manual by the \"Upright Citizens Brigade\" members.\nThe unscripted nature of improv also implies no predetermined knowledge about the props that might be useful in a scene. Improv companies may have at their disposal some number of readily accessible props that can be called upon at a moment's notice, but many improvisers eschew props in favor of the infinite possibilities available through mime. In improv, this is more commonly known as 'space object work' or 'space work', rather than 'mime', and the props and locations created by this technique, as 'space objects' created out of 'space substance', developed as a technique by Viola Spolin. As with all improv 'offers', improvisers are encouraged to respect the validity and continuity of the imaginary environment defined by themselves and their fellow performers; this means, for example, taking care not to walk through the table or \"miraculously\" survive multiple bullet wounds from another improviser's gun.\nBecause improvisers may be required to play a variety of roles without preparation, they need to be able to construct characters quickly with physicality, gestures, accents, voice changes, or other techniques as demanded by the situation. The improviser may be called upon to play a character of a different age or sex. Character motivations are an important part of successful improv scenes, and improvisers must therefore attempt to act according to the objectives that they believe their character seeks.\nIn improv formats with multiple scenes, an agreed-upon signal is used to denote scene changes. Most often, this takes the form of a performer running in front of the scene, known as a \"wipe\". Tapping a character in or out can also be employed. The performers not currently part of the scene often stand at the side or back of the stage, and can enter or exit the scene by stepping into or out of the stage center.\nCommunity.\nMany theatre troupes are devoted to staging improvisational performances and growing the improv community through their training centers.\nIn addition to for-profit theatre troupes, there are many college-based improv groups in the United States and around the world.\nIn Europe the special contribution to the theatre of the abstract, the surreal, the irrational and the subconscious have been part of the stage tradition for centuries. From the 1990s onwards a growing number of European Improv groups have been set up specifically to explore the possibilities offered by the use of the abstract in improvised performance, including dance, movement, sound, music, mask work, lighting, and so on. These groups are not especially interested in comedy, either as a technique or as an effect, but rather in expanding the improv genre so as to incorporate techniques and approaches that have long been a legitimate part of European theatre.\nNotable contributors to the field.\nThe Brave New Workshop Comedy Theater (BNW), is a sketch and improvisational comedy theatre based in Minneapolis, Minnesota. Started by Dudley Riggs in 1958, the artists of the BNW have been writing, performing and producing live sketch comedy and improvisation performances for 62 years \u2013 longer than any other theatre in the nation. Notable alumni of the BNW include Louie Anderson, Mo Collins, Tom Davis, Al Franken, Penn Jillette, Carl Lumbly, Paul Menzel, Pat Proft, Annie Reirson, Taylor Nikolai, Nancy Steen, Peter Tolan, Linda Wallem, Lizz Winstead, Peter MacNicol, Melissa Peterman, and Cedric Yarbrough.\nSome key figures in the development of improvisational theatre are Viola Spolin and her son Paul Sills, founder of Chicago's famed Second City troupe and originator of theatre games, and Del Close, founder of ImprovOlympic (along with Charna Halpern) and creator of a popular longform improv format known as the Harold. Others include Keith Johnstone, the British teacher and writer\u2013author of \"Impro\", who founded the Theatre Machine and whose teachings form the foundation of the popular shortform Theatresports format, Dick Chudnow, founder of ComedySportz which evolved its family-friendly show format from Johnstone's Theatersports, and Bill Johnson, creator/director of The Magic Meathands, who pioneered the concept of \"Commun-edy Outreach\" by tailoring performances to non-traditional audiences, such as the homeless and foster children.\nDavid Shepherd, with Paul Sills, founded the Compass Players in Chicago. Shepherd was intent on developing a true \"people's Theatre\", and hoped to bring political drama to the stockyards. The Compass went on to play in numerous forms and companies, in a number of cities including New York and Hyannis, after the founding of The Second City. A number of Compass members were also founding members of The Second City. In the 1970s, Shepherd began experimenting with group-created videos. He is the author of \"That Movie In Your Head\", about these efforts. In the 1970s, David Shepherd and Howard Jerome created the Improvisational Olympics, a format for competition based improv. The Improv Olympics were first demonstrated at Toronto's Homemade Theatre in 1976 and have been continued on as the Canadian Improv Games. In the United States, the Improv Olympics were later produced by Charna Halpern under the name \"ImprovOlympic\" and now as \"IO\"; IO operates training centres and theatres in Chicago and Los Angeles. At IO, Halpern combined Shepherd's \"Time Dash\" game with Del Close's \"Harold\" game; the revised format for the Harold became the fundamental structure for the development of modern longform improvisation.\nIn 1975 Jonathan Fox founded Playback Theatre, a form of improvised community theatre which is often not comedic and replays stories as shared by members of the audience.\nThe Groundlings is a popular and influential improv theatre and training center in Los Angeles, California. The late Gary Austin, founder of The Groundlings, taught improvisation around the country, focusing especially in Los Angeles. He was widely acclaimed as one of the greatest acting teachers in America. His work was grounded in the lessons he learned as an improviser at The Committee with Del Close, as well as in his experiences as founding director of The Groundlings. The Groundlings is often seen as the Los Angeles training ground for the \"second generation\" of improv performers and troupes. Stan Wells developed the \"Clap-In\" style of longform improvisation here, later using this as the basis for his own theatre, The Empty Stage, which in turn bred multiple troupes utilizing this style. David Koff, one of Stan's longtime students has brought Stan's philosophies to longform improv and his Clap-In style of editing to his Change Through Play Improv Studio in Portland, Oregon where he uses it to train his students for the stage.\nIn the late 1990s, Matt Besser, Amy Poehler, Ian Roberts, and Matt Walsh founded the Upright Citizens Brigade Theatre in New York and later they founded one in Los Angeles, each with an accompanying improv/sketch comedy school. In September 2011 the UCB opened a third theatre in New York City's East Village, known as UCBeast.\nHoopla Impro are the founders of the longest running improv theatre in the UK. They also run an annual UK improv festival and improv marathon.\nIn 2015, The Free Association opened in London as a counterpart to American improv schools.\nIn 2016, The Glasgow Improv Theatre started putting on shows and teaching classes at The Old Hairdresser's bar in Glasgow, growing the improv scene in Scotland.\nIn 2017, Bristol Improv Theatre (BIT) became the first permanent venue in the South West dedicated to improvisational theatre. The Bristol Improv Theatre, was founded in 2012 and had been hosting shows, workshops and festivals across various Bristol venues before finding a home at The Polish Ex-Servicemen's Club in Clifton, Bristol since 2013. Through crowdfunding, fundraising events and private investment, the BIT took over the building and reopened the venue as the Bristol Improv Theatre on the 3rd March 2017.\nGunter L\u00f6sel compared the existing improvisational theatre theories (including Moreno, Spolin, Johnstone, and Close), structured them and wrote a general theory of improvisational theatre.\nAlan Alda's book \"If I Understood You, Would I Have This Look on My Face?\" investigates the way in which improvisation improves communication in the sciences. The book is based on his work at Alan Alda Center for Communicating Science at Stony Brook University. The book has many examples of how improvisational theatre games can increase communication skills and develop empathy.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15043", "revid": "628775", "url": "https://en.wikipedia.org/wiki?curid=15043", "title": "International Space Station", "text": "Inhabited space station in low Earth orbit\nThe International Space Station (ISS) is a large space station that was assembled and is maintained in low Earth orbit by a collaboration of five space agencies and their contractors: NASA (United States), Roscosmos (Russia), ESA (Europe), JAXA (Japan), and CSA (Canada). As the largest space station ever constructed, it primarily serves as a platform for conducting scientific experiments in microgravity and studying the space environment.\nThe station is divided into two main sections: the Russian Orbital Segment (ROS), developed by Roscosmos, and the US Orbital Segment (USOS), built by NASA, ESA, JAXA, and CSA. A striking feature of the ISS is the Integrated Truss Structure, which connect the station's vast system of solar panels and radiators to its pressurized modules. These modules support diverse functions, including scientific research, crew habitation, storage, spacecraft control, and airlock operations. The ISS has eight docking and berthing ports for visiting spacecraft. The station orbits the Earth at an average altitude of and circles the Earth in roughly 93 minutes, completing 15.5 orbits per day.\nThe ISS programme combines two previously planned crewed Earth-orbiting stations: the United States' Space Station \"Freedom\" and the Soviet Union's\" Mir-2\". The first ISS module was launched in 1998, with major components delivered by Proton and Soyuz rockets and the Space Shuttle. Long-term occupancy began on 2 November 2000, with the arrival of the Expedition 1 crew. Since then, the ISS has remained continuously inhabited for , the longest continuous human presence in space. As of \u00a02025[ [update]], 290 individuals from 26 countries had visited the station.\nFuture plans for the ISS include the addition of at least one module, Axiom Space's Payload Power Thermal Module. The station is expected to remain operational until the end of 2030, after which it will be de-orbited using the US Deorbit Vehicle.\nPurpose.\nThe ISS was originally intended to be a laboratory, observatory, and factory while providing transportation, maintenance, and a low Earth orbit staging base for possible future missions to the Moon, Mars, and asteroids. However, not all of the uses envisioned in the initial memorandum of understanding between NASA and Roscosmos have been realised. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic, and educational purposes.\nScientific research.\nThe ISS provides a platform to conduct scientific research, with power, data, cooling, and crew available to support experiments. Small uncrewed spacecraft can also provide platforms for experiments, especially those involving zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers.\nThe ISS simplifies individual experiments by allowing groups of experiments to share the same launches and crew time. Research is conducted in a wide variety of fields, including astrobiology, astronomy, physical sciences, materials science, space weather, meteorology, and human research including space medicine and the life sciences. Scientists on Earth have timely access to the data and can suggest experimental modifications to the crew. If follow-on experiments are necessary, the routinely scheduled launches of resupply craft allows new hardware to be launched with relative ease. Crews fly expeditions of several months' duration, providing approximately 160 man-hours per week of labour with a crew of six. However, a considerable amount of crew time is taken up by station maintenance.\nPerhaps the most notable ISS experiment is the Alpha Magnetic Spectrometer (AMS), which is intended to detect dark matter and answer other fundamental questions about our universe. According to NASA, the AMS is as important as the Hubble Space Telescope. Currently docked on station, it could not have been easily accommodated on a free flying satellite platform because of its power and bandwidth needs. On 3 April 2013, scientists reported that hints of dark matter may have been detected by the AMS. According to the scientists, \"The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays\".\nThe space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, as well as small invertebrates called tardigrades can survive in this environment in an extremely dry state through desiccation.\nMedical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. These data will be used to determine whether high duration human spaceflight and space colonisation are feasible. In 2006, data on bone loss and muscular atrophy suggested that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars.\nMedical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.\nIn August 2020, scientists reported that bacteria from Earth, particularly \"Deinococcus radiodurans\" bacteria, which is highly resistant to environmental hazards, were found to survive for three years in outer space, based on studies conducted on the International Space Station. These findings supported the notion of panspermia, the hypothesis that life exists throughout the Universe, distributed in various ways, including space dust, meteoroids, asteroids, comets, planetoids or contaminated spacecraft.\nRemote sensing of the Earth, astronomy, and deep space research on the ISS have significantly increased during the 2010s after the completion of the US Orbital Segment in 2011. Throughout the more than 20 years of the ISS program, researchers aboard the ISS and on the ground have examined aerosols, ozone, lightning, and oxides in Earth's atmosphere, as well as the Sun, cosmic rays, cosmic dust, antimatter, and dark matter in the universe. Examples of Earth-viewing remote sensing experiments that have flown on the ISS are the Orbiting Carbon Observatory 3, ISS-RapidScat, ECOSTRESS, the Global Ecosystem Dynamics Investigation, and the Cloud Aerosol Transport System. ISS-based astronomy telescopes and experiments include SOLAR, the Neutron Star Interior Composition Explorer, the Calorimetric Electron Telescope, the Monitor of All-sky X-ray Image (MAXI), and the Alpha Magnetic Spectrometer.\nFreefall.\nResearchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of the data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues and the unusual protein crystals that can be formed in space.\nInvestigating the physics of fluids in microgravity will provide better models of the behaviour of fluids. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. Examining reactions that are slowed by low gravity and low temperatures will improve our understanding of superconductivity.\nThe study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on Earth. Other areas of interest include the effect of low gravity on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve knowledge about energy production and lead to economic and environmental benefits.\nExploration.\nThe ISS provides a location in the relative safety of low Earth orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance, and repair and replacement activities on-orbit. This will help develop essential skills in operating spacecraft farther from Earth, reduce mission risks, and advance the capabilities of interplanetary spacecraft. Referring to the MARS-500 experiment, a crew isolation experiment conducted on Earth, ESA states, \"Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations\". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a \"shorter version\" of MARS-500 may be carried out on the ISS.\nIn 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, \"When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars.\" A crewed mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India, and South Korea be invited to join the ISS partnership. NASA chief Charles Bolden stated in February 2011, \"Any mission to Mars is likely to be a global effort.\" Currently, US federal legislation prevents NASA co-operation with China on space projects without approval by the FBI and Congress.\nEducation and cultural outreach.\nThe ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, and email. ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. In one lesson, students can navigate a 3D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time.\nThe Japanese Aerospace Exploration Agency (JAXA) aims to inspire children to \"pursue craftsmanship\" and to heighten their \"awareness of the importance of life and their responsibilities in society\". Through a series of education guides, students develop a deeper understanding of the past and near-term future of crewed space flight, as well as that of Earth and life. In the JAXA \"Seeds in Space\" experiments, the mutation effects of spaceflight on plant seeds aboard the ISS are explored by growing sunflower seeds that have flown on the ISS for about nine months. In the first phase of \"Kib\u014d\" utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields.\nCultural activities are another major objective of the ISS programme. Tetsuo Tanaka, the director of JAXA's Space Environment and Utilization Center, has said: \"There is something about space that touches even people who are not interested in science.\"\nAmateur Radio on the ISS (ARISS) is a volunteer programme that encourages students worldwide to pursue careers in science, technology, engineering, and mathematics, through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from nine countries including several in Europe, as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the space station.\n\"First Orbit\" is a 2011 feature-length documentary film about Vostok 1, the first crewed space flight around the Earth. By matching the orbit of the ISS to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli is credited as the director of photography for this documentary film, as he recorded the majority of the footage himself during Expedition 26/27. The film was streamed in a global YouTube premiere in 2011 under a free licence through the website \"firstorbit.org\".\nIn May 2013, commander Chris Hadfield shot a music video of David Bowie's \"Space Oddity\" on board the station, which was released on YouTube. It was the first music video filmed in space.\nIn November 2017, while participating in Expedition 52/53 on the ISS, Paolo Nespoli made two recordings of his spoken voice (one in English and the other in his native Italian), for use on Wikipedia articles. These were the first content made in space specifically for Wikipedia.\nIn November 2021, a virtual reality exhibit called The Infinite featuring life aboard the ISS was announced.\nInternational co-operation.\nInvolving five space programs and fifteen countries, the International Space Station is the most politically and legally complex space exploration programme in history. The 1998 Space Station Intergovernmental Agreement sets forth the primary framework for international cooperation among the parties. A series of subsequent agreements govern other aspects of the station, ranging from jurisdictional issues to a code of conduct among visiting astronauts.\nBrazil was also invited to participate in the programme, the only developing country to receive such an invitation. Under the agreement framework, Brazil was to provide six pieces of hardware, and in exchange, would receive ISS utilization rights. However, Brazil was unable to deliver any of the elements due to a lack of funding and political priority within the country. Brazil officially dropped out of the ISS programme in 2007.\nFollowing the 2022 Russian invasion of Ukraine, continued cooperation between Russia and other countries on the International Space Station has been put into question. Roscosmos Director General Dmitry Rogozin insinuated that Russian withdrawal could cause the International Space Station to de-orbit due to lack of reboost capabilities, writing in a series of tweets, \"If you block cooperation with us, who will save the ISS from an unguided de-orbit to impact on the territory of the US or Europe? There's also the chance of impact of the 500-ton construction in India or China. Do you want to threaten them with such a prospect? The ISS doesn't fly over Russia, so all the risk is yours. Are you ready for it?\" (This latter claim is untrue: the ISS flies over all parts of the Earth between 51.6 degrees latitude north and south, approximately the latitude of Saratov.) Rogozin later tweeted that normal relations between ISS partners could only be restored once sanctions have been lifted, and indicated that Roscosmos would submit proposals to the Russian government on ending cooperation. NASA stated that, if necessary, US corporation Northrop Grumman has offered a reboost capability that would keep the ISS in orbit.\nOn 26 July 2022, Yury Borisov, Rogozin's successor as head of Roscosmos, submitted to Russian President Putin plans for withdrawal from the programme after 2024. However, Robyn Gatens, the NASA official in charge of the space station, responded that NASA had not received any formal notices from Roscosmos concerning withdrawal plans.\nConstruction.\nManufacturing.\nThe International Space Station is a product of global collaboration, with its components manufactured across the world.\nThe modules of the Russian Orbital Segment, including \"Zarya\" and \"Zvezda\", were produced at the Khrunichev State Research and Production Space Center in Moscow. \"Zvezda\" was initially manufactured in 1985 as a component for the \"Mir-2\" space station, which was never launched.\nMuch of the US Orbital Segment, including the \"Destiny\" and \"Unity\" modules, the Integrated Truss Structure, and solar arrays, were built at NASA's Marshall Space Flight Center in Huntsville, Alabama and Michoud Assembly Facility in New Orleans. These components underwent final assembly and processing for launch at the Operations and Checkout Building and the Space Station Processing Facility (SSPF) at the Kennedy Space Center in Florida.\nThe US Orbital Segment also hosts the \"Columbus\" module contributed by the European Space Agency and built in Germany, the \"Kib\u014d\" module contributed by Japan and built at the Tsukuba Space Center and the Institute of Space and Astronautical Science, along with the Canadarm2 and Dextre, a joint Canadian-U.S. endeavor. All of these components were shipped to the SSPF for launch processing.\nAssembly.\nThe assembly of the International Space Station, a major endeavour in space architecture, began in November 1998.\nModules in the Russian segment launched and docked autonomously, with the exception of \"Rassvet\". Other modules and components were delivered by the Space Shuttle, which then had to be installed by astronauts either remotely using robotic arms or during spacewalks, more formally known as extra-vehicular activities (EVAs). By 5\u00a0June 2011 astronauts had made over 159 EVAs to add components to the station, totaling more than 1,000 hours in space.\nThe beginning of the core of the ISS's tenure in orbit was the launch of the Russian-built \"Zarya\" module atop a Proton rocket on 20\u00a0November 1998. \"Zarya\" provided propulsion, attitude control, communications, and electrical power. Two weeks later on 4 December 1998, the American-made \"Unity\" was ferried aboard Space Shuttle \"Endeavour\" on STS-88 and joined with \"Zarya\". \"Unity\" provided the connection between the Russian and US segments of the station and would provide ports to connect future modules and visiting spacecraft.\nWhile the connection of two modules built on different continents, by nations that were once bitter rivals was a significant milestone, these two initial modules lacked life-support systems and the ISS remained unmanned for the next two years. At the time, the Russian station \"Mir\" was still inhabited.\nThe turning point arrived in July 2000 with the launch of the \"Zvezda\" module. Equipped with living quarters and life-support systems, \"Zvezda\" enabled continuous human presence aboard the station. The first crew, Expedition 1, arrived that November aboard Soyuz TM-31.\nThe ISS grew steadily over the following years, with modules delivered by both Russian rockets and the Space Shuttle.\nExpedition 1 arrived midway between the Space Shuttle flights of missions STS-92 and STS-97. These two flights each added segments of the station's Integrated Truss Structure, which provided the station with Ku band communications, additional attitude control needed for the additional mass of the USOS, and additional solar arrays. Over the next two years, the station continued to expand. A Soyuz-U rocket delivered the \"Pirs\" docking compartment. The Space Shuttles \"Discovery\", \"Atlantis\", and \"Endeavour\" delivered the American \"Destiny\" laboratory and \"Quest\" airlock, in addition to the station's main robot arm, the Canadarm2, and several more segments of the Integrated Truss Structure.\nTragedy struck in 2003 with the loss of the Space Shuttle \"Columbia\", which grounded the rest of the Shuttle fleet, halting construction of the ISS.Assembly resumed in 2006 with the arrival of STS-115 with \"Atlantis\", which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116, STS-117, and STS-118. As a result of the major expansion of the station's power-generating capabilities, more modules could be accommodated, and the US \"Harmony\" module and \"Columbus\" European laboratory were added. These were soon followed by the first two components of the Japanese \"Kib\u014d\" laboratory. In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of \"Kib\u014d\" was delivered in July 2009 on STS-127, followed by the Russian \"Poisk\" module. The US \"Tranquility\" module was delivered in February 2010 during STS-130, alongside the \"Cupola\", followed by the penultimate Russian module, \"Rassvet\", in May 2010. \"Rassvet\" was delivered by Space Shuttle \"Atlantis\" on STS-132 in exchange for the Russian Proton delivery of the US-funded \"Zarya\" module in 1998. The last pressurised module of the USOS, \"Leonardo\", was brought to the station in February 2011 on the final flight of \"Discovery\", STS-133.\nRussia's new primary research module \"Nauka\" docked in July 2021, along with the European Robotic Arm which can relocate itself to different parts of the Russian modules of the station. Russia's latest addition, the \"Prichal\" module, docked in November 2021.\nAs of June 2025, nasa.gov states that there are 43 different modules and elements installed on the ISS.\nStructure.\nThe ISS functions as a modular space station, enabling the addition or removal of modules from its structure for increased adaptability.\nBelow is a diagram of major station components. The \"Unity\" node joins directly to the \"Destiny\" laboratory; for clarity, they are shown apart. Similar cases are also seen in other parts of the structure.\nKey to box background colors:\nPressurised modules.\n\"Zarya\".\n\"Zarya\" (), also known as the \"Functional Cargo Block\" (), was the inaugural component of the ISS. Launched in 1998, it initially served as the ISS's power source, storage, propulsion, and guidance system. As the station has grown, \"Zarya\"'s role has transitioned primarily to storage, both internally and in its external fuel tanks.\nA descendant of the TKS spacecraft used in the \"Salyut\" programme, \"Zarya\" was built in Russia but is owned by the United States. Its name symbolizes the beginning of a new era of international space cooperation.\n\"Unity\".\n\"Unity\", also known as \"Node 1\", is the inaugural U.S.-built component of the ISS. Serving as the connection between the Russian and U.S. segments, this cylindrical module features six Common Berthing Mechanism locations (forward, aft, port, starboard, zenith, and nadir) for attaching additional modules. Measuring in diameter and in length, \"Unity\" was constructed of steel by Boeing for NASA at the Marshall Space Flight Center in Huntsville, Alabama. It was the first of three connecting nodes \u2013 \"Unity\", \"Harmony\", and \"Tranquility\" \u2013 that forms the structural backbone of the U.S. segment of the ISS.\n\"Zvezda\".\n\"Zvezda\" () launched in July 2000, is the core of the Russian Orbital Segment of the ISS. Initially providing essential living quarters and life-support systems, it enabled the first continuous human presence aboard the station. While additional modules have expanded the ISS's capabilities, Zvezda remains the command and control center for the Russian segment and it is where crews gather during emergencies.\nA descendant of the Salyut programme's DOS spacecraft, Zvezda was built by RKK Energia and launched atop a Proton rocket.\n\"Destiny\".\nThe \"Destiny\" laboratory is the primary research facility for U.S. experiments on the ISS. NASA's first permanent orbital research station since Skylab, the module was built by Boeing and launched aboard Space Shuttle \"Atlantis\" during STS-98. Attached to \"Unity\" over a period of five days in February 2001, \"Destiny\" has been a hub for scientific research ever since.\nWithin \"Destiny\", astronauts conduct experiments in fields such as medicine, engineering, biotechnology, physics, materials science, and Earth science. Researchers worldwide benefit from these studies. The module also houses life-support systems, including the Oxygen Generating System.\n\"Quest Joint Airlock\".\nThe \"Quest Joint Airlock\" enables extravehicular activities (EVAs) using either the U.S. Extravehicular Mobility Unit (EMU) or the Russian Orlan space suit.\nBefore its installation, conducting EVAs from the ISS was challenging due to a variety of system and design differences. Only the Orlan suit could be used from the Transfer Chamber on the \"Zvezda\" module (which was not a purpose-built airlock) and the EMU could only be used from the airlock on a visiting Space Shuttle, which could not accommodate the Orlan.\nLaunched aboard Space Shuttle \"Atlantis\" during STS-104 in July 2001 and attached to the Unity module, Quest is a , structure built by Boeing. It houses the crew airlock for astronaut egress, an equipment airlock for suit storage, and has facilities to accommodate astronauts during their overnight pre-breathe procedures to prevent decompression sickness.\nThe crew airlock, derived from the Space Shuttle, features essential equipment like lighting, handrails, and an Umbilical Interface Assembly (UIA) that provides life-support and communication systems for up to two spacesuits simultaneously. These can be either two EMUs, two Orlan suits, or one of each design.\n\"Poisk\".\n\"Poisk\" (), also known as the \"Mini-Research Module 2\" (), serves as both a secondary airlock on the Russian segment of the ISS and supports docking for Soyuz and Progress spacecraft, facilitates propellant transfers from the latter. Launched on 10\u00a0November 2009 attached to a modified Progress spacecraft, called Progress M-MIM2.\n\"Poisk\" provides facilities to maintain Orlan spacesuits and is equipped with two inward-opening hatches, a design change from \"Mir\", which encountered a dangerous situation caused by an outward-opening hatch that opened too quickly because of a small amount of air pressure remaining in the airlock. Since the departure of \"Pirs\" in 2021, it's become the sole airlock on the Russian segment.\n\"Harmony\".\n\"Harmony\", or \"Node 2\", is the central connecting hub of the US segment of the ISS, linking the U.S., European, and Japanese laboratory modules. It's also been called the \"utility hub\" of the ISS as it provides essential power, data, and life-support systems. The module also houses sleeping quarters for four crew members.\nLaunched on 23\u00a0October 2007 aboard Space Shuttle \"Discovery\" on STS-120, Harmony was initially attached to the Unity before being relocated to its permanent position at the front of the Destiny laboratory on 14\u00a0November 2007. This expansion added significant living space to the ISS, marking a key milestone in the construction of the U.S. segment.\n\"Tranquility\".\n\"Tranquility\", also known as \"Node 3\", is a module of the ISS. It contains environmental control systems, life-support systems, a toilet, exercise equipment, and an observation cupola.\nThe European Space Agency and the Italian Space Agency had \"Tranquility\" manufactured by Thales Alenia Space. A ceremony on 20\u00a0November 2009 transferred ownership of the module to NASA. On 8\u00a0February 2010, NASA launched the module on the Space Shuttle's STS-130 mission.\n\"Columbus\".\n\"Columbus\" is a science laboratory that is part of the ISS and is the largest single contribution to the station made by the European Space Agency.\nLike the \"Harmony\" and \"Tranquility\" modules, the \"Columbus\" laboratory was constructed in Turin, Italy by Thales Alenia Space. The functional equipment and software of the lab was designed by EADS in Bremen, Germany. It was also integrated in Bremen before being flown to the Kennedy Space Center in Florida in an Airbus Beluga jet. It was launched aboard Space Shuttle \"Atlantis\" on 7\u00a0February 2008, on flight STS-122. It is designed for ten years of operation. The module is controlled by the Columbus Control Centre, located at the German Space Operations Center, part of the German Aerospace Center in Oberpfaffenhofen near Munich, Germany.\nThe European Space Agency has spent \u20ac1.4 billion (about US$1.6 billion) on building \"Columbus\", including the experiments it carries and the ground control infrastructure necessary to operate them.\n\"Kib\u014d\".\n, also known as the \"Japanese Experiment Module\", is Japan's research facility on the ISS. It is the largest single module on the ISS, consisting of a pressurized lab, an exposed facility for conducting experiments in the space environment, two storage compartments, and a robotic arm. Attached to the \"Harmony\" module, \"Kib\u014d\" was assembled in space over three Space Shuttle missions: STS-123, STS-124 and STS-127.\n\"Cupola\".\nThe \"Cupola\" is an ESA-built observatory module of the ISS. Its name derives from the Italian word \"\", which means \"dome\". Its seven windows are used to conduct experiments, dockings and observations of Earth. It was launched aboard Space Shuttle mission STS-130 on 8 February 2010 and attached to the \"Tranquility\" (Node 3) module. With the \"Cupola\" attached, ISS assembly reached 85 per cent completion. The \"Cupola\"'s central window has a diameter of .\n\"Rassvet\".\n\"Rassvet\" (), also known as the \"Mini-Research Module 1\" () and formerly known as the \"Docking Cargo Module\" is primarily used for cargo storage and as a docking port for visiting spacecraft on the Russian segment of the ISS. \"Rassvet\" replaced the cancelled Docking and Storage Module and used a design largely based on the \"Mir Docking Module\" built in 1995.\nRassvet was delivered in on 14\u00a0May 2010 Space Shuttle \"Atlantis\" on STS-132 in exchange for the Russian Proton delivery of the US-funded \"Zarya\" module in 1998. \"Rassvet\" was attached to \"Zarya\" shortly thereafter.\n\"Leonardo\".\nThe \"Leonardo\" Permanent Multipurpose Module (PMM) was flown into space aboard the Space Shuttle \"Discovery\" on STS-133 on 24 February 2011 and installed on 1 March. \"Leonardo\" is primarily used for storage of spares, supplies and waste on the ISS, which was until then stored in many different places within the space station. It is also the personal hygiene area for the astronauts who live in the US Orbital Segment. The \"Leonardo\" PMM was a Multi-Purpose Logistics Module (MPLM) before 2011, but was modified into its current configuration. It was formerly one of two MPLM used for bringing cargo to and from the ISS with the Space Shuttle. The module was named for Italian polymath Leonardo da Vinci.\nBigelow Expandable Activity Module.\nThe Bigelow Expandable Activity Module (BEAM) is an experimental expandable space station module developed by Bigelow Aerospace, under contract to NASA, for testing as a temporary module on the International Space Station (ISS) from 2016 to at least 2020. It arrived at the ISS on 10\u00a0April 2016, was berthed to the station on 16\u00a0April at Tranquility Node 3, and was expanded and pressurized on 28\u00a0May 2016. In December 2021, Bigelow Aerospace conveyed ownership of the module to NASA, as a result of Bigelow's cessation of activity.\nInternational Docking Adapters.\nThe International Docking Adapter (IDA) is a spacecraft docking system adapter developed to convert APAS-95 to the NASA Docking System (NDS). An IDA is placed on each of the ISS's two open Pressurized Mating Adapters (PMAs), both of which are connected to the \"Harmony\" module.\nTwo International Docking Adapters are currently installed aboard the Station. Originally, IDA-1 was planned to be installed on PMA-2, located at \"Harmony\"'s forward port, and IDA-2 would be installed on PMA-3 at \"Harmony\"'s zenith. After IDA 1 was destroyed in a launch incident, IDA-2 was installed on PMA-2 on 19 August 2016, while IDA-3 was later installed on PMA-3 on 21 August 2019.\nBishop Airlock Module.\nThe NanoRacks Bishop Airlock Module is a commercially funded airlock module launched to the ISS on SpaceX CRS-21 on 6 December 2020. The module was built by NanoRacks, Thales Alenia Space, and Boeing. It will be used to deploy CubeSats, small satellites, and other external payloads for NASA, CASIS, and other commercial and governmental customers.\n\"Nauka\".\n\"Nauka\" (), also known as the \"Multipurpose Laboratory Module, Upgrade\" (), is a Roscosmos-funded component of the ISS that was launched on 21\u00a0July 2021, 14:58 UTC. In the original ISS plans, \"Nauka\" was to use the location of the Docking and Stowage Module (DSM), but the DSM was later replaced by the \"Rassvet\" module and moved to \"Zarya\"'s nadir port. \"Nauka\" was successfully docked to \"Zvezda\"'s nadir port on 29\u00a0July 2021, 13:29 UTC, replacing the \"Pirs\" module.\nIt had a temporary docking adapter on its nadir port for crewed and uncrewed missions until Prichal arrival, where just before its arrival it was removed by a departing Progress spacecraft.\n\"Prichal\".\n\"Prichal\" () is a spherical module that serves as a docking hub for the Russian segment of the ISS. Launched in November 2021, Prichal provides additional docking ports for Soyuz and Progress spacecraft, as well as potential future modules. \"Prichal\" features six docking ports: forward, aft, port, starboard, zenith, and nadir. One of these ports, equipped with an active hybrid docking system, enabled it to dock with the Nauka module. The remaining five ports are passive hybrids, allowing for docking of Soyuz, Progress, and heavier modules, as well as future spacecraft with modified docking systems. As of 2024, the forward, aft, port and starboard docking ports remain covered. \"Prichal\" was initially intended to be an element of the now canceled Orbital Piloted Assembly and Experiment Complex.\nUnpressurised elements.\nThe ISS has a large number of external components that do not require pressurisation. The largest of these is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. The ITS consists of ten separate segments forming a structure long.\nThe station was intended to have several smaller external components, such as six robotic arms, three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). While these platforms allow experiments (including MISSE, the STP-H3 and the Robotic Refueling Mission) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, their primary function is to store spare Orbital Replacement Units (ORUs). ORUs are parts that can be replaced when they fail or pass their design life, including pumps, storage tanks, antennas, and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. Several shuttle missions were dedicated to the delivery of ORUs, including STS-129, STS-133 and STS-134. As of \u00a02011[ [update]], only one other mode of transportation of ORUs had been used\u00a0\u2013 the Japanese cargo vessel HTV-2\u00a0\u2013 which delivered an FHRC and CTC-2 via its Exposed Pallet (EP).\nThere are also smaller exposure facilities mounted directly to laboratory modules; the \"Kib\u014d\" Exposed Facility serves as an external \"porch\" for the \"Kib\u014d\" complex, and a facility on the European \"Columbus\" laboratory provides power and data connections for experiments such as the European Technology Exposure Facility and the Atomic Clock Ensemble in Space. A remote sensing instrument, SAGE III-ISS, was delivered to the station in February 2017 aboard CRS-10, and the NICER experiment was delivered aboard CRS-11 in June 2017. The largest scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter.\nThe commercial \"Bartolomeo\" External Payload Hosting Platform, manufactured by Airbus, was launched on 6 March 2020 aboard CRS-20 and attached to the European \"Columbus\" module. It will provide an additional 12 external payload slots, supplementing the eight on the ExPRESS Logistics Carriers, ten on \"Kib\u014d\", and four on \"Columbus\". The system is designed to be robotically serviced and will require no astronaut intervention. It is named after Christopher Columbus's younger brother.\nMLM outfittings.\nIn May 2010, equipment for \"Nauka\" was launched on STS-132 (as part of an agreement with NASA) and delivered by Space Shuttle \"Atlantis\". Weighing 1.4 metric tons, the equipment was attached to the outside of \"Rassvet\" (MRM-1). It included a spare elbow joint for the European Robotic Arm (ERA) (which was launched with \"Nauka\") and an ERA-portable workpost used during EVAs, as well as RTOd add-on heat radiator and internal hardware alongside the pressurized experiment airlock.\nThe RTOd radiator adds additional cooling capability to \"Nauka\", which enables the module to host more scientific experiments.\nThe ERA was used to remove the RTOd radiator from \"Rassvet\" and transferred over to \"Nauka\" during VKD-56 spacewalk. Later it was activated and fully deployed on VKD-58 spacewalk. This process took several months. A portable work platform was also transferred over in August 2023 during VKD-60 spacewalk, which can attach to the end of the ERA to allow cosmonauts to \"ride\" on the end of the arm during spacewalks. However, even after several months of outfitting EVAs and RTOd heat radiator installation, six months later, the RTOd radiator malfunctioned before active use of Nauka (the purpose of RTOd installation is to radiate heat from Nauka experiments). The malfunction, a leak, rendered the RTOd radiator unusable for Nauka. This is the third ISS radiator leak after Soyuz MS-22 and Progress MS-21 radiator leaks. If a spare RTOd is not available, Nauka experiments will have to rely on Nauka's main launch radiator and the module could never be used to its full capacity.\nAnother MLM outfitting is a 4 segment external payload interface called means of attachment of large payloads (Sredstva Krepleniya Krupnogabaritnykh Obyektov, SKKO). Delivered in two parts to Nauka by Progress MS-18 (LCCS part) and Progress MS-21 (SCCCS part) as part of the module activation outfitting process. It was taken outside and installed on the ERA aft facing base point on Nauka during the VKD-55 spacewalk.\nRobotic arms and cargo cranes.\nThe Integrated Truss Structure (ITS) serves as a base for the station's primary remote manipulator system, the Mobile Servicing System (MSS), which is composed of three main components:\nA grapple fixture was added to \"Zarya\" on STS-134 to enable Canadarm2 to inchworm itself onto the ROS. Also installed during STS-134 was the Orbiter Boom Sensor System (OBSS), which had been used to inspect heat shield tiles on Space Shuttle missions and which can be used on the station to increase the reach of the MSS. Staff on Earth or the ISS can operate the MSS components using remote control, performing work outside the station without the need for space walks.\nJapan's Remote Manipulator System, which services the \"Kib\u014d\" Exposed Facility, was launched on STS-124 and is attached to the \"Kib\u014d\" Pressurised Module. The arm is similar to the Space Shuttle arm as it is permanently attached at one end and has a latching end effector for standard grapple fixtures at the other.\nThe European Robotic Arm, which will service the ROS, was launched alongside the \"Nauka\" module. The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically and may be discarded the same way. Crew use the two \"Strela\" () cargo cranes during EVAs for moving crew and equipment around the ROS. Each Strela crane has a mass of .\nFormer module.\n\"Pirs\".\nPirs (Russian: \u041f\u0438\u0440\u0441, lit.\u2009'Pier') was launched on 14\u00a0September 2001, as ISS Assembly Mission 4R, on a Russian Soyuz-U rocket, using a modified Progress spacecraft, Progress M-SO1, as an upper stage. Pirs was undocked by Progress MS-16 on 26\u00a0July 2021, 10:56 UTC, and deorbited on the same day at 14:51 UTC to make room for the \"Nauka\" module to be attached to the space station. Prior to its departure, Pirs served as the primary Russian airlock on the station, being used to store and refurbish the Russian Orlan spacesuits.\nPlanned components.\nAxiom segment.\nIn January 2020, NASA awarded Axiom Space a contract to build a commercial module for the ISS. The contract is under the NextSTEP2 program. NASA negotiated with Axiom on a firm fixed-price contract basis to build and deliver the module, which will attach to the forward port of the space station's \"Harmony (Node 2)\" module. Although NASA only commissioned one module, Axiom planned to build an entire segment consisting of five modules, including a node module, an orbital research and manufacturing facility, a crew habitat, and a \"large-windowed Earth observatory\". The Axiom segment was expected to greatly increase the capabilities and value of the space station, allowing for larger crews and private spaceflight by other organisations. Axiom planned to convert the segment into a stand-alone space station once the ISS is decommissioned, with the intention that this would act as a successor to the ISS. Canadarm2 is planned to continue its operations on Axiom Station after the retirement of ISS in 2030. In December 2024, Axiom Space revised their station assembly plans to require only one module to dock with the ISS before assembling Axiom Station in an independent orbit.\nAs of December 2024[ [update]], Axiom Space expects to launch one module, the Payload Power Thermal Module (PPTM), to the ISS no earlier than 2027. PPTM is expected to remain at the ISS until the launch of Axiom's Habitat One (Hab-1) module about one year later, after which it will detach from the ISS to join with Hab-1.\nUS Deorbit Vehicle.\nThe US Deorbit Vehicle (USDV) is a NASA-provided spacecraft intended to perform a controlled de-orbit and demise of the station after the end of its operational life in 2030. In June 2024, NASA awarded SpaceX a contract to build the Deorbit Vehicle. NASA plans to de-orbit ISS as soon as they have the \"minimum capability\" in orbit: \"the USDV and at least one commercial station.\"\nCancelled components.\nSeveral modules developed or planned for the station were cancelled over the course of the ISS programme. Reasons include budgetary constraints, the modules becoming unnecessary, and station redesigns after the 2003 \"Columbia\" disaster. The US Centrifuge Accommodations Module would have hosted science experiments in varying levels of artificial gravity. The US Habitation Module would have served as the station's living quarters. Instead, the living quarters are now spread throughout the station. The US Interim Control Module and ISS Propulsion Module would have replaced the functions of \"Zvezda\" in case of a launch failure. Two Russian Research Modules were planned for scientific research. They would have docked to a Russian Universal Docking Module. The Russian Science Power Platform would have supplied power to the Russian Orbital Segment independent of the ITS solar arrays.\nScience Power Modules 1 and 2 (Repurposed Components).\nScience Power Module 1 (SPM-1, also known as NEM-1) and Science Power Module 2 (SPM-2, also known as NEM-2) are modules that were originally planned to arrive at the ISS no earlier than 2024, and dock to the \"Prichal\" module, which is docked to the \"Nauka\" module. In April 2021, Roscosmos announced that NEM-1 would be repurposed to function as the core module of the proposed Russian Orbital Service Station (ROSS), launching no earlier than 2027 and docking to the free-flying \"Nauka\" module. NEM-2 may be converted into another core \"base\" module, which would be launched in 2028.\nXbase.\nDesigned by Bigelow Aerospace. In August 2016, Bigelow negotiated an agreement with NASA to develop a full-size ground prototype Deep Space Habitation based on the B330 under the second phase of Next Space Technologies for Exploration Partnerships. The module was called the Expandable Bigelow Advanced Station Enhancement (XBASE), as Bigelow hoped to test the module by attaching it to the International Space Station. However, in March 2020, Bigelow laid off all 88 of its employees, and as of \u00a02024[ [update]] the company remains dormant and is considered defunct, making it appear unlikely that the XBASE module will ever be launched.\nNautilus-X Centrifuge Demonstration.\nA proposal was put forward in 2011 for a first in-space demonstration of a sufficiently scaled centrifuge for artificial partial-g gravity effects. It was designed to become a sleep module for the ISS crew. The project was cancelled in favour of other projects due to budget constraints.\nOnboard systems.\nLife support.\nThe critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life-support systems are contained in the \"Zvezda\" service module. Some of these systems are supplemented by equipment in the USOS. The \"Nauka\" laboratory has a complete set of life-support systems.\nAtmospheric control systems.\nThe atmosphere on board the ISS is similar to that of Earth. Normal air pressure on the ISS is ; the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft.\nThe \"Elektron\" system aboard \"Zvezda\" and a similar system in \"Destiny\" generate oxygen aboard the station. The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. Carbon dioxide is removed from the air by the Vozdukh system in \"Zvezda\". Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters.\nPart of the ROS atmosphere control system is the oxygen supply. Triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The primary supply of oxygen is the Elektron unit which produces and by electrolysis of water and vents overboard. The system uses approximately one litre of water per crew member per day. This water is either brought from Earth or recycled from other systems. \"Mir\" was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning oxygen-producing Vika cartridges (see also ISS ECLSS). Each 'candle' takes 5\u201320 minutes to decompose at , producing of . This unit is manually operated.\nThe US Orbital Segment (USOS) has redundant supplies of oxygen, from a pressurised storage tank on the \"Quest\" airlock module delivered in 2001, supplemented ten years later by ESA-built Advanced Closed-Loop System (ACLS) in the \"Tranquility\" module (Node 3), which produces by electrolysis. Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane.\nPower and thermal control.\nDouble-sided solar arrays provide electrical power to the ISS. These bifacial cells collect direct sunlight on one side and light reflected off from the Earth on the other, and are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth. \nThe Russian segment of the station, like most spacecraft, uses 28\u00a0V\u00a0low voltage DC from two rotating solar arrays mounted on \"Zvezda\". The USOS uses 130\u2013180\u00a0V\u00a0DC from the USOS\u00a0PV array. Power is stabilised and distributed at 160\u00a0V\u00a0DC and converted to the user-required 124\u00a0V\u00a0DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The two station segments share power with converters.\nThe USOS solar arrays are arranged as four wing pairs, for a total production of 75 to 90 kilowatts. These arrays normally track the Sun to maximise power generation. Each array is about in area and long. In the complete configuration, the solar arrays track the Sun by rotating the \"alpha gimbal\" once per orbit; the \"beta gimbal\" follows slower changes in the angle of the Sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude.\nThe station originally used rechargeable nickel\u2013hydrogen batteries () for continuous power during the 45 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the orbit. They had a 6.5-year lifetime (over 37,000 charge/discharge cycles) and were regularly replaced over the anticipated 20-year life of the station. Starting in 2016, the nickel\u2013hydrogen batteries were replaced by lithium-ion batteries, which are expected to last until the end of the ISS program.\nThe station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units create current paths between the station and the ambient space plasma.\nThe station's systems and experiments consume a large amount of electrical power, almost all of which is converted to heat. To keep the internal temperature within workable limits, a passive thermal control system (PTCS) is made of external surface materials, insulation such as MLI, and heat pipes. If the PTCS cannot keep up with the heat load, an External Active Thermal Control System (EATCS) maintains the temperature. The EATCS consists of an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop. From the heat exchangers, ammonia is pumped into external radiators that emit heat as infrared radiation, then the ammonia is cycled back to the station. The EATCS provides cooling for all the US pressurised modules, including \"Kib\u014d\" and \"Columbus\", as well as the main power distribution electronics of the S0, S1 and P1 trusses. It can reject up to 70\u00a0kW. This is much more than the 14\u00a0kW of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss.\nCommunications and computers.\nThe ISS relies on various radio communication systems to provide telemetry and scientific data links between the station and mission control centres. Radio links are also used during rendezvous and docking procedures and for audio and video communication between crew members, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes.\nThe Russian Orbital Segment primarily uses the \"Lira\" antenna mounted on \"Zvezda\" for direct ground communication. It also had the capability to utilize the \"Luch\" data relay satellite system, which was in a state of disrepair when the station was built, but was restored to operational status in 2011 and 2012 with the launch of Luch-5A and Luch-5B. Additionally, the Voskhod-M system provides internal telephone communications and VHF radio links to ground control.\nThe US Orbital Segment (USOS) makes use of two separate radio links: S band (audio, telemetry, commanding \u2013 located on the P1/S1 truss) and Ku band (audio, video and data \u2013 located on the Z1 truss) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit, allowing for almost continuous real-time communications with Christopher C. Kraft Jr. Mission Control Center (MCC-H) in Houston, Texas. Data channels for the Canadarm2, European \"Columbus\" laboratory and Japanese \"Kib\u014d\" modules were originally also routed via the S band and Ku band systems, with the European Data Relay System and a similar Japanese system intended to eventually complement the TDRSS in this role.\nUHF radio is used by astronauts and cosmonauts conducting EVAs and other spacecraft that dock to or undock from the station. Automated spacecraft are fitted with their own communications equipment; the ATV used a laser attached to the spacecraft and the Proximity Communications Equipment attached to \"Zvezda\" to accurately dock with the station.\nThe US Orbital Segment of the ISS is equipped with approximately 100 commercial off-the-shelf laptops running Windows or Linux. These devices are modified to use the station's 28V DC power system and with additional ventilation since heat generated by the devices can stagnate in the weightless environment. NASA prefers to keep a high commonality between laptops and spare parts are kept on the station so astronauts can repair laptops when needed.\nThe laptops are divided into two groups: the Portable Computer System (PCS) and Station Support Computers (SSC).\nPCS laptops run Linux and are used for connecting to the station's primary Command &amp; Control computer (C&amp;C MDM), which runs on Debian Linux, a switch made from Windows in 2013 for reliability and flexibility. The primary computer supervises the critical systems that keep the station in orbit and supporting life. Since the primary computer has no display or keyboards, astronauts use a PCS laptop to connect as remote terminals via a USB to 1553 adapter. The primary computer experienced failures in 2001, 2007, and 2017. The 2017 failure required a spacewalk to replace external components.\nSSC laptops are used for everything else on the station, including reviewing procedures, managing scientific experiments, communicating over e-mail or video chat, and for entertainment during downtime. SSC laptops connect to the station's wireless LAN via Wi-Fi, which connects to the ground via the Ku band. While originally this provided speeds of 10\u00a0Mbit/s download and 3\u00a0Mbit/s upload from the station, NASA upgraded the system in 2019 and increased the speeds to 600\u00a0Mbit/s. ISS crew members have access to the internet.\nOperations.\nExpeditions.\nEach permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo spacecraft and all activities. Expeditions 1 to 6 consisted of three-person crews. After the destruction of NASA's Space Shuttle \"Columbia\", Expeditions 7 to 12 were reduced to two-person \"caretaker\" crews who could maintain the station, because a larger crew could not be fully resupplied by the small Russian Progress cargo spacecraft. After the Shuttle fleet returned to flight, three person crews also returned to the ISS beginning with Expedition 13. As the Shuttle flights expanded the station, crew sizes also expanded, eventually reaching six around 2010. With the arrival of crew on larger US commercial spacecraft beginning in 2020, crew size has been increased to seven, the number for which ISS was originally designed.\nOleg Kononenko of Roscosmos holds the record for the longest time spent in space and at the ISS, accumulating nearly 1,111 days in space over the course of five long-duration missions on the ISS (Expedition 17, 30/31, 44/45, 57/58/59 and 69/70/71). He also served as commander three times (Expedition 31, 58/59 and 70/71).\nPeggy Whitson of NASA and Axiom Space has spent the most time in space of any American, accumulating over 675 days in space during her time on Expeditions 5, 16, and 50/51/52 and Axiom Missions 2 and 4.\nPrivate flights.\nTravellers who pay for their own passage into space are termed spaceflight participants by Roscosmos and NASA, and are sometimes referred to as \"space tourists\", a term they generally dislike. As of \u00a02023[ [update]], thirteen space tourists have visited the ISS; nine were transported to the ISS on Russian Soyuz spacecraft, and four were transported on American SpaceX Dragon 2 spacecraft. For one-tourist missions, when professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. Space tourism was halted in 2011 when the Space Shuttle was retired and the station's crew size was reduced to six, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increased after 2013, allowing five Soyuz flights (15 seats) with only two expeditions (12 seats) required. The remaining seats were to be sold for around US$40\u00a0million each to members of the public who could pass a medical exam. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito, the first person to pay for his own passage to the ISS. \nAnousheh Ansari became the first self-funded woman to fly to the ISS as well as the first Iranian in space. Officials reported that her education and experience made her much more than a tourist, and her performance in training had been \"excellent.\" She did Russian and European studies involving medicine and microbiology during her 10-day stay. The 2009 documentary \"Space Tourists\" follows her journey to the station, where she fulfilled \"an age-old dream of man: to leave our planet as a 'normal person' and travel into outer space.\"\nIn 2008, spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. This is the first non-terrestrial geocache in history. At the same time, the Immortality Drive, an electronic record of eight digitised human DNA sequences, was placed aboard the ISS.\nAfter a 12-year hiatus, the first two wholly space tourism-dedicated private spaceflights to the ISS were undertaken. Soyuz MS-20 launched in December 2021, carrying visiting Roscosmos cosmonaut Alexander Misurkin and two Japanese space tourists under the aegis of the private company Space Adventures; in April 2022, the company Axiom Space chartered a SpaceX Dragon 2 spacecraft and sent its own employee astronaut Michael Lopez-Alegria and three space tourists to the ISS for Axiom Mission 1, followed in May 2023 by one more tourist, John Shoffner, alongside employee astronaut Peggy Whitson and two Saudi astronauts for the Axiom Mission 2.\nNASA private astronaut missions.\nThe first all private astronaut mission (PAM) to the ISS was Axiom Mission 1, which launched in April, 2022. After evaluating previous flights to ISS which included private spaceflight participants, and the first three flights including only private astronauts, NASA updated requirements for entities providing PAMs. As of \u00a02025[ [update]] the most recent mission was Axiom Mission 4. NASA intends to offer up to two PAM opportunities per year.\nFleet operations.\nVarious crewed and uncrewed spacecraft have supported the station's operations. Flights to the ISS have included 93 Progress, 72 Soyuz, 50 SpaceX Dragon 37 Space Shuttle, 21 Cygnus, 9 HTV, 5 ATV, and 2 Boeing Starliner missions.\nThere are currently eight docking ports for visiting spacecraft, with four additional ports installed but not yet put into service:\nForward ports are at the front of the station in its usual orientation and direction of travel. Aft is the opposite, at the rear. Nadir points toward Earth, while zenith points away from it. Port is to the left and starboard to the right when one's feet are toward Earth and one is facing forward, in the direction of travel.\nCargo spacecraft that will perform an orbital re-boost of the station will typically dock at an aft, forward or nadir-facing port.\nCrewed.\nAs of 2025https://, a total of 290 individuals from 26 countries had visited the International Space Station (ISS), including both government-sponsored astronauts and privately funded spaceflight participants. The United States accounted for 170 of these visitors, followed by Russia with 64, Japan with 11, and Canada with 9. Italy had 6 visitors, while France and Germany had each sent 4. Saudi Arabia, Sweden, and the United Arab Emirates each had 2 individuals visit the ISS. One person had traveled to the ISS from each of the following countries: Belarus, Belgium, Brazil, Denmark, Hungary, India, Israel, Kazakhstan, Malaysia, the Netherlands, Poland, South Africa, South Korea, Spain, Turkey, and the United Kingdom.\nUncrewed.\nUncrewed spaceflights are primarily used to deliver cargo to the station including crew supplies, scientific investigations, spacewalk equipment, vehicle hardware, propellant, water, and gases. Cargo resupply missions have typically used Russian Progress spacecraft, the now-retired European ATV, the Japanese HTV, and American Dragon and Cygnus spacecraft. Additionally, several Russian modules have been launched on uncrewed rockets and autonomously docked with the station.\nCurrently docked/berthed.\n\"All dates are UTC. Departure dates are the earliest possible (NET) and may change.\"\nScheduled missions.\n\"All dates are UTC. Launch dates are the earliest possible (NET) and may change.\"\nDocking and berthing of spacecraft.\nRussian spacecraft can autonomously rendezvous and dock with the station without human intervention. Once within about the spacecraft activates its Kurs docking navigation system, exchanging radio signals with the station's beacon to guide orbital manoeuvres. As it closes in, more accurate transceivers align the craft with the docking port and control the final approach. The crew supervises the procedure and can intervene using the TORU (Tele-robotically Operated Rendezvous Unit) system if required. Automated docking has been used by the Soviet programme since 1967, with Kurs introduced on \"Mir\" in 1986 and refined since. Though costly to develop, its reliability and standardised components have delivered significant long-term savings.\nThe American SpaceX Dragon 2 cargo and crewed spacecraft can autonomously rendezvous and dock with the station without human intervention. However, on crewed Dragon missions, the astronauts have the capability to intervene and fly the vehicle manually.\nOther automated cargo spacecraft typically use a semi-automated process when arriving and departing from the station. These spacecraft are instructed to approach and park near the station. Once the crew on board the station is ready, the spacecraft is commanded to come close to the station, so that it can be grappled by an astronaut using the Mobile Servicing System robotic arm. The final mating of the spacecraft to the station is achieved using the robotic arm (a process known as berthing). Spacecraft using this semi-automated process include the American Cygnus and the Japanese HTV-X. The now-retired American SpaceX Dragon 1, European ATV and Japanese HTV also used this process.\nLaunch and docking windows.\nPrior to a spacecraft's docking to the ISS, navigation and attitude control (GNC) is handed over to the ground control of the spacecraft's country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming spacecraft, so residue from its thrusters does not damage the cells. Before its retirement, Shuttle launches were often given priority over Soyuz, with occasional priority given to Soyuz arrivals carrying crew and time-critical cargoes, such as biological experiment materials.\nRepairs.\nOrbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Most are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms (ESPs) which also hold science experiments. Both kinds of pallets provide electricity for many parts that could be damaged by the cold of space and require heating. The larger logistics carriers also have local area network (LAN) connections for telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload.\nUnexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons. Serious problems include an air leak from the USOS in 2004, the venting of fumes from an \"Elektron\" oxygen generator in 2006, and the failure of the computers in the ROS in 2007 during STS-117 that left the station without thruster, \"Elektron\", \"Vozdukh\" and other environmental control system operations. In the latter case, the root cause was found to be condensation inside electrical connectors leading to a short circuit.\nDuring STS-120 in 2007 and following the relocation of the P6 truss and solar arrays, it was noted during unfurling that the solar array had torn and was not deploying properly. An EVA was carried out by Scott Parazynski, assisted by Douglas Wheelock. Extra precautions were taken to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic bearing surfaces, so the joint was locked to prevent further damage. Repairs to the joints were carried out during STS-126 with lubrication and the replacement of 11 out of 12 trundle bearings on the joint.\nIn September 2008, damage to the S1 radiator was first noticed in Soyuz imagery. The problem was initially not thought to be serious. The imagery showed that the surface of one sub-panel had peeled back from the underlying central structure, possibly because of micro-meteoroid or debris impact. On 15 May 2009, the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was then used to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak. It is also known that a Service Module thruster cover struck the S1 radiator after being jettisoned during an EVA in 2008, but its effect, if any, has not been determined.\nIn the early hours of 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down.\nPlanned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed because of an ammonia leak in one of four quick-disconnects. A second EVA on 11 August removed the failed pump module. A third EVA was required to restore Loop A to normal functionality.\nThe USOS's cooling system is largely built by the US company Boeing, which is also the manufacturer of the failed pump.\nThe four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. Each MBSU has two power channels that feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. In late 2011, MBSU-1 ceased responding to commands or sending data confirming its health. While still routing power correctly, it was scheduled to be swapped out at the next available EVA. A spare MBSU was already on board, but a 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before the electrical connection was secured. The loss of MBSU-1 limited the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem could be addressed.\nOn 5 September 2012, in a second six-hour EVA, astronauts Sunita Williams and Akihiko Hoshide successfully replaced MBSU-1 and restored the ISS to 100% power.\nOn 24 December 2013, astronauts installed a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a \"mini blizzard\" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history.\nMission control centres.\nThe components of the ISS are operated and monitored by their respective space agencies at mission control centres across the globe, primarily the Christopher C. Kraft Jr. Mission Control Center in Houston and the RKA Mission Control Center (TsUP) in Moscow, with support from Tsukuba Space Center in Japan, Payload Operations and Integration Center in Huntsville, Alabama, U.S., Columbus Control Center in Munich, Germany and Mobile Servicing System Control at the Canadian Space Agency's headquarters in Saint-Hubert, Quebec.\nOrbit, debris and visibility.\nAltitude and orbital inclination.\nThe ISS is currently maintained in a nearly circular orbit with a minimum mean altitude of and a maximum of , in the centre of the thermosphere, at an inclination of 51.6 degrees to Earth's equator with an eccentricity of 0.0002267. This orbit was selected because it is the lowest inclination that can be directly reached by Russian Soyuz and Progress spacecraft launched from Baikonur Cosmodrome at 46\u00b0 N latitude without overflying China or dropping spent rocket stages in inhabited areas. It travels at an average speed of , and completes 15.5 orbits per day (93 minutes per orbit). The station's altitude was allowed to fall around the time of each NASA shuttle flight to permit heavier loads to be transferred to the station. After the retirement of the shuttle, the nominal orbit of the space station was raised in altitude (from about 350\u00a0km to about 400\u00a0km). Other, more frequent supply spacecraft do not require this adjustment as they are substantially higher performance vehicles.\nAtmospheric drag reduces the altitude by about 2\u00a0km a month on average. Orbital boosting can be performed by the station's two main engines on the \"Zvezda\" service module, or Russian or European spacecraft docked to \"Zvezda\"'s aft port. The Automated Transfer Vehicle is constructed with the possibility of adding a second docking port to its aft end, allowing other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. Maintaining ISS altitude uses about 7.5 tonnes of chemical fuel per annum at an annual cost of about $210\u00a0million.\nThe Russian Orbital Segment contains the Data Management System, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. Initially, \"Zarya\", the first module of the station, controlled the station until a short time after the Russian service module \"Zvezda\" docked and was transferred control. \"Zvezda\" contains the ESA built DMS-R Data Management System. Using two fault-tolerant computers (FTC), \"Zvezda\" computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting.\nOrientation.\n\"Zvezda\" uses gyroscopes (reaction wheels) and thrusters to turn itself. Gyroscopes do not require propellant; instead they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer-controlled gyroscopes to handle its extra mass. When gyroscopes 'saturate', thrusters are used to cancel out the stored momentum. In February 2005, during Expedition 10, an incorrect command was sent to the station's computer, using about 14\u00a0kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, this can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which itself has no thrusters.\nDocked spacecraft can also be used to maintain station attitude, such as for troubleshooting or during the installation of the S3/S4 truss, which provides electrical power and data interfaces for the station's electronics.\nOrbital debris threats.\nThe low altitudes at which the ISS orbits are also home to a variety of space debris, including spent rocket stages, defunct satellites, explosion fragments (including materials from anti-satellite weapon tests), paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids, are a significant threat. Objects large enough to destroy the station can be tracked, and therefore are not as dangerous as smaller debris. Objects too small to be detected by optical and radar instruments, from approximately 1\u00a0cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are a threat because of their kinetic energy and direction in relation to the station. Spacewalking crew in spacesuits are also at risk of suit damage and consequent exposure to vacuum.\nBallistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels depend on their predicted exposure to damage. The station's shields and structure have different designs on the ROS and the USOS. On the USOS, Whipple Shields are used. The US segment modules consist of an inner layer made from aluminium, a intermediate layers of Kevlar and Nextel (a ceramic fabric), and an outer layer of stainless steel, which causes objects to shatter into a cloud before hitting the hull, thereby spreading the energy of impact. On the ROS, a carbon fibre reinforced polymer honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top.\nSpace debris is tracked remotely from the ground, and the station crew can be notified. If necessary, thrusters on the Russian Orbital Segment can alter the station's orbital altitude, avoiding the debris. These Debris Avoidance Manoeuvres (DAMs) are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. Ten DAMs had been performed by the end of 2009. Usually, an increase in orbital velocity of the order of 1\u00a0m/s is used to raise the orbit by one or two kilometres. If necessary, the altitude can also be lowered, although such a manoeuvre wastes propellant. If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their spacecraft in order to be able to evacuate in the event the station was seriously damaged by the debris. Partial station evacuations have occurred on 13 March 2009, 28 June 2011, 24 March 2012, 16 June 2015, November 2021, and 27 June 2024.\nThe November 2021 evacuation was caused by a Russian anti-satellite weapon test. NASA administrator Bill Nelson said it was unthinkable that Russia would endanger the lives of everyone on ISS, including their own cosmonauts.\nVisibility from Earth.\nThe ISS is visible in the sky to the naked eye as a visibly moving, bright white dot, when crossing the sky and being illuminated by the Sun, during twilight, the hours after sunset and before sunrise, when the station remains sunlit, outside of Earth's shadow, but the ground and sky are dark. It crosses the skies at latitudes between the polar regions. Depending on the path it takes across the sky, the time it takes the station to move across the horizon or from one to the other may be short or up to 10 minutes, while likely being only visible part of that time because of it moving into or out of Earth's shadow. It then returns around every 90 minutes, with the time of the day that it crosses the sky shifting over the course of some weeks, and therefore before returning to twilight and visible illumination.\nBecause of the size of its reflective surface area, the ISS is the brightest artificial object in the sky (excluding other satellite flares), with an approximate maximum magnitude of \u22124 when in sunlight and overhead (similar to Venus), and a maximum angular size of 63 arcseconds.\nTools are provided by a number of websites such as Heavens-Above (see \"Live viewing\" below) as well as smartphone applications that use orbital data and the observer's longitude and latitude to indicate when the ISS will be visible (weather permitting), where the station will appear to rise, the altitude above the horizon it will reach and the duration of the pass before the station disappears either by setting below the horizon or entering into Earth's shadow.\nIn November 2012 NASA launched its \"Spot the Station\" service, which sends people text and email alerts when the station is due to fly above their town. The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes.\nUnder specific conditions, the ISS can be observed at night on five consecutive orbits. Those conditions are 1) a mid-latitude observer location, 2) near the time of the solstice with 3) the ISS passing in the direction of the pole from the observer near midnight local time. The three photos show the first, middle and last of the five passes on 5\u20136\u00a0June 2014.\nAstrophotography.\nUsing a telescope-mounted camera to photograph the station is a popular hobby for astronomers, while using a mounted camera to photograph the Earth and stars is a popular hobby for crew. The use of a telescope or binoculars allows viewing of the ISS during daylight hours.\nTransits of the ISS in front of the Sun, particularly during an eclipse (and so the Earth, Sun, Moon, and ISS are all positioned approximately in a single line) are of particular interest for amateur astronomers.\nEnvironment, safety and crew health.\nEnvironment.\nFreefall environment.\nGravity at the altitude of the ISS is approximately 90% as strong as at Earth's surface, but objects in orbit are in a continuous state of freefall, resulting in an apparent state of weightlessness. This perceived weightlessness is disturbed by five effects:\nRadiation.\nThe ISS is partially protected from the space environment by Earth's magnetic field. From an average distance of about from the Earth's surface, depending on Solar activity, the magnetosphere begins to deflect solar wind around Earth and the space station. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. In 2005, during the initial \"proton storm\" of an X-3 class solar flare, the crew of Expedition 10 took shelter in a more heavily shielded part of the ROS designed for this purpose.\nSubatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by Earth's atmosphere. When they interact in sufficient quantity, their effect is visible to the naked eye in a phenomenon called an aurora. Outside Earth's atmosphere, ISS crews are exposed to approximately one millisievert each day (about a year's worth of natural exposure on Earth), resulting in a higher risk of cancer. Radiation can penetrate living tissue and damage the DNA and chromosomes of lymphocytes; being central to the immune system, any damage to these cells could contribute to the lower immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and medications may lower the risks to an acceptable level.\nRadiation levels on the ISS are between 12 and 28.8 milli rads per day, about five times greater than those experienced by airline passengers and crew, as Earth's electromagnetic field provides almost the same level of protection against solar and other types of radiation in low Earth orbit as in the stratosphere. For example, on a 12-hour flight, an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; this is one fifth the rate experienced by an astronaut in LEO. Additionally, airline passengers experience this level of radiation for a few hours of flight, while the ISS crew are exposed for their whole stay on board the station.\nMicrobiological environmental hazards.\nHazardous molds that can foul air and water filters may develop aboard space stations. They can produce acids that degrade metal, glass, and rubber. They can also be harmful to the crew's health. Microbiological hazards have led to a development of the LOCAD-PTS (a portable test system) which identifies common bacteria and molds faster than standard methods of culturing, which may require a sample to be sent back to Earth. Researchers in 2018 reported, after detecting the presence of five \"Enterobacter bugandensis\" bacterial strains on the ISS (none of which are pathogenic to humans), that microorganisms on the ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.\nContamination on space stations can be prevented by reduced humidity, and by using paint that contains mold-killing chemicals, as well as the use of antiseptic solutions. All materials used in the ISS are tested for resistance against fungi. Since 2016, a series of ESA-sponsored experiments have been conducted to test the anti-bacterial properties of various materials, with the goal of developing \"smart surfaces\" that mitigate bacterial growth in multiple ways, using the best method for a particular circumstance. Dubbed \"Microbial Aerosol Tethering on Innovative Surfaces\" (MATISS), the programme involves deployment of small plaques containing an array of glass squares covered with different test coatings. They remain on the station for six months before being returned to earth for analysis. The most recent and final experiment of the series was launched on 5 June 2023 aboard the SpaceX CRS-28 cargo mission to ISS, comprising four plaques. Whereas previous experiments in the series were limited to analysis by light microsocopy, this experiment uses quartz glass made of pure silica, which will allow spectrographic analysis. Two of the plaques were returned after eight months and the remaining two after 16 months.\nIn April 2019, NASA reported that a comprehensive study had been conducted into the microorganisms and fungi present on the ISS. The experiment was performed over a period of 14 months on three different flight missions, and involved taking samples from 8 predefined locations inside the station, then returning them to earth for analysis. In prior experiments, analysis was limited to culture-based methods, thus overlooking microbes which cannot be grown in culture. The present study used molecular-based methods in addition to culturing, resulting in a more complete catalog. The results may be useful in improving the health and safety conditions for astronauts, as well as better understanding other closed-in environments on Earth such as clean rooms used by the pharmaceutical and medical industries.\nNoise.\nSpace flight is not inherently quiet, with noise levels exceeding acoustic standards as far back as the Apollo missions. For this reason, NASA and the International Space Station international partners have developed noise control and hearing loss prevention goals as part of the health program for crew members. Specifically, these goals have been the primary focus of the ISS Multilateral Medical Operations Panel (MMOP) Acoustics Subgroup since the first days of ISS assembly and operations. The effort includes contributions from acoustical engineers, audiologists, industrial hygienists, and physicians who comprise the subgroup's membership from NASA, Roscosmos, the European Space Agency (ESA), the Japanese Aerospace Exploration Agency (JAXA), and the Canadian Space Agency (CSA).\nWhen compared to terrestrial environments, the noise levels experienced by astronauts and cosmonauts on the ISS may seem insignificant and typically occur at levels that would not be of major concern to the Occupational Safety and Health Administration \u2013 rarely reaching 85 dBA. But crew members are exposed to these levels 24 hours a day, seven days a week, with current missions averaging six months in duration. These levels of noise also impose risks to crew health and performance in the form of sleep interference and communication, as well as reduced alarm audibility.\nOver the 19 plus year history of the ISS, significant efforts have been put forth to limit and reduce noise levels on the ISS. During design and pre-flight activities, members of the Acoustic Subgroup have written acoustic limits and verification requirements, consulted to design and choose the quietest available payloads, and then conducted acoustic verification tests prior to launch. During spaceflights, the Acoustics Subgroup has assessed each ISS module's in flight sound levels, produced by a large number of vehicle and science experiment noise sources, to assure compliance with strict acoustic standards. The acoustic environment on ISS changed when additional modules were added during its construction, and as additional spacecraft arrive at the ISS. The Acoustics Subgroup has responded to this dynamic operations schedule by successfully designing and employing acoustic covers, absorptive materials, noise barriers, and vibration isolators to reduce noise levels. Moreover, when pumps, fans, and ventilation systems age and show increased noise levels, this Acoustics Subgroup has guided ISS managers to replace the older, noisier instruments with quiet fan and pump technologies, significantly reducing ambient noise levels.\nNASA has adopted most-conservative damage risk criteria (based on recommendations from the National Institute for Occupational Safety and Health and the World Health Organization), in order to protect all crew members. The MMOP Acoustics Subgroup has adjusted its approach to managing noise risks in this unique environment by applying, or modifying, terrestrial approaches for hearing loss prevention to set these conservative limits. One innovative approach has been NASA's Noise Exposure Estimation Tool (NEET), in which noise exposures are calculated in a task-based approach to determine the need for hearing protection devices (HPDs). Guidance for use of HPDs, either mandatory use or recommended, is then documented in the Noise Hazard Inventory, and posted for crew reference during their missions. The Acoustics Subgroup also tracks spacecraft noise exceedances, applies engineering controls, and recommends hearing protective devices to reduce crew noise exposures. Finally, hearing thresholds are monitored on-orbit, during missions.\nThere have been no persistent mission-related hearing threshold shifts among US Orbital Segment crewmembers (JAXA, CSA, ESA, NASA) during what is approaching 20 years of ISS mission operations, or nearly 175,000 work hours. In 2020, the MMOP Acoustics Subgroup received the Safe-In-Sound Award for Innovation for their combined efforts to mitigate any health effects of noise.\nFire and toxic gases.\nAn onboard fire or a toxic gas leak are other potential hazards. Ammonia is used in the external radiators of the station and could potentially leak into the pressurised modules.\nOverall health effects.\nOn 12 April 2019, NASA reported medical results from the Astronaut Twin Study. Astronaut Scott Kelly spent a year in space on the ISS, while his identical twin spent the year on Earth. Several long-lasting changes were observed, including those related to alterations in DNA and cognition, when one twin was compared with the other.\nIn November 2019, researchers reported that astronauts experienced serious blood flow and clot problems while on board the ISS, based on a six-month study of 11 healthy astronauts. The results may influence long-term spaceflight, including a mission to the planet Mars, according to the researchers.\nStress.\nThere is considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. Cosmonaut Valery Ryumin wrote in his journal during a particularly difficult period on board the \"Salyut\" 6 space station: \"All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 [5.5\u00a0m \u00d7 6\u00a0m] and leave them together for two months.\"\nNASA's interest in psychological stress caused by space travel, initially studied when their crewed missions began, was rekindled when astronauts joined cosmonauts on the Russian space station \"Mir\". Common sources of stress in early US missions included maintaining high performance under public scrutiny and isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child.\nA study of the longest spaceflight concluded that the first three weeks are a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. ISS crew flights typically last about five to six months.\nThe ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First-generation space stations had crews who spoke a single language; second- and third-generation stations have crew from many cultures who speak many languages. Astronauts must speak English and Russian, and knowing additional languages is even better.\nDue to the lack of gravity, confusion often occurs. Even though there is no up and down in space, some crew members feel like they are oriented upside down. They may also have difficulty measuring distances. This can cause problems like getting lost inside the space station, pulling switches in the wrong direction or misjudging the speed of an approaching vehicle during docking.\nMedical.\nThe physiological effects of long-term weightlessness include muscle atrophy, deterioration of the skeleton (osteopenia), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face.\nSleep is regularly disturbed on the ISS because of mission demands, such as incoming or departing spacecraft. Sound levels in the station are unavoidably high. The atmosphere is unable to thermosiphon naturally, so fans are required at all times to process the air which would stagnate in the freefall (zero-G) environment.\nTo prevent some of the adverse effects on the body, the station is equipped with: two TVIS treadmills (including the COLBERT); the ARED (Advanced Resistive Exercise Device), which enables various weightlifting exercises that add muscle without raising (or compensating for) the astronauts' reduced bone density; and a stationary bicycle. Each astronaut spends at least two hours per day exercising on the equipment. Astronauts use bungee cords to strap themselves to the treadmill.\nLife aboard.\nLiving quarters.\nThe living and working space aboard the International Space Station (ISS) is larger than a six-bedroom house, equipped with seven private sleeping quarters, three bathrooms, two dining rooms, a gym, and a panoramic 360-degree-view bay window.\nThe station provides dedicated crew quarters for long-term crew members. Two are located in \"Zvezda\", one in \"Nauka\", and four in \"Harmony\". These soundproof, person-sized booths offer privacy, ventilation, and basic amenities such as a sleeping bag, a reading lamp, and storage for personal items. The quarters in \"Zvezda\" include a small window but have less ventilation and soundproofing.\nVisiting crew members use tethered sleeping bags attached to available wall space or inside their spacecraft. While it is possible to sleep floating freely, this is generally avoided to prevent collisions with sensitive equipment. Proper ventilation is critical, as astronauts risk oxygen deprivation if exhaled carbon dioxide accumulates in a bubble around their heads.\nThe station's lighting system is adjustable, allowing for dimming, switching off, and colour temperature changes to support crew activities and rest.\nCrew activities.\nThe ISS operates on Coordinated Universal Time (UTC). A typical day aboard the ISS begins at 06:00 with wake-up, post-sleep routines, and a morning inspection of the station. After breakfast, the crew holds a daily planning conference with Mission Control, starting work around 08:10. Morning tasks include scheduled exercise, scientific experiments, maintenance, or operational duties. Following a one-hour lunch break at 13:05, the crew resumes their afternoon schedule of work and exercise. Pre-sleep activities, including dinner and a crew conference, begin at 19:30, with the scheduled sleep period starting at 21:30.\nThe crew works approximately 10 hours on weekdays and 5 hours on Saturdays, with the remaining time allocated for relaxation or catching up on tasks. Free time often involves enjoying personal hobbies, communicating with family, or gazing out at Earth through the station's windows. The crew can watch TV aboard the station.\nWhen the Space Shuttle was operating, the ISS crew aligned with the shuttle crew's Mission Elapsed Time, a flexible schedule based on the shuttle's launch.\nTo simulate night conditions, the station's windows are covered during designated sleep periods, as the ISS experiences 16 sunrises and sunsets daily due to its orbital speed.\nReflection and material culture.\nReflection of individual and crew characteristics are found particularly in the decoration of the station and expressions in general, such as religion. The latter has produced a certain material economy between the station and Russia in particular.\nThe micro-society of the station, as well as wider society, and possibly the emergence of distinct station cultures, is being studied by analyzing many aspects, from art to dust accumulation, as well as archaeologically how material of the ISS has been discarded.\nFood.\nFood aboard the International Space Station (ISS) is preserved and packaged to withstand long storage times, minimize waste, and prevent contamination of station systems. Because microgravity dulls taste, meals are often seasoned more heavily than on Earth. Crews particularly look forward to resupply missions, which deliver perishable items such as fresh fruit and vegetables. To reduce the risk of crumbs and spills damaging equipment, foods are prepared in specialized packaging, liquid condiments are preferred over powdered ones, and containers are secured with Velcro or magnets. Drinks are delivered as powders to be mixed with water, while soups and beverages are sipped from plastic bags with straws. Solid foods are eaten with utensils attached to trays by magnets, and any stray food must be collected to prevent it from clogging air filters and other systems.\nThe first galley was installed in \"Zvezda\", equipped with an electro-resistive can warmer and a water dispenser for both hot and ambient water. Many Russian meals are still packaged in cans, which are eaten directly, while others are provided in retort pouches rehydrated with the water dispenser.\nA second galley was later added to \"Unity\" to support the station's larger crew. It contains two briefcase-shaped food warmers, a refrigerator (added in 2008), and a water dispenser. Most food in the U.S. Orbital Segment is packaged in retort pouches, which are rehydrated if necessary and heated or chilled in a food warmer or refrigerator as desired.\nWhile crews occasionally gather for group meals in \"Unity\", especially during holidays or special occasions, they more often eat in small groups because of differing schedules. Russian cosmonauts also retain the option of dining separately in \"Zvezda\", where the can warmer is located. With the growing diversity of NASA's astronaut corps and the large number of international astronauts who have flown to the ISS, the variety of food available has expanded significantly. Efforts are made to provide meals that reflect astronauts' cultural backgrounds and personal preferences, and food is often shared among crew members.\nExperiments have also been conducted aboard the ISS to grow fresh vegetables in orbit. These studies aim to supplement astronauts' diets with additional nutrients, provide psychological benefits, and advance space agricultural techniques needed for long-duration missions to the Moon and Mars. As of 2023, crops grown include three types of lettuce, Chinese cabbage, mizuna mustard, and red Russian kale. Some of the plants are harvested and eaten by the crew, while others were returned to Earth for analysis. In the future, NASA plans to grow tomatoes and peppers, and eventually berries, beans, and other nutrient-rich foods. Such crops could offer not only improved nutrition, but also potential protection against space radiation for crew members who consume them.\nPersonal hygiene.\nThe ISS is equipped with three Russian-designed toilets, located in \"Zvezda\", \"Tranquility\" and \"Nauka\". Inside these \"Waste and Hygiene Compartments\" the occupant fastens themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a proper seal. A lever activates a powerful fan while opening a suction port at the bottom of the toilet bowl, and the airstream carries waste away. Solid waste is stored in individual bags placed in an aluminium container, which is later transferred to cargo spacecraft that will burn up on reentry. Liquid waste is collected through a hose with anatomically shaped funnel adapters so that both men and women can use the same system. The urine is diverted to the Water Recovery System, where it is processed into drinking water.\nShowers were first introduced on space stations in the early 1970s aboard \"Skylab\" and \"Salyut\"\u00a03. However, crews complained about the complexity of showering, and by the time of \"Salyut 6\" in the early 1980s, it had been reduced to a monthly activity. The ISS, like later Russian stations after has no shower; instead, astronauts clean themselves with wet wipes or with a water jet and using soap dispensed from a toothpaste-like tube. Rinseless shampoo and edible toothpaste are also provided to conserve water.\nEnd of mission.\nOriginally the ISS was planned to be a 15-year mission.\nTherefore, an end of mission had been worked on, but was several times postponed due to the success and support for the operation of the station. As a result, the oldest modules of the ISS have been in orbit for more than 20 years, with their reliability having decreased. It has been proposed to use funds elsewhere instead, for example for a return to the Moon. According to the Outer Space Treaty, the parties are legally responsible for all spacecraft or modules they launch. An unmaintained station would pose an orbital and re-entry hazard.\nRussia has stated that it plans to pull out of the ISS program after 2025. However, Russian modules will provide orbital station-keeping until 2028.\nThe US planned in 2009 to deorbit the ISS in 2016. But on 30 September 2015, Boeing's contract with NASA as prime contractor for the ISS was extended to 30 September 2020. Part of Boeing's services under the contract related to extending the station's primary structural hardware past 2020 to the end of 2028. In July 2018, the Space Frontier Act of 2018 was intended to extend operations of the ISS to 2030. This bill was unanimously approved in the Senate, but failed to pass in the U.S. House. In September 2018, the Leading Human Spaceflight Act was introduced with the intent to extend operations of the ISS to 2030, and was confirmed in December 2018. Congress later passed similar provisions in its CHIPS and Science Act, signed into law by U.S. President Joe Biden on 9 August 2022.\nIf until 2031 Commercial LEO Destinations providers are not sufficient to accommodate NASA's projects, NASA is suggesting to extend ISS operations beyond 2031.\nNASA's disposal plans.\nNASA considered originally several possible disposal options: natural orbital decay with random reentry (as with Skylab), boosting the station to a higher altitude (which would delay reentry), and a controlled de-orbit targeting a remote ocean area.\nNASA determined that random reentry carried an unacceptable risk of producing hazardous space debris that could hit people or property and re-boosting the station would be costly and could also create hazards.\nPrior to 2010, plans had contemplated using a slightly modified Progress spacecraft to de-orbit the ISS. However, NASA concluded Progress would not be adequate for the job, and decided on a spacecraft specifically designed for the task.\nIn January 2022, NASA announced a planned date of January 2031 to de-orbit the ISS using the \"U.S. Deorbit Vehicle\" and direct any remnants into a remote area of the South Pacific Ocean that has come to be known as the spacecraft cemetery. NASA plans to launch the deorbit vehicle in 2030, docking at the Harmony forward port. The deorbit vehicle will remain attached, dormant, for about a year as the station's orbit naturally decays to . The spacecraft would then conduct one or more orientation burns to lower the perigee to , followed by a final deorbiting burn.\nNASA began planning for the deorbit vehicle after becoming wary of Russia pulling out of the ISS abruptly, leaving the other partners with few good options for a controlled reentry. In June 2024, NASA selected SpaceX to develop the U.S. Deorbit Vehicle, a contract potentially worth $843 million. The vehicle will consist of an existing Cargo Dragon spacecraft which will be paired with a significantly lengthened trunk module which will be equipped with 46 Draco thrusters (instead of the normal 16) and will carry of propellant, nearly six times the normal load. NASA is still working to secure all the necessary funding to build, launch and operate the deorbit vehicle.\nOn 20 February 2025, Elon Musk, CEO of SpaceX and Senior Advisor to President Trump, suggested in a tweet that the International Space Station be de-orbited \"two years from now\" as Musk believes the station has \"served its purpose\" and has \"very little incremental utility\". Despite this, no official decisions on moving up the de-orbiting date have been made yet by the president.\nPost mission proposals and plans.\nThe follow-up to NASA's program/strategy is the Commercial LEO Destinations Program, meant to allow private industry to build and maintain their own stations, and NASA procuring access as a customer, starting in 2028. Similarly, the ESA has been seeking new private space stations to provide orbital services, as well as retrieve materials, from the ISS. Axiom Station is planned to begin as a single module temporarily hosted at the ISS in 2027. Additionally, there have been suggestions in the commercial space industry that the ISS could be converted to commercial operations after it is retired by government entities, including turning it into a space hotel.\nRussia previously has planned to use its orbital segment for the construction of its OPSEK station after the ISS is decommissioned. The modules under consideration for removal from the current ISS included the Multipurpose Laboratory Module (\"Nauka\"; \"MLM\"), launched in July 2021, and the other new Russian modules that are proposed to be attached to \"Nauka\". These newly launched modules would still be well within their useful lives in 2024. At the end of 2011, the Exploration Gateway Platform concept also proposed using leftover USOS hardware and \"Zvezda 2\" as a refuelling depot and service station located at one of the Earth\u2013Moon Lagrange points. However, the entire USOS was not designed for disassembly and will be discarded.\nWestern space industry has suggested in 2022 using the ISS as a platform to develop orbital salvage capacities, by companies such as CisLunar Industries working on using space debris as fuel, instead of plunging it into the ocean.\nNASA has stated that by July 2024 it has not seen any viable proposals for reuse of the ISS or parts of it.\nCost.\nThe ISS has been described as the most expensive single item ever constructed. As of 2010, the total cost was US$150\u00a0billion. This includes NASA's budget of $58.7\u00a0billion ($89.73\u00a0billion in 2021 dollars) for the station from 1985 to 2015, Russia's $12\u00a0billion, Europe's $5\u00a0billion, Japan's $5\u00a0billion, Canada's $2\u00a0billion, and the cost of 36 shuttle flights to build the station, estimated at $1.4\u00a0billion each, or $50.4\u00a0billion in total. Assuming 20,000 man-days of use from 2000 to 2015 by two- to six-person crews, each man-day would cost $7.5\u00a0million, less than half the inflation-adjusted $19.6\u00a0million ($5.5\u00a0million before inflation) per man-day of Skylab.\nIn culture.\nThe ISS has become an international symbol of human capabilities, particularly human cooperation and science, defining a cooperative international approach and period, instead of a looming commercialized and militarized interplanetary world.\nFilm.\nBeside numerous documentaries such as the IMAX documentaries \"Space Station 3D\" from 2002, or \"A Beautiful Planet\" from 2016, and films like \"Apogee of Fear\" (2012) and \"Yolki 5\" (2016) the ISS is the subject of feature films such as \"The Day After Tomorrow\" (2004), \"Love\" (2011), together with the Chinese station Tiangong 1 in \"Gravity\" (2013), \"Life\" (2017), and \"I.S.S.\" (2023).\nIn 2022, the movie \"The Challenge\" (\"Doctor's House Call\") was filmed aboard the ISS, and was notable for being the first feature film in which both professional actors and director worked together in space.\nLiterature.\nNeal Stephenson's 2015 novel Seveneves is the set largely on the ISS for the first and second parts of the novel. The ISS is depicted largely as it was when the novel was written, but with the addition of a large captured asteroid attached to the station.\nThe 2023 novel by English writer Samantha Harvey, \"Orbital\", is set in the space station. It won the 2024 Booker Prize\nCeridwen Dovey's \"Only the Astronauts\", a 2024 collection of short stories in which the narrator in each story is an inanimate object in space, includes the International Space Station.\nVideo games.\nThe ISS is blown up during the \"\" mission \"Second Sun\", in which the character Captain Price launches an ICBM into the earth's atmosphere; the resulting shockwave destroys the station.\nThe ISS is present in \"Far Cry New Dawn\" as an expedition site, having fallen to earth during a nuclear war.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAttributions.\n\u00a0This article incorporates https:// from \nExternal links.\nMultimedia.\n "}
{"id": "15044", "revid": "42195518", "url": "https://en.wikipedia.org/wiki?curid=15044", "title": "Irish", "text": "Irish commonly refers to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nIrish may also refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15045", "revid": "382357", "url": "https://en.wikipedia.org/wiki?curid=15045", "title": "Cosmicomics", "text": "1965 book by Italo Calvino\nCosmicomics () is a collection of twelve short stories by Italo Calvino first published in Italian in 1965 and in English in 1968. The stories were originally published between 1964 and 1965 in the Italian periodicals \"Il Caff\u00e8\" and \"Il Giorno\". Each story takes a scientific theory (some of which have since become deprecated) or phenomenon and builds an imaginative story around it. An always-extant being called Qfwfq explicitly narrates all of the stories save two. Every story is a memory of an event in the history of the universe.\nAll of the stories in \"Cosmicomics\", together with more of Qfwfq stories from \"t zero\" and other sources, are now available in a single volume collection, \"The Complete Cosmicomics\" (Penguin UK, 2009).\nThe first U.S. edition, translated by William Weaver, won the National Book Award in the translation category.\nContents.\nAll of the stories feature non-human characters who have been heavily anthropomorphized.\nAdaptations.\n\"\"La Luna\" by Enrico Casarosa, 2011\" is a short film based on the same premise as \"The Distance of the Moon.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15046", "revid": "47885804", "url": "https://en.wikipedia.org/wiki?curid=15046", "title": "IA-32", "text": "32-bit version of x86 architecture\nIA-32 (short for \"Intel Architecture, 32-bit\", commonly called \"i386\") is the 32-bit version of the x86 instruction set architecture, designed by Intel and first implemented in the 80386 microprocessor in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing; as a result, the \"IA-32\" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.\nWithin various programming language directives, IA-32 is still sometimes referred to as the \"i386\" architecture. In some other contexts, certain iterations of the IA-32 ISA are sometimes labelled \"i486\", \"i586\" and \"i686\", referring to the instruction supersets offered by the 80486, the P5 and the P6 microarchitectures respectively. These updates offered numerous additions alongside the base IA-32 set including floating-point capabilities and the MMX extensions.\nIntel was historically the largest manufacturer of IA-32 processors, with the second biggest supplier having been AMD. During the 1990s, VIA, Transmeta and other chip manufacturers also produced IA-32 compatible processors (e.g. WinChip). In the modern era, Intel still produced IA-32 processors under the Intel Quark microcontroller platform until 2019; however, since the 2000s, the majority of manufacturers (Intel included) moved almost exclusively to implementing CPUs based on the 64-bit variant of x86, x86-64. x86-64, by specification, offers legacy operating modes that operate on the IA-32 ISA for backwards compatibility. Even given the contemporary prevalence of x86-64, as of today, IA-32 protected mode versions of many modern operating systems are still maintained, e.g. Microsoft Windows (up to Windows 10), Windows Server (up to Windows Server 2008) and the Debian Linux distribution. In spite of IA-32's name (and causing some potential confusion), the 64-bit evolution of x86 that originated out of AMD would not be known as \"IA-64\", that name instead belonging to Intel's discontinued Itanium architecture.\nArchitectural features.\nThe primary defining characteristic of IA-32 is the availability of 32-bit general-purpose processor registers (for example, EAX and EBX), 32-bit integer arithmetic and logical operations, 32-bit offsets within a segment in protected mode, and the translation of segmented addresses to 32-bit linear addresses. The designers took the opportunity to make other improvements as well. Some of the most significant changes (relative to the 16-bit 286 instruction set) are:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15047", "revid": "49971626", "url": "https://en.wikipedia.org/wiki?curid=15047", "title": "Internalism and externalism", "text": "Philosophical terms\nInternalism and externalism are two opposite ways of integrating and explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings. Internal\u2013external distinction is a distinction used in philosophy to divide an ontology into two parts: an internal part concerning observation related to philosophy, and an external part concerning question related to philosophy.\nInternalism is the thesis that no fact about the world can provide reasons for action independently of desires and beliefs. Externalism is the thesis that reasons are to be identified with objective features of the world.\nMoral philosophy.\nMotivation.\nIn contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper \"'Ought' and Motivation\".)\nThese views in moral psychology have various implications. In particular, if motivational internalism is true, then amorality is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational \"externalist\", because the motivational externalist thinks that moral judgments about what is right do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire\u2014such as the desire to do the right thing\u2014is required (Brink, 2003), (Rosati, 2006).\nReasons.\nThere is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for an action. An \"internal reason\" is, roughly, something that one has in light of one's own \"subjective motivational set\"\u2014one's own commitments, desires (or wants), goals, etc. On the other hand, an \"external reason\" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not to kill oneself no matter what\u2014regardless of whether one wants to die.\nSome philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called \"internalism about reasons\" (or \"reasons internalism\"). \"Externalism about reasons\" (or \"reasons externalism\") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.\nConsider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative (\"Yes, Sasha has a reason not to steal from that poor person.\"), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative (\"No, Sasha does not have a reason not to steal from that poor person, though others might.\"). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason \"for Sasha's action\" not to steal from the poor person next to him only if he currently \"wants\" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals\u2014that is, part of what Williams calls his \"subjective motivational set\"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.\nEpistemology.\nJustification.\nInternalism.\nTwo main varieties of epistemic internalism about justification are access internalism and ontological internalism. Access internalists require that a believer must have internal access to the justifier(s) of their belief \"p\" in order to be justified in believing \"p\". For the access internalist, justification amounts to something like the believer being aware (or capable of being aware) of certain facts that make her belief in \"p\" rational, or them being able to give reasons for her belief in \"p\". At minimum, access internalism requires that the believer have some kind of reflective access or awareness to whatever justifies her belief. Ontological internalism is the view that justification for a belief is established by one's mental states. Ontological internalism can be distinct from access internalism, but the two are often thought to go together since we are generally considered to be capable of having reflective access to mental states.\nOne popular argument for internalism is known as the 'new evil demon problem'. The new evil demon problem indirectly supports internalism by challenging externalist views of justification, particularly reliabilism. The argument asks us to imagine a subject with beliefs and experiences identical to ours, but the subject is being systematically deceived by a malicious Cartesian demon so that all their beliefs turn out false. In spite of the subject's unfortunate deception, the argument goes, we do not think this subject ceases to be rational in taking things to be as they appear as we do. After all, it is possible that we could be radically deceived in the same way, yet we are still justified in holding most of our beliefs in spite of this possibility. Since reliabilism maintains that one's beliefs are justified via reliable belief-forming processes (where reliable means yielding true beliefs), the subject in the evil demon scenario would not likely have any justified beliefs according to reliabilism because all of their beliefs would be false. Since this result is supposed to clash with our intuitions that the subject is justified in their beliefs in spite of being systematically deceived, some take the new evil demon problem as a reason for rejecting externalist views of justification.\nExternalism.\nExternalist views of justification emerged in epistemology during the late 20th century. Externalist conceptions of justification assert that facts external to the believer can serve as the justification for a belief. According to the externalist, a believer need not have any internal access or cognitive grasp of any reasons or facts which make their belief justified. The externalist's assessment of justification can be contrasted with access internalism, which demands that the believer have internal reflective access to reasons or facts which corroborate their belief in order to be justified in holding it. Externalism, on the other hand, maintains that the justification for someone's belief can come from facts that are entirely external to the agent's subjective awareness.\nAlvin Goldman, one of the most well-known proponents of externalism in epistemology, is known for developing a popular form of externalism called reliabilism. In his paper, \u201cWhat is Justified Belief?\u201d Goldman characterizes the reliabilist conception of justification as such:\n\"If S\u2019s believing \"p\" at \"t\" results from a reliable cognitive belief-forming process (or set of processes), then S\u2019s belief in \"p\" at \"t\" is justified.\u201d\nGoldman notes that a reliable belief-forming process is one which generally produces true beliefs.\nA unique consequence of reliabilism (and other forms of externalism) is that one can have a justified belief without knowing one is justified (this is not possible under most forms of epistemic internalism). In addition, we do not yet know which cognitive processes are in fact reliable, so anyone who embraces reliabilism must concede that we do not always know whether some of our beliefs are justified (even though there is a fact of the matter).\nAs a response to skepticism.\nIn responding to skepticism, Hilary Putnam (1982) claims that semantic externalism yields \"an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word \"water\" referred to the substance whose chemical composition is H2O even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling \"water\" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Either I am a BIV, or I am not a BIV.\nIf I am not a BIV, then when I say \"I am not a BIV\", it is true.\nIf I am a BIV, then, when I say \"I am not a BIV\", it is true (because \"brain\" and \"vat\" would only pick out the brains and vats being simulated, not real brains and real vats).\nMy utterance of \"I am not a BIV\" is true.\nTo clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived \"Steve.\" When Steve is given an experience of walking through a park, semantic externalism allows for his thought, \"I am walking through a park\" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, \"I am a brain in a vat,\" to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.\nApart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words \"brain\" and \"vat\" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe \"I am not a brain in a vat,\" then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).\nAnother attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are \"about\" things, unlike a BIV's thoughts, which cannot be \"about\" things (DeRose, 1999).\nSemantics.\nSemantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.\nExternalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.\nSee also:\nPhilosophy of mind.\nWithin the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.\nThe traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various forms of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.\nAs to the traditional discussion on semantic externalism (often dubbed \"content externalism\"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, \"The Meaning of 'Meaning',\" (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.\nFor example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, \"I want my mommy,\" he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, \"I want my mommy,\" will be different from what Tina wants and what she says she wants when she says, \"I want my mommy.\"\nExternalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.\nPhilosophers now tend to distinguish between \"wide content\" (externalist mental content) and \"narrow content\" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.\nCritics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.\nCritics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains H2O. It seems this speaker could know a priori that they think that water is wet. This is the thesis of privileged access. It also seems that they could know on the basis of simple thought experiments that they can only think that water is wet if they live in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.\nAs mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.\nSee also:\nHistoriography of science.\nInternalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity. Imre Lakatos is a notable proponent of historiographical internalism.\nExternalism in the historiography of science is the view that the history of science is due to its social context \u2013 the socio-political climate and the surrounding economy determines scientific progress. Thomas Kuhn is a notable proponent of historiographical externalism.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15048", "revid": "8766034", "url": "https://en.wikipedia.org/wiki?curid=15048", "title": "Isolationism", "text": "Policy against engaging in international relations\nIsolationism is a term used to refer to a political philosophy advocating a foreign policy that opposes involvement in the political affairs, and especially the wars, of other countries. Thus, isolationism fundamentally advocates neutrality and opposes entanglement in military alliances and mutual defense pacts. In its purest form, isolationism opposes all commitments to foreign countries, including treaties and trade agreements. In the political science lexicon, there is also the term of \"non-interventionism\", which is sometimes improperly used to replace the concept of \"isolationism\". \"Non-interventionism\" is commonly understood as \"a foreign policy of political or military non-involvement in foreign relations or in other countries' internal affairs\". \"Isolationism\" should be interpreted more broadly as \"a foreign policy grand strategy of military and political non-interference in international affairs and in the internal affairs of sovereign states, associated with trade and economic protectionism and cultural and religious isolation, as well as with the inability to be in permanent military alliances, with the preservation, however, some opportunities to participate in temporary military alliances that meet the current interests of the state and in permanent international organizations of a non-military nature.\"\nThis contrasts with philosophies such as colonialism, expansionism, and liberal internationalism.\nIntroduction.\nIsolationism has been defined as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A policy or doctrine of trying to isolate one's country from the affairs of other nations by declining to enter into alliances, foreign economic commitments, international agreements, and generally attempting to make one's economy entirely self-reliant; seeking to devote the entire efforts of one's country to its own advancement, both diplomatically and economically, while remaining in a state of peace by avoiding foreign entanglements and responsibilities.\nBy country.\nBhutan.\nBefore 1999, Bhutan had banned television and the Internet in order to preserve its culture, environment, and identity. Eventually, Jigme Singye Wangchuck lifted the ban on television and the Internet. His son, Jigme Khesar Namgyel Wangchuck, was elected Druk Gyalpo of Bhutan, which helped forge the Bhutanese democracy. Bhutan has subsequently undergone a transition from an absolute monarchy to a constitutional monarchy multi-party democracy. The development of \"Bhutanese democracy\" has been marked by the active encouragement and participation of the reigning Bhutanese monarchs since the 1950s, beginning with legal reforms, and culminating in the enactment of Bhutan's Constitution.\nTourism in Bhutan was prohibited until 1974. Since then, the country has allowed foreigners to visit, but has tightly controlled tourism in an effort to preserve its natural and cultural heritage. As of 2022,[ [update]] tourists must pay a $200 per day fee on top of other travel expenses such as meals and accommodation. Prior to 2022, visitors were not allowed to travel independently and had to be accompanied by a tour guide. As of 2021,[ [update]] Bhutan does not maintain formal foreign relations with any of the five permanent members of the UN Security Council, notably including China, its neighbor to the north with which it has a historically tense relationship.\nCambodia.\nFrom 1431 to 1863, the Kingdom of Cambodia enforced an isolationist policy. The policy prohibited foreign contact with most outside countries. When Pol Pot and the Khmer Rouge came to power on 17 April 1975 and established Democratic Kampuchea, the urban population of every city, including Phnom Penh, was relocated to the countryside. This was ordered by the Communist Party of Kampuchea and the secret police Santebal, and they then established an infamous prison gulag inside the torture chamber called Tuol Sleng (S-21). Cambodia proceeded to implement the Year Zero policy, hastening isolation from the rest of the world. Ultimately, the authority of the Khmer Rouge and its isolationist policy would collapse in 1978 when the Vietnamese invaded the country and then overthrew Pol Pot on 7 January 1979.\nChina.\nAfter Zheng He's voyages in the 15th century, the foreign policy of the Ming dynasty in China became increasingly isolationist. The Hongwu Emperor was not the first to propose the policy to ban all maritime shipping in 1390. The Qing dynasty that came after the Ming dynasty often continued the Ming dynasty's isolationist policies. Wokou, which literally translates to \"Japanese pirates\" or \"dwarf pirates\", were pirates who raided the coastlines of China, Japan, and Korea, and were one of the key primary concerns, although the maritime ban was not without some control.\nIn the winter of 1757, the Qianlong Emperor declared that\u2014effective the next year\u2014Guangzhou was to be the only Chinese port permitted to foreign traders, beginning the Canton System.\nSince the division of the territory following the Chinese Civil War in 1949, China is divided into two regimes with the People's Republic of China solidified control on mainland China while the existing Republic of China was confined to the island of Taiwan as both governments lay claim to each other's sovereignty. While the PRC is recognized by the United Nations, European Union, and the majority of the world's states, the ROC remains diplomatically isolated although 15 states recognize it as \"China\" with some countries maintaining unofficial diplomatic relations through trade offices.\nJapan.\nFrom 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy called \"kaikin\". The policy prohibited foreign contact with most outside countries. The commonly held idea that Japan was entirely closed, however, is misleading. In fact, Japan maintained limited-scale trade and diplomatic relations with China, Korea, and the Ryukyu Islands, as well as the Dutch Republic as the only Western trading partner of Japan for much of the period.\nThe culture of Japan developed with limited influence from the outside world and had one of the longest stretches of peace in history. During this period, Japan developed thriving cities, castle towns, increasing commodification of agriculture and domestic trade, wage labor, increasing literacy and concomitant print culture, laying the groundwork for modernization even as the shogunate itself grew weak.\nKorea.\nIn 1863, Emperor Gojong took the throne of the Joseon Dynasty when he was a child. His father, Regent Heungseon Daewongun, ruled for him until Gojong reached adulthood. During the mid-1860s he was the main proponent of isolationism and the principal instrument of the persecution of both native and foreign Catholics.\nFollowing the division of the peninsula after independence from Japan at the end of World War II, Kim Il Sung inaugurated an isolationist nationalist regime in the North, which would continued by his son and grandson following his death in 1994.\nParaguay.\nIn 1814, three years after it gained its independence on May 14, 1811, Paraguay was taken over by the dictator Jos\u00e9 Gaspar Rodr\u00edguez de Francia. During his rule which lasted from 1814 until his death in 1840, he closed Paraguay's borders and prohibited trade or any relationship between Paraguay and the outside world. The Spanish settlers who had arrived in Paraguay just before it gained its independence were required to marry old colonists or the native Guaran\u00ed in order to create a single Paraguayan people.\nFrancia had a particular dislike of foreigners, and any foreigners who attempted to enter the country were not allowed to leave for an indefinite period of time. An independent character, he hated European influences and the Catholic Church and in order to try to keep foreigners at bay, he turned church courtyards into artillery parks and turned confession boxes into border sentry posts.\nUnited States.\nSome scholars, such as Robert J. Art, believe that the United States had an isolationist history, but most other scholars dispute that claim by describing the United States as following a strategy of unilateralism or non-interventionism rather than a strategy of isolationism. Robert Art makes his argument in \"A Grand Strategy for America\" (2003). Books that have made the argument that the United States followed unilaterism instead of isolationism include Walter A. McDougall's \"Promised Land, Crusader State\" (1997), John Lewis Gaddis's \"Surprise, Security, and the American Experience\" (2004), and Bradley F. Podliska's \"Acting Alone\" (2010). Both sides claim policy prescriptions from George Washington's Farewell Address as evidence for their argument. Bear F. Braumoeller argues that even the best case for isolationism, the United States in the interwar period, has been widely misunderstood and that Americans proved willing to fight as soon as they believed a genuine threat existed. Warren F. Kuehl and Gary B. Ostrower argue:\nEvents during and after the Revolution related to the treaty of alliance with France, as well as difficulties arising over the neutrality policy pursued during the French revolutionary wars and the Napoleonic wars, encouraged another perspective. A desire for separateness and unilateral freedom of action merged with national pride and a sense of continental safety to foster the policy of isolation. Although the United States maintained diplomatic relations and economic contacts abroad, it sought to restrict these as narrowly as possible in order to retain its independence. The Department of State continually rejected proposals for joint cooperation, a policy made explicit in the Monroe Doctrine's emphasis on unilateral action. Not until 1863 did an American delegate attend an international conference.\nCriticism.\nIsolationism has been criticized for the lack of aiding nations with major troubles. One notable example is that of American isolationism, which Benjamin Schwartz described as a \"tragedy\" inspired by Puritanism.\nSome modern American conservative commentators assert that labeling others as isolationist is used against individuals in a pejorative manner.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nWorks cited.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15049", "revid": "31952441", "url": "https://en.wikipedia.org/wiki?curid=15049", "title": "Indianapolis Colts", "text": "National Football League franchise in Indianapolis, Indiana\nThe Indianapolis Colts are a professional American football team based in Indianapolis. The Colts compete in the National Football League (NFL) as a member of the American Football Conference (AFC) South division. Since the 2008 season, the Colts have played their games in Lucas Oil Stadium. Previously, the team had played for over two decades (1984\u20132007) at the RCA Dome. Since 1987, the Colts have served as the host team for the NFL Scouting Combine.\nThe Colts have competed as a member club of the NFL since their founding in Baltimore, Maryland, in 1953, after then-owner Carroll Rosenbloom purchased the assets of the NFL's last founding Ohio League member Dayton Triangles\u2013Dallas Texans franchise. They were one of three NFL teams to join those of the American Football League (AFL) to form the AFC after the 1970 merger. While in Baltimore, the team advanced to the playoffs ten times and won three NFL Championship games in 1958, 1959, and 1968. The Baltimore Colts played in two Super Bowl games, losing to the New York Jets in Super Bowl\u00a0III and defeating the Dallas Cowboys in Super Bowl\u00a0V. The Colts moved to Indianapolis in 1984 and have since appeared in the playoffs sixteen times, won two conference championships, and played in two Super Bowl games; they defeated the Chicago Bears in Super Bowl\u00a0XLI, and lost to the New Orleans Saints in Super Bowl\u00a0XLIV (all four Super Bowls that the Colts have played in took place at the home stadium for the Miami Dolphins; while based in Baltimore, Super Bowls\u00a0III and V were held at the Orange Bowl in Miami, and while based in Indianapolis, Super Bowls\u00a0XLI and XLIV were at what is now Hard Rock Stadium in Miami Gardens).\nHistory.\nBaltimore Colts.\nThe All America Football Conference began play in the 1946 season. In its second year, the franchise assigned to the Miami Seahawks was moved to Maryland's major commercial and manufacturing city of Baltimore. After a fan contest, the team was renamed the Baltimore Colts after the city's history of horse breeding and racing. The team used silver and green as its colors. The Colts played for the next three seasons in the old AAFC until they agreed to merge with the old National Football League (of 1920\u20131922 to 1950) when the NFL was reorganized. The Baltimore Colts were one of the three former AAFC powerhouse teams to merge with the NFL at that time, the others being the San Francisco 49ers and the Cleveland Browns. This Colts team, now in the \"big league\" of professional American football for the first time, although with shaky financing and ownership, played only in the 1950 season of the NFL, and was later disbanded.\nCarroll Rosenbloom era (1953\u20131971).\nIn 1953, a new Baltimore-based group, heavily supported by the city's municipal government and with a large subscription-base of fan-purchased season tickets, led by local owner Carroll Rosenbloom won the rights to a new Baltimore NFL franchise.\nRosenbloom was awarded the remains of the former Dallas Texans team, who themselves had a long and winding history, with a small part of the franchise starting as the Boston Yanks in 1944, merging later with the Brooklyn Tigers, a franchise that had a far more deep and rich history, being previously known as the Dayton Triangles, one of the original old NFL teams established even before the League itself, in 1913. That team later became the New York Yanks in 1950, and many of the players from the New York Yankees of the former competing All-America Football Conference (1946\u201349) were added to the team to begin playing in the newly merged League for the 1950 season. The Yanks then moved to Dallas in Texas after the 1951 season having competed for two seasons, but played their final two \"home\" games of the 1952 season as a so-called \"road team\" at the Rubber Bowl football stadium in Akron, Ohio.\nThe NFL considers the Texans and Colts to be separate teams, although many of the earlier teams shared the same colors of blue and white. Thus, the Indianapolis Colts are legally considered to be a 1953 expansion team.\nWeeb Ewbank years (1954\u20131962).\nThe current version of the Colts football team played their first season in Baltimore in 1953, where the team compiled a 3\u20139 record under first-year head coach Keith Molesworth. The franchise struggled during the first few years in Baltimore, with the team not achieving their first winning record until the 1957 season.\nNFL champions (1958\u20131959).\nHowever, under head coach Weeb Ewbank and the leadership of quarterback Johnny Unitas, the Colts went on to a 9\u20133 record during the 1958 season and reached the NFL Championship Game for the first time by winning the NFL Western Conference. The Colts faced the New\u00a0York Giants in the 1958 NFL Championship Game, which is considered to be among the greatest contests in professional football history. The Colts defeated the Giants 23\u201317 in the first game to use the overtime rule, a game seen by 45\u00a0million persons.\nDuring the 1959 season, the team posted a 9\u20133 record and once again defeated the Giants in the NFL Championship Game to claim their second title.\nDon Shula years (1963\u20131969).\nThe Colts did not return to the NFL Championship for four seasons. In 1963, head coach Ewbank was replaced with the young Don Shula . In Shula's second season, the Colts compiled a 12\u20132 record, but lost to the Cleveland Browns in the NFL Championship 27\u20130.\nNFL champions (1968).\nIn 1968, the Colts, under Unitas and Shula, won their third NFL Championship but lost in Super Bowl\u00a0III.\nAfter the Colts trounced the Cleveland Browns 34\u20130 in the NFL Championship, many judged them among the \"greatest pro football teams of all time\" In Super Bowl III, they were favored by 18\u00a0points against the New York Jets, their counterparts from the American Football League. Instead, the Colts were stunned by the Jets, led by Joe Namath and Matt Snell under head coach Weeb Ewbank, who had previously won two NFL Championships with the Colts. Many in the sports media were surprised by the Jets' 16\u20137 victory, in the first Super Bowl win for the young AFL.\nDon McCafferty years (1970\u20131972).\nRosenbloom of the Colts, Art Modell of the Browns, and Art Rooney of the Pittsburgh Steelers agreed to have their teams join the ten AFL teams in the American Football Conference as part of the AFL\u2013NFL merger in 1970.\nSuper Bowl V champions (1970).\nThe Colts immediately went on a rampage in the new league, as new head coach Don McCafferty led the 1970 team to an 11\u20132\u20131 regular-season record, winning the AFC East title. In the first round of the NFL Playoffs, the Colts beat the Cincinnati Bengals 17\u20130; one week later in the first-ever AFC Championship Game, they beat the Oakland Raiders 27\u201317. Baltimore went on to win the first post-merger Super Bowl (Super Bowl\u00a0V), defeating the National Football Conference's Dallas Cowboys 16\u201313 on a Jim O'Brien field goal with five seconds left to play. The victory gave the Colts their fourth NFL championship and first Super Bowl victory. After the championship, the Colts returned to the playoffs in 1971 and defeated the Cleveland Browns in the first round, but lost to the Miami Dolphins in the AFC Championship.\nRobert Irsay era (1971\u20131996).\nCiting friction with the City of Baltimore and the local press, Rosenbloom traded the Colts franchise to Robert Irsay on July\u00a013, 1972, and received the Los\u00a0Angeles Rams in return. Under the new ownership, the Colts did not reach the postseason for three consecutive seasons after 1971, and after the 1972 season, starting quarterback and legend Johnny Unitas was traded to the San\u00a0Diego Chargers. After Unitas left, the Colts made the playoffs three consecutive seasons from 1975 to 1977, losing in the divisional round each time. The Colts' 1977 playoff loss in double overtime against the Oakland Raiders was the last playoff game for the team in Baltimore; it is known for the Ghost to the Post play. These consecutive championship teams featured 1976 NFL Most Valuable Player Bert Jones at quarterback and an outstanding defensive line, nicknamed the \"Sack Pack\".\nThe team endured nine consecutive losing seasons beginning in 1978. In 1981, the Colts defense allowed an NFL-record 533 points, set an all-time record for fewest sacks (13), and also set a modern record for fewest punt returns (12). The following year, the offense collapsed, including a game against the Buffalo Bills where the Colts' offense did not cross mid-field the entire game. The Colts finished 0\u20138\u20131 in the strike-shortened 1982 season, thereby earning the right to select Stanford quarterback John Elway with the first overall pick. Elway, however, refused to play for Baltimore, and using leverage as a draftee of the New\u00a0York Yankees baseball club, forced a trade to Denver. Behind an improved defense the team finished 7\u20139 in 1983, their last season in Baltimore.\nMove to Indianapolis.\nThe Baltimore Colts played their final home game in Baltimore on December\u00a018, 1983, against the Houston Oilers. Irsay continued to request upgrades to Memorial Stadium or construction of a new stadium. As a result of the poor performance on the field and the stadium issues, fan attendance and team revenue continued to dwindle. City officials were precluded from using tax-payer funds for the building of a new stadium, and the modest proposals that were offered by the city were not acceptable to either the Colts or the city's MLB franchise, the Orioles. However, all sides continued to negotiate. Relations between Irsay and the city of Baltimore deteriorated. Although Irsay assured fans that his ultimate desire was to stay in Baltimore, he nevertheless began discussions with several other cities willing to build new football stadiums, eventually narrowing the list of cities to Indianapolis and Phoenix. Under the administration of mayors Richard Lugar and then William Hudnut, Indianapolis had undertaken an ambitious effort to reinvent itself into a 'Great American City'. The Hoosier Dome, which was later renamed the RCA Dome, had been built specifically for, and was ready to host, an NFL expansion team.\nMeanwhile, in Baltimore, the situation worsened. The Maryland General Assembly intervened when a bill was introduced to give the city of Baltimore the right to seize ownership of the team by eminent domain. As a result, Irsay began serious negotiations with Hudnut to move the team before the Maryland legislature could pass the law. Indianapolis offered loans as well as the Hoosier Dome and a training complex. After the deal was reached, moving vans from Indianapolis-based Mayflower Transit were dispatched overnight to the team's Maryland training complex, arriving on the morning of March\u00a029, 1984. Once in Maryland, workers loaded all of the team's belongings, and by midday the trucks departed for Indianapolis, leaving nothing of the Colts organization that could be seized by Baltimore. The Baltimore Colts' Marching Band had to scramble to retrieve their equipment and uniforms before they were shipped to Indianapolis as well.\nThe move triggered a flurry of legal activity that ended when representatives of the city of Baltimore and the Colts organization reached a settlement in March 1986. Under the agreement, all lawsuits regarding the move were dismissed, and the Colts agreed to endorse a new NFL team for Baltimore.\nUpon the Colts' arrival in Indianapolis over 143,000 requests for season tickets were received in just two weeks. The Colts did not change their name despite Indianapolis' lack of history of horse breeding and racing. The move did not change the recent fortune of the Colts, with the team appearing in the postseason only once in the first 11\u00a0seasons in Indianapolis. During the 1984 season, the first in Indianapolis, the team went 4\u201312 and accounted for the lowest offensive yardage in the league. The 1985 and 1986 teams combined for only eight wins, including an 0\u201313 start in 1986 which prompted the firing of head coach Rod Dowhower, who was replaced by Ron Meyer. The Colts, however, did receive eventual Hall of Fame running back Eric Dickerson as a result of a trade during the 1987 season, and went on to compile a 9\u20136 record, thereby winning the AFC East and advancing to the postseason for the first time in Indianapolis; they lost that game to the Cleveland Browns.\nAfter 1987, the Colts did not see any real success for quite some time, with the team missing the postseason for seven consecutive seasons. The struggles came to a climax in 1991 when the team went 1\u201315 and was just one point away from the first all-losing season in the history of a 16-game schedule. The season resulted in the firing of head coach Ron Meyer and the return of former head coach Ted Marchibroda to the organization in 1992; he had coached the team from 1975 to 1979. The team continued to struggle under Marchibroda and Jim Irsay, son of Robert Irsay and general manager at the time. It was in 1994 that Robert Irsay brought in Bill Tobin to become the general manager of the Indianapolis Colts.\nUnder Tobin, the Colts drafted running back Marshall Faulk with the second overall pick in the 1994 NFL draft and acquired quarterback Jim Harbaugh as well. These Colts began to turn their fortunes around with playoff appearances in 1995 and 1996. The Colts won their first postseason game as the Indianapolis Colts in 1995 and advanced to the AFC Championship Game against the Pittsburgh Steelers, coming just a Hail Mary pass reception away from a trip to Super Bowl\u00a0XXX.\nMarchibroda retired after the 1995 season and was replaced by Lindy Infante in 1996. In the 1996 season, the Colts went 9\u20137 and had their season end in the Wild Card Round with a 42\u201314 loss to the PIttsburgh Steelers. After two consecutive playoff appearances, the Colts regressed and went 3\u201313 during the 1997 season.\nJim Irsay era (1997\u20132025).\nAlong with the disappointing season, the principal owner and man who moved the team to Indianapolis, Robert Irsay, died in January 1997 after years of declining health. Jim Irsay, Robert Irsay's son, entered the role of principal owner after his father's death and quickly began to change the organization. Irsay replaced general manager Tobin with Bill Polian in 1997 as the team decided to build through their number one overall pick in the 1998 draft.\nJim Mora years (1998\u20132001).\nJim Irsay began to shape the Colts one year after assuming control from his father by firing head coach Lindy Infante and hiring Bill Polian as the general manager of the organization. Polian in turn hired Jim E. Mora to become the next head coach of the team and drafted Tennessee Volunteer quarterback Peyton Manning, the son of New\u00a0Orleans Saints legend Archie Manning, with the first overall pick in the 1998 NFL draft.\nThe team and Manning struggled during the 1998 season, winning only three games; Manning threw a league high 28 interceptions. However, Manning did pass for 3,739\u00a0yards and threw 26 touchdown passes and was named to the NFL All-Rookie First Team. The Colts began to improve towards the end of the 1998 season and showed continued growth in 1999. Indianapolis drafted Edgerrin James in 1999 and continued to improve their roster heading into the upcoming season. The Colts went 13\u20133 in 1999 and finished first in the AFC East, their first division title since 1987. Indianapolis lost to the eventual AFC champion Tennessee Titans in the divisional playoffs.\nThe 2000 and 2001 Colts teams were considerably less successful compared to the 1999 team. The 2000 team went 10\u20136 and had their season end in the Wild Card Round with an overtime loss to the Miami Dolphins. Pressure began to mount on team administration and the coaching staff after a 6\u201310 season in 2001.\nTony Dungy years (2002\u20132008).\nMora was fired at the end of the season and was replaced by former Tampa Bay Buccaneers head coach Tony Dungy. Dungy and the team quickly changed the atmosphere of the organization and returned to the playoffs in 2002 with a 10\u20136 record, only for them to get shut out in the 2002 Wild Card Round to the New York Jets. The Colts also returned to the playoffs in 2003 and 2004 with 12\u20134 records and AFC South championships. The Colts lost to the New England Patriots and Tom Brady in the 2003 AFC Championship Game and in the 2004 divisional playoffs, thereby beginning a rivalry between the two teams, and between Manning and Brady. After two consecutive playoff losses to the Patriots, the Colts began the 2005 season with a 13\u20130 record, including a regular season victory over the Patriots, the first in the Manning era. During the season, Manning and Marvin Harrison broke the NFL record for touchdowns by a quarterback and receiver tandem. Indianapolis finished the 2005 season with a 14\u20132 record, the best record in the league that year and the best in a 16 games season for the franchise, but lost to the Pittsburgh Steelers in the divisional round.\nSuper Bowl XLI champions (2006).\nIndianapolis entered the 2006 season with a veteran quarterback, receivers, and defenders, and chose running back Joseph Addai in the 2006 draft. As in the previous season, the Colts began the season undefeated and went 9\u20130 before losing their first game against the Dallas Cowboys. Indianapolis finished the season with a 12\u20134 record and entered the playoffs for the fifth consecutive year, this time as the number three seed in the AFC. The Colts won their first two playoff games against the Kansas City Chiefs and the Baltimore Ravens to return to the AFC Championship Game for the first time since the 2003 playoffs, where they faced their rivals, the New England Patriots. In a classic game, the Colts overcame a 21\u20133 first-half deficit to win the game 38\u201334 and earned a trip to Super Bowl\u00a0XLI, the franchise's first Super Bowl appearance since 1970 and for the first based in Indianapolis. The Colts faced the Chicago Bears in the Super Bowl, winning the game 29\u201317 and giving Manning, Polian, Irsay, and Dungy, as well as the city of Indianapolis, their first Super Bowl title.\nThe Colts compiled a 13\u20133 record during the 2007 season; they lost to the San\u00a0Diego Chargers in the divisional playoffs, in what was the final game the Colts played at the RCA Dome before moving into Lucas Oil Stadium in 2008. The 2008 season began with Manning being sidelined for most of the pre-season due to surgery. Indianapolis began the season with a 3\u20134 record, but then won nine consecutive games to end the season at 12\u20134 and make in into the playoffs as a wild card team, eventually losing to the Chargers in the wild card round. After the season, Tony Dungy announced his retirement after seven seasons as head coach, having compiled an overall record of 92\u201333 with the team.&lt;ref name=\"Dungy Retires/Caldwell Hired\"&gt;&lt;/ref&gt;\nJim Caldwell years (2009\u20132011).\nJim Caldwell was hired as head coach of the team after Dungy, and led the team during the 2009 season. The Colts went 14\u20130 during the season to finish with a record of 14\u20132 after controversially benching their starters during the last two games. The Colts for the second time in the Manning era entered the playoffs with the best record in the AFC. The Colts managed victories over the Baltimore Ravens and New York Jets to advance to Super Bowl\u00a0XLIV against the New\u00a0Orleans Saints, but lost to the Saints 31\u201317.\nAt the completion of the 2009 season, the Colts had finished the first decade of the 2000s (2000\u20132009) with the most regular-season wins (115) and highest winning percentage (.719) of any team in the NFL during that span.\nThe 2010 team compiled a 10\u20136 record, the first time the Colts did not win 12 games since 2002, and lost to the New York Jets in the wild card round of the playoffs. The loss to the Jets was the last game for Peyton Manning as a Colt.\nAfter missing the preseason, Manning was ruled out for the Colts' opening game in Houston and eventually the entire 2011 season. Taking over as starter was veteran quarterback Kerry Collins, who had been signed to the team after dissatisfaction with backup quarterback Curtis Painter and Dan Orlovsky. However, even with a veteran quarterback, the Colts lost their first 13 games and finished the season with a 2\u201314 record, enough to receive the first overall pick in the 2012 draft. Immediately after the season, team president Bill Polian was fired, ending his 14-year tenure with the team. The change built the anticipation of the organization's decision regarding Manning's future with the team. The Peyton Manning era came to an end on March\u00a08, 2012, when Jim Irsay announced that Manning was being released from the roster after 13 seasons.\nChuck Pagano years (2012\u20132017).\nDuring the 2012 off-season owner Jim Irsay hired Ryan Grigson to be the General Manager. Grigson decided to let head coach Jim Caldwell go and Chuck Pagano was hired as the new head coach shortly thereafter. The Colts also began to release some higher paid and oft-injured veteran players, including Joseph Addai, Dallas Clark, and Gary Brackett. The Colts used their number one overall draft pick in 2012 to draft Stanford Cardinal quarterback Andrew Luck and also drafted his teammate Coby Fleener in the second round. The team also switched to a 3\u20134 defensive scheme.\nWith productive seasons from both Luck and veteran receiver Reggie Wayne, the Colts rebounded from the 2\u201314 season of 2011 with a 2012 season record of 11\u20135. The franchise, team, and fan base rallied behind head coach Chuck Pagano during his fight with leukemia. Clinching an unexpected playoff spot in the 2012\u201313 NFL playoffs, the 14th playoff berth for the club since 1995. The season ended in a 24\u20139 playoff loss to the eventual Super Bowl Champion Baltimore Ravens.\nTwo weeks into the 2013 season, the Colts traded their first-round selection in the 2014 NFL draft to the Cleveland Browns for running back Trent Richardson. In Week\u00a07, Luck led the Colts to a 39\u201333 win over his predecessor, Peyton Manning, and the undefeated Broncos. Luck went on to lead the Colts to a 15th division championship later that season. In the first round of the 2013 NFL playoffs, Andrew Luck led the Colts to a 45\u201344 victory over Kansas City, outscoring the Chiefs 35\u201313 in the second half in the second biggest comeback in NFL playoff history.\nDuring the 2014 season, Luck led the Colts to the AFC Championship game for the first time in his career after breaking the Colts' single-season passing yardage record previously held by Manning.\nAfter the Colts finished 8\u20138 in both the 2015 and 2016 seasons and missed the playoffs in back-to-back seasons for the first time since 1997\u201398, Grigson was fired as general manager. Just three of his previous 18 draft picks remained on the team at the time of his firing. On January\u00a030, 2017, the team hired Chris Ballard, who served as the Kansas City Chiefs Director of Football Operations, to replace Grigson.\nOn December 31, 2017, after winning the final game of the season and a final record of 4\u201312, the Colts parted ways with Pagano. Luck, who had suffered multiple injuries and missed nine games during the 2015 season, sat out the entire 2017 season recovering from shoulder surgery.\nIn the weeks following the end of the 2017 season, after two interviews, it was widely reported that the Colts would hire Josh McDaniels, offensive coordinator of the New England Patriots, to replace Pagano, after McDaniels fulfilled his obligations to the Patriots in Super Bowl\u00a0LII. On February\u00a08, 2018, the Colts announced McDaniels as their new head coach. Hours later, however, McDaniels rescinded his decision to be the head coach, and he returned to the Patriots.\nFrank Reich years (2018\u20132022).\nOn February 11, 2018, the Colts announced Frank Reich, then offensive coordinator of the Philadelphia Eagles, as their new head coach. In Reich's first season as head coach, Andrew Luck's return to the field got off to a shaky start, as the Colts began the 2018 season 1\u20135. However, they surged back to win nine of their last ten games to secure a 10\u20136 record and a playoff berth. They won a wild card game against their division rival Houston Texans before falling to the Kansas City Chiefs in the divisional round. Luck, benefiting from the Colts' best offensive line of his career, was named the 2018 Comeback Player of the Year.\nColts General Manager Chris Ballard achieved a historic feat in 2018 when two players he had drafted that year, guard Quenton Nelson and linebacker Shaquille Leonard were both named First-Team All-Pro. This was the first time two rookies from the same team received that honor since Hall-of-Famers Dick Butkus and Gale Sayers achieved the feat in 1965.\nOn August\u00a024, 2019, Luck informed the Colts that he would be retiring from the NFL after not attending training camp. He cited an unfulfilling cycle of injury and rehab as his primary reason for leaving football.\nOn November\u00a017, 2019, the Colts defeated the Jacksonville Jaguars for the team's 300th win in the Indianapolis era, with a record of 300\u2013267. Despite a promising 5\u20132 start and strong seasons from Leonard, Nelson, and newly acquired defensive end Justin Houston, the Colts struggled in the second half of the 2019 season with new starting quarterback Jacoby Brissett at the helm and finished the year with a 7\u20139 record.\nOn March\u00a017, 2020, the Colts signed longtime Los\u00a0Angeles Chargers quarterback and eight-time Pro Bowler Philip Rivers to a one-year deal worth $25\u00a0million. Rivers led the Colts to an 11\u20135 record and a playoff berth, where they then lost to the Buffalo Bills in the Wild Card Round of the NFL's first expanded playoffs.\nOn March\u00a017, 2021, the Colts traded a 2021 third-round pick and a 2022 second-round conditional pick for former Eagles quarterback Carson Wentz. Despite an All-Pro season from running back Jonathan Taylor, the Colts finished the season 9\u20138 after an upset loss to the Jacksonville Jaguars that eliminated the Colts from playoff contention. The Colts then traded Wentz and a second round pick to the Washington Commanders in exchange for three draft picks.\nOn March\u00a021, 2022, the Colts traded a 2022 third-round pick for longtime Atlanta Falcons quarterback Matt Ryan. After playing seven games in which he threw for nine touchdowns and nine interceptions, while also fumbling 11 times, Ryan was benched for the remainder of the season in favor of Sam Ehlinger.\nOn November\u00a07, 2022, the Colts fired Reich as head coach the day after losing by 23\u00a0points to the New\u00a0England Patriots to continue a disappointing 3\u20135\u20131 start. Longtime Colts center Jeff Saturday was subsequently named the interim head coach. Under Saturday, the Colts went 1\u20137, and overall, the Colts finished the 2022 season with a record of 4\u201312\u20131, their lowest win total since 2017.\nShane Steichen years (2023\u2013present).\nComing off their lowest win total since 2017, the Colts decided not to retain interim head coach Jeff Saturday and on February\u00a014, 2023, they hired Shane Steichen as their new head coach. Later in the offseason, the Colts released quarterbacks Matt Ryan and Nick Foles. The Colts would go on to select Florida Gators quarterback Anthony Richardson with the fourth pick in the 2023 NFL draft. In free agency, the Colts signed quarterback Gardner Minshew and kicker Matt Gay.\nOn October\u00a018, 2023, the Colts announced that Richardson would miss the remainder of the season with a Grade\u00a03 AC joint sprain. On October\u00a024, 2023, he successfully underwent shoulder surgery to repair the sprain. Backup quarterback Minshew was named by Steichen as the starter during Richardson's absence. During the 2023 NFL season, wide receiver Michael Pittman Jr. broke the Colts franchise record for the most receptions in the first four years of a player's career. Despite many injuries, including to Richardson and All-Pro running back Jonathan Taylor, the 2023 Colts rebounded from their 4\u201312\u20131 record in 2022, finishing 9\u20138 and narrowly missing the playoffs with a loss to the Houston Texans in the season finale. In 2024, On April\u00a025, 2024, the Colts selected UCLA defensive end Laiatu Latu with the 15th pick in the 2024 NFL draft. In 2024, the Colts failed to improve on their 9\u20138 record and were eliminated from the playoffs by a week 17 loss to the New York Giants, finishing 8\u20139.\nCarlie Irsay-Gordon era (2025\u2013present).\nOn May 21, 2025, Colts owner and CEO Jim Irsay died at the age of 65. On June 9, 2025, the team announced that Irsay's three daughters would inherit an equal ownership stake and would assume new roles within the organization. Irsay's oldest daughter, Carlie Irsay-Gordon, was named principal owner and CEO with middle daughter Casey Foyt named executive vice president and youngest daughter Kalen Jackson named chief brand officer and president of the Indianapolis Colts Foundation.\nLogos and uniforms.\nThe Colts' helmets in 1953 were white with a blue stripe. In 1954\u201355 they were blue with a white stripe and a pair of horseshoes at the rear of the helmet. For 1956, the colors were reversed, white helmet, blue stripe and horseshoes at the rear. In 1957, the horseshoes moved to their current location, one on each side of the helmet.\nThe blue jerseys have white shoulder stripes and the white jerseys have blue stripes. The team also wears white pants with blue stripes down the sides. Both designs originally had sleeve stripes, but by 1957, the uniforms changed to its current form, which evolved as materials changed.\nFor much of the team's history, the Colts wore blue socks, accenting them with two or three white stripes for much of their history in Baltimore and during the 2004 and 2005 seasons. From 1982 to 1987, the blue socks featured gray stripes. For a period lasting 1955 to 1958 and again from 1988 to 1992, the Colts wore white socks with either two or three blue stripes.\nFrom 1982 through 1986, the Colts wore gray pants with their blue jerseys. The gray pants featured a horseshoe on the top of the sides with the player's number inside the horseshoe. The Colts continued to wear white pants with their white jerseys throughout this period, and in 1987, the gray pants were retired.\nThe Colts wore blue pants with their white jerseys for the first three games of the 1995 season (pairing them with white socks), but then returned to white pants with both the blue and white jerseys. The team made some minor uniform adjustments before the start of the 2004 season, including reverting from blue to the traditional gray face masks, darkening their blue colors from a royal blue to speed blue, as well as adding two white stripes to the socks. In 2006, the stripes were removed from the socks.\nIn 2002, the Colts made a minor striping pattern change on their jerseys, having the stripes only on top of the shoulders then stop completely. Previously, the stripes used to go around to underneath the jersey sleeves. This was done because the Colts, like many other football teams, were beginning to manufacture the jerseys to be tighter to reduce holding calls and reduce the size of the sleeves. Although the white jerseys of the Minnesota Vikings at the time also had a similar striping pattern and continued as such (as well as the throwbacks the New\u00a0England Patriots wore in the Thanksgiving game against the Detroit Lions in 2002, though the Patriots later wore the same throwbacks in 2009 with truncated stripes and in 2010 became their official alternate uniform), the Colts and most college teams with this striping pattern did not make this adjustment.\nIn 2017, the Colts brought back the blue pants but paired them with the blue jerseys as part of the NFL Color Rush program.\nThe club revealed an updated wordmark logo, as well as updated numeral fonts, on April\u00a013, 2020. While blue and white remained the team's core colors, they added black as a tertiary color, with its usage restricted to the embroidered Nike swoosh on the white uniforms. Despite the wordmark change, the previous wordmarks were still painted on the Lucas Oil Stadium end zones until 2024, when the Colts unveiled a new turf surface.\nOn July\u00a020, 2023, the Colts unveiled a new alternate uniform, including a black alternate helmet. The jersey remained blue, but added black trim to the numbers and moved the white sleeve stripes to the shoulders. The \"Indiana\u00a0C\" alternate logo was placed on the left shoulder. Blue pants with white stripes are paired with this uniform. In a first for the franchise, the Colts would wear black helmets with the uniform, maintaining almost the same look as the primary white helmet save for the increased usage of black.\nFacilities.\nAfter 24 years of playing at the RCA Dome, the Colts moved to their new home Lucas Oil Stadium in the late 2008. In December 2004, the City of Indianapolis and Jim Irsay agreed to a new stadium deal at an estimated cost of $1\u00a0billion (including the Indiana Convention Center upgrades). In a deal estimated at $122\u00a0million, Lucas Oil Products won the naming rights to the stadium for 20\u00a0years.\nLucas Oil Stadium is a seven-level stadium that seats 63,000 for football. It can be reconfigured to seat 70,000 or more for NCAA basketball and football and concerts. It covers . The stadium features a retractable roof allowing the Colts to play home games outdoors for the first time since arriving in Indianapolis. Using FieldTurf, the playing surface is roughly below ground level. In addition to being larger than the RCA Dome, the new stadium features: 58 permanent concession stands, 90 portable concession stands, 13 escalators, 11 passenger elevators, 800 restrooms, HD video displays from Daktronics and replay monitors and 142 luxury suites. The stadium also features a retractable roof, with electrification technology developed by VAHLE, Inc. Other than being the home of the Colts, the stadium will host games in both the Men's and Women's NCAA basketball tournaments and will serve as the backup host for all NCAA Final Four Tournaments. The stadium hosted the Super Bowl for the 2011 season (Super Bowl XLVI). Lucas Oil Stadium has also hosted the Drum Corps International World Championships since 2009.\nRivalries.\nDivisional.\nHouston Texans.\nIn one of the newer rivalries in the NFL, the Colts and Houston Texans have intensified their animosity in recent years. Despite Indianapolis dominating the AFC South and this particular series under quarterback Peyton Manning in the 2000s, Houston has recently provided more competition, winning the division five times since 2011. As of the 2023 season, Indianapolis currently leads the series 33\u201311\u20131, including a win in the postseason in 2018.\nJacksonville Jaguars.\nThe Colts and Jacksonville Jaguars emerged as divisional rivals in the NFL when they were assigned to the AFC South division. Historically, the Colts have had the upper hand in this rivalry, particularly during the Peyton Manning era. Although the Jaguars have struggled to maintain a consistently competitive roster, they have managed to achieve significant upsets against the Colts and have recorded more victories against them than against the Titans and Texans. The 2020s have marked a period of increased competitiveness for both teams.\nTennessee Titans.\nThe Colts and Titans, the oldest rivalry in the AFC South, have been competing against each other since the 1970 season, originally as the Baltimore Colts and the Houston Oilers. They became divisional rivals in the 2002 season, leading to numerous contests for the AFC South title, with the Titans occasionally managing to wrest the title from the Colts. In recent years, however, the Colts have largely controlled the rivalry, thanks in part to the exceptional performances of quarterbacks Peyton Manning and Andrew Luck, with Luck finishing his career undefeated against the Titans, going 11\u20130. Nonetheless, the 2020s have seen a resurgence in competitiveness from both teams as they vie for the AFC South title and a playoff berth.\nAs of the 2023 season, the Colts lead the overall series, 37\u201322. The two teams have met once in the playoffs, with the Titans winning 19\u201316 in the .\nConference.\nBaltimore Ravens.\nAfter the Colts left for Indianapolis, Baltimore got a new NFL team called the Baltimore Ravens, and a rivalry somewhat developed much like the one between the Houston Texans and Tennessee Titans. The Colts and Ravens have met in the playoffs three times, with the Colts leading 2\u20131.\nNew England Patriots.\nThe rivalry between the Indianapolis Colts and New England Patriots is one of the NFL's newest rivalries. The rivalry is fueled by the quarterback comparison between Peyton Manning and Tom Brady during the 2000s. The Patriots owned the beginning of the series, defeating the Colts in six consecutive contests including the 2003 AFC Championship game and a 2004 AFC Divisional game. The Colts won the next three matches, notching two regular-season victories and a win in the 2006 AFC Championship game on the way to their win in Super Bowl XLI. On November 4, 2007, the Patriots defeated the Colts 24\u201320; in the next matchup on November 2, 2008, the Colts won 18\u201315 in a game that was one of the reasons the Patriots failed to make the playoffs; in the 2009 meeting, the Colts staged a spirited comeback to beat the Patriots 35\u201334; in 2010 the Colts almost staged another comeback, pulling within 31\u201328 after trailing 31\u201314 in the fourth quarter, but fell short due to a Patriots interception of a Manning pass late in the game; it turned out to be Manning's final meeting against the Patriots as a member of the Colts. After a dismal 2011 season that included a 31\u201324 loss to the Patriots, the Colts drafted Andrew Luck and in November of 2012 the two teams met with identical 6\u20133 records; the Patriots erased a 14\u20137 gap to win 59\u201324. The nature of this rivalry is ironic because the Colts and Patriots were division rivals from 1970 to 2001, but it did not become prominent in league circles until after Indianapolis was moved to the AFC South. On November 16, 2014, the New England Patriots traveled at 7\u20132 to play the 6\u20133 Colts at Lucas Oil Stadium. After a stellar four-touchdown performance by New England running back Jonas Gray, the Patriots defeated the Colts 42\u201320. The Patriots followed with a 45\u20137 defeat of the Colts in the 2014 AFC Championship Game. As of the 2023 season, the Patriots lead the all-time series 53\u201331.\nHistoric.\nIn the years 1953\u201366, the Colts played in the NFL Western Conference (also known as division), but did not have significant rivalries with other franchises in that alignment, as they were the easternmost team and the rest of the division included the Great Lakes franchises Green Bay, Detroit Lions, Chicago Bears, and after 1961, the Minnesota Vikings, along with the league's two West Coast teams in San Francisco and Los Angeles. The closest team to Baltimore was the Washington Redskins, but they were not in the same division and not very competitive during most years at that time.\nNew York Giants.\nIn 1958, Baltimore played its first NFL Championship Game against the 10\u20133 New York Giants. The Giants qualified for the championship after a tie-breaking playoff against the Cleveland Browns. Having already been defeated by the Giants in the regular season, Baltimore was not favored to win, yet proceeded to take the title in sudden death overtime. The Colts then repeated the feat by posting an identical record and routing the Giants in the 1959 final. Up until the Colts' back-to-back titles, the Giants had been the premier club in the NFL, and continued to be post-season stalwarts the next decade, losing three straight finals. The situation was reversed by the end of the decade, with Baltimore winning the 1968 NFL title and New York compiling less impressive results. In recent years, the Colts and Giants featured brothers as their starting quarterbacks (Peyton and Eli Manning respectively), leading to their occasional match-up being referred to as the \"Manning Bowl\". As of the 2023 season, the Colts lead the all-time series 12\u20137.\nNew York Jets.\nSuper Bowl III became the most famous upset in professional sports history as the American Football League's New York Jets won 16\u20137 over the overwhelmingly favored Colts. With the merger of the AFL and NFL the Colts and Jets were placed in the new AFC East. The two teams met twice a year (interrupted in 1982 by a player strike) 1970\u20132001; with the move of the Colts to the AFC South the two teams' rivalry actually escalated, as they met three times in the playoffs in the South's first nine seasons of existence; the Jets crushed the Colts 41\u20130 in the 2002 Wild Card playoff round; the Colts then defeated the Jets 30\u201317 in the 2009 AFC Championship Game; but the next year in the wild-card round the Jets pulled off another playoff upset of the Colts, winning 17\u201316; it was Peyton Manning's final game with the Colts. The Jets defeated the Colts 35\u20139 in 2012 in Andrew Luck's debut season; after two straight losses Luck led a 45\u201310 rout of the Jets in 2016.\nJoe Namath and Johnny Unitas were the focal point of the rivalry at its beginning, but they did not meet for a full game until September 24, 1972. Namath erupted with six touchdowns and 496 passing yards despite only 28 throws and 15 completions. Unitas threw for 376\u00a0yards and two scores but was sacked six times as the Jets won 44\u201334; the game was considered one of the top ten passing duels in NFL history. As of the 2023 season, the Colts lead the all-time series 44\u201332.\nPlayers of note.\nRing of Honor.\nThe Ring of Honor was established on September 23, 1996. There have been 20 inductees.\nStatistics and records.\nSeason-by-season record.\nThis is a partial list of the Colts' last five completed seasons. For the full season-by-season franchise results, see List of Indianapolis Colts seasons.\n\"Note: The finish, wins, losses, and ties columns list regular season results and exclude any postseason play.\"\nRadio and television coverage.\nThe Colts' flagship radio stations since 2007 are WFNI (1070 AM, currently silent but with its repeater signals at 93.5 FM and 107.5 FM continuing to function as \"93.5/107.5 The Fan\" using WIBC-HD2 as a signal source) and WLHK 97.1 FM. The 1070 AM frequency, then known as WIBC, had also been the flagship from 1984 to 1992 and from 1995 to 1997.\nMatt Taylor is the team's play-by-play announcer, succeeding Bob Lamey in 2018. Lamey held the job from 1984 to 1991 and again from 1995 to 2018. Former Colts backup quarterback Jim Sorgi serves as the \"color commentator\". Mike Jansen serves as the public address announcer at all Colts home games. Jansen has been the public address announcer since the 1998 season.\nThe team's local TV carriage rights were shaken up in mid-2014 when WTTV's owner Tribune Media came to terms with CBS to become the network's Indianapolis affiliate as of January 1, 2015, replacing WISH-TV. With the deal, both Tribune Media stations, including WXIN (channel 59) carry the bulk of the team's regular-season games starting with the 2015 NFL season. Also as of the 2015 season, WTTV and WXIN became the official Colts stations and air the team's preseason games, along with official team programming and coach's shows, and have a signage presence along the fascia of Lucas Oil Stadium.\nWISH's sister station WNDY-TV aired preseason games from 2011 to 2014, having replaced WTTV at that time.\nRadio station affiliates.\nStations that broadcast Colts games include:\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15051", "revid": "6326132", "url": "https://en.wikipedia.org/wiki?curid=15051", "title": "Immigration to the United States", "text": "Immigration has been a major source of population growth and cultural change in the United States throughout much of its history. As of January 2025, the United States has the largest immigrant population in the world in absolute terms, with 53.3 million foreign-born residents, representing 15.8% of the total U.S. population\u2014both record highs. While the United States represented about 4% of the total global population in 2024, 17% of all international migrants resided in the United States. In March 2025, the Federation for American Immigration Reform (FAIR) estimated that approximately 18.6 million illegal immigrants resided in the United States. In 2024, immigrants and their U.S.-born children number more than 93 million people, or 28% of the total U.S. population.According to the 2016 Yearbook of Immigration Statistics, the United States admitted a total of 1.18\u00a0million legal immigrants (618k new arrivals, 565k status adjustments) in 2016. Of these, 48% were the immediate relatives of United States citizens, 20% were family-sponsored, 13% were refugees or asylum seekers, 12% were employment-based preferences, 4.2% were part of the Diversity Immigrant Visa program, 1.4% were victims of a crime (U1) or their family members were (U2 to U5), and 1.0% who were granted the Special Immigrant Visa (SIV) for Iraqis and Afghans employed by the United States Government. The remaining 0.4% included small numbers from several other categories, including 0.2% who were granted suspension of deportation as an immediate relative of a citizen (Z13); persons admitted under the Nicaraguan and Central American Relief Act; children born after the issuance of a parent's visa; and certain parolees from the former Soviet Union, Cambodia, Laos, and Vietnam who were denied refugee status.\nBetween 1921 and 1965 policies such as the National Origins Formula limited immigration and naturalization opportunities for people from areas outside Northwestern Europe. Exclusion laws enacted as early as the 1880s generally prohibited or severely restricted immigration from Asia, and quota laws enacted in the 1920s curtailed Southern and Eastern European immigration. The civil rights movement led to the replacement of these ethnic quotas with per-country limits for family-sponsored and employment-based preference visas. Between 1970 and 2007, the number of first-generation immigrants living in the United States quadrupled from 9.6\u00a0million to 38.1\u00a0million residents. Census estimates show 45.3\u00a0million foreign born residents in the United States as of March 2018 and 45.4\u00a0million in September 2021, the lowest three-year increase in decades.\nIn 2017, out of the U.S. foreign-born population, some 45% (20.7\u00a0million) were naturalized citizens, 27% (12.3\u00a0million) were lawful permanent residents, 6% (2.2\u00a0million) were temporary lawful residents, and 23% (10.5\u00a0million) were unauthorized immigrants. The United States led the world in refugee resettlement for decades, admitting more refugees than the rest of the world combined.\nCauses of migration include poverty, crime and environmental degradation.\nSome research suggests that immigration is beneficial to the United States economy. With few exceptions, the evidence suggests that on average, immigration has positive economic effects on the native population, but it is mixed as to whether low-skilled immigration adversely affects low-skilled natives. Studies also show that immigrants have lower crime rates than natives in the United States. The economic, social, and political aspects of immigration have caused controversy regarding such issues as maintaining ethnic homogeneity, workers for employers versus jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior.\nHistory.\nDue to its history the United States can be described as an immigration country. American immigration history can be viewed in four epochs: the colonial period, the mid-19th century, the start of the 20th century, and post-1965. Each period brought distinct national groups, races, and ethnicities to the United States.\nColonial period.\nDuring the 17th century, approximately 400,000 English people migrated to America under European colonization. They comprised 83.5% of the white population at the time of the first census in 1790. From 1700 to 1775, between 350,000 and 500,000 Europeans immigrated: estimates vary in sources. Regarding English settlers of the 18th century, one source says 52,000 English migrated during the period of 1701 to 1775, although this figure is likely too low. 400,000\u2013450,000 of the 18th-century migrants were Scots, Scots-Irish from Ulster, Germans, Swiss, and French Huguenots. Over half of all European immigrants to Colonial America during the 17th and 18th centuries arrived as indentured servants. They numbered 350,000. From 1770 to 1775 (the latter year being when the American Revolutionary War began), 7,000 English, 15,000 Scots, 13,200 Scots-Irish, 5,200 Germans, and 3,900 Irish Catholics migrated to the Thirteen Colonies. According to Butler (2000), up to half of English migrants in the 18th century may have been young, single men who were well-skilled, trained artisans, like the Huguenots. Based on scholarly analysis, \"English\" was the largest single ancestry in all U.S. states at the time of the first census in 1790, ranging from a high of 82% in Massachusetts to a low of 35.3% in Pennsylvania, where Germans accounted for 33.3%.\nOrigins of immigrant stock in 1790.\nThe Census Bureau published preliminary estimates of the origins of the colonial American population by scholarly classification of the names of all White heads of families recorded in the 1790 census in a 1909 report entitled \"A Century of Population Growth\". These initial estimates were scrutinized and rejected following passage of the Immigration Act of 1924, when the government required accurate official estimates of the origins of the colonial stock population as basis for computing National Origins Formula immigration quotas in the 1920s. In 1927, proposed quotas based on CPG figures were rejected by the President's Committee chaired by the Secretaries of State, Commerce, and Labor, with the President reporting to Congress \"the statistical and historical information available raises grave doubts as to the whole value of these computations as the basis for the purposes intended\". Concluding that CPG \"had not been accepted by scholars as better than a first approximation of the truth\", an extensive scientific revision was produced, in collaboration with the American Council of Learned Societies (ACLS), as basis for computing contemporary legal immigration quotas. For this task scholars estimated the proportion of names of unique derivation from each of the major national stocks present in the population as of the 1790 census. The final results, later also published in the journal of the American Historical Association, are presented below:\n Estimated Nationalities of the White American population in the Continental United States as of the 1790 Census\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nHistorians estimate that fewer than one million immigrants moved to the United States from Europe between 1600 and 1799. By comparison, in the first federal census, in 1790, the population of the United States was enumerated to be 3,929,214.\nThese statistics do not include the 17.8% of the population who were enslaved, according to the 1790 census.\nEarly United States era.\nThe Naturalization Act of 1790 limited naturalization to \"free white persons\"; it was expanded to include black people in the 1860s and Asian people in the 1950s. This made the United States an outlier, since laws that made racial distinctions were uncommon in the world in the 18th century.\nThe 1794 Jay Treaty provided freedom of movement for Americans, British subjects, and Native Americans into British and American jurisdictions, Hudson's Bay Company land excepted. The treaty is still in effect to the degree that it allows Native Americans born in Canada (subject to a blood quantum test) to enter the United States freely.\nIn the early years of the United States, immigration (not counting the enslaved, who were treated as merchandise rather than people) was fewer than 8,000 people a year, including French refugees from the slave revolt in Haiti. Legal importation of enslaved African was prohibited after 1808, though many were smuggled in to sell. After 1820, immigration gradually increased. From 1836 to 1914, over 30 million Europeans migrated to the United States.\nFirst U.S. laws restricting immigration.\nAfter an initial wave of immigration from China following the California Gold Rush, racist attitudes toward the Chinese population of the West Coast led to Congress passing the very first U.S. law restricting immigration: The Page Act of 1875 banned Chinese women who, it was claimed, were arriving to engage in prostitution. This was followed by the Chinese Exclusion Act of 1882, banning virtually all immigration from China until the law's repeal in 1943. In the late 1800s, immigration from other Asian countries, especially to the West Coast, became more common.\nExclusion Era.\nThe peak year of European immigration was in 1907, when 1,285,349 persons entered the country. By 1910, 13.5\u00a0million immigrants were living in the United States.\nWhile the Chinese Exclusion Act of 1882 had already excluded immigrants from China, the immigration of people from Asian countries in addition to China was banned by the Immigration Act of 1917, also known as the Asiatic Barred Zone Act, which also banned homosexuals, people with intellectual disability, and people with an anarchist worldview. The Emergency Quota Act was enacted in 1921, limiting immigration from the Eastern Hemisphere by national quotas equal to 3 percent of the number of foreign-born from each nation in the 1910 census. The Act aimed to further restrict immigrants from Southern and Eastern Europe, particularly Italian, Slavic, and Jewish people, who had begun to enter the country in large numbers beginning in the 1890s. The temporary quota system was superseded by the National Origins Formula of the Immigration Act of 1924, which computed national quotas as a fraction of 150,000 in proportion to the national origins of the entire White American population as of the 1920 census, except those having origins in the nonquota countries of the Western Hemisphere (which remained unrestricted).\nOrigins of immigrant stock in 1920.\nThe National Origins Formula was a unique computation which attempted to measure the total contributions of \"blood\" from each national origin as a share of the total stock of White Americans in 1920, counting immigrants, children of immigrants, and the grandchildren of immigrants (and later generations), in addition to estimating the colonial stock descended from the population who had immigrated in the colonial period and were enumerated in the 1790 census. European Americans remained predominant, although there were shifts toward Southern, Central, and Eastern Europe from immigration in the period 1790 to 1920. The formula determined that ancestry derived from Great Britain accounted for over 40% of the American gene pool, followed by German ancestry at 16%, then Irish ancestry at 11%. The restrictive immigration quota system established by the Immigration Act of 1924, revised and re-affirmed by the Immigration and Nationality Act of 1952, sought to preserve this demographic makeup of America by allotting quotas in proportion to how much blood each national origin had contributed to the total stock of the population in 1920, as presented below:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nImmigration patterns of the 1930s were affected by the Great Depression. In the final prosperous year, 1929, there were 279,678 immigrants recorded, but in 1933, only 23,068 moved to the U.S. In the early 1930s, more people emigrated from the United States than to it. The U.S. government sponsored a Mexican Repatriation program which was intended to encourage people to voluntarily move to Mexico, but thousands were deported against their will. Altogether, approximately 400,000 Mexicans were repatriated; half of them were US citizens. Most of the Jewish refugees fleeing the Nazis and World War II were barred from coming to the United States. In the post-war era, the Justice Department launched Operation Wetback, under which 1,075,168 Mexicans were deported in 1954.\nSince 1965.\nThe Immigration and Nationality Act of 1965, also known as the Hart\u2013Celler Act, abolished the system of national-origin quotas. By equalizing immigration policies, the act resulted in new immigration from non-European nations, which changed the ethnic demographics of the United States. In 1970, 60% of immigrants were from Europe; this decreased to 15% by 2000.\nThe U.S. Immigration and Naturalization Law of 1965 abolished the former quota system and gave preference to people with skills regarded as being \"especially advantageous\" to the United States, which resulted in an increase in immigration from Asia. In the 1980s, this accelerated as the Federal government of the United States encouraged the immigration of engineers, mathematicians, and scientists from Asia, particularly India and China, to help support STEM-related endeavors in the country. Skilled immigration from these countries was strengthened through the Immigration Act of 1990. The National Academy of Sciences has supported U.S. policymakers to design legislation that attracts foreign mathematicians, engineers and scientists to emigrate to the United States.\nIn 1986 president Ronald Reagan signed immigration reform that gave amnesty to 3 million undocumented immigrants in the country.\nIn 1990, George H. W. Bush signed the Immigration Act of 1990, which increased legal immigration to the United States by 40%. In 1991, Bush signed the Armed Forces Immigration Adjustment Act 1991, allowing foreign service members who had served 12 or more years in the US Armed Forces to qualify for permanent residency and, in some cases, citizenship.\nIn November 1994, California voters passed Proposition 187 amending the state constitution, denying state financial aid to illegal immigrants. The federal courts voided this change, ruling that it violated the federal constitution.\nAppointed by President Bill Clinton, the U.S. Commission on Immigration Reform recommended reducing legal immigration from about 800,000 people per year to approximately 550,000. While an influx of new residents from different cultures presents some challenges, \"the United States has always been energized by its immigrant populations\", said President Bill Clinton in 1998. \"America has constantly drawn strength and spirit from wave after wave of immigrants... They have proved to be the most restless, the most adventurous, the most innovative, the most industrious of people.\"\nIn 2001, President George W. Bush discussed an accord with Mexican President Vicente Fox. Due to the September 11 attacks, the possible accord did not occur. From 2005 to 2013, the US Congress discussed various ways of controlling immigration. The Senate and House were unable to reach an agreement.\nNearly 8 million people immigrated to the United States from 2000 to 2005; 3.7\u00a0million of them entered without papers. Hispanic immigrants suffered job losses during the late-2000s recession, but since the recession's end in June 2009, immigrants posted a net gain of 656,000 jobs.\nNearly 14\u00a0million immigrants entered the United States from 2000 to 2010, and over one million persons were naturalized as U.S. citizens in 2008. The per-country limit applies the same maximum on the number of visas to all countries regardless of their population and has therefore had the effect of significantly restricting immigration of persons born in populous nations such as Mexico, China, India, and the Philippines\u2014the leading countries of origin for legally admitted immigrants to the United States in 2013; nevertheless, China, India, and Mexico were the leading countries of origin for immigrants overall to the United States in 2013, regardless of legal status, according to a U.S. Census Bureau study.\nOver 1 million immigrants were granted legal residence in 2011.\nFor those who enter the US illegally across the Mexico\u2013United States border and elsewhere, migration is difficult, expensive and dangerous. Virtually all undocumented immigrants have no avenues for legal entry to the United States due to the restrictive legal limits on green cards, and lack of immigrant visas for low-skilled workers. Participants in debates on immigration in the early 21st century called for increasing enforcement of existing laws governing illegal immigration to the United States, building a barrier along some or all of the Mexico-U.S. border, or creating a new guest worker program. Through much of 2006 the country and Congress was engaged in a debate about these proposals. As of \u00a02010[ [update]] few of these proposals had become law, though a partial border fence had been approved and subsequently canceled.\nModern reform attempts.\nBeginning with Ronald Reagan in the 1980s, presidents from both political parties have steadily increased the number of border patrol agents and instituted harsher punitive measures for immigration violations. Examples of these policies include Ronald Reagan's Immigration Reform and Control Act of 1986 and the Clinton-era Prevention Through Deterrence strategy. The sociologist Douglas Massey has argued that these policies have succeeded at producing a perception of border enforcement but have largely failed at preventing emigration from Latin America. Notably, rather than curtailing illegal immigration, the increase in border patrol agents decreased circular migration across the U.S.\u2013Mexico border, thus increasing the population of Hispanics in the U.S.\nPresidents from both parties have employed anti-immigrant rhetoric to appeal to their political base or to garner bi-partisan support for their policies. While Republicans like Reagan and Donald Trump have led the way in framing Hispanic immigrants as criminals, Douglas Massey points out that \"the current moment of open racism and xenophobia could not have happened with Democratic acquiescence\". For example, while lobbying for his 1986 immigration bill, Reagan framed unauthorized immigration as a \"national security\" issue and warned that \"terrorists and subversives are just two days' driving time\" from the border. Later presidents, including Democrats Bill Clinton and Barack Obama, used similar \"security\" rhetoric in their efforts to court Republican support for comprehensive immigration reform. In his 2013 State of the Union Address, Obama said \"real reform means strong border security, and we can build on the progress my administration has already made\u00a0\u2013 putting more boots on the southern border than at any time in our history\".\nFirst Trump administration policies.\nICE reports that it removed 240,255 immigrants in fiscal year 2016, as well as 226,119 in FY2017 and 256,085 in FY2018. Citizens of Central American countries (including Mexico) made up over 90% of removals in FY2017 and over 80% in FY2018.\nIn January 2017, U.S. President Donald Trump signed an executive order temporarily suspending entry to the United States by nationals of seven Muslim-majority countries. It was replaced by another executive order in March 2017 and by a presidential proclamation in September 2017, with various changes to the list of countries and exemptions. The orders were temporarily suspended by federal courts but later allowed to proceed by the Supreme Court, pending a definite ruling on their legality. Another executive order called for the immediate construction of a wall across the U.S.\u2013Mexico border, the hiring of 5,000 new border patrol agents and 10,000 new immigration officers, and federal funding penalties for sanctuary cities.\nThe \"zero-tolerance\" policy was put in place in 2018, which legally allows children to be separated from adults unlawfully entering the United States. This is justified by labeling all adults that enter unlawfully as criminals, thus subjecting them to criminal prosecution. The Trump Administration also argued that its policy had precedent under the Obama Administration, which had opened family detention centers in response to migrants increasingly using children as a way to get adults into the country. However, the Obama Administration detained families together in administrative, rather than criminal, detention.\nOther policies focused on what it means for an asylum seeker to claim credible fear. To further decrease the amount of asylum seekers into the United States, Attorney General Jeff Sessions released a decision that restricts those fleeing gang violence and domestic abuse as \"private crime\", therefore making their claims ineligible for asylum. These new policies that had been put in place were controversial for putting the lives of the asylum seekers at risk, to the point that the ACLU sued Jeff Sessions along with other members of the Trump Administration. The ACLU claimed that the policies put in place by the Trump Administration undermined the fundamental human rights of those immigrating into the United States, specifically women. They also claimed that these policies violated decades of settle asylum law.\nIn April 2020, President Trump said he will sign an executive order to temporarily suspend immigration to the United States because of the COVID-19 pandemic in the United States.\nBiden administration policies.\nIn January 2023, regarding the Mexico\u2013United States border crisis, Joe Biden announced a new immigration policy that would allow 30,000 migrants per month from Cuba, Haiti, Nicaragua and Venezuela but will also expel the migrants from those countries who violate US laws of immigration. The policy has faced criticism from \"immigration reform advocates and lawyers who decry any expansion of Title 42.\"\nOn October 31, 2023, Homeland Security Secretary Alejandro Mayorkas testified before the Senate Homeland Security Committee that more than 600,000 people illegally made their way into the United States without being apprehended by border agents during the 2023 fiscal year.\nIn fiscal year 2022, over one million immigrants (most of whom entered through family reunification) were granted legal residence, up from 707,000 in 2020.\nBorder Security and Asylum Reform in the Emergency National Security Supplemental Appropriations Act, 2024.\nThe 2024 Emergency National Security Supplemental Appropriations Act represents a change in the immigration system with a focus on strengthening border security and improving asylum processes. This bill, backed by both Republican senators and endorsed by President Biden seeks to address the surge in border crossings in the U.S. Mexico border by revolutionizing how migrants and asylum seekers are processed by border authorities. More specifically, asylum officers to consider certain bars to asylum during screening interviews, which were previously only considered by immigration judges. The legislation aims to streamline provisions for effective management.\nThe proposed law introduces an asylum procedure in the U.S. Border, where asylum officers from the U.S. Citizenship and Immigration Services (USCIS) can review asylum applications at a more rapid pace. This new process, called removal proceedings, is detailed in a new section of the Immigration and Nationality Act (INA) specifically Section 235B. The bill sets a bar for passing an asylum screening by requiring a \"reasonable possibility\" standard instead of the previous \"credible fear\" standard. Requiring more evidence at the preliminary screening stages at the same level needed for a full hearing. Notably excluded apprehended individuals between ports of entry from asylum eligibility except under narrow exceptions. This adjustment makes it more difficult for asylum seekers to qualify for a hearing in front of an immigration judge and has raised questions in regards to potential violations against the right to seek asylum and due process.\nFurthermore, the legislation establishes an emergency expulsion authority that empowers the branch to expel migrants and asylum seekers during times of \" extraordinary migration circumstances.\" When the seven-day average of encounters between ports of entry exceeds 2,500, the restrictions come into effect. The restrictions continue until the average falls below 1,500 for 14 consecutive days. If this occurs the DHS Secretary can promptly send migrants back to their home country unless they can prove they face a risk of persecution or torture.\nThe proposed legislation involves around $18.3\u00a0billion in funding for the Department of Homeland Security (DHS) to carry out the border policies and changes in the asylum process. Moreover, it designates $2.3\u00a0billion to support arrived refugees through the \"Refugee and Entrant Assistance\" program. The program itself is designed to fund a broad range of social services to newly arrived refugees, both through states and direct service grants. The bill outlines provisions for granting status to allies safeguarding most \"Documented Dreamers \" and issuing an additional 250,000 immigrant visas. It introduces a program for repatriation enabling asylum seekers to go to their home countries at any point during the proceedings. The proposed legislation also contains clauses that do not affect the humanitarian parole initiatives of the Biden administration, for individuals from Venezuela, Cuba, Haiti and Nicaragua. These individuals are granted approval to travel and a temporary period of parole in the United States.\nOrigins of the U.S. immigrant population, 1960\u20132016.\nNote: \"Other Latin America\" includes Central America, South America and the Caribbean.\nAccording to the Department of State, in the 2016 fiscal year 84,988 refugees were accepted into the US from around the world. In the fiscal year of 2017, 53,691 refugees were accepted to the US. There was a significant decrease after Trump took office; it continued in the fiscal year of 2018 when only 22,405 refugees were accepted into the US. This displays a massive drop in acceptance of refugees since the Trump Administration has been in place.\nOn September 26, 2019, the Trump administration announced that it planned to allow only 18,000 refugees to resettle in the United States in the 2020 fiscal year, its lowest level since the modern program began in 1980.\nIn 2020 the Trump administration announced that it planned to slash refugee admissions to U.S. for 2021 to a record low of 15,000 refugees down from a cap of 18,000 for 2020, making 2021 the fourth consecutive year of declining refugee admissions under the Trump term.\nThe Biden administration pledged to welcome 125,000 refugees in 2024.\nContemporary immigration.\nAs of 2018[ [update]], approximately half of immigrants living in the United States are from Mexico and other Latin American countries. \nUntil the 1930s most legal immigrants were male. By the 1990s women accounted for just over half of all legal immigrants. Contemporary immigrants tend to be younger than the native population of the United States, with people between the ages of 15 and 34 substantially overrepresented. Immigrants are also more likely to be married and less likely to be divorced than native-born Americans of the same age.\nImmigrants are likely to move to and live in areas populated by people with similar backgrounds. This phenomenon has remained true throughout the history of immigration to the United States. Seven out of ten immigrants surveyed by Public Agenda in 2009 said they intended to make the U.S. their permanent home, and 71% said if they could do it over again they would still come to the US. In the same study, 76% of immigrants say the government has become stricter on enforcing immigration laws since the September 11 attacks (\"9/11\"), and 24% report that they personally have experienced some or a great deal of discrimination.\nPublic attitudes about immigration in the U.S. were heavily influenced in the aftermath of the 9/11 attacks. After the attacks, 52% of Americans believed that immigration was a good thing overall for the U.S., down from 62% the year before, according to a 2009 Gallup poll. A 2008 Public Agenda survey found that half of Americans said tighter controls on immigration would do \"a great deal\" to enhance U.S. national security. Harvard political scientist and historian Samuel P. Huntington argued in his 2004 book \"Who Are We? The Challenges to America's National Identity\" that a potential future consequence of continuing massive immigration from Latin America, especially Mexico, could lead to the bifurcation of the United States.\nThe estimated population of illegal Mexican immigrants in the US decreased from approximately 7 million in 2007 to 6.1\u00a0million in 2011 Commentators link the reversal of the immigration trend to the economic downturn that started in 2008 and which meant fewer available jobs, and to the introduction of tough immigration laws in many states. According to the Pew Hispanic Center, the net immigration of Mexican born persons had stagnated in 2010, and tended toward going into negative figures. \nThe share of international job seekers looking to work in the U.S. declined sharply in 2025 as per a report from \"Indeed\". The slowing labor market and stricter immigration policy beginning with the Biden administration and accelerating under President Trump has led to further cooling demand for American jobs.\nMore than 80 cities in the United States, including Washington D.C., New York City, Los Angeles, Chicago, San Francisco, San Diego, San Jose, Salt Lake City, Phoenix, Dallas, Fort Worth, Houston, Detroit, Jersey City, Minneapolis, Denver, Baltimore, Seattle, Portland, Oregon and Portland, Maine, have sanctuary policies, which vary locally.\nCauses of migration.\n\"The current debate... is almost totally about what to do about immigrants when they get here. But the 800-pound gorilla that's missing from the table is what we have been doing there that brings them here, that drives them here\", according to Jeff Faux, an economist who is a distinguished fellow at the Economic Policy Institute.\nMany Central Americans are fleeing because of desperate social and economic circumstances in their countries. Some believe that the large number of Central American refugees arriving in the United States can be explained as a \"blowback\" to policies such as United States military interventions and covert operations that installed or maintained in power authoritarian leaders allied with wealthy land owners and multinational corporations who stop family farming and democratic efforts, which have caused drastically sharp social inequality, wide-scale poverty and rampant crime. Economic austerity dictated by neoliberal policies imposed by the International Monetary Fund and its ally, the U.S., has also been cited as a driver of the dire social and economic conditions, as has the U.S. \"war on drugs\", which has been understood as fueling murderous gang violence in the region. \nAnother major migration driver from Central America (Guatemala, Honduras, and El Salvador) are crop failures, which are (partly) caused by climate change. \nHalf of the people which are migrating from Latin America to the United States are from the Dry Corridor (El Corredor Seco) - a climatic zone that stretches through Guatemala, El Salvador, and Honduras suffering from different climate change impacts resulting in an flow of climate migrants to the United States. In 2007-2017 alone, immigration from the region to the US grew by a quart, and the trend is expected to continue, as far as climate change will continue to make the region drier and hotter. Climate change is crucial to understand migration from there as it have an impact on a variety of socioeconomic \"push factors\". Changes in climate patterns are part of the causes which make Mexicanes migrate as within so outside the country. In 2008-2021, 2.2 million Mexicans were displaced for reasons linked with climate change.\nOrigin countries.\nSource: US Department of Homeland Security, Office of Immigration Statistics\nCharts.\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\nDemography.\nExtent and destinations.\n&lt;includeonly&gt;&lt;templatestyles src=\"Chart/styles.css\"/&gt;&lt;/includeonly&gt;\nThe United States admitted more legal immigrants from 1991 to 2000, between ten and eleven million, than in any previous decade. In the most recent decade, the 10 million legal immigrants that settled in the U.S. represent roughly one third of the annual growth, as the U.S. population increased by 32\u00a0million (from 249\u00a0million to 281\u00a0million). By comparison, the highest previous decade was the 1900s, when 8.8\u00a0million people arrived, increasing the total U.S. population by one percent every year. Specifically, \"nearly 15% of Americans were foreign-born in 1910, while in 1999, only about 10% were foreign-born\".\nBy 1970, immigrants accounted for 4.7 percent of the US population and rising to 6.2 percent in 1980, with an estimated 12.5 percent in 2009. As of 2010[ [update]], 25% of US residents under age 18 were first- or second-generation immigrants. Eight percent of all babies born in the U.S. in 2008 belonged to illegal immigrant parents, according to a recent analysis of U.S. Census Bureau data by the Pew Hispanic Center.\nLegal immigration to the U.S. increased from 250,000 in the 1930s, to 2.5\u00a0million in the 1950s, to 4.5\u00a0million in the 1970s, and to 7.3\u00a0million in the 1980s, before becoming stable at about 10 million in the 1990s. Since 2000, legal immigrants to the United States number approximately 1,000,000 per year, of whom about 600,000 are \"Change of Status\" who already are in the U.S. Legal immigrants to the United States now are at their highest level ever, at just over 37,000,000 legal immigrants. In reports in 2005\u20132006, estimates of illegal immigration ranged from 700,000 to 1,500,000 per year. Immigration led to a 57.4% increase in foreign-born population from 1990 to 2000.\nForeign-born immigration has caused the U.S. population to continue its rapid increase with the foreign-born population doubling from almost 20 million in 1990 to over 47\u00a0million in 2015. In 2018, there were almost 90 million immigrants and U.S.-born children of immigrants (second-generation Americans) in the United States, accounting for 28% of the overall U.S. population.\nWhile immigration has increased drastically over the 20th century, the foreign-born share of the population is, at 13.4, only somewhat below what it was at its peak in 1910 at 14.7%. A number of factors may be attributed to the decrease in the representation of foreign-born residents in the United States. Most significant has been the change in the composition of immigrants; prior to 1890, 82% of immigrants came from North and Western Europe. From 1891 to 1920, that number decreased to 25%, with a rise in immigrants from East, Central, and South Europe, summing up to 64%. Animosity towards these ethnically different immigrants increased in the United States, resulting in much legislation to limit immigration in the 20th century.\nOrigin.\n&lt;templatestyles src=\"template:row hover highlight/styles.css\"/&gt;\nForeign-born population in the United States in 2019 by country of birth\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nViews of immigrants.\nReligion.\nIn the 2023-2024 Religious Landscape Survey carried out by Pew Research Center among those born outside the United States 58% identified as Christian, 14% as being from other religions and 26% said they were unaffiliated.\nEffects of immigration.\nImmigration to the United States significantly increases the population. The Census Bureau estimates that the US population will increase from 317 million in 2014 to 417\u00a0million in 2060 with immigration, when nearly 20% will be foreign-born. In particular, the population of Hispanic and Asian Americans is significantly increased by immigration, with both populations expected to see major growth. Overall, the Pew Report predicts the population of the United States will rise from 296 million in 2005 to 441\u00a0million in 2065, but only to 338\u00a0million with no immigration. The prevalence of immigrant segregation has brought into question the accuracy of describing the United States as a melting pot. Immigration to the United States has also increased religious diversity, with Islam, Hinduism, Buddhism, and Sikhism growing in the United States due to immigration. Changing demographics as a result of immigration have affected political affiliations. Immigrants are more likely than natives to support the Democratic Party. Interest groups that lobby for and against immigration play a role in immigration policy, with religious, ethnic, and business groups most likely to lobby on issues of immigration.\nImmigrants have not been found to increase crime in the United States, and immigrants overall are associated with lower crime rates than natives. Some research even suggests that increases in immigration may partly explain the reduction in the U.S. crime rate. According to one study, sanctuary cities\u2014which adopt policies designed to not prosecute people solely for being an illegal immigrant\u2014have no statistically meaningful effect on crime. Research suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of immigrants among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for immigrants. Crimmigration has emerged as a field in which critical immigration scholars conceptualize the current immigration law enforcement system.\nIncreased immigration to the United States has historically caused discrimination and racial unrest. Areas with higher minority populations may be subject to increased policing and harsher sentencing. Faculty in educational facilities have been found to be more responsive toward white students, though affirmative action policies may cause colleges to favor minority applicants. Evidence also shows the existence of racial discrimination in the housing market and the labor market. Discrimination also exists between different immigrant groups. According to a 2018 study of longitudinal earnings, most immigrants economically assimilate into the United States within a span of 20 years, matching the economic situations of non-immigrants of similar race and ethnicity.\nImmigration has been found to have little impact on the health of natives. Researchers have also found what is known as the \"healthy immigrant effect\", in which immigrants in general tend to be healthier than individuals born in the U.S. However, some illnesses are believed to have been introduced to the United States or caused to increase by immigration. Immigrants are more likely than native-born Americans to have a medical visit labeled uncompensated care.\nA significant proportion of American scientists and engineers are immigrants. Graduate students are more likely to be immigrants than undergraduate students, as immigrants often complete undergraduate training in their native country before immigrating. 33% of all U.S. PhDs in science and engineering were awarded to foreign-born graduate students as of 2004.\nEconomic impact.\nHigh-skilled immigration and low-skilled immigration have both been found to make economic conditions better for the average immigrant and the average American. The overall impact of immigration on the economy tends to be minimal. Research suggests that diversity has a net positive effect on productivity and economic prosperity. Contributions by immigrants through taxation and the economy have been found to exceed the cost of services they use. Overall immigration has not had much effect on native wage inequality but low-skill immigration has been linked to greater income inequality in the native population. Labor unions have historically opposed immigration over economic concerns.\nImmigrants have also been found to raise economic productivity, as they are more likely to take jobs that natives are unwilling to do. Research indicates that immigrants are more likely to work in risky jobs than U.S.-born workers, partly due to differences in average characteristics, such as immigrants' lower English language ability and educational attainment. Refugees have been found to integrate more slowly into the labor market than other immigrants, but they have also been found to increase government revenue overall. Immigration has also been correlated with increased innovation and entrepreneurship, and immigrants are more likely to start businesses than people born in The United States.\nUndocumented immigrants have also been found to have a positive effect on economic conditions in the United States. According to NPR in 2005, about 3% of illegal immigrants were working in agriculture, and the H-2A visa allows U.S. employers to bring foreign nationals to the United States to fill temporary agricultural jobs. States that imposed harsher immigration laws were found to suffer significant economic losses.\nIn May 2024, research conducted at Federal Reserve Bank of Kansas City suggested that immigration to the United States surged during 2022\u20132023 and the inflow of migrants to the country acted as a powerful catalyst in cooling overheated labor markets and tempering wage growth across industries and states. The study showed that from Dec 2021 to Dec 2023 there existed a negative correlation between an industry's migrant employment growth and wage growth: an increase in migrant employment growth of 1 percentage point lead to a 0.7 percentage point reduction in wage growth. It was found that an increase in employment growth, stemming from migrant workers, of 1 percentage point lead to a 0.5 percentage point reduction in job vacancy rates.\nPublic opinion.\nThe largely ambivalent feeling of Americans toward immigrants is shown by a positive attitude toward groups that have been visible for a century or more, and much more negative attitude toward recent arrivals. For example, a 1982 national poll by the Roper Center at the University of Connecticut showed respondents a card listing a number of groups and asked, \"Thinking both of what they have contributed to this country and have gotten from this country, for each one tell me whether you think, on balance, they've been a good or a bad thing for this country\", which produced the results shown in the table. \"By high margins, Americans are telling pollsters it was a very good thing that Poles, Italians, and Jews immigrated to America. Once again, it's the newcomers who are viewed with suspicion. This time, it's the Mexicans, the Filipinos, and the people from the Caribbean who make Americans nervous.\"\nIn a 2002 study, which took place soon after the September 11 attacks, 55% of Americans favored decreasing legal immigration, 27% favored keeping it at the same level, and 15% favored increasing it.\nIn 2006, the immigration-reduction advocacy think tank the Center for Immigration Studies released a poll that found that 68% of Americans think U.S. immigration levels are too high, and just 2% said they are too low. They also found that 70% said they are less likely to vote for candidates that favor increasing legal immigration. In 2004, 55% of Americans believed legal immigration should remain at the current level or increased and 41% said it should be decreased. The less contact a native-born American has with immigrants, the more likely they would have a negative view of immigrants.\nOne of the most important factors regarding public opinion about immigration is the level of unemployment; anti-immigrant sentiment is where unemployment is highest, and vice versa.\nSurveys indicate that the U.S. public consistently makes a sharp distinction between legal and illegal immigration, and generally views those perceived as \"playing by the rules\" with more sympathy than immigrants who have entered the country illegally.\nAccording to a Gallup poll in July 2015, immigration is the fourth-most important problem facing the United States and seven percent of Americans said it was the most important problem facing America today. In March 2015, another Gallup poll provided insight into American public opinion on immigration; the poll revealed that 39% of people worried about immigration \"a great deal\". A January poll showed that only 33% of Americans were satisfied with the current state of immigration in America.\nBefore 2012, a majority of Americans supported securing United States borders compared to dealing with illegal immigration in the United States. In 2013, that trend has reversed and 55% of people polled by Gallup revealed that they would choose \"developing a plan to deal with immigrants who are currently in the U.S. illegally\". Changes regarding border control are consistent across party lines, with the percentage of Republicans saying that \"securing U.S. borders to halt flow of illegal immigrants\" is extremely important decreasing from 68% in 2011 to 56% in 2014. Meanwhile, Democrats who chose extremely important shifted from 42% in 2011 to 31% in 2014. In July 2013, 87% of Americans said they would vote in support of a law that would \"allow immigrants already in the country to become U.S. citizens if they meet certain requirements including paying taxes, having a criminal background check and learning English\". However, in the same survey, 83% also said they would support the tightening of U.S. border security.\nDonald Trump's campaign for presidency focused on a rhetoric of reducing illegal immigration and toughening border security. In July 2015, 48% of Americans thought that Donald Trump would do a poor job of handling immigration problems. In November 2016, 55% of Trump's voters thought that he would do the right thing regarding illegal immigration. In general, Trump supporters are not united upon how to handle immigration. In December 2016, Trump voters were polled and 60% said that \"undocumented immigrants in the U.S. who meet certain requirements should be allowed to stay legally\". After Trump claimed during his 2024 presidential campaign that immigrants are \"poisoning the blood of our country\", a Public Religion Research Institute survey showed that 34% of Americans agreed, and 35% agreed that \"immigrants are invading our country and replacing our cultural and ethnic background\".\nAmerican opinion regarding how immigrants affect the country and how the government should respond to illegal immigration have changed over time. In 2006, out of all U.S. adults surveyed, 28% declared that they believed the growing number of immigrants helped American workers and 55% believed that it hurt American workers. In 2016, those views had changed, with 42% believing that they helped and 45% believing that they hurt. The PRRI 2015 American Values Atlas showed that between 46% and 53% of Americans believed that \"the growing number of newcomers from other countries\u00a0... strengthens American society\". In the same year, between 57% and 66% of Americans chose that the U.S. should \"allow [immigrants living in the U.S. illegally] a way to become citizens provided they meet certain requirements\".\nIn February 2017, the American Enterprise Institute released a report on recent surveys about immigration issues. In July 2016, 63% of Americans favored the temporary bans of immigrants from areas with high levels of terrorism and 53% said the U.S. should allow fewer refugees to enter the country. In November 2016, 55% of Americans were opposed to building a border wall with Mexico. Since 1994, Pew Research center has tracked a change from 63% of Americans saying that immigrants are a burden on the country to 27%.\nThe Trump administration's zero-tolerance policy was reacted to negatively by the public. One of the main concerns was how detained children of illegal immigrants were treated. Due to very poor conditions, a campaign was begun called \"Close the Camps\". Detainment facilities were compared to concentration and internment camps.\nAfter the 2021 evacuation from Afghanistan in August 2021, an NPR/Ipsos poll (\u00b14.6%) found 69% of Americans supported resettling in the United States Afghans who had worked with the U.S., with 65% support for Afghans who \"fear repression or persecution from the Taliban\". There was lower support for other refugees: 59% for those \"fleeing from civil strife and violence in Africa\", 56% for those \"fleeing from violence in Syria and Libya\", and 56% for \"Central Americans fleeing violence and poverty\". 57% supported the Trump-era Remain in Mexico policy, and 55% supported legalizing the status of those illegally brought to the U.S. as children (as proposed in the DREAM Act).\nReligious responses.\nReligious figures in the United States have stated their views on the topic of immigration as informed by their religious traditions.\nLegal issues.\nLaws concerning immigration and naturalization include the Immigration Act of 1990 (IMMACT), the Antiterrorism and Effective Death Penalty Act (AEDPA), the Illegal Immigration Reform and Immigrant Responsibility Act (IIRIRA), the Naturalization Act of 1790, the Alien and Sedition Acts of 1798, the Chinese Exclusion Act of 1882, and the Johnson-Reed Act of 1924. AEDPA and IIRARA exemplify many categories of criminal activity for which immigrants, including green card holders, can be deported and have imposed mandatory detention for certain types of cases. The Johnson-Reed Act limited the number of immigrants and the Chinese Exclusion Act banned immigration from China altogether.\nRefugees are able to gain legal status in the United States through asylum, and a specified number of legally defined refugees, who either apply for asylum overseas or after arriving in the U.S., are admitted annually. In 2014, the number of asylum seekers accepted into the U.S. was about 120,000. By comparison, about 31,000 were accepted in the UK and 13,500 in Canada. Asylum offices in the United States receive more applications for asylum than they can process every month and every year, and these continuous applications cause a significant backlog.\nRemoval proceedings are considered administrative proceedings under the authority of the United States Attorney General, and thus part of the executive branch rather than the judicial branch of government. in removal proceedings in front of an immigration judge, cancellation of removal is a form of relief that is available for some long-time residents of the United States. Eligibility may depend on time spent in the United States, criminal record, or family in the country. Members of Congress may submit private bills granting residency to specific named individuals. The United States allows immigrant relatives of active duty military personnel to reside in the United States through a green card.\nAs of 2015, there are estimated to be 11 to 12 million unauthorized immigrants in the United States, making up about 5% of the civilian labor force. Under the Deferred Action for Childhood Arrivals (DACA) program, unauthorized immigrants that arrived as children were granted exemptions to immigration law. According to an August 2025 Pew Research Center report, the unauthorized immigrant population grew by 3.5 million between 2021 and 2023, reaching a record 14 million. \nMost immigration proceedings are civil matters, though criminal charges are applicable when evading border enforcement, committing fraud to gain entry, or committing identity theft to gain employment. Due process protections under the Fifth Amendment to the United States Constitution have been found to apply to immigration proceedings, but those of the Sixth Amendment to the United States Constitution have not due to their nature as civil matters.\nIn 2021 a new system establishes by The U.S. Citizenship Act, for responsibly manage and secure U.S. border's, for safety of families and communities, and better manage migration across the Hemisphere, sent by President Biden to U.S. Congress.\nIn Department of State v. Mu\u00f1oz, U.S. Supreme court decided that U.S. citizens do not have a fundamental liberty to admit their foreign spouses\nImmigration in popular culture.\nThe history of immigration to the United States is the history of the country itself, and the journey from beyond the sea is an element found in American folklore, appearing in many works, such as \"The Godfather\", \"Gangs of New York\", \"The Song of Myself\", Neil Diamond's \"America\", and the animated feature \"An American Tail\".\nFrom the 1880s to the 1910s, vaudeville dominated the popular image of immigrants, with very popular caricature portrayals of ethnic groups. The specific features of these caricatures became widely accepted as accurate portrayals.\nIn \"The Melting Pot\" (1908), playwright Israel Zangwill (1864\u20131926) explored issues that dominated Progressive Era debates about immigration policies. Zangwill's theme of the positive benefits of the American melting pot resonated widely in popular culture and literary and academic circles in the 20th century; his cultural symbolism\u00a0\u2013 in which he situated immigration issues\u00a0\u2013 likewise informed American cultural imagining of immigrants for decades, as exemplified by Hollywood films.\nThe popular culture's image of ethnic celebrities often includes stereotypes about immigrant groups. For example, Frank Sinatra's public image as a superstar contained important elements of the \"American Dream\" while simultaneously incorporating stereotypes about Italian Americans that were based in nativist and Progressive responses to immigration.\nThe process of assimilation has been a common theme of popular culture. For example, \"lace-curtain Irish\" refers to middle-class Irish Americans desiring assimilation into mainstream society in counterpoint to the older, more raffish \"shanty Irish\". The occasional malapropisms and social blunders of these upward mobiles were lampooned in vaudeville, popular song, and the comic strips of the day such as \"Bringing Up Father\", starring Maggie and Jiggs, which ran in daily newspapers for 87 years (1913 to 2000). In \"The Departed\" (2006), Staff Sergeant Dignam regularly points out the dichotomy between the lace-curtain Irish lifestyle Billy Costigan enjoyed with his mother, and the shanty Irish lifestyle of Costigan's father. Since the late 20th century popular culture has paid special attention to Mexican immigration; the film \"Spanglish\" (2004) tells of a friendship of a Mexican housemaid (played by Paz Vega) and her boss (played by Adam Sandler).\nImmigration in literature.\nNovelists and writers have captured much of the color and challenge in their immigrant lives through their writings.\nRegarding Irish women in the 19th century, there were numerous novels and short stories by Harvey O'Higgins, Peter McCorry, Bernard O'Reilly and Sarah Orne Jewett that emphasize emancipation from Old World controls, new opportunities and expansiveness of the immigrant experience.\nFears of population decline have at times fueled anti-emigration sentiment in foreign countries. Hladnik studies three popular novels of the late 19th century that warned Slovenes not to migrate to the dangerous new world of the United States. In India some politicians oppose emigration to the United States because of a supposed brain drain of highly qualified and educated Indian nationals.\nJewish American writer Anzia Yezierska wrote her novel \"Bread Givers\" (1925) to explore such themes as Russian-Jewish immigration in the early 20th century, the tension between Old and New World Yiddish culture, and women's experience of immigration. A well established author Yezierska focused on the Jewish struggle to escape the ghetto and enter middle- and upper-class America. In the novel, the heroine, Sara Smolinsky, escapes from New York City's \"down-town ghetto\" by breaking tradition. She quits her job at the family store and soon becomes engaged to a rich real-estate magnate. She graduates college and takes a high-prestige job teaching public school. Finally Sara restores her broken links to family and religion.\nThe Swedish author Vilhelm Moberg, in the mid-20th century, wrote a series of four novels describing one Swedish family's migration from Sm\u00e5land to Minnesota in the late 19th century, a destiny shared by almost one million people. The author emphasizes the authenticity of the experiences as depicted (although he did change names). These novels have been translated into English (\"The Emigrants\", 1951, \"Unto a Good Land\", 1954, \"The Settlers\", 1961, \"The Last Letter Home\", 1961). The musical \"Kristina fr\u00e5n Duvem\u00e5la\" by ex-ABBA members Bj\u00f6rn Ulvaeus and Benny Andersson is based on this story.\n\"The Immigrant\" is a musical by Steven Alper, Sarah Knapp, and Mark Harelik. The show is based on the story of Harelik's grandparents, Matleh and Haskell Harelik, who traveled to Galveston, Texas in 1909.\nDocumentary films.\nIn their documentary \"\", filmmakers Shari Robertson and Michael Camerini examine the American political system through the lens of immigration reform from 2001 to 2007. Since the debut of the first five films, the series has become an important resource for advocates, policy-makers and educators.\nThat film series premiered nearly a decade after the filmmakers' landmark documentary film \"Well-Founded Fear\" which provided a behind-the-scenes look at the process for seeking asylum in the United States. That film still marks the only time that a film crew was privy to the private proceedings at the U.S. Immigration and Naturalization Service (INS), where individual asylum officers ponder the often life-or-death fate of immigrants seeking asylum.\nThe documentary \"Trafficked with Mariana van Zeller\" argued that weapons smuggling from the United States contributed to insecurity in Latin America, itself triggering more migration to the United States.\nOverall approach to regulation.\nUniversity of North Carolina School of Law professor Hiroshi Motomura has identified three approaches the United States has taken to the legal status of immigrants in his book \"Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States\". The first, dominant in the 19th century, treated immigrants as in transition; in other words, as prospective citizens. As soon as people declared their intention to become citizens, they received multiple low-cost benefits, including the eligibility for free homesteads in the Homestead Act of 1862, and in many states, the right to vote. The goal was to make the country more attractive, so large numbers of farmers and skilled craftsmen would settle new lands.\nBy the 1880s, a second approach took over, treating newcomers as \"immigrants by contract\". An implicit deal existed where immigrants who were literate and could earn their own living were permitted in restricted numbers. Once in the United States, they would have limited legal rights, but were not allowed to vote until they became citizens, and would not be eligible for the New Deal government benefits available in the 1930s.\nThe third policy is \"immigration by affiliation\", originating in the later half of the 20th century, which Motomura argues is the treatment which depends on how deeply rooted people have become in the country. An immigrant who applies for citizenship as soon as permitted, has a long history of working in the United States, and has significant family ties, is more deeply affiliated and can expect better treatment.\nThe American Dream is the belief that through hard work and determination, any American can achieve a better life, usually in terms of financial prosperity and enhanced personal freedom of choice. According to historians, the rapid economic and industrial expansion of the U.S. is not simply a function of being a resource rich, hard working, and inventive country, but the belief that anybody could get a share of the country's wealth if he or she was willing to work hard. This dream has been a major factor in attracting immigrants to the United States.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15052", "revid": "6379643", "url": "https://en.wikipedia.org/wiki?curid=15052", "title": "Image and Scanner Interface Specification", "text": "Image and Scanner Interface Specification (ISIS) is an industry standard interface for image scanning technologies, developed by Pixel Translations in 1990 (which became EMC Corporation's Captiva Software and later acquired by OpenText).\nISIS is an open standard for scanner control and a complete image-processing framework. It is currently supported by a number of application and scanner vendors.\nFunctions.\nThe modular design allows the scanner to be accessed both directly or with built-in routines to handle most situations automatically. A message-based interface with tags is used so that features, operations, and formats not yet supported by ISIS can be added as desired without waiting for a new version of the specification.\nThe standard addresses all of the issues that an application using a scanner needs to be concerned with. Functions include but are not limited to selecting, installing, and configuring a new scanner; setting scanner-specific parameters; scanning, reading and writing files, and fast image scaling, rotating, displaying, and printing. Drivers have been written to dynamically process data for operations such as converting grayscale to binary image data.\nAn ISIS interface can run scanners at or above their rated speed by linking drivers together in a pipe so that data flows from a scanner driver to compression driver, to packaging driver, to a file, viewer, or printer in a continuous stream, usually without the need to buffer more than a small portion of the full image. As a result of using the piping method, each driver can be optimized to perform one function well. Drivers are typically small and modular in order to make it simple to add new functionality to an existing application.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15053", "revid": "30587355", "url": "https://en.wikipedia.org/wiki?curid=15053", "title": "Ivo Caprino", "text": "Norwegian film director (1920\u20132001)\nIvo Caprino (17 February 1920\u00a0\u2013 8 February 2001) was a Norwegian film director and writer, best known for his puppet films. His most noted film, \"Fl\u00e5klypa Grand Prix\" (Pinchcliffe Grand Prix), was made in 1975.\nEarly life.\nCaprino was born 17 February 1920 in Oslo, the son of Italian furniture designer Mario Caprino and the artist, Ingeborg \"Ingse\" Gude, who was a granddaughter of the painter Hans Gude.\nEarly career.\nIn the mid-1940s, Caprino helped his mother design puppets for a puppet theatre, which inspired him to try making a film using his mother's designs. Ivo used the surplus puppets as inspiration for his first animated film, \"Tim and T\u00f8ffe\" (1948), the result of their collaboration. The eight minute film, however, was not released until 1949. Several other films followed, including two 15-minute shorts that are still shown regularly in Norway, \"Veslefrikk med Fela\" (\"Little Freddy and his Fiddle\"), based on a Norwegian folk tale; and \"Karius og Baktus\", a story by Thorbj\u00f8rn Egner of two little trolls\u2014representing caries and bacterium\u2014living in a boy's teeth. Gude made the puppets for these films as well.\nWork with Ingse Caprino.\nFollowing the success of \"Tim og T\u00f8ffe\", Gude was involved in all of her son's films until 1963. Gude made some puppets for a production by Frithjof Tidemand-Johannessen. Caprino had set up a film studio in the manor house, and Gude started working full-time on new puppets, which often had luscious proportions. The film, \"Veslefrikk med Fela\", was awarded the best children's film at the 13th Venice International Film Festival in 1952. The commissioned production, \"Den standhaftige tinnsoldat\" (\"The Steadfast Tin Soldier\"), won several international awards.\nGude filled the role of cinematographer on the last film collaboration with her son. The puppet's voice role being played by Liv Str\u00f8msted. Gude died 9 December 1963 at Snar\u00f8ya. The production of puppets was afterwards taken over by her granddaughter Ivonne Caprino.\nInnovations.\nIn 1949, when making \"Tim og T\u00f8ffe\", Caprino patented a method for controlling the puppets' movements in real time. It was a mechanical keyboard connected to the puppets, allowing him to move even minor parts like the eyes. Caprino's films received good reviews, and he quickly became a celebrity in Norway. When he switched to traditional stop motion film-making, Caprino tried to maintain the impression that he was still using some kind of \"magic\" technology to make the puppets move, even though all his later films were made with traditional stop motion techniques. Another innovative method used by the team, was the use of condoms for the creation of the puppets' facial skin.\nIn addition to the short films, Caprino produced dozens of advertising films with puppets. In 1959, he directed a live action feature film, \"Ugler i Mosen\", which also contained stop motion sequences. He then made a feature film about Peter Christen Asbj\u00f8rnsen, who had travelled around Norway in the 19th century collecting traditional folk tales. The plan was to use live action for the sequences showing Asbj\u00f8rnsen, and then to realise the folk tales using stop motion. Unfortunately, Caprino was unable to secure funding for the project, so he ended up making the planned folk tale sequences as separate 16-minute puppet films, book-ended by live action sequences showing Asbj\u00f8rnsen.\n\"The Pinchcliffe Grand Prix\".\nIn 1970, Caprino and his small team of collaborators, started work on a 25-minute TV special, which eventually became \"The Pinchcliffe Grand Prix\". Based on a series of books by Norwegian cartoonist and author, Kjell Aukrust, it featured a group of eccentric characters all living in the small village of Pinchcliffe. The TV special was a collection of sketches based on Aukrust's books, with no real story line. After 1.5 years of work, it was decided that it didn't really work as a whole, so production on the TV special was stopped (except for some very short clips, no material from it has ever been seen by the public), and Caprino and Aukrust instead wrote a screenplay for a feature film using the characters and environments that had already been built.\nThe result was \"The Pinchcliffe Grand Prix\", which stars Theodore Rimspoke (No. Reodor Felgen) and his two assistants, Sonny Duckworth (No. Solan Gundersen), a cheerful and optimistic bird, and Lambert (No. Ludvig), a nervous, pessimistic and melancholic hedgehog. Theodore works as a bicycle repairman, though he spends most of his time inventing weird Rube Goldberg-like contraptions. One day, the trio discover that one of Theodore's former assistants, Rudolph Gore-Slimey (), has stolen his design for a race car engine, and has become a world champion Formula One driver. Sonny secures funding from an Arab oil sheik who happens to be vacationing in Pinchcliffe, and the trio then build a gigantic racing car, \"Il Tempo Gigante\"\u2014a fabulous construction with two engines, radar, and its own blood bank. Theodore then enters a race, and ends up winning, beating Gore-Slimey despite his attempts at sabotage.\nThe film was made in 3.5 years by a team of five people. Caprino directed and animated. Bjarne Sandemose (Caprino's principal collaborator throughout his career) built the sets and the cars, and was in charge of the technical side. Ingeborg Riiser modeled the puppets and Gerd Alfsen made the costumes and props. When it came out in 1975, The movie was a large success in Norway, selling one million tickets in its first year of release. It remains the biggest box office hit of all time in Norway. Caprino Studios claims it has sold 5.5 million tickets to date.\nThere is a rollercoaster replica of Il Tempo Gigante at Hunderfossen Familiepark.\nLater career.\nExcept for some TV work in the late 1970s, Caprino made no more puppet films, focusing instead on creating attractions for the \"Hunderfossen\" theme park outside Lillehammer based on his folk tale movies, and making tourist films using a custom built multi camera setup of his own design that shoots 280 degrees panorama movies.\nDeath and afterwards.\nCaprino was born and died in Oslo, but lived all of his life at Snar\u00f8ya, B\u00e6rum. He died in 2001 after having lived several years with cancer. Since Caprino's death, his son Remo has had moderate success developing a computer game based on \"Fl\u00e5klypa Grand Prix\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15054", "revid": "96647", "url": "https://en.wikipedia.org/wiki?curid=15054", "title": "Intel 80286", "text": "Microprocessor model\nThe Intel 80286 (also marketed as the iAPX 286 and often called Intel 286) is a 16-bit microprocessor that was introduced on February 1, 1982. It was the first 8086-based CPU with separate, non-multiplexed address and data buses and also the first with memory management and wide protection abilities. It had a data size of 16 bits, and had an address width of 24 bits, which could address up to 16MB of memory with a suitable operating system such as Windows compared to 1MB for the 8086. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186, it can correctly execute most software written for the earlier Intel 8086 and 8088 processors.\nThe 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s. In 1987, Intel shipped its five-millionth 80286 microprocessor.\nHistory and performance.\nIntel's first 80286 chips were specified for a maximum clockrate of 5, 6 or 8\u00a0MHz and later releases for 12.5\u00a0MHz. AMD and Harris later produced 16\u00a0MHz, 20\u00a0MHz and 25\u00a0MHz parts. Intel, Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery-powered devices. Intel's CMOS version of the 80286 was the 80C286.\nOn average, the 80286 was said to have a speed of about 0.21 instructions per clock on \"typical\" programs, although it could be significantly faster on optimized code and in tight loops, as many instructions could execute in 2 clock cycles each. The 6\u00a0MHz, 10\u00a0MHz, and 12\u00a0MHz models were reportedly measured to operate at 0.9\u00a0MIPS, 1.5\u00a0MIPS, and 2.66\u00a0MIPS respectively.\nThe later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating-system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones). This E-2 stepping part may have been available in later 1986.\nIntel second sourced this microprocessor to Fujitsu Limited in about 1985.\nVariants.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nArchitecture.\nIntel expected the 286 to be used primarily in industrial automation, transaction processing, and telecommunications, instead of in personal computers.\nThe CPU was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: the address unit, bus unit, instruction unit, and execution unit, organized into a loosely coupled (buffered) pipeline, just as in the 8086. It was produced in a 68-pin package, including PLCC (plastic leaded chip carrier), LCC (leadless chip carrier) and PGA (pin grid array) packages.\nThe performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e., a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements seven years later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses, but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286, while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.\nThe 80286 included, in addition to all of the 8086 instructions, all of the new instructions of the 80186: ENTER, LEAVE, BOUND, INS, OUTS, PUSHA, POPA, PUSH immediate, IMUL immediate, and immediate shifts and rotates. The 80286 also added new instructions for protected mode: ARPL, CLTS, LAR, LGDT, LIDT, LLDT, LMSW, LSL, LTR, SGDT, SIDT, SLDT, SMSW, STR, VERR, and VERW. Some of the instructions for protected mode can (or must) be used in real mode to set up and switch to protected mode, and a few (such as SMSW and LMSW) are useful for real mode itself.\nThe Intel 80286 had a 24-bit address bus and as such had a 16\u00a0MB physical address space, compared to the 1\u00a0MB address space of prior x86 processors. It was the first x86 processor to support virtual memory supporting up to 1\u00a0GB via segmentation. However, memory cost and the initial rarity of software using the memory above 1\u00a0MB meant that until late in its production, 80286 computers rarely shipped with more than 1\u00a0MB of RAM. Additionally, there was a performance penalty involved in accessing extended memory from real mode as noted below.\nFeatures.\nProtected mode.\nThe 286 was the first of the x86 CPU family to support \"protected virtual-address mode\", commonly called \"protected mode\". In addition, it was the first commercially available microprocessor with on-chip memory management unit (MMU) capabilities (systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller). This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market.\nSeveral additional instructions were introduced in the protected mode of 80286, which are helpful for multitasking operating systems.\nAnother important feature of 80286 is the prevention of unauthorized access. This is achieved by:\nIn 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers:\nBy design, the 286 could not revert from protected mode to the basic 8086-compatible \"real address mode\" (\"real mode\") without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry, as well as specialized code in the ROM BIOS and the 8042 keyboard microcontroller to enable software to cause the reset, allowing real-mode reentry while retaining active memory and returning control to the program that initiated the reset. (The BIOS is necessarily involved because it obtains control directly whenever the CPU resets.) Though it worked correctly, the method imposed a huge performance penalty.\nIn theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules (newly proposed with the introduction of the 80286) were followed; however, as many DOS programs did not conform to those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily and to provide an emulation of real mode within protected mode. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode; essentially, in protected mode the 80286 was designed to be a new processor with many similarities to its predecessors, while real mode on the 80286 was offered for smaller-scale systems that could benefit from a more advanced version of the 80186 CPU core, with advantages such as higher clock rates, faster instruction execution (measured in clock cycles), and unmultiplexed buses, but not the 24-bit (16 MB) memory space.\nTo support protected mode, new instructions have been added: ARPL, VERR, VERW, LAR, LSL, SMSW, SGDT, SIDT, SLDT, STR, LMSW, LGDT, LIDT, LLDT, LTR, CLTS. There are also new exceptions (internal interrupts): invalid opcode, coprocessor not available, double fault, coprocessor segment overrun, stack fault, segment overrun/general protection fault, and others only for protected mode.\nOS support.\nThe protected mode of the 80286 was not routinely utilized in PC applications until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A disk operating system could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory.\nThe difficulty lay in the incompatibility of older real-mode DOS programs with protected mode. They could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs typically would directly access data and code segments that did not belong to them, as real mode allowed them to do without restriction; in contrast, the design intent of protected mode was to prevent programs from accessing any segments other than their own unless special access was explicitly allowed. While it was possible to set up a protected-mode environment that allowed all programs access to all segments (by putting all segment descriptors into the Global Descriptor Table (GDT) and assigning them all the same privilege level), this undermined nearly all of the advantages of protected mode except the extended (24-bit) address space. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly (i.e., ugly from an internal technical viewpoint) but would still run a majority of the old programs. Protected mode also did not provide a significant enough performance advantage over the 8086-compatible real mode to justify supporting its capabilities; actually, except for task switches when multitasking, it yielded a performance disadvantage, by slowing down many instructions through a litany of added privilege checks. In protected mode, registers were still 16-bit, and the programmer was still forced to use a memory map composed of 64\u00a0kB segments, just like in real mode.\nIntel had not expected the lack of virtual machine support for 8086 software to be a problem, because it thought that new software using all of the 80286's capabilities would quickly appear. Bill Gates referred to the 80286 as a \"brain-damaged\" chip, because it cannot use virtual machines to multitask multiple MS-DOS applications with an operating system like Microsoft Windows. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode).\nIn January 1985, Digital Research previewed the Concurrent DOS 286 1.0 operating system developed in cooperation with Intel. The product would function strictly as an 80286 native-mode (i.e. protected-mode) operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation. This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip. In August, after extensive testing on E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said that there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said that the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, in the E-2 step, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster. Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and point-of-sale terminals in 1986. Digital Research's FlexOS 286 version 1.3, a derivation of Concurrent DOS 286, was developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS, but the same limitations affected it.\nOther operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984), Coherent, and Minix. These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS applications or other real-mode programs.\nWhen designing the 80386 Intel engineers were aware of, and agreed with, the 80286's poor reputation. They enhanced the 80386's protected mode to address more memory, and also added the separate virtual 8086 mode, a mode within protected mode with much better MS-DOS compatibility.\nSupport components.\nThis is a list of bus interface components that connect to an Intel 80286 microprocessor.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15055", "revid": "48736052", "url": "https://en.wikipedia.org/wiki?curid=15055", "title": "Ivanhoe", "text": "1820 novel by Walter Scott\nIvanhoe: A Romance ( ) by Walter Scott is a historical novel published in three volumes, in December 1819, as one of the Waverley novels. It marked a shift away from Scott's prior practice of setting stories in Scotland and in the more recent past. It became one of Scott's best-known and most influential novels.\nSet in England in the Middle Ages, with colourful descriptions of a tournament, outlaws, a witch trial, and divisions between Jews and Christians, Normans and Saxons, the novel was credited by many, including Thomas Carlyle and John Ruskin, with inspiring increased interest in chivalric romance and medievalism. As John Henry Newman put it, Scott \"had first turned men's minds in the direction of the Middle Ages\". It was also credited with influencing contemporary popular perceptions of historical figures such as King Richard the Lionheart, Prince John, and Robin Hood.\nComposition and sources.\nIn June 1819, Walter Scott still suffered from the severe stomach pains that had forced him to dictate the last part of \"The Bride of Lammermoor\", and also most of \"A Legend of the Wars of Montrose\", which he finished at the end of May. By the beginning of July, at the latest, Scott had started dictating his new novel \"Ivanhoe\", again with John Ballantyne and William Laidlaw as amanuenses. For the second half of the manuscript, Scott was able to take up the pen, and completed \"Ivanhoe: A Romance\" in early November 1819.\nFor detailed information about the Middle Ages Scott drew on three works by the antiquarian Joseph Strutt: \"Horda Angel-cynnan or a Compleat View of the Manners, Customs, Arms, Habits etc. of the Inhabitants of England\" (1775\u201376), \"Dress and Habits of the People of England\" (1796\u201399), and \"Sports and Pastimes of the People of England\" (1801). Two historians gave him a solid grounding in the period: Robert Henry with \"The History of Great Britain\" (1771\u201393), and Sharon Turner with \"The History of the Anglo-Saxons from the Earliest Period to the Norman Conquest\" (1799\u20131805). His clearest debt to an original medieval source involved the Templar Rule, reproduced in \"The Theatre of Honour and Knight-Hood\" (1623) translated from the French of Andr\u00e9 Favine. Scott was happy to introduce details from the later Middle Ages, and Chaucer was particularly helpful, as (in a different way) was the fourteenth-century romance \"Richard Coeur de Lion\". The figure of Locksley in the story and many elements of the tale are undoubtedly influenced by Scott's association with Joseph Ritson, who had earlier compiled \"Robin Hood: a collection of all the ancient poems, songs and ballads now extant relative to that celebrated English outlaw (1795).\"\nEditions.\n\"Ivanhoe\" was published by Archibald Constable in Edinburgh. All first editions carry the date of 1820, but it was released on 20 December 1819 and issued in London on the 29th by Hurst, Robinson and Co.. As with all of the Waverley novels before 1827, publication was anonymous. The print run was 10,000 copies, and the cost was \u00a31 10\"s\" (\u00a31.50, equivalent in purchasing power to \u00a3149 in 2021). It is possible that Scott was involved in minor changes to the text during the early 1820s but his main revision was carried out in 1829 for the 'Magnum' edition where the novel appeared in Volumes 16 and 17 in September and October 1830.\nThe standard modern edition, by Graham Tulloch, appeared as Volume 8 of the Edinburgh Edition of the Waverley Novels in 1998: this is based on the first edition with emendations principally from Scott's manuscript in the second half of the work; the new Magnum material is included in Volume 25b.\nPlot summary.\n\"Ivanhoe\", set in 1194, focuses on one of the remaining Anglo-Saxon noble families when the nobility in England was overwhelmingly Norman. It follows the Saxon protagonist, Sir Wilfred of Ivanhoe, who is out of favour with his father for Sir Wilfred's allegiance to the Norman king Richard the Lionheart. After the failure of the Third Crusade, many Crusaders were still returning to their homes in Europe. King Richard, who had been captured by Leopold of Austria on his return journey to England, was believed to still be in captivity.\nOpening.\nProtagonist Wilfred of Ivanhoe is disinherited by his father Cedric of Rotherwood for supporting the Norman King Richard and for falling in love with the Lady Rowena, a ward of Cedric and descendant of the Saxon Kings of England. Cedric planned to have Rowena marry the powerful Lord Athelstane, a pretender to the Crown of England by his descent from the last Saxon King, Harold Godwinson. Ivanhoe accompanies King Richard on the Third Crusade, where he is said to have played a notable role in the Siege of Acre.\nThe book opens with a scene of Norman knights and prelates seeking the hospitality of Cedric. They are guided there by a pilgrim, known at that time as a palmer. That same night, Isaac of York, a Jewish moneylender, seeks refuge at Rotherwood on his way to the tournament at Ashby. Following the night's meal, the palmer observes one of the Normans, the Templar Brian de Bois-Guilbert, issue orders to his Saracen soldiers to capture Isaac.\nThe palmer then assists in Isaac's escape from Rotherwood, with the additional aid of the swineherd Gurth.\nIsaac of York offers to repay his debt to the palmer with a suit of armour and a war horse to participate in the tournament at Ashby-de-la-Zouch Castle, on his inference that the palmer was secretly a knight. The palmer is taken by surprise, but accepts the offer.\nThe tournament.\nThe tournament is presided over by Prince John. Also in attendance are Cedric, Athelstane, Lady Rowena, Isaac of York, his daughter Rebecca, Robin of Locksley and his men, Prince John's advisor Waldemar Fitzurse, and numerous Norman knights.\nOn the first day of the tournament, in a bout of individual jousting, a mysterious knight, identifying himself only as \"Desdichado\" (described in the book as Spanish, taken by the Saxons to mean \"Disinherited\"), defeats Bois-Guilbert. The masked knight declines to reveal himself despite Prince John's request, but is nevertheless declared the champion of the day and is permitted to choose the Queen of the Tournament. He bestows this honour upon Lady Rowena.\nOn the second day, at a melee, Desdichado is the leader of one party, opposed by his former adversaries. Desdichado's side is soon hard-pressed and he himself beset by multiple foes until rescued by a knight nicknamed \"Le Noir Faineant\" ('the Black Sluggard'), who thereafter departs in secret. When forced to unmask himself to receive his coronet (the sign of championship), Desdichado is identified as Wilfred of Ivanhoe, returned from the Crusades. This causes much consternation to Prince John and his court who now fear the imminent return of King Richard.\nIvanhoe is severely wounded in the competition yet his father does not move quickly to tend to him. Instead, Rebecca, a skilled physician, tends to him while they are lodged near the tournament and then convinces her father to take Ivanhoe with them to their home in York when he is fit for that trip. The conclusion of the tournament includes feats of archery by Locksley, such as splitting a willow reed with his arrow. Prince John's dinner for the local Saxons ends in insults.\nCapture and rescue.\nIn the forests between Ashby and York, Isaac, Rebecca and the wounded Ivanhoe are abandoned by their guards, who fear bandits and take all of Isaac's horses. Cedric, Athelstane and the Lady Rowena meet them and agree to travel together. The party is captured by de Bracy and his companions and taken to Torquilstone, the castle of Front-de-B\u0153uf. The swineherd Gurth and Wamba the jester manage to escape, and then encounter Locksley, who plans a rescue.\nThe Black Knight, having taken refuge for the night in the hut of local friar, the Holy Clerk of Copmanhurst, volunteers his assistance on learning about the captives from Robin of Locksley. They then besiege the Castle of Torquilstone with Robin's own men, including the friar and assorted Saxon yeomen. Inside Torquilstone, de Bracy expresses his love for the Lady Rowena but is refused. Brian de Bois-Guilbert tries to rape Rebecca and is thwarted. He then tries to seduce her and is rebuffed. Front-de-B\u0153uf tries to wring a hefty ransom from Isaac of York, but Isaac refuses to pay unless his daughter is freed.\nWhen the besiegers deliver a note to yield up the captives, their Norman captors demand a priest to administer the Final Sacrament to Cedric; whereupon Cedric's jester Wamba slips in disguised as a priest, and takes the place of Cedric, who escapes and brings important information to the besiegers on the strength of the garrison and its layout. On his way out, Cedric meets the Saxon crone Ulrica, who vows revenge on Front-de-B\u0153uf and advises Cedric to tell the besiegers. The besiegers storm the castle. The castle is set aflame during the assault by Ulrica, the daughter of the original lord of the castle, Lord Torquilstone, as revenge for her father's death. Front-de-B\u0153uf is killed in the fire while de Bracy surrenders to the Black Knight, who identifies himself as King Richard and releases de Bracy. Bois-Guilbert escapes with Rebecca while Isaac is captured by the Clerk of Copmanhurst. The Lady Rowena is saved by Cedric, while the still-wounded Ivanhoe is rescued from the burning castle by King Richard. In the fighting, Athelstane is wounded and presumed dead while attempting to rescue Rebecca, whom he mistakes for Rowena.\nRebecca's trial and Ivanhoe's reconciliation.\nFollowing the battle, Locksley plays host to King Richard. Word is conveyed by de Bracy to Prince John of the King's return and the fall of Torquilstone. In the meantime, Bois-Guilbert rushes with his captive to the nearest Templar Preceptory, where Lucas de Beaumanoir, the Grand Master of the Templars, takes umbrage at Bois-Guilbert's infatuation and subjects Rebecca to a trial for witchcraft. At Bois-Guilbert's secret request, she claims the right to trial by combat; and Bois-Guilbert, who had hoped to fight as Rebecca's champion, is devastated when the Grand Master orders him to fight on behalf of the Templestowe. Rebecca then writes to her father to procure a champion for her. Cedric organizes Athelstane's funeral at Coningsburgh, in the midst of which the Black Knight arrives with Ivanhoe. Cedric, who had not been present at Locksley's carousal, is ill-disposed towards the knight upon learning his true identity, but Richard calms Cedric and reconciles him with his son. During this conversation, Athelstane emerges\u2014not dead, but laid in his coffin alive by monks desirous of the funeral money. Over Cedric's renewed protests, Athelstane pledges his homage to the Norman King Richard and urges Cedric to allow Rowena to marry Ivanhoe, to which Cedric finally agrees.\nSoon after this reconciliation, Ivanhoe receives word from Isaac beseeching him to fight on Rebecca's behalf. Ivanhoe, riding day and night, arrives in time for the trial by combat; however, both horse and man are exhausted, with little chance of victory. Bois-Guilbert refuses to fight but Ivanhoe accuses him of breaking his word and the Templar reacts fiercely. His face becomes flushed and he is ready for combat. The two knights make one charge at each other with lances, Bois-Guilbert appearing to have the advantage. Ivanhoe and his horse go down, but Bois-Guilbert also falls though barely touched. Ivanhoe quickly gets up to finish the fight with his sword, but Bois-Guilbert does not rise and dies a victim of his own contending passions.\nIvanhoe and Rowena marry and live a long and happy life together. Fearing further persecution, Rebecca and her father plan to quit England for Granada. Before leaving, Rebecca comes to Rowena shortly after the wedding to bid her a solemn farewell. Ivanhoe's military service ends with the death of King Richard five years later.\nCharacters.\n\"(principal characters in bold)\"\nChapter summary.\nDedicatory Epistle: An imaginary letter to the Rev. Dr Dryasdust from Laurence Templeton who has found the materials for the following tale mostly in the Anglo-Norman Wardour Manuscript. He wishes to provide an English counterpart to the preceding Waverley novels, in spite of various difficulties arising from the chronologically remote setting made necessary by the earlier progress of civilisation south of the Border.\nVolume One.\nCh. 1: Historical sketch. Gurth the swineherd and Wamba the jester discuss life under Norman rule.\nCh. 2: Wamba and Gurth wilfully misdirect a group of horsemen headed by Prior Aymer and Brian de Bois-Guilbert seeking shelter at Cedric's Rotherwood. Aymer and Bois-Guilbert discuss the beauty of Cedric's ward Rowena and are redirected, this time correctly, by a palmer [Ivanhoe in disguise].\nCh. 3: Cedric anxiously awaits the return of Gurth and the pigs. Aymer and Bois-Guilbert arrive.\nCh. 4: Bois-Guilbert admires Rowena as she enters for the evening feast.\nCh. 5: During the feast: Isaac enters and is befriended by the palmer; Cedric laments the decay of the Saxon language; the palmer refutes Bois-Guilbert's assertion of Templar supremacy with an account of a tournament in Palestine, where Ivanhoe defeated him; the palmer and Rowena give a pledge for a return match; and Isaac is thunderstruck by Bois-Guilbert's denial of his assertion of poverty.\nCh. 6: Next day the palmer tells Rowena that Ivanhoe will soon be home. He offers to protect Isaac from Bois-Guilbert, whom he has overheard giving instructions for his capture. On the road to Sheffield Isaac mentions a source of horse and armour of which he guesses the palmer has need.\nCh. 7: As the audience for a tournament at Ashby de la Zouch assembles, Prince John amuses himself by making fun of Athelstane and Isaac.\nCh. 8: After a series of Saxon defeats in the tournament the 'Disinherited Knight' [Ivanhoe] triumphs over Bois-Guilbert and the other Norman challengers.\nCh. 9: The Disinherited Knight nominates Rowena as Queen of the Tournament.\nCh. 10: The Disinherited Knight refuses to ransom Bois-Guilbert's armour, declaring that their business is not concluded. He instructs his attendant, Gurth in disguise, to convey money to Isaac to repay him for arranging the provision of his horse and armour. Gurth does so, but Rebecca secretly refunds the money.\nCh. 11: Gurth is assailed by a band of outlaws, but they spare him on hearing his story and after he has defeated one of their number, a miller, at quarter-staves.\nCh. 12: The Disinherited Knight's party triumph at the tournament, with the aid of a knight in black [Richard in disguise]; he is revealed as Ivanhoe and faints as a result of the wounds he has incurred.\nCh. 13: John encourages De Bracy to court Rowena and receives a warning from France that Richard has escaped. Locksley [Robin Hood] triumphs in an archery contest.\nCh. 14: At the tournament banquet Cedric continues to disown his son (who has been associating with the Normans) but drinks to the health of Richard, rather than John, as the noblest of that race.\nVolume Two.\nCh. 1 (15): De Bracy (disguised as a forester) tells Fitzurse of his plan to capture Rowena and then 'rescue' her in his own person.\nCh. 2 (16): The Black Knight is entertained by a hermit [Friar Tuck] at Copmanhurst.\nCh. 3 (17): The Black Knight and the hermit exchange songs.\nCh. 4 (18): (Retrospect: Before going to the banquet Cedric learned that Ivanhoe had been removed by unknown carers; Gurth was recognised and captured by Cedric's cupbearer Oswald.) Cedric finds Athelstane unresponsive to his attempts to interest him in Rowena, who is herself only attracted by Ivanhoe.\nCh. 5 (19): Rowena persuades Cedric to escort Isaac and Rebecca, who have been abandoned (along with a sick man [Ivanhoe] in their care) by their hired protectors. Wamba helps Gurth to escape again. De Bracy mounts his attack, during which Wamba escapes. He meets up with Gurth and they encounter Locksley who, after investigation, advises against a counter-attack, the captives not being in immediate danger.\nCh. 6 (20): Locksley sends two of his men to watch De Bracy. At Copmanhurst he meets the Black Knight who agrees to join in the rescue.\nCh. 7 (21): De Bracy tells Bois-Guilbert he has decided to abandon his 'rescue' plan, mistrusting his companion though the Templar says it is Rebecca he is interested in. On arrival at Torquilstone castle Cedric laments its decline.\nCh. 8 (22): Under threat of torture Isaac agrees to pay Front-de-B\u0153uf a thousand pounds, but only if Rebecca is released.\nCh. 9 (23): De Bracy uses Ivanhoe's danger from Front-de-B\u0153uf to put pressure on Rowena, but he is moved by her resulting distress. The narrator refers the reader to historical instances of baronial oppression in medieval England.\nCh. 10 (24): A hag Urfried [Ulrica] warns Rebecca of her forthcoming fate. Rebecca impresses Bois-Guilbert by her spirited resistance to his advances.\nCh. 11 (25): Front-de-B\u0153uf rejects a written challenge from Gurth and Wamba. Wamba offers to spy out the castle posing as a confessor.\nCh. 12 (26): Entering the castle, Wamba exchanges clothes with Cedric who encounters Rebecca and Urfried.\nCh. 13 (27): Urfried recognises Cedric as a Saxon and, revealing herself as Ulrica, tells her story which involves Front-de-B\u0153uf murdering his father, who had killed her father and seven brothers when taking the castle, and had become her detested lover. She says she will give a signal when the time is ripe for storming the castle. Front-de-B\u0153uf sends the presumed friar with a message to summon reinforcements. Athelstane defies him, claiming that Rowena is his fianc\u00e9e. The monk Ambrose arrives seeking help for Aymer who has been captured by Locksley's men.\nCh. 14 (28): (Retrospective chapter detailing Rebecca's care for Ivanhoe from the tournament to the assault on Torquilstone.)\nCh. 15 (29): Rebecca describes the assault on Torquilstone to the wounded Ivanhoe, disagreeing with his exalted view of chivalry.\nCh. 16 (30): Front-de-B\u0153uf being mortally wounded, Bois-Guilbert and De Bracy discuss how best to repel the besiegers. Ulrica sets fire to the castle and exults over Front-de-B\u0153uf who perishes in the flames.\nVolume Three.\nCh. 1 (31): (The chapter opens with a retrospective account of the attackers' plans and the taking of the barbican.) The Black Knight defeats De Bracy, making himself known to him as Richard, and rescues Ivanhoe. Bois-Guilbert rescues Rebecca, striking down Athelstane who thinks she is Rowena. Ulrica perishes in the flames after singing a wild pagan hymn.\nCh. 2 (32): Locksley supervises the orderly division of the spoil. Friar Tuck brings Isaac whom he has made captive, and engages in good-natured buffeting with the Black Knight.\nCh. 3 (33): Locksley arranges ransom terms for Isaac and Aymer. Aymer agrees to write on Isaac's behalf to Bois-Guilbert, to urge Rebecca's release, in exchange for Isaac loaning him money to pay his ransom to the banditti.\nCh. 4 (34): De Bracy informs John that Richard is in England. Together with Fitzurse he threatens to desert John, but the prince responds cunningly.\nCh. 5 (35): At York, Isaac stays with a friend, Nathan, as he strives to rescue Rebecca from the Templestowe. At the priory the Grand-Master Beaumanoir tells Conrade Mountfitchet that he intends to take a hard line with Templar irregularities. Arriving, Isaac shows him a letter from Aymer to Bois-Guilbert referring to Rebecca, whom Beaumanoir determines must be a witch.\nCh. 6 (36): Beaumanoir tells Preceptor Albert Malvoisin of his outrage at Rebecca's presence in the preceptory. Albert informs Bois-Guilbert of her trial for sorcery, and warns Bois-Guilbert not to defend her. Mountfichet says he will seek evidence against her, including bribing a few fake witnesses with fabricated stories.\nCh. 7 (37): Rebecca is found guilty of witchcraft and sentenced to death. At Bois-Guilbert's secret prompting she demands that a champion defend her in trial by combat.\nCh. 8 (38): Rebecca's demand is accepted, Bois-Guilbert being appointed champion for the prosecution. Bearing a message to her father, the peasant Higg meets him and Nathan on their way to the preceptory, and Isaac goes in search of Ivanhoe.\nCh. 9 (39): Rebecca rejects Bois-Guilbert's offer to fail to appear for the combat in return for her love. Albert persuades him that it is in his interest to appear.\nCh. 10 (40): The Black Knight leaves Ivanhoe to travel to Coningsburgh castle for Athelstane's funeral, and Ivanhoe follows him the next day. The Black Knight is rescued by Locksley from an attack carried out by Fitzurse on John's orders, and reveals his identity as Richard to his companions, prompting Locksley to identify himself as Robin Hood.\nCh. 11 (41): Richard talks to Ivanhoe and dines with the outlaws before Robin arranges a false alarm to put an end to the delay. The party arrive at Coningsburgh.\nCh. 12 (42): Richard procures Ivanhoe's pardon from his father. Athelstane appears, not dead, giving his allegiance to Richard and surrendering Rowena to Ivanhoe. Ivanhoe and Richard each receive a message and disappear from Coningsburgh.\nCh. 13 (43): Rebecca is tied to the stake, and no champion appears. Bois-Guilbert, racked by guilt, begs her to run away with him. Rebecca refuses. Ivanhoe, exhausted from his ride and not fully recovered from his injury, appears as Rebecca's champion, and as they charge Bois-Guilbert dies the victim of his contending passions.\nCh. 14 (44): Beaumanoir and his Templars leave Richard defiantly. Cedric agrees to the marriage of Ivanhoe and Rowena. Rebecca takes her leave of Rowena, leaving a message of her thanks to Ivanhoe for saving her, before her father and she quit England to make a new life under the tolerant King of Granada.\nStyle.\nCritics of the novel have treated it as a romance intended mainly to entertain boys. \"Ivanhoe\" maintains many of the elements of the Romance genre, including the quest, a chivalric setting, and the overthrowing of a corrupt social order to bring on a time of happiness. Other critics assert that the novel creates a realistic and vibrant story, idealising neither the past nor its main character.\nThemes.\nScott treats themes similar to those of some of his earlier novels, like \"Rob Roy\" and \"The Heart of Midlothian\", examining the conflict between heroic ideals and modern society. In the latter novels, industrial society becomes the centre of this conflict as the \"backward\" Scots and the \"advanced\" English have to arise from chaos to create unity. Similarly, the Normans in \"Ivanhoe\", who represent a more sophisticated culture, and the Saxons, who are poor, disenfranchised, and resentful of Norman rule, band together and begin to mould themselves into one people. The conflict between the Saxons and Normans focuses on the losses both groups must experience before they can be reconciled and thus forge a united England. The particular loss is in the extremes of their own cultural values, which must be disavowed in order for the society to function. For the Saxons, this value is the final admission of the hopelessness of the Saxon cause. The Normans must learn to overcome the materialism and violence in their own codes of chivalry. Ivanhoe and Richard represent the hope of reconciliation for a unified future.\nIvanhoe, though of a more noble lineage than some of the other characters, represents a middling individual in the medieval class system who is not exceptionally outstanding in his abilities, as is expected of other quasi-historical fictional characters, such as the Greek heroes. Critic Gy\u00f6rgy Luk\u00e1cs points to middling main characters like Ivanhoe in Walter Scott's other novels as one of the primary reasons Scott's historical novels depart from previous historical works, and better explore social and cultural history.\nAllusions to real history and geography.\nThe location of the novel is centred upon southern Yorkshire, north-west Leicestershire and northern Nottinghamshire in England. Castles mentioned within the story include Ashby de la Zouch Castle (now a ruin in the care of English Heritage), York (though the mention of Clifford's Tower, likewise an extant English Heritage property, is anachronistic, it not having been called that until later after various rebuilds) and 'Coningsburgh', which is based upon Conisbrough Castle, in the ancient town of Conisbrough near Doncaster (the castle also being a popular English Heritage site). In the novel, Aymer is the Prior of Jorvaulx, a historical spelling of the great Jervaulx Abbey of Yorkshire. Reference is made within the story to York Minster, where the climactic wedding takes place, and to the Bishop of Sheffield, although the Diocese of Sheffield did not exist at either the time of the novel or the time Scott wrote the novel and was not founded until 1914. Such references suggest that Robin Hood lived or travelled in the region.\nConisbrough is so dedicated to the story of \"Ivanhoe\" that many of its streets, schools, and public buildings are named after characters from the book.\nSir Walter Scott took the title of his novel, the name of its hero, from the Buckinghamshire village of Ivinghoe.\n\"The name of Ivanhoe,\" he says in his 1830 Introduction to the Magnum edition, \"was suggested by an old rhyme.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Tring, Wing, and Ivanhoe,\nFor striking of a blow,\nHampden did forego,\nAnd glad he could escape so.\nIvanhoe is an alternate name for Ivinghoe first recorded in 1665.\nOlder rural people in the Ivinghoe area most probably pronounced the name the same as Ivanhoe, according to Prof. Paul Kerswill of the University of York, a Fellow of the British Academy (FBA).\nIt is most probable Scott had direct knowledge of Ivinghoe and did some research before using it as the title for his novel, as he did for the other places mentioned in the novel.\nThe presence of Sir Walter Scott was recorded in Berkhamsted that is just eight miles away from Ivinghoe.\nIn the novel he speaks also of \"the rich fief of Ivanhoe\". The Manor of Ivanhoe is listed in the largest 20% of settlements recorded in Domesday.\nLasting influence on the Robin Hood legend.\nThe modern conception of Robin Hood as a cheerful, decent, patriotic rebel owes much to \"Ivanhoe\".\n\"Locksley\" becomes Robin Hood's title in the Scott novel, and it has been used ever since to refer to the legendary outlaw. Scott appears to have taken the name from an anonymous manuscript\u2014written in 1600\u2014that employs \"Locksley\" as an epithet for Robin Hood. Owing to Scott's decision to make use of the manuscript, Robin Hood from Locksley has been transformed for all time into \"Robin of Locksley\", alias Robin Hood. (There is, incidentally, a village called Loxley in Yorkshire.)\nScott makes the 12th-century's Saxon-Norman conflict a major theme in his novel. The original medieval stories about Robin Hood did not mention any conflict between Saxons and Normans; it was Scott who introduced this theme into the legend. The characters in \"Ivanhoe\" refer to Prince John and King Richard I as \"Normans\"; contemporary medieval documents from this period do not refer to either of these two rulers as Normans. Recent re-tellings of the story retain Scott's emphasis on the Norman-Saxon conflict.\nScott also shunned the late-16th-century depiction of Robin as a dispossessed nobleman (the Earl of Huntingdon).\nThis, however, has not prevented Scott from making an important contribution to the noble-hero strand of the legend, too, because some subsequent motion picture treatments of Robin Hood's adventures give Robin traits that are characteristic of Ivanhoe as well. The most notable Robin Hood films are the lavish Douglas Fairbanks 1922 silent film, the 1938 triple Academy Award-winning \"Adventures of Robin Hood\" with Errol Flynn as Robin (which contemporary reviewer Frank Nugent links specifically with \"Ivanhoe\"), and the 1991 box-office success ' with Kevin Costner. There is also the Mel Brooks spoof '.\nIn most versions of Robin Hood, both Ivanhoe and Robin, for instance, are returning Crusaders. They have quarrelled with their respective fathers, they are proud to be Saxons, they display a highly evolved sense of justice, they support the rightful king even though he is of Norman-French ancestry, they are adept with weapons, and they each fall in love with a \"fair maid\" (Rowena and Marian, respectively).\nThis particular time-frame was popularised by Scott. He borrowed it from the writings of the 16th-century chronicler John Mair or a 17th-century ballad presumably to make the plot of his novel more gripping. Medieval balladeers had generally placed Robin about two centuries later in the reign of Edward I, II or III.\nRobin's familiar feat of splitting his competitor's arrow in an archery contest appears for the first time in \"Ivanhoe\".\nHistorical accuracy.\nThe general political events depicted in the novel are fairly accurate; the novel tells of the period just after King Richard's imprisonment in Austria following the Crusade and of his return to England after a ransom is paid. Yet the story is also heavily fictionalised. Scott himself acknowledged that he had taken liberties with history in his \"Dedicatory Epistle\" to \"Ivanhoe\". Modern readers are cautioned to understand that Scott's aim was to create a compelling novel set in a historical period, not to provide a book of history.\nThere has been criticism of Scott's portrayal of the bitter extent of the \"enmity of Saxon and Norman, represented as persisting in the days of Richard\" as \"unsupported by the evidence of contemporary records that forms the basis of the story.\" Historian E. A. Freeman criticised Scott's novel, stating its depiction of a Saxon\u2013Norman conflict in late twelfth-century England was unhistorical. Freeman cited medieval writer Walter Map, who claimed that tension between the Saxons and Normans had declined by the reign of Henry I. Freeman also cited the late twelfth-century book \"Dialogus de Scaccario\" by Richard FitzNeal. This book claimed that the Saxons and Normans had so merged through intermarriage and cultural assimilation that (outside the aristocracy) it was impossible to tell \"one from the other.\" Finally, Freeman ended his critique of Scott by saying that by the end of the twelfth century, the descendants of both Saxons and Normans in England referred to themselves as \"English\", not \"Saxon\" or \"Norman\".\nHowever, Scott may have intended to suggest parallels between the Norman Conquest, which takes place roughly 130 years before the setting of \"Ivanhoe\", and Scott's native Scotland, which had united with England in 1707 roughly the same length of time ago, and witnessed a resurgence in Scottish nationalism evidenced by the emergence of Robert Burns, the famous poet who deliberately chose to work in Scots vernacular though he was an educated man and spoke modern English eloquently. Some experts suggest that Scott deliberately used \"Ivanhoe\" to illustrate his own combination of Scottish patriotism and unionism.\nThe novel generated a new name in English\u2014Cedric. The original Saxon name had been \"Cerdic\" but Scott misspelled it\u2014an example of metathesis. \"It is not a name but a misspelling\" said satirist H. H. Munro.\nIn England in 1194, it would have been anachronistic for Rebecca, a Jewish woman, to be charged with witchcraft. In medieval witch trials, it was usually the belief in witchcraft that was prosecuted as a heresy, a charge a non-Christian woman would not have been subject to. Death did not become the usual penalty until the 15th century and even then, the form of execution used for witches in England was hanging, not burning. The conductor of the trial, the Grand Master Of The Templars, is referred to as Lucas de Beaumanoir, whereas the historically real Master during that time was Gilbert Horal. There are other various minor errors, e.g. the description of the tournament at Ashby owes more to the 14th century, most of the coins mentioned by Scott are exotic, William Rufus is said to have been John Lackland's grandfather, but he was actually his great-great-uncle, and Wamba (disguised as a monk) says \"I am a poor brother of the Order of St Francis\", but St. Francis of Assisi only began his preaching ten years after the death of Richard I. Also, in Chapter 43, Bois-Guilbert commences the fight being mounted on his horse named Zamor, which he claimed that he had won from the \"Soldan of Trebizond\". This is anachronistic, as the Comnenids founded the rump Byzantine Empire of Trebizond only in 1204, just by the end of the Fourth Crusade. Lastly, in the novel's ultimate chapter, Rebecca and her father move to Granada to spend the rest of their lives under Mohammed Boabdil. In fact, the real Muhammad XII of Granada, popularly known to the Western world as Boabdil, was not even born before 1460, and the Emirate of Granada established before 1230.\nDespite this fancifulness, \"Ivanhoe\" does make some prescient historical points. The novel is occasionally critical of King Richard, \"who seems to love adventure more than he loves the well-being of his subjects\"\u2014in contrast to the idealised, romantic view of Richard popular at the time, but rather echoes the way King Richard is often judged by historians today.\nReception.\nMost of the original reviewers gave \"Ivanhoe\" an enthusiastic or broadly favourable reception.\nAs usual, Scott's descriptive powers and his ability to present the matters of the past were generally praised. More than one reviewer found the work notably poetic. Several of them found themselves transported imaginatively to the remote period of the novel, although some problems were recognised: the combining of features from the high and late Middle Ages; an awkwardly created language for the dialogue; and antiquarian overload. The author's excursion into England was generally judged a success, the forest outlaws and the creation of 'merry England' attracting particular praise. Rebecca was almost unanimously admired, especially in her farewell scene. The plot was either criticised for its weakness, or just regarded as of less importance than the scenes and characters. The scenes at Torquilstone were judged horrible by several critics, with special focus on Ulrica. Athelstane's resurrection found no favour, the kindest response being that of Francis Jeffrey in \"The Edinburgh Review\" who suggested (writing anonymously, like all the reviewers) that it was 'introduced out of the very wantonness of merriment'.\nLetitia Elizabeth Landon, who was a devotee of Scott's, wrote a poetical illustration to a picture of . by Thomas Allom in Fisher's Drawing Room Scrap Book, 1838.\nThe Eglinton Tournament of 1839 held by the 13th Earl of Eglinton at Eglinton Castle in Ayrshire was inspired by and modelled on \"Ivanhoe\".\nOn 5 November 2019, \"BBC News\" included \"Ivanhoe\" on its list of the 100 most influential novels.\nFilm, TV or theatrical adaptations.\nFilms.\nThe novel has been the basis for several motion pictures:\nTelevision.\nThere have also been many television adaptations of the novel, including:\nOperas.\nVictor Sieg's dramatic cantata \"Ivanho\u00e9\" won the Prix de Rome in 1864 and premiered in Paris the same year. \"Ivanhoe\" was the grand opera by Arthur Sullivan and Julian Sturgis (Sturgis was recommended by Sullivan's oft-time partner W. S. Gilbert). It debuted in 1891, and ran for 155 consecutive performances. Other operas based on the novel have been composed by Gioachino Rossini (\"Ivanho\u00e9\"), Thomas Sari (\"Ivanho\u00e9\"), Bartolomeo Pisani (\"Rebecca\"), A. Castagnier (\"R\u00e9becca\"), Otto Nicolai (\"Il Templario\"), and Heinrich Marschner (\"Der Templer und die J\u00fcdin\"). Rossini's opera is a \"pasticcio\" (an opera in which the music for a new text is chosen from pre-existent music by one or more composers). Scott attended a performance of it and recorded in his journal, \"It was an opera, and, of course, the story sadly mangled and the dialogue, in part nonsense.\"\nLegacy.\nThe railway running through Ashby-de-la-Zouch was known as the Ivanhoe line between 1993 and 2005, in reference to the book's setting in the locality.\nA portion of the Silver Lake neighborhood in Los Angeles was established in 1887 as a real estate tract called Ivanhoe. (Realtors John C. Byram and Robert W. Poindexter were behind the tract; it is a myth that it was named decades earlier by Scottish settler Hugo Reid, as he never lived in this section of Los Angeles County.) The upper reservoir and an elementary school are still named Ivanhoe while many of the streets in the area reference Scott's other works and characters such as Locksley, Rowena, Kenilworth, Waverly [sic], Avenel, and St. George.\nIvanhoe, North Carolina is named after \"Ivanhoe\".\nExternal links.\n\u00a0 incorporates text from a publication now in the public domain:\u00a0"}
{"id": "15056", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=15056", "title": "Isoelectric point", "text": "pH at which a molecule carries no net electric charge\nThe isoelectric point (pI, pH(I), IEP), is the pH at which a molecule carries no net electrical charge or is electrically neutral in the statistical mean. The standard nomenclature to represent the isoelectric point is pH(I). However, pI is also used. For brevity, this article uses pI. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H+).\nSurfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H+/HO\u2212, the net surface charge is affected by the pH of the liquid in which the solid is submerged.\nThe pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative native PAGE, which uses a constant pH to separate proteins, or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is the first step in 2-D polyacrylamide gel electrophoresis.\nIn biomolecules, proteins can be separated by ion exchange chromatography. Biological proteins are made up of zwitterionic amino acid compounds; the net charge of these proteins can be positive or negative depending on the pH of the environment. The specific pI of the target protein can be used to model the process around and the compound can then be purified from the rest of the mixture. Buffers of various pH can be used for this purification process to change the pH of the environment. When a mixture containing a target protein is loaded into an ion exchanger, the stationary matrix can be either positively-charged (for mobile anions) or negatively-charged (for mobile cations). At low pH values, the net charge of most proteins in the mixture is positive \u2013 in cation exchangers, these positively-charged proteins bind to the negatively-charged matrix. At high pH values, the net charge of most proteins is negative, where they bind to the positively-charged matrix in anion exchangers. When the environment is at a pH value equal to the protein's pI, the net charge is zero, and the protein is not bound to any exchanger, and therefore, can be eluted out.\nCalculating pI values.\nFor an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.\n formula_1\nThe pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.\nExamples.\nIn the two examples (on the right) the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units. Thus in the gas phase, the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution\n&lt;chem&gt;H2NCH2CO2H &lt;=&gt; H3N+CH2CO2-&lt;/chem&gt;\nis not known.\nThe other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of is negligible at the isoelectric point in this case.\nIf the pI is greater than the pH, the molecule will have a positive charge.\nPeptides and proteins.\nA number of algorithms for estimating isoelectric points of peptides and proteins have been developed. Most of them use Henderson\u2013Hasselbalch equation with different pK values. For instance, within the model proposed by Bjellqvist and co-workers, the pKs were determined between closely related immobilines by focusing the same sample in overlapping pH gradients. Some improvements in the methodology (especially in the determination of the pK values for modified amino acids) have been also proposed. More advanced methods take into account the effect of adjacent amino acids \u00b13 residues away from a charged aspartic or glutamic acid, the effects on free C terminus, as well as they apply a correction term to the corresponding pK values using genetic algorithm. Other recent approaches are based on a support vector machine algorithm and pKa optimization against experimentally known protein/peptide isoelectric points.\nMoreover, experimentally measured isoelectric point of proteins were aggregated into the databases. Recently, a database of isoelectric points for all proteins predicted using most of the available methods had been also developed.\nIn practice, a protein with an excess of basic aminoacids (arginine, lysine and/or histidine) will bear an isoelectric point roughly greater than 7 (basic), while a protein with an excess of acidic aminoacids (aspartic acid and/or glutamic acid) will often have an isoelectric point lower than 7 (acidic).\nThe electrophoretic linear (horizontal) separation of proteins by Ip along a pH gradient in a polyacrylamide gel (also known as isoelectric focusing), followed by a standard molecular weight linear (vertical) separation in a second polyacrylamide gel (SDS-PAGE), constitutes the so called two-dimensional gel electrophoresis or PAGE 2D. This technique allows a thorough separation of proteins as distinct \"spots\", with proteins of high molecular weight and low Ip migrating to the upper-left part of the bidimensional gel, while proteins with low molecular weight and high Ip locate to the bottom-right region of the same gel.\nCeramic materials.\nThe isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominant surface species is M-O\u2212, while at pH values below the IEP, M-OH2+ species predominate. Some approximate values of common ceramics are listed below:\n\"Note: The following list gives the isoelectric point at 25\u00a0\u00b0C for selected materials in water. The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. Moreover, the precise measurement of isoelectric points can be difficult, thus many sources often cite differing values for isoelectric points of these materials.\"\nMixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, a synthetically prepared amorphous aluminosilicate (Al2O3-SiO2) was initially measured as having IEP of 4.5 (the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value). Significantly higher IEP values (pH 6 to 8) have been reported for 3Al2O3-2SiO2 by others. Similarly, also IEP of barium titanate, BaTiO3 was reported in the range 5\u20136 while others got a value of 3. Mixtures of titania (TiO2) and zirconia (ZrO2) were studied and found to have an isoelectric point between 5.3\u20136.9, varying non-linearly with %(ZrO2). The surface charge of the mixed oxides was correlated with acidity. Greater titania content led to increased Lewis acidity, whereas zirconia-rich oxides displayed Br::onsted acidity. The different types of acidities produced differences in ion adsorption rates and capacities.\nVersus point of zero charge.\nThe terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.\nIn systems in which H+/OH\u2212 are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different from the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.\nAccording to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p\"K\"\u2212 and p\"K\"+ to define the two conditions in terms of the relative number of charged sites:\nformula_2\nFor large \u0394p\"K\" (&gt;4 according to Jolivet), the predominant species is MOH while there are relatively few charged species \u2013 so the PZC is relevant. For small values of \u0394p\"K\", there are many charged species in approximately equal numbers, so one speaks of the IEP.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15058", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15058", "title": "International reply coupon", "text": "Coupon exchangeable for postage stamps\nAn international reply coupon (IRC) is a coupon that can be exchanged for one or more postage stamps representing the minimum postage for an unregistered priority airmail letter sent to another Universal Postal Union (UPU) member country. IRCs are accepted by all UPU member countries. UPU number: CN01.\nUPU member postal services are obliged to exchange an IRC for postage, but are not obliged to sell them.\nThe purpose of the IRC is to allow a person to send someone in another country a letter, along with the cost of postage for a reply. If the addressee is within the same country, there is no need for an IRC because a self-addressed stamped envelope (SASE) or return postcard will suffice; but if the addressee is in another country an IRC removes the necessity of acquiring foreign postage or sending appropriate currency.\nDescription.\nInternational reply coupons (in French, \"Coupons-R\u00e9ponse Internationaux\") are printed in blue ink on paper that has the letters \"UPU\" in large characters in the watermark. The front of each coupon is printed in French. The reverse side of the coupon, which has text relating to its use, is printed in German, English, Arabic, Chinese, Spanish, and Russian. Under Universal Postal Union regulations, participating member countries are not required to place a control stamp or postmark on the international reply coupons that they sell. Therefore, some foreign issue reply coupons that are tendered for redemption may bear the name of the issuing country (generally in French) rather than the optional control stamp or postmark.\nThe Nairobi Model was an international reply coupon printed by the Universal Postal Union which is approximately 3.75 inches by 6 inches and had an expiration date of 31 December 2013. This model was designed by Rob Van Goor, a graphic artist from the Luxembourg Post. It was selected from among 10 designs presented by Universal Postal Union member countries. Van Goor interpreted the theme of the contest \u2013 \"The Postage Stamp: A Vehicle for Exchange\" \u2013 by depicting the world being cradled by a hand and the perforated outline of a postage stamp.\nThe Doha Model is named for the 25th UPU congress held in Doha, Qatar, in 2012. The Doha model, designed by Czech artist and graphic designer Michal Sindelar, shows cupped hands catching a stream of water, to celebrate the theme of Water for Life. It expires after 31 December 2017.\nThe Istanbul Model was designed by graphic artist Nguyen Du (Vietnam) and features a pair of hands and a dove against an Arctic backdrop representing sustainable development in the postal sector. Ten countries participated in the competition which was held 7 October 2016, during the UPU congress in Istanbul, Turkey. It expires after 31 December 2021.\nThe Abidjan Model, named for the 27th Congress held in Abidjan, Ivory Coast from 9 to 27 August 2021, was designed by graphic artist Valeryia Tsimakhavets (Belarus) and features a tree with new leaves and birds which represent the ecosystem and climate protection. This model was originally set to expire after 31 December 2025, but the expiration date was later extended to 31 December 2026.\nHistory.\nThe IRC was introduced in 1906 at a Universal Postal Union congress in Rome. At the time an IRC could be exchanged for a single-rate, ordinary postage stamp for surface delivery to a foreign country, as this was before the introduction of airmail services. An IRC is exchangeable in a UPU member country for the minimum postage of a priority or unregistered airmail letter to a foreign country.\nIRCs are ordered from the UPU headquarters in Bern, Switzerland by postal authorities. They are generally available at large post offices; in the U.S., they were requisitioned along with regular domestic stamps by any post office that had sufficient demand for them.\nPrices for IRCs vary by country. In the United States in November 2012, the purchase price was $2.20 USD; however, the US Postal Service discontinued sales of IRCs on 27 January 2013 due to declining demand. Britain's Royal Mail also stopped selling IRCs on 31 December 2011, citing minimal sales and claiming that the average post office sold less than one IRC per year. IRCs purchased in foreign countries may be used in the United States toward the purchase of postage stamps and embossed stamped envelopes at the current one-ounce First Class International rate (US$1.20 as of November 2020) per coupon.\nIRCs are often used by amateur radio operators sending QSL cards to each other; it has traditionally been considered good practice and common courtesy to include an IRC when writing to a foreign operator and expecting a reply by mail. If the operator's home country does not sell IRCs, then a foreign IRC may be used.\nPrevious editions of the IRC, the Beijing, Nairobi, Doha and Istanbul Models and all post-2000 versions, bear an expiration date. The current IRC, the Abidjan Model, will be valid until the end of 2026.\nCountry details.\nSince about 2010, several countries have ceased selling International Reply Coupons. Below is information about different countries.\nAustralia.\nAs of November 2025[ [update]], Australia Post still sells International Reply Coupons at a cost of $4.80 each.\nChina (Mainland).\nAs of 2024[ [update]], China Post sells an IRC for 12 CNY, and exchanges a valid IRC for 7.4 CNY worth of postage (including equivalent imprinted stamps on postal stationery).\nFinland.\nAs of November 2024[ [update]], Posti sells International Reply Coupons for 3.75 EUR, and exchanges a valid one for 2.75 EUR worth of stamps.\nFrance.\nAs of 2024[ [update]], La Poste sells an IRC for 1.96 EUR.\nGermany.\nDeutsche Post discontinued IRCs by July 1, 2025.\nHong Kong.\nAs of December 2022[ [update]], International reply coupons are sold by the HongKong Post for 19 HKD, and exchanges a valid IRC for 5.5 HKD worth of postage.\nItaly.\nPoste Italiane is slowly phasing out the sales of IRC. The last series, named \"Istanbul\", was issued in 2017 in only 10,000 pieces, which expired at the end of 2021.\nJapan.\nAfter October 2023, The Japan Post sells an IRC for 180 JPY, and exchanges a valid IRC for 160 JPY worth of stamps.\n160 JPY is enough to deliver a letter all over the world by airmail (120 JPY by shipmail).\nLuxembourg.\nAs of 2024[ [update]], Post Luxembourg sells an IRC for 2.20 EUR.\nMacao.\nAs of 2024[ [update]], Macau Post and Telecommunications (CTT) sells an IRC for 12 MOP, and exchanges a valid IRC for 6 MOP worth of postage, with a cost of redeeming stamps for 5 MOP.&lt;ref name=\"\u7b2c62/2005\u865f\u884c\u653f\u547d\u4ee4\"&gt;&lt;/ref&gt;\nNorway.\nAs of March 2024[ [update]], Posten sells IRCs online, and at their 30 remaining post offices. The cost is 38 NOK per IRC.\nSingapore.\nSingapore Post sells IRCs at its GPO in Paya Lebar. IRCs are sold for S$2.50, and can be redeemed for S$1.40.\nSwitzerland.\nInternational reply coupons are sold by the Swiss Post in packs of 10 for 30 CHF.\nTaiwan.\nAfter the adoption of United Nations General Assembly Resolution 2758, the Republic of China (Taiwan) lost its membership in the Universal Postal Union starting from April 13, 1972, and Chunghwa Post no longer issues IRCs since then. As of 2024[ [update]], they only accept and exchange valid Japanese IRCs, for 13 TWD worth of stamps per Japanese IRC.\nVietnam.\nAs of 2025[ [update]], Vietnam Post sells an IRC for 27,000 VND. \nThailand.\nThailand Post sells an IRC for 53 THB as of 2020.\nTurkey.\nAs of 2022[ [update]], Turkish Post only sells IRCs through their administration branches.\nTurkish Post exchanges a valid IRC for 10 TRY worth of stamps as of 2021. 10 TRY is not enough to deliver a letter all over the world. (E.g. a postcard to Japan requires 21 TRY.)\nUnited Kingdom.\nThe Royal Mail stopped selling IRCs on 31 December 2011 due to a lack of demand.\nUnited States.\nThe United States Postal Service stopped selling international reply coupons on 27 January 2013.\nThe Ponzi scheme.\nIn 1920, Charles Ponzi made use of the idea that profit could be made by taking advantage of the differing postal rates in different countries to buy IRCs cheaply in one country and exchange them for stamps of a higher value in another country. His attempts to raise money for this venture became instead the fraudulent Ponzi scheme.\nIn practice, the overhead on buying and selling large numbers of the very low-value IRCs precluded any profitability. The selling price and exchange value in stamps in each country have been adjusted to some extent to remove some of the potential for profit, but ongoing fluctuations in currency value and exchange rates make it impossible to achieve this completely, as long as stamps represent a specific currency value, instead of acting as vouchers granting specific postal services, devoid of currency nomination.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15059", "revid": "2842084", "url": "https://en.wikipedia.org/wiki?curid=15059", "title": "Isaac Bonewits", "text": "American Neopagan leader and writer (1949\u20132012)\nPhillip Emmons Isaac Bonewits (October 1, 1949 \u2013 August 12, 2010) was an American Neo-Druid who wrote a number of books on the subject of Neopaganism and magic. Bonewits was a public speaker, liturgist, singer and songwriter, and founder of the Neopagan organizations \u00c1r nDra\u00edocht F\u00e9in (ADF) and the Aquarian Anti-Defamation League. \nEarly life and education.\nBonewits was born on October 1, 1949, in Royal Oak, Michigan, as the fourth of five children. His father was a Presbyterian while his mother a Catholic. Spending much of his childhood in Ferndale, Michigan, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966 and graduated in 1970 with a Bachelor of Arts in magic, perhaps becoming the first and only person known to have ever received any kind of academic degree in magic from an accredited university.\nIn 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America (RDNA). Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18-year-old Bonewits was also recruited by the Church of Satan, but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary \"Satanis: The Devil's Mass\". Bonewits, in his article \"My Satanic Adventure\", asserts that the rituals in \"Satanis\" were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.\nCareer.\n1970s: writer and editor.\nBonewits' first book, \"Real Magic\", was published in 1971. Between 1973 and 1975 Bonewits was employed as the editor of \"Gnostica\" magazine in Minnesota (published by Llewellyn Publications). He established an offshoot group of the Reformed Druids of North America (RDNA) called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite, in his words, his \"lifelong status as a gentile\"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.\nIn 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.\n1980s: founding of \u00c1r nDra\u00edocht F\u00e9in.\nThroughout his life Bonewits had varying degrees of involvement with occult groups including Gardnerian Wicca and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn). Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book \"Authentic Thaumaturgy\" to gamers as a way of organizing \"Dungeons &amp; Dragons\" games.\nIn 1983, Bonewits founded \u00c1r nDra\u00edocht F\u00e9in (also known as \"A Druid Fellowship\" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization. Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.\nMusician and activist.\nA songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).\nBonewits encouraged charity programs to help Neopagan seniors, and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.\nPersonal life.\nBonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years. Paperwork and legalities caught up on December 31, 2007, making them legally married.\nBonewits' only child was born to Lipp in 1990.\nIllness and death.\nIn 1990, Bonewits was diagnosed with eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.\nOn October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer, for which he underwent treatment. He died at home, on August 12, 2010, surrounded by his family.\nAccusations of sexual assault.\nIn 2018, accusations of sexual abuse against a minor rose against ADF founder Bonewits relating to his relationship with Moira Greyland when she was six years old. Greyland said in her book, 'The Last Closet: the Dark Side of Avalon':\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Some people called him the Pagan pope [\u2026] I hated Isaac, and refused to be in the same room with him, even if the only way I could articulate my objections to him was to say 'he tickled me'.\nIn light of this accusation, ADF, the lead pagan organization that Issac Bonewits founded, removed his name from their website and repudiated him.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;To preserve the health of our organization, we must cut out the blight that is Isaac Bonewits\u2019 legacy. We sever the ties both historical and spiritual that bind us to him. For his actions against children, Isaac Bonewits will no longer be named as a beloved ancestor of ADF, nor is he welcome at our sacred fire.\nMay his memory and his dark actions fade with the rising of the sun.\nContributions to Neopaganism.\nIn his book \"Real Magic\" (1971), Bonewits proposed his \"Laws of Magic\". These \"laws\" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970s to be part of his publishing project \"Library of the Occult\".\nBonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15060", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=15060", "title": "Isaac Bonewits laws of magic", "text": ""}
{"id": "15062", "revid": "1782949", "url": "https://en.wikipedia.org/wiki?curid=15062", "title": "Intel 8080", "text": "8-bit microprocessor\nThe Intel 8080 is Intel's second 8-bit microprocessor. Introduced in April 1974, the 8080 was an enhanced non-binary compatible successor to the earlier Intel 8008 microprocessor. Originally intended for use in embedded systems such as calculators, cash registers, computer terminals, and industrial robots, its performance soon led to adoption in a broader range of systems, ultimately launching the microcomputer industry.\nSeveral key design choices contributed to the 8080\u2019s success. Its 40\u2011pin package simplified interfacing compared to the 8008\u2019s 18\u2011pin design, enabling a more efficient data bus. The transition to NMOS technology provided faster transistor speeds than the 8008's PMOS, also making it TTL compatible. An expanded instruction set and a full 16-bit address bus allowed the 8080 to access up to 64\u00a0KB of memory, quadrupling the capacity of its predecessor. A broader selection of support chips further enhanced its functionality. Many of these improvements stemmed from customer feedback, as designer Federico Faggin and others at Intel heard from industry about shortcomings in the 8008 architecture.\nThe 8080 found its way into early personal computers such as the Altair 8800 and subsequent S-100 bus systems, and it served as the original target CPU for the CP/M operating system. It directly influenced the later x86 architecture which was designed so that its assembly language closely resembled that of the 8080, permitting many instructions to map directly from one to the other.\nOriginally operating at a clock rate of 2\u00a0MHz, with common instructions taking between 4 and 11 clock cycles, the 8080 was capable of executing several hundred thousand instructions per second. Later, two faster variants, the 8080A-1 and 8080A-2, offered improved clock speeds of 3.125\u00a0MHz and 2.63\u00a0MHz, respectively. In most applications, the processor was paired with two support chips, the 8224 clock generator/driver and the 8228 bus controller, to manage its timing and data flow.\nHistory.\nMicroprocessor customers were reluctant to adopt the 8008 because of limitations such as the single addressing mode, low clock speed, low pin count, and small on-chip stack, which restricted the scale and complexity of software. There were several proposed designs for the 8080, ranging from simply adding stack instructions to the 8008 to a complete departure from all previous Intel architectures. The final design was a compromise between the proposals.\nThe conception of the 8080 began in the summer of 1971, when Intel wrapped up development of the 4004 and were still working on the 8008. After rumors about the \"CPU on a chip\" came out, Intel started to see interest in the microprocessor from all sorts of customers. At the same time, Federico Faggin \u2013 who led the design of the 4004 and became the primary architect of the 8080 \u2013 was giving some technical seminars on both of the aforementioned microprocessors and visiting customers. He found that they were complaining about the architecture and performance of said microprocessors, especially the 8008 \u2013 as its speed at 0.5 MHz was \"not adequate.\"\nFaggin later proposed the chip to Intel's management and pushed for its implementation in the spring of 1972, as development of the 8008 was wrapping up. However, much to his surprise and frustration, Intel didn't approve the project. Faggin says that Intel wanted to see how the market would react to the 4004 and 8008 first, while others noted the problems Intel was having getting its latest generation of memory chips out the door and wanted to focus on that. As a result, Intel didn't approve of the project until fall of that year. Faggin hired Masatoshi Shima, who helped design the logic of the 4004 with him, from Japan in November 1972. Shima did the detailed design under Faggin's direction, using the design methodology for random logic with silicon gate that Faggin had created for the 4000 family and the 8008. \nThe 8080 was explicitly designed to be a general-purpose microprocessor for a larger number of customers. Much of the development effort was spent trying to integrate the functionalities of the 8008's supplemental chips into one package. It was decided early in development that the 8080 was not to be binary-compatible with the 8008, instead opting for source compatibility once run through a transpiler, to allow new software to not be subject to the same restrictions as the 8008. For the same reason, as well as to expand the capabilities of stack-based routines and interrupts, the stack was moved to external memory. \nNoting the specialized use of general-purpose registers by programmers in mainframe systems, Faggin with Shima and Stanley Mazor decided the 8080's registers would be specialized, with register pairs having a different set of uses. This also allowed the engineers to more effectively use transistors for other purposes. \nShima finished the layout in August 1973. Production of the chip later began in December of that year. After the development of NMOS logic fabrication, a prototype of the 8080 was completed in January 1974. It had a flaw, in that driving with standard TTL devices increased the ground voltage because high current flowed into the narrow line. Intel had already produced 40,000 units of the 8080 at the direction of the sales section before Shima characterized the prototype. After working out some typical last-minute issues, Intel introduced the product in March 1974. It was released a month later as requiring Low-power Schottky TTL (LS TTL) devices. The 8080A fixed this flaw.\nIntel offered an instruction set simulator for the 8080 named INTERP/80 to run compiled PL/M programs. It was written in FORTRAN IV by Gary Kildall while he worked as a consultant for Intel.\nThere is only one patent on the 8080 with the following names: Federico Faggin, Masatoshi Shima, Stanley Mazor.\nDescription.\nProgramming model.\nThe Intel 8080 is the successor to the 8008. It uses the same basic instruction set and register model as the 8008, although it is neither source code compatible nor binary code compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080. The 8080 also adds 16-bit operations in its instruction set. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 has addressing modes to directly access its full 16-bit memory space. The internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack-pointer (SP) register. The 8080's 40-pin DIP packaging provides a 16-bit address bus and an 8-bit data bus which more efficiently access 64\u00a0KiB (216 bytes) of memory.\nRegisters.\nThe processor has seven 8-bit registers (A, B, C, D, E, H, and L), where A is the primary 8-bit accumulator. The other six registers can be used as either individual 8-bit registers or in three 16-bit register pairs (BC, DE, and HL, referred to as B, D and H in Intel documents) depending on the particular instruction. Some instructions can also use the HL register pair as a (limited) 16-bit accumulator. A pseudo-register M, which refers to the dereferenced memory location pointed to by HL, can be used almost anywhere other registers can be used. The 8080 has a 16-bit stack pointer to memory, replacing the 8008's internal stack, and a 16-bit program counter.\nFlags.\nThe processor maintains internal flag bits (a status register), which indicate the results of arithmetic and logical instructions. Only certain instructions affect the flags. The flags are:\nThe carry bit can be set or complemented by specific instructions. Conditional-branch instructions test the various flag status bits. The accumulator and the flags together are called the PSW, or program status word. PSW can be pushed to or popped from the stack.\nCommands, instructions.\nAs with many other 8-bit processors, all instructions are encoded in one byte (including register numbers, but excluding immediate data), for simplicity. Some can be followed by one or two bytes of data, which can be an immediate operand, a memory address, or a port number. Like more advanced processors, it has automatic CALL and RET instructions for multi-level procedure calls and returns (which can even be conditionally executed, like jumps) and instructions to save and restore any 16-bit register pair on the machine stack. Eight one-byte call instructions () for subroutines exist at the fixed addresses 00h, 08h, 10h, ..., 38h. These are intended to be supplied by external hardware in order to invoke a corresponding interrupt service routine, but are also often employed as fast system calls. The slowest instruction is , which exchanges the register pair HL with the last item pushed on the stack.\n8-bit instructions.\nAll 8-bit operations with two operands can only be performed on the 8-bit accumulator (the A register). The other operand can be either an immediate value, another 8-bit register, or a memory byte addressed by the 16-bit register pair HL. Increments and decrements can be performed on any 8 bit register or an HL-addressed memory byte. Direct copying is supported between any two 8-bit registers and between any 8-bit register and an HL-addressed memory byte. Due to the regular encoding of the instruction (using a quarter of available opcode space), there are redundant codes to copy a register into itself (, for instance), which are of little use, except for delays. However, the systematic opcode for is instead used to encode the halt () instruction, halting execution until an external reset or interrupt occurs.\n16-bit operations.\nAlthough the 8080 is generally an 8-bit processor, it has limited abilities to perform 16-bit operations. Any of the three 16-bit register pairs (BC, DE, or HL, referred to as B, D, H in Intel documents) or SP can be loaded with an immediate 16-bit value (using ), incremented or decremented (using and ), or added to HL (using ). By adding HL to itself, it is possible to achieve the same result as a 16-bit arithmetical left shift with one instruction. The only 16-bit instructions that affect any flag is , which sets the CY (carry) flag in order to allow for programmed 24-bit or 32-bit arithmetic (or larger), needed to implement floating-point arithmetic. BC, DE, HL, or PSW can be copied to and from the stack using and . A stack frame can be allocated using and . A branch to a computed pointer can be executed with . loads HL from directly addressed memory and stores HL likewise. The instruction exchanges the values of the HL and DE register pairs. exchanges last item pushed on stack with HL. None of these 16-bit operations were supported on the earlier Intel 8008.\nInput/output scheme.\nInput output port space.\nThe 8080 supports 256 input/output (I/O) ports, accessed via dedicated I/O instructions taking port addresses as operands. This I/O mapping scheme is regarded as an advantage, as it frees up the processor's limited address space. Many CPU architectures instead use so-called memory-mapped I/O (MMIO), in which a common address space is used for both RAM and peripheral chips. This removes the need for dedicated I/O instructions, although a drawback in such designs may be that special hardware must be used to insert wait states, as peripherals are often slower than memory. However, in some simple 8080 computers, I/O is indeed addressed as if they were memory cells, \"memory-mapped\", leaving the I/O commands unused. I/O addressing can also sometimes employ the fact that the processor outputs the same 8-bit port address to both the lower and the higher address byte (i.e., would put the address 0505h on the 16-bit address bus). Similar I/O-port schemes are used in the backward-compatible Zilog Z80 and Intel 8085, and the closely related x86 microprocessor families.\nSeparate stack space.\nOne of the bits in the processor state word (see below) indicates that the processor is accessing data from the stack. Using this signal, it is possible to implement a separate stack memory space. This feature is seldom used.\nStatus word.\nFor more advanced systems, during the beginning of each machine cycle, the processor places an eight bit status word on the data bus. This byte contains flags that determine whether the memory or I/O port is accessed and whether it is necessary to handle an interrupt.\nThe interrupt system state (enabled or disabled) is also output on a separate pin. For simple systems, where the interrupts are not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio-86RK computer made in the Soviet Union, for instance).\nInterrupts.\nHardware interrupts are initiated by asserting the interrupt request (INT) pin. At the next opcode fetch cycle (M1), the interrupt will be acknowledged with the INTA state code. At this time, an instruction is \"jammed\" (Intel's word) by external hardware on the data bus. This can be a one-byte codice_1 instruction, or if using an Intel 8259, a codice_2 instruction. Interrupts may be enabled and disabled with codice_3 and codice_4 instructions, respectively. Interrupts are disabled after an INTA; they must be re-enabled explicitly by the interrupt service routine. The 8080 does not support non-maskable interrupts.\nExample code.\nThe following 8080/8085 assembler source code is for a subroutine named that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.\nPin use.\nThe address bus has its own 16 pins, and the data bus has 8 pins that are usable without any multiplexing. Using the two additional pins (read and write signals), it is possible to assemble simple microprocessor devices very easily. Only the separate IO space, interrupts, and DMA need added chips to decode the processor pin signals. However, the pin load capacity is limited; even simple computers often require bus amplifiers.\nThe processor needs three power sources (\u22125, +5, and +12\u00a0V) and two non-overlapping high-amplitude synchronizing signals. However, at least the late Soviet version \u041a\u0420580\u0412\u041c80\u0410 was able to work with a single +5\u00a0V power source, the +12\u00a0V pin being connected to +5\u00a0V and the \u22125\u00a0V pin to ground.\nThe pin-out table, from the chip's accompanying documentation, describes the pins as follows:\nSupport chips.\nA key factor in the success of the 8080 was the broad range of support chips available, providing serial communications, counter/timing, input/output, direct memory access, and programmable interrupt control amongst other functions:\nPhysical implementation.\nThe 8080 integrated circuit has an NMOS design, which employed non\u2011saturated enhancement mode transistors as loads, which demanded supplementary voltage levels (+12\u00a0V and \u22125\u00a0V) alongside the standard TTL compatible +5\u00a0V.\nIt was manufactured in a silicon gate process using a minimal feature size of 6\u00a0\u03bcm. A single layer of metal is used to interconnect the approximately 4,500 transistors in the design, but the higher resistance polysilicon layer, which required higher voltage for some interconnects, is implemented with transistor gates. The die size is approximately 20\u00a0mm2.\nCommercial impact.\nApplications and successors.\nThe 8080 was used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more able, Zilog Z80 processor would capitalize on this, with Z80 and CP/M becoming the dominant CPU and OS combination of the period c.\u20091976 to 1983 much like x86 and MS-DOS a decade later).\nIn 1979, even after the introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 each.\nThe first single-board microcomputers, such as MYCRO-1 and the \"dyna-micro\" / MMD-1 (see: Single-board computer) were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, California, in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 is as the \"brain\" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which takes large amounts of user data from reel-to-reel tape and images it onto microfiche. The Auto-COM instruments also include an entire automated film cutting, processing, washing, and drying sub-system.\nSeveral early arcade video games were built around the 8080 microprocessor. The first commercially available arcade video game to incorporate a microprocessor was \"Gun Fight\", Midway Games' 8080-based reimplementation of Taito's discrete-logic \"Western Gun\", which was released in November 1975. (A pinball machine which incorporated a Motorola 6800 processor, \"The Spirit of '76\", had already been released the previous month.) The 8080 was then used in later Midway arcade video games and in Taito's 1978 \"Space Invaders\", one of the most successful and well-known of all arcade video games.\nZilog introduced the Z80, which has a compatible machine language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085.\nLater, Intel issued the assembly-language compatible (but not binary-compatible) 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made the NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supports an 8080 emulation mode. This is also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its instruction set architecture (ISA), made a lasting impact on computer history.\nA number of processors compatible with the Intel 8080A were manufactured in the Eastern Bloc: the KR580VM80A (initially marked as \u041a\u0420580\u0418\u041a80) in the Soviet Union, the MCY7880 made by Unitra CEMI in Poland, the MHB8080A made by TESLA in Czechoslovakia, the 8080APC made by Tungsram / MEV in Hungary, and the MMN8080 made by Microelectronica Bucharest in Romania.\n, the 8080 is still in production at Lansdale Semiconductors.\nIndustry change.\nThe 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett-Packard, or IBM. A manufacturer would produce the whole computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was designed for almost any application \"except\" a complete computer system. Hewlett-Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 is a terminal which runs the programming language BASIC on the 8080. Microsoft's founding product, Microsoft BASIC, was originally programmed for the 8080.\nThe 8080 and 8085 gave rise to the 8086, which was designed as a source code compatible, albeit not binary compatible, extension of the 8080. This design, in turn, later spawned the x86 family of chips, which continue to be Intel's primary line of processors. Many of the 8080's core machine instructions and concepts survive in the widespread x86 platform. Examples include the registers named \"A\", \"B\", \"C\", and \"D\" and many of the flags used to control conditional jumps. 8080 assembly code can still be directly translated into x86 instructions, since all of its core elements are still present.\nUS Patent.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;, Federico Faggin, Masatoshi Shima, Stanley Mazor\n , \"MOS computer employing a plurality of separate chips\", issued \u00a0. This patent contains three claims. The first two relate to the status word multiplexed onto the data bus. The third claim is for the instruction which can be invoked by pulling the data bus high. The prior art 8008 required more complicated instruction jamming circuitry.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15063", "revid": "1782949", "url": "https://en.wikipedia.org/wiki?curid=15063", "title": "Intel 8086", "text": "16-bit microprocessor\nThe 8086 (also called iAPX 86) is a 16-bit microprocessor chip released by Intel on June 8, 1978 after development began in early 1976. It was followed by the Intel 8088 in 1979, which was a slightly modified chip with an external 8-bit data bus (allowing the use of cheaper and fewer supporting ICs).\nThe 8086 gave rise to the x86 architecture, which eventually became Intel's most successful line of processors. On June 5, 2018, Intel released a limited-edition CPU celebrating the 40th anniversary of the Intel 8086, called the Intel Core i7-8086K.\nHistory.\nBackground.\nIn 1972, Intel launched the 8008, Intel's first 8-bit microprocessor. It implemented an instruction set designed by Datapoint Corporation with programmable CRT terminals in mind, which also proved to be fairly general-purpose. The device needed several additional ICs to produce a functional computer, in part due to it being packaged in a small 18-pin \"memory package\", which ruled out the use of a separate address bus (Intel was primarily a DRAM manufacturer at the time).\nTwo years later, Intel launched the 8080, employing the new 40-pin DIL packages originally developed for calculator ICs to enable a separate address bus. It had an extended instruction set that is source-compatible (not binary compatible) with the 8008 and also included some 16-bit instructions to make programming easier. The 8080 device was eventually replaced by the depletion-load-based 8085 (1977), which used a single +5\u00a0V power supply instead of the three different operating voltages of earlier chips. Other well known 8-bit microprocessors that emerged during these years are Motorola 6800 (1974), General Instrument PIC16X (1975), MOS Technology 6502 (1975), Zilog Z80 (1976), and Motorola 6809 (1978).\nThe first x86 design.\nThe 8086 project started in May 1976 and was originally intended as a temporary substitute for the ambitious and delayed iAPX 432 project. It was an attempt to draw attention from the less-delayed 16-bit and 32-bit processors of other manufacturers \u2014 Motorola, Zilog, and National Semiconductor.\nWhile the 8086 was a 16-bit microprocessor, it used a similar architecture as Intel's 8-bit microprocessors (8008, 8080, and 8085). This allowed assembly language programs written in 8-bit to seamlessly migrate. New instructions and features \u2014 such as signed integers, base+offset addressing, and self-repeating operations \u2014 were added. Instructions were added to assist source code compilation of nested functions in the ALGOL-family of languages, including Pascal and PL/M. According to principal architect Stephen P. Morse, this was a result of a more software-centric approach. Other enhancements included microcode instructions for the multiply and divide assembly language instructions. Designers also anticipated coprocessors, such as 8087 and 8089, so the bus structure was designed to be flexible.\nThe first revision of the instruction set and high level architecture was ready after about three months, and as almost no CAD tools were used, four engineers and 12\u00a0layout people were simultaneously working on the chip. The 8086 took a little more than two years from idea to working product, which was considered fast for a complex design in the 1970s.\nThe 8086 was sequenced using a mixture of random logic and microcode and was implemented using depletion-load nMOS circuitry with approximately 20,000\u00a0active transistors (29,000 counting all ROM and PLA sites). It was soon moved to a new refined nMOS manufacturing process called HMOS (for High performance MOS) that Intel originally developed for manufacturing of fast static RAM products. This was followed by HMOS-II, HMOS-III versions, and, eventually, a fully static CMOS version for battery powered devices, manufactured using Intel's CHMOS processes. The original chip measured 33\u00a0mm\u00b2 and minimum feature size was 3.2\u00a0\u03bcm. The MUL and DIV instructions were very slow due to being microcoded so x86 programmers usually just used the bit shift instructions for multiplying and dividing instead. \nThe 8086 was die-shrunk to 2 \u03bcm in 1981; this version also corrected a stack register bug in the original 3.5 \u03bcm chips. Later 1.5 \u03bcm and CMOS variants were outsourced to other manufacturers and not developed in-house.\nThe architecture was defined by Stephen P. Morse with some help from Bruce Ravenel (the architect of the 8087) in refining the final revisions. Logic designer Jim McKevitt and John Bayliss were the lead engineers of the hardware-level development team and Bill Pohlman the manager for the project. The legacy of the 8086 is enduring in the basic instruction set of today's personal computers and servers; the 8086 also lent its last two digits to later extended versions of the design, such as the Intel 286 and the Intel 386, all of which eventually became known as the x86 family. In addition, the PCI Vendor ID for system devices produced by Intel is 8086.\nDetails.\nBuses and operation.\nAll internal registers, as well as internal and external data buses, are 16\u00a0bits wide, which firmly established the \"16-bit microprocessor\" identity of the 8086. A 20-bit external address bus provides a 1\u00a0MiB physical address space (220 = 1,048,576 x 1 byte). This address space is addressed by means of internal memory \"segmentation\". The data bus is multiplexed with the address bus in order to fit all of the control lines into a standard 40-pin dual in-line package. It provides a 16-bit I/O address bus, supporting 64\u00a0KB of separate I/O space. The maximum linear address space is limited to 64\u00a0KB, simply because internal address/index registers are only 16\u00a0bits wide. Programming over 64\u00a0KB memory boundaries involves adjusting the segment registers (see below); this difficulty existed until the 80386 architecture introduced wider (32-bit) registers (the memory management hardware in the 80286 did not help in this regard, as its registers are still only 16 bits wide).\nHardware modes of 8086.\nSome of the control pins, which carry essential signals for all external operations, have more than one function depending upon whether the device is operated in \"min\" or \"max\" mode. The former mode is intended for small single-processor systems, while the latter is for medium or large systems using more than one processor (a kind of multiprocessor mode). Maximum mode is required when using an 8087 or 8089 coprocessor. The voltage on pin 33 (MN/MX) determines the mode. Changing the state of pin 33 changes the function of certain other pins, most of which have to do with how the CPU handles the (local) bus. The mode is usually hardwired into the circuit and therefore cannot be changed by software. The workings of these modes are described in terms of timing diagrams in Intel datasheets and manuals. In minimum mode, all control signals are generated by the 8086 itself.\nRegisters and instruction.\nThe 8086 has eight more-or-less general 16-bit registers (including the stack pointer but excluding the instruction pointer, flag register and segment registers). Four of them, AX, BX, CX, DX, can also be accessed as 8-bit register pairs (see figure) while the other four, SI, DI, BP, SP, are 16-bit only.\nDue to a compact encoding inspired by 8-bit processors, most instructions are one-address or two-address operations, which means that the result is stored in one of the operands. At most one of the operands can be in memory, but this memory operand can also be the \"destination\", while the other operand, the \"source\", can be either \"register\" or \"immediate\". A single memory location can also often be used as both \"source\" and \"destination\" which, among other factors, further contributes to a code density comparable to (and often better than) most eight-bit machines at the time.\nThe degree of generality of most registers is much greater than in the 8080 or 8085. However, 8086 registers were more specialized than in most contemporary minicomputers and are also used implicitly by some instructions. While perfectly sensible for the assembly programmer, this makes register allocation for compilers more complicated compared to more orthogonal 16-bit and 32-bit processors of the time such as the PDP-11, VAX, 68000, 32016, etc. On the other hand, being more regular than the rather minimalistic but ubiquitous 8-bit microprocessors such as the 6502, 6800, 6809, 8085, MCS-48, 8051, and other contemporary accumulator-based machines, it is significantly easier to construct an efficient code generator for the 8086 architecture.\nAnother factor for this is that the 8086 also introduced some new instructions (not present in the 8080 and 8085) to better support stack-based high-level programming languages such as Pascal and PL/M; some of the more useful instructions are codice_1, and ret \"size\", supporting the \"Pascal calling convention\" directly. (Several others, such as codice_2 and codice_3, were added in the subsequent 80186, 80286, and 80386 processors.)\nA 64\u00a0KB (one segment) stack growing towards lower addresses is supported in hardware; 16-bit words are pushed onto the stack, and the top of the stack is pointed to by SS:SP. There are 256\u00a0interrupts, which can be invoked by both hardware and software. The interrupts can cascade, using the stack to store the return addresses.\nThe 8086 has 64\u00a0K of 8-bit (or alternatively 32\u00a0K of 16-bit word) I/O port space.\nFlags.\nThe 8086 has a 16-bit flags register. Nine of these condition code flags are active, and indicate the current state of the processor: Carry flag (CF), Parity flag (PF), Auxiliary carry flag (AF), Zero flag (ZF), Sign flag (SF), Trap flag (TF), Interrupt flag (IF), Direction flag (DF), and Overflow flag (OF).\nAlso referred to as the status word, the layout of the flags register is as follows:\nSegmentation.\nThere are also four 16-bit segment registers (see figure) that allow the 8086 CPU to access one megabyte of memory in an unusual way. Rather than concatenating the segment register with the address register, as in most processors whose address space exceeds their register size, the 8086 shifts the 16-bit segment four bits left before adding it to the 16-bit offset (16\u00d7segment + offset), therefore producing a 20-bit external (or effective or physical) address from the 32-bit segment:offset pair. As a result, any external address could be referred to by up to 212 = 4096 different segment:offset pairs.\nAlthough considered complicated and cumbersome by many programmers, this scheme also has advantages; a small program (less than 64\u00a0KB) can be loaded starting at a fixed offset (such as 0000) in its own segment, avoiding the need for relocation, with at most 15\u00a0bytes of alignment waste.\nCompilers for the 8086 family commonly support two types of pointer, \"near\" and \"far\". Near pointers are 16-bit offsets implicitly associated with the program's code or data segment and so can be used only within parts of a program small enough to fit in one segment. Far pointers are 32-bit segment:offset pairs resolving to 20-bit external addresses. Some compilers also support \"huge\" pointers, which are like far pointers except that pointer arithmetic on a huge pointer treats it as a linear 20-bit pointer, while pointer arithmetic on a far pointer wraps around within its 16-bit offset without touching the segment part of the address.\nTo avoid the need to specify \"near\" and \"far\" on numerous pointers, data structures, and functions, compilers also support \"memory models\" which specify default pointer sizes. The \"tiny\" (max 64K), \"small\" (max 128K), \"compact\" (data &gt; 64K), \"medium\" (code &gt; 64K), \"large\" (code,data &gt; 64K), and \"huge\" (individual arrays &gt; 64K) models cover practical combinations of near, far, and huge pointers for code and data. The \"tiny\" model means that code and data are shared in a single segment, just as in most 8-bit based processors, and can be used to build \".com\" files for instance. Precompiled libraries often come in several versions compiled for different memory models.\nAccording to Morse et al.. the designers actually contemplated using an 8-bit shift (instead of 4-bit), in order to create a 16\u00a0MB physical address space. However, as this would have forced segments to begin on 256-byte boundaries, and 1\u00a0MB was considered very large for a microprocessor around 1976, the idea was dismissed. Also, there were not enough pins available on a low cost 40-pin package for the additional four address bus pins.\nIn principle, the address space of the x86 series \"could\" have been extended in later processors by increasing the shift value, as long as applications obtained their segments from the operating system and did not make assumptions about the equivalence of different segment:offset pairs. In practice the use of \"huge\" pointers and similar mechanisms was widespread and the flat 32-bit addressing made possible with the 32-bit offset registers in the 80386 eventually extended the limited addressing range in a more general way.\nThe instruction stream is fetched from memory as words and is addressed internally by the processor to the byte level as necessary. An instruction stream queuing mechanism allows up to 6 bytes of the instruction stream to be queued while waiting for decoding and execution. The queue acts as a first-in-first-out (FIFO) buffer, from which the Execution Unit (EU) extracts instruction bytes as required. Whenever there is space for at least two bytes in the queue, the BIU will attempt a word fetch memory cycle. If the queue is empty (following a branch instruction, for example), the first byte into the queue immediately becomes available to the EU.\nPorting older software.\nSmall programs could ignore the segmentation and just use plain 16-bit addressing. This allows 8-bit software to be quite easily ported to the 8086. The authors of most DOS implementations took advantage of this by providing an Application Programming Interface very similar to CP/M as well as including the simple \".com\" executable file format, identical to CP/M. This was important when the 8086 and MS-DOS were new, because it allowed many existing CP/M (and other) applications to be quickly made available, greatly easing acceptance of the new platform.\nInterrupts.\nInterrupts on the 8086 are can be either software or hardware-initiated. Interrupts are long calls that also save the processor status. Interrupt routines typically end with a codice_4 instruction. All interrupts have a 8-bit interrupt number associated with them. This number is used to look up a segment:offset in a 256 element interrupt vector table stored at addresses 0-3FFH. When any type of interrupt is encountered, the processor status is pushed, CS and IP are pushed, and the interrupt number is multiplied by four to index a new execution address which is loaded from the vector table. \nThere are three types of software interrupt instructions: codice_5, codice_6, and a single-byte codice_7 used for debugging.\nThere are two kinds of hardware interrupts: maskable and non-maskable.\nNon-maskable interrupts are higher priority than maskable interrupts. They cannot be disabled by interrupt enable. A low to high transition on the NMI pin essentially causes an codice_8 to execute.\nMaskable interrupts are enabled and disabled by the codice_9 and codice_10 instructions respectively. When the INTR is asserted by a hardware device, the 8086 asserts INTA twice, reading an 8-bit interrupt number from the bus. This number is multiplied by four to point to the associated interrupt service routine address in the vector table. Maskable interrupts are disabled when INTA is asserted, but are re-enabled upon executing the codice_4 instruction at the end of the interrupt service routine.\nExample code.\nThe following 8086 assembly source code is for a subroutine named codice_12 that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time.\nThe example code uses the BP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory.\nPerformance.\nAlthough partly shadowed by other design choices in this particular chip, the multiplexed address and data buses limit performance slightly; transfers of 16-bit or 8-bit quantities are done in a four-clock memory access cycle, which is faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions vary from one to six bytes, fetch and execution are made concurrent and decoupled into separate units (as it remains in today's x86 processors): The \"bus interface unit\" feeds the instruction stream to the \"execution unit\" through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full-width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, as well as other enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).\nAs can be seen from these tables, operations on registers and immediates were fast (between 2 and 4 cycles), while memory-operand instructions and jumps were quite slow; jumps took more cycles than on the simple 8080 and 8085, and the 8088 (used in the IBM PC) was additionally hampered by its narrower bus. The reasons why most memory related instructions were slow were threefold:\nHowever, memory access performance was drastically enhanced with Intel's next generation of 8086 family CPUs. The 80186 and 80286 both had dedicated address calculation hardware, saving many cycles, and the 80286 also had separate (non-multiplexed) address and data buses.\nFloating point.\nThe 8086/8088 could be connected to a mathematical coprocessor to add hardware/microcode-based floating-point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek (\"not\" 8087-compatible) eventually came up with high-performance floating-point coprocessors that competed with the 8087.\nChip versions.\nThe clock frequency was originally limited to 5\u00a0MHz, but the last versions in HMOS were specified for 10\u00a0MHz. HMOS-III and CMOS versions were manufactured for a long time (at least a while into the 1990s) for embedded systems, although its successor, the 80186/80188 (which includes some on-chip peripherals), has been more popular for embedded use.\nThe 80C86, the CMOS version of the 8086, was used in many portable computers and embedded systems, including the GridPad, Toshiba T1200, HP 110, and finally the 1998\u20131999 Lunar Prospector.\nFor the packaging, the Intel 8086 was available both in ceramic and plastic DIP packages.\nList of Intel 8086.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nDerivatives and clones.\nCompatible\u2014and, in many cases, enhanced\u2014versions were manufactured by Fujitsu, Harris/Intersil, OKI, Siemens, Texas Instruments, NEC, Mitsubishi, and AMD. For example, the NEC V20 and NEC V30 pair were hardware-compatible with the 8088 and 8086 even though NEC made original Intel clones \u03bcPD8088D and \u03bcPD8086D respectively, but incorporated the instruction set of the 80186 along with some (but not all) of the 80186 speed enhancements, providing a drop-in capability to upgrade both instruction set and processing speed without manufacturers having to modify their designs. Such relatively simple and low-power 8086-compatible processors in CMOS are still used in embedded systems.\nThe electronics industry of the Soviet Union was able to replicate the 8086 through . The resulting chip, K1810VM86, was binary and pin-compatible with the 8086.\ni8086 and i8088 were respectively the cores of the Soviet-made PC-compatible EC1831 and EC1832 desktops. (EC1831 is the EC identification of IZOT 1036C and EC1832 is the EC identification of IZOT 1037C, developed and manufactured in Bulgaria. EC stands for \u0415\u0434\u0438\u043d\u0430\u044f \u0421\u0438\u0441\u0442\u0435\u043c\u0430.) However, the EC1831 computer (IZOT 1036C) had significant hardware differences from the IBM PC prototype. The EC1831 was the first PC-compatible computer with dynamic bus sizing (US Pat. No 4,831,514). Later some of the EC1831 principles were adopted in PS/2 (US Pat. No 5,548,786) and some other machines (UK Patent Application, Publication No. GB-A-2211325, Published June 28, 1989).\nMicrocomputers using the 8086.\nOne of the most influential microcomputers of all, the IBM PC, used the Intel 8088, a version of the 8086 with an 8-bit data bus.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15064", "revid": "1782949", "url": "https://en.wikipedia.org/wiki?curid=15064", "title": "Intel 8088", "text": "Intel microprocessor model\nThe Intel 8088 (\"eighty-eighty-eight\", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on June 1, 1979, the 8088 has an eight-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range are unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)\u2014only the bus interface unit (BIU) is different. The 8088 was used in the original IBM PC and in IBM PC compatible clones.\nHistory and description.\nThe 8088 was designed at Intel's laboratory in Haifa, Israel (as were a large number of Intel's processors) as a lower-cost version of the 8086.\nThe 8088 was targeted at economical systems by allowing the use of an eight-bit data path and eight-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's new design office and laboratory in Haifa.\nVariants of the 8088 with more than 5\u00a0MHz maximal clock frequency include the 8088\u20132, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximal frequency of 8\u00a0MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8\u00a0MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin-compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16\u00a0MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licensed Dynalogic Hyperion clone, in a move that was regarded as signaling a major new direction for the company. The available CMOS version was outsourced to Oki Electronic Industry Co., Ltd. When announced, the list price of the 8088 was US$124.80. The plastic package version was introduced in July 1981 for USD $14.10. Prices was quoted are for quantities of 100. Intel second sourced this microprocessor to Fujitsu Limited.\nDifferences from the 8086.\nThe 8088 is architecturally very similar to the 8086. The main difference is that there are only eight data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer BHE (this is the high-order byte select on the 8086\u2014the 8088 does not have a high-order byte on its eight-bit data bus). Instead it outputs a maximum mode status, SS0. Combined with the IO/M and DT/R signals, the bus cycles can be decoded (it generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals whether a memory access or input/output access is being made has had its sense reversed. The pin on the 8088 is IO/M. On the 8086 part it is IO/M. The reason for the reversal is that it makes the 8088 compatible with the 8085.\nPerformance.\nDepending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the \"average\" performance for the Intel 8088 ranged approximately from 0.33 to 1\u00a0million instructions per second. Meanwhile, the codice_1 and codice_2 instructions, taking two and three cycles respectively, yielded an \"absolute peak\" performance of between &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20443 and &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442\u00a0MIPS per MHz, that is, somewhere in the range 3\u20135\u00a0MIPS at 10\u00a0MHz.\nThe speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to eight bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the four-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means four clocks to transfer two bytes, on the 8088 it is four clocks per byte. Therefore, for example, a two-byte shift or rotate instruction, which takes the EU only two clock cycles to execute, actually takes eight clock cycles to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and in general, because so many basic instructions execute in fewer than four clocks per instruction byte\u2014including almost all the ALU and data-movement instructions on register operands and some of these on memory operands\u2014it is practically impossible to avoid idling the EU in the 8088 at least &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20444 of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).\nA side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.\nThe 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU. Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four-clock period bus transfer cycle is not particularly streamlined. Contrast this with the two-clock period bus cycle of the 6502 CPU and the 80286's three-clock period bus cycle with pipelining down to two cycles for most transfers. Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. The same is probably not true on the 80186 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.\nFinally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU, where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires four clock cycles if not taken, but if taken, it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.\nIntel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100\u2013200 clock cycles each. Many simple multiplications by small constants (besides powers of 2, for which shifts can be used) can be done much faster using dedicated short subroutines. The 80286 and 80386 each greatly increase the execution speed of these multiply and divide instructions.\nSelection for use in the IBM PC.\nThe original IBM PC is the most influential microcomputer to use the 8088. It has a clock frequency of 4.77\u00a0MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some preferred the new Motorola 68000, and others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which were in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.\nIBM chose the 8088 over the 8086 because Intel offered a better price for the former and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses. These were mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's), which were already well known by many engineers, further reducing cost.\nThe descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software-compatible processors, including the Intel Core processors, which are popular today.\nGallery.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15066", "revid": "3138265", "url": "https://en.wikipedia.org/wiki?curid=15066", "title": "Insulator (electricity)", "text": "Material that does not conduct an electric current\nAn electrical insulator is a material in which electric current does not flow freely. The atoms of the insulator have tightly bound electrons which cannot readily move. Other materials\u2014semiconductors and conductors\u2014conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. The most common examples are non-metals.\nA perfect insulator does not exist because even the materials used as insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as electrical breakdown, and the voltage at which it occurs is called the breakdown voltage of an insulator. Some materials such as glass, paper and PTFE, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics which can be thermoset or thermoplastic in nature.\nInsulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called \"insulation\". The term \"insulator\" is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.\nPhysics of conduction in solids.\nElectrical insulation is the absence of electrical conduction. Electronic band theory (a branch of physics) explains that electric charge flows when quantum states of matter are available into which electrons can be excited. This allows electrons to gain energy and thereby move through a conductor, such as a metal, if an electric potential difference is applied to the material. If no such states are available, the material is an insulator.\nMost insulators have a large band gap. This occurs because the \"valence\" band containing the highest energy electrons is full, and a large energy gap separates this band from the next band above it. There is always some voltage (called the breakdown voltage) that gives electrons enough energy to be excited into this band. Once this voltage is exceeded, electrical breakdown occurs, and the material ceases being an insulator, passing charge. This is usually accompanied by physical or chemical changes that permanently degrade the material and its insulating properties.\nWhen the electric field applied across an insulating substance exceeds in any location the threshold breakdown field for that substance, the insulator suddenly becomes a conductor, causing a large increase in current, an electric arc through the substance. Electrical breakdown occurs when the electric field in the material is strong enough to accelerate free charge carriers (electrons and ions, which are always present at low concentrations) to a high enough velocity to knock electrons from atoms when they strike them, ionizing the atoms. These freed electrons and ions are in turn accelerated and strike other atoms, creating more charge carriers, in a chain reaction. Rapidly the insulator becomes filled with mobile charge carriers, and its resistance drops to a low level. In a solid, the breakdown voltage is proportional to the band gap energy. When corona discharge occurs, the air in a region around a high-voltage conductor can break down and ionise without a catastrophic increase in current. However, if the region of air breakdown extends to another conductor at a different voltage it creates a conductive path between them, and a large current flows through the air, creating an \"electric arc\". Even a vacuum can suffer a sort of breakdown, but in this case the breakdown or vacuum arc involves charges ejected from the surface of metal electrodes rather than produced by the vacuum itself.\nIn addition, all insulators become conductors at very high temperatures as the thermal energy of the valence electrons is sufficient to put them in the conduction band.\nIn certain capacitors, shorts between electrodes formed due to dielectric breakdown can disappear when the applied electric field is reduced.\nUses.\nA flexible coating of an insulator is often applied to electric wire and cable; this assembly is called \"insulated wire\". Wires sometimes don't use an insulating coating, just air, when a solid (e.g. plastic) coating may be impractical. Wires that touch each other produce cross connections, short circuits, and fire hazards. In coaxial cable the center conductor must be supported precisely in the middle of the hollow shield to prevent electro-magnetic wave reflections. Wires that expose high voltages can cause human shock and electrocution hazards.\nMost insulated wire and cable products have maximum ratings for voltage and conductor temperature. The product may not have an ampacity (current-carrying capacity) rating, since this is dependent on the surrounding environment (e.g. ambient temperature).\nIn electronic systems, printed circuit boards are made from epoxy plastic and fibreglass. The nonconductive boards support layers of copper foil conductors. In electronic devices, the tiny and delicate active components are embedded within nonconductive epoxy or phenolic plastics, or within baked glass or ceramic coatings.\nIn microelectronic components such as transistors and ICs, the silicon material is normally a conductor because of doping, but it can easily be selectively transformed into a good insulator by the application of heat and oxygen. Oxidised silicon is quartz, i.e. silicon dioxide, the primary component of glass.\nIn high voltage systems containing transformers and capacitors, liquid insulator oil is the typical method used for preventing arcs. The oil replaces air in spaces that must support significant voltage without electrical breakdown. Other high voltage system insulation materials include ceramic or glass wire holders, gas, vacuum, and simply placing wires far enough apart to use air as insulation.\nInsulation in electrical apparatus.\nThe most important insulation material is air. A variety of solid, liquid, and gaseous insulators are also used in electrical apparatus. In smaller transformers, generators, and electric motors, insulation on the wire coils consists of up to four thin layers of polymer varnish film. Film-insulated magnet wire permits a manufacturer to obtain the maximum number of turns within the available space. Windings that use thicker conductors are often wrapped with supplemental fiberglass insulating tape. Windings may also be impregnated with insulating varnishes to prevent electrical corona and reduce magnetically induced wire vibration. Large power transformer windings are still mostly insulated with paper, wood, varnish, and mineral oil; although these materials have been used for more than 100 years, they still provide a good balance of economy and adequate performance. Busbars and circuit breakers in switchgear may be insulated with glass-reinforced plastic insulation, treated to have low flame spread and to prevent tracking of current across the material.\nIn older apparatus made up to the early 1970s, boards made of compressed asbestos may be found; while this is an adequate insulator at power frequencies, handling or repairs to asbestos material can release dangerous fibers into the air and must be carried out cautiously. Wire insulated with felted asbestos was used in high-temperature and rugged applications from the 1920s. Wire of this type was sold by General Electric under the trade name \"Deltabeston.\"\nLive-front switchboards up to the early part of the 20th century were made of slate or marble. Some high voltage equipment is designed to operate within a high pressure insulating gas such as sulfur hexafluoride. Insulation materials that perform well at power and low frequencies may be unsatisfactory at radio frequency, due to heating from excessive dielectric dissipation.\nElectrical wires may be insulated with polyethylene, crosslinked polyethylene (either through electron beam processing or chemical crosslinking), PVC, Kapton, rubber-like polymers, oil impregnated paper, Teflon, silicone, or modified ethylene tetrafluoroethylene (ETFE). Larger power cables may use compressed inorganic powder, depending on the application.\nFlexible insulating materials such as PVC (polyvinyl chloride) are used to insulate the circuit and prevent human contact with a 'live' wire \u2013 one having voltage of 600 volts or less. Alternative materials are likely to become increasingly used due to EU safety and environmental legislation making PVC less economic.\nIn electrical apparatus such as motors, generators, and transformers, various insulation systems are used, classified by their maximum recommended working temperature to achieve acceptable operating life. Materials range from upgraded types of paper to inorganic compounds.\nClass I and Class II insulation.\nAll portable or hand-held electrical devices are insulated to protect their user from harmful shock.\nClass I insulation requires that the metal body and other exposed metal parts of the device be connected to earth via a \"grounding wire\" that is earthed at the main service panel\u2014but only needs basic insulation on the conductors. This equipment needs an extra pin on the power plug for the grounding connection.\nClass II insulation means that the device is \"double insulated\". This is used on some appliances such as electric shavers, hair dryers and portable power tools. Double insulation requires that the devices have both basic and supplementary insulation, each of which is sufficient to prevent electric shock. All internal electrically energized components are totally enclosed within an insulated body that prevents any contact with \"live\" parts. In the EU, double insulated appliances all are marked with a symbol of two squares, one inside the other.\nTelegraph and power transmission insulators.\nConductors for overhead high-voltage electric power transmission are bare, and are insulated by the surrounding air. Conductors for lower voltages in distribution may have some insulation but are often bare as well. Insulating supports are required at the points where they are supported by utility poles or transmission towers. Insulators are also required where wire enters buildings or electrical devices, such as transformers or circuit breakers, for insulation from the case. Often these are bushings, which are hollow insulators with the conductor inside them.\nMaterials.\nInsulators used for high-voltage power transmission are made from glass, porcelain or composite polymer materials. Porcelain insulators are made from clay, quartz or alumina and feldspar, and are covered with a smooth glaze to shed water. Insulators made from porcelain rich in alumina are used where high mechanical strength is a criterion. Porcelain has a dielectric strength of about 4\u201310\u00a0kV/mm. Glass has a higher dielectric strength, but it attracts condensation and the thick irregular shapes needed for insulators are difficult to cast without internal strains. Some insulator manufacturers stopped making glass insulators in the late 1960s, switching to ceramic materials.\nSome electric utilities use polymer composite materials for some types of insulators. These are typically composed of a central rod made of fibre reinforced plastic and an outer weathershed made of silicone rubber or ethylene propylene diene monomer rubber (EPDM). Composite insulators are less costly, lighter in weight, and have excellent hydrophobic properties. This combination makes them ideal for service in polluted areas. However, these materials do not yet have the long-term proven service life of glass and porcelain.\nDesign.\nThe electrical breakdown of an insulator due to excessive voltage can occur in one of two ways:\nMost high voltage insulators are designed with a lower flashover voltage than puncture voltage, so they flash over before they puncture, to avoid damage.\nDirt, pollution, salt, and particularly water on the surface of a high voltage insulator can create a conductive path across it, causing leakage currents and flashovers. The flashover voltage can be reduced by more than 50% when the insulator is wet. High voltage insulators for outdoor use are shaped to maximise the length of the leakage path along the surface from one end to the other, called the creepage length, to minimise these leakage currents. To accomplish this the surface is moulded into a series of corrugations or concentric disc shapes. These usually include one or more \"sheds\"; downward facing cup-shaped surfaces that act as umbrellas to ensure that the part of the surface leakage path under the 'cup' stays dry in wet weather. Minimum creepage distances are 20\u201325\u00a0mm/kV, but must be increased in high pollution or airborne sea-salt areas.\nTypes.\nInsulators are characterized in several common classes:\nSheath insulator.\nSheath insulators are insulators that protect a full-length of bottom-contact third rail. Sheath insulators must protect the full length to prevent accidental conduction or contact. Sheath insulators also protect the third rail physically.\nSheath insulators are commonly made out of epoxy, due to its longevity and low conductivity. Ceramic is another choice, and is often used in addition to plastic.\nSuspension insulators.\nPin-type insulators are unsuitable for voltages greater than about 69\u00a0kV line-to-line. Higher voltage transmission lines usually use modular suspension insulator designs. The wires are suspended from a 'string' of identical disc-shaped insulators that attach to each other with metal clevis pin or ball-and-socket links. The advantage of this design is that insulator strings with different breakdown voltages, for use with different line voltages, can be constructed by using different numbers of the basic units. String insulators can be made for any practical transmission voltage by adding insulator elements to the string. Also, if one of the insulator units in the string breaks, it can be replaced without discarding the entire string.\nEach unit is constructed of a ceramic or glass disc with a metal cap and pin cemented to opposite sides. To make defective units obvious, glass units are designed so that an overvoltage causes a puncture arc through the glass instead of a flashover. The glass is heat-treated so it shatters, making the damaged unit visible. However the mechanical strength of the unit is unchanged, so the insulator string stays together.\nStandard suspension disc insulator units are in diameter and long, can support a load of , have a dry flashover voltage of about 72\u00a0kV, and are rated at an operating voltage of 10\u201312\u00a0kV. However, the flashover voltage of a string is less than the sum of its component discs, because the electric field is not distributed evenly across the string but is strongest at the disc nearest to the conductor, which flashes over first. Metal \"grading rings\" are sometimes added around the disc at the high voltage end, to reduce the electric field across that disc and improve flashover voltage.\nIn very high voltage lines the insulator may be surrounded by corona rings. These typically consist of toruses of aluminium (most commonly) or copper tubing attached to the line. They are designed to reduce the electric field at the point where the insulator is attached to the line, to prevent corona discharge, which results in power losses.\nHistory.\nThe first electrical systems to make use of insulators were telegraph lines; direct attachment of wires to wooden poles was found to give very poor results, especially during damp weather.\nThe first glass insulators used in large quantities had an unthreaded pinhole. These pieces of glass were positioned on a tapered wooden pin, vertically extending upwards from the pole's crossarm (commonly only two insulators to a pole and maybe one on top of the pole itself). Natural contraction and expansion of the wires tied to these \"threadless insulators\" resulted in insulators unseating from their pins, requiring manual reseating.\nAmongst the first to produce ceramic insulators were companies in the United Kingdom, with Stiff and Doulton using stoneware from the mid-1840s, Joseph Bourne (later renamed Denby) producing them from around 1860 and Bullers from 1868. Utility patent number http:// was granted to Louis A. Cauvet on 25 July 1865 for a process to produce insulators with a threaded pinhole: pin-type insulators still have threaded pinholes.\nThe invention of suspension-type insulators made high-voltage power transmission possible. As transmission line voltages reached and passed 60,000 volts, the insulators required become very large and heavy, with insulators made for a safety margin of 88,000 volts being about the practical limit for manufacturing and installation. Suspension insulators, on the other hand, can be connected into strings as long as required for the line's voltage.\nA large variety of telephone, telegraph and power insulators have been made; some people collect them, both for their historic interest and for the aesthetic quality of many insulator designs and finishes. One collectors organisation is the US National Insulator Association, which has over 9,000 members.\nInsulation of antennas.\nOften a broadcasting radio antenna is built as a mast radiator, which means that the entire mast structure is energised with high voltage and must be insulated from the ground. Steatite mountings are used. They have to withstand not only the voltage of the mast radiator to ground, which can reach values up to 400\u00a0kV at some antennas, but also the weight of the mast construction and dynamic forces. Arcing horns and lightning arresters are necessary because lightning strikes to the mast are common.\nGuy wires supporting antenna masts usually have strain insulators inserted in the cable run, to keep the high voltages on the antenna from short circuiting to ground or creating a shock hazard. Often guy cables have several insulators, placed to break up the cable into lengths that prevent unwanted electrical resonances in the guy. These insulators are usually ceramic and cylindrical or egg-shaped (see picture). This construction has the advantage that the ceramic is under compression rather than tension, so it can withstand greater load, and that if the insulator breaks, the cable ends are still linked.\nThese insulators also have to be equipped with overvoltage protection equipment. For the dimensions of the guy insulation, static charges on guys have to be considered. For high masts, these can be much higher than the voltage caused by the transmitter, requiring guys divided by insulators in multiple sections on the highest masts. In this case, guys which are grounded at the anchor basements via a coil - or if possible, directly - are the better choice.\nFeedlines attaching antennas to radio equipment, particularly twin-lead type, often must be kept at a distance from metal structures. The insulated supports used for this purpose are called \"standoff insulators\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15067", "revid": "19372301", "url": "https://en.wikipedia.org/wiki?curid=15067", "title": "Internetworking", "text": "Interconnecting different types of computer networks\nInternetworking is the practice of interconnecting multiple computer networks. Typically, this enables any pair of hosts in the connected networks to exchange messages irrespective of their hardware-level networking technology. The resulting system of interconnected networks is called an \"internetwork\", or simply an \"internet\".\nThe most notable example of internetworking is the Internet, a network of networks based on many underlying hardware technologies. The Internet is defined by a unified global addressing system, packet format, and routing methods provided by the Internet Protocol.103\nThe term \"internetworking\" is a combination of the components \"inter\" (between) and \"networking\". An earlier term for an internetwork is catenet, a short-form of \"(con)catenating networks\".\nHistory.\nThe first international heterogenous resource sharing network was developed by the computer science department at University College London (UCL) who interconnected the ARPANET with early British academic networks beginning in 1973. In the ARPANET, the network elements used to connect individual networks were called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. By 1973-4, researchers in France, the United States, and the United Kingdom had worked out an approach to internetworking where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible, as demonstrated in the CYCLADES network. Researchers at Xerox PARC outlined the idea of Ethernet and the PARC Universal Packet (PUP) for internetworking. Research at the National Physical Laboratory in the United Kingdom found that establishing a common host protocol would be more reliable and efficient. The ARPANET connection to UCL later evolved into SATNET. In 1977, ARPA demonstrated a three-way internetworking experiment, which linked a mobile vehicle in PRNET with nodes in the ARPANET, and, via SATNET, to nodes at UCL. The X.25 protocol, on which public data networks were based in the 1970s and 1980s, was supplemented by the X.75 protocol which enabled internetworking.\nToday the interconnecting gateways are called routers. The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks.\nCatenet.\nCatenet, a short-form of \"(con)catenating networks,\" is obsolete terminology for a system of packet-switched communication networks interconnected via gateways.\nThe term was coined by Louis Pouzin, who designed the CYCLADES network, in an October 1973 note circulated to the International Network Working Group, which was published in a 1974 paper \"A Proposal for Interconnecting Packet Switching Networks\". Pouzin was a pioneer of internetworking at a time when \"network\" meant what is now called a local area network. Catenet was the concept of linking these networks into a \"network of networks\" with specifications for compatibility of addressing and routing. The term was used in technical writing in the late 1970s and early 1980s, including in RFCs and IENs. Catenet was gradually displaced by the short-form of the term internetwork, \"internet\" (lower-case \"i\"), when the Internet Protocol spread more widely from the mid 1980s and the use of the term internet took on a broader sense and became well known in the 1990s.\nInterconnection of networks.\nInternetworking, a combination of the components \"inter\" (between) and \"networking\", started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network.\nTo build an internetwork, the following are needed: A standardized scheme to address packets to any host on any participating network; a standardized protocol defining format and handling of transmitted packets; components interconnecting the participating networks by routing packets to their destinations based on standardized addresses.\nAnother type of interconnection of networks often occurs within enterprises at the link layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers and having an internetworking software layer that applications employ.\nThe Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate transport layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.\nNetworking models.\nTwo architectural models are commonly used to describe the protocols and methods used in internetworking. The Open System Interconnection (OSI) reference model was developed under the auspices of the International Organization for Standardization (ISO) and provides a rigorous description for layering protocol functions from the underlying hardware to the software interface concepts in user applications. Internetworking is implemented in the Network Layer (Layer 3) of the model.\nThe Internet Protocol Suite, also known as the TCP/IP model, was not designed to conform to the OSI model and does not refer to it in any of the normative specifications in Request for Comments and Internet standards. Despite similar appearance as a layered model, it has a much less rigorous, loosely defined architecture that concerns itself only with the aspects of the style of networking in its own historical provenance. It assumes the availability of any suitable hardware infrastructure, without discussing hardware-specific low-level interfaces, and that a host has access to this local network to which it is connected via a link layer interface.\nFor a period in the late 1980s and early 1990s, the network engineering community was polarized over the implementation of competing protocol suites, commonly known as the Protocol Wars. It was unclear which of the OSI model and the Internet protocol suite would result in the best and most robust computer networks.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15068", "revid": "50316851", "url": "https://en.wikipedia.org/wiki?curid=15068", "title": "Infantry", "text": "Military personnel who engage in ground combat\nInfantry, or infantryman are a type of soldier who specialize in ground combat, typically fighting dismounted. Historically the term was used to describe foot soldiers, i.e. those who march and fight on foot. In modern usage, the term broadly encompasses a wide variety of subspecialties, including light infantry, irregular infantry, heavy infantry, mountain infantry, motorized infantry, mechanized infantry, airborne infantry, air assault infantry, and naval infantry. Other subtypes of infantry, such as line infantry and mounted infantry, were once commonplace but fell out of favor in the 19th century with the invention of more accurate and powerful weapons.\nEtymology and terminology.\nIn English, use of the term \"infantry\" began about the 1570s, describing soldiers who march and fight on foot. The word derives from Middle French , from older Italian (also Spanish) \"infanteria\" (foot soldiers too inexperienced for cavalry), from Latin \"\u012bnf\u0101ns\" (without speech, newborn, foolish), from which English also gets \"infant\". The individual-soldier term \"infantryman\" was not coined until 1837. In modern usage, foot soldiers of any era are now considered infantry and infantrymen. Some military forces, such as the Canadian Army, use the term \"infanteer\" as opposed to \"infantryman\" to refer to an individual soldier of the infantry \nFrom the mid-18th century until 1881, the British Army named its infantry as numbered regiments \"of Foot\" to distinguish them from cavalry and dragoon regiments (see List of Regiments of Foot).\nInfantry equipped with special weapons were often named after that weapon, such as grenadiers for their grenades, or fusiliers for their \"fusils\". These names can persist long after the weapon speciality; examples of infantry units that retained such names are the Royal Irish Fusiliers and the Grenadier Guards.\nDragoons were created as mounted infantry, with horses for travel between battles; they were still considered infantry since they dismounted before combat. However, if light cavalry was lacking in an army, any available dragoons might be assigned their duties; this practice increased over time, and dragoons eventually received all the weapons and training as both infantry and cavalry, and could be classified as both. Conversely, starting about the mid-19th century, regular cavalry have been forced to spend more of their time dismounted in combat due to the ever-increasing effectiveness of enemy infantry firearms. Thus most cavalry transitioned to mounted infantry. As with grenadiers, the \"dragoon\" and \"cavalry\" designations can be retained long after their horses, such as in the Royal Dragoon Guards, Royal Lancers, and King's Royal Hussars.\nSimilarly, motorised infantry have trucks and other unarmed vehicles for non-combat movement, but are still infantry since they leave their vehicles for any combat. Most modern infantry have vehicle transport, to the point where infantry being motorised is generally assumed, and the few exceptions might be identified as modern \"light infantry\". Mechanised infantry go beyond motorised, having transport vehicles with combat abilities, armoured personnel carriers (APCs), providing at least some options for combat without leaving their vehicles. In modern infantry, some APCs have evolved to be infantry fighting vehicles (IFVs), which are transport vehicles with more substantial combat abilities, approaching those of light tanks. Some well-equipped mechanised infantry can be designated as \"armoured infantry\". Given that infantry forces typically also have some tanks, and given that most armoured forces have more mechanised infantry units than tank units in their organisation, the distinction between mechanised infantry and armour forces has blurred.\nHistory.\nThe first military forces in history were infantry. In antiquity, infantry were armed with early melee weapons such as a spear, axe, or sword, or an early ranged weapon like a javelin, sling, or bow, with a few infantrymen being expected to use both a melee and a ranged weapon. With the development of gunpowder, infantry began converting to primarily firearms. By the time of Napoleonic warfare, infantry, cavalry and artillery formed a basic triad of ground forces, though infantry usually remained the most numerous. With armoured warfare, armoured fighting vehicles have replaced the horses of cavalry, and airpower has added a new dimension to ground combat, but infantry remains pivotal to all modern combined arms operations.\nThe first warriors, adopting hunting weapons or improvised melee weapons, before the existence of any organised military, likely started essentially as loose groups without any organisation or formation. But this changed sometime before recorded history; the first ancient empires (2500\u20131500 BC) are shown to have some soldiers with standardised military equipment, and the training and discipline required for battlefield formations and manoeuvres: regular infantry. Though the main force of the army, these forces were usually kept small due to their cost of training and upkeep, and might be supplemented by local short-term mass-conscript forces using the older irregular infantry weapons and tactics; this remained a common practice almost up to modern times.\nBefore the adoption of the chariot to create the first mobile fighting forces c.\u20092000 BC, all armies were pure infantry. Even after, with a few exceptions like the Mongol Empire, infantry has been the largest component of most armies in history.\nIn the Western world, from Classical Antiquity through the Middle Ages (c. 8th century BC to 15th century AD), infantry are categorised as either heavy infantry or light infantry. After the fall of Rome, the quality of heavy infantry declined, and warfare was dominated by heavy cavalry, such as knights, forming small elite units for decisive shock combat, supported by peasant infantry militias and assorted light infantry from the lower classes. Towards the end of Middle Ages, this began to change, where more professional and better trained light infantry could be effective against knights, such as the English longbowmen in the Hundred Years' War. By the start of the Renaissance, the infantry began to return to a larger role, with Swiss pikemen and German Landsknechts filling the role of heavy infantry again, using dense formations of pikes to drive off any cavalry.\nDense formations are vulnerable to ranged weapons. Technological developments allowed the raising of large numbers of light infantry units armed with ranged weapons, without the years of training expected for traditional high-skilled archers and slingers. This started slowly, first with crossbowmen, then hand cannoneers and arquebusiers, each with increasing effectiveness, marking the beginning of early modern warfare, when firearms rendered the use of heavy infantry obsolete. The introduction of musketeers using bayonets in the mid 17th century began replacement of the pike with the infantry square replacing the pike square.\nTo maximise their firepower, musketeer infantry were trained to fight in wide lines facing the enemy, creating line infantry. These fulfilled the central battlefield role of earlier heavy infantry, using ranged weapons instead of melee weapons. To support these lines, smaller infantry formations using dispersed skirmish lines were created, called light infantry, fulfilling the same multiple roles as earlier light infantry. Their arms were no lighter than line infantry; they were distinguished by their skirmish formation and flexible tactics.\nThe modern rifleman infantry became the primary force for taking and holding ground on battlefields as an element of combined arms. As firepower continued to increase, use of infantry lines diminished, until all infantry became light infantry in practice. Modern classifications of infantry have since expanded to reflect modern equipment and tactics, such as motorised infantry, mechanised or armoured infantry, mountain infantry, marine infantry, and airborne infantry.\nEquipment.\nBeyond main arms and armour, an infantryman's \"military kit\" generally includes combat boots, battledress or combat uniform, camping gear, heavy weather gear, survival gear, secondary weapons and ammunition, weapon service and repair kits, health and hygiene items, mess kit, rations, filled water canteen, and all other consumables each infantryman needs for the expected duration of time operating away from their unit's base, plus any special mission-specific equipment. One of the most valuable pieces of gear is the entrenching tool\u2014basically a folding spade\u2014which can be employed not only to dig important defences, but also in a variety of other daily tasks, and even sometimes as a weapon. Infantry typically have shared equipment on top of this, like tents or heavy weapons, where the carrying burden is spread across several infantrymen. In all, this can reach for each soldier on the march. Such heavy infantry burdens have changed little over centuries of warfare; in the late Roman Republic, legionaries were nicknamed \"Marius' mules\" as their main activity seemed to be carrying the weight of their legion around on their backs, a practice that predates the eponymous Gaius Marius.\nWhen combat is expected, infantry typically switch to \"packing light\", meaning reducing their equipment to weapons, ammunition, and other basic essentials, and leaving other items deemed unnecessary with their transport or baggage train, at camp or rally point, in temporary hidden caches, or even (in emergencies) simply discarding the items. Additional specialised equipment may be required, depending on the mission or to the particular terrain or environment, including satchel charges, demolition tools, mines, or barbed wire, carried by the infantry or attached specialists.\nHistorically, infantry have suffered high casualty rates from disease, exposure, exhaustion and privation\u2014often in excess of the casualties suffered from enemy attacks. Better infantry equipment to support their health, energy, and protect from environmental factors greatly reduces these rates of loss, and increase their level of effective action. Health, energy, and morale are greatly influenced by how the soldier is fed, so militaries issue standardised field rations that provide palatable meals and enough calories to keep a soldier well-fed and combat-ready.\nCommunications gear has become a necessity, as it allows effective command of infantry units over greater distances, and communication with artillery and other support units. Modern infantry can have GPS, encrypted individual communications equipment, surveillance and night vision equipment, advanced intelligence and other high-tech mission-unique aids.\nArmies have sought to improve and standardise infantry gear to reduce fatigue for extended carrying, increase freedom of movement, accessibility, and compatibility with other carried gear, such as the American all-purpose lightweight individual carrying equipment (ALICE).\nWeapons.\nInfantrymen are defined by their primary arms \u2013 the personal weapons and body armour for their own individual use. The available technology, resources, history, and society can produce quite different weapons for each military and era, but common infantry weapons can be distinguished in a few basic categories.\nInfantrymen often carry secondary or back-up weapons, sometimes called a sidearm or ancillary weapons. Infantry with ranged or polearms often carried a sword or dagger for possible hand-to-hand combat. The \"pilum\" was a javelin the Roman legionaries threw just before drawing their primary weapon, the \"gladius\" (short sword), and closing with the enemy line.\nModern infantrymen now treat the bayonet as a backup weapon, but may also have handguns as sidearms. They may also deploy anti-personnel mines, booby traps, incendiary, or explosive devices defensively before combat.\nProtection.\nInfantry have employed many different methods of protection from enemy attacks, including various kinds of armour and other gear, and tactical procedures.\nThe most basic is personal armour. This includes shields, helmets and many types of armour \u2013 padded linen, leather, lamellar, mail, plate, and kevlar. Initially, armour was used to defend both from ranged and close combat; even a fairly light shield could help defend against most slings and javelins, though high-strength bows and crossbows might penetrate common armour at very close range. Infantry armour had to compromise between protection and coverage, as a full suit of attack-proof armour would be too heavy to wear in combat.\nAs firearms improved, armour for ranged defence had to be made thicker and heavier, which hindered mobility. With the introduction of the heavy arquebus designed to pierce standard steel armour, it was proven easier to make heavier firearms than heavier armour; armour transitioned to be only for close combat purposes. Pikemen armour tended to be just steel helmets and breastplates, and gunners had very little or no armour at all. By the time of the musket, the dominance of firepower shifted militaries away from any close combat, and use of armour decreased, until infantry typically went without wearing any armour.\nHelmets were added back during World War I as artillery began to dominate the battlefield, to protect against their fragmentation and other blast effects beyond a direct hit. Modern developments in bullet-proof composite materials like kevlar have started a return to body armour for infantry, though the extra weight is a notable burden.\nIn modern times, infantrymen must also often carry protective measures against chemical and biological attack, including military gas masks, counter-agents, and protective suits. All of these protective measures add to the weight an infantryman must carry, and may decrease combat efficiency.\nInfantry-served weapons.\nEarly crew-served weapons were siege weapons, like the ballista, trebuchet, and battering ram. Modern versions include machine guns, anti-tank missiles, and infantry mortars.\nFormations.\nBeginning with the development of the first regular military forces, close-combat regular infantry fought less as unorganised groups of individuals and more in coordinated units, maintaining a defined tactical formation during combat, for increased battlefield effectiveness; such infantry formations and the arms they used developed together, starting with the spear and the shield.\nA spear has decent attack abilities with the additional advantage keeping opponents at distance; this advantage can be increased by using longer spears, but this could allow the opponent to side-step the point of the spear and close for hand-to-hand combat where the longer spear is near useless. This can be avoided when each spearman stays side by side with the others in close formation, each covering the ones next to him, presenting a solid wall of spears to the enemy that they cannot get around.\nSimilarly, a shield has decent defence abilities, but is literally hit-or-miss; an attack from an unexpected angle can bypass it completely. Larger shields can cover more, but are also heavier and less manoeuvrable, making unexpected attacks even more of a problem. This can be avoided by having shield-armed soldiers stand close together, side-by-side, each protecting both themselves and their immediate comrades, presenting a solid shield wall to the enemy.\nThe opponents for these first formations, the close-combat infantry of more tribal societies, or any military without regular infantry (so called \"barbarians\") used arms that focused on the individual \u2013 weapons using personal strength and force, such as larger swinging swords, axes, and clubs. These take more room and individual freedom to swing and wield, necessitating a more loose organisation. While this may allow for a fierce running attack (an initial shock advantage) the tighter formation of the heavy spear and shield infantry gave them a local manpower advantage where several might be able to fight each opponent.\nThus tight formations heightened advantages of heavy arms, and gave greater local numbers in melee. To also increase their staying power, multiple rows of heavy infantrymen were added. This also increased their shock combat effect; individual opponents saw themselves literally lined-up against several heavy infantryman each, with seemingly no chance of defeating all of them. \"Heavy infantry\" developed into huge solid block formations, up to a hundred meters wide and a dozen rows deep.\nMaintaining the advantages of heavy infantry meant maintaining formation; this became even more important when two forces with heavy infantry met in battle; the solidity of the formation became the deciding factor. Intense discipline and training became paramount. Empires formed around their military.\nOrganization.\nThe organization of military forces into regular military units is first noted in Egyptian records of the Battle of Kadesh (c.\u20091274 BC). Soldiers were grouped into units of 50, which were in turn grouped into larger units of 250, then 1,000, and finally into units of up to 5,000 \u2013 the largest independent command. Several of these Egyptian \"divisions\" made up an army, but operated independently, both on the march and tactically, demonstrating sufficient military command and control organisation for basic battlefield manoeuvres. Similar hierarchical organizations have been noted in other ancient armies, typically with approximately 10 to 100 to 1,000 ratios (even where base 10 was not common), similar to modern sections (squads), companies, and regiments.\nTraining.\nThe training of the infantry has differed drastically over time and from place to place. The cost of maintaining an army in fighting order and the seasonal nature of warfare precluded large permanent armies.\nThe antiquity saw everything from the well-trained and motivated citizen armies of Greece and Rome, the tribal host assembled from farmers and hunters with only passing acquaintance with warfare and masses of lightly armed and ill-trained militia put up as a last ditch effort. Kushite king Taharqa enjoyed military success in the Near East as a result of his efforts to strengthen the army through daily training in long-distance running.\nIn medieval times the foot soldiers varied from peasant levies to semi-permanent companies of mercenaries, foremost among them the Swiss, English, Aragonese and German, to men-at-arms who went into battle as well-armoured as knights, the latter of which at times also fought on foot.\nThe creation of standing armies\u2014permanently assembled for war or defence\u2014saw increase in training and experience. \nThe introduction of national and mass armies saw an establishment of minimum requirements and the introduction of special troops (first of them the engineers going back to medieval times, but also different kinds of infantry adopted to specific terrain, bicycle, motorcycle, motorised and mechanised troops) culminating with the introduction of highly trained special forces during the first and second World War.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15069", "revid": "49291118", "url": "https://en.wikipedia.org/wiki?curid=15069", "title": "Identity function", "text": "Function that returns its argument unchanged\nIn mathematics, an identity function, also called an identity relation, identity map or identity transformation, is a function that always returns the value that was used as its argument, unchanged. That is, when f is the identity function, the equality \"f\"(\"x\") = \"x\" is true for all values of x to which f can be applied.\nDefinition.\nFormally, if \"X\" is a set, the identity function \"f\" on \"X\" is defined to be a function with \"X\" as its domain and codomain, satisfying\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nIn other words, the function value \"f\"(\"x\") in the codomain \"X\" is always the same as the input element \"x\" in the domain \"X\". The identity function on X is clearly an injective function as well as a surjective function (its codomain is also its range), so it is bijective.\nThe identity function \"f\" on \"X\" is often denoted by id\"X\".\nIn set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or \"diagonal\" of \"X\".\nAlgebraic properties.\nIf \"f\" : \"X\" \u2192 \"Y\" is any function, then \"f\" \u2218 id\"X\" = \"f\" = id\"Y\" \u2218 \"f\", where \"\u2218\" denotes function composition. In particular, id\"X\" is the identity element of the monoid of all functions from \"X\" to \"X\" (under function composition).\nSince the identity element of a monoid is unique, one can alternately define the identity function on \"M\" to be this identity element. Such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of \"M\" need not be functions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15070", "revid": "1126265", "url": "https://en.wikipedia.org/wiki?curid=15070", "title": "I386", "text": "32-bit microprocessor by Intel\nThe Intel 386, originally released as the 80386 and later renamed i386, is the first 32-bit processor in the line, marking it a significant evolution in the x86 microarchitecture. It is the third-generation x86 architecture microprocessor developed jointly by AMD, IBM and Intel. Pre-production samples of the 386 were released to select developers in October 1985, while mass production commenced in June 1986. It implements the IA-32 microarchitecture, and is the first CPU to do so. It was the central processing unit (CPU) of many workstations and high-end personal computers of the time. It began to fall out of public use starting with the release of the i486 processor in 1989, while in embedded systems the 386 remained in widespread use until Intel finally discontinued it in 2007.\nCompared to its predecessor the Intel 80286 (\"286\"), the 80386 added a three-stage instruction pipeline which it brings up to total of 6-stage instruction pipeline, extended the architecture from 16-bits to 32-bits, and added an on-chip memory management unit. This paging translation unit made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging. The 386 featured three operating modes: real mode, virtual mode, and protected mode. The protected mode, which debuted in the 286, was extended to allow the 386 to address up to 4 GB of memory. With the addition of segmented addressing system, it can expand up to 64 TB of virtual memory. The all new virtual 8086 mode (or \"VM86\") made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible.\nThe 32-bit i386 can correctly execute most code intended for the earlier 16-bit processors such as 8086 and 80286 that were ubiquitous in early PCs. As the original implementation of the 32-bit extension of the 80286 architecture, the i386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the \"i386 architecture\", \"x86\", or \"IA-32\", depending on context. Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086).\nProduction history.\nIn the early 1980s Intel, the creator of 80286, was aware of that microprocessor's poor reputation. The company's own engineers believed that Motorola 68000 was superior to their \"ugly duckling\". Bill Gates called 80286 \"brain dead\", and important customer IBM thought that its architecture was a flawed dead end. While the company expected that Intel i432 would be its future architecture, i432 was very slow and many also believed unsuitable. Groups worked on various successors, including a completely new architecture (\"P4\") from i432 designer Glen Myers that resembled DEC VAX, and another (\"P7\") intended to combine Myers's work and i432 technology.\nAlthough many in the company believed that a 32-bit successor to 80286 was nonviable, Gene Hill and 80286 co-designer Robert Childs secretly worked on the \"stepchild\" project and persuaded others of its potential over Myers's plan, which people such as John Crawford compared to the events at Data General in \"The Soul of a New Machine\". Binary compatibility with the Intel 8086 architecture the recently introduced IBM PC used was at first not seen as important, and many disliked the older CPUs' segmented memory model. A greater priority was a 32-bit flat memory model so 80386 can, like 68000, run Unix well.\n80386 development began in 1982 under the internal name of P3. Intel previously used NMOS logic but 80386 was its first CMOS product, consistent with the industry trend. The rapidly growing IBM PC installed base made supporting its software library more important, and Intel salespeople told customers that their 286 software would run on 386. The 386 designers thus supported both flat and segmented memory models, what Crawford described as \"the best of both worlds\". Pat Gelsinger led the port of Amdahl UTS to the CPU to confirm Unix's viability. The limited die size made difficult incorporating, for marketing purposes, a CPU cache twice as large as the 68020's. The team's Jim Slager later described both CPUs' caches as useless, but he and his colleagues succeeded.\nThe tape-out of the 80386 development was finalized in July 1985. The 80386 was introduced as pre-production samples for software development workstations in October 1985. Intel had exited the DRAM market to focus on microprocessors, so the former \"stepchild\" was vital to its future; the company moved memory engineers to the 80386 project, improving the die shrink. The forthcoming product persuaded customers that the 80286 was not a dead end, increasing the latter's sales.\n80386 manufacturing in volume began in June 1986, along with the first plug-in device that allowed existing 80286-based computers to be upgraded to the 386, the Translator 386 by American Computer and Peripheral. The 80386 being sole sourced made the CPU very expensive, but it was very successful. Hill recalled representing the design team at a \"PC Magazine\" awards ceremony:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;But what really sunk in my brain was to look around the room, because all the awards that year were for the 386 boxes, software, boards, chips, peripheral chips. It was all 386. So here was a whole ballroom full of people, the elite in their various industries that were getting awards just because of the 386. So it really struck me how many jobs and how much businesses had been created from the 386. It became not only the king of Intel, but the king of many industries, not just the PC industry.\nAlthough the multiple segment models were rarely used their existence may have benefited Intel, because the complexity slowed other companies' ability to second source the CPU. Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was justified upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was the Deskpro 386, designed and manufactured by Compaq; this marked the first time a fundamental component in the IBM PC compatible de facto standard was updated by a company other than IBM.\nThe first versions of the 386 have 275,000 transistors. The 20\u00a0MHz version operates at 4\u20135 MIPS. It also performs between 8,000 and 9,000 Dhrystones per second. The 25\u00a0MHz 386 version is capable of 7 MIPS. A 33\u00a0MHz 80386 was reportedly measured to operate at about 11.4 and 11.5 MIPS. At that same speed, it has the performance of 8 VAX MIPS. These processors run about 4.4 clocks per instruction.\nAfter AMD and Chips and Technologies released 386-compatible CPUs, Intel in 1992 lowered the price of its 25-MHz 80486SX processor to less than that of the 33-MHz 80386. An industry analyst said that Intel wanted customers to move to the competition-free 486. The strategy was very successful; by 1993 many computer companies had discontinued 80386 products or planned to do so later that year. Customers who found that Windows 3.1 ran slowly with 386 were willing to pay $200\u2013300 more for 486; Dell reported that 80486-based computers were 70% of sales.\nIn May 2006, Intel announced that i386 production would stop at the end of September 2007. Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an i386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the i386 processor, such as the BlackBerry 950 and Nokia 9000 Communicator. Linux continued to support i386 processors until December 11, 2012, when the kernel cut 386-specific instructions in version 3.8.\nArchitecture.\nThe processor was a significant evolution in the x86 architecture, and extended a long line of processors that stretched back to the Intel 8008. The predecessor of the 80386 was the Intel 80286, a 16-bit processor with a segment-based memory management and protection system. The 80386 added a three-stage instruction pipeline which it brought up to total of 6-stage instruction pipeline, extended the architecture from 16-bits to 32-bits, and added an on-chip memory management unit. This paging translation unit made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging.\nThe 80386 featured three operating modes: real mode, virtual mode, and protected mode. The protected mode, which debuted in the 286, was extended to allow the 386 to address up to 4 GB of memory. With the addition of segmented addressing system, it can expand up to 64 TB of virtual memory. The all new virtual 8086 mode (or \"VM86\") made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible. It features scaled indexing and 64-bit barrel shifter.\nThe ability for a 386 to be set up to act like it had a flat memory model in protected mode despite the fact that it uses a segmented memory model in all modes was arguably the most important feature change for the x86 processor family until AMD released the x86-64 in 2003.\nSeveral new instructions have been added to 386: BSF, BSR, BT, BTS, BTR, BTC, CDQ, CWDE, LFS, LGS, LSS, MOVSX, MOVZX, SETcc, SHLD, SHRD.\nTwo new segment registers have been added (FS and GS) for general-purpose programs. The single Machine Status Word of the 286 grew into eight control registers CR0\u2013CR7. Debug registers DR0\u2013DR7 were added for hardware breakpoints. New forms of the MOV instruction are used to access them.\nThe chief architect in the development of the 80386 was John H. Crawford. He was responsible for extending the 80286 architecture and instruction set to 32-bits, and then led the microprogram development for the 80386 chip.\nThe i486 and P5 Pentium line of processors were descendants of the i386 design.\nData types.\nThe following data types are directly supported and thus implemented by one or more i386 machine instructions; these data types are briefly described here.:\nExample code.\nThe following i386 assembly source code is for a subroutine named codice_1 that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time.\nThe example code uses the EBP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory.\nBusiness importance.\nThe first PC based on the Intel 80386 was the Compaq Deskpro 386, introduced in September 1986. By extending the 16/24-bit IBM PC/AT standard into a natively 32-bit computing environment, Compaq became the first company to design and manufacture such a major technical hardware advance on the PC platform. IBM was offered use of the 80386, but had manufacturing rights for the earlier 80286. IBM therefore chose to rely on that processor for a couple more years. The early success of the Compaq Deskpro 386 played an important role in legitimizing the PC \"clone\" industry and in de-emphasizing IBM's role within it. The first computer system sold with the 386SX was the Compaq Deskpro 386S, released in July 1988.\nPrior to the 386, the difficulty of manufacturing microchips and the uncertainty of reliable supply made it desirable that any mass-market semiconductor be multi-sourced, that is, made by two or more manufacturers, the second and subsequent companies manufacturing under license from the originating company. The 386 was for \"a time\" (4.7 years) only available from Intel, since Andy Grove, Intel's CEO at the time, made the decision not to encourage other manufacturers to produce the processor as second sources. This decision was ultimately crucial to Intel's success in the market. The 386 was the first significant microprocessor to be single-sourced. Single-sourcing the 386 allowed Intel greater control over its development and substantially greater profits in later years.\nAMD introduced its compatible Am386 processor in March 1991 after overcoming legal obstacles, thus ending Intel's 4.7-year monopoly on 386-compatible processors. From 1991 IBM also manufactured 386 chips under license for use only in IBM PCs and boards.\nIn 1991, while studying computer science at University of Helsinki, Linus Torvalds began a project that later became the Linux kernel. He wrote the program specifically for the hardware he was using and independent of an operating system because he wanted to use the functions of his new 80386 PC.\nEarly problems.\nIntel originally intended for the 80386 to debut at 16\u00a0MHz. However, due to poor yields, it was instead introduced at 12.5\u00a0MHz.\nEarly in production, Intel discovered a marginal circuit that could cause a system to return incorrect results from 32-bit multiply operations. Not all of the processors already manufactured were affected, so Intel tested its inventory. Processors that were found to be bug-free were marked with a double sigma (\u03a3\u03a3), and affected processors were marked \"16 BIT S/W ONLY\". These latter processors were sold as good parts, since at the time 32-bit capability was not relevant for most users.\nThe i387 math coprocessor was not ready in time for the introduction of the 80386, and so many of the early 80386 motherboards instead provided a socket and hardware logic to make use of an 80287. In this configuration the FPU operated asynchronously to the CPU, usually with a clock rate of 10\u00a0MHz. The original Compaq Deskpro 386 is an example of such design.\nPin-compatible upgrades.\nIntel later offered a modified version of its 486DX in i386 packaging, branded as the Intel RapidCAD. This provided an upgrade path for users with i386-compatible hardware. The upgrade was a pair of chips that replaced both the i386 and i387. Since the 486DX design contained an FPU, the chip that replaced the i386 contained the floating-point functionality, and the chip that replaced the i387 served very little purpose. However, the latter chip was necessary in order to provide the FERR signal to the mainboard and appear to function as a normal floating-point unit.\nThird parties offered a wide range of upgrades, for both SX and DX systems. The most popular ones were based on the Cyrix 486DLC/SLC core, which typically offered a substantial speed improvement due to its more efficient instruction pipeline and internal L1 SRAM cache. The cache was usually 1 KB, or sometimes 8 KB in the TI variant. Some of these upgrade chips (such as the 486DRx2/SRx2) were marketed by Cyrix themselves, but they were more commonly found in kits offered by upgrade specialists such as Kingston, Evergreen Technologies and Improve-It Technologies. Some of the fastest CPU upgrade modules featured the IBM SLC/DLC family (notable for its 16 KB L1 cache), or even the Intel 486 itself. Many 386 upgrade kits were advertised as being simple drop-in replacements, but often required complicated software to control the cache or clock doubling. Part of the problem was that on most 386 motherboards, the A20 line was controlled entirely by the motherboard with the CPU being unaware, which caused problems on CPUs with internal caches.\nOverall, it was very difficult to configure upgrades to produce the results advertised on the packaging, and upgrades were often not very stable or not fully compatible.\nModels and variants.\nEarly 5 V models.\ni386DX.\nOriginal version, released in October 1985. The 16\u00a0MHz version was available for 299\u00a0USD in quantities of 100. The 20\u00a0MHz version was available for 599 USD in quantities of 100. The 33\u00a0MHz version was made available on April 10, 1989.\nM80386.\nThe military version was made using the CHMOS III process technology. It was made to withstand 105 Rads (Si) or greater. It was available for US$945 each in quantities of 100.\n80386SX.\nIn 1988, Intel introduced the 80386SX, most often referred to as the 386SX, a cut-down version of the 80386 with a 16-bit data bus, mainly intended for lower-cost PCs aimed at the home, educational, and small-business markets, while the 386DX remained the high-end variant used in workstations, servers, and other demanding tasks. The CPU remained fully 32-bit internally, but the 16-bit bus was intended to simplify circuit-board layout and reduce total cost. The 16-bit bus simplified designs but hampered performance. Only 24 pins were connected to the address bus, therefore limiting addressing to 16\u00a0MB, but this was not a critical constraint at the time. Performance differences were due not only to differing data-bus widths, but also due to performance-enhancing cache memories often employed on boards using the original chip. This version can run 32-bit application software at 70 to 90 percent the speed of the regular Intel386 DX CPU.\nThe original 80386 was subsequently renamed i386DX to avoid confusion. However, Intel subsequently used the \"DX\" suffix to refer to the floating-point capability of the i486DX. The 387SX was an 80387 part that was compatible with the 386SX (i.e. with a 16-bit databus). The 386SX was packaged in a surface-mount QFP and sometimes offered in a socket to allow for an upgrade.\nThe 16\u00a0MHz 386SX contains the 100-lead BQFP. It was available for USD $165 in quantities of 1000. It has the performance of 2.5 to 3 MIPS as well. The low-power version was available on April 10, 1989. This version that uses 20 to 30 percent less power and has higher operating temperature than the regular version, up to 100\u00a0\u00b0C.\n80386SL.\nThe 80386SL was introduced as a power-efficient version for laptop computers. The processor offered several power-management options (e.g. SMM), as well as different \"sleep\" modes to conserve battery power. It also contained support for an external cache of 16 to 64 KB. The extra functions and circuit implementation techniques caused this variant to have over 3 times as many transistors as the i386DX. The i386SL was first available at 20\u00a0MHz clock speed, with the 25\u00a0MHz model later added. With this system, it reduced up to 40% foot space than the Intel386 SX system. That translate to lighter and more portable cost-effective system.\nDave Vannier, the chief architect designed this microprocessor. It took them two years to complete this design since it uses the existing 386 architecture to implement. That assist with advanced computer-aided design tools which includes a complete simulation of system board. This die contains the 386 CPU core, AT Bus Controller, Memory Controller, Internal Bus Controller, Cache Control Logic along with Cache Tag SRAM and Clock. This CPU contains 855,000 transistors using one-micron CHMOS IV technology. It was available for USD $176 in 1,000 unit in quantities. The 25-MHz version was available in samples for USD $189 in 1,000-piece quantities, that version was finally made available in production by the end of 1991. It supports up to 32 MB of physical address space. There was a 20-MHz cacheless version of Intel386 SL microprocessor, at the press time samples of this version were available for USD $101 in 1,000-piece quantities. There were low-voltage 20-Mhz version and cacheless 16- and 20-Mhz version microprocessors. These low voltage uses 3.3 Volts to supplied them and they do support full static mode as well. They were available for USD $94, $48 and $78 respectively in 1,000 pieces quantities.\nSnapIn 386.\nIn May 1991, Intel introduced an upgrade for IBM PS/2 Model 50 and 60 systems which contain 80286 microprocessors, converting them to 32-bit systems. The SnapIn 386 module is a daughtercard with 20-MHz 386SX and 16-Kbyte direct-mapped cache SRAM memory. It directly plugs into the existing 286 socket with no cables, jumpers or switches. In the winter of 1992, an additional to this module now supported to IBM PS/2 Model 50 Z, 30 286 and 25 286 systems. Both modules were available for USD $495.\nRapidCAD.\nA specially packaged Intel 486DX and a dummy floating-point unit (FPU) designed as pin-compatible replacements for an i386 processor and i387 FPU.\nVersions for embedded systems.\n80376.\nThis was an embedded version of the 80386SX which did not support real mode and paging in the MMU.\ni386EX, i386EXTB and i386EXTC.\nSystem and power management and built in peripheral and support functions: Two 82C59A interrupt controllers; Timer, Counter (3 channels); Asynchronous SIO (2 channels); Synchronous SIO (1 channel); Watchdog timer (Hardware/Software); PIO. Usable with 80387SX or i387SL FPUs.\ni386CXSA and i386SXSA (or i386SXTA).\nTransparent power management mode, integrated MMU and TTL compatible inputs (only 386SXSA). Usable with i387SX or i387SL FPUs.\ni386CXSB.\nTransparent power management mode and integrated MMU. Usable with i387SX or i387SL FPUs.\nObsolescence.\nWindows 95 was the only entry in the Windows 9x series to officially support the 386, requiring at least a 386DX, though a 486 or better was recommended; Windows 98 requires a 486DX or higher. In the Windows NT family, Windows NT 3.51 was the last version with 386 support.\nDebian GNU/Linux dropped 386 support with the release of 3.1 (\"Sarge\") in 2005 and completely removed support in 2007 with 4.0 (\"Etch\"). Citing the maintenance burden around SMP primitives, the Linux kernel developers cut support from the development codebase in December 2012, later released as kernel version 3.8.\nAmong the BSDs, FreeBSD's 5.x releases were the last to support the 386; support for the 386SX was cut with release 5.2, while the remaining 386 support was removed with the 6.0 release in 2005. OpenBSD removed 386 support with version 4.2 (2007), DragonFly BSD with release 1.12 (2008), and NetBSD with the 5.0 release (2009).\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15072", "revid": "10574408", "url": "https://en.wikipedia.org/wiki?curid=15072", "title": "Instruction register", "text": "Register in a CPU control unit holding the currently-executing instruction\nIn computing, the instruction register (IR) or current instruction register (CIR) is the part of a CPU's control unit that holds the instruction currently being executed or decoded. In simple processors, each instruction to be executed is loaded into the instruction register, which holds it while it is decoded, prepared and ultimately executed, which can take several steps.\nSome of the complicated processors use a pipeline of instruction registers where each stage of the pipeline does part of the decoding, preparation or execution and then passes it to the next stage for its step. Modern processors can even do some of the steps out of order as decoding on several instructions is done in parallel.\nDecoding the op-code in the instruction register includes determining the instruction, determining where its operands are in memory, retrieving the operands from memory, allocating processor resources to execute the command (in super scalar processors), etc.\nThe output of the IR is available to control circuits, which generate the timing signals that control the various processing elements involved in executing the instruction.\nIn the instruction cycle, the instruction is loaded into the instruction register after the processor fetches it from the memory location pointed to by the program counter.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15073", "revid": "8331790", "url": "https://en.wikipedia.org/wiki?curid=15073", "title": "Lists of islands", "text": "This is a list of the lists of islands in the world grouped by country, by continent, by body of water, and by other classifications. For rank-order lists, see the other lists of islands below.\nLists of islands by body of water.\nBy ocean:\nBy other bodies of water:\nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\nThis article includes . &lt;br&gt; If an [ internal link] incorrectly led you here, you may wish to change the link to point directly to the intended article."}
{"id": "15075", "revid": "38527202", "url": "https://en.wikipedia.org/wiki?curid=15075", "title": "INTERCAL", "text": "Esoteric programming language\nThe Compiler Language With No Pronounceable Acronym (INTERCAL) is an esoteric programming language that was created as a parody by Don Woods and James M. Lyon, two Princeton University students, in 1972. It satirizes aspects of the various programming languages at the time, as well as the proliferation of proposed language constructs and notations in the 1960s.\nThere are two maintained implementations of INTERCAL dialects: C-INTERCAL (created in 1990), maintained by Eric S. Raymond and Alex Smith, and CLC-INTERCAL, maintained by Claudio Calvelli.\nHistory.\nAccording to the original manual by the authors,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The full name of the compiler is \"Compiler Language With No Pronounceable Acronym\", which is, for obvious reasons, abbreviated \"INTERCAL\".\nThe original Princeton implementation used punched cards and the EBCDIC character set. To allow INTERCAL to run on computers using ASCII, substitutions for two characters had to be made: codice_1 substituted for codice_2 as the \"mingle\" operator, \"represent[ing] the increasing cost of software in relation to hardware\", and codice_3 was substituted for codice_4 as the unary exclusive-or operator to \"correctly express the average person's reaction on first encountering exclusive-or\". In recent versions of C-INTERCAL, the older operators are supported as alternatives; INTERCAL programs may now be encoded in ASCII, Latin-1, or UTF-8.\nDetails.\nINTERCAL was intended to be completely different from all other computer languages. Common operations in other languages have cryptic and redundant syntax in INTERCAL. From the INTERCAL Reference Manual:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It is a well-known and oft-demonstrated fact that a person whose work is incomprehensible is held in high esteem. For example, if one were to state that the simplest way to store a value of 65536 in a 32-bit INTERCAL variable is:\nDO :1 &lt;- #0\u00a2#256\nany sensible programmer would say that that was absurd. Since this is indeed the simplest method, the programmer would be made to look foolish in front of his boss, who would of course happen to turn up, as bosses are expected to do. The effect would be no less devastating for the programmer having been correct.\nINTERCAL has many other features designed to make it even more aesthetically unpleasing to the programmer: it uses statements such as \"READ OUT\", \"IGNORE\", \"FORGET\", and modifiers such as \"PLEASE\". This last keyword provides two reasons for the program's rejection by the compiler: if \"PLEASE\" does not appear often enough, the program is considered insufficiently polite, and the error message says this; if it appears too often, the program could be rejected as excessively polite. Although this feature existed in the original INTERCAL compiler, it was undocumented.\nDespite the language's intentionally obtuse and wordy syntax, INTERCAL is nevertheless Turing-complete: given enough memory, INTERCAL can solve any problem that a Universal Turing machine can solve. Most implementations of INTERCAL do this very slowly, however. A Sieve of Eratosthenes benchmark, computing all prime numbers less than 65536, was tested on a Sun SPARCstation 1 in 1992. In C, it took less than half a second; the same program in INTERCAL took over seventeen hours.\nDocumentation.\nThe INTERCAL Reference Manual contains many paradoxical, nonsensical, or otherwise humorous instructions:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Caution! Under no circumstances confuse the mesh with the interleave operator, except under confusing circumstances!\nThe manual also contains a \"tonsil\", as explained in this footnote: \"4) Since all other reference manuals have appendices, it was decided that the INTERCAL manual should contain some other type of removable organ.\"\nThe INTERCAL manual gives unusual names to all non-alphanumeric ASCII characters: single and double quotes are 'sparks' and \"rabbit ears\" respectively. (The exception is the ampersand: as the Jargon File states, \"what could be sillier?\") The assignment operator, represented as an equals sign (INTERCAL's \"half mesh\") in many other programming languages, is in INTERCAL a left-arrow, codice_5, made up of an \"angle\" and a \"worm\", obviously read as \"gets\".\nSyntax.\nInput (using the codice_6 instruction) and output (using the codice_7 instruction) do not use the usual formats; in INTERCAL-72, WRITE IN inputs a number written out as digits in English (such as SIX FIVE FIVE THREE FIVE), and READ OUT outputs it in \"butchered\" Roman numerals. More recent versions have their own I/O systems.\nComments can be achieved by using the inverted statement identifiers involving NOT or N'T; these cause lines to be initially ABSTAINed from so that they have no effect. (A line can be ABSTAINed from even if it does not have valid syntax; syntax errors happen at runtime, and only then when the line is un-ABSTAINed.)\nData structures.\nINTERCAL-72 (the original version of INTERCAL) had only four data types: the 16-bit integer (represented with a codice_8, called a \"spot\"), the 32-bit integer (codice_9, a \"twospot\"), the array of 16-bit integers (codice_10, a \"tail\"), and the array of 32-bit integers (codice_11, a \"hybrid\"). There are 65535 available variables of each type, numbered from codice_12 to codice_13 for 16-bit integers, for instance. However, each of these variables has its own stack on which it can be pushed and popped (STASHed and RETRIEVEd, in INTERCAL terminology), increasing the possible complexity of data structures. More modern versions of INTERCAL have by and large kept the same data structures, with appropriate modifications; TriINTERCAL, which modifies the radix with which numbers are represented, can use a 10-trit type rather than a 16-bit type, and CLC-INTERCAL implements many of its own data structures, such as \"classes and lectures\", by making the basic data types store more information rather than adding new types. Arrays are dimensioned by assigning to them as if they were a scalar variable. Constants can also be used, and are represented by a codice_14 (\"mesh\") followed by the constant itself, written as a decimal number; only integer constants from 0 to 65535 are supported.\nOperators.\nThere are only five operators in INTERCAL-72. Implementations vary in which characters represent which operation, and many accept more than one character, so more than one possibility is given for many of the operators.\nContrary to most other languages, AND, OR, and XOR are unary operators, which work on consecutive bits of their argument; the most significant bit of the result is the operator applied to the least significant and most significant bits of the input, the second-most-significant bit of the result is the operator applied to the most and second-most significant bits, the third-most-significant bit of the result is the operator applied to the second-most and third-most bits, and so on. The operator is placed between the punctuation mark specifying a variable name or constant and the number that specifies which variable it is, or just inside grouping marks (i.e. one character later than it would be in programming languages like C).\nSELECT and INTERLEAVE (which is also known as MINGLE) are infix binary operators. SELECT takes the bits of its first operand that correspond to \"1\" bits of its second operand and removes the bits that correspond to \"0\" bits, shifting towards the least significant bit and padding with zeroes: 51 (110011 in binary) SELECT 21 (10101 in binary) is 5 (101 in binary). MINGLE alternates bits from its first and second operands (in such a way that the least significant bit of its second operand is the least significant bit of the result).\nThere is no operator precedence; grouping marks must be used to disambiguate the precedence where it would otherwise be ambiguous (the grouping marks available are codice_15 (\"spark\"), which matches another spark, and codice_16 (\"rabbit ears\"), which matches another rabbit ears; the programmer is responsible for using these in such a way that they make the expression unambiguous).\nControl structures.\nINTERCAL statements all start with a \"statement identifier\"; in INTERCAL-72, this can be codice_17, codice_18, or codice_19, all of which mean the same to the program (but using one of these too heavily causes the program to be rejected, an undocumented feature in INTERCAL-72 that was mentioned in the C-INTERCAL manual), or an inverted form (with codice_20 or codice_21 appended to the identifier). Backtracking INTERCAL, a modern variant, also allows variants using codice_22 (possibly combined with PLEASE or DO) as a statement identifier, which introduces a choice-point. Before the identifier, an optional line number (an integer enclosed in parentheses) can be given; after the identifier, a percent chance of the line executing can be given in the format codice_23, which defaults to 100%.\nIn INTERCAL-72, the main control structures are NEXT, RESUME, and FORGET. codice_24 branches to the line specified, remembering the next line that would be executed if it weren't for the NEXT on a call stack (other identifiers than DO can be used on any statement, DO is given as an example); codice_25 removes \"expression\" entries from the top of the call stack (this is useful to avoid the error that otherwise happens when there are more than 80 entries), and codice_26 removes \"expression\" entries from the call stack and jumps to the last line remembered.\nC-INTERCAL also provides the COME FROM instruction, written codice_27; CLC-INTERCAL and the most recent C-INTERCAL versions also provide computed COME FROM (codice_28 and NEXT FROM, which is like COME FROM but also saves a return address on the NEXT STACK.\nAlternative ways to affect program flow, originally available in INTERCAL-72, are to use the IGNORE and REMEMBER instructions on variables (which cause writes to the variable to be silently ignored and to take effect again, so that instructions can be disabled by causing them to have no effect), and the ABSTAIN and REINSTATE instructions on lines or on types of statement, causing the lines to have no effect or to have an effect again respectively.\nHello, world.\nThe traditional \"Hello, world!\" program demonstrates how different INTERCAL is from standard programming languages. In C, it could read as follows:\nint main(void) {\n printf(\"Hello, world!\\n\");\n return 0;\nThe equivalent program in C-INTERCAL is longer and harder to read:\nDO ,1 &lt;- #13\nPLEASE DO ,1 SUB #1 &lt;- #238\nDO ,1 SUB #2 &lt;- #108\nDO ,1 SUB #3 &lt;- #112\nDO ,1 SUB #4 &lt;- #0\nDO ,1 SUB #5 &lt;- #64\nDO ,1 SUB #6 &lt;- #194\nDO ,1 SUB #7 &lt;- #48\nPLEASE DO ,1 SUB #8 &lt;- #22\nDO ,1 SUB #9 &lt;- #248\nDO ,1 SUB #10 &lt;- #168\nDO ,1 SUB #11 &lt;- #24\nDO ,1 SUB #12 &lt;- #16\nDO ,1 SUB #13 &lt;- #162\nPLEASE READ OUT ,1\nPLEASE GIVE UP\nDialects.\nThe original Woods\u2013Lyon INTERCAL was very limited in its input/output capabilities: the only acceptable input were numbers with the digits spelled out, and the only output was an extended version of Roman numerals.\nThe C-INTERCAL reimplementation, being available on the Internet, has made the language more popular with devotees of esoteric programming languages. The C-INTERCAL dialect has a few differences from original INTERCAL and introduced a few new features, such as a COME FROM statement and a means of doing text I/O based on the Turing Text Model.\nThe authors of C-INTERCAL also created the TriINTERCAL variant, based on the Ternary numeral system and generalizing INTERCAL's set of operators.\nA more recent variant is Threaded Intercal, which extends the functionality of COME FROM to support multithreading.\nCLC-INTERCAL has a library called INTERNET for networking functionality including being an INTERCAL server, and also includes features such as Quantum Intercal, which enables multi-value calculations in a way purportedly ready for the first quantum computers.\nIn early 2017 a .NET Implementation targeting the .NET Framework appeared on GitHub. This implementation supports the creation of standalone binary libraries and interop with other programming languages. \nImpact and discussion.\nIn the article \"A Box, Darkly: Obfuscation, Weird Languages, and Code Aesthetics\", INTERCAL is described under the heading \"Abandon all sanity, ye who enter here: INTERCAL\". The compiler and commenting strategy are among the \"weird\" features described:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The compiler, appropriately named \"ick\", continues the parody. Anything the compiler can't understand, which in a normal language would result in a compilation error, is just skipped. This \"forgiving\" feature makes finding bugs very difficult; it also introduces a unique system for adding program comments. The programmer merely inserts non-compileable text anywhere in the program, being careful not to accidentally embed a bit of valid code in the middle of their comment.\nIn \"Technomasochism\", Lev Bratishenko characterizes the INTERCAL compiler as a dominatrix:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;If PLEASE was not encountered often enough, the program would be rejected; that is, ignored without explanation by the compiler. Too often and it would still be rejected, this time for sniveling. Combined with other words that are rarely used in programming languages but appear as statements in INTERCAL, the code reads like someone pleading.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15076", "revid": "10783139", "url": "https://en.wikipedia.org/wiki?curid=15076", "title": "International Data Encryption Algorithm", "text": "Symmetric-key block cipher\nIn cryptography, the International Data Encryption Algorithm (IDEA), originally called Improved Proposed Encryption Standard (IPES), is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991. The algorithm was intended as a replacement for the Data Encryption Standard (DES). IDEA is a minor revision of an earlier cipher, the Proposed Encryption Standard (PES).\nThe cipher was designed under a research contract with the Hasler Foundation, which became part of Ascom-Tech AG. The cipher was patented in a number of countries but was freely available for non-commercial use. The name \"IDEA\" is also a trademark. The last patents expired in 2012, and IDEA is now patent-free and thus completely free for all uses.\nIDEA was used in Pretty Good Privacy (PGP) v2.0 and was incorporated after the original cipher used in v1.0, BassOmatic, was found to be insecure. IDEA is an optional algorithm in the OpenPGP standard.\nOperation.\nIDEA operates on 64-bit blocks using a 128-bit key and consists of a series of 8 identical transformations (a \"round\", see the illustration) and an output transformation (the \"half-round\"). The processes for encryption and decryption are similar. IDEA derives much of its security by interleaving operations from different groups \u2014 modular addition and multiplication, and bitwise eXclusive OR (XOR) \u2014 which are algebraically \"incompatible\" in some sense. In more detail, these operators, which all deal with 16-bit quantities, are:\nAfter the 8 rounds comes a final \u201chalf-round\u201d, the output transformation illustrated below (the swap of the middle two values cancels out the swap at the end of the last round, so that there is no net swap):\nStructure.\nThe overall structure of IDEA follows the Lai\u2013Massey scheme. XOR is used for both subtraction and addition. IDEA uses a key-dependent half-round function. To work with 16-bit words (meaning 4 inputs instead of 2 for the 64-bit block size), IDEA uses the Lai\u2013Massey scheme twice in parallel, with the two parallel round functions being interwoven with each other. To ensure sufficient diffusion, two of the sub-blocks are swapped after each round.\nKey schedule.\nEach round uses 6 16-bit sub-keys, while the half-round uses 4, a total of 52 for 8.5 rounds. The first 8 sub-keys are extracted directly from the key, with K1 from the first round being the lower 16 bits; further groups of 8 keys are created by rotating the main key left 25 bits between each group of 8. This means that it is rotated less than once per round, on average, for a total of 6 rotations.\nDecryption.\nDecryption works like encryption, but the order of the round keys is inverted, and the subkeys for the odd rounds are inversed. For instance, the values of subkeys K1\u2013K4 are replaced by the inverse of K49\u2013K52 for the respective group operation, K5 and K6 of each group should be replaced by K47 and K48 for decryption.\nSecurity.\nThe designers analysed IDEA to measure its strength against differential cryptanalysis and concluded that it is immune under certain assumptions. No successful linear or algebraic weaknesses have been reported. As of 2007[ [update]], the best attack applied to all keys could break IDEA reduced to 6 rounds (the full IDEA cipher uses 8.5 rounds). Note that a \"break\" is any attack that requires less than 2128 operations; the 6-round attack requires 264 known plaintexts and 2126.8 operations.\nBruce Schneier thought highly of IDEA in 1996, writing: \"In my opinion, it is the best and most secure block algorithm available to the public at this time.\" (\"Applied Cryptography\", 2nd ed.) However, by 1999 he was no longer recommending IDEA due to the availability of faster algorithms, some progress in its cryptanalysis, and the issue of patents.\nIn 2011 full 8.5-round IDEA was broken using a meet-in-the-middle attack. Independently in 2012, full 8.5-round IDEA was broken using a narrow-bicliques attack, with a reduction of cryptographic strength of about 2\u00a0bits, similar to the effect of the previous bicliques attack on AES; however, this attack does not threaten the security of IDEA in practice.\nWeak keys.\nThe very simple key schedule makes IDEA subject to a class of weak keys; some keys containing a large number of 0 bits produce weak encryption. These are of little concern in practice, being sufficiently rare that they are unnecessary to avoid explicitly when generating keys randomly. A simple fix was proposed: XORing each subkey with a 16-bit constant, such as 0x0DAE.\nLarger classes of weak keys were found in 2002.\nThis is still of negligible probability to be a concern to a randomly chosen key, and some of the problems are fixed by the constant XOR proposed earlier, but the paper is not certain if all of them are. A more comprehensive redesign of the IDEA key schedule may be desirable.\nAvailability.\nA patent application for IDEA was first filed in Switzerland (CH A 1690/90) on May 18, 1990, then an international patent application was filed under the Patent Cooperation Treaty on May 16, 1991. Patents were eventually granted in Austria, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland, the United Kingdom, (European Patent Register entry for https://, filed May 16, 1991, issued June 22, 1994 and expired May 16, 2011), the United States (https://, issued May 25, 1993 and expired January 7, 2012) and Japan (JP 3225440, expired May 16, 2011).\nMediaCrypt AG is now offering a successor to IDEA and focuses on its new cipher (official release in May 2005) IDEA NXT, which was previously called FOX.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15077", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=15077", "title": "Indoor rower", "text": "Exercise machine simulating watercraft rowing\nAn indoor rower, or rowing machine, is a machine used to simulate the action of watercraft rowing for the purpose of exercise or training for rowing. Modern indoor rowers are known as ergometers (colloquially erg or ergo) because they measure work performed by the rower (that can be measured in ergs). Indoor rowing has become established as a sport, drawing a competitive environment from around the world. The term \"indoor rower\" also refers to a participant in this sport.\nHistory.\nChabrias, an Athenian admiral of the 4th century BC, introduced the first rowing machines as supplemental military training devices. \"To train inexperienced oarsmen, Chabrias built wooden rowing frames onshore where beginners could learn technique and timing before they went onboard ship.\"\nEarly rowing machines are known to have existed from the mid-1800s, a US patent being issued to W.B. Curtis in 1872 for a particular hydraulic-based damper design. Machines using linear pneumatic resistance were common around 1900. One of the most popular was the Narragansett hydraulic rower, manufactured in Rhode Island from around 1900\u20131960.\nIn the 1970s, the Gjessing-Nilson ergometer from Norway used a friction brake mechanism with industrial strapping applied over the broad rim of the flywheel. Weights hanging from the strap ensured that an adjustable and predictable friction could be calculated.\nThe first air resistance ergometers were introduced around 1980 by Repco. In 1981, Peter and Richard Dreissigacker, and Jonathan Williams, filed for U.S. patent protection, as joint inventors of a \"Stationary Rowing Unit\". The first commercial embodiment of the Concept2 \"rowing ergometer\" was the Model A, a fixed-frame sliding-seat design using a bicycle wheel with fins attached for air resistance. In 1986, The Model B introduced a solid cast flywheel and the first digital performance monitor, which proved revolutionary. This machine's capability of accurate calibration combined with easy transportability spawned the sport of competitive indoor rowing, and revolutionised training and selection procedures for watercraft rowing. Later models were the C (1993) and D (2003).\nIn 1995, Casper Rekers, a Dutch engineer, was granted a U.S. patent for a (US 5382210A) \"Dynamically Balanced Rowing Simulator\". This device differed from the prior art in that the flywheel and footrests are fixed to a carriage, the carriage being free to slide fore and aft on a rail or rails integral to the frame. The seat is also free to slide fore and aft on a rail or rails integral to the frame.\nEquipment.\nMotion type.\nModern indoor rowers have their resistance provided by a flywheel. Indoor rowers that use flywheel resistance are classified into two motion types. In both models, the user's rowing movement causes the footrests and seat to move further and closer apart in accordance with the user's stroke. The difference between the two types is that the footrests move or do not move relative to the ground.\nThe first type is distinguished by the Dreissigacker/Williams mechanism. This design has the flywheel and footrests fastened to a stationary frame, and the seat can slide fore and aft on a rail or rails built into the stationary frame. Therefore, during use, the seat moves relative to the footrests and also relative to ground, while the flywheel and footrests remain stationary relative to ground.\nThe second type is characterised by the Rekers device. With this type, both the seat and the footrests are free to slide fore and aft on a rail or rails integral to a stationary frame. Therefore, during use, the seat and the footrests move relative to each other, and both also move relative to ground.\nDamper type.\nPiston resistance comes from hydraulic cylinders that are attached to the handles of the rowing machine.\nBraked flywheel resistance models comprise magnetic, air, and water resistance rowers.\nMagnetic resistance models control resistance by means of permanent magnets or electromagnets.\nAir resistance models use vanes on the flywheel to provide the flywheel braking needed to generate resistance.\nWater resistance models consist of a paddle revolving in an enclosed tank of water.\nDual Resistance Rower is a professional fitness equipment with fan and magnetic brake resistance for a variety of intensity levels from warm-ups to HIIT intervals.\nSlides.\nSometimes, slides are placed underneath the machine, which allows it to move back and forth smoothly as if there were water beneath the rower. The slides can be connected in rows or columns so that rowers are forced to move together on the ergometer, similarly to the way they would match up their rhythm in a boat.\nIndoor rowers usually also display estimates of rowing boat speed and energy used by the athlete.\nUse.\nExercise.\nRowing is an example of a method of aerobic exercise, which has been observed to improve athletes' VO2 peak. Indoor rowing primarily works the cardiovascular systems with typical workouts consisting of steady pieces of 20\u201340 minutes.\nThe standard measurement of speed on an ergometer is generally known as the \"split\", or the amount of time in minutes and seconds required to travel at the current pace. Other standard measurement units on the indoor rowing machine include calories and watts.\nTesting.\nAlthough ergometer tests are used by rowing coaches to evaluate rowers and are part of athlete selection for many senior and junior national rowing teams, data suggests \"physiological and performance tests performed on a rowing ergometer are not good indicators of on-water performance\".\nSome standard indoor rower ergometer tests include: 250 m ergometer test, 2000 m ergometer test, 5\u00a0km ergometer test, 16 km ergometer test and the 30-minute ergometer test.\nTechnique.\nRowing on an ergometer requires four basic phases to complete one stroke; the catch, the drive, the finish and the recovery. The catch is the initial part of the stroke. The drive is where the power from the rower is generated while the finish is the final part of the stroke. Then, the recovery is the initial phase to begin taking a new stroke. The phases repeat until a time duration or a distance is completed. At each stage of the stroke, the back should remain in a neutral, flat position, pivoting at the hips to avoid injury.\nCatch.\nKnees are bent with the shins in a vertical position. The back should be roughly parallel to the thigh without hyperflexion (leaning forward too far). The arms and shoulders should be extended forward and relaxed. The arms should also be level.\nDrive.\nThe drive is initiated by a push and extension of the legs; the body remains in the catch posture at this point of the drive. As the legs continue to full extension, the hip angle opens and the rower engages the core to begin the motion of the body levering backward, adding to the work of the legs. When the legs are fully extended, the rower begins to pull the handle toward the chest with their arms, completing the stroke with the handle halfway up the body and the forearms parallel to the ground.\nFinish (or release).\nThe legs are at full extension and flat. The shoulders are slightly behind the pelvis, and the arms are in full contraction with the elbows bent and hands against the chest below the nipples. The back of the rower is still maintained in an upright posture and wrists should be flat.\nRecovery.\nThe recovery is a slow slide back to the initial part of the stroke, it gives the rower time to recover from the previous stroke. During the recovery, the actions are in reverse order of the drive. The recovery is initiated by the extensions of the arms until fully extended in front of the body. The torso is then engaged by pivoting at the hips to move the torso in front of the hips. Weight transfers from the back of the seat to the front of the seat at this time. When the hands come over the knees, the legs are bent at the knees, moving the slide towards the front of the machine. As the back becomes more parallel to the thighs, the recovery is completed when the shins are perpendicular to the ground. At this point the recovery transitions to the catch for the next stroke.\nSport.\nThe first indoor rowing competition was held in Cambridge, Massachusetts, in February 1982 with participation of 96 on-water rowers who called themselves the \"Charles River Association of Sculling Has-Beens\", hence the acronym, \"CRASH-B\". The core events for indoor rowing competitions that are currently competed in at the World Rowing Indoor Championships are the individual 500m, individual 2000m, individual 1 hour, and 3-minute teams event. Events at other indoor rowing competitions include the mile and the 2500-meter.\nMost competitions are organised into categories based on sex, age, and weight class.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15078", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=15078", "title": "Internetwork Packet Exchange", "text": "Network protocol\nInternetwork Packet Exchange (IPX) is the network-layer protocol in the IPX/SPX protocol suite. IPX is derived from Xerox Network Systems' IDP. It also has the ability to act as a transport layer protocol.\nThe IPX/SPX protocol suite was very popular through the late 1980s and mid-1990s because it was used by Novell NetWare, a network operating system. Due to Novell NetWare's popularity, IPX became a prominent protocol for internetworking.\nA big advantage of IPX was a small memory footprint of the IPX driver, which was vital for DOS and Windows up to Windows 95 due to the limited size at that time of conventional memory. Another IPX advantage was easy configuration of its client computers. However, IPX does not scale well for large networks such as the Internet. As such, IPX usage decreased as the boom of the Internet made TCP/IP nearly universal. \nComputers and networks can run multiple network protocols, so almost all IPX sites also ran TCP/IP, to allow Internet connectivity. It was also possible to run later Novell products without IPX, with the beginning of full support for both IPX and TCP/IP by NetWare version 5 in late 1998.\nDescription.\nA big advantage of IPX protocol is its little or no need for configuration. In the time when protocols for dynamic host configuration did not exist and the BOOTP protocol for centralized assigning of addresses was not common, the IPX network could be configured almost automatically. A client computer uses the MAC address of its network card as the node address and learns what it needs to know about the network topology from the servers or routers \u2013 routes are propagated by Routing Information Protocol, services by Service Advertising Protocol.\nA small IPX network administrator had to care only\nIPX packet structure.\nEach IPX packet begins with a header with the following structure:\nThe Packet Type values are:\nIPX addressing.\nAn IPX address has the following structure:\nNetwork number.\nThe network number allows to address (and communicate with) the IPX nodes which do not belong to the same network or \"cabling system\". The cabling system is a network in which a data link layer protocol can be used for communication. To allow communication between different networks, they must be connected with IPX routers. A set of interconnected networks is called an internetwork. Any Novell NetWare server may serve as an IPX router. Novell also supplied stand-alone routers. Other vendors ' multiprotocol routers often support IPX routing. Using different frame formats in one cabling system is possible, but it works similarly as if separate cabling systems were used (i.e. different network numbers must be used for different frame formats even in the same cabling system and a router must be used to allow communication between nodes using different frame formats in the same cabling system).\nNode number.\nThe node number is used to address an individual computer (or more exactly, a network interface) in the network. Client stations use its network interface card MAC address as the node number.\nThe value FF:FF:FF:FF:FF:FF may be used as a node number in a destination address to broadcast a packet to \"all nodes in the current network\".\nSocket number.\nThe socket number serves to select a process or application in the destination node.\nThe presence of a socket number in the IPX address allows the IPX to act as a transport layer protocol, comparable with the User Datagram Protocol (UDP) in the Internet protocol suite.\nComparison with IP.\nThe IPX network number is conceptually identical to the network part of the IP address (the parts with netmask bits set to 1); the node number has the same meaning as the bits of IP address with netmask bits set to 0. The difference is that the boundary between network and node part of address in IP is variable, while in IPX it is fixed. As the node address is usually identical to the MAC address of the network adapter, the Address Resolution Protocol is not needed in IPX.\nFor routing, the entries in the IPX routing table are similar to IP routing tables; routing is done by network address, and for each network address a network:node of the next router is specified in a similar fashion an IP address/netmask is specified in IP routing tables.\nThere are three routing protocols available for IPX networks. In early IPX networks, a version of Routing Information Protocol (RIP) was the only available protocol to exchange routing information. Unlike RIP for IP, it uses delay time as the main metric, retaining the hop count as a secondary metric. Since NetWare 3, the NetWare Link Services Protocol (NLSP) based on IS-IS is available, which is more suitable for larger networks. Cisco routers implement an IPX version of EIGRP protocol as well.\nFrame formats.\nIPX can be transmitted over Ethernet using one of the following 4 frame formats or encapsulation types:\nIn non-Ethernet networks, only 802.2 and SNAP frame types are available."}
{"id": "15079", "revid": "50591554", "url": "https://en.wikipedia.org/wiki?curid=15079", "title": "International human rights instruments", "text": "Treaties for the protection of human rights\nInternational human rights instruments are the treaties and other international texts that serve as legal sources for international human rights law and the protection of human rights in general. There are many varying types, but most can be classified into two broad categories: \"declarations\", adopted by bodies such as the United Nations General Assembly, which are by nature declaratory, so not legally-binding although they may be politically authoritative and very well-respected soft law, and often express guiding principles; and \"conventions\" that are multi-party treaties that are designed to become legally binding, usually include prescriptive and very specific language, and usually are concluded by a long procedure that frequently requires ratification by each states' legislature. Less well known are certain \"recommendations\" which, while similar to conventions in that they are multilaterally agreed, cannot be ratified and instead serve to establish common standards. There may also be administrative guidelines that are agreed multilaterally by states, as well as the statutes of tribunals or other institutions. A specific prescription or principle from any of these various international instruments can, over time, attain the status of customary international law whether it is specifically accepted by a state or not, just because it is well-recognized and followed over a sufficiently long time.\nInternational human rights instruments can be divided further into \"global instruments\", to which any state in the world can be a party, and \"regional instruments\", which are restricted to states in a particular region of the world.\nMost conventions and recommendations (but few declarations) establish mechanisms for monitoring and establish bodies to oversee their implementation. In some cases these bodies that may have relatively little political authority or legal means, and may be ignored by member states; in other cases these mechanisms have bodies with great political authority and their decisions are almost always implemented. A good example of the latter is the European Court of Human Rights.\nMonitoring mechanisms also vary as to the degree of individual access to expose cases of abuse and plea for remedies. Under some conventions or recommendations \u2013 e.g. the European Convention on Human Rights \u2013 individuals or states are permitted, subject to certain conditions, to take individual cases to a full-fledged tribunal at international level. Sometimes, this can be done in national courts because of universal jurisdiction.\nThe Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights, and the International Covenant on Economic, Social and Cultural Rights together with other international human rights instruments are sometimes referred to as the \"International Bill of Human Rights\". International human rights instruments are identified by the OHCHR and most are referenced on the OHCHR website.\nConventions.\nGlobal.\nAccording to OHCHR, there are 9 \"core\" international human rights instruments and several optional protocols.\nSeveral more human rights instruments exist. A few examples:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15080", "revid": "8240947", "url": "https://en.wikipedia.org/wiki?curid=15080", "title": "Indian removal", "text": "Early 19th-century United States domestic policy\nThe Indian removal was the United States government's policy of ethnic cleansing through the forced displacement of self-governing tribes of American Indians from their ancestral homelands in the eastern United States to lands west of the Mississippi River\u2014specifically, to a designated Indian Territory (roughly, present-day Oklahoma), which many scholars have labeled a genocide. The Indian Removal Act of 1830, the key law which authorized the removal of Native tribes, was signed into law by United States president Andrew Jackson on May 28, 1830. Although Jackson took a hard line on Indian removal, the law was primarily enforced during the Martin Van Buren administration, 1837 to 1841. After the enactment of the Act, approximately 60,000 members of the Cherokee, Muscogee (Creek), Seminole, Chickasaw, and Choctaw nations (including thousands of their black slaves) were forcibly removed from their ancestral homelands, with thousands dying during the Trail of Tears.\nIndian removal, a popular policy among incoming settlers, was a consequence of actions first by the European colonists and then later on by the American settlers in the nation during the thirteen colonies and then after the revolution, in the United States of America also until the mid-20th century.\nThe origins of the policy date back to the administration of James Monroe, but it addressed conflicts which had occurred between the American settlers and Indigenous tribes since the 17th century and were escalating into the early 19th century (as settlers pushed westward in accordance with the cultural belief of manifest destiny). Historical views of Indian removal have been reevaluated since that time. Widespread contemporary acceptance of the policy, due in part to the popular embrace of the concept of manifest destiny, has given way to a more somber perspective. Historians have often described the removal of American Indians as paternalism, ethnic cleansing, or genocide.\nBackground.\nAmerican leaders in the Revolutionary and early US eras debated about whether Native Americans should be treated as individuals or as nations.\nDeclaration of Independence.\nIn the indictment section of the Declaration of Independence, the Indigenous inhabitants of the United States are referred to as \"merciless Indian Savages\", reflecting a commonly held view at the time by the colonists in the United States.\nBenjamin Franklin.\nIn a draft \"Proposed Articles of Confederation\" presented to the Continental Congress on May 10, 1775, Benjamin Franklin called for a \"perpetual Alliance\" with the Indians in the nation about to be born, particularly with the six nations of the Iroquois Confederacy:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Article XI. A perpetual alliance offensive and defensive is to be entered into as soon as may be with the Six Nations; their Limits to be ascertained and secured to them; their Land not to be encroached on, nor any private or Colony Purchases made of them hereafter to be held good, nor any Contract for Lands to be made but between the Great Council of the Indians at Onondaga and the General Congress. The Boundaries and Lands of all the other Indians shall also be ascertained and secured to them in the same manner; and Persons appointed to reside among them in proper Districts, who shall take care to prevent Injustice in the Trade with them, and be enabled at our general Expense by occasional small Supplies, to relieve their personal Wants and Distresses. And all Purchases from them shall be by the Congress for the General Advantage and Benefit of the United Colonies.\nEarly congressional acts.\nThe Confederation Congress passed the Northwest Ordinance of 1787 (a precedent for US territorial expansion would occur for years to come), calling for the protection of Native American \"property, rights, and liberty\"; the US Constitution of 1787 (Article I, Section 8) made Congress responsible for regulating commerce with the Indian tribes. In 1790, the new US Congress passed the Indian Nonintercourse Act (renewed and amended in 1793, 1796, 1799, 1802, and 1834) to protect and codify the land rights of recognized tribes.\nGeorge Washington.\nPresident George Washington, in his 1790 address to the Seneca Nation which called the pre-Constitutional Indian land-sale difficulties \"evils\", said that the case was now altered and pledged to uphold Native American \"just rights\". In March and April 1792, Washington met with 50 tribal chiefs in Philadelphia\u2014including the Iroquois\u2014to discuss strengthening the friendship between them and the United States. Later that year, in his fourth annual message to Congress, Washington stressed the need to build peace, trust, and commerce with Native Americans:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I cannot dismiss the subject of Indian affairs without again recommending to your consideration the expediency of more adequate provision for giving energy to the laws throughout our interior frontier, and for restraining the commission of outrages upon the Indians; without which all pacific plans must prove nugatory. To enable, by competent rewards, the employment of qualified and trusty persons to reside among them, as agents, would also contribute to the preservation of peace and good neighbourhood. If, in addition to these expedients, an eligible plan could be devised for promoting civilization among the friendly tribes, and for carrying on trade with them, upon a scale equal to their wants, and under regulations calculated to protect them from imposition and extortion, its influence in cementing their interests with our's [sic] could not but be considerable.\nIn his seventh annual message to Congress in 1795, Washington intimated that if the US government wanted peace with the Indians it must behave peacefully; if the US wanted raids by Indians to stop, raids by American \"frontier inhabitants\" must also stop.\nThomas Jefferson.\nIn his \"Notes on the State of Virginia\" (1785), Thomas Jefferson defended Native American culture and marveled at how the tribes of Virginia \"never submitted themselves to any laws, any coercive power, any shadow of government\" due to their \"moral sense of right and wrong\". He wrote to the Marquis de Chastellux later that year, \"I believe the Indian then to be in body and mind equal to the whiteman\". Jefferson's desire, as interpreted by Francis Paul Prucha, was for Native Americans to intermix with European Americans and become one people. To achieve that end as president, Jefferson offered US citizenship to some Indian nations and proposed offering them credit to facilitate trade.\nOn 27 February 1803, Jefferson wrote in a letter to William Henry Harrison:In this way our settlements will gradually circumbscribe &amp; approach the Indians, &amp; they will in time either incorporate with us as citizens of the US. or remove beyond the Missisipi. The former is certainly the termination of their history most happy for themselves. But in the whole course of this, it is essential to cultivate their love. As to their fear, we presume that our strength &amp; their weakness is now so visible that they must see we have only to shut our hand to crush them, &amp; that all our liberalities to them proceed from motives of pure humanity only.\nJeffersonian policy.\nAs president, Thomas Jefferson developed a far-reaching Indian policy with two primary goals. He wanted to assure that the Native nations (not foreign nations) were tightly bound to the new United States, as he considered the security of the nation to be paramount. He also wanted to \"civilize\" them into adopting an agricultural, rather than a hunter-gatherer, lifestyle. These goals would be achieved through treaties and the development of trade.\nJefferson initially promoted an American policy which encouraged Native Americans to become assimilated, or \"civilized\". He made sustained efforts to win the friendship and cooperation of many Native American tribes as president, repeatedly articulating his desire for a united nation of whites and Indians as in his November 3, 1802, letter to Seneca spiritual leader Handsome Lake:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nWhen a delegation from the Cherokee Nation's Upper Towns lobbied Jefferson for the full and equal citizenship promised to Indians living in American territory by George Washington, his response indicated that he was willing to grant citizenship to those Indian nations who sought it. In his eighth annual message to Congress on November 8, 1808, he presented a vision of white and Indian unity:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAs some of Jefferson's other writings illustrate, however, he was ambivalent about Indian assimilation and used the words \"exterminate\" and \"extirpate\" about tribes who resisted American expansion and were willing to fight for their lands. Jefferson intended to change Indian lifestyles from hunting and gathering to farming, largely through \"the decrease of game rendering their subsistence by hunting insufficient\". He expected the change to agriculture to make them dependent on white Americans for goods, and more likely to surrender their land or allow themselves to be moved west of the Mississippi River. In an 1803 letter to William Henry Harrison, Jefferson wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIn that letter, Jefferson spoke about protecting the Indians from injustices perpetrated by settlers:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAccording to the treaty of February 27, 1819, the US government would offer citizenship and of land per family to Cherokees who lived east of the Mississippi. Native American land was sometimes purchased, by treaty or under duress. The idea of land exchange, that Native Americans would give up their land east of the Mississippi in exchange for a similar amount of territory west of the river, was first proposed by Jefferson in 1803 and first incorporated into treaties in 1817 (years after the Jefferson presidency). The Indian Removal Act of 1830 included this concept.\nJohn C. Calhoun's plan.\nUnder President James Monroe, Secretary of War John C. Calhoun devised the first plans for Indian removal. Monroe approved Calhoun's plans by late 1824 and, in a special message to the Senate on January 27, 1825, requested the creation of the Arkansaw and Indian Territories; the Indians east of the Mississippi would voluntarily exchange their lands for lands west of the river. The Senate accepted Monroe's request, and asked Calhoun to draft a bill which was killed in the House of Representatives by the Georgia delegation. President John Quincy Adams assumed the Calhoun\u2013Monroe policy, and was determined to remove the Indians by non-forceful means; Georgia refused to consent to Adams' request, forcing the president to forge a treaty with the Cherokees granting Georgia the Cherokee lands. On July 26, 1827, the Cherokee Nation adopted a written constitution (modeled on that of the United States) which declared that they were an independent nation with jurisdiction over their own lands. Georgia contended that it would not countenance a sovereign state within its own territory, and asserted its authority over Cherokee territory. When Andrew Jackson became president as the candidate of the newly-organized Democratic Party, he agreed that the Indians should be forced to exchange their eastern lands for western lands (including relocation) and vigorously enforced Indian removal.\nOpposition to removal from US citizens.\nAlthough Indian removal was a popular policy, it was also opposed on legal and moral grounds; it also ran counter to the formal, customary diplomatic interaction between the federal government and the Native nations. Author and critic John Neal wrote fiction in opposition to Indian removal policy. The short stories \"Otter-Bag, the Oneida Chief\" (1829) and \"David Whicher\" (1832) was his response to Jacksonian policy, as well as prevailing themes in American literature depicting white and Native Americans as irreconcilable enemies. Ralph Waldo Emerson wrote the widely-published letter \"A Protest Against the Removal of the Cherokee Indians from the State of Georgia\" in 1838, shortly before the Cherokee removal. Emerson criticizes the government and its removal policy, saying that the removal treaty was illegitimate; it was a \"sham treaty\", which the US government should not uphold. He describes removal as such a dereliction of all faith and virtues, such a denial of justice... in the dealing of a nation with its own allies and wards since the earth was made... a general expression of despondency, of disbelief, that any goodwill accrues from a remonstrance on an act of fraud and robbery, appeared in those men to whom we naturally turn for aid and counsel. Emerson concludes his letter by saying that it should not be a political issue, urging President Martin Van Buren to prevent the enforcement of Cherokee removal. Other individual settlers and settler social organizations throughout the United States also opposed removal.\nNative American response to removal.\nNative groups reshaped their governments, made constitutions and legal codes, and sent delegates to Washington to negotiate policies and treaties to uphold their autonomy and ensure federally-promised protection from the encroachment of states. They thought that acclimating, as the US wanted them to, would stem removal policy and create a better relationship with the federal government and surrounding states.\nNative American nations had differing views about removal. Although most wanted to remain on their native lands and do anything possible to ensure that, others believed that removal to a nonwhite area was their only option to maintain their autonomy and culture. The US used this division to forge removal treaties with (often) minority groups who became convinced that removal was the best option for their people. These treaties were often not acknowledged by most of a nation's people. When Congress ratified the removal treaty, the federal government could use military force to remove Native nations if they had not moved (or had begun moving) by the date stipulated in the treaty.\nIndian Removal Act.\nWhen Andrew Jackson became president of the United States in 1829, his government took a hard line on Indian removal; Jackson abandoned his predecessors' policy of treating Indian tribes as separate nations, aggressively pursuing all Indians east of the Mississippi who claimed constitutional sovereignty and independence from state laws. They were to be removed to reservations in Indian Territory, west of the Mississippi (present-day Oklahoma), where they could exist without state interference. At Jackson's request, Congress began a debate on an Indian-removal bill. After fierce disagreement, the Senate passed the bill by a 28\u201319 vote; the House had narrowly passed it, 102\u201397. Jackson signed the Indian Removal Act into law on May 30, 1830.\nThat year, most of the Five Civilized Tribes\u2014the Chickasaw, Choctaw, Creek, Seminole, and Cherokee\u2014lived east of the Mississippi. The Indian Removal Act implemented federal-government policy towards its Indian populations, moving Native American tribes east of the Mississippi to lands west of the river. Although the act did not authorize the forced removal of indigenous tribes, it enabled the president to negotiate land-exchange treaties.\nChoctaw.\nOn September 27, 1830, the Choctaw signed the Treaty of Dancing Rabbit Creek and became the first Native American tribe to be removed. The agreement was one of the largest transfers of land between the US government and Native Americans which was not the result of war. The Choctaw signed away their remaining traditional homelands, opening them up for European\u2013American settlement in Mississippi Territory. When the tribe reached Little Rock, a chief called its trek a \"trail of tears and death\".\nIn 1831, French historian and political scientist Alexis de Tocqueville witnessed an exhausted group of Choctaw men, women and children emerging from the forest during an exceptionally cold winter near Memphis, Tennessee, on their way to the Mississippi to be loaded onto a steamboat. He wrote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the whole scene there was an air of ruin and destruction, something which betrayed a final and irrevocable adieu; one couldn't watch without feeling one's heart wrung. The Indians were tranquil but sombre and taciturn. There was one who could speak English and of whom I asked why the Chactas were leaving their country. \"To be free,\" he answered, could never get any other reason out of him. We\u00a0... watch the expulsion\u00a0... of one of the most celebrated and ancient American peoples.\nCherokee.\nWhile the Indian Removal Act made the move of the tribes voluntary, it was often abused by government officials. The best-known example is the Treaty of New Echota, which was signed by a small faction of twenty Cherokee tribal members (not the tribal leadership) on December 29, 1835. Most of the Cherokee later blamed the faction and the treaty for the tribe's forced relocation in 1838. An estimated 4,000 Cherokee died in the march, which is known as the Trail of Tears. Missionary organizer Jeremiah Evarts urged the Cherokee Nation to take its case to the US Supreme Court.\nThe Marshall court heard the case in \"Cherokee Nation v. Georgia\" (1831), but declined to rule on its merits; the court declaring that the Native American tribes were not sovereign nations, and could not \"maintain an action\" in US courts. In an opinion written by Chief Justice Marshall in \"Worcester v. Georgia\" (1832), individual states had no authority in American Indian affairs.\nThe state of Georgia defied the Supreme Court ruling, and the desire of settlers and land speculators for Indian lands continued unabated; some whites claimed that Indians threatened peace and security. The Georgia legislature passed a law forbidding settlers from living on Indian territory after March 31, 1831, without a license from the state; this excluded missionaries who opposed Indian removal.\nSeminole.\nThe Seminole refused to leave their Florida lands in 1835, leading to the Second Seminole War. Osceola was a Seminole leader of the people's fight against removal. Based in the Everglades, Osceola and his band used surprise attacks to defeat the US Army in a number of battles. In 1837, Osceola was duplicitously captured by order of US General Thomas Jesup when Osceola came under a flag of truce to negotiate peace near Fort Peyton. Osceola died in prison of illness; the war resulted in over 1,500 US deaths, and cost the government $20 million. Some Seminole traveled deeper into the Everglades, and others moved west. The removal continued, and a number of wars broke out over land. In 1823, the Seminole signed the Treaty of Moultrie Creek, which reduced their 34 million to 4 million acres.\nMuskogee (Creek).\nIn the aftermath of the Treaties of Fort Jackson, and the Washington, the Muscogee were confined to a small strip of land in present-day east central Alabama. The Creek national council signed the Treaty of Cusseta in 1832, ceding their remaining lands east of the Mississippi to the US and accepting relocation to the Indian Territory. Most Muscogee were removed to the territory during the Trail of Tears in 1834, although some remained behind. Although the Creek War of 1836 ended government attempts to convince the Creek population to leave voluntarily, Creeks who had not participated in the war were not forced west (as others were). The Creek population was placed into camps and told that they would be relocated soon. Many Creek leaders were surprised by the quick departure but could do little to challenge it. The 16,000 Creeks were organized into five detachments who were to be sent to Fort Gibson. The Creek leaders did their best to negotiate better conditions, and succeeded in obtaining wagons and medicine. To prepare for the relocation, Creeks began to deconstruct their spiritual lives; they burned piles of lightwood over their ancestors' graves to honor their memories, and polished the sacred plates which would travel at the front of each group. They also prepared financially, selling what they could not bring. Many were swindled by local merchants out of valuable possessions (including land), and the military had to intervene. The detachments began moving west in September 1836, facing harsh conditions. Despite their preparations, the detachments faced bad roads, worse weather, and a lack of drinkable water. When all five detachments reached their destination, they recorded their death toll. The first detachment, with 2,318 Creeks, had 78 deaths; the second had 3,095 Creeks, with 37 deaths. The third had 2,818 Creeks, and 12 deaths; the fourth, 2,330 Creeks and 36 deaths. The fifth detachment, with 2,087 Creeks, had 25 deaths. In 1837 outside of Baton Rouge, Louisiana over 300 Creeks being forcibly removed to Western prairies drowned in the Mississippi River.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Friends and Brothers \u2013 By permission of the Great Spirit above, and the voice of the people, I have been made President of the United States, and now speak to you as your Father and friend, and request you to listen. Your warriors have known me long. You know I love my white and red children, and always speak with a straight, and not with a forked tongue; that I have always told you the truth ... Where you now are, you and my white children are too near to each other to live in harmony and peace. Your game is destroyed, and many of your people will not work and till the earth. Beyond the great River Mississippi, where a part of your nation has gone, your Father has provided a country large enough for all of you, and he advises you to remove to it. There your white brothers will not trouble you; they will have no claim to the land, and you can live upon it you and all your children, as long as the grass grows or the water runs, in peace and plenty. It will be yours forever. For the improvements in the country where you now live, and for all the stock which you cannot take with you, your Father will pay you a fair price\u00a0...\nChickasaw.\nUnlike other tribes, who exchanged lands, the Chickasaw were to receive financial compensation of $3\u00a0million from the United States for their lands east of the Mississippi River. They reached an agreement to purchase of land from the previously-removed Choctaw in 1836 after a bitter five-year debate, paying the Chocktaw $530,000 for the westernmost Choctaw land. Most of the Chickasaw moved in 1837 and 1838. The $3\u00a0million owed to the Chickasaw by the US went unpaid for nearly 30 years.\nAftermath.\nThe Five Civilized Tribes were resettled in the new Indian Territory. The Cherokee occupied the northeast corner of the territory and a strip of land in Kansas on its border with the territory. Some indigenous nations resisted the forced migration more strongly. The few who stayed behind eventually formed tribal groups, including the Eastern Band of Cherokee (based in North Carolina), the Mississippi Band of Choctaw Indians, the Seminole Tribe of Florida, and the Creeks in Alabama (including the Poarch Band).\nRemovals.\nNorth.\nTribes in the Old Northwest were smaller and more fragmented than the Five Civilized Tribes, so the treaty and emigration process was more piecemeal. Following the Northwest Indian War, most of the modern state of Ohio was taken from native nations in the 1795 Treaty of Greenville. Tribes such as the already-displaced Lenape (Delaware tribe), Kickapoo and Shawnee, were removed from Indiana, Michigan, and Ohio during the 1820s. The Potawatomi were forced out of Wisconsin and Michigan in late 1838, and were resettled in Kansas Territory. Communities remaining in present-day Ohio were forced to move to Louisiana, which was then controlled by Spain.\nBands of Shawnee, Ottawa, Potawatomi, Sauk, and Meskwaki (Fox) signed treaties and relocated to the Indian Territory. In 1832, the Sauk leader Black Hawk led a band of Sauk and Fox back to their lands in Illinois; the US Army and Illinois militia defeated Black Hawk and his warriors in the Black Hawk War, and the Sauk and Fox were relocated to present-day Iowa. The Miami were split, with many of the tribe resettled west of the Mississippi River during the 1840s.\nIn the Second Treaty of Buffalo Creek (1838), the Senecas transferred all their land in New York (except for one small reservation) in exchange for of land in Indian Territory. The federal government would be responsible for the removal of the Senecas who opted to go west, and the Ogden Land Company would acquire their New York lands. The lands were sold by government officials, however, and the proceeds were deposited in the US Treasury. Maris Bryant Pierce, a \"young chief\" served as a lawyer representing four territories of the Seneca tribe, starting in 1838. The Senecas asserted that they had been defrauded, and sued for redress in the Court of Claims. The case was not resolved until 1898, when the United States awarded $1,998,714.46 (~$ in 2024) in compensation to \"the New York Indians\". The US signed treaties with the Senecas and the Tonawanda Senecas in 1842 and 1857, respectively. Under the treaty of 1857, the Tonawandas renounced all claim to lands west of the Mississippi in exchange for the right to buy back the Tonawanda Reservation from the Ogden Land Company. Over a century later, the Senecas purchased a plot (part of their original reservation) in downtown Buffalo to build the Seneca Buffalo Creek Casino.\nChanged perspective.\nHistorical views of Indian removal have been reevaluated since that time. Widespread contemporary acceptance of the policy, due in part to the popular embrace of the concept of manifest destiny, has given way to a more somber perspective. Historians have often described the removal of Native Americans as paternalism, ethnic cleansing, or genocide. Historian David Stannard has called it genocide.\nAndrew Jackson's reputation.\nAndrew Jackson's Indian policy stirred a lot of public controversy before his enactment, but virtually none among historians and biographers of the 19th and early 20th century. However, his recent reputation has been negatively affected by his treatment of the Indians. Historians who admire Jackson's strong presidential leadership, such as Arthur M. Schlesinger, Jr., would gloss over the Indian Removal in a footnote. In 1969, Francis Paul Prucha defended Jackson's Indian policy and wrote that Jackson's removal of the Five Civilized Tribes from the hostile political environment of the Old South to Oklahoma probably saved them. Jackson was sharply attacked by political scientist Michael Rogin and historian Howard Zinn during the 1970s, primarily on this issue; Zinn called him an \"exterminator of Indians\". According to historians Paul R. Bartrop and Steven L. Jacobs, however, Jackson's policies do not meet the criteria for physical or cultural genocide. Historian Sean Wilentz describes the view of Jacksonian \"infantilization\" and \"genocide\" of the Indians, as a historical caricature, which \"turns tragedy into melodrama, exaggerates parts at the expense of the whole, and sacrifices nuance for sharpness\"."}
{"id": "15081", "revid": "46933780", "url": "https://en.wikipedia.org/wiki?curid=15081", "title": "Green Party (Ireland)", "text": "Irish political party\nThe Green Party (, lit.\u2009'Green Alliance') is a green political party that operates in the Republic of Ireland and Northern Ireland. It holds a pro-European stance. It was founded as the Ecology Party of Ireland in 1981 by Dublin teacher Christopher Fettes. The party became the Green Alliance in 1983 and adopted its current English language name in 1987 while the Irish name was kept unchanged. The party leader is Roderic O'Gorman, the deputy leader is Senator R\u00f3is\u00edn Garvey and the cathaoirleach (chairperson) is Janet Horner. Green Party candidates have been elected to most levels of representation: local government (in both the Republic of Ireland and Northern Ireland), D\u00e1il \u00c9ireann, the Northern Ireland Assembly, and the European Parliament.\nThe Green Party first entered the D\u00e1il in 1989. It has participated in the Irish government twice, from 2007 to 2011 as junior partner in a coalition with Fianna F\u00e1il, and from 2020 to 2024 in a coalition with Fianna F\u00e1il and Fine Gael. Following the first period in government, the party suffered a wipeout in the February 2011 election, losing all six of its TDs. In the February 2016 election, it returned to the D\u00e1il with two seats. Following this, Grace O'Sullivan was elected to the Seanad on 26 April that year of 2016 and Joe O'Brien was elected to D\u00e1il \u00c9ireann in the 2019 Dublin Fingal by-election. In the 2020 general election, the party had its best result ever, securing 12 TDs and becoming the fourth largest party in the 33rd D\u00e1il before losing all but one seat in the 2024 general election.\nHistory.\nEarly years and first rise.\nThe Green Party began life as the \"Ecology Party\" in 1981, with Christopher Fettes serving as the party's first chairperson. The party's first public appearance was modest: the event announced that they would be contesting the November 1982 general election, and was attended by their seven election candidates, 20 party supporters, and one singular journalist. Fettes had opened the meeting by noting the party didn't expect to win any seats. Willy Clingan, the journalist present, recalled that \"The Ecology Party introduced its seven election candidates at the nicest and most endearingly honest press conference of the whole campaign\". The Ecology party took 0.2% of the vote that year.\nFollowing a name change to the \"Green Alliance\", it contested the 1984 European elections, with party founder Roger Garland winning 1.9% in the Dublin constituency. The following year, it won its first election when Marcus Counihan was elected to Killarney Urban District Council at the 1985 local elections, buoyed by winning 5,200 first preference votes as a European candidate in Dublin the previous year. The party nationally ran 34 candidates and won 0.6% of the vote.\nThe party continued to struggle until the 1989 general election when the Green Party (as it was now named) won its first seat in D\u00e1il \u00c9ireann, when Roger Garland was elected in Dublin South. Garland lost his seat at the 1992 general election, while Trevor Sargent gained a seat in Dublin North. In the 1994 European election, Patricia McKenna topped the poll in the Dublin constituency and Nuala Ahern won a seat in Leinster. They retained their European Parliament seats in the 1999 European election, although the party lost five councillors in local elections held that year despite an increase in its vote. At the 1997 general election, the party gained a seat when John Gormley won a D\u00e1il seat in Dublin South-East.\nAt the 2002 general election, the party made a breakthrough, getting six Teachta\u00ed D\u00e1la (TDs) elected to the D\u00e1il with 4% of the national vote. However, in the 2004 European election, the party lost both of its European Parliament seats. In the 2004 local elections, it increased its number of councillors at county level from eight to 18 (out of 883) and at town council level from five to 14 (out of 744).\nThe party gained its first representation in the Northern Ireland Assembly in 2007, the Green Party in Northern Ireland having become a regional branch of the party the previous year.\nFirst term in government.\nThe Green Party entered government for the first time after the 2007 general election, held on 24 May. Although its share of first-preference votes increased at the election, the party failed to increase the number of TDs returned. Mary White won a seat for the first time in Carlow\u2013Kilkenny; however, Dan Boyle lost his seat in Cork South-Central. The party had approached the 2007 general election on an independent platform, not ruling any out coalition partners while expressing its preference for an alternative to the outgoing coalition of Fianna F\u00e1il and the Progressive Democrats. Neither the outgoing government nor an alternative of Fine Gael, Labour and the Green Party had sufficient seats to form a majority. Fine Gael ruled out a coalition arrangement with Sinn F\u00e9in, opening the way for Green Party negotiations with Fianna F\u00e1il.\nSome saw the idea of going into coalition with Fianna F\u00e1il as a \"sell-out\". Before the negotiations began, Ciar\u00e1n Cuffe TD wrote on his blog that \"a deal with Fianna F\u00e1il would be a deal with the devil\u2026 and [the Green Party would be] decimated as a Party\". After protracted negotiations, a draft programme for government was agreed to between the Greens and Fianna F\u00e1il. Early Green demands included the introduction of legislation on corporate donations, a moratorium on using public land to build private hospitals, and altering the route of the M3 motorway near the Hill of Tara; none of these demands appeared in the final government programme. On 13 June 2007, Green members at the Mansion House in Dublin voted 86% in favour (441 to 67; with 2 spoilt votes) of entering coalition with Fianna F\u00e1il. The following day, the six Green Party TDs voted for the re-election of Bertie Ahern as Taoiseach. New party leader John Gormley was appointed as Minister for the Environment, Heritage and Local Government and Eamon Ryan was appointed as Minister for Communications, Energy and Natural Resources. Trevor Sargent was appointed as Minister of State at the Department of Agriculture, Fisheries and Food with responsibility for Food and Horticulture.\nBefore its entry into government, the Green Party had been a vocal supporter of the Shell to Sea movement, the campaign to reroute the M3 motorway away from Tara, and (to a lesser extent) the campaign to end United States military use of Shannon Airport. After the party entered government there were no substantive changes in government policy on these issues, which meant that Eamon Ryan oversaw the Corrib gas project while he was in office. The Green Party had, at its 2007 annual conference, made an inquiry into the irregularities surrounding the project (see Corrib gas controversy) a precondition of entering government, but changed its stance during post-election negotiations with Fianna F\u00e1il.\nThe 2008 budget did not include a carbon levy on fuels such as petrol, diesel and home heating oil, which the Green Party had sought before the election. A carbon levy was, however, introduced in the 2010 Budget. The 2008 budget did include a separate carbon budget announced by Gormley, which introduced new energy efficiency tax credit, a ban on incandescent bulbs from January 2009, a tax scheme incentivising commuters' purchases of bicycles and a new scale of vehicle registration tax based on carbon emissions.\nAt a special convention on whether to support the Treaty of Lisbon on 19 January 2008, the party voted 63.5% in favour of supporting the Treaty; this fell short of the party's two-thirds majority requirement for policy issues. As a result, the Green Party did not have an official campaign in the first Lisbon Treaty referendum, although individual members were involved on different sides. The referendum did not pass in 2008, and following the Irish government's negotiation with EU member states of additional legal guarantees and assurances, the Green Party held another special convention meeting in Dublin on 18 July 2009 to decide its position on the second Lisbon referendum. Precisely two-thirds of party members present voted to campaign for a 'Yes' in the referendum. This was the first time in the party's history that it had campaigned in favour of a European treaty.\nThe government's response to the post-2008 banking crisis significantly affected the party's support, and it suffered at the 2009 local elections, returning with only three County Council seats in total and losing its entire traditional Dublin base, with the exception of a Town Council seat in Balbriggan.\nD\u00e9irdre de B\u00farca, one of two Green senators nominated by Taoiseach Bertie Ahern in 2007, resigned from the party and her seat in 2010, in part owing to the party's inability to secure her a job in the European Commission. On 23 February 2010, Trevor Sargent resigned as Minister of State for Food and Horticulture owing to allegations over contacting Garda\u00ed about a criminal case involving a constituent, with Ciar\u00e1n Cuffe being appointed as his replacement the following March. By 2010, opinion polls showed strong support for an immediate election with the Greens polling at just 2%.\nThe Green Party supported the passage of legislation for EC\u2013ECB\u2013IMF financial support for Ireland's bank bailout. On 19 January, the party derailed Taoiseach Brian Cowen's plans to reshuffle his cabinet when it refused to endorse Cowen's intended replacement ministers, forcing Cowen to redistribute the vacant portfolios among incumbent ministers. The Greens were angered at not having been consulted about this effort, and went as far as to threaten to pull out of the coalition unless Cowen set a firm date for an election due that spring. He ultimately set the date for 11 March.\nOn 23 January 2011, the Green Party met with Cowen following his resignation as leader of senior coalition partner Fianna F\u00e1il the previous afternoon. The Green Party then announced it was breaking off the coalition and going into opposition with immediate effect. Ministers Gormley and Ryan resigned as cabinet ministers, and Cuffe and White resigned as Ministers of State. Green Party leader John Gormley said at a press conference announcing the withdrawal: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;For a very long time we in the Green Party have stood back in the hope that Fianna F\u00e1il could resolve persistent doubts about their party leadership. A definitive resolution of this has not yet been possible. And our patience has reached an end.\nIn almost four years in Government, from 2007 to 2011, the Green Party contributed to the passage of civil partnership for same-sex couples, the introduction of major planning reform, a major increase in renewable energy output, progressive budgets, and a nationwide scheme of home insulation retrofitting.\nWipeout, recovery, and second government term.\nFallout from the 2011 general election.\nThe party lost all of its six TDs at the 2011 general election, including former Ministers John Gormley and Eamon Ryan. Three of their six TDs lost their deposits. The party's share of the vote fell below 2%, meaning that they could not reclaim election expenses, and their lack of parliamentary representation led to the ending of state funding for the party. The party candidates in the 2011 election to the Seanad were Dan Boyle and Niall \u00d3 Brolch\u00e1in; neither was elected, and as a result, for the first time since 1989 the Green Party had no representatives in the Oireachtas.\nIn the aftermath of the wipeout Eamon Ryan was elected as party leader on 27 May 2011, succeeding John Gormley, while Catherine Martin was later appointed the deputy leader of the party.\n2016 to 2019 electoral successes.\nAt the 2016 general election Ryan and Martin gained two seats in the D\u00e1il while Grace O'Sullivan picked up a seat in the Seanad. In doing so the Green party became the first Irish political party to lose all their seats in a general election but come back and win seats in a subsequent election. The Greens continued to pick up momentum in 2019, performing quite well during the concurrent 2019 local elections and 2019 European Parliament election while in November that same year the party saw Pippa Hackett win a seat in the Seanad and Joe O'Brien bring home the party's first ever by-election win in the 2019 Dublin Fingal by-election.\nReturn to government.\nAt the 2020 general election, the party had its best result ever, winning 7.1% of the first-preference votes and returning 12 TDs, an increase of ten from the last election. It became the fourth-largest party in the D\u00e1il and entered government in coalition with Fianna F\u00e1il and Fine Gael. Ryan, Martin and Roderic O'Gorman were appointed as cabinet ministers, with four Green Ministers of State. Clare Bailey, the leader of the Green Party in Northern Ireland, was amongst a number of Green members who stood against the coalition. She said it proposed the \"most fiscally conservative arrangements in a generation\" and that \"the economic and finances behind this deal will really lead to some of the most vulnerable being hit the hardest\", as well as it not doing enough on climate and social justice. She also said the deal \"fails to deliver on our promise to tackle homelessness and provide better healthcare\", \"represents an unjust recovery\" and \"sets out an inadequate and vague pathway towards climate action\". The party returned two senators at the 2020 Seanad election, with a further two senators nominated by the Taoiseach, Miche\u00e1l Martin bringing the total party representation in the Oireachtas to 16. In July 2020, Eamon Ryan retained his leadership of the party with a narrow leadership election victory over Catherine Martin in the 2020 Green Party leadership election by 994 votes to 946, a margin of 48 votes.\nInternal disputes.\nDespite the success at the general election, the party found itself dogged by infighting and resignations. Saoirse McHugh, a candidate in the 2019 European elections, 2020 general election and the 2020 Seanad election, resigned from the party upon the Greens entering government with Fine Gael and Fianna F\u00e1il, parties she believed would damage public enthusiasm for environmentalist policies by pairing them with \"socially regressive\" policies. Over the course of 2020, four councillors as well as both the leader of the Young Greens and the leader of the Queer Greens left the party, all citing either bullying within the party or dissatisfaction with the coalition and its policies as the cause. Amongst the resignations were councillors Lorna Bogue and Liam Sinclair, who subsequently formed a new left-wing green party called Rabharta in June 2021.\nInfighting continued in 2021 when the party Cathaoirleach Hazel Chu, the Lord Mayor of Dublin, launched a campaign to run for the 2021 Seanad by-elections with the support of six members of the Green Party Parliamentary Party, but without official backing from the party. Senators Pippa Hackett, Pauline O'Reilly and R\u00f3is\u00edn Garvey tabled a motion of no confidence in Chu as Cathaoirleach of the party. Deputy leader, Catherine Martin urged the senators to withdraw the motion and it was later replaced with a motion calling for Chu to temporarily step aside from the position as party chair for the duration of the election. This was passed by 11 votes to five at a meeting of the Parliamentary Party. The Executive Council of the party, however, decided not to follow the Parliamentary Party's decision and Chu remained in the position of Cathaoirleach until the end of her term in December 2021. Chu ran in the 2022 Dublin University by-election and was commended by the party for \"championing climate action and inclusion\".\nIn May 2022, Green TDs Neasa Hourigan and Patrick Costello were suspended from the party for six months after they went against the party whip and voted for an opposition motion calling for the new National Maternity Hospital to be built on land wholly owned by the state. Hourigan was suspended again in March 2023, this time for 15 months, after she voted against the government on the issue of ending a ban on evictions.\nNotable achievements.\nThe Climate Action and Low Carbon Development (Amendment) Act 2021 was one of the Greens' flagship policies. The law enacted a legally binding path to net zero emissions by 2050. Five-year carbon budgets produced by the Climate Change Advisory Council will dictate the path to carbon neutrality, with the aim of the first two budgets creating a 51% reduction by 2030. The five-year budgets will not be legally binding.\nThe party also secured significantly increased budgets for active and sustainable travel including greenways and cycle lanes, the LocalLink rural bus network, decreases in public transport fares, a new forestry programme, increased incentives for solar and for retrofit, and the recognition of the circular economy. Outside of the core climate and environmental policies the party also implemented a pilot scheme for a basic income for the arts sector and large cuts in childcare costs.\nIn June 2024, the European Union Environment Council approved the Nature Restoration Law which was described as \"among the EU's biggest environmental policies\". The role of Eamon Ryan in convincing other ministers to support the law was considered to be pivotal to its success.\n2024 onwards.\nAt the 2024 local elections, the party lost almost half of its council seats across the country, although it topped the poll in four local electoral areas in Dublin City. At the 2024 European Parliament elections, Ciar\u00e1n Cuffe and Grace O'Sullivan both lost their seats, while in the Limerick mayoral election, the party's candidate, Brian Leddin, won 2.89% of the vote and was eliminated on the fifth count.\nOn 18 June 2024, Eamon Ryan announced his resignation as party leader. Ryan also announced that he would not be seeking re-election as a TD for Dublin Bay South at the next general election. Later the same day Catherine Martin announced her resignation as deputy leader and would not be seeking the leadership role. On 19 June, Roderic O'Gorman and Senator Pippa Hackett both announced that they would be seeking nominations for the leadership position. On 8 July, O'Gorman was narrowly elected over Hackett with 51.89% of the vote. On 14 July, Senator R\u00f3is\u00edn Garvey narrowly defeated Neasa Hourigan with 51% of the vote to become deputy leader.\nAt the 2024 general election the party retained one of their twelve seats, with party leader Roderic O'Gorman being elected. The party won 3% of the vote. In the 2025 Seanad election, former TD and Minister of State Malcolm Noonan was elected to the 27th Seanad topping the poll on the Agricultural Panel.\nIn the 2025 Irish presidential election the party endorsed the campaign of Catherine Connolly, joining a broad left alliance in support of her candidacy which included Social Democrats, People Before Profit, Labour, Sinn F\u00e9in and several left-leaning independents. In October 2025, former TD Brian Leddin quit the party, citing his unhappiness at the decision to endorse Connolly. Also in October 2025, R\u00f3is\u00edn Garvey resigned her position as deputy leader of the party and quit the party.\nIdeology and policies.\nThe Green Party has seven \"founding principles\", which are:\n Broadly, these founding principles reflect the \"four pillars\" of green politics observed by the majority of Green Parties internationally: ecological wisdom, social justice, grassroots democracy, and nonviolence. They also reflect the six guiding principles of the Global Greens, which also includes a respect for diversity as a principle.\nWhile strongly associated with environmentalist policies, the party also has policies covering all other key areas. These include protection of the Irish language, lowering the voting age in Ireland to 16, and support for universal healthcare. The party also advocates that terminally ill people should have the right to legally choose assisted dying, stating \"provisions should apply only to those with a terminal illness which is likely to result in death within six months\". It also states that \"such a right would only apply where the person has a clear and settled intention to end their own life which is proved by making, and signing, a written declaration to that effect. Such a declaration must be countersigned by two qualified doctors\".\nInternal factions.\nAs other like-minded green parties, it has eco-socialist/green left and more moderate factions. In parallel to other Green Parties in Europe, the 1980s and 1990s saw a division within the Irish Green Party between two factions; the \"Realists\" (nicknamed the \"Realos\") and the \"Fundamentalists (nicknamed the \"Fundies\"). The 'Realists' advocated taking a pragmatic approach to politics, which would mean having to accept some compromises on policy in order to get party members elected and into government in order to enact change. The 'Fundamentalists' advocated more radical policies and rejected appeals for pragmatism, citing that the looming effects of Climate Change would leave no time for compromise. Following a national convention in 1998 which saw a realist majority of members defeat a minority of fundamentalist members on a number of votes, and the party subsequently enter government for the first time in 2007, the factionalism of the 'Realists vs the Fundamentalists' was seen to have wilted away with the 'Realists' becoming the ascendent faction. However, in some respects, the division only laid dormant.\nFollowing the 2019 local elections and the 2020 general election, the party had more elected representatives than ever before as well as its highest ever membership. On 22 July 2020, several prominent members of the party formed the \"Just Transition Greens\", an affiliate group within the party with a green left/eco-socialist outlook, who have the objective of moving the party towards policies based on the concept of a \"Just Transition\". During the 2020 Green Party leadership election, a significant aspect of the candidacy of Catherine Martin was that it was suggested that Martin could better represent the views of these individuals within the party than the incumbent Eamon Ryan.\nOrganisation.\nThe National Executive Committee is the organising committee of the party. It comprises the party leader Roderic O'Gorman, the deputy leader R\u00f3is\u00edn Garvey, the Cathaoirleach Janet Horner, the National Coordinator, the General Secretary (in a non-voting role), a Young Greens representative, the Treasurer and ten members elected annually at the party convention.\nLeadership.\nCathaoirleach.\nNote: Although Christopher Fettes chaired the party initially, the position of Cathaoirleach was not created until 2002.\nLeadership organisation.\nThe party did not have a national leader until 2001. At a special \"Leadership Convention\" in Kilkenny on 6 October 2001, Trevor Sargent was elected the first official leader of the Green Party while Mary White was elected deputy leader. Sargent was re-elected to his position in 2003 and again in 2005. The party's constitution requires that a leadership election be held within six months of a general election.\nSargent resigned the leadership in the wake of the 2007 general election to the 30th D\u00e1il. During the campaign, Sargent had promised that he would not lead the party into Government with Fianna F\u00e1il. At the election the party retained six D\u00e1il seats, making it the most likely partner for Fianna F\u00e1il. Sargent and the party negotiated a coalition government; at the 12 June 2007 membership meeting to approve the agreement, he announced his resignation as leader.\nIn the subsequent leadership election, John Gormley became the new leader on 17 July 2007, defeating Patricia McKenna by 478 votes to 263. Mary White was subsequently re-elected as the deputy Leader. Gormley served as Minister for the Environment, Heritage and Local Government from July 2007 until the Green Party's exit from government in January 2011.\nFollowing the election defeats of 2011, Gormley announced his intention not to seek another term as Green Party leader. Eamon Ryan was elected as the new party leader, over party colleagues Phil Kearney and Cllr Malcolm Noonan in a postal ballot election of party members in May 2011. Monaghan-based former councillor Catherine Martin defeated Down-based Dr John Barry and former Senator Mark Dearey to the post of deputy leader on 11 June 2011 during the party's annual convention. Roderic O'Gorman was elected party chairperson.\nThe Green Party lost all its D\u00e1il seats in the 2011 general election. Party Chairman Dan Boyle and D\u00e9irdre de B\u00farca were nominated by the Taoiseach to Seanad \u00c9ireann after the formation of the Fianna F\u00e1il\u2013Progressive Democrats\u2013Green Party government in 2007, and Niall \u00d3 Brolch\u00e1in was elected in December 2009. De B\u00farca resigned in February 2010, and was replaced by Mark Dearey. Neither Boyle nor O'Brolchain was re-elected to Seanad \u00c9ireann in the Seanad election of 2011, leaving the Green Party without Oireachtas representation until the 2016 general election, in which it regained two D\u00e1il seats.\nRyan's leadership was challenged by deputy leader Catherine Martin in 2020 after the 2020 government formation; he narrowly won a poll of party members, 994 votes (51.2%) to 946.\nIrish and European politics.\nThe Green Party is organised throughout the island of Ireland, with regional structures in both the Republic of Ireland and Northern Ireland. The Green Party Northern Ireland voted to become a regional partner of the Green Party in Ireland in 2005 at its annual convention, and again in a postal ballot in March 2006. Brian Wilson, formerly a councillor for the Alliance Party, won the Green Party's first seat in the Northern Ireland Assembly in the 2007 election. Steven Agnew later held that seat from the 2011 election until his resignation in September 2019.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15083", "revid": "40123752", "url": "https://en.wikipedia.org/wiki?curid=15083", "title": "Information Science", "text": ""}
{"id": "15085", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=15085", "title": "Iconoclasm", "text": "Destruction of religious images\nIconoclasm (from grc \" ' ()\"\u00a0'figure, icon' and \" ' ()\"\u00a0'to break') is the belief in the importance of the destruction of icons and other images or monuments, often for religious or political reasons. Those who engage in or support iconoclasm are called iconoclasts, a term that has come to be applied figuratively and more broadly to anyone who challenges \"cherished beliefs or venerated institutions on the grounds that they are erroneous or pernicious\".\nConversely, one who reveres or venerates religious images is called (by iconoclasts) an \"iconolater\"; in a Byzantine context, such a person is called an \"iconodule\" or \"iconophile\". Iconoclasm does not generally encompass the destruction of the images of a specific ruler after their death or overthrow, a practice better known as \"damnatio memoriae\".\nWhile iconoclasm may be carried out by adherents of a different religion, it is more commonly the result of sectarian disputes between factions of the same religion. The term originates from the Byzantine Iconoclasm, the struggles between proponents and opponents of religious icons in the Byzantine Empire from 726 to 842 AD. While the enthusiasm for iconoclasm varies among faiths, the practice is more common in religions which oppose idolatry, such as the Abrahamic religions. Outside of the religious context, iconoclasm can refer to movements for widespread destruction in symbols of an ideology or cause, such as the destruction of monarchist symbols during the French Revolution.\nEarly religious iconoclasm.\nAncient era.\nIn the Bronze Age, the most significant episode of iconoclasm occurred in Egypt during the Amarna Period, when Akhenaten, based in his new capital of Akhetaten, instituted a significant shift in Egyptian artistic styles alongside a campaign of intolerance towards the traditional gods and a new emphasis on a state monolatristic tradition focused on the god Aten, the Sun disk\u2014many temples and monuments were destroyed as a result:\nIn rebellion against the old religion and the powerful priests of Amun, Akhenaten ordered the eradication of all of Egypt's traditional gods. He sent royal officials to chisel out and destroy every reference to Amun and the names of other deities on tombs, temple walls, and cartouches to instill in the people that the Aten was the one true god.\nPublic references to Akhenaten were destroyed soon after his death. Comparing the ancient Egyptians with the Israelites, Jan Assmann writes:\nFor Egypt, the greatest horror was the destruction or abduction of the cult images. In the eyes of the Israelites, the erection of images meant the destruction of divine presence; in the eyes of the Egyptians, this same effect was attained by the destruction of images. In Egypt, iconoclasm was the most terrible religious crime; in Israel, the most terrible religious crime was idolatry. In this respect Osarseph alias Akhenaten, the iconoclast, and the Golden Calf, the paragon of idolatry, correspond to each other inversely, and it is strange that Aaron could so easily avoid the role of the religious criminal. It is more than probable that these traditions evolved under mutual influence. In this respect, Moses and Akhenaten became, after all, closely related.\nJudaism.\nAccording to the Hebrew Bible, God instructed the Israelites to \"destroy all [the] engraved stones, destroy all [the] molded images, and demolish all [the] high places\" of the Canaanites as soon as they entered the Promised Land.\nKing Hezekiah purged Solomon's Temple in Jerusalem and all figures were also destroyed in the Land of Israel, including the Nehushtan, as recorded in the Second Book of Kings. His reforms were reversed in the reign of his son Manasseh.\nIconoclasm in Christian history.\nScattered expressions of opposition to the use of images have been reported: the Synod of Elvira appeared to endorse iconoclasm; Canon 36 states: \"Pictures are not to be placed in churches, so that they do not become objects of worship and adoration.\" A possible translation is also: \"There shall be no pictures in the church, lest what is worshipped and adored should be depicted on the walls.\" The date of this canon is disputed. Proscription ceased after the destruction of pagan temples. However, widespread use of Christian iconography only began as Christianity increasingly spread among Gentiles after the legalization of Christianity by Roman Emperor Constantine (c.\u00a0312 AD). During the process of Christianisation under Constantine, Christian groups destroyed the images and sculptures of the Roman Empire's polytheist state religion.\nAmong early church theologians, iconoclastic tendencies were supported by theologians such as Tertullian, Clement of Alexandria, Origen, Lactantius, Justin Martyr, Eusebius and Epiphanius.\nByzantine era.\nThe period after the reign of Byzantine Emperor Justinian (527\u2013565) evidently saw a huge increase in the use of images, both in volume and quality, and a gathering aniconic reaction.\nOne notable change within the Byzantine Empire came in 695, when Justinian II's government added a full-face image of Christ on the obverse of imperial gold coins. The change caused the Caliph Abd al-Malik to stop his earlier adoption of Byzantine coin types. He started a purely Islamic coinage with lettering only. A letter by the Patriarch Germanus, written before 726 to two iconoclast bishops, says that \"now whole towns and multitudes of people are in considerable agitation over this matter\", but there is little written evidence of the debate.\nGovernment-led iconoclasm began with Byzantine Emperor Leo\u00a0III, who issued a series of edicts between 726 and 730 against the veneration of images. The religious conflict created political and economic divisions in Byzantine society; iconoclasm was generally supported by the Eastern, poorer, non-Greek peoples of the Empire who had to frequently deal with raids from the new Muslim Empire. On the other hand, the wealthier Greeks of Constantinople and the peoples of the Balkan and Italian provinces strongly opposed iconoclasm.\nPre-Reformation.\nPeter of Bruys opposed the usage of religious images, the Strigolniki were also possibly iconoclastic. Claudius of Turin was the bishop of Turin from 817 until his death. He is most noted for teaching iconoclasm.\nReformation era.\nThe first iconoclastic wave happened in Wittenberg in the early 1520s under reformers Thomas M\u00fcntzer and Andreas Karlstadt. In 1522 Karlstadt published his tract, https://. (\"On the removal of images\"), which added to the growing unrest in Wittenberg. Martin Luther, then concealed under the pen-name of 'Junker J\u00f6rg', intervened to calm things down. Luther argued that the mental picturing of Christ when reading the Scriptures was similar in character to artistic renderings of Christ.\nIn contrast to the Lutherans who favoured certain types of sacred art in their churches and homes, the Reformed (Calvinist) leaders, in particular Andreas Karlstadt, Huldrych Zwingli and John Calvin, encouraged the removal of religious images by invoking the Decalogue's prohibition of idolatry and the manufacture of graven (sculpted) images of God. As a result, individuals attacked statues and images, most famously in the \"beeldenstorm\" across the Low Countries in 1566.\nThe belief of iconoclasm caused havoc throughout Europe. In 1523, specifically due to the Swiss reformer Huldrych Zwingli, a vast number of his followers viewed themselves as being involved in a spiritual community that in matters of faith should obey neither the visible Church nor lay authorities. According to Peter George Wallace, \"Zwingli's attack on images, at the first debate, triggered iconoclastic incidents in Z\u00fcrich and the villages under civic jurisdiction that the reformer was unwilling to condone.\" Due to this action of protest against authority, \"Zwingli responded with a carefully reasoned treatise that men could not live in society without laws and constraint\".\nSignificant iconoclastic riots took place in Basel (in 1529), Z\u00fcrich (1523), Copenhagen (1530), M\u00fcnster (1534), Geneva (1535), Augsburg (1537), Scotland (1559), Rouen (1560), and Saintes and La Rochelle (1562). Calvinist iconoclasm in Europe \"provoked reactive riots by Lutheran mobs\" in Germany and \"antagonized the neighbouring Eastern Orthodox\" in the Baltic region.\nThe Seventeen Provinces (now the Netherlands, Belgium, and parts of Northern France) were disrupted by widespread Calvinist iconoclasm in the summer of 1566.\nDuring the Reformation in England, which started during the reign of Henry VIII, and was urged on by reformers such as Hugh Latimer and Thomas Cranmer, limited official action was taken against religious images in churches in the late 1530s. Henry's young son, Edward VI, came to the throne in 1547 and, under Cranmer's guidance, issued injunctions for religious reforms in the same year and in 1549 the Putting away of Books and Images Act.\nDuring the English Civil War, the Parliamentarians reorganised the administration of East Anglia into the Eastern Association of counties. This covered some of the wealthiest counties in England, which in turn financed a substantial and significant military force. After Earl of Manchester was appointed the commanding officer of these forces, in turn he appointed Smasher Dowsing as Provost Marshal, with a warrant to demolish religious images which were considered to be superstitious or linked with popism. Bishop Joseph Hall of Norwich described the events of 1643 when troops and citizens, encouraged by a Parliamentary ordinance against superstition and idolatry, behaved thus:\nLord what work was here! What clattering of glasses! What beating down of walls! What tearing up of monuments! What pulling down of seats! What wresting out of irons and brass from the windows! What defacing of arms! What demolishing of curious stonework! What tooting and piping upon organ pipes! And what a hideous triumph in the market-place before all the country, when all the mangled organ pipes, vestments, both copes and surplices, together with the leaden cross which had newly been sawn down from the Green-yard pulpit and the service-books and singing books that could be carried to the fire in the public market-place were heaped together.\nProtestant Christianity was not uniformly hostile to the use of religious images. Martin Luther taught the \"importance of images as tools for instruction and aids to devotion\", stating: \"If it is not a sin but good to have the image of Christ in my heart, why should it be a sin to have it in my eyes?\" Lutheran churches retained ornate church interiors with a prominent crucifix, reflecting their high view of the real presence of Christ in Eucharist. As such, \"Lutheran worship became a complex ritual choreography set in a richly furnished church interior.\" For Lutherans, \"the Reformation renewed rather than removed the religious image\".\nLutheran scholar Jeremiah Ohl writes:Zwingli and others for the sake of saving the Word rejected all plastic art; Luther, with an equal concern for the Word, but far more conservative, would have all the arts to be the servants of the Gospel. \"I am not of the opinion\" said [Luther], \"that through the Gospel all the arts should be banished and driven away, as some zealots want to make us believe; but I wish to see them all, especially music, in the service of Him Who gave and created them.\" Again he says: \"I have myself heard those who oppose pictures, read from my German Bible... But this contains many pictures of God, of the angels, of men, and of animals, especially in the Revelation of St. John, in the books of Moses, and in the book of Joshua. We therefore kindly beg these fanatics to permit us also to paint these pictures on the wall that they may be remembered and better understood, inasmuch as they can harm as little on the walls as in books. Would to God that I could persuade those who can afford it to paint the whole Bible on their houses, inside and outside, so that all might see; this would indeed be a Christian work. For I am convinced that it is God's will that we should hear and learn what He has done, especially what Christ suffered. But when I hear these things and meditate upon them, I find it impossible not to picture them in my heart. Whether I want to or not, when I hear, of Christ, a human form hanging upon a cross rises up in my heart: just as I see my natural face reflected when I look into water. Now if it is not sinful for me to have Christ's picture in my heart, why should it be sinful to have it before my eyes?The Ottoman Sultan Suleiman the Magnificent, who had pragmatic reasons to support the Dutch Revolt (the rebels, like himself, were fighting against Spain) also completely approved of their act of \"destroying idols\", which accorded well with Muslim teachings.\n16th century Protestant iconoclasm had various effects on visual arts: it encouraged the development of art with violent images such as martyrdoms, of pieces whose subject was the dangers of idolatry, or art stripped of objects with overt Catholic symbolism: the still life, landscape and genre paintings.\nOther instances.\nIn Japan during the early modern age, the spread of Catholicism also involved the repulsion of non-Christian religious structures, including Buddhist temples and Shinto shrines and figures. At times of conflict with rivals or some time after the conversion of several daimyos, Christian converts would often destroy Buddhist and Shinto religious structures.\nMany of the moai of Easter Island were toppled during the 18th century in the iconoclasm of civil wars before any European encounter. Other instances of iconoclasm may have occurred throughout Eastern Polynesia during its conversion to Christianity in the 19th century.\nAfter the Second Vatican Council in the late 20th century, some Roman Catholic parish churches discarded much of their traditional imagery and art which critics call iconoclasm.\nMuslim iconoclasm.\nIslam has a strong tradition of forbidding the depiction of figures, especially religious figures, with some Sunnis forbidding it entirely.\nIn the history of Islam, the act of removing idols from the Ka'ba in Mecca has great symbolic and historic importance for all believers.\nIn general, Muslim societies have avoided the depiction of living beings (both animals and humans) within such sacred spaces as mosques and madrasahs. This ban on figural representation is not based on the Qur'an, instead, it is based on traditions which are described within the Hadith. The prohibition of figuration has not always been extended to the secular sphere, and a robust tradition of figural representation exists within Muslim art.\nHowever, Western authors have tended to perceive \"a long, culturally determined, and unchanging tradition of violent iconoclastic acts\" within Islamic society.\nEarly Islam in Arabia.\nThe first act of Muslim iconoclasm dates to the beginning of Islam, in 630, when the various statues of Arabian deities housed in the Kaaba in Mecca were destroyed. There is a tradition that Muhammad spared a fresco of Mary and Jesus. This act was intended to bring an end to the idolatry which, in the Muslim view, characterized Jahiliyyah.\nThe destruction of the idols of Mecca did not, however, determine the treatment of other religious communities living under Muslim rule after the expansion of the caliphate. Most Christians under Muslim rule, for example, continued to produce icons and to decorate their churches as they wished. A major exception to this pattern of tolerance in early Islamic history was the \"Edict of Yaz\u012bd\", issued by the Umayyad caliph Yaz\u012bd II in 722\u2013723. This edict ordered the destruction of crosses and Christian images within the territory of the caliphate. Researchers have discovered evidence that the order was followed, particularly in present-day Jordan, where archaeological evidence shows the removal of images from the mosaic floors of some, although not all, of the churches that stood at this time. But Yaz\u012bd's iconoclastic policies were not continued by his successors, and Christian communities of the Levant continued to make icons without significant interruption from the sixth century to the ninth.\nEgypt.\nAl-Maqr\u012bz\u012b, writing in the 15th century, attributes the missing nose on the Great Sphinx of Giza to iconoclasm by Muhammad Sa'im al-Dahr, a Sufi Muslim in the mid-1300s. He was reportedly outraged by local Muslims making offerings to the Great Sphinx in the hope of controlling the flood cycle, and he was later executed for vandalism. However, whether this was actually the cause of the missing nose has been debated by historians. Mark Lehner, having performed an archaeological study, concluded that it was broken with instruments at an earlier unknown time between the 3rd and 10th centuries.\nOttoman conquests.\nCertain conquering Muslim armies have used local temples or houses of worship as mosques. An example is Hagia Sophia in Istanbul (formerly Constantinople), which was converted into a mosque in 1453. Most icons were desecrated and the rest were covered with plaster. In 1934 the government of Turkey decided to convert the Hagia Sophia into a museum and the restoration of the mosaics was undertaken by the American Byzantine Institute beginning in 1932.\nContemporary events.\nCertain Muslim denominations continue to pursue iconoclastic agendas. There has been much controversy within Islam over the recent and apparently on-going destruction of historic sites by Saudi Arabian authorities, prompted by the fear they could become the subject of \"idolatry\".\nA recent act of iconoclasm was the 2001 destruction of the giant Buddhas of Bamyan by the then-Taliban government of Afghanistan. The act generated worldwide protests and was not supported by other Muslim governments and organizations. It was widely perceived in the Western media as a result of the Muslim prohibition against figural decoration. Such an account overlooks \"the coexistence between the Buddhas and the Muslim population that marveled at them for over a millennium\" before their destruction. According to art historian F. B. Flood, analysis of the Taliban's statements regarding the Buddhas suggest that their destruction was motivated more by political than by theological concerns. Taliban spokesmen have given many different explanations of the motives for the destruction.\nDuring the Tuareg rebellion of 2012, the radical Islamist militia Ansar Dine destroyed various Sufi shrines from the 15th and 16th centuries in the city of Timbuktu, Mali. In 2016, the International Criminal Court (ICC) sentenced Ahmad al-Faqi al-Mahdi, a former member of Ansar Dine, to nine years in prison for this destruction of cultural world heritage. This was the first time that the ICC convicted a person for such a crime.\nThe Islamic State of Iraq and the Levant carried out iconoclastic attacks such as the destruction of Shia mosques and shrines. Notable incidents include blowing up the Mosque of the Prophet Yunus (Jonah) and destroying the Shrine to Seth in Mosul.\nIconoclasm in India.\nDuring Hindu-Buddhist era.\nIn early Medieval India, there were numerous recorded instances of temple desecration by Indian kings against rival Indian kingdoms, which involved conflicts between devotees of different Hindu deities, as well as conflicts between Hindus, Buddhists, and Jains.\nDuring the Muslim era.\nRecords from the campaign recorded in the \"Chach Nama\" record the destruction of temples during the early 8th century when the Umayyad governor of Damascus, al-Hajjaj ibn Yusuf, mobilized an expedition of 6000 cavalry under Muhammad bin Qasim in 712.\nHistorian Upendra Thakur records the persecution of Hindus and Buddhists:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe Somnath temple and Mahmud of Ghazni.\nPerhaps the most notorious episode of iconoclasm in India was Mahmud of Ghazni's attack on the Somnath Temple from across the Thar Desert. In 1026 during the reign of Bhima I, the prominent Turkic-Muslim ruler Mahmud of Ghazni raided Gujarat, plundering the Somnath Temple and breaking its jyotirlinga despite pleas by Brahmins not to break it. He took away a booty of 20 million dinars. The attack may have been inspired by the belief that an idol of the goddess Manat had been secretly transferred to the temple. According to the Ghaznavid court-poet Farrukhi Sistani, who claimed to have accompanied Mahmud on his raid, \"Somnat\" (as rendered in Persian) was a garbled version of \"su-manat\" referring to the goddess Manat. According to him, as well as a later Ghaznavid historian Abu Sa'id Gardezi, the images of the other goddesses were destroyed in Arabia but the one of Manat was secretly sent away to Kathiawar (in modern Gujarat) for safekeeping. Since the idol of Manat was an aniconic image of black stone, it could have been easily confused with a lingam at Somnath. Mahmud is said to have broken the idol and taken away parts of it as loot and placed so that people would walk on it. In his letters to the Caliphate, Mahmud exaggerated the size, wealth and religious significance of the Somnath temple, receiving grandiose titles from the Caliph in return.\nThe wooden structure was replaced by Kumarapala (r. 1143\u201372), who rebuilt the temple out of stone.\nFrom the Mamluk dynasty onward.\nHistorical records which were compiled by the Muslim historian Maulana Hakim Saiyid Abdul Hai attest to the religious violence which occurred during the Mamluk dynasty under Qutb-ud-din Aybak. The first mosque built in Delhi, the \"Quwwat al-Islam\" was built with demolished parts of 20 Hindu and Jain temples. This pattern of iconoclasm was common during his reign.\nDuring the Delhi Sultanate, a Muslim army led by Malik Kafur, a general of Alauddin Khalji, pursued four violent campaigns into south India, between 1309 and 1311, against the Hindu kingdoms of Devgiri (Maharashtra), Warangal (Telangana), Dwarasamudra (Karnataka) and Madurai (Tamil Nadu). Many Temples were plundered; Hoysaleswara Temple and others were ruthlessly destroyed.\nIn Kashmir, Sikandar Shah Miri (1389\u20131413) began expanding, and unleashed religious violence that earned him the name \"but-shikan\", or 'idol-breaker'. He earned this sobriquet because of the sheer scale of desecration and destruction of Hindu and Buddhist temples, shrines, ashrams, hermitages, and other holy places in what is now known as Kashmir and its neighboring territories. Firishta states: \"After the emigration of the Brahmins, Sikundur ordered all the temples in Kashmeer to be thrown down.\" He destroyed vast majority of Hindu and Buddhist temples in his reach in Kashmir region (north and northwest India).\nA regional tradition, along with the Hindu text \"Madala Panji\", states that Kalapahar attacked and damaged the Konark Sun Temple\nin 1568, as well as many others in Orissa.\nSome of the most dramatic cases of iconoclasm by Muslims are found in parts of India where Hindu and Buddhist temples were razed and mosques erected in their place. Aurangzeb, the 6th Mughal Emperor, destroyed the famous Hindu temples at Varanasi and Mathura, turning back on his ancestor Akbar's policy of religious freedom and establishing Sharia across his empire.\nDuring the Goa Inquisition.\nExact data on the nature and number of Hindu temples destroyed by the Christian missionaries and Portuguese government are unavailable. Some 160 temples were allegedly razed to the ground in Tiswadi (Ilhas de Goa) by 1566. Between 1566 and 1567, a campaign by Franciscan missionaries destroyed another 300 Hindu temples in Bardez (North Goa). In Salcete (South Goa), approximately another 300 Hindu temples were destroyed by the Christian officials of the Inquisition. Numerous Hindu temples were destroyed elsewhere at Assolna and Cuncolim by Portuguese authorities. A 1569 royal letter in Portuguese archives records that all Hindu temples in its colonies in India had been burnt and razed to the ground. The English traveller Sir Thomas Herbert, 1st Baronet who visited Goa in the 1600s writes: \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;... as also the ruins of 200 Idol Temples which the Vice-Roy Antonio Norogna totally demolisht, that no memory might remain, or monuments continue, of such gross Idolatry. For not only there, but at Salsette also were two Temples or places of prophane Worship; one of them (by incredible toil cut out of the hard Rock) was divided into three Iles or Galleries, in which were figured many of their deformed Pagotha's, and of which an Indian (if to be credited) reports that there were in that Temple 300 of those narrow Galleries, and the Idols so exceeding ugly as would affright an European Spectator; nevertheless this was a celebrated place, and so abundantly frequented by Idolaters, as induced the Portuguise in zeal with a considerable force to master the Town and to demolish the Temples, breaking in pieces all that monstrous brood of mishapen Pagods. In Goa nothing is more observable now than the fortifications, the Vice-Roy and Arch-bishops Palaces, and the Churches. ...\nIn modern India.\nB. R. Ambedkar and his supporters on 25 December 1927 in the Mahad Satyagraha strongly criticised, condemned and then burned copies of \"Manusmriti\" on a pyre in a specially dug pit. \"Manusmriti\", one of the sacred Hindu texts, is the religious basis of casteist laws and values of Hinduism and hence was/is the reason of social and economic plight of millions of untouchables and lower caste Hindus. Ambedkarites continue to observe 25 December as \"Manusmriti Dahan Divas\" (Manusmriti Burning Day) and burn copies of \"Manusmriti\" on this day.\nThe most high-profile case of iconoclasm in independent India was in 1992. A Hindu mob, led by the Vishva Hindu Parishad and Bajrang Dal, destroyed the 430-year-old Islamic Babri Masjid in Ayodhya which is claimed to have been built upon a previous Hindu temple.\nIconoclasm in East Asia.\nChina.\nThere have been a number of anti-Buddhist campaigns in Chinese history that led to the destruction of Buddhist temples and images. One of the most notable of these campaigns was the Great Anti-Buddhist Persecution of the Tang dynasty.\nDuring and after the 1911 Xinhai Revolution, there was widespread destruction of religious and secular images in China.\nDuring the Northern Expedition in Guangxi in 1926, Kuomintang General Bai Chongxi led his troops in destroying Buddhist temples and smashing Buddhist images, turning the temples into schools and Kuomintang party headquarters. It was reported that almost all of the viharas in Guangxi were destroyed and the monks were removed. Bai also led a wave of anti-foreignism in Guangxi, attacking Americans, Europeans, and other foreigners, and generally making the province unsafe for foreigners and missionaries. Westerners fled from the province and some Chinese Christians were also attacked as imperialist agents. The three goals of the movement were anti-foreignism, anti-imperialism and anti-religion. Bai led the anti-religious movement against superstition. Huang Shaohong, also a Kuomintang member of the New Guangxi clique, supported Bai's campaign. The anti-religious campaign was agreed upon by all Guangxi Kuomintang members.\nThere was extensive destruction of religious and secular imagery in Tibet after it was invaded and occupied by China.\nMany religious and secular images were destroyed during the Cultural Revolution of 1966\u20131976, ostensibly because they were a holdover from China's traditional past (which the Communist regime led by Mao Zedong reviled). The Cultural Revolution included widespread destruction of historic artworks in public places and private collections, whether religious or secular. Objects in state museums were mostly left intact.\nSouth Korea.\nAccording to an article in \"Buddhist-Christian Studies\":Over the course of the last decade [1990s] a fairly large number of Buddhist temples in South Korea have been destroyed or damaged by fire by Christian fundamentalists. More recently, Buddhist statues have been identified as idols, and attacked and decapitated in the name of Jesus. Arrests are hard to effect, as the arsonists and vandals work by stealth of night.\nAngkor.\nBeginning c.\u20091243\u00a0AD with the death of Indravarman II, the Khmer Empire went through a period of iconoclasm. At the beginning of the reign of the next king, Jayavarman VIII, the kingdom went back to Hinduism and the worship of Shiva. Many of the Buddhist images were destroyed by Jayavarman VIII, who reestablished previously Hindu shrines that had been converted to Buddhism by his predecessor. Carvings of the Buddha at temples such as Preah Khan were destroyed, and during this period the Bayon Temple was made a temple to Shiva, with the central\u00a0 statue of the Buddha cast to the bottom of a nearby well.\nPolitical iconoclasm.\n\"Damnatio memoriae\".\nRevolutions and changes of regime, whether through uprising of the local population, foreign invasion, or a combination of both, are often accompanied by the public destruction of statues and monuments identified with the previous regime. This may also be known as \"damnatio memoriae\", the ancient Roman practice of official obliteration of the memory of a specific individual. Stricter definitions of \"iconoclasm\" exclude both types of action, reserving the term for religious or more widely cultural destruction. In many cases, such as Revolutionary Russia or Ancient Egypt, this distinction can be hard to make.\nAmong Roman emperors and other political figures subject to decrees of \"damnatio memoriae\" were Sejanus, Publius Septimius Geta, and Domitian. Several Emperors, such as Domitian and Commodus had during their reigns erected numerous statues of themselves, which were pulled down and destroyed when they were overthrown.\nThe perception of \"damnatio memoriae\" in the Classical world as an act of erasing memory has been challenged by scholars who have argued that it \"did not negate historical traces, but created gestures which served to \"dishonor\" the record of the person and so, in an oblique way, to confirm memory\", and was in effect a spectacular display of \"pantomime forgetfulness\". Examining cases of political monument destruction in modern Irish history, Guy Beiner has demonstrated that iconoclastic vandalism often entails subtle expressions of ambiguous remembrance and that, rather than effacing memory, such acts of de-commemorating effectively preserve memory in obscure forms.\nDuring the French Revolution.\nThroughout the radical phase of the French Revolution, iconoclasm was supported by members of the government as well as the citizenry. Numerous monuments, religious works, and other historically significant pieces were destroyed in an attempt to eradicate any memory of the Old Regime. A statue of King Louis XV in the Paris square which until then bore his name, was pulled down and destroyed. This was a prelude to the guillotining of his successor Louis XVI in the same site, renamed \"Place de la R\u00e9volution\" (at present Place de la Concorde). Later that year, the bodies of many French kings were exhumed from the Basilica of Saint-Denis and dumped in a mass grave.\nSome episodes of iconoclasm were carried out spontaneously by crowds of citizens, including the destruction of statues of kings during the insurrection of 10 August 1792 in Paris. Some were directly sanctioned by the Republican government, including the Saint-Denis exhumations. Nonetheless, the Republican government also took steps to preserve historic artworks, notably by founding the Louvre museum to house and display the former royal art collection. This allowed the physical objects and national heritage to be preserved while stripping them of their association with the monarchy. Alexandre Lenoir saved many royal monuments by diverting them to preservation in a museum.\nThe statue of Napoleon on the column at Place Vend\u00f4me, Paris was also the target of iconoclasm several times: destroyed after the Bourbon Restoration, restored by Louis-Philippe, destroyed during the Paris Commune and restored by Adolphe Thiers.\nAfter Napoleon conquered the Italian city of Pavia, local Pavia Jacobins destroyed the Regisole, a bronze classical equestrian monument dating back to Classical times. The Jacobins considered it a symbol of Royal authority, but it had been a prominent Pavia landmark for nearly a thousand years and its destruction aroused much indignation and precipitated a revolt by inhabitants of Pavia against the French, which was quelled by Napoleon after a furious urban fight.\nOther examples.\nOther examples of political destruction of images include:\nIn the Soviet Union.\nDuring and after the October Revolution, widespread destruction of religious and secular imagery in Russia took place, as well as the destruction of imagery related to the Imperial family. The Revolution was accompanied by destruction of monuments of tsars, as well as the destruction of imperial eagles at various locations throughout Russia. According to Christopher Wharton:In front of a Moscow Cathedral, crowds cheered as the enormous statue of Tsar Alexander\u00a0III was bound with ropes and gradually beaten to the ground. After a considerable amount of time, the statue was decapitated and its remaining parts were broken into rubble.The Soviet Union actively destroyed religious sites, including Russian Orthodox churches and Jewish cemeteries, in order to discourage religious practice and curb the activities of religious groups.\nDuring the Hungarian Revolution of 1956 and during the Revolutions of 1989, protesters often attacked and took down sculptures and images of Joseph Stalin, such as the Stalin Monument in Budapest.\nThe fall of Communism in 1989\u20131991 was also followed by the destruction or removal of statues of Vladimir Lenin and other Communist leaders in the former Soviet Union and in other Eastern Bloc countries. Particularly well-known was the destruction of \"Iron Felix\", the statue of Felix Dzerzhinsky outside the KGB's headquarters. Another statue of Dzerzhinsky was destroyed in a Warsaw square that was named after him during communist rule, but which is now called Bank Square.\nIn the United States.\nDuring the American Revolution, the Sons of Liberty pulled down and destroyed the gilded lead statue of George III of the United Kingdom on Bowling Green (New York City), melting it down to be recast as ammunition. Sometimes relatively intact monuments are moved to a collected display in a less prominent place, as in India and also post-Communist countries.\nIn August 2017, a statue of a Confederate soldier dedicated to \"the boys who wore the gray\" was pulled down from its pedestal in front of Durham County Courthouse in North Carolina by protesters. This followed the events at the 2017 Unite the Right rally in response to growing calls to remove Confederate monuments and memorials across the U.S.\n2020 demonstrations.\nDuring the George Floyd protests of 2020, demonstrators pulled down dozens of statues which they considered symbols of the Confederacy, slavery, segregation, or racism, including the statue of Williams Carter Wickham in Richmond, Virginia.\nFurther demonstrations in the wake of the George Floyd protests have resulted in the removal of:\nMultiple statues of early European explorers and founders were also vandalized, including those of Christopher Columbus, George Washington, and Thomas Jefferson.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15086", "revid": "14835592", "url": "https://en.wikipedia.org/wiki?curid=15086", "title": "IWW (disambiguation)", "text": "IWW, or Industrial Workers of the World (known as the Wobblies), is an international union founded in 1905.\nIWW may also refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15087", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=15087", "title": "Imbolc", "text": "Gaelic festival marking the start of spring\nImbolc or Imbolg (), also called Saint Brigid's Day (; ; ), is a Gaelic traditional festival on 1 February. It marks the beginning of spring, and in Christianity, it is the feast day of Saint Brigid, Ireland's patroness saint. Historically, its many folk traditions were widely observed throughout Ireland, Scotland and the Isle of Man. Imbolc falls about halfway between the winter solstice and the spring equinox and is one of the four Gaelic seasonal festivals, along with Bealtaine, Lughnasadh and Samhain.\nImbolc is mentioned in early Irish literature, although less often than the other seasonal festivals. Historians suggest that Imbolc was originally a pre-Christian (or pagan) festival associated with the lambing season, the coming of spring, and possibly the goddess Brigid, proposing that the saint and her feast day might be Christianizations. A feast of Saint Brigid was first mentioned in the Middle Ages, but its customs were not recorded in detail until the early modern era. In recent centuries, Brigid's crosses have been woven on St Brigid's Day and hung over doors and windows to protect against fire, illness, and evil. People also made a doll of Brigid (a ), which was paraded around the community by girls, sometimes accompanied by 'strawboys'. Brigid was said to visit one's home on St Brigid's Eve. To receive her blessings, people would make a bed for Brigid, leave her food and drink, and set items of clothing outside for her to bless. Holy wells would be visited, a special meal would be had, and the day was traditionally linked with weather lore.\nAlthough many of its traditions died out in the 20th century, it is still observed by some Christians as a religious holiday and by some non-Christians as a cultural one, and its customs have been revived in some places. Since the later 20th century, Celtic neopagans and Wiccans have observed Imbolc as a religious holiday. Since 2023, \"Imbolc/St Brigid's Day\" has been an annual public holiday in Ireland.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nName.\nThe etymology of \"Imbolc\" or \"Imbolg\" is unclear. A common explanation is that it comes from the Old Irish (Modern Irish: ), meaning 'in the belly', and refers to the pregnancy of ewes at this time of year. Joseph Vendryes derived it from Old Irish (intensive prefix) and ('wash, cleanse'), linking it to ('to wash/cleanse oneself') and suggesting it referred to a ritual cleansing. Eric P. Hamp derives it from a Proto-Indo-European root meaning both 'milk' and 'cleansing'. The early 10th century Cormac's Glossary has an entry for , calling it the beginning of spring and deriving it from \"o\u00ed-melg\" ('ewe milk'), explaining it as \"the time that sheep's milk comes\". However, linguists believe this is a folk etymology; the writer's respelling of the word to give it an understandable origin.\nThe 12th century Book of Leinster version of the \"T\u00e1in B\u00f3 C\u00faailnge\" ('Cattle Raid of Cooley') indicates that Imbolc (spelt \"imolg\" and \"imbuilg\") is three months after the 1 November festival of Samhain. In the Stowe version, a 14th century modernisation of the same text, Imbolc is changed to \"F\u00e9l Brigde\" (St Brigid's Feast), which suggests that St Brigid's Day replaced or absorbed Imbolc. Another Old Irish poem about the \"T\u00e1in\" in the \"Metrical Dindshenchas\" says: \"\", which Edward Gwynn translates \"after Candlemas, rough was their herding\". Candlemas is the Christian holy day which falls on 2 February and is known in Irish as , 'feast day of Mary of the Candles'.\nIn a 17th century manuscript of the story \"Agallamh na Sean\u00f3rach\", Imbolc is spelled \"Iomfhoilcc\".\nPeter O'Connell's Irish-English dictionary (1843) identifies \"Oimelc\" or \"Imbulc\" with \"F\u00e9il Brighde\", the Feast of Saint Brigid.\nOrigins.\nHistorians such as Ronald Hutton and D\u00e1ith\u00ed \u00d3 h\u00d3g\u00e1in argue that Imbolc must have pre-Christian origins. It is suggested that Imbolc originally marked the onset of the lambing season, the arrival of fresh sheep milk after a period of food shortage, and the beginning of preparations for the spring sowing. Joseph Vendryes and Christian-Joseph Guyonvarc'h suggested that it may have also been a purification festival, similar to the ancient Roman festival \"Februa\" or \"Lupercalia\", which took place at the same time of year.\nSome scholars argue that the date of Imbolc was significant in Ireland since the Neolithic. A few passage tombs in Ireland are aligned with the sunrise around the times of Imbolc and Samhain. This includes the Mound of the Hostages on the Hill of Tara, and Cairn L at Slieve na Calliagh. Frank Prendergast argues that this alignment is so rare that it is a product of chance.\nHutton writes that Imbolc must have been \"important enough for its date to be dedicated subsequently to Brigid \u2026 the Mother Saint of Ireland\". Cogitosus, writing in the late 7th century, is the first to mention a feast day of Saint Brigid being observed in Kildare on 1 February. Brigid is said to have lived in the 6th century AD, and founded the important monastery of Kildare. She became the focus of a major cult. However, there are few secular historical facts about her, and her early hagiographies \"are mainly anecdotes and miracle stories, some of which are deeply rooted in Irish pagan folklore\". It has been suggested by some authors that Saint Brigid is based on the goddess Brigid, or that she was a real person and some of the lore of the goddess was transferred to her. Like the saint, the goddess is associated with wisdom, poetry, healing, protection, blacksmithing, and domesticated animals, according to \"Cormac's Glossary\" and \"Lebor Gab\u00e1la \u00c9renn\". It is suggested that Imbolc, which celebrates the start of lambing, was linked with Brigid in her role as a fertility goddess. Hutton says that the goddess might have already been linked to Imbolc and this was continued by making it the saint's feast day (see \"Interpretatio Christiana\"). Or it could be that Imbolc's association with milk drew the saint to it, because of a legend that she had been the wet-nurse of Jesus Christ.\nProminent folklorist Se\u00e1n \u00d3 S\u00failleabh\u00e1in wrote: \"The main significance of the Feast of St. Brigid would seem to be that it was a Christianisation of one of the focal points of the agricultural year in Ireland, the starting point of preparations for the spring sowing. Every manifestation of the cult of the saint (or of the deity she replaced) is bound up in some way with food production\".\nHistorical customs.\nThe festival of Imbolc is mentioned in several early Irish manuscripts, but they say very little about its original rites and customs. Imbolc was one of four main seasonal festivals in Gaelic Ireland, along with Beltane (1 May), Lughnasadh (1 August) and Samhain (1 November). The tale \"Tochmarc Emire\", which survives in a 10th-century version, names \"Imbolc\" as one of four seasonal festivals, and says it is \"when the ewes are milked at spring's beginning\". This linking of Imbolc with the arrival of lambs and sheep's milk probably reflected farming customs that ensured lambs were born before calves. In late winter/early spring, sheep could survive better than cows on the sparse vegetation, and farmers sought to resume milking as soon as possible due to their dwindling stores. The \"Hibernica Minora\" includes an Old Irish poem about the four seasonal festivals. Translated by Kuno Meyer (1894), it says, \"Tasting of each food according to order, this is what is proper at Imbolc: washing the hands, the feet, the head\". This suggests ritual cleansing. As a seasonal festival, the timing of Imbolc might originally have been more fluid and linked to the onset of lambing and the blooming of blackthorn.\nFrom the 18th century to the mid-20th century, many St Brigid's Day traditions were recorded by folklorists and other writers. They tell us how it was celebrated then and shed light on how it may have been celebrated in the past.\nBrigid's crosses.\nIn Ireland, Brigid's crosses (\"pictured\") are traditionally made on St Brigid's Day. A Brigid's cross usually consists of rushes woven into a four-armed equilateral cross, although there were also three-armed crosses. They are traditionally hung over doors, windows, and stables to welcome Brigid and for protection against fire, lightning, illness, and evil spirits. The crosses are generally left until the next St Brigid's Day. In western Connacht, people made a \"\" ('s girdle); a great ring of rushes with a cross woven in the middle. Young boys would carry it around the village, inviting people to step through it and be blessed.\nWelcoming Brigid.\nOn St Brigid's Eve, Brigid was said to visit virtuous households and bless the inhabitants. As Brigid represented the light half of the year and the power that will bring people from the dark season of winter into spring, her presence was vital at this time of year.\nBefore going to bed, people would leave items of clothing or strips of cloth outside for Brigid to bless. The next morning, they would be brought inside and believed to have powers of healing and protection.\nBrigid would be symbolically invited into the house and a bed would often be made for her. In Ulster, a family member representing Brigid would circle the home three times carrying rushes. They would knock the door three times, asking to be let in. On the third attempt, they are welcomed in, a meal is had, and the rushes are then made into crosses or a bed for Brigid. In 18th-century Mann, the custom was to stand at the door with a bundle of rushes and say \"Brede, Brede, come to my house tonight. Open the door for Brede and let Brede come in\". Similarly, in County Donegal, the family member who was sent to fetch the rushes knelt on the front step and repeated three times, \"Go on your knees, open your eyes, and let in St Brigid\". Those inside the house answered three times, \"She's welcome\". The rushes were then strewn on the floor as a carpet or bed for Brigid. In the 19th century, some old Manx women would make a bed for Brigid in the barn with food, ale, and a candle on a table. The custom of making Brigid's bed was prevalent in the Hebrides of Scotland, where it was recorded as far back as the 17th century. A bed of hay or a basket-like cradle would be made for Brigid. Someone would then call out three times: \"'\" (\", come in; thy bed is ready\"). A corn dolly called the \"dealbh Br\u00edde\" (icon of Brigid) would be laid in the bed and a white wand, usually made of birch, would be laid beside it. It represented the wand that Brigid was said to use to make the vegetation start growing again. Women in some parts of the Hebrides would also dance while holding a large cloth and calling out \"'\" (\", come over and make your bed\").\nIn the Outer Hebrides, ashes from the fire would be raked smooth, and, in the morning, people would look for some mark on the ashes as a sign that Brigid had visited. If there was no mark, they believed bad fortune would come unless they buried a cockerel at the meeting of three streams as an offering and burned incense on their fire that night.\nBrigid's procession.\nIn Ireland and Scotland, a representation of Brigid would be paraded around the community by girls and young women. Usually, it was a doll known as a ' ('little Brigid'), called a 'Breedhoge' or 'Biddy' in English. It would be made from rushes or reeds and clad in bits of cloth, flowers, or shells. In the Hebrides of Scotland, a bright shell or crystal called the ' (guiding star of Brigid) was set on its chest. The girls would carry it in procession while singing a hymn to Brigid. All wore white with their hair unbound as a symbol of purity and youth. They visited every house in the area, where they received either food or more decoration for the . Afterward, they feasted in a house with the set in a place of honour, and put it to bed with lullabies. When the meal was done, the local young men humbly asked for admission, made obeisance to the , and joined the girls in dancing and merrymaking. In many places, only unwed girls could carry the , but in some both boys and girls carried it.\nIn parts of Ireland, rather than carrying a , a girl took on the role of Brigid. Escorted by other girls, she went house-to-house wearing 'Brigid's crown' and carrying 'Brigid's shield' and 'Brigid's cross', all made from rushes. The procession in some places included 'strawboys', who wore conical straw hats, masks and played folk music; much like the wrenboys. Up until the mid-20th century, children in Ireland still went house-to-house asking for pennies for \"poor Biddy\", or money for the poor. In County Kerry, men in white robes sang from house to house.\nWeather lore.\nThe festival is traditionally associated with weather lore, and the old tradition of watching to see if serpents or badgers came from their winter dens may be a forerunner of the North American Groundhog Day. A Scottish Gaelic proverb about the day is:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nImbolc was believed to be when the Cailleach\u2014the divine hag of Gaelic tradition\u2014gathers her firewood for the rest of the winter. Legend has it that if she wishes to make the winter last a good while longer, she will make sure the weather on Imbolc is bright and sunny so that she can gather plenty of firewood. Therefore, people would be relieved if Imbolc is a day of foul weather, as it means the Cailleach is asleep and winter is almost over. At Imbolc on the Isle of Man, where she is known as \"\", the Cailleach is said to take the form of a gigantic bird carrying sticks in her beak.\nOther customs.\nFamilies would have a special meal or supper on St Brigid's Eve to mark the last night of winter. This typically included food such as colcannon, sowans, dumplings, barmbrack or bannocks. Often, some of the food and drink would be set aside for Brigid.\nIn Ireland, a spring cleaning was customary around St Brigid's Day.\nPeople traditionally visit holy wells and pray for health while walking 'sunwise' around the well. They might then leave offerings, typically coins or strips of cloth/ribbon (see clootie well). Historically, water from the well was used to bless the home, family members, livestock, and fields.\nScottish writer Donald Alexander Mackenzie also recorded in the 19th century that offerings were made \"to earth and sea\". The offering could be milk poured into the ground or porridge poured into the water as a libation.\nIn County Kilkenny, graves were decorated with box and laurel flowers (or any other flowers that could be found at that time). A Branch of Virginity was decorated with white ribbons and placed on the grave of a recently deceased maiden.\nPresent day customs.\nSt Brigid's Day and Imbolc are observed by Christians and non-Christians. Some people still make Brigid's crosses and s or visit holy wells dedicated to St Brigid on 1 February. Brigid's Day parades have been revived in the town of Killorglin, County Kerry, which holds a yearly \"Biddy's Day Festival\". Men and women wearing elaborate straw hats and masks visit public houses carrying a to ward off evil spirits and bring good luck for the coming year. There are folk music sessions, historical talks, film screenings, drama productions, and cross-weaving workshops. The main event is a torchlight parade of 'Biddy groups' through the town. Since 2009 a yearly \"Brigid of Faughart Festival\" is held in County Louth. This celebrates Brigid as both saint and goddess and includes the long-standing pilgrimage to Faughart as well as music, poetry, and lectures. The \"Imbolc International Music Festival\" of folk music is held in Derry at this time of year. In England, the village of Marsden, West Yorkshire holds a biennial \"Imbolc Fire Festival\" which includes a lantern procession, fire performers, music, fireworks, and a symbolic battle between giant characters representing the Green Man and Jack Frost.\nMore recently, Irish embassies have hosted yearly events on St Brigid's Day to celebrate famous women of the Irish diaspora and showcase the work of Irish female emigrants in the arts. In 2022, Dublin hosted its first \"Brigit Festival\", celebrating \"the contributions of Irish women\" past and present through exhibitions, tours, lectures, films, and a concert.\nIn 2016, the Green Party proposed that St Brigid's Day be made a public holiday in Ireland. This was put into effect in 2022 after the party entered government, and \"Imbolc/St Brigid's Day\" has been a yearly public holiday since 2023 to mark both the saint's feast day and the seasonal festival. A government statement noted that it would be the first Irish public holiday named after a woman, and \"means that all four of the traditional Celtic seasonal festival will now be public holidays\". The public holiday is observed on the first Monday of February, except for years where 1 February happens to fall on a Friday, in which case the holiday is observed on that Friday instead.\nNeopaganism.\nImbolc or Imbolc-based festivals are observed by some Neopagans, though practices vary widely. While some attempt to closely emulate the historic accounts of Imbolc, others rely on many sources to inspire their celebrations. Festivals typically fall near 1 February in the Northern Hemisphere and 1 August in the Southern Hemisphere.\nSome Neopagans celebrate the festival at the astronomical midpoint between the winter solstice and spring equinox \u2014 in the Northern Hemisphere, this is usually on 3 or 4 February \u2014 while others rely on the full moon nearest this point. Some Neopagans designate Imbolc based on other natural phenomena, such as the emergence of primroses, dandelions, or similar local flora.\nCeltic Reconstructionist.\nCeltic Reconstructionists strive to reconstruct ancient Celtic religion. Their religious practices are based on research and historical accounts, but may be modified slightly to suit modern life. They avoid syncretism (i.e., combining practises from different cultures). They usually celebrate the festival at the start of spring, or on the full moon nearest this. Many use traditional songs and rites from sources such as \"The Silver Bough\" and \"The Carmina Gadelica\". It is a time of honouring the goddess Brigid, and many of her dedicants choose this time of year for rituals to her.\nWicca and Neo-Druidry.\nWiccans and Neo-Druids celebrate Imbolc as one of the eight Sabbats in their Wheel of the Year, following Midwinter and preceding Ostara. In Wicca, Imbolc is commonly associated with the goddess Brigid; as such, it is sometimes seen as a \"women's holiday\" with specific rites only for female members of a coven. Among Dianic Wiccans, Imbolc is the traditional time for initiations.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15088", "revid": "42775921", "url": "https://en.wikipedia.org/wiki?curid=15088", "title": "Isaiah", "text": "Israelite prophet\nIsaiah ( or ; , \"Y\u0259\u0161a\u02bfy\u0101h\u016b\", \"Yahweh is salvation\"; also known as Isaias or Esaias from ) was the 8th-century BC Israelite prophet after whom the Book of Isaiah is named.\nThe text of the Book of Isaiah refers to Isaiah as \"the prophet\", but the exact relationship between the Book of Isaiah and the actual prophet Isaiah is complicated. The traditional view is that all 66 chapters of the book of Isaiah were written by one man, Isaiah, possibly in two periods between 740 BC and c. 686 BC, separated by approximately 15 years.\nAnother widely held view suggests that parts of the first half of the book (chapters 1\u201339) originated with the historical prophet, interspersed with prose commentaries written in the time of King Josiah 100 years later, and that the remainder of the book dates from immediately before and immediately after the end of the 6th-century BC exile in Babylon (almost two centuries after the time of the historical prophet), and that perhaps these later chapters represent the work of an ongoing school of prophets who prophesied in accordance with his prophecies.\nBiography.\nThe first verse of the Book of Isaiah states that Isaiah prophesied during the reigns of Uzziah (or Azariah), Jotham, Ahaz, and Hezekiah, the kings of Judah. Uzziah's reign was 52 years in the middle of the 8th century BC, and Isaiah must have begun his ministry a few years before Uzziah's death, probably in the 740s BC. He may have been contemporary for some years with Manasseh. Thus, Isaiah may have prophesied for as long as 64 years.\nAccording to some modern interpretations, Isaiah's wife was called \"the prophetess\", either because she was endowed with the prophetic gift, like Deborah and Huldah, or simply because she was the \"wife of the prophet\". They had two sons, naming the elder Shear-Jashub, meaning \"A remnant shall return\", and the younger Maher-Shalal-Hash-Baz, meaning, \"Quickly to spoils, plunder speedily.\"\nSoon after this, Shalmaneser V determined to subdue the northern Kingdom of Israel, taking over and destroying Samaria and beginning the Assyrian captivity. So long as Ahaz reigned, the kingdom of Judah was untouched by the Assyrian power. But when Hezekiah gained the throne, he was encouraged to rebel \"against the king of Assyria\", and entered into an alliance with the king of Egypt. The king of Assyria threatened the king of Judah, and at length invaded the land. Sennacherib's campaign in the Levant brought his powerful army into Judah. Hezekiah was reduced to despair, and submitted to the Assyrians. But after a brief interval, war broke out again. Again Sennacherib led an army into Judah, one detachment of which threatened Jerusalem. Isaiah on that occasion encouraged Hezekiah to resist the Assyrians, whereupon Sennacherib sent a threatening letter to Hezekiah, which he \"spread before the LORD\".\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Then Isaiah son of Amoz sent this message to Hezekiah: \"Thus said GOD, the God of Israel, to whom you have prayed, concerning King Sennacherib of Assyria\u2014 \nthis is the word that GOD has spoken concerning him:\nFair Maiden Zion despises you,\nShe mocks at you;\nFair Jerusalem shakes\nHer head at you. \nWhom have you blasphemed and reviled?\nAgainst whom made loud your voice\nAnd haughtily raised your eyes?\nAgainst the Holy One of Israel!\nAccording to the account in 2 Kings 19 (and its derivative account in 2 Chronicles 32) an angel of God fell on the Assyrian army and slew 185,000 of its men in one night. \"Like Xerxes in Greece, Sennacherib never recovered from the shock of the disaster in Judah. He made no more expeditions against either Judea or Egypt.\"\nThe remaining years of Hezekiah's reign were peaceful. Isaiah probably lived to its close, and possibly into the reign of Manasseh. The time and manner of his death are not specified in either the Bible or other primary sources. The Talmud says that he suffered martyrdom by being sawn in two under the orders of Manasseh.\nThe book of Isaiah, along with the book of Jeremiah, is distinctive in the Hebrew bible for its direct portrayal of the \"wrath of the LORD\" as presented, for example, in Isaiah 9:19 stating \"Through the wrath of the LORD of hosts is the land darkened, and the people shall be as the fuel of the fire.\"\nIn Christianity.\nThe Ascension of Isaiah, a pseudepigraphical Christian text dated to sometime between the end of the 1st century and the beginning of the 3rd, gives a detailed story of Isaiah confronting an evil false prophet and ending with Isaiah being martyred \u2013 none of which is attested in the original Biblical account.\nGregory of Nyssa (c. 335\u2013395) believed that the Prophet Isaiah \"knew more perfectly than all others the mystery of the religion of the Gospel\". Jerome (c. 342\u2013420) also lauds the Prophet Isaiah, saying \"He was more of an Evangelist than a Prophet, because he described all of the Mysteries of the Church of Christ so vividly that you would assume he was not prophesying about the future, but rather was composing a history of past events.\" Of specific note are the songs of the Suffering Servant, which Christians say are a direct prophetic revelation of the nature, purpose, and detail of the death of Jesus Christ.\nThe Book of Isaiah is quoted many times by New Testament writers. The Gospel of John says that Isaiah \"saw Jesus' glory and spoke about him.\"\nThe Eastern Orthodox Church celebrates Saint Isaiah the Prophet with Saint Christopher on May 9. Isaiah is also listed on the page of saints for May 9 in the Roman martyrology of the Roman Catholic Church.\nIn the Book of Mormon.\nThe Book of Mormon quotes Jesus Christ as stating that \"great are the words of Isaiah\", and that all things prophesied by Isaiah have been and will be fulfilled. The Book of Mormon and Doctrine and Covenants also quote Isaiah more than any other prophet from the Old Testament. Additionally, members of the Church of Jesus Christ of Latter-day Saints consider the founding of the church by Joseph Smith in the 19th century to be a fulfillment of Isaiah 11, the translation of the Book of Mormon to be a fulfillment of Isaiah 29, and the building of Latter-day Saint temples as a fulfillment of .\nIn Islam.\nIsaiah () is not mentioned by name in the Quran or the Hadith, but appears frequently as a prophet in Muslim sources such as the qi\u1e63a\u1e63 al-anbiy\u0101\u02be and various tafsirs. Al-Tabari (310/923) provides the typical accounts for Islamic traditions regarding Isaiah. He is listed among the prophets in the book of salawat Dalail al-Khayrat. He is further mentioned and accepted as a prophet by other Islamic scholars such as ibn Kathir, Abu Ishaq al-Tha'labi and al-Kisa'i and also modern scholars such as Muhammad Asad and Abdullah Yusuf Ali.\nAccording to Muslim scholars, Isaiah prophesied the coming of Jesus and Muhammad, although the claim is disputed by other religious scholars. Isaiah's narrative in Islamic literature can be divided into three sections. The first establishes Isaiah as a prophet of Judea during the reign of Hezekiah; the second relates Isaiah's actions during the siege of Jerusalem in 597 BC by Sennacherib; and the third warns the nation of coming doom. Paralleling the Hebrew Bible, Islamic tradition states that Hezekiah was king in Jerusalem during Isaiah's time. Hezekiah heard and obeyed Isaiah's advice, but could not quell the turbulence in Israel. This tradition maintains that Hezekiah was a righteous man and that the turbulence worsened after him. After the death of the king, Isaiah told the people not to forsake God, and warned Israel to cease from its persistent sin and disobedience. Muslim tradition maintains that the unrighteous of Judea in their anger sought to kill Isaiah.\nIn a death resembling the one found in \"Lives of the Prophets\", Muslim exegesis recounts Isaiah's martyrdom by Israelites sawing him asunder.\nIn the courts of al-Ma'mun, the seventh Abbasid caliph, Ali al-Ridha, the great-grandson of Muhammad and prominent scholar of his era, was questioned by the Exilarch to prove through the Torah that both Jesus and Muhammad were prophets. Among his several proofs, al-Ridha references the Book of Isaiah, stating \"Sha'ya (Isaiah), the Prophet, said in the Torah concerning what you and your companions say 'I have seen two riders to whom (He) illuminated earth. One of them was on a donkey and the other was on a camel. Who is the rider of the donkey, and who is the rider of the camel?'\" The Exilarch was unable to answer with certainty. Al-Ridha goes on to state that \"As for the rider of the donkey, he is 'Isa (Jesus); and as for the rider of the camel, he is Muhammad, may Allah bless him and his family. Do you deny that this (statement) is in the Torah?\" The Rabbi responds \"No, I do not deny it.\"\nIn Judaism.\nAllusions in Jewish rabbinic literature to Isaiah contain various expansions, elaborations and inferences that go beyond what is presented in the text of the Bible.\nOrigin and calling.\nAccording to the ancient rabbis, Isaiah was a descendant of Judah and Tamar, and his father Amoz was the brother of King Amaziah.\nWhile Isaiah, says the Midrash, was walking up and down in his study he heard God saying \"Whom shall I send?\" Then Isaiah said \"Here am I; send me!\" Thereupon God said to him,\" My children are troublesome and sensitive; if you are ready to be insulted and even beaten by them, you may accept My message; if not, you would better renounce it\". Isaiah accepted the mission, and was the most forbearing, as well as the most patriotic, among the prophets, always defending Israel and imploring forgiveness for its sins. When Isaiah said \"I dwell in the midst of a people of unclean lips\", he was rebuked by God for speaking in such terms of His people.\nFurther accounts state that Isaiah was actually the maternal grandfather of King Manasseh, which would make Queen Consort Hephzibah from 2 Kings 21:1 his daughter and King Hezekiah his son-in-law. Hephzibah's name (lit.\u2009'my delight (is) in her') was used as a symbolic name for Zion following its restoration to the favor of Yahweh in Isaiah 62.\nHis death.\nIt is related in the Talmud that Rabbi Simeon ben Azzai found in Jerusalem an account wherein it was written that King Manasseh killed Isaiah. King Manasseh said to Isaiah \"Moses, your master, said 'No man may see God and live'; but you have said 'I saw the Lord seated upon his throne'\"; and went on to point out other contradictions\u2014as between Deuteronomy and Isaiah 40; between Exodus 33 and 2 Kings Isaiah thought: \"I know that he will not accept my explanations; why should I increase his guilt?\" He then uttered the tetragrammaton, a cedar-tree opened, and Isaiah disappeared within it. King Manasseh ordered the cedar to be sawn asunder, and when the saw reached his mouth Isaiah died; thus was he punished for having said \"I dwell in the midst of a people of unclean lips\".\nA somewhat different version of this legend is given in the Jerusalem Talmud. According to that version Isaiah, fearing King Manasseh, hid himself in a cedar-tree, but his presence was betrayed by the fringes of his garment, and King Manasseh caused the tree to be sawn in half. A passage of the Targum to Isaiah quoted by Jolowicz states that when Isaiah fled from his pursuers and took refuge in the tree, and the tree was sawn in half, the prophet's blood spurted forth. The legend of Isaiah's martyrdom spread to the Arabs and to the Christians as, for example, Athanasius the bishop of Alexandria (c. 318) wrote, \"Isaiah was sawn asunder\".\nArchaeology.\nIn February 2018, archaeologist Eilat Mazar announced that she and her team had discovered a small seal impression which reads \"[belonging] to Isaiah nvy\" (could be reconstructed and read as \"[belonging] to Isaiah the prophet\") during the Ophel excavations, just south of the Temple Mount in Jerusalem. The tiny bulla was found \"only 10 feet away\" from where an intact bulla bearing the inscription \"[belonging] to King Hezekiah of Judah\" was discovered in 2015 by the same team. Although the name \"Isaiah\" in the Paleo-Hebrew alphabet is unmistakable, the damage on the bottom left part of the seal causes difficulties in confirming the word \"prophet\" or a name \"Navi\", casting some doubts whether this seal really belongs to the prophet Isaiah.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15089", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=15089", "title": "Interpreted language", "text": ""}
{"id": "15092", "revid": "21640150", "url": "https://en.wikipedia.org/wiki?curid=15092", "title": "Id", "text": ""}
{"id": "15095", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=15095", "title": "Intifada", "text": "Arabic term for uprising or rebellion\nIntifada () is an Arabic word for a rebellion or uprising, or a resistance movement. It can also be used to refer to a civilian uprising against oppression.\nIn the 20th century, the word \"intifada\" has been used to describe various uprisings. In the Iraqi Intifada in 1952, Iraqi parties took to the streets to protest their monarchy. Other later examples include the Western Sahara's Zemla Intifada, the First Sahrawi Intifada, and the Second Sahrawi Intifada. In the context of the Israeli\u2013Palestinian conflict, it refers to uprising by Palestinian people against Israeli occupation or Israel, involving both violent and nonviolent methods of resistance, including the First Intifada (1987\u20131993) and the Second Intifada (2000\u20132005).\nIn Arabic-language usage, any uprising can be referred to as an intifada, including the 1916 Easter Rising, the 1943 Warsaw Ghetto Uprising, and the 1949 Jeju uprising. When used in English outside of the Arab World, the word has primarily referred to the two Palestinian uprisings against Israeli occupation.\nLexical information.\nMorphology.\n\"Intif\u0101\u1e0da\" () is an Arabic verbal noun ( \"ma\u1e63dar\") of instance ( \"ism marra\") of the verb \"intafa\u1e0da\" (), derived from the triconsonantal Semitic root \"n-f-\u1e0d\" () related to shaking (off), dusting (off), and making something shiver.1157 The verb \"intafa\u1e0da\" is in the verb form \"ifta\u02bfala\", referred to in Western sources as 'form VIII,' denoting reflexivity.1157\nMeaning.\nThe \"Dictionary of Modern Written Arabic\" gives the meaning of the verb \"intafa\u1e0da\" as: \"to be shaken off, be dusted off; to shake; to shudder, shiver, tremble; to shake off from oneself; to wake up, come to consciousness,\" as in \" to shake off one's lethargy,\" and of its verbal noun \"intif\u0101\u1e0da\" (pl. \"intif\u0101\u1e0d\u0101t\") as a \"shiver, shudder, tremor; awakening (pol.); popular uprising.\"1157\nIsraeli\u2013Palestinian conflict.\nIn the context of Palestine, the word \"intifada\" refers to attempts to \"shake off\" the Israeli occupation of the West Bank and the Gaza Strip in the First and Second Intifadas. The term was originally chosen to signify \"aggressive nonviolent resistance\"; in the 1980s, Palestinian students adopted \"intifada\" as less confrontational than terms in earlier militant rhetoric since it bore no connotation of violence. The First Intifada was characterized by protests, general strikes, economic boycotts, and riots, including the widespread throwing of stones and Molotov cocktails at the Israeli army and its infrastructure in the West Bank and Gaza. The Second Intifada was characterized by a period of heightened violence. The suicide bombings carried out by Palestinian assailants became one of the more prominent features of the Second Intifada and mainly targeted Israeli civilians, contrasting the relatively less violent nature of the First Intifada.\nThe phrase \"Globalize the intifada\" is a slogan used to promote worldwide activism in solidarity with Palestinian resistance. The phrase and those associated with it have caused controversy, particularly concerning their impact and connotations. Critics have claimed it encourages widespread violence or terrorism.\nList of events named \"Intifada\".\nIn Arabic-language texts, uprisings anywhere can be referred to using the word \"intifada\", including, for example, the 1916 Easter Rising (), the 1943 Warsaw Ghetto Uprising (), and the 1949 Jeju uprising ().\nIn English, the word may refer to these events, overwhelmingly in the Arabic-speaking world:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15097", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=15097", "title": "Ionosphere", "text": "Ionized part of Earth's upper atmosphere\nThe ionosphere () is the ionized part of the upper atmosphere of Earth, from about to above sea level, a region that includes the thermosphere and parts of the mesosphere and exosphere. The ionosphere is ionized by solar radiation. It plays an important role in atmospheric electricity and forms the inner edge of the magnetosphere. It has practical importance because, among other functions, it influences radio propagation to distant places on Earth. Travel through this layer also impacts GPS signals, resulting in effects such as deflection in their path and delay in the arrival of the signal.\nHistory of discovery.\nAs early as 1839, the German mathematician and physicist Carl Friedrich Gauss postulated that an electrically conducting region of the atmosphere could account for observed variations of Earth's magnetic field. Sixty years later, Guglielmo Marconi received the first trans-Atlantic radio signal on December 12, 1901, in St. John's, Newfoundland (now in Canada) using a kite-supported antenna for reception. The transmitting station in Poldhu, Cornwall, used a spark-gap transmitter to produce a signal with a frequency of approximately 500\u00a0kHz and a power of 100 times more than any radio signal previously produced. The message received was three dots, the Morse code for the letter S. To reach Newfoundland the signal would have to bounce off the ionosphere twice. Dr. Jack Belrose has contested this, however, based on theoretical and experimental work. However, Marconi did achieve transatlantic wireless communications in Glace Bay, Nova Scotia, one year later.\nIn 1902, Oliver Heaviside proposed the existence of the Kennelly\u2013Heaviside layer of the ionosphere which bears his name. Heaviside's proposal included means by which radio signals are transmitted around the Earth's curvature. Also in 1902, Arthur Edwin Kennelly discovered some of the ionosphere's radio-electrical properties.\nIn 1912, the U.S. Congress imposed the Radio Act of 1912 on amateur radio operators, limiting their operations to frequencies above 1.5\u00a0MHz (wavelength 200 meters or smaller). The government thought those frequencies were useless. This led to the discovery of HF radio propagation via the ionosphere in 1923.\nIn 1925, observations during a solar eclipse in New York by Dr. Alfred N. Goldsmith and his team demonstrated the influence of sunlight on radio wave propagation, revealing that short waves became weak or inaudible while long waves steadied during the eclipse, thus contributing to the understanding of the ionosphere's role in radio transmission.\nIn 1926, Scottish physicist Robert Watson-Watt introduced the term \"ionosphere\" in a letter published only in 1969 in \"Nature\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We have in quite recent years seen the universal adoption of the term 'stratosphere'..and..the companion term 'troposphere'...\u00a0The term 'ionosphere', for the region in which the main characteristic is large scale ionisation with considerable mean free paths, appears appropriate as an addition to this series.\nIn the early 1930s, test transmissions of Radio Luxembourg inadvertently provided evidence of the first radio modification of the ionosphere; HAARP ran a series of experiments in 2017 using the eponymous Luxembourg Effect.\nEdward V. Appleton was awarded a Nobel Prize in 1947 for his confirmation in 1927 of the existence of the ionosphere. Lloyd Berkner first measured the height and density of the ionosphere. This permitted the first complete theory of short-wave radio propagation. Maurice V. Wilkes and J. A. Ratcliffe researched the topic of radio propagation of very long radio waves in the ionosphere. Vitaly Ginzburg has developed a theory of electromagnetic wave propagation in plasmas such as the ionosphere.\nIn 1962, the Canadian satellite Alouette 1 was launched to study the ionosphere. Following its success were Alouette 2 in 1965 and the two ISIS satellites in 1969 and 1971, further AEROS-A and -B in 1972 and 1975, all for measuring the ionosphere.\nOn July 26, 1963, the first operational geosynchronous satellite Syncom 2 was launched. On board radio beacons on this satellite (and its successors) enabled \u2013 for the first time \u2013 the measurement of total electron content (TEC) variation along a radio beam from geostationary orbit to an earth receiver. (The rotation of the plane of polarization directly measures TEC along the path.) Australian geophysicist Elizabeth Essex-Cohen from 1969 onwards was using this technique to monitor the atmosphere above Australia and Antarctica.\nGeophysics.\nThe ionosphere is a shell of electrons and electrically charged atoms and molecules that surrounds the Earth, stretching from a height of about to more than . It exists primarily due to ultraviolet radiation from the Sun.\nThe lowest part of the Earth's atmosphere, the troposphere, extends from the surface to about . Above that is the stratosphere, followed by the mesosphere. In the stratosphere incoming solar radiation creates the ozone layer. At heights of above , in the thermosphere, the atmosphere is so thin that free electrons can exist for short periods of time before they are captured by a nearby positive ion. The number of these free electrons is sufficient to affect radio propagation. This portion of the atmosphere is partially \"ionized\" and contains a plasma which is referred to as the ionosphere.\nUltraviolet (UV), X-ray and shorter wavelengths of solar radiation are \"ionizing,\" since photons at these frequencies contain sufficient energy to dislodge an electron from a neutral gas atom or molecule upon absorption. In this process the light electron obtains a high velocity so that the temperature of the created electronic gas is much higher (of the order of thousand K) than the one of ions and neutrals. The reverse process to ionization is recombination, in which a free electron is \"captured\" by a positive ion. Recombination occurs spontaneously, and causes the emission of a photon carrying away the energy produced upon recombination. As gas density increases at lower altitudes, the recombination process prevails, since the gas molecules and ions are closer together. The balance between these two processes determines the quantity of ionization present.\nIonization depends primarily on the Sun and its Extreme Ultraviolet (EUV) and X-ray irradiance which varies strongly with solar activity. The more magnetically active the Sun is, the more sunspot active regions there are on the Sun at any one time. Sunspot active regions are the source of increased coronal heating and accompanying increases in EUV and X-ray irradiance, particularly during episodic magnetic eruptions that include solar flares that increase ionization on the sunlit side of the Earth and solar energetic particle events that can increase ionization in the polar regions. Thus the degree of ionization in the ionosphere follows both a diurnal (time of day) cycle and the 11-year solar cycle. There is also a seasonal dependence in ionization degree since the local winter hemisphere is tipped away from the Sun, thus there is less received solar radiation. Radiation received also varies with geographical location (polar, auroral zones, mid-latitudes, and equatorial regions). There are also mechanisms that disturb the ionosphere and decrease the ionization.\nSydney Chapman proposed that the region below the ionosphere be called neutrosphere\n(the neutral atmosphere).\nLayers of ionization.\nAt night the F layer is the only layer of significant ionization present, while the ionization in the E and D layers is extremely low. During the day, the D and E layers become much more heavily ionized, as does the F layer, which develops an additional, weaker region of ionization known as the F1 layer. The F2 layer persists by day and night and is the main region responsible for the refraction and reflection of radio waves.\nD layer.\nThe D layer is the innermost layer, above the surface of the Earth. Ionization here is due to Lyman series-alpha hydrogen radiation at a wavelength of 121.6 nanometre (nm) ionizing nitric oxide (NO). In addition, solar flares can generate hard X-rays (wavelength &lt; 1 nm) that ionize N2 and O2. Recombination rates are high in the D layer, so there are many more neutral air molecules than ions.\nMedium frequency (MF) and lower high frequency (HF) radio waves are significantly attenuated within the D layer, as the passing radio waves cause electrons to move, which then collide with the neutral molecules, giving up their energy. Lower frequencies experience greater absorption because they move the electrons farther, leading to greater chance of collisions. This is the main reason for absorption of HF radio waves, particularly at 10\u00a0MHz and below, with progressively less absorption at higher frequencies. This effect peaks around noon and is reduced at night due to a decrease in the D layer's thickness; only a small part remains due to cosmic rays. A common example of the D layer in action is the disappearance of distant AM broadcast band stations in the daytime.\nDuring solar proton events, ionization can reach unusually high levels in the D-region over high and polar latitudes. Such very rare events are known as Polar Cap Absorption (PCA) events, because the increased ionization significantly enhances the absorption of radio signals passing through the region. In fact, absorption levels can increase by many tens of dB during intense events, which is enough to absorb most (if not all) transpolar HF radio signal transmissions. Such events typically last less than 24 to 48 hours.\nE layer.\nThe E layer is the middle layer, above the surface of the Earth. Ionization is due to soft X-ray (1\u201310\u00a0nm) and far ultraviolet (UV) solar radiation ionization of molecular oxygen (O2). Normally, at oblique incidence, this layer can only reflect radio waves having frequencies lower than about 10\u00a0MHz and may contribute a bit to absorption on frequencies above. However, during intense sporadic E events, the Es layer can reflect frequencies up to 50\u00a0MHz and higher. The vertical structure of the E layer is primarily determined by the competing effects of ionization and recombination. At night the E layer weakens because the primary source of ionization is no longer present. After sunset an increase in the height of the E layer maximum increases the range to which radio waves can travel by reflection from the layer.\nThis region is also known as the Kennelly\u2013Heaviside layer or simply the Heaviside layer. Its existence was predicted in 1902 independently and almost simultaneously by the American electrical engineer Arthur Edwin Kennelly (1861\u20131939) and the British physicist Oliver Heaviside (1850\u20131925). In 1924 its existence was detected by Edward V. Appleton and Miles Barnett.\nEs layer.\nThe Es layer (sporadic E-layer) is characterized by small, thin clouds of intense ionization, which can support reflection of radio waves, frequently up to 50\u00a0MHz and rarely up to 450\u00a0MHz. Sporadic-E events may last for just a few minutes to many hours. Sporadic E propagation makes VHF-operating by radio amateurs very exciting when long-distance propagation paths that are generally unreachable \"open up\" to two-way communication. There are multiple causes of sporadic-E that are still being pursued by researchers. This propagation occurs every day during June and July in northern hemisphere mid-latitudes when high signal levels are often reached. The skip distances are generally around . Distances for one hop propagation can be anywhere from . Multi-hop propagation over is also common, sometimes to distances of or more.\nF layer.\nThe F layer or region, also known as the Appleton\u2013Barnett layer, extends from about to more than above the surface of Earth. It is the layer with the highest electron density, which implies signals penetrating this layer will escape into space. Electron production is dominated by extreme ultraviolet (UV, 10\u2013100\u00a0nm) radiation ionizing atomic oxygen. The F layer consists of one layer (F2) at night, but during the day, a secondary peak (labelled F1) often forms in the electron density profile. Because the F2 layer remains by day and night, it is responsible for most skywave propagation of radio waves and long distance high frequency (HF, or shortwave) radio communications.\nAbove the F layer, the number of oxygen ions decreases and lighter ions such as hydrogen and helium become dominant. This region above the F layer peak and below the plasmasphere is called the topside ionosphere.\nFrom 1972 to 1975 NASA launched the AEROS and AEROS B satellites to study the F region.\nIonospheric model.\nAn ionospheric model is a mathematical description of the ionosphere as a function of location, altitude, day of year, phase of the sunspot cycle and geomagnetic activity. Geophysically, the state of the ionospheric plasma may be described by four parameters: \"electron density, electron and ion temperature\" and, since several species of ions are present, \"ionic composition\". Radio propagation depends uniquely on electron density.\nModels are usually expressed as computer programs. The model may be based on basic physics of the interactions of the ions and electrons with the neutral atmosphere and sunlight, or it may be a statistical description based on a large number of observations or a combination of physics and observations. One of the most widely used models is the International Reference Ionosphere (IRI), which is based on data and specifies the four parameters just mentioned. The IRI is an international project sponsored by the Committee on Space Research (COSPAR) and the International Union of Radio Science (URSI). The major data sources are the worldwide network of ionosondes, the powerful incoherent scatter radars (Jicamarca, Arecibo, Millstone Hill, Malvern, St Santin), the ISIS and Alouette topside sounders, and in situ instruments on several satellites and rockets. IRI is updated yearly. IRI is more accurate in describing the variation of the electron density from bottom of the ionosphere to the altitude of maximum density than in describing the total electron content (TEC). Since 1999 this model is \"International Standard\" for the terrestrial ionosphere (standard TS16457).\nPersistent anomalies to the idealized model.\nIonograms allow deducing, via computation, the true shape of the different layers. Nonhomogeneous structure of the electron/ion-plasma produces rough echo traces, seen predominantly at night and at higher latitudes, and during disturbed conditions.\nWinter anomaly.\nDuring summer, the Sun illuminates the Earth at a steeper angle, leading to an increase in daytime ion production in the F2 layer, particularly in electron density. Extreme ultraviolet (EUV) radiation plays a key role in the formation of ions in this region of the ionosphere. \nIn summer, the composition of the neutral atmosphere\u2014mainly N\u2082, O\u2082, and atomic oxygen (O)\u2014changes. In particular, the reduction in atomic oxygen and the enhanced transport processes of neutral nitrogen accelerate ion loss mechanisms. These seasonal variations lead to increased recombination, diffusion, and loss of ions during summer. As a result, despite the higher ion production in summer, the greater losses cause the total F2 ionization to decrease; this explains why daytime maximum electron densities are higher in winter (the local winter season) than in summer.\nThe \u201cwinter anomaly\u201d or winter effect is observed almost continuously at mid-latitudes in the Northern Hemisphere. That is, during the local winter months, the critical frequencies of the F2 layer (and consequently the maximum electron density) are higher than in summer. This behavior persists in the Northern Hemisphere regardless of solar activity; however, it is rarely observed or appears weak in the Southern Hemisphere during periods of low solar energy.\nDuring periods of low solar activity in the Southern Hemisphere, this anomaly is generally absent. The behavior of the F2 layer differs significantly between the two hemispheres; in particular, in the south, the correlation between the winter anomaly and variations in F2 electron density with solar activity is quite weak. Regional geomagnetic features, such as the South Atlantic Magnetic Anomaly, along with atmospheric circulation, contribute to the inconsistency of the winter anomaly in the Southern Hemisphere.\nMany statistical and observational studies have shown that both variations in atmospheric components (such as the O/N\u2082 ratio) and solar and magnetic activity contribute to triggering this anomaly. Scientists such as Torr &amp; Torr, Rishbeth, and Roble have demonstrated that the composition of neutral gases plays a key role in the ion loss mechanisms of the F2 layer.\nEquatorial anomaly.\nWithin approximately \u00b1 20 degrees of the \"magnetic equator\", is the \"equatorial anomaly.\" It is the occurrence of a trough in the ionization in the F2 layer at the equator and crests at about 17 degrees in magnetic latitude. The Earth's magnetic field lines are horizontal at the magnetic equator. Solar heating and tidal oscillations in the lower ionosphere move plasma up and across the magnetic field lines. This sets up a sheet of electric current in the E region which, with the horizontal magnetic field, forces ionization up into the F layer, concentrating at \u00b1 20 degrees from the magnetic equator. This phenomenon is known as the \"equatorial fountain\".\nEquatorial electrojet.\nThe worldwide solar-driven wind results in the so-called Sq (solar quiet) current system in the E region of the Earth's ionosphere (ionospheric dynamo region) ( altitude). Resulting from this current is an electrostatic field directed west\u2013east (dawn\u2013dusk) in the equatorial day side of the ionosphere. At the magnetic dip equator, where the geomagnetic field is horizontal, this electric field results in an enhanced eastward current flow within \u00b1 3 degrees of the magnetic equator, known as the equatorial electrojet.\nEphemeral ionospheric perturbations.\nX-rays: sudden ionospheric disturbances (SID).\nX-rays and extreme ultraviolet (EUV) radiation emitted during solar flares rapidly increase electron density in the ionosphere, particularly in the D region, thereby enhancing the absorption of high-frequency (HF) radio waves. This phenomenon leads to a sudden ionospheric disturbance (SID) and can cause radio blackouts lasting for hours. Analyses using GNSS (Global Navigation Satellite System) data and TEC (Total Electron Content) measurements clearly demonstrate the fluctuations in electron density and the increased absorption caused by solar flares. Furthermore, electrons in the D region quickly recombine after the flare, allowing transmission conditions to return to normal.\nDuring solar flares, high-energy X-rays and ultraviolet radiation reach the upper layers of the Earth's atmosphere, causing molecular ionization. As a result, short-term decreases in ozone concentration and temporary increases in ionospheric density have been observed. During a strong proton event in 1982, ozone concentration was temporarily observed to decrease by as much as 70%.\nX-ray and UV radiation emitted during solar activity rapidly alter the temperature and electrical conductivity of the ionosphere, leading to disruptions in shortwave and high-frequency (HF) radio communications. The thickness and altitude of the ionosphere can increase or decrease, directly affecting HF radio propagation, and signal transmission may take hours to return to normal, particularly following solar flares.\nProtons: polar cap absorption (PCA).\nAssociated with solar flares is a release of high-energy protons. These particles can hit the Earth within 15 minutes to 2 hours of the solar flare. The protons spiral around and down the magnetic field lines of the Earth and penetrate into the atmosphere near the magnetic poles increasing the ionization of the D and E layers. PCA's typically last anywhere from about an hour to several days, with an average of around 24 to 36 hours. Coronal mass ejections can also release energetic protons that enhance D-region absorption in the polar regions.\nStorms.\nGeomagnetic storms and ionospheric storms are temporary and intense disturbances of the Earth's magnetosphere and ionosphere.\nDuring a geomagnetic storm the F\u2082 layer will become unstable, fragment, and may even disappear completely. In the Northern and Southern polar regions of the Earth aurorae will be observable in the night sky.\nLightning.\nLightning can cause ionospheric perturbations in the D-region in one of two ways. The first is through VLF (very low frequency) radio waves launched into the magnetosphere. These so-called \"whistler\" mode waves can interact with radiation belt particles and cause them to precipitate onto the ionosphere, adding ionization to the D-region. These disturbances are called \"lightning-induced electron precipitation\" (LEP) events.\nAdditional ionization can also occur from direct heating/ionization as a result of huge motions of charge in lightning strikes. These events are called early/fast.\nIn 1925, C. T. R. Wilson proposed a mechanism by which electrical discharge from lightning storms could propagate upwards from clouds to the ionosphere. Around the same time, Robert Watson-Watt, working at the Radio Research Station in Slough, UK, suggested that the ionospheric sporadic E layer (Es) appeared to be enhanced as a result of lightning but that more work was needed. In 2005, C. Davis and C. Johnson, working at the Rutherford Appleton Laboratory in Oxfordshire, UK, demonstrated that the Es layer was indeed enhanced as a result of lightning activity. Their subsequent research has focused on the mechanism by which this process can occur.\nApplications.\nRadio communication.\nDue to the ability of ionized atmospheric gases to refract high frequency (HF, or shortwave) radio waves, the ionosphere can reflect radio waves directed into the sky back toward the Earth. Radio waves directed at an angle into the sky can return to Earth beyond the horizon. This technique, called \"skip\" or \"skywave\" propagation, has been used since the 1920s to communicate at international or intercontinental distances. The returning radio waves can reflect off the Earth's surface into the sky again, allowing greater ranges to be achieved with multiple hops. This communication method is variable and unreliable, with reception over a given path depending on time of day or night, the seasons, weather, and the 11-year sunspot cycle. During the first half of the 20th century it was widely used for transoceanic telephone and telegraph service, and business and diplomatic communication. Due to its relative unreliability, shortwave radio communication has been mostly abandoned by the telecommunications industry, though it remains important for high-latitude communication where the availability of satellite-based radio communication may be insufficient. Shortwave broadcasting is useful in crossing international boundaries and covering large areas at low cost. Automated services still use shortwave radio frequencies, as do radio amateur hobbyists for private recreational contacts and to assist with emergency communications during natural disasters. Armed forces use shortwave so as to be independent of vulnerable infrastructure, including satellites, and the low latency of shortwave communications make it attractive to stock traders, where milliseconds count.\nMechanism of refraction.\nWhen a radio wave reaches the ionosphere, the electric field in the wave forces the electrons in the ionosphere into oscillation at the same frequency as the radio wave. Some of the radio-frequency energy is given up to this resonant oscillation. The oscillating electrons will then either be lost to recombination or will re-radiate the original wave energy. Total refraction can occur when the collision frequency of the ionosphere is less than the radio frequency, and if the electron density in the ionosphere is great enough.\nA qualitative understanding of how an electromagnetic wave propagates through the ionosphere can be obtained by recalling geometric optics. Since the ionosphere is a plasma, it can be shown that the refractive index is less than unity. Hence, the electromagnetic \"ray\" is bent away from the normal rather than toward the normal as would be indicated when the refractive index is greater than unity. It can also be shown that the refractive index of a plasma, and hence the ionosphere, is frequency-dependent, see Dispersion (optics).\nThe critical frequency is the limiting frequency at or below which a radio wave is reflected by an ionospheric layer at vertical incidence. If the transmitted frequency is higher than the plasma frequency of the ionosphere, then the electrons cannot respond fast enough, and they are not able to re-radiate the signal. It is calculated as shown below:\n formula_1\nwhere N = electron density per m3 and fcritical is in Hz.\nThe Maximum Usable Frequency (MUF) is defined as the upper frequency limit that can be used for transmission between two points at a specified time.\n formula_2\nwhere formula_3 = angle of arrival, the angle of the wave relative to the horizon, and sin is the sine function.\nThe cutoff frequency is the frequency below which a radio wave fails to penetrate a layer of the ionosphere at the incidence angle required for transmission between two specified points by refraction from the layer.\nGPS/GNSS ionospheric correction.\nThere are a number of models used to understand the effects of the ionosphere on global navigation satellite systems. The Klobuchar model is currently used to compensate for ionospheric effects in GPS. This model was developed at the U.S. Air Force Geophysical Research Laboratory circa 1974 by John (Jack) Klobuchar. The Galileo navigation system uses the NeQuick model. Galileo broadcasts 3 coefficients to compute the effective ionization level, which is then used by the NeQuick model to compute a range delay along the line-of-sight.\nThe Klobuchar model is a widely used empirical model designed to mitigate ionospheric delays in GPS satellites. It was developed around 1974 by John A. Klobuchar at the U.S. Air Force Geophysics Laboratory. The model represents the ionosphere as a single-layer, two-dimensional structure and calculates ionospheric delay using eight parameters (the Ion \u03b1 and Ion \u03b2 coefficients). These parameters are updated seasonally and at different times of the day by GPS master control stations. In particular, at mid-latitudes, the model can reduce the impact of the ionosphere by approximately 50%. It is used to compute vertical Total Electron Content (TEC) values at the height of the ionosphere (around 350\u2013450 km) and to derive slant TEC values based on the angle at which the signal passes through the ionosphere.\nThe NeQuick model is a more sophisticated and higher-accuracy model used in the Galileo navigation system. It calculates ionospheric electron density numerically based on geographic coordinates, time (UT), and ionospheric height parameters. The Galileo system broadcasts three coefficients to account for ionospheric delay, which are then used with the NeQuick model to compute the ionospheric range delay along the signal path. Compared to the Klobuchar model for GPS, the NeQuick model provides approximately 20% better horizontal positioning accuracy and 11% better vertical accuracy for single-frequency receivers.\nBoth models are primarily designed to improve positioning accuracy, especially for single-frequency GNSS receivers. Due to the high cost of multi-frequency receivers, the Klobuchar and NeQuick models provide real-time and cost-effective solutions. Today, the Klobuchar model is used as the standard for GPS, mitigating approximately 50% of ionospheric error, while the NeQuick model, with its more detailed structure, can compensate for up to 70% of ionospheric effects. Corrections made using these models reduce the impact of variations in the signal\u2019s travel time through the ionosphere, thereby enhancing the accuracy of GNSS-based navigation and positioning.\nOther applications.\nThe open system electrodynamic tether, which uses the ionosphere, is being researched. The space tether uses plasma contactors and the ionosphere as parts of a circuit to extract energy from the Earth's magnetic field by electromagnetic induction.\nMeasurements.\nOverview.\nScientists explore the structure of the ionosphere by a wide variety of methods. They include:\nA variety of experiments, such as HAARP (High Frequency Active Auroral Research Program), involve high power radio transmitters to modify the properties of the ionosphere. These investigations focus on studying the properties and behavior of ionospheric plasma, with particular emphasis on being able to understand and use it to enhance communications and surveillance systems for both civilian and military purposes. HAARP was started in 1993 as a proposed twenty-year experiment, and is currently active near Gakona, Alaska.\nThe SuperDARN radar project researches the high- and mid-latitudes using coherent backscatter of radio waves in the 8 to 20\u00a0MHz range. Coherent backscatter is similar to Bragg scattering in crystals and involves the constructive interference of scattering from ionospheric density irregularities. The project involves more than 11 countries and multiple radars in both hemispheres.\nScientists are also examining the ionosphere by the changes to radio waves, from satellites and stars, passing through it. The Arecibo Telescope located in Puerto Rico, was originally intended to study Earth's ionosphere.\nIonograms.\nIonograms show the virtual heights and critical frequencies of the ionospheric layers and which are measured by an ionosonde. An ionosonde sweeps a range of frequencies, usually from 0.1 to 30\u00a0MHz, transmitting at vertical incidence to the ionosphere. As the frequency increases, each wave is refracted less by the ionization in the layer, and so each penetrates further before it is reflected. Eventually, a frequency is reached that enables the wave to penetrate the layer without being reflected. For ordinary mode waves, this occurs when the transmitted frequency just exceeds the peak plasma, or critical, frequency of the layer. Tracings of the reflected high frequency radio pulses are known as ionograms. Reduction rules are given in: \"URSI Handbook of Ionogram Interpretation and Reduction\", edited by William Roy Piggott and Karl Rawer, Elsevier Amsterdam, 1961 (translations into Chinese, French, Japanese and Russian are available).\nIncoherent scatter radars.\nIncoherent scatter radars operate above the critical frequencies. Therefore, the technique allows probing the ionosphere, unlike ionosondes, also above the electron density peaks. The thermal fluctuations of the electron density scattering the transmitted signals lack coherence, which gave the technique its name. Their power spectrum contains information not only on the density, but also on the ion and electron temperatures, ion masses and drift velocities. Incoherent scatter radars can also measure neutral atmosphere movements, such as atmospheric tides, after making assumptions about ion-neutral collision frequency across the ionospheric dynamo region.\nGNSS radio occultation.\nRadio occultation is a remote sensing technique where a GNSS signal tangentially scrapes the Earth, passing through the atmosphere, and is received by a Low Earth Orbit (LEO) satellite. As the signal passes through the atmosphere, it is refracted, curved and delayed. An LEO satellite samples the total electron content and bending angle of many such signal paths as it watches the GNSS satellite rise or set behind the Earth. Using an Inverse Abel's transform, a radial profile of refractivity at that tangent point on earth can be reconstructed.\nMajor GNSS radio occultation missions include the GRACE, CHAMP, and COSMIC.\nIndices of the ionosphere.\nIn empirical models of the ionosphere such as Nequick, the following indices are used as indirect indicators of the state of the ionosphere.\nSolar intensity.\nF10.7 and R12 are two indices commonly used in ionospheric modelling. Both are valuable for their long historical records covering multiple solar cycles. F10.7 is a measurement of the intensity of solar radio emissions at a frequency of 2800\u00a0MHz made using a ground radio telescope. R12 is a 12 months average of daily sunspot numbers. The two indices have been shown to be correlated with each other.\nHowever, both indices are only indirect indicators of solar ultraviolet and X-ray emissions, which are primarily responsible for causing ionization in the Earth's upper atmosphere. We now have data from the GOES spacecraft that measures the background X-ray flux from the Sun, a parameter more closely related to the ionization levels in the ionosphere.\nIonospheres of other planets and natural satellites.\nObjects in the Solar System that have appreciable atmospheres (i.e., all of the major planets and many of the larger natural satellites) generally produce ionospheres. Planets known to have ionospheres include Venus, Mars, Jupiter, Saturn, Uranus, and Neptune.\nThe atmosphere of Titan includes an ionosphere that ranges from about in altitude and contains carbon compounds. Ionospheres have also been observed at Io, Europa, Ganymede, Triton, and Pluto.\nSee also.\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15099", "revid": "157530", "url": "https://en.wikipedia.org/wiki?curid=15099", "title": "Ido (language)", "text": ""}
{"id": "15100", "revid": "4490355", "url": "https://en.wikipedia.org/wiki?curid=15100", "title": "Interlingua", "text": "Constructed language\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nInterlingua (, ) is an international auxiliary language (IAL) developed between 1937 and 1951 by the American International Auxiliary Language Association (IALA). It is a constructed language of the \"naturalistic\" variety, whose vocabulary, grammar, and other characteristics are derived from natural languages. Interlingua literature maintains that (written) Interlingua is comprehensible to the billions of people who speak Romance languages, though it is actively spoken by only a few hundred.\nInterlingua was developed to combine a simple, mostly regular grammar with a vocabulary common to a wide range of western European languages, making it easy to learn for those whose native languages were sources of Interlingua's vocabulary and grammar.\nThe name Interlingua comes from the Latin words ', meaning 'between', and ', meaning 'tongue' or 'language'. These morphemes are the same in Interlingua; thus, \"Interlingua\" would mean 'between language'.\nOverview.\nInterlingua focuses on common vocabulary shared by Western European languages, which are often descended from or heavily influenced by the Latin language (such as the Romance languages) and Greek language. Interlingua organizers have four \"primary control languages\" where, by default, a word (or variant thereof) is expected to appear in at least three of them to qualify for inclusion in Interlingua. These are English; French; Italian; and a combination of Spanish and Portuguese which are treated as a single mega-language for Interlingua purposes. Additionally, German and Russian have been dubbed \"secondary control languages\". While the result is often akin to Neo-Latin as the most frequent source of commonality, Interlingua words can have origins in any language, as long as they have drifted into the primary control languages as loanwords. For example, the Japanese words \"geisha\" and \"samurai\" and the Finnish word \"sauna\" are used in most Western European languages, and therefore in Interlingua as well; similarly, the Guugu Yimithirr word \"gangurru\" is used in latinized form (Interlingua: , English: \"kangaroo\").\nThe maintainers of Interlingua attempt to keep the grammar simple and word formation regular, and use only a small number of roots and affixes. This is intended to make the language quicker to learn.\nHistory.\nThe American heiress Alice Vanderbilt Morris (1874\u20131950) became interested in linguistics and the international auxiliary language movement in the early 1920s. In 1924, Morris and her husband, Dave Hennen Morris, established the non-profit International Auxiliary Language Association (IALA) in New York City. Their aim was to place the study of IALs on a more complex and scientific basis. Morris developed the research programme of IALA in consultation with Edward Sapir, William Edward Collinson, and Otto Jespersen.\nInvestigations of the auxiliary language problem were in progress at the International Research Council, the American Council on Education, the American Council of Learned Societies, the British, French, Italian, and American Associations for the advancement of science, and other groups of specialists. Morris created IALA as a continuation of this work.\nInternational Auxiliary Language Association.\nThe IALA became a major supporter of mainstream American linguistics. Numerous studies by Sapir, Collinson, and Morris Swadesh in the 1930s and 1940s, for example, were funded by IALA. Alice Morris edited several of these studies and provided much of IALA's financial support. For example, Morris herself edited Sapir and Morris Swadesh's 1932 cross-linguistic study of ending-point phenomena, and Collinson's 1937 study of indication. IALA also received support from groups such as the Carnegie Corporation, the Ford Foundation, the Research Corporation, and the Rockefeller Foundation.\nIn its early years, IALA concerned itself with three tasks: finding other organizations around the world with similar goals; building a library of books about languages and interlinguistics; and comparing extant IALs, including Esperanto, Esperanto II, Ido, Peano's Interlingua (Latino sine flexione), Novial, and Interlingue (Occidental). In pursuit of the last goal, it conducted parallel studies of these languages, with comparative studies of national languages.\nAt the Second International Interlanguage Congress, held in Geneva in 1931, IALA began to break new ground; 27 recognized linguists signed a testimonial of support for IALA's research programme. An additional eight added their signatures at the third congress, convened in Rome in 1933. That same year, Herbert N. Shenton and Edward Thorndike became influential in IALA's work by authoring studies in the interlinguistic field.\nThe first steps towards the finalization of Interlingua were taken in 1937, when a committee of 24 linguists from 19 universities published \"Some Criteria for an International Language and Commentary\". However, the outbreak of World War II in 1939 cut short the intended biannual meetings of the committee.\nDevelopment of a new language.\nOriginally, the association had not intended to create its own language. Its goal was to identify which auxiliary language already available was best suited for international communication, and how to promote it more effectively. However, after ten years of research, many members of IALA concluded that none of the existing interlanguages were up to the task. By 1937, the members had made the decision to create a new language, to the surprise of the world's interlanguage community.\nTo that point, much of the debate had been equivocal on the decision to use naturalistic (e.g., Peano's Interlingua, Novial and Occidental) or systematic (e.g., Esperanto and Ido) words. During the war years, proponents of a naturalistic interlanguage won out. The first support was Thorndike's paper; the second was a concession by proponents of the systematic languages that thousands of words were already present in many, or even a majority, of the European languages. Their argument was that systematic derivation of words was a Procrustean bed, forcing the learner to unlearn and re-memorize a new derivation scheme when a usable vocabulary was already available. IALA from that point assumed the position that a naturalistic language would be best.\nIALA's research activities were based in Liverpool, before relocating to New York due to the outbreak of World War II, where E. Clark Stillman established a new research staff. Stillman, with the assistance of Alexander Gode, constructed the methodology for selecting Interlingua vocabulary based on a comparison of control languages.\nIn 1943 Stillman left for war work and Gode became Acting Director of Research. IALA began to develop models of the proposed language, the first of which were presented in Morris's \"General Report\" in 1945.\nThe four models.\nFrom 1946 to 1948, French linguist Andr\u00e9 Martinet was Director of Research. During this period IALA continued to develop models and conducted polling to determine the optimal form of the final language. In 1946, IALA sent an extensive survey to more than 3,000 language teachers and related professionals on three continents.\nModel P was unchanged from 1945; Model M was relatively modern in comparison to more classical P. Model K was slightly modified in the direction of Ido. The resulting four models that were canvassed were:\nAn example sentence:\nThe vote total ended up as follows: P 26.6%, M 37.5%, C 20%, and K 15%. The two more schematic models, C and K, were rejected. Of the two naturalistic models, M attracted somewhat more support than P. Taking national biases into account (for example, the French who were polled disproportionately favored Model M), IALA decided on a compromise between models M and P, with certain elements of C.\nFinalization.\nThe German-American Gode and the French Martinet did not get along. Martinet resigned and took up a position at Columbia University in 1948, and Gode took on the last phase of Interlingua's development. His task was to combine elements of Model M and Model P; take the flaws seen in both by the polled community and repair them with elements of Model C as needed; and develop a vocabulary. Alice Vanderbilt Morris died in 1950, and the funding that had sustained IALA ceased, but sufficient funds remained to publish a dictionary and grammar. The vocabulary and grammar of Interlingua were first presented in 1951, when IALA published the finalized \"\" and the Interlingua\u2013English Dictionary (IED). In 1954, IALA published an introductory manual entitled \"Interlingua a Prime Vista\" (\"Interlingua at First Sight\").\nInterlingua as presented by the IALA is very close to Peano's Interlingua (Latino sine flexione), both in its grammar and especially in its vocabulary. A distinct abbreviation was adopted: IA instead of IL.\nInterlingua's first decades.\nAn early practical application of Interlingua was the scientific newsletter \"Spectroscopia Molecular\", published from 1952 to 1980. In 1954, the Second World Cardiological Congress in Washington, D.C. released summaries of its talks in both English and Interlingua. Within a few years, it found similar use at nine further medical congresses. Between the mid-1950s and the late 1970s, some thirty scientific and medical journals provided article summaries in Interlingua. Gode wrote a monthly column in Interlingua in the \"Science Newsletter\" published by the Science Service from the early 1950s until his death in 1970.\nIALA closed its doors in 1953 but was not formally dissolved until 1956 or later. Its role in promoting Interlingua was largely taken on by Science Service, which hired Gode as head of its newly formed Interlingua Division. Hugh E. Blair, Gode's close friend and colleague, became his assistant. A successor organization, the Interlingua Institute, was founded in 1970 to promote Interlingua in the US and Canada. The new institute supported the work of other linguistic organizations, made considerable scholarly contributions and produced Interlingua summaries for scholarly and medical publications. One of its largest achievements was two immense volumes on phytopathology produced by the American Phytopathological Society in 1976 and 1977.\nBeginning in the 1980s, UMI has held international conferences every two years (typical attendance at the earlier meetings was 50 to 100) and launched a publishing programme that eventually produced over 100 volumes. Several Scandinavian schools undertook projects that used Interlingua as a means of teaching the international scientific and intellectual vocabulary.\nIn 2000, the Interlingua Institute was dissolved amid funding disputes with the UMI; the American Interlingua Society, established the following year, succeeded the institute.\nInterlingua today.\nThe original goal of an interlanguage meant for global events has faced competition from English as a lingua franca and International English in the 21st century. The scientific community frequently uses English in international conferences and publications, for example, rather than Interlingua. However, the rise of the Internet has made it easier for the general public with an interest in constructed languages to learn Interlingua. Interlingua is promoted internationally by the Union Mundial pro Interlingua. Periodicals and books are produced by national organizations, such as the Societate American pro Interlingua, the Svenska S\u00e4llskapet f\u00f6r Interlingua, and the Union Brazilian pro Interlingua.\n\"Panorama in Interlingua\" is the most prominent of several Interlingua periodicals. It is a 28-page magazine published bimonthly that covers current events, science, editorials, and Interlingua.\nCommunity.\nIt is not certain how many people have an active knowledge of Interlingua. Most constructed languages other than Esperanto have very few speakers. The Hungarian census of 2001, which collected information about languages spoken, found just two people in the entire country who claimed to speak Interlingua.\nAdvocates say that Interlingua's greatest advantage is that it is the most widely \"understood\" international auxiliary language besides Interlingua (IL) de A.p.I. by virtue of its naturalistic (as opposed to schematic) grammar and vocabulary, allowing those familiar with a Romance language, and educated speakers of English, to read and understand it without prior study.\nInterlingua web pages include editions of Wikipedia and Wiktionary, and a number of periodicals, including \"Panorama in Interlingua\" from the Union Mundial pro Interlingua (UMI).\nEvery two years, the UMI organizes an international conference in a different country. In the year between, the Scandinavian Interlingua societies co-organize a conference in Sweden, as a number of Interlingua speakers are in Scandinavia. National organizations such as the Union Brazilian pro Interlingua also organize regular conferences.\nInterlingua is taught in some high schools and universities, sometimes as a means of teaching other languages quickly, presenting interlinguistics, or introducing an international vocabulary. A two-week course was taught at the University of Granada in Spain in 2007, for example.\nAs of 2019[ [update]], Google Keyboard supports Interlingua.\nOrthography.\nInterlingua has a largely phonemic orthography.\nInterlingua alphabet.\nInterlingua uses the 26 letters of the ISO basic Latin alphabet with no diacritics. The alphabet, pronunciation in IPA and letter names in Interlingua are:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCollateral orthography.\nThe book \"Grammar of Interlingua\" defines in \u00a715 a \"collateral orthography\" that defines how a word is spelt in Interlingua once assimilated regardless of etymology.\nPhonology.\nInterlingua is primarily a written language, and the pronunciation is not entirely settled. The sounds in parentheses are not used by all speakers.\nPronunciation.\nFor the most part, consonants are pronounced as in English, while the vowels are like Spanish. Written double consonants may be geminated as in Italian for extra clarity or pronounced as single as in English or French. Interlingua has five falling diphthongs, , and , although and are rare.\nStress.\nThe \"general rule\" is that stress falls on the vowel before the last consonant (e.g., , 'language', , 'to be', , 'requirement') ignoring the final plural (e.g. , the plural of , still has the same stress as the singular), and where that is not possible, on the first vowel (, 'way', , 'I create'). There are a few exceptions, and the following rules account for most of them:\nSpeakers may pronounce all words according to the general rule mentioned above. For example, is acceptable, although is more common.\nPhonotactics.\nInterlingua has no explicitly defined phonotactics. However, the prototyping procedure for determining Interlingua words, which strives for internationality, should in general lead naturally to words that are easy for most learners to pronounce. In the process of forming new words, an ending cannot always be added without a modification of some kind in between. A good example is the plural \"-s\", which is always preceded by a vowel to prevent the occurrence of a hard-to-pronounce consonant cluster at the end. If the singular does not end in a vowel, the final \"-s\" becomes \"-es.\"\nLoanwords.\nUnassimilated foreign loanwords, or borrowed words, are spelled as in their language of origin. Their spelling may contain diacritics, or accent marks. If the diacritics do not affect pronunciation, they are removed.\nVocabulary.\nThe maintainers of Interlingua select words for it based on their presence and commonality in languages dubbed 'control' languages. These are Spanish, Portuguese, Italian, French, and English, with German and Russian dubbed as secondary controls. An Interlingua word's origin can be from any language so long as it has spread to the control languages. Spanish and Portuguese, both West Iberian languages, are treated as one unit. The largest number of Interlingua words are of Latin origin, with the Greek and Germanic languages providing the second and third largest number. The remainder of the vocabulary originates in Slavic languages and non-Indo-European languages.\nEligibility.\nA word, that is a form with meaning, is eligible for the Interlingua vocabulary if it is verified by at least three of the four primary control languages. Either secondary control language can substitute for a primary language. Any word of Indo-European origin found in a control language can contribute to the eligibility of an international word. In some cases, the archaic or \"potential\" presence of a word can contribute to its eligibility.\nA word can be potentially present in a language when a derivative is present, but the word itself is not. English \"proximity\", for example, gives support to Interlingua , meaning 'near, close'. This counts as long as one or more control languages actually have this basic root word, which the Romance languages all do. Potentiality also occurs when a concept is represented as a compound or derivative in a control language, the morphemes that make it up are themselves international, and the combination adequately conveys the meaning of the larger word. An example is Italian (lit. 'flamebearer'), meaning 'match, lucifer', which leads to Interlingua , or 'match'. This word is thus said to be potentially present in the other languages although they may represent the meaning with a single morpheme.\nWords do not enter the Interlingua vocabulary solely because cognates exist in a sufficient number of languages. If their meanings have become different over time, they are considered different words for the purpose of Interlingua eligibility. If they still have one or more meanings in common, however, the word can enter Interlingua with this smaller set of meanings.\nIf this procedure did not produce an international word, the word for a concept was originally taken from Latin (see below). This only occurred with a few grammatical particles.\nForm.\nThe form of an Interlingua word is considered an \"international prototype\" with respect to the other words. On the one hand, it should be neutral, free from characteristics peculiar to one language. On the other hand, it should maximally capture the characteristics common to all contributing languages. As a result, it can be transformed into any of the contributing variants using only these language-specific characteristics. If the word has any derivatives that occur in the source languages with appropriate parallel meanings, then their morphological connection must remain intact; for example, the Interlingua word for 'time' is spelled and not or in order to match it with its derived adjectives, such as .\nThe language-specific characteristics are closely related to the sound laws of the individual languages; the resulting words are often close or even identical to the most recent form common to the contributing words. This sometimes corresponds with that of Vulgar Latin. At other times, it is much more recent or even contemporary. It is never older than the classical period.\nAn illustration.\nThe French , Italian , Spanish , and Portuguese appear quite different, but they descend from a historical form . German , Dutch and English \"eye\" (cf. Czech and Polish , Russian and Ukrainian ()) are related to this form in that all three descend from Proto-Indo-European . In addition, international derivatives like \"ocular\" and occur in all of Interlingua's control languages. Each of these forms contributes to the eligibility of the Interlingua word. German and English base words do not influence the form of the Interlingua word, because their Indo-European connection is considered too remote. Instead, the remaining base words and especially the derivatives determine the form found in Interlingua.\nFree word-building.\nWords can also be included in Interlingua by deriving them using Interlingua words and affixes; a method called free word-building. Thus, in the Interlingua\u2013English Dictionary (IED), Alexander Gode followed the principle that every word listed is accompanied by all of its clear compounds and derivatives, along with the word or words it is derived from. A reader skimming through the IED notices many entries followed by large groups of derived and compound words. A good example is the Interlingua word , which is followed by , , , , , , , and many other words.\nOther words in the IED do not have derivatives listed. Gode saw these words as potential word families. Although all derived words in the IED are found in at least one control language, speakers may make free use of Interlingua roots and affixes. For example, ('jade') can be used to form , ('to jadify, make into jade, make look like jade'), , and so on. These word forms would be impermissible in English but would be good Interlingua.\nWord-building by analogy.\nGode and Hugh E. Blair explained in the that the \"basic principle of practical word-building\" is analogical. If a pattern can be found in the existing international vocabulary, new words can be formed according to that pattern. A meaning of the suffix is 'person who practices the art or science of...' This suffix allows the derivation of from , from , and so on. An Interlingua speaker can freely form from and from by following the same pattern.\nUsefulness and clarity.\nAs noted above, the only limits to free word-building in Interlingua are \"clarity\" and \"usefulness\". These concepts are touched upon here:\nAny number of words could be formed by stringing roots and affixes together, but some would be more useful than others. For example, the English word \"rainer\" means 'a person who rains', but most people would be surprised that it is included in English dictionaries. The corresponding Interlingua word is unlikely to appear in a dictionary because of its lack of utility. Interlingua, like any traditional language, \"could\" build up large numbers of these words, but this would be undesirable.\nGode stressed the principle of \"clarity\" in free word-building. As Gode noted, the noun ('mariner') can be formed from the adjective , because its meaning is clear. The noun meaning 'navy' cannot be formed, because its meaning would not be clear from the adjective and suffix that gave rise to it.\nGrammar.\nInterlingua has been developed to omit any grammatical feature that is absent from any one primary control language. Thus, Interlingua has no noun\u2013adjective agreement by gender, case, or number (cf. Spanish and Portuguese or Italian , 'black female cats'), because this is absent from English, and it has no progressive verb tenses (English \"I am reading\"), because they are absent from French. Conversely, Interlingua distinguishes singular nouns from plural nouns because all the control languages do. With respect to the secondary control languages, Interlingua has articles, unlike Russian.\nThe definite article is invariable, as in English (\"the\"). Nouns have no grammatical gender. Plurals are formed by adding \"-s\", or \"-es\" after a final consonant. Personal pronouns take one form for the subject and one for the direct object and reflexive. In the third person, the reflexive is always \"se\". Most adverbs are derived regularly from adjectives by adding , or after a \"-c\". An adverb can be formed from any adjective in this way.\nVerbs take the same form for all persons (, , , 'I live', 'you live', 'she lives'). The indicative (, 'appear', 'appears') is the same as the imperative ( 'appear!'), and there is no subjunctive. Three common verbs usually take short forms in the present tense: for 'is', 'am', 'are;' for 'has', 'have;' and for 'go', 'goes'. A few irregular verb forms are available, but rarely used.\nThere are four simple tenses (present, past, future, and conditional), three compound tenses (past, future, and conditional), and the passive voice. The compound structures employ an auxiliary plus the infinitive or the past participle (e.g., , 'He has arrived'). Simple and compound tenses can be combined in various ways to express more complex tenses (e.g., , 'We would have died').\nWord order is subject\u2013verb\u2013object, except that a direct object pronoun or reflexive pronoun comes before the verb (, 'I see them'). Adjectives may precede or follow the nouns they modify, but they most often follow it. The position of adverbs is flexible, though constrained by common sense.\nThe grammar of Interlingua has been described as similar to that of the Romance languages, but simplified, primarily under the influence of English. A 1991 paper argued that Interlingua's grammar was similar to the simple grammars of Japanese and particularly Chinese.\nF. P. Gopsill has written that Interlingua has no irregularities, although Gode's \"Interlingua Grammar\" suggests that Interlingua has a small number of irregularities.\nReception.\nOne criticism that applies to naturalistic constructed languages in general is that if an educated traveller is willing to learn a naturalistic conlang, they may find it even more useful to learn a natural language outright, such as International English. Planned conlangs at least hold out the promise of \"fixing\" or standardizing certain irregular aspects of natural languages and providing unique advantages, despite the lack of speakers, but naturalistic conlangs have to compete with the natural languages they are based on. In practice, conferences with international attendance tend to be held in a natural language popular among the attendees rather than an international auxiliary language.\nSamples.\nFrom an essay by Alexander Gode:\nInterlingua has detached itself from the movement for the development and introduction of a universal language for all humanity. Whether or not one believes that a language for all humanity is possible, whether or not one believes that Interlingua will become such a language is totally irrelevant from the point of view of Interlingua itself. The only fact that matters (from the point of view of Interlingua itself) is that Interlingua, thanks to its ambition of reflecting the cultural and thus linguistic homogeneity of the West, is capable of rendering tangible services at this precise moment in the history of the world. It is by its present contributions and not by the promises of its adherents that Interlingua wishes to be judged.\nFlags and symbols.\nAs with Esperanto, there have been proposals for a flag of Interlingua; the proposal by Czech translator Karel Podrazil is recognized by multilingual sites. It consists of a white four-pointed star extending to the edges of the flag and dividing it into an upper blue and lower red half. The star is symbolic of the four cardinal directions, and the two halves symbolize Romance and non-Romance speakers of Interlingua who understand each other.\nAnother symbol of Interlingua is the \"Blue Marble\" surrounded by twelve stars on a black or blue background, echoing the twelve stars of the Flag of Europe (because the source languages of Interlingua are purely European).\nSee also.\n&lt;templatestyles src=\"Column/styles.css\"/&gt;\n&lt;templatestyles src = \"Column/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15101", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=15101", "title": "Isotactic", "text": ""}
{"id": "15102", "revid": "50813145", "url": "https://en.wikipedia.org/wiki?curid=15102", "title": "Isle of Wight", "text": "County and island of England\nThe Isle of Wight ( ) is an island off the south coast of England which, together with its surrounding uninhabited islets and skerries, is also a ceremonial county. The county is bordered by Hampshire across the Solent strait to the north, and is otherwise surrounded by the English Channel. Its largest settlement is Ryde, and the administrative centre is Newport.\nThe Isle of Wight has a land area of and had a population of 140,794 in 2022, making it the largest and second-most populous English island. The island is largely rural, with the largest settlements primarily on the coast. These include Ryde in the north-east, Shanklin and Sandown in the south-east, and the large villages of Totland and Freshwater in the west. Newport is located inland at the point at which the River Medina broadens into its estuary, and Cowes and East Cowes flank the estuary on the northern coast. For local government purposes the island is a unitary authority area. It was historically part of Hampshire.\nThe island is known for its mild climate, coastal scenery, and verdant landscape of fields, downland, and chines. It is said to be the sunniest place of Great Britain. It has been designated a UNESCO Biosphere Reserve. The distance between the Isle of Wight and mainland Great Britain is between . The island also contains dinosaur fossils.\nThe island has played an essential part in the defence of the ports of Southampton and Portsmouth and has been near the front line of conflicts through the ages, having faced the Spanish Armada and weathered the Battle of Britain. From the Victorian era significant urban development took place as the island developed into a tourist destination; it was home to the poets Algernon Charles Swinburne and Alfred, Lord Tennyson, and Queen Victoria built her summer residence and final home, Osborne House, at East Cowes. It has a maritime and industrial tradition of boat-building, sail-making, the manufacture of flying boats, hovercraft, and Britain's space rockets. The island hosts annual music festivals, including the Isle of Wight Festival, which in 1970 was the largest rock music event ever held.\nName.\nThe oldest records that give a name for the Isle of Wight are from the Roman Empire. It was called or in Latin and () or in Ancient Greek. Latin , Old English , and Old Welsh and were recorded from the Anglo-Saxon period. In medieval Irish sources such as the it is found as , , and . The Domesday Book called the island . The modern Welsh name is ( meaning 'island'). These are all variants of the same name, possibly sharing a Celtic origin with Welsh 'work', a cognate of both Latin ('lever', or literally 'the act of lifting') and Old English ('weight'). It may mean 'place of the division,' since the island divides the two arms of the Solent.\nIn Old English, inhabitants of the Isle were known as .\nHistory.\nStone Age.\nDuring Pleistocene glacial periods sea levels were lower than at present, and the area that today forms the Solent was part of the valley of the now extinct Solent River. The river flowed eastward from Dorset, following the course of the modern Solent strait. The river travelled east of the Isle of Wight before flowing southwest towards the major Channel River system. At these times, extensive gravel terraces associated with the Solent River and the forerunners of the island's modern rivers were deposited. During warmer interglacial periods, silts, beach gravels, clays, and muds of marine and estuarine origin were deposited due to higher sea levels, suggesting similar marine or estuary conditions to those experienced today.\nThe earliest clear evidence of Lower Palaeolithic archaic human occupation on what is now the Isle of Wight is found close to Priory Bay. More than 300 acheulean handaxes have been recovered from the beach and cliff slopes, originating from a sequence of Pleistocene gravels dating approximately to MIS 11-MIS 9 (424,000\u2013374,000 years ago). Reworked and abraded artefacts found at the site may be considerably older however, closer to 500,000 years old. The identity of the hominids who produced these tools is unknown. However, sites and fossils of the same age range in Europe are often attributed to \"Homo heidelbergensis\" or early populations of Neanderthals.\nA Middle Palaeolithic Mousterian flint assemblage, consisting of 50 handaxes and debitage, has been recovered from Great Pan Farm in the Medina Valley near Newport. Gravel sequences at the site have been dated to the MIS 3 interstadial during the last glacial period (c.\u200950,000 years ago). These tools are associated with the late Neanderthal occupation, and evidence of late Neanderthal presence is seen across Britain at this time.\nNo significant evidence of Upper Palaeolithic activity exists on the Isle of Wight. This period is associated with the expansion and establishment of populations of modern human (\"Homo sapiens\") hunter-gatherers in Europe, beginning around 45,000 years ago. However, evidence of late Upper Palaeolithic activity has been found at nearby sites on the mainland, notably Hengistbury Head in Dorset, dating to just before the onset of the Holocene and the end of the last glacial period c.\u200911,700 years ago. \nEvidence of Mesolithic hunter-gatherer occupation on the island is generally found along the river valleys, particularly along the Solent coastline of the island and in the former catchment of the western Yar. Other key terrestrial sites are found at Newtown Creek, Werrar, and Wootton-Quarr.\nA submerged escarpment below sea level off Bouldnor Cliff on the island's Solent coastline has yielded an internationally significant mesolithic archaeological site. The Bouldnor Cliff site exhibits evidence of seasonal occupation by Mesolithic hunter-gatherers dating to c.\u20096050 BC. Finds include flint tools, burnt flint, worked timbers, wooden platforms, and pits. The worked wood shows evidence of splitting large planks from oak trunks, interpreted as being intended for use as dug-out canoes. DNA analysis of sediments at the site yielded wheat DNA, not found in Britain until 2,000 years after the occupation at Bouldnor Cliff. It has been suggested this is evidence of wide-reaching trade in Mesolithic Europe; however, the contemporaneity of the wheat with the Mesolithic occupation has been contested. Owing to lower sea levels during the Mesolithic the hunter-gatherer site was located on a river bank surrounded by wetlands and woodland. As sea levels rose throughout the early Holocene the Solent flooded, submerging the site.\nFrom c.\u20096,000 years ago migrations of farming populations to Britain from northwest Europe brought the onset of the Neolithic, largely replacing and assimilating previous mesolithic hunter-gatherer populations. On the Isle of Wight Neolithic occupation is attested to by flint tool finds, pottery and monuments. The Isle of Wight's neolithic communities were agriculturalists, farming livestock and crops. The Isle of Wight's most recognisable neolithic site is the Longstone at Mottistone, the remains of an early Neolithic long barrow. Initially constructed with two standing stones at the entrance, only one remains upright today. The site would have likely served as a communal tomb and ritual site for nearby farming communities. A Neolithic mortuary enclosure has also been identified on Tennyson Down near Freshwater.\nBronze Age and Iron Age.\nFrom c.\u20094,400\u00a0\u2013 c.\u20094,200 years ago Britain experienced a new wave of migrations from continental Europe, linked to the Bell Beaker Culture. Bell beaker migrants are typically thought to have introduced metal-working to Britain marking the beginning of the Bronze Age. Evidence of early Bronze Age occupation on the Isle of Wight include distinctive bell beaker pots, flint tools, occupation sites and finds of bronze weapons and tools, occurring either individually or in hoard deposits such as the famous Arreton hoard. Highly visible evidence of early Bronze Age activity on the Isle of Wight comes in the form of the barrow monuments present across the island's chalk downland. It is likely these barrows were high-status burial sites, and often occur in 'cemeteries' a notable example being Five Barrows near Brook. \nBronze Age Britain had large tin reserves in Cornwall and Devon areas, which was necessary to smelt bronze. At that time, the sea level was much lower, and carts of tin were brought across the Solent at low tide for export, possibly on the Ferriby Boats. Anthony Snodgrass suggests that a shortage of tin, as a part of the Bronze Age Collapse and trade disruptions in the Mediterranean around 1300 BC, forced metalworkers to seek an alternative to bronze.\nFrom the 7th century BC, during the Late Iron Age, the Isle of Wight, like the rest of Great Britain, was occupied by the Celtic Britons, in the form of the Durotriges tribe, as attested by finds of their coins, for example, the South Wight Hoard, and the Shalfleet Hoard. The island was known as \"Ynys Weith\" in Brittonic Celtic. Southeastern Britain experienced significant immigration, which is reflected in the current residents' genetic makeup. As the Iron Age began, tin value likely dropped sharply, greatly changing the Isle of Wight's economy. Trade, however, continued, as evidenced by the local abundance of European Iron Age coins.\nRoman period.\nJulius Caesar reported that the Belgae took the Isle of Wight in about 85\u00a0BC and recognised the culture of this general region as \"Belgic\" but made no reference to Vectis. The Roman historian Suetonius mentions that the island was captured by the commander Vespasian. The Romans built no towns on the island, but the remains of at least seven Roman villas have been found, indicating the prosperity of local agriculture. First-century exports were principally hides, enslaved people, hunting dogs, grain, cattle, silver, gold, and iron.\nEarly medieval period.\nThere are indications that the island had vast trading links, with a port at Bouldnor, evidence of Bronze Age tin trading, and finds of Late Iron Age coins. Starting in AD 449, the 5th and 6th centuries saw groups of Germanic-speaking peoples from Northern Europe crossing the English Channel and gradually set about conquering the region.\nDuring the Early Middle Ages, the island was settled by Jutes as the heathen kingdom of the Wihtwara. In Asser\u2019s \"Life of Alfred\", he states that the West Saxon kings Cerdic and Cynric granted lordship of the Isle of Wight to two brothers, Stuf and Wihtgar, said to be of Jutish and Gothic origin and cousins of Cynric. The brothers then set about exterminating the native Britons, either killing them or driving them into exile. According to Bede, in 685, the Isle of Wight was invaded by King C\u00e6dwalla of Wessex, who attempted to violently replace the Jutish inhabitants with his own followers. In 686, the native King Arwald was killed in battle, and the island became the last part of English lands to be converted to Christianity.\nIt suffered especially from Viking raids and was often used as a winter base by Viking raiders when they could not reach Normandy. Later, both Earl Tostig and his brother Harold Godwinson (who became King Harold II) held manors on the island.\nNorman Conquest to 18th century.\nThe Norman Conquest of 1066 created the position of Lord of the Isle of Wight; the island was given by William the Conqueror to his kinsman William FitzOsbern. Carisbrooke Priory and the fort of Carisbrooke Castle were then founded. Allegiance was sworn to FitzOsbern rather than the king; the Lordship was subsequently granted to the de Redvers family by Henry I after his succession in 1100.\nFor nearly 200 years the island was a semi-independent feudal fiefdom, with the de Redvers family ruling from Carisbrooke. The final private owner was the Countess Isabella de Fortibus, who, on her deathbed in 1293, was persuaded to sell it to Edward I. Subsequently, the island was under the control of the English Crown and its Lordship a royal appointment.\nThe island continued to be attacked from the continent: it was raided in 1374 by the fleet of Castile and in 1377 by French raiders who burned several towns, including Newtown.\nUnder Henry VIII, who developed the Royal Navy and its Portsmouth base, the island was fortified at Yarmouth, Cowes, East Cowes, and Sandown.\nThe French invasion on 21 July 1545 (famous for the sinking of the \"Mary Rose\" on the 19th) was repulsed by local militia.\nOn 1 May 1647, Swedish and English ships clashed in a brief skirmish off the island, ending in the Swedish fleet being able to escape.\nDuring the English Civil War, King Charles I fled to the Isle of Wight, believing he would receive sympathy from Governor Robert Hammond. Still, Hammond imprisoned the king in Carisbrooke Castle.\nDuring the Seven Years' War, the island was a staging post for British troops departing on expeditions against the French coast, such as the Raid on Rochefort. During 1759, with a planned French invasion imminent, a large force of soldiers was stationed there. The French called off their invasion following the Battle of Quiberon Bay.\n19th century.\nIn the spring of 1817, the twenty-one year old John Keats spent time in Carisbrooke and Shanklin, where he found inspiration in the countryside and coast, and worked on his long poem \"Endymion\".\nIn the mid-1840s, potato blight was first found in the UK on the island, having arrived from Belgium. It was later transmitted to Ireland.\nIn the 1860s, what remains in real terms the most expensive ever government spending project saw fortifications built on the island and in the Solent, as well as elsewhere along the south coast, including the Palmerston Forts, The Needles Batteries, and Fort Victoria, because of fears about possible French invasion.\nThe future Queen Victoria spent childhood holidays on the island and became fond of it. When she became queen, she made Osborne House her winter home. Subsequently, the island became a fashionable holiday resort for many, including Alfred, Lord Tennyson, Julia Margaret Cameron, and Charles Dickens (who wrote much of \"David Copperfield\" there), as well as the French painter Berthe Morisot and members of European royalty.\nUntil the queen's example, the island had been rural, with most people employed in farming, fishing, or boat-building. The boom in tourism, spurred by growing wealth and leisure time and by Victoria's presence, led to the significant urban development of the island's coastal resorts. As one report summarises, \"The Queen's regular presence on the island helped put the Isle of Wight 'on the map' as a Victorian holiday and wellness destination ... and her former residence Osborne House is now one of the most visited attractions on the island.\" While on the island, the queen used a bathing machine that could be wheeled into the water on Osborne Beach; inside the small wooden hut, she could undress and then bathe, without being visible to others. Her machine had a changing room and a WC with plumbing. The refurbished machine is now displayed at the beach.\nOn 14 January 1878, Alexander Graham Bell demonstrated an early version of the telephone to the queen, placing calls to Cowes, Southampton, and London. These were the first publicly-witnessed long-distance telephone calls in the UK. The queen tried the device and considered the process to be \"quite extraordinary\" although the sound was \"rather faint\". She later asked to buy the equipment that was used, but Bell offered to make \"a set of telephones\" specifically for her.\nThe world's first radio station was set up by Guglielmo Marconi in 1897, during her reign, at the Needles Battery, at the western tip of the island. A high mast was erected near the Royal Needles Hotel as part of an experiment on communicating with ships at sea. That location is now the site of the Marconi Monument. In 1898 the first paid wireless telegram (called a \"Marconigram\") was sent from this station, and the island was for some time the home of the National Wireless Museum near Ryde.\nQueen Victoria died at Osborne House on 22 January 1901 at 81.\n20th century and later.\nDuring the Second World War, the island was frequently bombed. With its proximity to German-occupied France, the island hosted observation stations, transmitters, and the RAF radar station at Ventnor. Adolf Hitler personally suggested an invasion of the Isle of Wight as a supplementary operation for Operation Sealion, and the possibility of an invasion was incorporated into Fuhrer Directive 16. Field Marshal Alan Brooke, in charge of defending the UK during 1940, was sceptical about being able to hold the island in the face of an invasion, instead considering that British forces would retreat to the western side of the island rather than commit forces against what might be a diversionary landing. In the end no invasion of the island was carried out as German naval commanders feared any invasion force might be cut off by British naval forces, particularly Royal Navy submarines.\nThe island was the starting point for one of the earlier Operation Pluto pipelines to feed fuel to Europe after the Normandy landings.\nThe Needles Battery was used to develop and test the Black Arrow and Black Knight space rockets, which were subsequently launched from Woomera, Australia.\nThe Isle of Wight Festival was a large rock festival near Afton Down, West Wight, in August 1970, following two smaller events in 1968 and 1969. The 1970 show was one of the last public performances by Jimi Hendrix and attracted somewhere between 600,000 and 700,000 attendees. The festival was revived in 2002 in a different format and is now an annual event.\nOn 26 October 2020, an oil tanker, the \"Nave Andromeda\", suspected to have been hijacked by Nigerian stowaways, was stormed southeast of the island by the Special Boat Service. Seven people believed to be Nigerians seeking UK asylum were handed over to Hampshire Police.\nGovernance.\nThe island had a single Member of Parliament until 2024. The Isle of Wight constituency covered the entire island, with 138,300 permanent residents in 2011, being one of the most populated constituencies in the United Kingdom (more than 50% above the English average). Following passage of the Parliamentary Voting System and Constituencies Act 2011, the Sixth Periodic Review of Westminster constituencies was to have changed this, but this was deferred to no earlier than October 2022 by the Electoral Registration and Administration Act 2013. Thus the single constituency remained for the 2015, 2017 and 2019 general elections. However, two separate constituencies, Isle of Wight East and Isle of Wight West were created for the island under the 2022 review, and were first contested in the 2024 general election.\nThe Isle of Wight is a ceremonial and non-metropolitan county. Since the abolition of its two borough councils and restructuring of the Isle of Wight County Council into the new Isle of Wight Council in 1995, it has been administered by a single tier Island Council which has the same powers as a unitary authority in England.\nThere have been small regionalist movements: the Vectis National Party and the Isle of Wight Party; but they have attracted little support at elections.\nGeography and environment.\nThe Isle of Wight is situated between the Solent and the English Channel, is roughly rhomboid in shape, and covers an area of . Slightly more than half, mainly in the west, is designated as the Isle of Wight Area of Outstanding Natural Beauty. The island has of farmland, of developed areas, and of coastline. Its landscapes are diverse, leading to its oft-quoted description as \"England in miniature\". In June 2019 the whole island was designated a UNESCO Biosphere Reserve, recognising the sustainable relationships between its residents and the local environment.\nWest Wight is predominantly rural, with dramatic coastlines dominated by the chalk downland ridge, running across the whole island and ending in the Needles stacks. The southwestern quarter is commonly referred to as the Back of the Wight, and has a unique character. The highest point on the island is St Boniface Down in the south east, which at is a marilyn. The most notable habitats on the rest of the island are probably the soft cliffs and sea ledges, which are scenic features, important for wildlife, and internationally protected.\nThe island has three principal rivers. The River Medina flows north into the Solent, the Eastern Yar flows roughly northeast to Bembridge Harbour, and the Western Yar flows the short distance from Freshwater Bay to a relatively large estuary at Yarmouth. Without human intervention the sea might well have split the island into three: at the west end where a bank of pebbles separates Freshwater Bay from the marshy backwaters of the Western Yar east of Freshwater, and at the east end where a thin strip of land separates Sandown Bay from the marshy Eastern Yar basin.\nThe Undercliff between St Catherine's Point and Bonchurch is the largest area of landslip morphology in western Europe.\nThe north coast is unusual in having four high tides each day, with a double high tide every twelve and a half hours. This arises because the western Solent is narrower than the eastern; the initial tide of water flowing from the west starts to ebb before the stronger flow around the south of the island returns through the eastern Solent to create a second high water.\nGeology.\nThe Isle of Wight is made up of a variety of rock types dating from early Cretaceous (around 127 million years ago) to the middle of the Palaeogene (around 30 million years ago). The geological structure is dominated by a large monocline which causes a marked change in age of strata from the northern younger Tertiary beds to the older Cretaceous beds of the south. This gives rise to a dip of almost 90 degrees in the chalk beds, seen best at the Needles.\nThe northern half of the island is mainly composed of clays, with the southern half formed of the chalk of the central east\u2013west downs, as well as Upper and Lower Greensands and Wealden strata. These strata continue west from the island across the Solent into Dorset, forming the basin of Poole Harbour (Tertiary) and the Isle of Purbeck (Cretaceous) respectively. The chalky ridges of the Isle of Wight and Purbeck were a single formation before they were breached by waters from the River Frome during the last ice age, forming the Solent and turning the Isle of Wight into an island. The Needles, along with Old Harry Rocks on Purbeck, represent the edges of this breach.\nAll the rocks found on the island are sedimentary, such as limestones, mudstones and sandstones. They are rich in fossils; many can be seen exposed on beaches as the cliffs erode. Lignitic coal is present in small quantities within seams, and can be seen on the cliffs and shore at Whitecliff Bay. Fossilised molluscs have been found there, and also on the northern coast along with fossilised crocodiles, turtles and mammal bones; the youngest date back to around 30\u00a0million years ago.\nThe island is one of the most important areas in Europe for dinosaur fossils. The eroding cliffs often reveal previously hidden remains, particularly along the Back of the Wight. Dinosaur bones and fossilised footprints can be seen in and on the rocks exposed around the island's beaches, especially at Yaverland and Compton Bay, from the strata of the Wessex Formation. As a result, the island has been nicknamed \"Dinosaur Island\" and Dinosaur Isle was established in 2001.\nThe area was affected by sea level changes during the repeated Quaternary glaciations. The island probably became separated from the mainland about 125,000 years ago, during the Ipswichian interglacial.\nClimate.\nLike the rest of the UK, the island has an oceanic climate, but is somewhat milder and sunnier, which makes it a holiday destination. It also has a longer growing season. Lower Ventnor and the neighbouring Undercliff have a particular microclimate, because of their sheltered position south of the downs. The island enjoys 1,800\u20132,100 hours of sunshine a year. Some years have almost no snow in winter, and only a few days of hard frost. The island is in Hardiness zone 9.\nFlora and fauna.\nThe Isle of Wight is one of the few places in England where the European red squirrel is still flourishing, as no competing grey squirrels are to be found there. Other mammalian species on the island include the European badger, hedgehog, least weasel, red fox and stoat, with the hedgehogs proving to be quite popular amongst locals and visitors alike; in 2019, a rescue and rehabilitation group was organised to assist them, called Save Our Hedgehogs Isle of Wight. The island is also home to several protected species, such as the European dormouse and several rare bats, including the western barbastelle.\nThere are several species of deer on the island, both endemic and non-native, all of which are monitored and surveyed annually by the organisation Isle of Wight Deer Conservation. According to the British Deer Society (BDS), the Isle of Wight Biodiversity Group would like to see the island's ecosystems and flora preserved, one method being to keep the island \"deer-free\"; however, of the five types of deer documented, the European red deer and roe deer are truly native species, having been known to swim to the island from the mainland.\nThe diminutive Chinese Reeve's muntjac or barking deer\u2014so-called due to its signature dog-like \"bark\" when threatened\u2014is one of the smallest deer species on Earth and is present on the island. The Asian sika (the second-largest species on the island) and Eurasian fallow deer also will journey to the island from the mainland, generally seen in very small herds, in pairs, or alone. Ultimately, all five of the deer species seen on the Isle of Wight are adept swimmers, thus any that are observed may or may not be long-term Island residents. Nonetheless, the island deer (that are present at any given time) tend to remain strategically hidden and are generally thought of as being difficult to spot, even on such a small island. Besides deer, there exists a colony of feral goats on Ventnor's downs.\nThe Glanville fritillary, a species of butterfly, has a distribution in the United Kingdom largely restricted to the edges of the island's crumbling cliffs.\nA competition in 2002 named the pyramidal orchid as the Isle of Wight's county flower.\nThe occurrence of species and habitats of conservation importance in the island's waters has led to the designation of a suite of marine protected areas seeking to protect these features, including marine conservation zones (MCZs) and special areas of conservation (SACs). The island's marine environment also forms a component of its UNESCO Biosphere Reserve, and is part of the Western English Channel Important Marine Mammal Area (IMMA).\nEconomy.\nSocio-economic data.\nThe table below shows the regional gross value (in millions of pounds) added by the Isle of Wight economy, at current prices, compiled by the Office for National Statistics.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAccording to the 2011 census, the island's population of 138,625 lives in 61,085 households, giving an average household size of 2.27 people.\n41% of households own their home outright and a further 29% own with a mortgage, so in total 70% of households are owned (compared to 68% for South East England).\nCompared to South East England, the island has fewer children (19% aged 0\u201317 compared to 22% for the South East) and more elderly (24% aged 65+ compared to 16% for the South East), giving an average age of 44 years for an island resident compared to 40 in South East England.\nIndustry and agriculture.\nThe largest industry on the island is tourism, but it also has a significant agriculture including sheep, dairy farming and arable crops. Traditional agricultural commodities are more difficult to market off the island because of transport costs, but local farmers have succeeded in exploiting some specialist markets, with the higher price of such products absorbing the transport costs. One of the most successful agricultural sectors is now the growing of crops under cover, particularly salad crops including tomatoes and cucumbers. The island has a warmer climate and a longer growing season than much of the United Kingdom. Garlic has been grown in Newchurch for many years, and is, in part, exported to France. This has led to the establishment of an annual Garlic Festival at Newchurch, which is one of the largest events of the local calendar.\nA favourable climate supports two vineyards, including one of the oldest in the British Isles at Adgestone. Lavender is grown for its oil. The largest agricultural sector has been dairying, but due to low milk prices and strict legislation for UK milk producers, the dairy industry has been in decline: there were nearly 150 producers in the mid-1980s, but now just 24.\nMaritime industries, especially the making of sailcloth and boat building, have long been associated with the island, although this has diminished in recent years. GKN operates what began as the British Hovercraft Corporation, a subsidiary of (and known latterly as) Westland Aircraft, although they have reduced the extent of plant and workforce and sold the main site. Previously it had been the independent company Saunders-Roe, one of the island's most notable historic firms that produced many flying boats and the world's first hovercraft.\nAnother manufacturing activity is in composite materials, used by boat-builders and the wind turbine manufacturer Vestas, which has a wind turbine blade factory and testing facilities in West Medina Mills and East Cowes.\nBembridge Airfield is the home of Britten-Norman, manufacturers of the Islander and Trislander aircraft. This is shortly to become the site of the European assembly line for Cirrus light aircraft. The Norman Aeroplane Company is a smaller aircraft manufacturing company operating in Sandown. There have been three other firms that built planes on the island.\nIn 2005, Northern Petroleum began exploratory drilling for oil at its Sandhills-2 borehole at Porchfield, but ceased operations in October that year after failing to find significant reserves.\nBreweries.\nThere are three breweries on the island. Goddards Brewery in Ryde opened in 1993. David Yates, who was head brewer of the Island Brewery, started brewing as Yates Brewery at the Inn at St Lawrence in 2000. Ventnor Brewery, which closed in 2009, was the last incarnation of Burt's Brewery, brewing since the 1840s in Ventnor. Until the 1960s most pubs were owned by Mews Brewery, situated in Newport near the old railway station, but it closed and the pubs were taken over by Strong's, and then by Whitbread. By some accounts Mews beer was apt to be rather cloudy and dark. In the 19th century they pioneered the use of screw top cans for export to British India.\nServices.\nTourism and heritage.\nThe island's heritage is a major asset that has for many years supported its tourist economy. Holidays focused on natural heritage, including wildlife and geology, are becoming an alternative to the traditional British seaside holiday, which went into decline in the second half of the 20th century due to the increased affordability of foreign holidays. The island is still an important destination for coach tours from other parts of the United Kingdom.\nTourism is still the largest industry, and most island towns and villages offer hotels, hostels and camping sites. In 1999, it hosted 2.7 million visitors, with 1.5 million staying overnight, and 1.2 million day visits; only 150,000 of these were from abroad. Between 1993 and 2000, visits increased at an average rate of 3% per year.\nAt the turn of the 19th century the island had ten pleasure piers, including two at Ryde and a \"chain pier\" at Seaview. The Victoria Pier in Cowes succeeded the earlier Royal Pier but was itself removed in 1960. The piers at Ryde, Seaview, Sandown, Shanklin and Ventnor originally served a coastal steamer service that operated from Southsea on the mainland. The piers at Seaview, Shanklin, Ventnor and Alum Bay were all destroyed by various storms during the 20th century; only the railway pier at Ryde and the piers at Sandown, Totland Bay (currently closed to the public) and Yarmouth survive.\nBlackgang Chine is the oldest theme park in Britain, opened in 1843. The skeleton of a dead whale that its founder Alexander Dabell found in 1844 is still on display.\nAs well as its more traditional attractions, the island is often host to walking or cycling holidays through the attractive scenery. An annual walking festival has attracted considerable interest. The Isle of Wight Coastal Path follows the coastline as far as possible, deviating onto roads where the route along the coast is impassable.\nThe tourist board for the island is Visit Isle of Wight, a non-profit company. It is the Destination Management Organisation for the Isle of Wight, a public and private sector partnership led by the private sector, and consists of over 1,200 companies, including the ferry operators, the local bus company, rail operator and tourism providers working together to collectively promote the island. Its income is derived from the Wight BID, a business improvement district levy fund.\nA major contributor to the local economy is sailing and marine-related tourism.\nSummer Camp at Camp Beaumont is an attraction at the old Bembridge School site.\nMedia.\nThe main local newspaper purchased is the \"Isle of Wight County Press\". Its circulation has declined over the years, estimated at 11,575 in 2024, especially after it was taken over by Newsquest in July 2017. In 2018 a new free newspaper was launched, the \"Isle of Wight Observer\".\nOn-line news websites include \"Island Echo\", launched in May 2012, and \"On the Wight\".\nThe island has a local commercial radio station and a community radio station: commercial station Isle of Wight Radio has broadcast in the medium-wave band since 1990 and on 107.0\u00a0MHz (with three smaller transmitters on 102.0\u00a0MHz) FM since 1998, as well as streaming on the Internet. Community station Vectis Radio has broadcast online since 2010, and in 2017 started broadcasting on FM 104.6. The station operates from the Riverside Centre in Newport. The island is also covered by a number of local stations on the mainland, including the BBC station BBC Radio Solent broadcast from Southampton. The island's not-for-profit community radio station Angel Radio opened in 2007. Angel Radio began broadcasting on 91.5\u00a0MHz from studios in Cowes and a transmitter near Newport.\nImportant broadcasting infrastructure includes Chillerton Down transmitting station with a mast that is the tallest structure on the island, and Rowridge transmitting station, which broadcasts the main television signal both locally and for most of Hampshire and parts of Dorset and West Sussex.\nCulture.\nLanguage and dialect.\nThe local accent is similar to the traditional dialect of Hampshire, featuring the dropping of some consonants and an emphasis on longer vowels. It is similar to the West Country dialects heard in South West England, but less pronounced.\nThe island has its own local and regional words. Some, such as \"nipper/nips\" (a young male person), are still sometimes used and shared with neighbouring areas of the mainland. A few are unique to the island, for example \"overner\" and \"caulkhead\" (see below). Others are more obscure and now used mainly for comic emphasis, such as \"mallishag\" (meaning \"caterpillar\"), \"gurt\" meaning \"large\", \"nammit\" (a mid-morning snack) and \"gallybagger\" (\"scarecrow\", and now the name of a local cheese).\nIdentity.\nThere remains occasional confusion between the Isle of Wight as a county and its former position within Hampshire. The island was regarded and administered as a part of Hampshire until 1 April 1890, when its distinct identity was recognised with the formation of Isle of Wight County Council (see also \"Politics of the Isle of Wight\"). However, it remained a part of Hampshire until the local government reforms of 1974, when it became a full ceremonial county with its own Lord Lieutenant.\nIn January 2009, the first general flag for the county was accepted by the Flag Institute.\nIsland residents are sometimes referred to as \"Vectensians\", \"Vectians\" or, if born on the island, \"caulkheads\". One theory is that this last comes from the once prevalent local industry of caulking or sealing wooden boats; the term became attached to islanders either because they were so employed, or as a derisory term for perceived unintelligent labourers from elsewhere. The term \"overner\" is used for island residents originating from the mainland (an abbreviated form of \"overlander\", which is an archaic term for \"outsider\" still found in parts of Australia).\nResidents refer to the island as \"The Island\", as did Jane Austen in \"Mansfield Park\", and sometimes to the UK mainland as \"North Island\".\nTo promote the island's identity and culture, the High Sheriff, Robin Courage, founded an Isle of Wight Day; the first was held on 24 September 2016.\nSport.\nSport plays a key part in the culture of the Isle of Wight. Sports include golf, marathon, cycling and sailing.\nThe motorcycle speedway team Isle of Wight Warriors compete at the Smallbrook Stadium.\nUntil their folding in 2016, the ice hockey team Isle of Wight Raiders played at the Ryde Arena. They had a feeder team Vectis Tigers which in turn had a youth feeder team Isle of Wight Wildcats.\nIn football, the now-disbanded Ryde Sports F.C., founded in 1888, was one of the eight founder members of the Hampshire League in 1896. Currently there are several non-league clubs, the most notable of which are Brading Town, Cowes Sports, East Cowes Vics and Newport IOW. There is also an Isle of Wight Saturday Football League which feeds into the Hampshire League with two divisions and two reserve team leagues.\nThe island competes in the Island Games, a collection of sports which are hosted and played by small, usually European islands. The Isle of Wight have competed in all editions of the tournament, winning 203 gold medals, 208 silver medals and 220 bronze medals totalling to 631 medals.\nMusic.\nThe island is home to the Isle of Wight Festival and until 2016, Bestival, before it was relocated to Lulworth Estate in Dorset. In 1970, the festival was headlined by Jimi Hendrix attracting an audience of 600,000, some six times the local population at the time. It is the home of the bands The Bees, Trixie's Big Red Motorbike, Level 42, and Wet Leg.\nTransport.\nThe Isle of Wight has of roadway. It does not have a motorway, although there is a short stretch of dual carriageway towards the north of Newport near the hospital and prison.\nA comprehensive bus network operated by Southern Vectis links most settlements, with Newport as its central hub.\nJourneys away from the island involve a ferry journey. Car ferry and passenger catamaran services are run by Wightlink and Red Funnel, and a hovercraft passenger service (the only such remaining in the world) by Hovertravel.\nThe island formerly had its own railway network of over , but only one line remains in regular use. The Island Line is part of the United Kingdom's National Rail network, running a little under from Shanklin to Ryde Pier Head, where there is a connecting ferry service to Portsmouth Harbour station on the mainland network. The line was opened by the Isle of Wight Railway in 1864, and from 1996 to 2007 was run by the smallest train operating company on the network, Island Line Trains. It is notable for utilising old ex-London Underground rolling stock, due to the small size of its tunnels and unmodernised signalling. Branching off the Island Line at Smallbrook Junction is the heritage Isle of Wight Steam Railway, which runs for to the outskirts of Wootton on the former line to Newport.\nThere are two airfields for general aviation, Isle of Wight Airport at Sandown and Bembridge Airport.\nThe island has over of cycleways, many of which can be enjoyed off-road. The principal trails are:\nPrisons.\nThe Isle of Wight is near the densely populated south of England, yet separated from the mainland. This position led to it hosting three prisons: Albany, Camp Hill and Parkhurst, all located outside Newport near the main road to Cowes. Albany and Parkhurst were among the few Category A prisons in the UK until they were downgraded in the 1990s. The downgrading of Parkhurst was precipitated by a major escape: three prisoners (two murderers and a blackmailer) escaped from the prison on 3 January 1995 for four days, before being recaptured. Parkhurst enjoyed notoriety as one of the toughest jails in the United Kingdom, and housed many notable inmates including the Yorkshire Ripper Peter Sutcliffe, New Zealand drug lord Terry Clark and the Kray twins.\nCamp Hill is located adjacent but to the west of Albany and Parkhurst, on the very edge of Parkhurst Forest, having been converted first to a borstal and later to a Category C prison. It was built on the site of an army camp (both Albany and Parkhurst were barracks); there is a small estate of tree-lined roads with the former officers' quarters (now privately owned) to the south and east. Camp Hill closed as a prison in March 2013.\nThe management of all three prisons was merged into a single administration, under HMP Isle of Wight in April 2009.\nEducation.\nThere are 69 local education authority-maintained schools on the Isle of Wight, and two independent schools. As a rural community, many of these are small and with fewer pupils than in urban areas. The Isle of Wight College is located on the outskirts of Newport.\nFrom September 2010, there was a transition period from the three-tier system of primary, middle and high schools to the two-tier system that is usual in England. Some schools have now closed, such as Chale C.E. Primary. Others have become \"federated\", such as Brading C.E. Primary and St Helen's Primary. Christ the King College started as two \"middle schools\", Trinity Middle School and Archbishop King Catholic Middle School, but has now been converted into a dual-faith secondary school and sixth form.\nSince September 2011 five new secondary schools, with an age range of 11 to 18 years, replaced the island's high schools (as a part of the previous three-tier system).\nNotable people.\nNotable residents have included:\n19th century.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n20th century onwards.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOverseas names.\nThe Isle of Wight has given names to many parts of former colonies, most notably Isle of Wight County in Virginia founded by settlers from the island in the 17th century. Its county seat is a town named Isle of Wight.\nOther notable examples include:\nCultural references.\nNovels.\nJulian Barnes' novel \"England, England\" broaches the idea of replicating England in a theme park on the Isle of Wight.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15104", "revid": "411305", "url": "https://en.wikipedia.org/wiki?curid=15104", "title": "Irresistible Force (production identity)", "text": ""}
{"id": "15107", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=15107", "title": "Internet Control Message Protocol", "text": "Internet protocol used for error messages in network operations\nThe Internet Control Message Protocol (ICMP) is a supporting protocol in the Internet protocol suite. It is used by network devices, including routers, to send error messages and operational information indicating success or failure when communicating with another IP address. For example, an error is indicated when a requested service is not available or that a host or router could not be reached. ICMP differs from transport protocols such as TCP and UDP in that it is not typically used to exchange data between systems, nor is it regularly employed by end-user network applications (with the exception of some diagnostic tools like ping and traceroute).\nA separate Internet Control Message Protocol (called ICMPv6) is used with IPv6.\nTechnical details.\nICMP is part of the Internet protocol suite as defined in RFC 792. ICMP messages are typically used for diagnostic or control purposes or generated in response to errors in IP operations (as specified in RFC 1122). ICMP errors are directed to the source IP address of the originating packet.\nFor example, every device (such as an intermediate router) forwarding an IP datagram first decrements the time to live (TTL) field in the IP header by one. If the resulting TTL is 0, the packet is discarded and an ICMP \"time exceeded\" message is sent to the datagram's source address.\nMany commonly used network utilities are based on ICMP messages. The traceroute command can be implemented by transmitting IP datagrams with specially set IP TTL header fields, and looking for ICMP time exceeded in transit and \"destination unreachable\" messages generated in response. The related ping utility is implemented using the ICMP \"echo request\" and \"echo reply\" messages.\nICMP uses the basic support of IP as if it were a higher-level protocol, however, ICMP is actually an integral part of IP. Although ICMP messages are contained within standard IP packets, ICMP messages are usually processed as a special case, distinguished from normal IP processing. In many cases, it is necessary to inspect the contents of the ICMP message and deliver the appropriate error message to the application responsible for transmitting the IP packet that prompted the ICMP message to be sent.\nICMP is a network-layer protocol; this makes it a layer 3 protocol in the seven-layer OSI model. Based on the four-layer TCP/IP model, ICMP is an internet-layer protocol, which makes it a layer 2 protocol in the Internet Standard RFC 1122 TCP/IP four-layer model or a layer 3 protocol in the modern five-layer TCP/IP protocol definitions (by Kozierok, Comer, Tanenbaum, Forouzan, Kurose, Stallings).\nThere is no port number associated with an ICMP packet, as these numbers are associated with protocols in the transport layer above, such as TCP and UDP.\nDatagram structure.\nThe ICMP packet is encapsulated in an IPv4 packet. The packet consists of header and data sections.\nHeader.\nThe ICMP header starts after the IPv4 header and is identified by its protocol number, \"1\". All ICMP packets have an eight-byte header and variable-sized data section. The first four bytes of the header have fixed format, while the last four bytes depend on the type and code of the ICMP packet.\nExtensions.\nICMP messages can be extended with extra information.\nThis information is carried in one or more Extension Objects, which are preceded by an ICMP Extension Header.\n!style=\"min-width:42px; border-bottom:none; border-right:none;\"|\"Offset\"\n!style=\"border-left:none;\"|Octet\n!colspan=\"8\"|0\n!colspan=\"8\"|1\n!colspan=\"8\"|2\n!colspan=\"8\"|3\n!style=\"min-width: 42px;border-top: none;\"|Octet\n!style=\"min-width: 42px;\"|Bit\n!style=\"min-width:11px;\"|0\n!style=\"min-width:11px;\"|1\n!style=\"min-width:11px;\"|2\n!style=\"min-width:11px;\"|3\n!style=\"min-width:11px;\"|4\n!style=\"min-width:11px;\"|5\n!style=\"min-width:11px;\"|6\n!style=\"min-width:11px;\"|7\n!style=\"min-width:11px;\"|8\n!style=\"min-width:11px;\"|9\n!style=\"min-width:16px;\"|10\n!style=\"min-width:16px;\"|11\n!style=\"min-width:16px;\"|12\n!style=\"min-width:16px;\"|13\n!style=\"min-width:16px;\"|14\n!style=\"min-width:16px;\"|15\n!style=\"min-width:16px;\"|16\n!style=\"min-width:16px;\"|17\n!style=\"min-width:16px;\"|18\n!style=\"min-width:16px;\"|19\n!style=\"min-width:16px;\"|20\n!style=\"min-width:16px;\"|21\n!style=\"min-width:16px;\"|22\n!style=\"min-width:16px;\"|23\n!style=\"min-width:16px;\"|24\n!style=\"min-width:16px;\"|25\n!style=\"min-width:16px;\"|26\n!style=\"min-width:16px;\"|27\n!style=\"min-width:16px;\"|28\n!style=\"min-width:16px;\"|29\n!style=\"min-width:16px;\"|30\n!style=\"min-width:16px;\"|31\n!style=\"width:35px;\"|0\n!style=\"width:30px;\"|0\n!style=\"width:35px;\"|4\n!style=\"width:30px;\"|32\nExtension objects have the following general structure:\n!style=\"min-width:42px; border-bottom:none; border-right:none;\"|\"Offset\"\n!style=\"border-left:none;\"|Octet\n!colspan=\"8\"|0\n!colspan=\"8\"|1\n!colspan=\"8\"|2\n!colspan=\"8\"|3\n!style=\"min-width: 42px;border-top: none;\"|Octet\n!style=\"min-width: 42px;\"|Bit\n!style=\"min-width:11px;\"|0\n!style=\"min-width:11px;\"|1\n!style=\"min-width:11px;\"|2\n!style=\"min-width:11px;\"|3\n!style=\"min-width:11px;\"|4\n!style=\"min-width:11px;\"|5\n!style=\"min-width:11px;\"|6\n!style=\"min-width:11px;\"|7\n!style=\"min-width:11px;\"|8\n!style=\"min-width:11px;\"|9\n!style=\"min-width:16px;\"|10\n!style=\"min-width:16px;\"|11\n!style=\"min-width:16px;\"|12\n!style=\"min-width:16px;\"|13\n!style=\"min-width:16px;\"|14\n!style=\"min-width:16px;\"|15\n!style=\"min-width:16px;\"|16\n!style=\"min-width:16px;\"|17\n!style=\"min-width:16px;\"|18\n!style=\"min-width:16px;\"|19\n!style=\"min-width:16px;\"|20\n!style=\"min-width:16px;\"|21\n!style=\"min-width:16px;\"|22\n!style=\"min-width:16px;\"|23\n!style=\"min-width:16px;\"|24\n!style=\"min-width:16px;\"|25\n!style=\"min-width:16px;\"|26\n!style=\"min-width:16px;\"|27\n!style=\"min-width:16px;\"|28\n!style=\"min-width:16px;\"|29\n!style=\"min-width:16px;\"|30\n!style=\"min-width:16px;\"|31\n!style=\"width:35px;\"|0\n!style=\"width:30px;\"|0\n!style=\"width:35px;\"|4\n!style=\"width:30px;\"|32\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15108", "revid": "23675406", "url": "https://en.wikipedia.org/wiki?curid=15108", "title": "ICMP (disambiguation)", "text": "ICMP is the Internet Control Message Protocol, used in computer networking. It may also refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15109", "revid": "37022606", "url": "https://en.wikipedia.org/wiki?curid=15109", "title": "Inverse limit", "text": "Construction in category theory\nIn mathematics, the inverse limit (also called the projective limit) is a construction that allows one to \"glue together\" several related objects, the precise gluing process being specified by morphisms between the objects. Thus, inverse limits can be defined in any category although their existence depends on the category that is considered. They are a special case of the concept of limit in category theory. \nBy working in the dual category, that is by reversing the arrows, an inverse limit becomes a direct limit or \"inductive limit\", and a \"limit\" becomes a colimit.\nFormal definition.\nAlgebraic objects.\nWe start with the definition of an inverse system (or projective system) of groups and homomorphisms. Let formula_1 be a directed poset (not all authors require \"I\" to be directed). Let (\"A\"\"i\")\"i\"\u2208\"I\" be a family of groups and suppose we have a family of homomorphisms formula_2 for all formula_3 (note the order) with the following properties:\nThen the pair formula_7 is called an \"inverse system\" of groups and morphisms over formula_8, and the morphisms formula_9 are called the transition morphisms of the system.\nThe inverse limit of the inverse system formula_7 is the subgroup of the direct product of the &amp;NoBreak;&amp;NoBreak;'s defined as\nformula_11\nThe definition above of an inverse system implies, that formula_12 is closed under pointwise multiplication, and therefore a group, since\nformula_13\nfor all &amp;NoBreak;&amp;NoBreak; and every &amp;NoBreak;&amp;NoBreak;\nThe inverse limit formula_12 comes equipped with \"natural projections\" which pick out the \"i\"th component of the direct product for each formula_15 in formula_8. The inverse limit and the natural projections satisfy a universal property described in the next section.\nThis same construction may be carried out if the formula_5's are sets, semigroups, topological spaces, rings, modules (over a fixed ring), algebras (over a fixed ring), etc., and the homomorphisms are morphisms in the corresponding category. The inverse limit will also belong to that category. More generally, this construction applies when the &amp;NoBreak;&amp;NoBreak; belong to a variety in the sense of universal algebra, that is, a type of algebraic structures, whose axioms are unconditional (fields do not form an algebra, since zero does not have a multiplicative inverse).\nGeneral definition.\nThe inverse limit can be defined abstractly in an arbitrary category by means of a universal property. Let formula_18 be an inverse system of objects and morphisms in a category \"C\" (same definition as above). The inverse limit of this system is an object \"X\" in \"C\" together with morphisms \u03c0\"i\": \"X\" \u2192 \"X\"\"i\" (called \"projections\") satisfying \u03c0\"i\" = formula_9 \u2218 \u03c0\"j\" for all \"i\" \u2264 \"j\". The pair (\"X\", \u03c0\"i\") must be universal in the sense that for any other such pair (\"Y\", \u03c8\"i\") there exists a unique morphism \"u\": \"Y\" \u2192 \"X\" such that the diagram\ncommutes for all \"i\" \u2264 \"j\". The inverse limit is often denoted\nformula_20\nwith the inverse system formula_21 and the canonical projections formula_22 being understood.\nIn some categories, the inverse limit of certain inverse systems does not exist. If it does, however, it is unique in a strong sense: given any two inverse limits \"X\" and \"X\"' of an inverse system, there exists a \"unique\" isomorphism \"X\"\u2032 \u2192 \"X\" commuting with the projection maps.\nInverse systems and inverse limits in a category \"C\" admit an alternative description in terms of functors. Any partially ordered set \"I\" can be considered as a small category where the morphisms consist of arrows \"i\" \u2192 \"j\" if and only if \"i\" \u2264 \"j\". An inverse system is then just a contravariant functor \"I\" \u2192 \"C\". Let formula_23 be the category of these functors (with natural transformations as morphisms). An object \"X\" of \"C\" can be considered a trivial inverse system, where all objects are equal to \"X\" and all arrow are the identity of \"X\". This defines a \"trivial functor\" from \"C\" to formula_24 The inverse limit, if it exists, is defined as a right adjoint of this trivial functor.\nDerived functors of the inverse limit.\nFor an abelian category \"C\", the inverse limit functor\nformula_41\nis left exact. If \"I\" is ordered (not simply partially ordered) and countable, and \"C\" is the category Ab of abelian groups, the Mittag-Leffler condition is a condition on the transition morphisms \"f\"\"ij\" that ensures the exactness of formula_42. Specifically, Eilenberg constructed a functor\nformula_43\n(pronounced \"lim one\") such that if (\"A\"\"i\", \"f\"\"ij\"), (\"B\"\"i\", \"g\"\"ij\"), and (\"C\"\"i\", \"h\"\"ij\") are three inverse systems of abelian groups, and\nformula_44\nis a short exact sequence of inverse systems, then\nformula_45\nis an exact sequence in Ab.\nMittag-Leffler condition.\nIf the ranges of the morphisms of an inverse system of abelian groups (\"A\"\"i\", \"f\"\"ij\") are \"stationary\", that is, for every \"k\" there exists \"j\" \u2265 \"k\" such that for all \"i\" \u2265 \"j\" :formula_46 one says that the system satisfies the Mittag-Leffler condition.\nThe name \"Mittag-Leffler\" for this condition was given by Bourbaki in their chapter on uniform structures for a similar result about inverse limits of complete Hausdorff uniform spaces. Mittag-Leffler used a similar argument in the proof of Mittag-Leffler's theorem.\nThe following situations are examples where the Mittag-Leffler condition is satisfied: \nAn example where formula_47 is non-zero is obtained by taking \"I\" to be the non-negative integers, letting \"A\"\"i\" = \"p\"\"i\"Z, \"B\"\"i\" = Z, and \"C\"\"i\" = \"B\"\"i\" / \"A\"\"i\" = Z/\"p\"\"i\"Z. Then\nformula_48\nwhere Z\"p\" denotes the p-adic integers.\nFurther results.\nMore generally, if \"C\" is an arbitrary abelian category that has enough injectives, then so does \"C\"\"I\", and the right derived functors of the inverse limit functor can thus be defined. The \"n\"th right derived functor is denoted\nformula_49\nIn the case where \"C\" satisfies Grothendieck's axiom (AB4*), Jan-Erik Roos generalized the functor lim1 on Ab\"I\" to series of functors limn such that\nformula_50\nIt was thought for almost 40 years that Roos had proved (in ) that lim1 \"A\"\"i\" = 0 for (\"A\"\"i\", \"f\"\"ij\") an inverse system with surjective transition morphisms and \"I\" the set of non-negative integers (such inverse systems are often called \"Mittag-Leffler sequences\"). However, in 2002, Amnon Neeman and Pierre Deligne constructed an example of such a system in a category satisfying (AB4) (in addition to (AB4*)) with lim1 \"A\"\"i\" \u2260 0. Roos has since shown (in \"Derived functors of inverse limits revisited\") that his result is correct if \"C\" has a set of generators (in addition to satisfying (AB3) and (AB4*)).\nBarry Mitchell has shown (in \"The cohomological dimension of a directed set\") that if \"I\" has cardinality formula_51 (the \"d\"th infinite cardinal), then \"R\"\"n\"lim is zero for all \"n\" \u2265 \"d\" + 2. This applies to the \"I\"-indexed diagrams in the category of \"R\"-modules, with \"R\" a commutative ring; it is not necessarily true in an arbitrary abelian category (see Roos' \"Derived functors of inverse limits revisited\" for examples of abelian categories in which lim\"n\", on diagrams indexed by a countable set, is nonzero for\u00a0\"n\"\u00a0&gt;\u00a01).\nRelated concepts and generalizations.\nThe categorical dual of an inverse limit is a direct limit (or inductive limit). More general concepts are the limits and colimits of category theory. The terminology is somewhat confusing: inverse limits are a class of limits, while direct limits are a class of colimits."}
{"id": "15111", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=15111", "title": "Interplanetary spaceflight", "text": "Crewed or uncrewed travel between stars or planets\nInterplanetary spaceflight or interplanetary travel is spaceflight (crewed or uncrewed) between bodies within a single planetary system. Spaceflights become interplanetary by accelerating spacecrafts beyond orbital speed, reaching escape velocity relative to Earth at 11.2 km/s, entering heliocentric orbit, possibly accelerating further, often by performing gravity assist flybys at Earth and other planets. Most of today's spaceflight remains Earth bound, with much less being interplanetary, all of which performed by uncrewed spacecrafts, and only just a few spaceflights having accelerated beyond, to system escape velocity, eventually performing interstellar spaceflight.\nUncrewed space probes have flown to all the observed planets in the Solar System as well as to dwarf planets Pluto and Ceres, and several asteroids. Orbiters and landers return more information than fly-by missions. Crewed flights have landed on the Moon and have been planned, from time to time, for Mars, Venus and Mercury. While many scientists appreciate the knowledge value that uncrewed flights provide, the value of crewed missions is more controversial. Science fiction writers propose a number of benefits, including the mining of asteroids, access to solar power, and room for colonization in the event of an Earth catastrophe.\nA number of techniques have been developed to make interplanetary flights more economical. Advances in computing and theoretical science have already improved some techniques, while new proposals may lead to improvements in speed, fuel economy, and safety. Travel techniques must take into consideration the velocity changes necessary to travel from one body to another in the Solar System. For orbital flights, an additional adjustment must be made to match the orbital speed of the destination body. Other developments are designed to improve rocket launching and propulsion, as well as the use of non-traditional sources of energy. Using extraterrestrial resources for energy, oxygen, and water would reduce costs and improve life support systems.\nAny crewed interplanetary flight must include certain design requirements. Life support systems must be capable of supporting human lives for extended periods of time. Preventative measures are needed to reduce exposure to radiation and ensure optimum reliability.\nCurrent achievements in interplanetary travel.\nRemotely guided space probes have flown by all of the observed planets of the Solar System from Mercury to Neptune, with the \"New Horizons\" probe having flown by the dwarf planet Pluto and the \"Dawn\" spacecraft currently orbiting the dwarf planet Ceres. The most distant spacecraft, \"Voyager 1\" and \"Voyager 2\" have left the Solar System as of 8 December 2018 while \"Pioneer 10\", \"Pioneer 11\", and \"New Horizons\" are on course to leave it.\nIn general, planetary orbiters and landers return much more detailed and comprehensive information than fly-by missions. Space probes have been placed into orbit around all the five planets known to the ancients: The first being Venus (Venera 7, 1970), Mars (Mariner 9, 1971), Jupiter (\"Galileo\", 1995), Saturn (\"Cassini/Huygens\", 2004), and most recently Mercury (\"MESSENGER\", March 2011), and have returned data about these bodies and their natural satellites.\nThe NEAR Shoemaker mission in 2000 orbited the large near-Earth asteroid 433 Eros, and was even successfully landed there, though it had not been designed with this maneuver in mind. The Japanese ion-drive spacecraft \"Hayabusa\" in 2005 also orbited the small near-Earth asteroid 25143 Itokawa, landing on it briefly and returning grains of its surface material to Earth. Another ion-drive mission, \"Dawn\", has orbited the large asteroid Vesta (July 2011 \u2013 September 2012) and later moved on to the dwarf planet Ceres, arriving in March 2015.\nRemotely controlled landers such as Viking, Pathfinder and the two Mars Exploration Rovers have landed on the surface of Mars and several Venera and Vega spacecraft have landed on the surface of Venus, with the latter deploying balloons to the planet's atmosphere. The \"Huygens\" probe successfully landed on Saturn's moon, Titan.\nNo crewed missions have been sent to any planet of the Solar System. NASA's Apollo program, however, landed twelve people on the Moon and returned them to Earth. The American Vision for Space Exploration, originally introduced by U.S. President George W. Bush and put into practice through the Constellation program, had as a long-term goal to eventually send human astronauts to Mars. However, on February 1, 2010, President Barack Obama proposed cancelling the program in Fiscal Year 2011. An earlier project which received some significant planning by NASA included a crewed fly-by of Venus in the Manned Venus Flyby mission, but was cancelled when the Apollo Applications Program was terminated due to NASA budget cuts in the late 1960s.\nReasons for interplanetary travel.\nThe costs and risk of interplanetary travel receive a lot of publicity\u2014spectacular examples include the malfunctions or complete failures of probes without a human crew, such as Mars 96, Deep Space 2, and Beagle 2 (the article List of Solar System probes gives a full list).\nMany astronomers, geologists and biologists believe that exploration of the Solar System provides knowledge that could not be gained by observations from Earth's surface or from orbit around Earth. However, they disagree about whether human-crewed missions justify their cost and risk. Critics of human spaceflight argue that robotic probes are more cost-effective, producing more scientific knowledge per dollar spent; robots do not need costly life-support systems, can be sent on one-way missions, and are becoming more capable as artificial intelligence advances. Others argue that either astronauts or spacefaring scientists, advised by Earth-based scientists, can respond more flexibly and intelligently to new or unexpected features of whatever region they are exploring. \nSome members of the general public mainly value space activities for whatever tangible benefits they may deliver to themselves or to the human race as a whole. So far the only benefits of this type have been \"spin-off\" technologies which were developed for space missions and then were found to be at least as useful in other activities. However, public support, at least in the US, remains higher for basic scientific research than for human space flight; a 2023 survey found that Americans rate basic research as their third-highest priority for NASA, after monitoring Earth-endangering asteroids and understanding climate change. Support for scientific research is about four times higher than for human flight to the Moon or Mars.\nBesides spinoffs, other practical motivations for interplanetary travel are more speculative. But science fiction writers have a fairly good track record in predicting future technologies\u2014for example geosynchronous communications satellites (Arthur C. Clarke) and many aspects of computer technology (Mack Reynolds).\nMany science fiction stories feature detailed descriptions of how people could extract minerals from asteroids and energy from sources including orbital solar panels (unhampered by clouds) and the very strong magnetic field of Jupiter. Some claim that such techniques may be the only way to provide rising standards of living without being stopped by pollution or by depletion of Earth's resources (for example peak oil).\nThere are also non-scientific motives for human spaceflight, such as adventure or the belief that humans have a spiritually fated destiny in space.\nFinally, establishing completely self-sufficient colonies in other parts of the Solar System could, if feasible, prevent the human species from being exterminated by several possible events (see Human extinction). One of these possible events is an asteroid impact like the one which may have resulted in the Cretaceous\u2013Paleogene extinction event. Although various Spaceguard projects monitor the Solar System for objects that might come dangerously close to Earth, current asteroid deflection strategies are crude and untested. To make the task more difficult, carbonaceous chondrites are rather sooty and therefore very hard to detect. Although carbonaceous chondrites are thought to be rare, some are very large and the suspected \"dinosaur-killer\" may have been a carbonaceous chondrite.\nSome scientists, including members of the Space Studies Institute, argue that the vast majority of mankind eventually will live in space and will benefit from doing so.\nEconomical travel techniques.\nOne of the main challenges in interplanetary travel is producing the very large velocity changes necessary to travel from one body to another in the Solar System.\nDue to the Sun's gravitational pull, a spacecraft moving farther from the Sun will slow down, while a spacecraft moving closer will speed up. Also, since any two planets are at different distances from the Sun, the planet from which the spacecraft starts is moving around the Sun at a different speed than the planet to which the spacecraft is travelling (in accordance with Kepler's Third Law). Because of these facts, a spacecraft desiring to transfer to a planet closer to the Sun must decrease its speed with respect to the Sun by a large amount in order to intercept it, while a spacecraft traveling to a planet farther out from the Sun must increase its speed substantially. Then, if additionally the spacecraft wishes to enter into orbit around the destination planet (instead of just flying by it), it must match the planet's orbital speed around the Sun, usually requiring another large velocity change.\nSimply doing this by brute force \u2013 accelerating in the shortest route to the destination and then matching the planet's speed \u2013 would require an extremely large amount of fuel. And the fuel required for producing these velocity changes has to be launched along with the payload, and therefore even more fuel is needed to put both the spacecraft and the fuel required for its interplanetary journey into orbit. Thus, several techniques have been devised to reduce the fuel requirements of interplanetary travel.\nAs an example of the velocity changes involved, a spacecraft travelling from low Earth orbit to Mars using a simple trajectory must first undergo a change in speed (also known as a delta-v), in this case an increase, of about 3.8\u00a0km/s. Then, after intercepting Mars, it must change its speed by another 2.3\u00a0km/s in order to match Mars' orbital speed around the Sun and enter an orbit around it. For comparison, launching a spacecraft into low Earth orbit requires a change in speed of about 9.5\u00a0km/s.\nHohmann transfers.\nFor many years economical interplanetary travel meant using the Hohmann transfer orbit. Hohmann demonstrated that the lowest energy route between any two orbits is an elliptical \"orbit\" which forms a tangent to the starting and destination orbits. Once the spacecraft arrives, a second application of thrust will re-circularize the orbit at the new location. In the case of planetary transfers this means directing the spacecraft, originally in an orbit almost identical to Earth's, so that the aphelion of the transfer orbit is on the far side of the Sun near the orbit of the other planet. A spacecraft traveling from Earth to Mars via this method will arrive near Mars orbit in approximately 8.5 months, but because the orbital velocity is greater when closer to the center of mass (i.e. the Sun) and slower when farther from the center, the spacecraft will be traveling quite slowly and a small application of thrust is all that is needed to put it into a circular orbit around Mars. If the manoeuver is timed properly, Mars will be \"arriving\" under the spacecraft when this happens.\nThe Hohmann transfer applies to any two orbits, not just those with planets involved. For instance it is the most common way to transfer satellites into geostationary orbit, after first being \"parked\" in low Earth orbit. However, the Hohmann transfer takes an amount of time similar to \u00bd of the orbital period of the outer orbit, so in the case of the outer planets this is many years \u2013 too long to wait. It is also based on the assumption that the points at both ends are massless, as in the case when transferring between two orbits around Earth for instance. With a planet at the destination end of the transfer, calculations become considerably more difficult.\nGravitational slingshot.\nThe gravitational slingshot technique uses the gravity of planets and moons to change the speed and direction of a spacecraft without using fuel. In typical example, a spacecraft is sent to a distant planet on a path that is much faster than what the Hohmann transfer would call for. This would typically mean that it would arrive at the planet's orbit and continue past it. However, if there is a planet between the departure point and the target, it can be used to bend the path toward the target, and in many cases the overall travel time is greatly reduced. A prime example of this are the two crafts of the Voyager program, which used slingshot effects to change trajectories several times in the outer Solar System. It is difficult to use this method for journeys in the inner part of the Solar System, although it is possible to use other nearby planets such as Venus or even the Moon as slingshots in journeys to the outer planets.\nThis maneuver can only change an object's velocity relative to a third, uninvolved object, \u2013 possibly the \"centre of mass\" or the Sun. There is no change in the velocities of the two objects involved in the maneuver relative to each other. The Sun cannot be used in a gravitational slingshot because it is stationary compared to rest of the Solar System, which orbits the Sun. It may be used to send a spaceship or probe into the galaxy because the Sun revolves around the center of the Milky Way.\nPowered slingshot.\nA powered slingshot is the use of a rocket engine at or around closest approach to a body (periapsis). The use at this point multiplies up the effect of the delta-v, and gives a bigger effect than at other times.\nFuzzy orbits.\nComputers did not exist when Hohmann transfer orbits were first proposed (1925) and were slow, expensive and unreliable when gravitational slingshots were developed (1959). Recent advances in computing have made it possible to exploit many more features of the gravity fields of astronomical bodies and thus calculate even lower-cost trajectories. Paths have been calculated which link the Lagrange points of the various planets into the so-called Interplanetary Transport Network. Such \"fuzzy orbits\" use significantly less energy than Hohmann transfers but are much, much slower. They aren't practical for human crewed missions because they generally take years or decades, but may be useful for high-volume transport of low-value commodities if humanity develops a space-based economy.\nAerobraking.\nAerobraking uses the atmosphere of the target planet to slow down. It was first used on the Apollo program where the returning spacecraft did not enter Earth orbit but instead used a S-shaped vertical descent profile (starting with an initially steep descent, followed by a leveling out, followed by a slight climb, followed by a return to a positive rate of descent continuing to splash-down in the ocean) through Earth's atmosphere to reduce its speed until the parachute system could be deployed enabling a safe landing. Aerobraking does not require a thick atmosphere \u2013 for example most Mars landers use the technique, and Mars' atmosphere is only about 1% as thick as Earth's.\nAerobraking converts the spacecraft's kinetic energy into heat, so it requires a heatshield to prevent the craft from burning up. As a result, aerobraking is only helpful in cases where the fuel needed to transport the heatshield to the planet is less than the fuel that would be required to brake an unshielded craft by firing its engines. This can be addressed by creating heatshields from material available near the target.\nImproved technologies and methodologies.\nSeveral technologies have been proposed which both save fuel and provide significantly faster travel than the traditional methodology of using Hohmann transfers. Some are still just theoretical, but over time, several of the theoretical approaches have been tested on spaceflight missions. For example, the Deep Space 1 mission was a successful test of an ion drive. These improved technologies typically focus on one or more of:\nBesides making travel faster or cost less, such improvements could also allow greater design \"safety margins\" by reducing the imperative to make spacecraft lighter.\nImproved rocket concepts.\nAll rocket concepts are limited by the Tsiolkovsky rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, of initial (\"M\"0, including fuel) to final (\"M\"1, fuel depleted) mass. The main consequence is that mission velocities of more than a few times the velocity of the rocket motor exhaust (with respect to the vehicle) rapidly become impractical, as the dry mass (mass of payload and rocket without fuel) falls to below 10% of the entire rocket's wet mass (mass of rocket with fuel).\nNuclear thermal and solar thermal rockets.\nIn a nuclear thermal rocket or solar thermal rocket a working fluid, usually hydrogen, is heated to a high temperature, and then expands through a rocket nozzle to create thrust. The energy replaces the chemical energy of the reactive chemicals in a traditional rocket engine. Due to the low molecular mass and hence high thermal velocity of hydrogen these engines are at least twice as fuel efficient as chemical engines, even after including the weight of the reactor.\nThe US Atomic Energy Commission and NASA tested a few designs from 1959 to 1968. The NASA designs were conceived as replacements for the upper stages of the Saturn V launch vehicle, but the tests revealed reliability problems, mainly caused by the vibration and heating involved in running the engines at such high thrust levels. Political and environmental considerations make it unlikely such an engine will be used in the foreseeable future, since nuclear thermal rockets would be most useful at or near the Earth's surface and the consequences of a malfunction could be disastrous. Fission-based thermal rocket concepts produce lower exhaust velocities than the electric and plasma concepts described below, and are therefore less attractive solutions. For applications requiring high thrust-to-weight ratio, such as planetary escape, nuclear thermal is potentially more attractive.\nElectric propulsion.\nElectric propulsion systems use an external source such as a nuclear reactor or solar cells to generate electricity, which is then used to accelerate a chemically inert propellant to speeds far higher than achieved in a chemical rocket. Such drives produce feeble thrust, and are therefore unsuitable for quick maneuvers or for launching from the surface of a planet. But they are so economical in their use of\nworking mass that they can keep firing continuously for days or weeks, while chemical rockets use up reaction mass so quickly that they can only fire for seconds or minutes. Even a trip to the Moon is long enough for an electric propulsion system to outrun a chemical rocket \u2013 the Apollo missions took 3 days in each direction.\nNASA's Deep Space One was a very successful test of a prototype ion drive, which fired for a total of 678 days and enabled the probe to run down Comet Borrelly, a feat which would have been impossible for a chemical rocket. \"Dawn\", the first NASA operational (i.e., non-technology demonstration) mission to use an ion drive for its primary propulsion, successfully orbited the large main-belt asteroids 1 Ceres and 4 Vesta. A more ambitious, nuclear-powered version was intended for a Jupiter mission without human crew, the Jupiter Icy Moons Orbiter (JIMO), originally planned for launch sometime in the next decade. Due to a shift in priorities at NASA that favored human crewed space missions, the project lost funding in 2005. A similar mission is currently under discussion as the US component of a joint NASA/ESA program for the exploration of Europa and Ganymede.\nA NASA multi-center Technology Applications Assessment Team led from the Johnson Spaceflight Center, has as of January 2011 described \"Nautilus-X\", a concept study for a multi-mission space exploration vehicle useful for missions beyond low Earth orbit (LEO), of up to 24 months duration for a crew of up to six. Although Nautilus-X is adaptable to a variety of mission-specific propulsion units of various low-thrust, high specific impulse (Isp) designs, nuclear ion-electric drive is shown for illustrative purposes. It is intended for integration and checkout at the International Space Station (ISS), and would be suitable for deep-space missions from the ISS to and beyond the Moon, including Earth/Moon L1, Sun/Earth L2, near-Earth asteroidal, and Mars orbital destinations. It incorporates a reduced-g centrifuge providing artificial gravity for crew health to ameliorate the effects of long-term 0g exposure, and the capability to mitigate the space radiation environment.\nFission powered rockets.\nThe electric propulsion missions already flown, or currently scheduled, have used solar electric power, limiting their capability to operate far from the Sun, and also limiting their peak acceleration due to the mass of the electric power source. Nuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, can reach speeds much greater than chemically powered vehicles.\nFusion rockets.\nFusion rockets, powered by nuclear fusion reactions, would \"burn\" such light element fuels as deuterium, tritium, or 3He. Because fusion yields about 1% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases only about 0.1% of the fuel's mass-energy. However, either fission or fusion technologies can in principle achieve velocities far higher than needed for Solar System exploration, and fusion energy still awaits practical demonstration on Earth.\nOne proposal using a fusion rocket was Project Daedalus. Another fairly detailed vehicle system, designed and optimized for crewed Solar System exploration, \"Discovery II\", based on the D3He reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of &gt;300\u00a0km/s with an acceleration of ~1.7\u202210\u22123 \"g\", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%.\nFusion rockets are considered to be a likely source of interplanetary transport for a planetary civilization.\nExotic propulsion.\nSee the spacecraft propulsion article for a discussion of a number of other technologies that could, in the medium to longer term, be the basis of interplanetary missions. Unlike the situation with interstellar travel, the barriers to fast interplanetary travel involve engineering and economics rather than any basic physics.\nSolar sails.\nSolar sails rely on the fact that light reflected from a surface exerts pressure on the surface. The radiation pressure is small and decreases by the square of the distance from the Sun, but unlike rockets, solar sails require no fuel. Although the thrust is small, it continues as long as the Sun shines and the sail is deployed.\nThe original concept relied only on radiation from the Sun \u2013 for example in Arthur C. Clarke's 1965 story \"Sunjammer\". More recent light sail designs propose to boost the thrust by aiming ground-based lasers or masers at the sail. Ground-based lasers or masers can also help a light-sail spacecraft to \"decelerate\": the sail splits into an outer and inner section, the outer section is pushed forward and its shape is changed mechanically to focus reflected radiation on the inner portion, and the radiation focused on the inner section acts as a brake.\nAlthough most articles about light sails focus on interstellar travel, there have been several proposals for their use within the Solar System.\nCurrently, the only spacecraft to use a solar sail as the main method of propulsion is IKAROS which was launched by JAXA on May 21, 2010. It has since been successfully deployed, and shown to be producing acceleration as expected. Many ordinary spacecraft and satellites also use solar collectors, temperature-control panels and Sun shades as light sails, to make minor corrections to their attitude and orbit without using fuel. A few have even had small purpose-built solar sails for this use (for example Eurostar E3000 geostationary communications satellites built by EADS Astrium).\nCyclers.\nIt is possible to put stations or spacecraft on orbits that cycle between different planets, for example a Mars cycler would synchronously cycle between Mars and Earth, with very little propellant usage to maintain the trajectory. Cyclers are conceptually a good idea, because massive radiation shields, life support and other equipment only need to be put onto the cycler trajectory once. A cycler could combine several roles: habitat (for example it could spin to produce an \"artificial gravity\" effect), or a mothership (providing life support for the crews of smaller spacecraft which hitch a ride on it). Cyclers could also possibly make excellent cargo ships for resupply of a colony.\nSpace elevator.\nA space elevator is a theoretical structure that would transport material from a planet's surface into orbit. The idea is that, once the expensive job of building the elevator is complete, an indefinite number of loads can be transported into orbit at minimal cost. Even the simplest designs avoid the vicious circle of rocket launches from the surface, wherein the fuel needed to travel the last 10% of the distance into orbit must be lifted all the way from the surface, requiring even more fuel, and so on. More sophisticated space elevator designs reduce the energy cost per trip by using counterweights, and the most ambitious schemes aim to balance loads going up and down and thus make the energy cost close to zero. Space elevators have also sometimes been referred to as \"beanstalks\", \"space bridges\", \"space lifts\", \"space ladders\" and \"orbital towers\".\nA terrestrial space elevator is beyond our current technology, although a lunar space elevator could theoretically be built using existing materials.\nSkyhook.\nA skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit. In addition, it has been suggested that the rotating skyhook is \"not engineeringly feasible using presently available materials\".\nLaunch vehicle and spacecraft reusability.\nThe SpaceX Starship is designed to be fully and rapidly reusable, making use of the SpaceX reusable technology that was developed during 2011\u20132018 for Falcon 9 and Falcon Heavy launch vehicles.\nSpaceX CEO Elon Musk estimates that the reusability capability alone, on both the launch vehicle and the spacecraft associated with the Starship will reduce overall system costs per tonne delivered to Mars by at least two orders of magnitude over what NASA had previously achieved.\nStaging propellants.\nWhen launching interplanetary probes from the surface of Earth, carrying all energy needed for the long-duration mission, payload quantities are necessarily extremely limited, due to the basis mass limitations described theoretically by the rocket equation. One alternative to transport more mass on interplanetary trajectories is to use up nearly all of the upper stage propellant on launch, and then refill propellants in Earth orbit before firing the rocket to escape velocity for a heliocentric trajectory. These propellants could be stored on orbit at a propellant depot, or carried to orbit in a propellant tanker to be directly transferred to the interplanetary spacecraft. For returning mass to Earth, a related option is to mine raw materials from a solar system celestial object, refine, process, and store the reaction products (propellant) on the Solar System body until such time as a vehicle needs to be loaded for launch.\nOn-orbit tanker transfers.\nAs of 2019, SpaceX is developing a system in which a reusable first stage vehicle would transport a crewed interplanetary spacecraft to Earth orbit, detach, return to its launch pad where a tanker spacecraft would be mounted atop it, then both fueled, then launched again to rendezvous with the waiting crewed spacecraft. The tanker would then transfer its fuel to the human crewed spacecraft for use on its interplanetary voyage. The SpaceX Starship is a stainless steel-structure spacecraft propelled by six Raptor engines operating on densified methane/oxygen propellants. It is -long, -diameter at its widest point, and is capable of transporting up to of cargo and passengers per trip to Mars, with on-orbit propellant refill before the interplanetary part of the journey.\nPropellant plant on a celestial body.\nAs an example of a funded project currently under development, a key part of the system SpaceX has designed for Mars in order to radically decrease the cost of spaceflight to interplanetary destinations is the placement and operation of a physical plant on Mars to handle production and storage of the propellant components necessary to launch and fly the Starships back to Earth, or perhaps to increase the mass that can be transported onward to destinations in the outer Solar System.\nThe first Starship to Mars will carry a small propellant plant as a part of its cargo load. The plant will be expanded over multiple synods as more equipment arrives, is installed, and placed into mostly-autonomous production.\nThe SpaceX propellant plant will take advantage of the large supplies of carbon dioxide and water resources on Mars, mining the water (H2O) from subsurface ice and collecting CO2 from the atmosphere. A chemical plant will process the raw materials by means of electrolysis and the Sabatier process to produce oxygen (O2) and methane (CH4), and then liquefy it to facilitate long-term storage and ultimate use.\nUsing extraterrestrial resources.\nCurrent space vehicles attempt to launch with all their fuel (propellants and energy supplies) on board that they will need for their entire journey, and current space structures are lifted from the Earth's surface. Non-terrestrial sources of energy and materials are mostly a lot further away, but most would not require lifting out of a strong gravity field and therefore should be much cheaper to use in space in the long term.\nThe most important non-terrestrial resource is energy, because it can be used to transform non-terrestrial materials into useful forms (some of which may also produce energy). At least two fundamental non-terrestrial energy sources have been proposed: solar-powered energy generation (unhampered by clouds), either directly by solar cells or indirectly by focusing solar radiation on boilers which produce steam to drive generators; and electrodynamic tethers which generate electricity from the powerful magnetic fields of some planets (Jupiter has a very powerful magnetic field).\nWater ice would be very useful and is widespread on the moons of Jupiter and Saturn:\nOxygen is a common constituent of the Moon's crust, and is probably abundant in most other bodies in the Solar System. Non-terrestrial oxygen would be valuable as a source of water ice only if an adequate source of hydrogen can be found. Possible uses include:\nUnfortunately hydrogen, along with other volatiles like carbon and nitrogen, are much less abundant than oxygen in the inner Solar System.\nScientists expect to find a vast range of organic compounds in some of the planets, moons and comets of the outer Solar System, and the range of possible uses is even wider. For example, methane can be used as a fuel (burned with non-terrestrial oxygen), or as a feedstock for petrochemical processes such as making plastics. And ammonia could be a valuable feedstock for producing fertilizers to be used in the vegetable gardens of orbital and planetary bases, reducing the need to lift food to them from Earth.\nEven unprocessed rock may be useful as rocket propellant if mass drivers are employed.\nDesign requirements for crewed interplanetary travel.\nLife support.\nLife support systems must be capable of supporting human life for weeks, months or even years. A breathable atmosphere of at least must be maintained, with adequate amounts of oxygen, nitrogen, and controlled levels of carbon dioxide, trace gases and water vapor.\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to human spaceflight, including a human mission to Mars.\nRadiation.\nOnce a vehicle leaves low Earth orbit and the protection of Earth's magnetosphere, it enters the Van Allen radiation belt, a region of high radiation. Beyond the Van Allen belts, radiation levels generally decrease, but can fluctuate over time. These high energy cosmic rays pose a health threat. Even the minimum levels of radiation during these fluctuations is comparable to the current annual limit for astronauts in low-Earth orbit.\nScientists of Russian Academy of Sciences are searching for methods of reducing the risk of radiation-induced cancer in preparation for the mission to Mars. They consider as one of the options a life support system generating drinking water with low content of deuterium (a stable isotope of hydrogen) to be consumed by the crew members. Preliminary investigations have shown that deuterium-depleted water features certain anti-cancer effects. Hence, deuterium-free drinking water is considered to have the potential of lowering the risk of cancer caused by extreme radiation exposure of the Martian crew.\nIn addition, coronal mass ejections from the Sun are highly dangerous, and are fatal within a very short timescale to humans unless they are protected by massive shielding.\nReliability.\nAny major failure to a spacecraft en route is likely to be fatal, and even a minor one could have dangerous results if not repaired quickly, something difficult to accomplish in open space. The crew of the Apollo 13 mission survived despite an explosion caused by a faulty oxygen tank (1970).\nLaunch windows.\nFor astrodynamics reasons, economic spacecraft travel to other planets is only practical within certain time windows. Outside these windows the planets are essentially inaccessible from Earth with current technology. This constrains flights and limits rescue options in the case of an emergency.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15112", "revid": "26919195", "url": "https://en.wikipedia.org/wiki?curid=15112", "title": "Wave interference", "text": "Phenomenon resulting from the superposition of two waves\nIn physics, interference is a phenomenon in which two coherent waves are combined by adding their intensities or displacements with due consideration for their phase difference. The resultant wave may have greater amplitude (constructive interference) or lower amplitude (destructive interference) if the two waves are in phase or out of phase, respectively.\nInterference effects can be observed with all types of waves, for example, light, radio, acoustic, surface water waves, gravity waves, or matter waves as well as in loudspeakers as electrical waves.\nEtymology.\nThe word \"interference\" is derived from the Latin words \"inter\" which means \"between\" and \"fere\" which means \"hit or strike\", and was used in the context of wave superposition by Thomas Young in 1801.\nMechanisms.\nThe principle of superposition of waves states that when two or more propagating waves of the same type are incident on the same point, the resultant amplitude at that point is equal to the vector sum of the amplitudes of the individual waves. If a crest of a wave meets a crest of another wave of the same frequency at the same point, then the amplitude is the sum of the individual amplitudes\u2014this is constructive interference. If a crest of one wave meets a trough of another wave, then the amplitude is equal to the difference in the individual amplitudes\u2014this is known as destructive interference. In ideal mediums (water, air are almost ideal) energy is always conserved, at points of destructive interference, the wave amplitudes cancel each other out, and the energy is redistributed to other areas. For example, when two pebbles are dropped in a pond, a pattern is observable; but eventually waves continue, and only when they reach the shore is the energy absorbed away from the medium.\nConstructive interference occurs when the phase difference between the waves is an even multiple of \u03c0 (180\u00b0), whereas destructive interference occurs when the difference is an odd multiple of \u03c0. If the difference between the phases is intermediate between these two extremes, then the magnitude of the displacement of the summed waves lies between the minimum and maximum values.\nConsider, for example, what happens when two identical stones are dropped into a still pool of water at different locations. Each stone generates a circular wave propagating outwards from the point where the stone was dropped. When the two waves overlap, the net displacement at a particular point is the sum of the displacements of the individual waves. At some points, these will be in phase, and will produce a maximum displacement. In other places, the waves will be in anti-phase, and there will be no net displacement at these points. Thus, parts of the surface will be stationary\u2014these are seen in the figure above and to the right as stationary blue-green lines radiating from the centre.\nInterference of light is a unique phenomenon in that we can never observe superposition of the EM field directly as we can, for example, in water. Superposition in the EM field is an assumed phenomenon and necessary to explain how two light beams pass through each other and continue on their respective paths. Prime examples of light interference are the famous double-slit experiment, laser speckle, anti-reflective coatings and interferometers. \nIn addition to the classical wave model for understanding optical interference, quantum matter waves also demonstrate interference. \nReal-valued wave functions.\nThe above can be demonstrated in one dimension by deriving the formula for the sum of two waves. The equation for the amplitude of a sinusoidal wave traveling to the right along the x-axis is\nformula_1\nwhere formula_2 is the peak amplitude, formula_3 is the wavenumber and formula_4 is the angular frequency of the wave. Suppose a second wave of the same frequency and amplitude but with a different phase is also traveling to the right\nformula_5\nwhere formula_6 is the phase difference between the waves in radians. The two waves will superpose and add: the sum of the two waves is\nformula_7\nUsing the trigonometric identity for the sum of two cosines: formula_8 this can be written\nformula_9\nThis represents a wave at the original frequency, traveling to the right like its components, whose amplitude is proportional to the cosine of formula_10.\nBetween two plane waves.\nA simple form of interference pattern is obtained if two plane waves of the same frequency intersect at an angle.\nOne wave is travelling horizontally, and the other is travelling downwards at an angle \u03b8 to the first wave. Assuming that the two waves are in phase at the point B, then the relative phase changes along the \"x\"-axis. The phase difference at the point A is given by\nformula_17\nIt can be seen that the two waves are in phase when\nformula_18\nand are half a cycle out of phase when\nformula_19\nConstructive interference occurs when the waves are in phase, and destructive interference when they are half a cycle out of phase. Thus, an interference fringe pattern is produced, where the separation of the maxima is\nformula_20\nand \"df\" is known as the fringe spacing. The fringe spacing increases with increase in wavelength, and with decreasing angle \u03b8.\nThe fringes are observed wherever the two waves overlap and the fringe spacing is uniform throughout.\nBetween two spherical waves.\nA point source produces a spherical wave. If the light from two point sources overlaps, the interference pattern maps out the way in which the phase difference between the two waves varies in space. This depends on the wavelength and on the separation of the point sources. The figure to the right shows interference between two spherical waves. The wavelength increases from top to bottom, and the distance between the sources increases from left to right.\nWhen the plane of observation is far enough away, the fringe pattern will be a series of almost straight lines, since the waves will then be almost planar.\nMultiple beams.\nInterference occurs when several waves are added together provided that the phase differences between them remain constant over the observation time.\nIt is sometimes desirable for several waves of the same frequency and amplitude to sum to zero (that is, interfere destructively, cancel). This is the principle behind, for example, 3-phase power and the diffraction grating. In both of these cases, the result is achieved by uniform spacing of the phases.\nIt is easy to see that a set of waves will cancel if they have the same amplitude and their phases are spaced equally in angle. Using phasors, each wave can be represented as formula_21 for formula_22 waves from formula_23 to formula_24, where\nformula_25\nTo show that\nformula_26\none merely assumes the converse, then multiplies both sides by formula_27\nThe Fabry\u2013P\u00e9rot interferometer uses interference between multiple reflections.\nA diffraction grating can be considered to be a multiple-beam interferometer; since the peaks which it produces are generated by interference between the light transmitted by each of the elements in the grating; see interference vs. diffraction for further discussion.\nComplex valued wave functions.\nMechanical and gravity waves can be directly observed: they are real-valued wave functions; optical and matter waves cannot be directly observed: they are complex valued wave functions. Some of the differences between real valued and complex valued wave interference include:\nOptical wave interference.\nBecause the frequency of light waves (~1014 Hz) is too high for currently available detectors to detect the variation of the electric field of the light, it is possible to observe only the intensity of an optical interference pattern. The intensity of the light at a given point is proportional to the square of the average amplitude of the wave. This can be expressed mathematically as follows. The displacement of the two waves at a point r is:\nformula_28\nformula_29\nwhere A represents the magnitude of the displacement, \u03c6 represents the phase and \u03c9 represents the angular frequency.\nThe displacement of the summed waves is\nformula_30\nThe intensity of the light at r is given by\nformula_31\nThis can be expressed in terms of the intensities of the individual waves as\nformula_32\nThus, the interference pattern maps out the difference in phase between the two waves, with maxima occurring when the phase difference is a multiple of 2\u03c0. If the two beams are of equal intensity, the maxima are four times as bright as the individual beams, and the minima have zero intensity.\nClassically the two waves must have the same polarization to give rise to interference fringes since it is not possible for waves of different polarizations to cancel one another out or add together. Instead, when waves of different polarization are added together, they give rise to a wave of a different polarization state.\nQuantum mechanically the theories of Paul Dirac and Richard Feynman offer a more modern approach. Dirac showed that every quanta or photon of light acts on its own which he famously stated as \"every photon interferes with itself\". Richard Feynman showed that by evaluating a path integral where all possible paths are considered, that a number of higher probability paths will emerge. In thin films for example, film thickness which is not a multiple of light wavelength will not allow the quanta to traverse, only reflection is possible. \nLight source requirements.\nThe discussion above assumes that the waves which interfere with one another are monochromatic, i.e. have a single frequency\u2014this requires that they are infinite in time. This is not, however, either practical or necessary. Two identical waves of finite duration whose frequency is fixed over that period will give rise to an interference pattern while they overlap. Two identical waves which consist of a narrow spectrum of frequency waves of finite duration (but shorter than their coherence time), will give a series of fringe patterns of slightly differing spacings, and provided the spread of spacings is significantly less than the average fringe spacing, a fringe pattern will again be observed during the time when the two waves overlap.\nConventional light sources emit waves of differing frequencies and at different times from different points in the source. If the light is split into two waves and then re-combined, each individual light wave may generate an interference pattern with its other half, but the individual fringe patterns generated will have different phases and spacings, and normally no overall fringe pattern will be observable. However, single-element light sources, such as sodium- or mercury-vapor lamps have emission lines with quite narrow frequency spectra. When these are spatially and colour filtered, and then split into two waves, they can be superimposed to generate interference fringes. All interferometry prior to the invention of the laser was done using such sources and had a wide range of successful applications.\nA laser beam generally approximates much more closely to a monochromatic source, and thus it is much more straightforward to generate interference fringes using a laser. The ease with which interference fringes can be observed with a laser beam can sometimes cause problems in that stray reflections may give spurious interference fringes which can result in errors.\nNormally, a single laser beam is used in interferometry, though interference has been observed using two independent lasers whose frequencies were sufficiently matched to satisfy the phase requirements.\nThis has also been observed for widefield interference between two incoherent laser sources.\nIt is also possible to observe interference fringes using white light. A white light fringe pattern can be considered to be made up of a 'spectrum' of fringe patterns each of slightly different spacing. If all the fringe patterns are in phase in the centre, then the fringes will increase in size as the wavelength decreases and the summed intensity will show three to four fringes of varying colour. Young describes this very elegantly in his discussion of two slit interference. Since white light fringes are obtained only when the two waves have travelled equal distances from the light source, they can be very useful in interferometry, as they allow the zero path difference fringe to be identified.\nOptical arrangements.\nTo generate interference fringes, light from the source has to be divided into two waves which then have to be re-combined. Traditionally, interferometers have been classified as either amplitude-division or wavefront-division systems.\nIn an amplitude-division system, a beam splitter is used to divide the light into two beams travelling in different directions, which are then superimposed to produce the interference pattern. The Michelson interferometer and the Mach\u2013Zehnder interferometer are examples of amplitude-division systems.\nIn wavefront-division systems, the wave is divided in space\u2014examples are Young's double slit interferometer and Lloyd's mirror.\nInterference can also be seen in everyday phenomena such as iridescence and structural coloration. For example, the colours seen in a soap bubble arise from interference of light reflecting off the front and back surfaces of the thin soap film. Depending on the thickness of the film, different colours interfere constructively and destructively.\nQuantum interference.\nQuantum interference \u2013 the observed wave-behavior of matter \u2013 resembles optical interference. Let formula_33 be a wavefunction solution of the Schr\u00f6dinger equation for a quantum mechanical object. Then the probability of observing the object in the interval formula_34 is formula_35 where * indicates complex conjugation. Quantum interference concerns the issue of this probability when the wavefunction is expressed as a sum or linear superposition of two terms formula_36:\nformula_37\nUsually, formula_38 and formula_39 correspond to distinct situations A and B. When this is the case, the equation formula_36 indicates that the object can be in situation A or situation B. The above equation can then be interpreted as: The probability of finding the object at formula_41 is the probability of finding the object at formula_41 when it is in situation A plus the probability of finding the object at formula_41 when it is in situation B plus an extra term. This extra term, which is called the \"quantum interference term\", is formula_44 in the above equation. As in the classical wave case above, the quantum interference term can add (constructive interference) or subtract (destructive interference) from formula_45 in the above equation depending on whether the quantum interference term is positive or negative. If this term is absent for all formula_41, then there is no quantum mechanical interference associated with situations A and B.\nThe best known example of quantum interference is the double-slit experiment. In this experiment, matter waves from electrons, atoms or molecules approach a barrier with two slits in it. The part of the wavefunction going through one slit is associated with formula_38 while the part going through the other slit is associated with formula_39. The interference pattern occurs on the far side, observed by detectors suitable to the particles originating the matter wave. The pattern matches the optical double slit pattern.\nApplications.\nBeat.\nIn acoustics, a beat is an interference pattern between two sounds of slightly different frequencies, \"perceived\" as a periodic variation in volume whose rate is the difference of the two frequencies.\nWith tuning instruments that can produce sustained tones, beats can be readily recognized. Tuning two tones to a unison will present a peculiar effect: when the two tones are close in pitch but not identical, the difference in frequency generates the beating. The volume varies like in a tremolo as the sounds alternately interfere constructively and destructively. As the two tones gradually approach unison, the beating slows down and may become so slow as to be imperceptible. As the two tones get further apart, their beat frequency starts to approach the range of human pitch perception, the beating starts to sound like a note, and a combination tone is produced. This combination tone can also be referred to as a missing fundamental, as the beat frequency of any two tones is equivalent to the frequency of their implied fundamental frequency.\nInterferometry.\nInterferometry is an experimental technique for measuring or using interference. It can be used with many types of waves. All interferometers require a source of coherent waves.s \nOptical interferometry.\nThe simplest interferometer has a pinhole to create a coherent source followed by a mask with two holes and a screen to observe the interference. This gives the double-slit experiment. Modern versions replace the initial pinhole with the coherent light of a laser. Other wave-front splitting interferometers use mirror or prisms to split and recombine waves; amplitude splitting devices use thin dielectric films. Multiple beam interferometers can include lenses. \nThe results of the Michelson\u2013Morley experiment are generally considered to be the first strong evidence against the theory of a luminiferous aether and in favor of special relativity.\nInterferometry has been used in defining and calibrating length standards. When the metre was defined as the distance between two marks on a platinum-iridium bar, Michelson and Beno\u00eet used interferometry to measure the wavelength of the red cadmium line in the new standard, and also showed that it could be used as a length standard. Sixty years later, in 1960, the metre in the new SI system was defined to be equal to 1,650,763.73 wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum. This definition was replaced in 1983 by defining the metre as the distance travelled by light in vacuum during a specific time interval. Interferometry is still fundamental in establishing the calibration chain in length measurement.\nInterferometry is used in the calibration of slip gauges (called gauge blocks in the US) and in coordinate-measuring machines. It is also used in the testing of optical components.\nRadio interferometry.\nIn 1946, a technique called astronomical interferometry was developed. Astronomical radio interferometers usually consist either of arrays of parabolic dishes or two-dimensional arrays of omni-directional antennas. All of the telescopes in the array are widely separated and are usually connected together using coaxial cable, waveguide, optical fiber, or other type of transmission line. Interferometry increases the total signal collected, but its primary purpose is to vastly increase the resolution through a process called Aperture synthesis. This technique works by superposing (interfering) the signal waves from the different telescopes on the principle that waves that coincide with the same phase will add to each other while two waves that have opposite phases will cancel each other out. This creates a combined telescope that is equivalent in resolution (though not in sensitivity) to a single antenna whose diameter is equal to the spacing of the antennas farthest apart in the array.\nAcoustic interferometry.\nAn acoustic interferometer is an instrument for measuring the physical characteristics of sound waves in a gas or liquid, such velocity, wavelength, absorption, or impedance. A vibrating crystal creates ultrasonic waves that are radiated into the medium. The waves strike a reflector placed parallel to the crystal, reflected back to the source and measured.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15114", "revid": "42069556", "url": "https://en.wikipedia.org/wiki?curid=15114", "title": "Indictable offence", "text": "Offence which can only be tried on an indictment after a preliminary hearing\nIn many common law jurisdictions (e.g. England and Wales, Ireland, Canada, Hong Kong, India, Australia, New Zealand, Malaysia, Singapore), an indictable offence is an offence which can only be tried on an indictment after a preliminary hearing to determine whether there is a \"prima facie\" case to answer or by a grand jury (in contrast to a summary offence). A similar concept in the United States is known as a felony, which for federal crimes, also requires an indictment. In Scotland, which is a hybrid common law jurisdiction, the procurator fiscal will commence solemn proceedings for serious crimes to be prosecuted on indictment before a jury.\nAustralia.\nIn Australia, an indictable offence is more serious than a summary offence, and one where the defendant has the right to trial by jury. They include crimes such as murder, rape, and threatening or endangering life. The system is underpinned by various state and territory acts and the \"Commonwealth Crimes Act 1914\".\nIn South Australia, New South Wales, and Queensland, indictable offences are further split into two categories: major indictable offences (including murder, rape, and threatening or endangering life) are heard in the state's Supreme Court, while minor indictable offences are heard in the District Court. In South Australia, minor indictable offences are generally heard in magistrates courts, although the defendant may elect to be heard in the District Court.\nCanada.\nIn Canada, an indictable offence is a crime that is more serious than a summary offence. Examples of indictable offences include theft over $5,000, breaking and entering, aggravated sexual assault, and murder. Maximum penalties for indictable offences are different depending on the crime and can include life in prison. There are minimum penalties for some indictable offences.\nEngland and Wales.\nIn relation to England and Wales, the expression \"indictable offence\" means an offence which, if committed by an adult, is triable on indictment, whether it is exclusively so triable or triable either way; and the term \"indictable\", in its application to offences, is to be construed accordingly. In this definition, references to the way or ways in which an offence is triable are to be construed without regard to the effect, if any, of section 22 of the Magistrates' Courts Act 1980 on the mode of trial in a particular case.\nAn either-way offence allows the defendant to elect between trial by jury on indictment in the Crown Court and summary trial in a magistrates' court. However, the election may be overruled by the magistrates' court if the facts suggest that the sentencing powers of a magistrates' court would be inadequate to reflect the seriousness of the offence. \nIn relation to some indictable offences, for example criminal damage, only summary trial is available unless the damage caused exceeds \u00a35,000. \nA youth court has jurisdiction to try all indictable offences with the exception of homicide and certain firearms offences, and will normally do so provided that the available sentencing power of two years' detention is adequate to punish the offender if found guilty.\nHistory.\nSee section 64 of the Criminal Law Act 1977. \nGrand juries were abolished in 1933.\nOffences triable only on indictment.\nSome offences such as murder and rape are considered so serious that they can only be tried on indictment at the Crown Court where the widest range of sentencing powers is available to the judge.\nThe expression \"indictable-only offence\" was defined by section 51 of the Crime and Disorder Act 1998, as originally enacted, as an offence triable only on indictment. Sections 51 and 52 of, and Schedule 3 to, that Act abolished committal proceedings for such offences and made other provisions in relation to them.\nWhen the accused is charged with an indictable-only offence, he or she will be tried in the Crown Court. The rules are different in England and Wales in respect of those under 18 years of age.\nSee also section 14(a) of the Criminal Law Act 1977.\nNew Zealand.\nSimilarly in New Zealand, a rape or murder charge will be tried at the High Court, while less serious offences such as theft will be tried at the District Court. However, the District Court can hold both jury and summary trials.\nUnited States.\nIn the United States, federal felonies always require an indictment from a grand jury before proceeding to trial. In contrast, while misdemeanors may proceed to trial on indictment, this is not required, as they may also proceed on information or complaint. Different states have different policies; since the requirement of an indictment by grand jury is not incorporated against the states, in many states, an indictment is not required for a felony case to proceed. However, some states do still use grand jury indictments for felony-level offenses and may use other terminology. For instance, in New Jersey, whose constitution requires all \"crimes\" to be charged by indictment but allows lesser \"offenses\" not to be, felony-level offenses are commonly called \"indictable offenses\", including in the New Jersey Penal Code, to avoid confusion between the narrow technical definition of the word \"crime\" from the state's constitutional jurisprudence and the broader sense.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15116", "revid": "21606851", "url": "https://en.wikipedia.org/wiki?curid=15116", "title": "Inter Milan", "text": "Association football club in Italy\nFootball club\nFootball Club Internazionale Milano, commonly referred to as Internazionale () or simply Inter (), and colloquially known as Inter Milan in other countries, including English-speaking countries, is an Italian professional football club based in Milan, Lombardy. Inter is the only Italian team to have always participated in the top division of Italian football since its debut in 1909 and the only one that has never been relegated in Serie B. Since 1947, Inter has shared the San Siro stadium with AC Milan\u2014the club from which it originally split. The San Siro is the largest stadium in Italy, with a capacity of 75,817. The long-standing rivalry between the two clubs, known as the \"Derby della Madonnina\", is one of the most widely followed derbies in world football.\nFounded in 1908 following a schism within the Milan Foot-Ball and Cricket Club (now AC Milan), Inter won its first championship in 1910. Since its formation, the club has won 37 domestic trophies, including 20 league titles, nine Coppa Italia, and eight Supercoppa Italiana. From 2006 to 2010, the club won five successive league titles, equaling the all-time record at that time. They have won the European Cup/Champions League three times, their latest win in 2010 completed an unprecedented Italian seasonal treble, with Inter winning the Serie A and the Coppa Italia the same year. The club has also won three UEFA Cups, two Intercontinental Cups, and one FIFA Club World Cup. Inter is the only Italian club that won at least an official trophy in every decade since the foundation of the club in 1908.\nInter has the highest home game attendance in Italy and the fourth-highest attendance in Europe. Since May 2024, the club has been owned by American asset management company Oaktree Capital Management.\nHistory.\nFoundation and early years (1908\u20131960).\nThe club was founded on 9 March 1908 as \"Football Club Internazionale\", when a group of players left the Milan Cricket and Football Club (now AC Milan) to form a new club because they wanted to accept more foreign players. The name of the club derives from the wish of its founding members to accept foreign players as well as Italians. The club won its first championship in 1910 and its second in 1920. The captain and coach of the first championship winning team was Virgilio Fossati, who was later killed in battle while serving in the Italian army during World War I.\nIn 1922, Inter was at risk of relegation to the Second Division of Northern League, but they remained in the top league after winning two play-offs.\nSix years later, during the Fascist era, the club merged with the \"Unione Sportiva Milanese\" and, for political reasons, was renamed \"Societ\u00e0 Sportiva Ambrosiana\". During the 1928\u201329 season, the team wore white jerseys with a red cross emblazoned on it; the jersey's design was inspired by the flag and coat of arms of the city of Milan. In 1929, the new club chairman Oreste Simonotti changed the club's name to \"Associazione Sportiva Ambrosiana\" and restored the previous black-and-blue jerseys; however, supporters continued to call the team \"Inter\", and in 1931, new chairman Pozzani succumbed to shareholder pressure and changed the name to \"Associazione Sportiva Ambrosiana-Inter\".\nInter won its third championship titles in 1930 with the Hungarian coach Arpad Weisz in the first ever edition of Serie A, and the fourth in 1938 with former player Armando Castellazzi as a 33 years old coach, that set the record for the youngest coach ever to win the national title that lasts to this day. Inter also got their first Coppa Italia (Italian Cup) in 1939 with the decisive goal in the final scored by Olympic gold medal and top scorer in 1936 Olympics Annibale Frossi. Inter main star and the captain of the team in this period was Giuseppe Meazza, one of the greatest Italian player of all time with two World Cup won with the National team and the greatest scorer in Inter history with 284 goals, and after whom the San Siro stadium is officially named after his death in 1980. 38 goals scored by Meazza in 39 matches in 1929-1930 is a seasonal record in Inter history still unbeaten today. Inter ended also three consecutive times in 2nd place between 1933 and 1935; in those years many South Americans of Italian origin arrived in Milan to circumvent the regime's rules that prohibit the hiring of foreign players: Uruguayan players like World Cup Winner in 1930 Hector Scarone and Ernesto Mascheroni and also Ricardo Faccio and Francesco Frione, and Argentinian like Attilio Demar\u00eda that stayed 10 seasons with the club. A fifth championship followed in 1940, that ended a decade dominated by three teams: Inter, Bologna and the historic rival Juventus, while AC Milan didn't win a title for 44 years from 1907 to 1951 and didn't win a single derby for a record 17 matches from 1928 to 1938.\nIn the 1930s Inter also played for seven times in one of the first major European football cups, the Central European Cup, with Meazza that was a record three times topscorer of the competition; coached by \u00c1rp\u00e1d Weisz Inter reached the final in 1933, when after had won the first leg in Milan 2\u20131, lost 3\u20131 in 9 men against Austria Vienna. 4 out of 11 players of that team: Meazza, Luigi Allemandi, Attilio Demar\u00eda and Armando Castellazzi would go on to win the 1934 World Cup with Italian national team, while other five Inter players will contribute to the win of 1938 World Cup with Italy: Meazza, Ugo Locatelli, Giovanni Ferrari, Pietro Ferraris and Renato Olmi.\nAfter the end of World War II, the club's name changed back to its original one, Internazionale, and it come close to win Serie A title in two occasions, one in the last season of Grande Torino in 1949, against whom Inter were the last team to face them on 30 April 1949 five days before the Superga air disaster, and in 1951 for just one point, with the contribution of great players acquired by president Carlo Masseroni in these years, like Gino Armano, Amedeo Amadei, the first Dutch player in club history Faas Wilkes and the Hungarian Istv\u00e1n Nyers from Stade Fran\u00e7ais; Inter will win its sixth championship in 1953 and its seventh in 1954, for the first time in two consecutive years, coached by Alfredo Foni and led by two of the most prolific strikers in club history: Istv\u00e1n Nyers and Benito Lorenzi with the Swedish Lennart Skoglund that completed the offensive trio. One of the crucial matches of the 1954 Scudetto was the direct clash for the title, that saw Inter victory over Juventus for 6-0, the club's biggest victory in the Derby d'Italia.\nIn May 1955, Angelo Moratti became the new owner of Inter, and in the first years of his presidency got disappointing results despite strong players like forwards, Eddie Firmani and the Argentinian Angelillo who scored an all-time record in a season in Serie A with 18 teams: 33 goals in 33 matches in 1958-1959 season, tied also Meazza seasonal record of 38 goals in 39 matches.\nMoratti in the following years put foundations to one of the greatest team in football history starting from the debut of a 16 years old Mario Corso and the acquisition of Aristide Guarneri in 1958, and under Argentinian coach Helenio Herrera in 1960 with the signing of Giacinto Facchetti and Armando Picchi.\n\"Grande Inter\" (1960\u20131967).\nIn 1960, manager Helenio Herrera joined Inter from Barcelona and in his first season as a coach in Milan, after having led the table for most of the season, lost the title in the last games of the season, with the infamous episode during Juventus\u2013Inter held in Turin in April 1961 when the match was stopped after 30 minutes when Juventus supporters invaded the pitch, with Inter being awarded the game 2\u20130. Then, after two months, in June before the last decisive match of Serie A with the two teams tied in first place, the Italian Football Federation, presided by Juventus president Umberto Agnelli, decided that the match between the two teams had to be replayed after the last game scheduled for the season; with Inter loss and a draw for Juventus, the following match became useless and in open contestation Angelo Moratti ordered Herrera to put the Inter youth team against the Turinese squad: the match ended 9\u20131 for Juventus, with the only goal scored for Inter by an 18-year-old, the son of Valentino Mazzola, Sandro Mazzola who later would become one of the greatest legends in the history of the club.\n After his first season in Milan, Herrera brought with him for a record fee of 25 million pesetas Spanish midfielder Luis Su\u00e1rez who won the European Footballer of the Year in 1960 for his role in Barcelona's La Liga/Fairs Cup double. Herrera would transform Inter into one of the leading teams in Europe that would win three Serie A titles in four years, two European Cups and two Intercontinental Cups in a row. He modified a 5\u20133\u20132 tactic known as the \"Verrou\" (\"door bolt\"), which created greater flexibility for counterattacks. The \"catenaccio\" system was invented by an Austrian coach, Karl Rappan. Rappan's original system was implemented with four fixed defenders, playing a strict man-to-man marking system, plus a playmaker in the middle of the field, who plays the ball together with two midfield wings. Herrera would modify it by adding a fifth defender, the sweeper or libero, behind the two centre backs. The sweeper or \"libero\", who acted as the free man, would deal with any attackers who went through the two centre backs. Inter finished third in the Serie A in his first season, second the next year and first in his third season. Then followed a back-to-back European Cup victory in 1964 and 1965, earning him the title \"il Mago\" (\"the Wizard\"). The core of Herrera's team were the goalkeeper Giuliano Sarti, the full-backs Tarcisio Burgnich and Giacinto Facchetti, Armando Picchi the sweeper, Su\u00e1rez the playmaker, the Brazilian Jair the right winger, Mario Corso the left winger and Sandro Mazzola, who played on the inside-right.\nAfter the Serie A title won in the previous season, in 1964 Inter reached the European Cup Final by beating Borussia Dortmund in the semi-final and Partizan in the quarter-final. In the final in Praterstadion, Vienna, they met Real Madrid, a team that had reached seven out of the nine finals to date. Mazzola scored two goals and one from Milani in a 3\u20131 victory, becoming also the first ever team to win the tournament without losing a single game.\nThe team also won the Intercontinental Cup; after having lost the first match in Argentine against Independiente 1\u20130, Inter won second leg 2\u20130 in San Siro with goals from Mazzola and Corso, in the third decisive match played at the Santiago Bernabeu, Inter won in extra-time with a goal from Mario Corso, the first Italian club to win the trophy and become club world champion.\nIn 1964, Inter added other important players Angelo Domenghini, Gianfranco Bedin and another Spanish Joaqu\u00edn Peir\u00f3, who played with consistency and was decisive in European Cup where three foreign players could play at the same time, while in Serie A only two were allowed to play.\nA year later, after have defeated Liverpool in the semi-final second leg 3\u20130 recovering from a 3\u20131 defeat at Anfield, with Facchetti scoring the decisive goal, Inter repeated the feat by beating two-time winner Benfica in the final held at home, from a Jair goal, and then again beat Independiente in the Intercontinental Cup with a 3\u20130 win in San Siro, with two goals from Mazzola and one from Peir\u00f2, and a draw in Argentine, becoming the first European team to win two times in a row the competition. Inter came close to winning the Treble for the first time in European football history that year, after having also won the Serie A title, but lost the Coppa Italia final against Juventus in a game played in the last days of August 1965.\nFacchetti was voted second in 1965 Ballon d'Or rankings, just missing out the chance to become the first defender to win the award.\nInter again reached semi-finals of the European cup in 1966, but this time lost against a Real Madrid team that would go on to win the tournament, while in national championship Herrera's squad won the tenth scudetto in club history, the first Star.\nAt the end of the season, Moratti signed two of the greatest players of all time: Franz Beckenbauer and Eusebio; but, after 1966 World Cup when Italian National Team was eliminated by North Korea, the Italian Federation decided to block new signings of foreign players, a ban which lasted until 1980; thus, the contracts with the two players were cancelled.\nIn 1967, after Inter eliminated Real Madrid in quarter-finals, with Su\u00e1rez and Jair injured, Inter lost the European Cup final in Lisbon 2\u20131 to Celtic; a week later, despite the first position, with a lost against Mantova in the last match of the championship, Inter lost also the Serie A title and, a week later, the Coppa Italia semi-final against Padova, putting an end de facto to the Grande Inter cicle with the first season without trophy since 1961\u20131962. During that year, the club changed its name to \"Football Club Internazionale Milano\", and in 1968 after 13 years Angelo Moratti sold the team to Ivanoe Fraizzoli, and also Helenio Herrera left the team.\nSubsequent achievements (1967\u20131991).\nFacchetti, captain of Italian National team for an all-time record of 11 years, Burnich and Guarneri formed also the defense of Italy that won UEFA Euro 1968 with Mazzola and Domenghini, and that with the addition of Inter players Roberto Boninsegna and Mario Bertini also reached final of 1970 World Cup against Brazil, tournament known also for the famous semifinal match, the so-called \"Game of the Century\" against Germany.\nFollowing the golden era of the 1960s, Inter managed to win their eleventh league title in 1971 under the coach Giovanni Invernizzi who took over the job during the season with the team that made a great comeback after have had a difficult start, with 23 consecutive matches without a loss, with Roberto Boninsegna, one of the greatest strikers in club history, who led the league with 24 goals in that seasons and repeated the feat the following season with 22. Inter reached for the second time in five years the European Cup final in 1972 after have defetead Borussia M\u00f6nchengladbach, Standard Li\u00e8ge and Celtic in the semi-final, with a team which still featured Facchetti, Mazzola, Burnich, Jair, Bedin and Corso (the latter who could not play in the remain matches of the competition for disqualification after the red card at the end of the first match against M\u00f6nchengladbach) and also a young Gabriele Oriali. The final held in Rotterdam saw the victory for 2\u20130 of Johan Cruyff's Ajax that won the trophy for the second consecutive season.\nThe return of Helenio Herrera in 1973 as Inter coach lasted only 16 matches, for a heart attack that obliged him to leave the coaching job. Mazzola retired in 1977 and Facchetti, the last member of La Grande Inter, retired in 1978 as the most prolific defender in the history of Serie A with 59 goals scored and after having won the last trophy of his career, the Coppa Italia.\nInter won their twelfth scudetto in 1980, the last one won in the history of Serie A by a team composed entirely of Italian players, and also added two to its Coppa Italia tally, in 1977\u201378 (with the future captain Graziano Bini who scored the decisive goal in the final against Napoli) and in 1981\u201382, both under coach Eugenio Bersellini. Alessandro Altobelli, who later became the all-time leading scorer in Coppa Italia and in International competition for the club and played for Inter for 11 seasons, scored 209 goals, second only to Giuseppe Meazza. Altobelli also scored three goals against Juventus in a 4-0 victory on 11 November 1979, a feat which was repeated again five years later on the same day on 11 November 1984, with the same result, this time with the first two goals in Serie A for Karl-Heinz Rummenigge.\nIn this period, AC Milan were relegated two times in Serie B, the first time in 1980 for implications involved in the Totonero scandal and then again after the team ended its 1981\u201382 campaign in third-last place.\nIn 1981, Inter reached for the sixth time in six participations the semi-final of the European Cup, this time against Real Madrid, a classic match, and who they would encounter again in three different European competitions throughout the 1980s: in UEFA Cup Winners' Cup quarter-finals in 1983 and in Uefa Cup semi-finals in 1985 and 1986.\n Karl-Heinz Rummenigge considered one of the greatest player in the world and one of the five Germans who played for Inter in the 1980s\nGiuseppe Bergomi, the youngest player to make his professional debut in first team in the history of the club at 16 years old, one month and eight days in January 1980, remained at Inter for all his career for a record 20 seasons, till the end of 1998-1999 season. Bergomi with Oriali, Altobelli, Gianpiero Marini and Ivano Bordon were part of Italy squad that won 1982 FIFA World Cup.\nThe Italian federation reopened the possibility to sign foreign players in 1980; in the following years, Inter signed among others: Herbert Prohaska, Hansi M\u00fcller from VfB Stuttgart, two times Ballon d'Or winner Karl-Heinz Rummenigge from Bayern Munich (who formed a deadly duo with Altobelli), Liam Brady and Argentinian Daniel Passarella. Other important players in that time were Italians Walter Zenga (voted as World's Best Goalkeeper by IFFHS for three years in a row in 1989, 1990 and 1991) and the defenders Giuseppe Baresi, Bergomi and Riccardo Ferri.\nLed by the German duo of Andreas Brehme and Lothar Matth\u00e4us, with Aldo Serena top scorer in Serie A with 22 goals, Argentine Ram\u00f3n D\u00edaz and Nicola Berti, Inter, coached by Giovanni Trapattoni, captured the 1989 Serie A championship setting many record, the so called \u201cScudetto dei Record\u201d: ended with an all-time record for most points in Serie A history with 18 teams with 58 points out of 68, 26 victories out of 34 matches, the best offence and the best defense, with an 11 point margin over Maradona's Napoli and 12 point margin over AC Milan, coached by Sacchi (with two points per victory, rule that lasted until the end of 1993-1994 season). Inter were unable to defend their title in the following season in a very competitive Serie A that saw six different teams win in seven years, and despite adding fellow German J\u00fcrgen Klinsmann to the squad and winning their first Supercoppa Italiana at the start of the season.\nMixed fortunes (1991\u20132004).\nThe 1990s were disappointing years in terms of victories, while their great rivals, Milan and Juventus, achieved successes mainly at a domestic level in Serie A, and also winning the renamed UEFA Champions League once each.\nInter enjoyed little success in the domestic league standings, their worst coming in 1993\u201394 when they finished in thirteenth position, just one point above the relegation zone. Nevertheless, they achieved prestigious European success, with three UEFA Cup victories out of four finals, in 1991, 1994 and 1998.\nAfter the win of the 1990 World Cup of West Germany led by three Inter players, Matthews was awarded the Ballon d'Or and ended 1990\u20131991, his most prolific season in career, with 23 goals, including six in 1991 UEFA Cup, the first European trophy since the Grande Inter period; Trapattoni left the team after five seasons. At the end of 1991, Mattheus was awarded also with the first ever FIFA World Player of the Year.\nIn 1992, after a disappointing season, to replace the three German players that left in the summer and with the new coach Osvaldo Bagnoli, Inter signed important players like the future Ballon d'Or Matthias Sammer, Rub\u00e9n Sosa and Igor Shalimov, the first Russian player in club history; others were ultimately less successful, like the former European Golden Boot winner Darko Pancev and Salvatore Schillaci; Inter ended the season in second place behind AC Milan, coached by Fabio Capello.\nIn the following season, Inter acquired from Ajax Wim Jonk and Dennis Bergkamp that, with eight goals in the competition, led Inter to their second victory in UEFA Cup, despite the worst result in club history in Serie A.\nWith Massimo Moratti's takeover from Ernesto Pellegrini in 1995, Inter twice broke the world record transfer fee in this period (\u00a319.5\u00a0million for Ronaldo from Barcelona in 1997 and \u00a331\u00a0million for Christian Vieri from Lazio two years later). Among Moratti's first acquisitions in 1995 there were Javier Zanetti from Banfield, who would stay at Inter until 2014 with a record of 858 game played and with a record 13 seasons as captain, Paul Ince from Manchester United and Roberto Carlos from Palmeiras, who was sold the following season to Real Madrid, with many regrets and recriminations from fans.\nHowever, the 1990s remained the only decade in Inter's history, in which they did not win a single Serie A championship. This persistent lack of success led to poor relations between the fanbase and the chairman, the managers and even some individual players.\nIn the 1996\u20131997 season, Inter reached a third UEFA Cup final, losing this time on penalties in the second leg at the Giuseppe Meazza against Schalke 04, with Roy Hodgson resigning shortly afterwards. In the 1997\u20131998 season, with the acquisition of the European Golden Shoe and later Ballon d'Or and FIFA World Player of the Year winner Ronaldo, under coach Luigi Simoni, Inter had won their third UEFA Cup in Paris, beating Lazio 3\u20130, with goals from Ivan Zamorano, Zanetti and Ronaldo, and nearly won Serie A title, with many controversial refereeing decisions. This culminated in the decisive match against Juventus in Turin, with Inter only one point behind with four games left, when referee didn't give a penalty to Ronaldo, but a few seconds later, gave a penalty to Juventus; this generated a turmoil on the pitch and a big scandal. The referee sent off Simoni, and president Moratti left the building shortly afterwards, saying to journalists: \u201cI\u2019m not sticking around just to be made fun of\u201d.\nAt the end of 1998, Inter was ranked by IFFHS as Best Club in the World for that year.\nMoratti later became a target of the fans, especially when he sacked the much-loved coach Simoni after a few games into the 1998\u201399 season, five days after Inter defeated Real Madrid 3\u20131 at San Siro in Champions League group stage, with two goals from Roberto Baggio, and having just received the Italian manager of the year award for 1998 the day before being dismissed. That season, despite four coaching changes, Inter reached the Champions League quarter-finals, where they were eliminated by Manchester United, who would go on to win the trophy that year; Inter failed to qualify for any European competition for the first time in seven years, finishing in eighth place.\nThe following season, in 1999\u20132000, Moratti appointed former Juventus manager Marcello Lippi, and signed players such as Angelo Peruzzi, Laurent Blanc, Iv\u00e1n C\u00f3rdoba, Clarence Seedorf from Real Madrid and also Vieri and Jugovi\u0107; he also sold other important players, like Diego Simeone, Youri Djorkaeff, Aron Winter and Gianluca Pagliuca. The team came close to their first domestic success since 1989 when they reached the Coppa Italia final, only to be defeated by Lazio, in a match remembered for the second severe injury to the right knee of Ronaldo, who was returning after five months of inactivity, and which would keep him out for more than a year and a half.\nInter's misfortunes continued the following season, losing the 2000 Supercoppa Italiana match against Lazio 4\u20133, after initially taking the lead through new signing Robbie Keane. They were also eliminated in the preliminary round of the Champions League by Swedish club Helsingborg, with \u00c1lvaro Recoba missing a crucial late penalty. Lippi was sacked after only a single game of the new season, following Inter's first-ever Serie A defeat to Reggina. Marco Tardelli, chosen to replace Lippi, failed to improve results and is remembered by Inter fans as the manager who lost 6\u20130 in the city derby against a weak AC Milan, who finished the season in sixth place behind Inter, who finished in fifth.\nAfter the unfortunate decision to sell Andrea Pirlo to rival AC Milan in the summer of 2001 for 35 billion Italian lira, in the next season with new coach Hector Cuper, the acquisition of the second most expensive goalkeeper in the world at that time Francesco Toldo, Marco Materazzi and the return after injury of Ronaldo in pair with Vieri (a dream couple that played only 11 matches for a total of 667 minutes in three years, scoring 18 goals), not only did Inter manage to make it to the UEFA Cup semi-finals, but were also only 45 minutes away from capturing the \"Scudetto\" when they needed to maintain their one-goal advantage away to Lazio. Inter were 2\u20131 up after only 24 minutes. Lazio equalised during first half injury time, and then scored two more goals by Simeone and Simone Inzaghi in the second half to secure victory that saw Juventus win the championship, Roma ended second and Inter third. After brilliant performances and having won 2002 World Cup with Brazil, Ronaldo demanded to be sold to Real Madrid for \u20ac45 million, and was replaced by Hernan Crespo from Lazio for \u20ac40 million. Seedorf was sold to AC Milan and Fabio Cannavaro was acquired from Parma.\nThe next season, Inter finished as league runners-up, with Vieri that was top scorer of Serie A with 24 goals in 23 matches, while Crespo set a new record for UCL Group stage, with eight goals in six matches, but missed almost the rest of the season for a severe injury in January. In October 2002, in a home game against Lyon, Inter was defeated for the first time in its history at home in European Cup/UEFA Champions League after 33 matches in 39 years. Inter reached 2002\u201303 Champions League semi-finals against AC Milan, that were played also without the injured Vieri, and was eliminated on the away goals rule with two draw in the same stadium, the San Siro.\nAfter only one season, Crespo was sold to Chelsea for 26 million \u20ac and was replaced by Julio Cruz from Bologna for 9,5 million \u20ac.\nThe 2003\u20132004 season started well, with an historic win for Inter and for Italian football in the Champions League at Highbury against Arsenal of Invincibles with a 3\u20130 victory, as well as a win against Dinamo Kyiv; but, after a draw against Brescia in Serie A, in October coach Cuper was sacked and was replaced by Alberto Zaccheroni, who couldn't help to avoid the elimination from the Champions League group stage. Despite acquisition in January of strong players like Dejan Stankovic and Adriano, Inter ultimately finished only in fourth place in Serie A. Other members of the Inter \"family\" during this period who \"suffered\" were the likes of Vieri and Cannavaro, both of whom had their restaurants in Milan vandalised after the second defeats of the season to the \"Rossoneri\" 3\u20132 in February 2004 in Serie A; but the most important was the resignation from presidency by Massimo Moratti in favour of Giacinto Facchetti in January 2004, that lasted until the premature death of Inter legend in September 2006.\nComeback and unprecedented treble (2004\u20132011).\nOn 8 July 2004, Inter appointed former Lazio manager Roberto Mancini as its new head coach, with players who will make the history of Inter like Esteban Cambiasso, Julio Cesar, and in 2005 Walter Samuel and Luis Figo from Real Madrid. In his first season, the team collected 72 points from 18 wins, 18 draws and only two losses, as well as winning the Coppa Italia against Roma, with two goals from Adriano, and later the Supercoppa Italiana in Turin against Juventus with a goal from Juan Sebasti\u00e1n Ver\u00f3n. After Adriano's dominating performances in the 2004 Copa Am\u00e9rica and the 2005 FIFA Confederations Cup, both won by Brazil, he was awarded the IFFHS World's Best International Goal Scorer in 2005.\nOn 11 May 2006, Inter won the Coppa Italia title for the second season in a row after defeating Roma with a 4\u20131 aggregate victory (a 1\u20131 scoreline in Rome and a 3\u20131 win at the San Siro).\nInter were awarded the 2005\u201306 Serie A championship retrospectively, after title-winning Juventus was relegated for match fixing and illecits involved among others referee designators Bergamo and Pairetto and referees, with their executives Moggi and Giraudo, who were at Juventus since 1994, banned for life from football, and points were stripped also from other clubs involved, including AC Milan, due to the implications in \"Calciopoli\" scandal. During the following season, Inter with new players like Maicon, Maxwell, Patrick Vieira, Zlatan Ibrahimovic and the return of Crespo from Chelsea, went on a record-breaking run of 17 consecutive victories in Serie A, starting on 25 September 2006, with a 4\u20131 home victory over Livorno, and ending on 28 February 2007, after a 1\u20131 draw at home to Udinese. On 22 April 2007, Inter won their second consecutive \"Scudetto\"\u2014and first on the field since 1989\u2014when they defeated Siena 2\u20131 at Stadio Artemio Franchi, ended the season with an all time Serie A record of 97 points and an all-time record margin of 22 points over second place Roma. Italian World Cup-winning defender Marco Materazzi scored both goals.\nIn this period, Inter also reached two UCL quarter-finals in 2005 and 2006, and the UCL round of 16 in 2007: on the last two occasions, Inter was eliminated via away goals rules by Villarreal and Valencia.\nInter started the 2007\u201308 season with the goal of winning both Serie A and the Champions League in the year of the centenary of the foundation of the club. The team started well in the league, topping the table from the first round of matches, and also managed to qualify for the Champions League knockout stage. However, a late collapse, leading to a 2\u20130 defeat with ten men away to Liverpool on 19 February in the Champions League, brought manager Roberto Mancini's future at Inter into question, while domestic form took a sharp turn of fortune, with the team failing to win in the three following Serie A games. After being eliminated by Liverpool in the Champions League, Mancini announced his intention to leave his job immediately, only to change his mind the following day. On the final day of the 2007\u201308 Serie A season, Inter played Parma away, that had to win to not be relegated in Serie B after 18 years; Roma scored in Catania and was in the first place until Zlatan Ibrahimovi\u0107, ten minutes after having come on to the pitch in the second half, scored two goals and sealed their third consecutive championship. Mancini, however, was sacked soon after, due to his previous announcement to leave the club.\nOn 2 June 2008, Inter appointed former Porto and Chelsea boss Jos\u00e9 Mourinho as the new head coach. In his first season, the \"Nerazzurri\" won a Suppercoppa Italiana and a fourth consecutive title, though falling in the Champions League in the first knockout round for a third-straight year, losing to eventual finalists Manchester United. In winning the league title, Inter became the first club since 1949 to win the title for four consecutive seasons, and joined Torino and Juventus as the only clubs to accomplish this feat, as well as being the first club based outside Turin.\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Julio Cesar\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; L\u00facio\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Samuel\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Maicon\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Chivu\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Zanetti (C)\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Cambiasso\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Eto'o\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Pandev\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Sneijder\nem; top:-0.65em; width:10em; line-height:1.4em; white-space:nowrap\"&gt; Milito\n 2010 UEFA Champions League final starting lineup\nIn the summer of 2009, Inter laid the foundation for maybe the greatest single season of its history: after have signed Diego Milito and Thiago Motta from Genoa, L\u00facio from Bayern Munich, the club agreed to sell Ibrahimovic to Barcelona in change for Samuel Eto'o plus 49 million euros. The transfer window ended with the signing of Wesley Sneijder from Real Madrid on August 26, who three days later played against AC Milan, a game which ended in 4-0 victory. \nInter won the 2009\u201310 Champions League, defeating in round of 16 one of the favourites, Ancelotti's Chelsea, winning both legs, the latter with the first win in Stamford Bridge with a goal from Samuel Eto'o. Then, they beat CSKA Moscow and reigning champions, the Barcelona of Pep Guardiola in the semi-final, with the second leg at the Camp Nou played with ten men for most of the match; they then beat Bayern Munich 2\u20130 in the final in Madrid, with two goals from Diego Milito. In this season, Chelsea, Barcelona and Bayern all won their domestic championship. Inter also won the 2009\u201310 Serie A title by two points over Roma, the fifth title in a row, and the 2010 Coppa Italia by defeating the same side 1\u20130 in the final. This made Inter the first and only Italian team to win the treble. At the end of the season, Mourinho left the club to manage Real Madrid; he was replaced by Rafael Ben\u00edtez.\nIn the summer, Inter sold 20-year-old Balotelli to Manchester City for 29,5 million euros, the second highest for the club at that time.\nOn 21 August 2010, Inter defeated Roma 3\u20131 and won the 2010 Supercoppa Italiana, their fourth trophy of the year. In December 2010, they claimed the FIFA Club World Cup for the first time after a 3\u20130 win against Mazembe in the final, becoming for the third time world champion. However, after this win, on 23 December 2010, due to their declining performance in Serie A, the club fired Ben\u00edtez. He was replaced by Leonardo the following day. Inter was also ranked for the second time in 2010 as Best Club in the World by IFFHS.\nLeonardo started with 30 points from 12 games, with an average of 2.5 points per game, better than his predecessors Ben\u00edtez and Mourinho. On 6 March 2011, Leonardo set a new Italian Serie A record by collecting 33 points in 13 games; the previous record was 32 points in 13 games, made by Fabio Capello in the 2004\u201305 season. Leonardo led the club to the quarter-finals of the Champions League, after having defeated Bayern Munich once again in Round of 16, recovering from a 0\u20131 home defeat with a 2\u20133 win in Munich, with decisive goals from Sneijder and Goran Pandev, before subsequently losing to Schalke 04; Inter ended the season second in Serie A and won the Coppa Italia title. At the end of the season, however, he resigned, and was followed by new managers Gian Piero Gasperini, Claudio Ranieri (who qualified Inter for Round of 16 of UCL) and Andrea Stramaccioni, all hired during the following season. Inter finished sixth place in the championship, ending a Serie A record of ten consecutive qualifications for the Champions League, and their first season without a trophy since 2003\u20132004.\nChanges in ownership (2011\u20132019).\nFrom 2011 to fulfill UEFA Financial Fair Play Regulations and making the club more economically sustainable, Inter started to decrease dramatically transfers fee and team's salaries (the payrolls was decreased up to one third in two years), sold veterans with higher salaries and replaced them with younger players, that weakened the competitiveness of the team for a number of years: in August 2011, Eto'o was sold to Anzhi; in January 2012, Thiago Motta left for PSG; in the summer of 2012, Julio Cesar, Maicon and Lucio; Sneijder in January 2013.\nOn 1 August 2012, the club announced that Moratti was to sell a minority stake of the club to a Chinese consortium led by Kenneth Huang. On the same day, Inter announced an agreement was formed with China Railway Construction Corporation Limited for a new stadium project, however, the deal with the Chinese eventually collapsed. The 2012\u201313 season was the worst in recent club history, with Inter finishing ninth in Serie A and failing to qualify for any European competitions, but it was also notable as the first team to win at the new Juventus Stadium, ending the 49 match unbeaten streak of Juventus in Serie A, with two goals from Milito and one from Rodrigo Palacio. Walter Mazzarri was appointed to replace Stramaccioni as the manager for 2013\u201314 season on 24 May 2013, having ended his tenure at Napoli. He guided the club to fifth in Serie A and to 2014\u201315 UEFA Europa League qualification; after the season, the last players of 2010s treble that remained left the team: Chivu, Samuel, Zanetti, Milito and Cambiasso.\nOn 15 October 2013, an Indonesian consortium (International Sports Capital HK) led by Erick Thohir, Handy Soetedjo and Rosan Roeslani, signed an agreement to acquire 70% of Inter shares from Internazionale Holding S.r.l. Immediately after the deal, Moratti's Internazionale Holding S.r.l. still retained 29.5% of the shares of FC Internazionale Milano S.p.A. After the deal, the shares of Inter was owned by a chain of holding companies, namely International Sports Capital S.p.A. of Italy (for 70% stake), International Sports Capital HK Limited and Asian Sports Ventures HK Limited of Hong Kong. Asian Sports Ventures HK Limited, itself another intermediate holding company, was owned by Nusantara Sports Ventures HK Limited (60% stake, a company owned by Thohir), Alke Sports Investment HK Limited (20% stake) and Aksis Sports Capital HK Limited (20% stake).\nThohir, who also co-owned Major League Soccer (MLS) club D.C. United and Indonesia Super League (ISL) club Persib Bandung, announced on 2 December 2013 that Inter and D.C. United had formed a strategic partnership. During the Thohir era the club began to modify its financial structure from one reliant on continual owner investment to a more self-sustainable business model, although the club still breached UEFA Financial Fair Play Regulations in 2015. The club was fined and received a squad reduction in UEFA competitions, with additional penalties suspended during the probation period. During this time, Roberto Mancini returned as the club manager on 14 November 2014, with Inter finishing eighth. Inter finished 2015\u20132016 season fourth, failing to return to the Champions League.\nOn 6 June 2016, Suning Holdings Group (via a Luxembourg-based subsidiary Great Horizon S.\u00e1 r.l.) a company owned by Zhang Jindong, co-founder and chairman of Suning Commerce Group, acquired a majority stake of Inter from Thohir's consortium International Sports Capital S.p.A. and from Moratti family's remaining shares in Internazionale Holding S.r.l. According to various filings, the total investment from Suning was \u20ac270\u00a0million. The deal was approved by an extraordinary general meeting on 28 June 2016, from which Suning Holdings Group had acquired a 68.55% stake in the club.\nThe first season of new ownership, however, started with poor performance in pre-season friendlies. On 8 August 2016, Inter parted company with head coach Roberto Mancini by mutual consent over disagreements regarding the club's direction, especially with new signings Joao Mario for 44,75 million \u20ac (the second most expensive player in club history at that time) and Gabigol for 29,5 million \u20ac. He was replaced by Frank de Boer, who was sacked on 1 November 2016 after leading Inter to a 4W\u20132D\u20135L record in 11 Serie A games as head coach. The successor, Stefano Pioli, could not prevent the team from getting the worst group result in UEFA competitions in the club's history. Despite an eight-game winning streak, he and the club parted away before season's end, when it became clear they would finish outside the league's top three for the sixth consecutive season. On 9 June 2017, former Roma coach Luciano Spalletti was appointed as Inter manager, signing a two-year contract, and eleven months later Inter secured a UEFA Champions League group stage spot thanks to a 3\u20132 victory against Lazio in the final game of 2017\u201318 Serie A.\nAmong the best tactical moves from Spalletti, there was the change of position for Marcelo Brozovic who became one of the best defensive midfielders in European football.\nDue to this success, in August the club extended the contract with Spalletti to 2021.\nOn 4 July 2018, Inter officially signed from Racing Club de Avellaneda the 20-year-old Argentinian striker Lautaro Martinez for 25 million \u20ac, who will later become one of the best and most representative player of the club.\nOn 26 October 2018, Steven Zhang was appointed as the new president of the club, and on 13 December 2018\nGiuseppe Marotta officially joined Inter Milan as CEO for sport. On 25 January 2019, the club officially announced that LionRock Capital from Hong Kong had reached an agreement with International Sports Capital HK Limited, in order to acquire its 31.05% shares in Inter and to become the club's new minority shareholder.\nAfter the 2018\u201319 Serie A season, despite Inter finishing fourth, Spalletti was sacked.\nRenewed successes (2019\u2013present).\nOn 31 May 2019, Inter appointed former Juventus and Italian manager Antonio Conte as their new coach, signing a three-year deal; In mid 2019 Inter acquired Romelu Lukaku from Manchester United for 74 million \u20ac, the new most expensive player in the history of the club, Nicol\u00f2 Barella for 44.5 million \u20ac from Cagliari and sold Mauro Icardi, one of the best strikers in Italy in the preceding years (two times Serie A top scorer in 2015 and 2018), to Paris Saint-Germain for 50 million \u20ac. Alessandro Bastoni, who had been acquired from Atalanta in 2017 at the age of 18 for 31.1 million \u20ac, made his debut for Inter in the 2019\u201320 season and with Milan \u0160kriniar and Stefan de Vrij formed a strong defensive trio in a 3-5-2 formation that will be the best defense in Serie A in the following years.\nIn September 2019, Steven Zhang was elected to the board of the European Club Association. In the 2019\u201320 season, Inter Milan finished as runner-up, as they won 2\u20130 against Atalanta on the last matchday. After an early elimination in UCL group stage ending third behind Barcelona and Borussia Dortmund, they also reached the 2020 UEFA Europa League final played on 21 August in Cologne behind closed doors: despite two goals scored by Lukaku and Diego Godin they eventually lost 3\u20132 to Sevilla. Inter improved team with signings of new players, among others, in January 2020 Christian Eriksen from Tottenham for 27 million \u20ac and in July Achraf Hakimi from Borussia Dortmund for 43 million \u20ac.\nDespite the worst result in Champions League group stage in the club's history ending fourth with only six points, after a record 11 consecutive victories since the beginning of the second half of the season and following Inter win in Crotone and Atalanta's draw against Sassuolo on 2 May 2021, Internazionale were confirmed as champions for the first time in eleven years, ending Juventus' run of nine consecutive titles, with Zhang family that became the first foreign ownership to win serie A. However, despite securing Serie A glory, Conte left the club by mutual consent on 26 May 2021. The departure was reportedly due to disagreements between Conte and the board over player transfers. In June 2021, Simone Inzaghi was appointed as Conte's replacement. On 6 July 2021 Achraf Hakimi was sold to Paris Saint-Germain for \u20ac60\u00a0million that was replaced by Denzel Dumfries from PSV Eindhoven, on 8 August 2021 Romelu Lukaku was sold to Chelsea for \u20ac115\u00a0million, representing the most expensive association football transfer by an Italian football club ever, while Eriksen couldn't play anymore in Italy after the cardiac arrest he suffered in European Championship with the implementation of an Implantable Cardioverter Defibrillator.\nNicol\u00f2 Barella and Alessandro Bastoni were part of Italian National team that won UEFA Euro 2020 played in the summer of 2021.\nInter qualified in the UCL Round of 16 for the first time in ten years, but despite the club's first-ever win at Anfield Road thanks to a goal from Lautaro Mart\u00ednez, they were eliminated by Liverpool. On 12 January 2022, Inter won the Supercoppa Italiana, defeating Juventus 2\u20131 at San Siro. After conceding a goal to the opponent, Inter equalised with a penalty scored by Lautaro Mart\u00ednez, and the match finished 1\u20131 in regulation time. In the last second of the extra time, Alexis S\u00e1nchez scored the winning goal, giving Inter the first trophy of the season, also Simone Inzaghi's first trophy as Inter manager. On 11 May 2022, Inter won the Coppa Italia, defeating Juventus 4\u20132 at Stadio Olimpico. After normal time had ended 2\u20132, with Nicol\u00f2 Barella and Hakan \u00c7alhano\u011flu scoring Inter's goals, Ivan Peri\u0161i\u0107's brace in the extra time gave Inter the win and a second title of the season. The 2021\u201322 Serie A campaign saw Inter finish in second place, being the most prolific attacking side with 84 goals. After the autumn break for the 2022 FIFA World Cup won by Lautaro Martinez's Argentina, on 18 January 2023 Inter won the Supercoppa Italiana, defeating Milan 3\u22120 at King Fahd International Stadium, thanks to goals from Federico Dimarco, Edin D\u017eeko, and Lautaro Mart\u00ednez.\nInter passed again the UCL group stage after having eliminated Barcelona, and then after having defeated Porto and Benfica, qualified for the semifinals of the competition. On 16 May 2023, Inter defeated archrivals Milan in the semi-finals of 2022\u201323 UEFA Champions League with goals from D\u017eeko and Henrikh Mkhitaryan in the first leg and a goal from Martinez in the second leg, advanced to the Champions League final for the first time since 2010. However, they were defeated at the Atat\u00fcrk Olympic Stadium 1\u22120 by Manchester City after a second half goal from midfielder Rodri.\nIn July 2023, Inter sold for 50 million \u20ac goalkeeper Andre Onana to Manchester United, acquired the prior season for free like Hakan \u00c7alhano\u011flu in 2021, Henrikh Mkhitaryan in 2022 and Marcus Thuram in 2023. Samir Handanovi\u0107 retired after 11 seasons and 455 appearances for the club and an all-time career record in Serie A history of 26 penalties saved, Brozovic was sold to Al Nassr, \u0160kriniar moved to Paris Saint-Germain on a free transfer, while other players were added to the squad: Davide Frattesi from Sassuolo (33 million \u20ac), Benjamin Pavard (30 million \u20ac) and Yann Sommer (6.9 million \u20ac) both from Bayern Munich.\nInter started very well the season with five consecutive win including a 5 to 1 victory over AC Milan, the largest in Milan derby since 2009.\nIn January 2024 Inter won its eighth Supercoppa Italiana and its third consecutive, in a new format with four teams, tying the record set by AC Milan in the 1990s for consecutive wins, after having defeated Lazio 3\u20130 and then in the final match Napoli 1\u20130, with a late goal by Lautaro Mart\u00ednez. On 22 April 2024, Inter secured their 20th Serie A title and the second Star by defeating Milan 2\u20131 at the San Siro in a record sixth consecutive Derby della Madonnina win in a dominant season ending with 94 points, 19 over second-place Milan. The team had the best attack in the league with 89 goals made and the best defense with only 22 goals conceded, a +67 difference, the best in Serie A since the 1950\u20131951 season.\nOn 22 May 2024, Oaktree Capital Management assumed ownership of Inter Milan following the default of Suning Holdings Group on a substantial loan given in May 2021 to the club in order to cover losses incurred during the COVID-19 pandemic. The firm took control of the club after Suning Holdings Group failed to repay a debt of \u20ac395 million ($428 million). This development was confirmed by Oaktree in an emailed statement. As a consequence, the new ownership chose to appoint CEO Giuseppe Marotta as the club's new chairman.\nInter Milan began their first season under the new ownership by drawing 2\u20132 to Genoa. The 2024\u201325 season ultimately proved to be a disappointment for the Nerazzurri as despite looking likely for a treble with a month of the season to go, Inter would finish runner up to Napoli for a point in a tight Serie A season and exit the Coppa Italia in the semi finals after losing to AC Milan who also previously in January came from behind to beat them in the Supercoppa Italiana final. In the Champions League Inter finished fourth in the new league phase ensuring automatic advancement to the round of 16, with only one goal conceded in eight matches, a record in UCL League phase. Inter then beat Feyenoord 4\u20131 on aggregate in the round of 16, Bayern Munich 4\u20133 on aggregate in the quarter finals (winning in Munich ending a run of 22 matches in 4 years of unbeaten home run for the German team) and Barcelona in a thrilling 7\u20136 semi final tie: after a 3\u20133 in Barcelona in the first leg, a win for 4\u20133 in San Siro in extra time after being up 2\u20130, then down 2\u20133 until the equalizer in the last minutes in regulation time with a goal from Acerbi and then the decisive goal from Frattesi which meant Inter would make the Champions League final for the seventh time in their history and second time in three seasons. Despite having lost only once in 14 matches in the UCL this season, Inter lost the final 5\u20130 to Paris Saint-Germain ensuring a first trophy less season since the 2019\u201320 season. \nThree days after the final Simone Inzaghi left the club via mutual consent, two weeks before Inter's first game in the inaugural FIFA Club World Cup. Former Inter player Cristian Chivu was appointed as the new head coach.\nIn FIFA Club World Cup played in the United States, after a draw against Monterrey in the first match played at the Rose Bowl in Pasadena, they won the next two matches at the Lumen Field in Seattle: the first against the Urawa Red Diamonds two-one and the second against River Plate two-nil, going through the group in first place, only to be eliminated in the round of 16 of the competition.\nColours and badge.\nOne of the founders of Inter, a painter named Giorgio Muggiani, was responsible for the design of the first Inter logo in 1908. The first design incorporated the letters \"FCIM\" in the centre of a series of circles that formed the badge of the club. The basic elements of the design have remained constant even as finer details have been modified over the years. Starting from the 1999\u20132000 season, the original club crest was reduced in size, to create space for the addition of the club's name and foundation year at the upper and lower part of the logo respectively.\nIn 2007, the logo was returned to the pre-1999\u20132000 era. It was given a more modern look with a smaller \"Scudetto\" star and lighter colour scheme. This version was used until July 2014, when the club decided to undertake a rebranding. The most significant difference between the current and the previous logo is the omission of the star from other media except match kits.\nSince its founding in 1908, Inter have almost always worn black and blue stripes, earning them the nickname \"Nerazzurri\". According to the tradition, the colours were adopted to represent the nocturnal sky: in fact, the club was established on the night of 9 March, at 23:30; moreover, blue was chosen by Giorgio Muggiani because he considered it to be the opposite colour to red, worn by the Milan Cricket and Football Club rivals.\nDuring the 1928\u201329 season, however, Inter were forced by Fascist regime to abandon their black and blue uniforms. In 1928, Inter's name and philosophy made the ruling Fascist Party uneasy; as a result, during the same year the 20-year-old club was merged with \"Unione Sportiva Milanese\": the new club was named \"Societ\u00e0 Sportiva Ambrosiana\" after the patron saint of Milan. The flag of Milan (the red cross on white background) replaced the traditional black and blue. In 1929, the black-and-blue jerseys were restored, and after World War II, when the Fascists had fallen from power, the club reverted to their original name. In 2008, Inter celebrated their centenary with a red cross on their away shirt. The cross is reminiscent of the flag of their city, and they continue to use the pattern on their third kit. In 2014, the club adopted a predominantly black home kit with thin blue pinstripes before returning to a more traditional design the following season.\nAnimals are often used to represent football clubs in Italy \u2013 the grass snake, called \"Biscione\", represents Inter. The snake is a symbol for the city of Milan, appearing often in Milanese heraldry as a coiled viper with a man in its jaws. The symbol is present on the coat of arms of the House of Sforza (which ruled over Italy from Milan during the Renaissance period), the city of Milan, the historical Duchy of Milan (a 400-year state of the Holy Roman Empire) and Insubria (a historical region the city of Milan falls within). For the 2010\u201311 season, Inter's away kit featured the snake.\nStadium.\nThe team's stadium is the 75,923 seat San Siro, officially known as the \"Stadio Giuseppe Meazza\" after the former player who represented for 14 seasons Inter and for two Milan. The more commonly used name, \"San Siro\", is the name of the district where it is located. San Siro has been the home of Milan since 1926, when it was privately built by funding from Milan's chairman at the time, Piero Pirelli. Construction was performed by 120 workers, and took &lt;templatestyles src=\"Fraction/styles.css\" /&gt;13+1\u20442 months to complete. The stadium was owned by the club until it was sold to the city in 1935, and since 1947 it has been shared with Inter, when they were accepted as joint tenant.\nThe first game played at the stadium was on 19 September 1926, when Inter beat Milan 6\u20133 in a friendly match. Milan played its first league game in San Siro on 19 September 1926, losing 1\u20132 to Sampierdarenese. From an initial capacity of 35,000 spectators, the stadium has undergone several major renovations. A major structural renovation was made for the 2016 UEFA Champions League Final while another one took place in late 2021 to host the UEFA Nations League final. The stadium is going to be refurbished again in time for Milano Cortina 2026.\nBased on the English model for stadiums, San Siro is specifically designed for football matches, as opposed to many multi-purpose stadiums used in Serie A. It is therefore renowned in Italy for its atmosphere during matches, owing to the closeness of the stands to the pitch.\nNew Milano Stadium.\nSince 2012, various proposals and projects by Massimo Moratti have alternated regarding a possible construction of a new Inter stadium.\n Between June and July 2019, Inter and Milan announced the agreement for the construction of a new shared stadium in the San Siro area. In the winter of 2021, Giuseppe Sala, the mayor of Milan, gave official permission for the construction of the new stadium next to San Siro, which is expected to be partially demolished and refunctionalised after the 2026 Olympic Games. In early 2022, Inter and Milan revealed a \"plan B\" to relocate the construction of the new Milano stadium in the Greater Milan, away from the San Siro area.\nSupporters and rivalries.\nAccording to an August 2024 research by \"Ipsos\"., Inter is the second-most supported football club in Italy, only being second to Juventus. In the early years (until the First World War), Inter fans from the city of Milan were typically middle class, while Milan fans were typically working class. During Massimo Moratti's ownership, Inter fans were considered to be on the moderate left. At the same time, during Silvio Berlusconi's reign, Milan fans were viewed as belonging to the centre-right.\nThe traditional ultras group of Inter is \"Boys San\"; which are one of the oldest Italian ultras groups, being founded in 1969. Politically, one group (Irriducibili) of Inter Ultras are right-wing and this group has relations with the Lazio ultras. As well as the main group (apolitical) of \"Boys San\", there are five more significant groups: \"Viking\" (apolitical), \"Irriducibili\" (right-wing), \"Ultras\" (apolitical), \"Brianza Alcoolica\" (apolitical) and \"Imbastisci\" (left-wing).\nInter's most vocal fans gather in the Curva Nord, or north curve of the San Siro. This longstanding tradition has led to the Curva Nord being synonymous with the club's most die-hard supporters, who unfurl banners and wave flags in support of their team. Throughout 2024, the Curva Nord (labelled as the \"Curva Nord Milano\") collaborated with rap duo \u00a5$ (composed of Kanye West and Ty Dolla Sign) on multiple occasions, appearing as a choir on the chart-topping hit song \"Carnival\" (alongside rapping on its chorus) featuring Playboi Carti and Rich the Kid, and on the \u00a5$ remix of \"Like That\", featuring Future and record producer Metro Boomin.\nInter have several rivalries, two of which are highly significant in Italian football; firstly, they participate in the intracity \"Derby della Madonnina\" with Milan; the rivalry has existed ever since Inter splintered off from Milan in 1908. The name of the derby refers to the Blessed Virgin Mary atop the Milan Cathedral. The match usually creates a lively atmosphere, with numerous (often humorous or offensive) banners unfolded before the match. Flares are commonly present, but they also led to the abandonment of the second leg of the 2004\u201305 Champions League quarter-final matchup between Milan and Inter on 12 April, after a flare thrown from the crowd by an Inter supporter struck Milan keeper Dida on the shoulder.\nThe other principal rivalry is with Juventus; matches between the two clubs are known as the \"Derby d'Italia\". Up until the 2006 Italian football scandal, which saw Juventus relegated, the two were the only Italian clubs never to have played below Serie A. In the 2000s, Inter developed a rivalry with Roma, who finished as runners-up to Inter in all but one of Inter's five \"Scudetto\"-winning seasons between 2005\u201306 and 2009\u201310. The two sides have also contested in five Coppa Italia finals and four Supercoppa Italiana finals since 2006. Other clubs, like Atalanta and Napoli, are also considered among their rivals. Their supporters collectively go by \"Interisti\", or \"Nerazzurri.\"\nHonours.\nInter have won 37 domestic trophies, including the Serie A twenty times, the Coppa Italia nine times, and the Supercoppa Italiana eight times. From 2006 to 2010, the club won five successive league titles, equalling the all-time record before 2017, when Juventus won their sixth successive league title. They have won the UEFA Champions League three times: two back-to-back in 1964 and 1965, and then another in 2010; the last completed an unprecedented Italian treble with the Coppa Italia and the \"Scudetto\". The club has also won three UEFA Europa League, two Intercontinental Cup, and one FIFA Club World Cup.\nInter has never been relegated from the top flight of Italian football in its entire existence. It is the sole club to have competed in Serie A and its predecessors in every season since its debut in 1909.\nClub statistics and records.\nJavier Zanetti holds the records for both total appearances and Serie A appearances for Inter, with 858 official games played in total and 618 in Serie A.\nGiuseppe Meazza is Inter's all-time top goalscorer, with 284 goals in 408 games. Behind him, in second place, is Alessandro Altobelli with 209 goals in 466 games, and Roberto Boninsegna in third place, with 173 goals over 287 games.\nHelenio Herrera had the longest reign as Inter coach, with nine years (eight consecutive) in charge, and is the most successful coach in Inter history with three \"Scudetti\", two European Cups, and two Intercontinental Cup wins. Jos\u00e9 Mourinho, who was appointed on 2 June 2008, completed his first season in Italy by winning the Serie A title and the Supercoppa Italiana; in his second season he won the first \"treble\" in Italian history: the Serie A, Coppa Italia and the UEFA Champions League.\n \"As of 2 September 2025\"\nPlayers.\nFirst-team squad.\n&lt;templatestyles src=\"Template:Football squad player/styles.css\" /&gt;\n \"As of 30 November 2025\"\nInter Milan U23 and Youth Sector.\n&lt;templatestyles src=\"Template:Football squad player/styles.css\" /&gt;\n \"As of 1 September 2025\"\nOut on loan.\n&lt;templatestyles src=\"Template:Football squad player/styles.css\" /&gt;\nRetired numbers.\n3 \u2013 Giacinto Facchetti, left back, played for his entire career at Inter from 1960 to 1978 \"(posthumous honour)\". The number was retired on 8 September 2006, four days after Facchetti had died from cancer aged 64. The last player to wear the number 3 shirt was Argentinian center back Nicol\u00e1s Burdisso, who took on the number 16 shirt for the rest of the season.\n4 \u2013 Javier Zanetti, wing-back/full-back, played 858 games for Inter between 1995 and his retirement in the summer of 2014. In June 2014, club chairman Erick Thohir confirmed that Zanetti's number 4 was to be retired out of respect.\n\"As of 2025[ [update]]\"\nChairmen and managers.\nChairmen history.\nBelow is a list of Inter chairmen from 1908 until the present day.\nManagerial history.\nBelow is a list of Inter coaches from 1909 until the present day.\nCorporate.\nInter have varied between 6th and 20th in the Deloitte Football Money League between 1996 and 2025. They currently sit 14th in the 2023\u201324 season, with \u20ac391 million in revenues, split between matchday (\u20ac81 million), broadcasting (\u20ac198 million) and commercial (\u20ac112 million) revenues.\nMassimo Moratti took over as president of Inter in 1995, inheriting not just a majority stake but also a legacy passed down from his father, Angelo Moratti. During his tenure, Moratti invested enormous sums of his own wealth to modernize and elevate the club. Estimates suggest he spent between \u20ac600 million and \u20ac1.5 billion on player transfers alone during his time as majority owner. Marquee signings in the late 90s included Ronaldo and Christian Vieri (for a then-world-record \u20ac48 million). Moratti sold a 70% stake to a consortium led by Indonesian businessman Erick Thohir through International Sports Capital for approximately \u20ac250 million, valuing the club at around $480 million. Thohir gained majority control, while Massimo Moratti retained a minority stake of around 30%. The deal represented a change of strategy aimed at expanding the club\u2019s presence in Asian markets.\nIn June 2016, Chinese electronics retailer Suning Commerce Group acquired nearly 70% of Italian football club Inter Milan for \u20ac270 million ($307 million), marking the largest takeover of a European football club by a Chinese firm at the time. Thohir retained a minority stake of around 30% and remained club president, while Massimo Moratti, the long-time owner of Inter sold his remaining shares, fully exiting the club. As part of the transaction, Suning also agreed to assume a significant portion of the club\u2019s existing debt.\nDuring the 2020-21 season, the club posted a record loss of \u20ac245.6\u202fmillion (US\u202f$285\u202fmillion) during the COVID interrupted season, the largest ever reported by an Italian club, due to the complete loss of matchday revenue and reduced sponsorship income. As a result, US-based investment firm Oaktree Capital Management guaranteed a US$336 million loan to cover operating expenses, the sum payable back plus interest within three years.\nOn 22 May 2024, Oaktree assumed ownership of the club with Suning missing the deadline on a \u20ac395 million debt payment, resulting in Oaktree's right to take control of the club.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15117", "revid": "25859633", "url": "https://en.wikipedia.org/wiki?curid=15117", "title": "IgNobel Prize", "text": ""}
{"id": "15118", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=15118", "title": "Imbolg", "text": ""}
