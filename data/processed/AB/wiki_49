{"id": "15226", "revid": "40347300", "url": "https://en.wikipedia.org/wiki?curid=15226", "title": "Irredentism", "text": "Territorial claim\nIrredentism is one state's desire to annex the territory of another state. This desire can be motivated by ethnic reasons because the population of the territory is ethnically similar to or the same as the population of the parent state. Historical reasons may also be responsible, i.e., that the territory previously formed part of the parent state. Difficulties in applying the concept to concrete cases have given rise to academic debates about its precise definition. Disagreements concern whether either or both ethnic and historical reasons have to be present and whether non-state actors can also engage in irredentism. A further dispute is whether attempts to absorb a full neighboring state are also included. There are various types of irredentism. For typical forms of irredentism, the parent state already exists before the territorial conflict with a neighboring state arises. There are also forms of irredentism in which the parent state is newly created by uniting an ethnic group spread across several countries. Another distinction concerns whether the country to which the disputed territory currently belongs is a regular state, a former colony, or a collapsed state.\nA central research topic concerning irredentism is the question of how it is to be explained or what causes it. Many explanations hold that ethnic homogeneity within a state makes irredentism more likely. Discrimination against the ethnic group in the neighboring territory is another contributing factor. A closely related explanation argues that national identities based primarily on ethnicity, culture, and history increase irredentist tendencies. Another approach is to explain irredentism as an attempt to increase power and wealth. In this regard, it is argued that irredentist claims are more likely if the neighboring territory is relatively rich. Many explanations also focus on the regime type and hold that democracies are less likely to engage in irredentism while anocracies are particularly open to it.\nIrredentism has been an influential force in world politics since the mid-nineteenth century. It has been responsible for many armed conflicts, even though international law is hostile to it and irredentist movements often fail to achieve their goals. The term was originally coined from the Italian phrase \"Italia irredenta\" and referred to an Italian movement after 1878 claiming parts of Switzerland and the Austro-Hungarian Empire. Often discussed cases of irredentism include Nazi Germany's annexation of the Sudetenland in 1938, Somalia's invasion of Ethiopia in 1977, and Argentina's invasion of the Falkland Islands in 1982. Further examples are attempts to establish a Greater Serbia following the breakup of Yugoslavia in the early 1990s and Russia's annexation of Crimea in 2014.\nIrredentism is closely related to revanchism and secession. Revanchism is an attempt to annex territory belonging to another state. It is motivated by the goal of taking revenge for a previous grievance, in contrast to the goal of irredentism of building an ethnically unified nation-state. In the case of secession, a territory breaks away and forms an independent state instead of merging with another state.\nDefinition and etymology.\nThe term \"irredentism\" was coined from the Italian phrase (unredeemed Italy). This phrase originally referred to territory in Austria-Hungary that was mostly or partly inhabited by ethnic Italians. In particular, it applies to Trentino and Trieste, but also Gorizia, Istria, Fiume, and Dalmatia during the 19th and early 20th centuries. Irredentist projects often use the term \"Greater\" to label the desired outcome of their expansion, as in \"Greater Serbia\" or \"Greater Russia\".\nIrredentism is often understood as the claim that territories belonging to one state should be incorporated into another state because their population is ethnically similar or because it historically belonged to the other state before. Many definitions of irredentism have been proposed to give a more precise formulation. Despite a wide overlap concerning its general features, there is no consensus about its exact characterization. The disagreements matter for evaluating whether irredentism was the cause of war which is difficult in many cases and different definitions often lead to opposite conclusions.\nThere is wide consensus that irredentism is a form of territorial dispute involving the attempt to annex territories belonging to a neighboring state. However, not all such attempts constitute forms of irredentism and there is no academic consensus on precisely what other features need to be present. This concerns disagreements about who claims the territory, for what reasons they do so, and how much territory is claimed. Most scholars define irredentism as a claim made by one state on the territory of another state. In this regard, there are three essential entities to irredentism: (1) an irredentist state or parent state, (2) a neighboring host state or target state, and (3) the disputed territory belonging to the host state, often referred to as . According to this definition, popular movements demanding territorial change by non-state actors do not count as irredentist in the strict sense. A different definition characterizes irredentism as the attempt of an ethnic minority to break away and join their \"real\" motherland even though this minority is a non-state actor.\nThe reason for engaging in territorial conflict is another issue, with some scholars stating that irredentism is primarily motivated by ethnicity. In this view, the population in the neighboring territory is ethnically similar and the intention is to retrieve the area to unite the people. This definition implies, for example, that the majority of the border disputes in the history of Latin America were not forms of irredentism. Usually, irredentism is defined in terms of the motivation of the irredentist state, even if the territory is annexed against the will of the local population. Other theorists focus more on the historical claim that the disputed territory used to be part of the state's ancestral homeland. This is close to the literal meaning of the original Italian expression as \"unredeemed land\". In this view, the ethnicity of the people inhabiting this territory is not important. However, it is also possible to combine both characterizations, i.e. that the motivation is either ethnic or historical or both. Some scholars, like Benjamin Neuberger, include geographical reasons in their definitions.\nA further disagreement concerns the amount of area that is to be annexed. Usually, irredentism is restricted to the attempt to incorporate some parts of another state. In this regard, irredentism challenges established borders with the neighboring state but does not challenge the existence of the neighboring state in general. However, some definitions of irredentism also include attempts to absorb the whole neighboring state and not just a part of it. In this sense, claims by both South Korea and North Korea to incorporate the whole of the Korean Peninsula would be considered a form of irredentism.\nA popular view combining many of the elements listed above holds that irredentism is based on incongruence between the borders of a state and the boundaries of the corresponding nation. State borders are usually clearly delimited, both physically and on maps. National boundaries, on the other hand, are less tangible since they correspond to a group's perception of its historic, cultural, and ethnic boundaries. Irredentism may manifest if state borders do not correspond to national boundaries. The objective of irredentism is to enlarge a state to establish a congruence between its borders and the boundaries of the corresponding nation.\nTypes.\nVarious types of irredentism have been proposed. However, not everyone agrees that all the types listed here constitute forms of irredentism and it often depends on what definition is used. According to political theorists Naomi Chazan and Donald L. Horowitz, there are two types of irredentism. The typical case involves one state that intends to annex territories belonging to a neighboring state. Nazi Germany\u2019s claim on the Sudetenland of Czechoslovakia is an example of this form of irredentism.\nFor the second type, there is no pre-existing parent state. Instead, a cohesive group existing as a minority in multiple countries intends to unify to form a new parent state. The intended creation of a Kurdistan state uniting the Kurds living in Turkey, Syria, Iraq, and Iran is an example of the second type. If such a project is successful for only one segment, the result is secession and not irredentism. This happened, for example, during the breakup of Yugoslavia when Yugoslavian Slovenes formed the new state of Slovenia while the Austrian Slovenes did not join them and remained part of Austria. Not all theorists accept that the second type constitutes a form of irredentism. In this regard, it is often argued that it is too similar to secession to maintain a distinction between the two. For example, political scholar Benyamin Neuberger holds that a pre-existing parent state is necessary for irredentism.\nPolitical scientist Thomas Ambrosio restricts his definition to cases involving a pre-existing parent state and distinguishes three types of irredentism: (1) between two states, (2) between a state and a former colony, and (3) between a state and a collapsed state. The typical case is between two states. A textbook example of this is Somalia's invasion of Ethiopia. In the second case of decolonization, the territory to be annexed is a former colony of another state and not a regular part of it. An example is the Indonesian invasion and occupation of the former Portuguese colony of East Timor. In the case of state collapse, one state disintegrates and a neighboring state absorbs some of its former territories. This was the case for the irredentist movements by Croatia and Serbia during the breakup of Yugoslavia.\nExplanations.\nExplanations of irredentism try to determine what causes irredentism, how it unfolds, and how it can be peacefully resolved. Various hypotheses have been proposed but there is still very little consensus on how irredentism is to be explained despite its prevalence and its long history of provoking armed conflicts. Some of these proposals can be combined but others conflict with each other and the available evidence may not be sufficient to decide between them. An active research topic in this regard concerns the reasons for irredentism. Many countries have ethnic kin outside their borders. But only a few are willing to engage in violent conflicts to annex foreign territory in an attempt to unite their kin. Research on the causes of irredentism tries to explain why some countries pursue irredentism but others do not. Relevant factors often discussed include ethnicity, nationalism, economic considerations, the desire to increase power, and the type of regime.\nEthnicity and nationalism.\nA common explanation of irredentism focuses on ethnic arguments. It is based on the observation that irredentist claims are primarily advanced by states with a homogenous ethnic population. This is explained by the idea that, if a state is composed of several ethnic groups, then annexing a territory inhabited primarily by one of those groups would shift the power balance in favor of this group. For this reason, other groups in the state are likely to internally reject the irredentist claims. This inhibiting factor is not present for homogeneous states. A similar argument is also offered for the enclave to be annexed: an ethnically heterogenous enclave is less likely to desire to be absorbed by another state for ethnic reasons since this would only benefit one ethnic group. These considerations explain, for example, why irredentism is not very common in Africa since most African states are ethnically heterogeneous. Relevant factors for the ethnic motivation for irredentism are how large the dominant ethnic group is relative to other groups and how large it is in absolute terms. It also matters whether the ethnic group is relatively dispersed or located in a small core area and whether it is politically disadvantaged.\nExplanations focusing on nationalism are closely related to ethnicity-based explanations. Nationalism can be defined as the claim that the boundaries of a state should match those of the nation. According to constructivist accounts, for example, the dominant national identity is one of the central factors behind irredentism. In this view, identities based on ethnicity, culture, and history can easily invite tendencies to enlarge national borders. They may justify the goal of integrating ethnically and culturally similar territories. Civic national identities focusing more on a political nature, on the other hand, are more closely tied to pre-existing national boundaries.\nStructural accounts use a slightly different approach and focus on the relationship between nationalism and the regional context. They focus on the tension between state sovereignty and national self-determination. State sovereignty is the principle of international law holding that each state has sovereignty over its own territory. It means that states are not allowed to interfere with essentially domestic affairs of other states. National self-determination, on the other hand, concerns the right of people to determine their own international political status. According to the structural explanation, emphasis on national self-determination may legitimize irredentist claims while the principle of state sovereignty defends the status quo of the existing sovereign borders. This position is supported by the observation that irredentist conflicts are much more common during times of international upheavals.\n \nAnother factor commonly cited as a force fueling irredentism is discrimination against the main ethnic group in the enclave. Irredentist states often try to legitimize their aggression against neighbors by presenting them as humanitarian interventions aimed at protecting their discriminated ethnic kin. This justification was used, for example, in Armenia's engagement in the Nagorno-Karabakh conflict, in Serbia's involvement in the Croatian War of Independence, and in Russia's annexation of Crimea. Some political theorists, like David S. Siroky and Christopher W. Hale, hold that there is little empirical evidence for arguments based on ethnic homogeneity and discrimination. In this view, they are mainly used as a pretext to hide other goals, such as material gain.\nAnother relevant factor is the outlook of the population inhabiting the territory to be annexed. The desire of the irredentist state to annex a foreign territory and the desire of that territory to be annexed do not always overlap. In some cases, a minority group does not want to be annexed, as was the case for the Crimean Tatars in Russia's annexation of Crimea. In other cases, a minority group would want to be annexed but the intended parent state is not interested.\nPower and economy.\nVarious accounts stress the role of power and economic benefits as reasons for irredentism. Realist explanations focus on the power balance between the irredentist state and the target state: the more this power balance shifts in favor of the irredentist state, the more likely violent conflicts become. A key factor in this regard is also the reaction of the international community, i.e. whether irredentist claims are tolerated or rejected. Irredentism can be used as a tool or pretext to increase the parent state's power. Rational choice theories study how irredentism is caused by decision-making processes of certain groups within a state. In this view, irredentism is a tool used by elites to secure their political interests. They do so by appealing to popular nationalist sentiments. This can be used, for example, to gain public support against political rivals or to divert attention away from domestic problems.\nOther explanations focus on economic factors. For example, larger states enjoy advantages that come with having an increased market and decreased per capita cost of defense. However, there are also disadvantages to having a bigger state, such as the challenges that come with accommodating a wider range of citizens' preferences. Based on these lines of thought, it has been argued that states are more likely to advocate irredentist claims if the enclave is a relatively rich territory.\nRegime type.\nAn additional relevant factor is the regime type of both the irredentist state and the neighboring state. In this regard, it is often argued that democratic states are less likely to engage in irredentism. One reason cited is that democracies often are more inclusive of other ethnic groups. Another is that democracies are in general less likely to engage in violent conflicts. This is closely related to democratic peace theory, which claims that democracies try to avoid armed conflicts with other democracies. This is also supported by the observation that most irredentist conflicts are started by authoritarian regimes. However, irredentism constitutes a paradox for democratic systems. The reason is that democratic ideals pertaining to the ethnic group can often be used to justify its claim, which may be interpreted as the expression of a popular will toward unification. But there are also cases of irredentism made primarily by a government that is not broadly supported by the population.\nAccording to Siroky and Hale, anocratic regimes are most likely to engage in irredentist conflicts and to become their victim. This is based on the idea that they share some democratic ideals favoring irredentism but often lack institutional stability and accountability. This makes it more likely for the elites to consolidate their power using ethno-nationalist appeals to the masses.\nImportance, reactions, and consequences.\nIrredentism is a widespread phenomenon and has been an influential force in world politics since the mid-nineteenth century. It has been responsible for countless conflicts. There are still many unresolved irredentist disputes today that constitute discords between nations. In this regard, irredentism is a potential source of conflict in many places and often escalates into military confrontations between states. For example, international relation theorist Markus Kornprobst argues that \"no other issue over which states fight is as war-prone as irredentism\". Political scholar Rachel Walker points out that \"there is scarcely a country in the world that is not involved in some sort of irredentist quarrel ... although few would admit to this\". Political theorists Stephen M. Saideman and R. William Ayres argue that many of the most important conflicts of the 1990s were caused by irredentism, such as the wars for a Greater Serbia and a Greater Croatia. Irredentism carries a lot of potential for future conflicts since many states have kin groups in adjacent countries. It has been argued that it poses a significant danger to human security and the international order. For these reasons, irredentism has been a central topic in the field of international relations.\nFor the most part, international law is hostile to irredentism. For example, the United Nations Charter calls for respect for established territorial borders and defends state sovereignty. Similar outlooks are taken by the Organization of African Unity, the Organization of American States, and the Helsinki Final Act. Since irredentist claims are based on conflicting sovereignty assertions, it is often difficult to find a working compromise. Peaceful resolutions of irredentist conflicts often result in mutual recognition of de facto borders rather than territorial change. International relation theorists Martin Griffiths \"et al.\" argue that the threat of rising irredentism may be reduced by focusing on political pluralism and respect for minority rights.\nIrredentist movements, peaceful or violent, are rarely successful. In many cases, despite aiming to help ethnic minorities, irredentism often has the opposite effect and ends up worsening their living conditions. On the one hand, the state still in control of those territories may decide to further discriminate against them as an attempt to decrease the threat to its national security. On the other hand, the irredentist state may merely claim to care about the ethnic minorities but, in truth, use such claims only as a pretext to increase its territory or to destabilize an opponent.\nOften-discussed examples.\nThe emergence of irredentism is tied to the rise of modern nationalism and the idea of a nation-state, which are often linked to the French Revolution. However, some political scholars, like Griffiths \"et al.\", argue that phenomena similar to irredentism existed even before. For example, part of the justification for the crusades was to liberate fellow Christians from Muslim rule and to redeem the Holy Land. Nonetheless, most theorists see irredentism as a more recent phenomenon. The term was coined in the 19th century and is linked to border disputes between modern states.\nNazi Germany's annexation of the Sudetenland in 1938 is an often-cited example of irredentism. At the time, the Sudetenland formed part of Czechoslovakia but had a majority German population. Adolf Hitler justified the annexation based on his allegation that Sudeten Germans were being mistreated by the Czechoslovak government. The Sudetenland was yielded to Germany following the Munich Agreement in an attempt to prevent the outbreak of a major war.\nSomalia's invasion of Ethiopia in 1977 is frequently discussed as a case of African irredentism. The goal of this attack was to unite the significant Somali population living in the Ogaden region with their kin by annexing this area to create a Greater Somalia. The invasion escalated into a war of attrition that lasted about eight months. Somalia was close to reaching its goal but failed in the end, mainly due to an intervention by socialist countries.\nArgentina's invasion of the Falkland Islands in 1982 is cited as an example of irredentism in South America, where the Argentine military government sought to exploit national sentiment over the islands to deflect attention from domestic concerns. President Juan Per\u00f3n exploited the issue to reduce British influence in Argentina, instituting educational reform teaching the islands were Argentine and creating a strong nationalist sentiment over the issue. The war ended with a victory for the UK after about two months even though many analysts considered the Argentine military position unassailable. Although defeated, Argentina did not officially declare the cessation of hostilities until 1989 and successive Argentine governments have continued to claim the islands. The islands are now self-governing with the UK responsible for defence and foreign relations. Referendums in 1986 and 2013 show a preference for British sovereignty among the population. Both the UK and Spain claimed sovereignty in the 18th Century and Argentina claims the islands as a colonial legacy from independence in 1816.\nThe breakup of Yugoslavia in the early 1990s resulted in various irredentist projects. They include Slobodan Milo\u0161evi\u0107's attempts to establish a Greater Serbia by absorbing some regions of neighboring states that were part of former Yugoslavia. A simultaneous similar project aimed at the establishment of a Greater Croatia.\nRussia's annexation of Crimea in 2014 is a more recent example of irredentism. Beginning in the 15th century CE, the Crimean peninsula was a Tartar Khanate. However, in 1783 the Russian Empire broke a previous treaty and annexed Crimea. In 1954, when both Russia and Ukraine were part of the Soviet Union, it was transferred from Russia to Ukraine. Sixty years later, Russia alleged that the Ukrainian government did not uphold the rights of ethnic Russians inhabiting Crimea, using this as a justification for the annexation in March 2014. However, it has been claimed that this was only a pretext to increase its territory and power. Ultimately, Russia invaded the mainland territory of Ukraine in February 2022, thereby escalating the war that continues to the present day.\nOther frequently discussed cases of irredentism include disputes between Pakistan and India over Jammu and Kashmir as well as China's claims on Taiwan.\nRelated concepts.\nEthnicity.\nEthnicity plays a central role in irredentism since most irredentist states justify their expansionist agenda based on shared ethnicity. In this regard, the goal of unifying parts of an ethnic group in a common nation-state is used as a justification for annexing foreign territories and going to war if the neighboring state resists. Ethnicity is a grouping of people according to a set of shared attributes and similarities. It divides people into groups based on attributes like physical features, customs, tradition, historical background, language, culture, religion, and values. Not all these factors are equally relevant for every ethnic group. For some groups, one factor may predominate, as in ethno-linguistic, ethno-racial, and ethno-religious identities. In most cases, ethnic identities are based on a set of common features.\nA central aspect of many ethnic identities is that all members share a common homeland or place of origin. This place of origin does not have to correspond to the area where the majority of the ethnic group currently lives in case they migrated from their homeland. Another feature is a common language or dialect. In many cases, religion also forms a vital aspect of ethnicity. Shared culture is another significant factor. It is a wide term and can include characteristic social institutions, diet, dress, and other practices. It is often difficult to draw clear boundaries between people based on their ethnicity. For this reason, some definitions focus less on actual objective features and stress instead that what unites an ethnic group is a subjective belief that such common features exist. In this view, the common belief matters more than the extent to which those shared features actually exist. Examples of large ethnic groups are the Han Chinese, the Arabs, the Bengalis, the Punjabis, and the Turks.\nSome theorists, like sociologist John Milton Yinger, use terms like \"ethnic group\" or \"ethnicity\" as near-synonyms for \"nation\". Nations are usually based on ethnicity but what sets them apart from ethnicity is their political form as a state or a state-like entity. The physical and visible aspects of ethnicity, such as skin color and facial features, are often referred to as race, which may thus be understood as a subset of ethnicity. However, some theorists, like sociologist Pierre van den Berghe, contrast the two by restricting ethnicity to cultural traits and race to physical traits.\nEthnic solidarity can provide a sense of belonging as well as physical and mental security. It can help people identify with a common purpose. However, ethnicity has also been the source of many conflicts. It has been responsible for various forms of mass violence, including ethnic cleansing and genocide. The perpetrators usually form part of the ruling majority and target ethnic minority groups. Not all ethnic-based conflicts involve mass violence, like many forms of ethnic discrimination.\nNationalism and nation-state.\nIrredentism is often seen as a product of modern nationalism, i.e. the claim that a nation should have its own sovereign state. In this regard, irredentism emerged with and depends on the modern idea of nation-states. The start of modern nationalism is often associated with the French Revolution in 1789. This spawned various nationalist revolutions in Europe around the mid-nineteenth century. They often resulted in a replacement of dynastic imperial governments. A central aspect of nationalism is that it sees states as entities with clearly delimited borders that should correspond to national boundaries. Irredentism reflects the importance people ascribe to these borders and how exactly they are drawn. One difficulty in this regard is that the exact boundaries are often difficult to justify and are therefore challenged in favor of alternatives. Irredentism manifests some of the most aggressive aspects of modern nationalism. It can be seen as a side effect of nationalism paired with the importance it ascribes to borders and the difficulties in agreeing on them.\nSecession.\nIrredentism is closely related to secession. Secession can be defined as \"an attempt by an ethnic group claiming a homeland to withdraw with its territory from the authority of a larger state of which it is a part.\" Irredentism, by contrast, is initiated by members of an ethnic group in one state to incorporate territories across their border housing ethnically kindred people. Secession happens when a part of an existing state breaks away to form an independent entity. This was the case, for example, in the United States, when many of the slaveholding southern states decided to secede from the Union to form the Confederate States of America in 1861.\nIn the case of irredentism, the break-away area does not become independent but merges into another entity. Irredentism is often seen as a government decision, unlike secession. Both movements are influential phenomena in contemporary politics but, as Horowitz argues, secession movements are much more frequent in postcolonial states. However, he also holds that secession movements are less likely to succeed since they usually have very few military resources compared to irredentist states. For this reason, they normally need prolonged external assistance, often from another state. However, such state policies are subject to change. For example, the Indian government supported the Sri Lankan Tamil secessionists up to 1987 but then reach an agreement with the Sri Lankan government and helped suppress the movement.\nHorowitz holds that it is important to distinguish secessionist and irredentist movements since they differ significantly concerning their motivation, context, and goals. Despite these differences, irredentism and secessionism are closely related nonetheless. In some cases, the two tendencies may exist side by side. It is also possible that the advocates of one movement change their outlook and promote the other. Whether a movement favors irredentism or secessionism is determined, among other things, by the prospects of forming an independent state in contrast to joining another state. A further factor is whether the irredentist state is likely to espouse a similar ideology to the one found in the territory intending to break away. The anticipated reaction of the international community is an additional factor, i.e. whether it would embrace, tolerate, or reject the detachment or the absorption by another state.\nRevanchism.\nIrredentism and revanchism are closely related, differing in terms of motive. Irredentism has a positive goal of building a \"greater\" state that fulfills the ideals of a nation-state. It aims to unify people claimed to belong together through their shared national identity based on ethnicity, culture, and history.\nThe goal for revanchism is more negative because it focuses on revenge for some earlier grievance or injustice and aims to reverse territorial losses due to a previous defeat. Contrasting irredentism with revanchism, political scientist Anna M. Wittmann argues that Germany's annexation of the Sudetenland in 1938 constitutes irredentism because of its emphasis on a shared language and ethnicity, whereas Germany's invasion of Poland the following year constitutes revanchism due to its justification as revenge for previous territorial losses. The term \"revanchism\" comes from the French term , meaning \"revenge\". It was originally used in the aftermath of the Franco-Prussian War for nationalists intending to reclaim the lost territory of Alsace-Lorraine. Saddam Hussein justified the Iraqi invasion of Kuwait in 1990 by claiming Kuwait had always been an integral part of Iraq and only became an independent nation due to the interference of the British Empire.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15227", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=15227", "title": "Inuit languages", "text": "Branch of the Eskaleut language family\nThe Inuit languages are a closely related group of indigenous American languages traditionally spoken across the North American Arctic and the adjacent subarctic regions as far south as Labrador. The Inuit languages are one of the two branches of the Eskimoan language family, the other being the Yupik languages, which are spoken in Alaska and the Russian Far East. Most Inuit live in one of three countries: Greenland, a self-governing territory within the Kingdom of Denmark; Canada, specifically in Nunavut, the Inuvialuit Settlement Region of the Northwest Territories, the Nunavik region of Quebec, and the Nunatsiavut and NunatuKavut regions of Labrador; and the United States, specifically in northern and western Alaska.\nThe total population of Inuit speaking their traditional languages is difficult to assess with precision, since most counts rely on self-reported census data that may not accurately reflect usage or competence. Greenland census estimates place the number of Inuit language speakers there at roughly 50,000. According to the 2021 Canadian census, the Inuit population of Canada is 70,540, of which 33,790 report Inuit as their first language. Greenland and Canada account for the bulk of Inuit speakers, although about 7,500 Alaskans speak some variety of an Inuit language out of a total population of over 13,000 Inuit. An estimated 7,000 Greenlandic Inuit live in Denmark, the largest group outside of North America. Thus, the total population of Inuit speakers is about 100,000 people.\nAlthough they are from two different language families, Inuit also speak both Inuit Sign Language (IUR) in Canada and Greenlandic Sign Language in Greenland. It is unknown to academia if the two sign languages are related. It also remains unknown to what extent IUR is spoken across Inuit Nunangat. Finally, even though IUR is slowly being replaced by American Sign Language, there are efforts to support the native sign language underway.\nNomenclature.\nThe traditional language of the Inuit is a system of closely interrelated dialects that are not readily comprehensible from one end of the Inuit world to the other; some people do not think of it as a single language but rather a group of languages. However, there are no clear criteria for breaking the Inuit language into specific member languages since it forms a dialect continuum. Each band of Inuit understands its neighbours, and most likely its neighbours' neighbours; but at some remove, comprehensibility drops to a very low level.\nAs a result, Inuit in different places use different words for its own variants and for the entire group of languages, and this ambiguity has been carried into other languages, creating a great deal of confusion over what labels should be applied to it.\nIn Greenland the official form of Inuit language, and the official language of the state, is called \"Kalaallisut\". In other languages, it is often called \"Greenlandic\" or some cognate term. The Inuit languages of Alaska are called \"Inupiatun\", but the variants of the Seward Peninsula are distinguished from the other Alaskan variants by calling them \"Qawiaraq\", or for some dialects, \"Bering Strait Inupiatun\".\nIn Canada, the word \"Inuktitut\" is routinely used to refer to all Canadian variants of the Inuit traditional language, and it is under that name that it is recognised as one of the official languages of Nunavut and the Northwest Territories. However, one of the variants of western Nunavut, and the eastern Northwest Territories, is called \"Inuinnaqtun\" to distinguish itself from the dialects of eastern Canada, while the variants of the Northwest Territories are sometimes called \"Inuvialuktun\" and have in the past sometimes been called \"Inuktun\". In those dialects, the name is sometimes rendered as \"Inuktitun\" to reflect dialectal differences in pronunciation. The Inuit language of Quebec is called \"Inuttitut\" by its speakers, and often by other people, but this is a minor variation in pronunciation. In Labrador, the language is called \"Inuttut\" or, often in official documents, by the more descriptive name \"Labradorimiutut\". Furthermore, Canadians\u00a0\u2013 both Inuit and non-Inuit\u00a0\u2013 sometimes use the word \"Inuktitut\" to refer to \"all\" Inuit language variants, including those of Alaska and Greenland.\nThe phrase \"Inuit language\" is largely limited to professional discourse, since in each area, there is one or more conventional terms that cover all the local variants; or it is used as a descriptive term in publications where readers can't necessarily be expected to know the locally used words. In Nunavut the government groups all dialects of Inuktitut and Inuinnaqtun under the term \"Inuktut\".\nAlthough many people refer to the Inuit language as \"Eskimo language\", this is a broad term that also includes the Yupik languages, and is in addition strongly discouraged in Canada and diminishing in usage elsewhere. See the article on \"Eskimo\" for more information on this word.\nClassification and history.\nThe Inuit languages constitute a branch of the Eskimo\u2013Aleut language family. They are closely related to the Yupik languages and more remotely to Aleut. These other languages are all spoken in western Alaska, United States, and eastern Chukotka, Russia. They are not discernibly related to other indigenous languages of the Americas or northeast Asia, although there have been some unsubstantiated proposals that they are distantly related to the Uralic languages of western Siberia and northern Europe, in a proposed Uralo-Siberian grouping, or even to the Indo-European languages as part of a Nostratic superphylum. Some had previously lumped them in with the Paleosiberian languages, though that is a geographic rather than a linguistic grouping.\nEarly forms of the Inuit language are believed to have been spoken by the Thule people, who migrated east from Beringia towards the Arctic Archipelago, which had been occupied by people of the Dorset culture since the beginning of the 2nd millennium. By 1300, the Inuit and their language had reached western Greenland, and finally east Greenland roughly at the same time the Viking colonies in southern Greenland disappeared. It is generally believed that it was during this centuries-long eastward migration that the Inuit language became distinct from the Yupik languages spoken in Western Alaska and Chukotka.\nUntil 1902, a possible enclave of the Dorset, the \"Sadlermiut\" (in modern Inuktitut spelling \"Sallirmiut\"), existed on Southampton Island. Almost nothing is known about their language, but the few eyewitness accounts tell of them speaking a \"strange dialect\". This suggests that they also spoke an Inuit language, but one quite distinct from the forms spoken in Canada today.\nThe Yupik and Inuit languages are very similar syntactically and morphologically. Their common origin can be seen in a number of cognates:\nThe western Alaskan variants retain a large number of features present in proto-Inuit language and in Yup'ik, enough so that they might be classed as Yup'ik languages if they were viewed in isolation from the larger Inuit world.\nGeographic distribution and variants.\nThe Inuit languages are a fairly closely linked set of languages which can be broken up using a number of different criteria. Traditionally, Inuit describe dialect differences by means of place names to describe local idiosyncrasies in language: The dialect of Igloolik versus the dialect of Iqaluit, for example. However, political and sociological divisions are increasingly the principal criteria for describing different variants of the Inuit languages because of their links to different writing systems, literary traditions, schools, media sources and borrowed vocabulary. This makes any partition of the Inuit language somewhat problematic. This article will use labels that try to synthesise linguistic, sociolinguistic and political considerations in splitting up the Inuit dialect spectrum. This scheme is not the only one used or necessarily one used by Inuit themselves, but its labels do try to reflect the usages most seen in popular and technical literature.\nIn addition to the territories listed below, some 7,000 Greenlandic speakers are reported to live in mainland Denmark, and according to the 2001 census roughly 200 self-reported Inuktitut native speakers regularly live in parts of Canada which are outside traditional Inuit lands.\nAlaska.\nOf the roughly 13,000 Alaskan I\u00f1upiat, as few as 3000 may still be able to speak the I\u00f1upiaq, with most of them over the age of 40. Alaskan Inupiat speak three distinct dialects, which have difficult mutual intelligibility:\nCanada.\nThe Inuit languages are official in the Northwest Territories and Nunavut (the dominant language in the latter); have a high level of official support in Nunavik, a semi-autonomous portion of Quebec; and are still spoken in some parts of Labrador. Generally, Canadians refer to all dialects spoken in Canada as \"Inuktitut\", but the terms \"Inuvialuktun\", \"Inuinnaqtun\", and \"Inuttut\" (also called \"Nunatsiavummiutut\", \"Labradorimiutut\" or \"Inuttitut\") have some currency in referring to the variants of specific areas.\nGreenland.\nGreenland counts approximately 50,000 speakers of the Inuit languages, over 90% of whom speak west Greenlandic dialects at home.\nGreenlandic was strongly supported by the Danish Christian mission (conducted by the Danish state church) in Greenland. Several major dictionaries were created, beginning with Poul Egedes's (1750) and culminating with Samuel Kleinschmidt's (1871) ('The Greenlandic Dictionary'), which contained a Greenlandic grammatical system that has formed the basis of modern Greenlandic grammar. Together with the fact that until 1925 Danish was not taught in the public schools, these policies had the consequence that Greenlandic has always and continues to enjoy a very strong position in Greenland, both as a spoken as well as written language.\nPhonology and phonetics.\nEastern Canadian Inuit language variants have fifteen consonants and three vowels (which can be long or short).\nConsonants are arranged with five places of articulation: bilabial, alveolar, palatal, velar and uvular; and three manners of articulation: voiceless stops, voiced continuants, and nasals, as well as two additional sounds\u2014voiceless fricatives. The Alaskan dialects have an additional manner of articulation, the \"retroflex\", which was present in proto-Inuit language. Retroflexes have disappeared in all the Canadian and Greenlandic dialects. In Natsilingmiutut, the voiced palatal stop derives from a former retroflex.\nAlmost all Inuit language variants have only three basic vowels and make a phonological distinction between short and long forms of all vowels. The only exceptions are at the extreme edges of the Inuit world: parts of Greenland, and in western Alaska.\nMorphology and syntax.\nThe Inuit languages, like other Eskimo\u2013Aleut languages, have very rich morphological systems in which a succession of different morphemes are added to root words (like verb endings in European languages) to indicate things that, in languages like English, would require several words to express. (See also: Agglutinative language and Polysynthetic language) All Inuit words begin with a root morpheme to which other morphemes are suffixed. The language has hundreds of distinct suffixes, in some dialects as many as 700. Fortunately for learners, the language has a highly regular morphology. Although the rules are sometimes very complicated, they do not have exceptions in the sense that English and other Indo-European languages do.\nThis system makes words very long, and potentially unique. For example, in central Nunavut Inuktitut:\n&lt;templatestyles src=\"Interlinear/styles.css\" /&gt;\nThis sort of word construction is pervasive in the Inuit languages and makes them very unlike English. In one large Canadian corpus\u00a0\u2013 the \"Nunavut Hansard\"\u00a0\u2013 92% of all words appear only once, in contrast to a small percentage in most English corpora of similar size. This makes the application of Zipf's law quite difficult in the Inuit language. Furthermore, the notion of a part of speech can be somewhat complicated in the Inuit languages. Fully inflected verbs can be interpreted as nouns. The word ilisaijuq can be interpreted as a fully inflected verb: \"he studies\", but can also be interpreted as a noun: \"student\". That said, the meaning is probably obvious to a fluent speaker, when put in context.\nThe morphology and syntax of the Inuit languages vary to some degree between dialects, and the article \"Inuit grammar\" describes primarily central Nunavut dialects, but the basic principles will generally apply to all of them and to some degree to Yupik languages as well.\nVocabulary.\nToponymy and names.\nBoth the names of places and people tend to be highly prosaic when translated. \"Iqaluit\", for example, is simply the plural of the noun \"iqaluk\" \"fish\" (\"Arctic char\", \"salmon\" or \"trout\" depending on dialect). \"Igloolik\" (\"Iglulik\") means \"place with houses\", a word that could be interpreted as simply \"town\"; \"Inuvik\" is \"place of people\"; \"Baffin Island\", \"Qikiqtaaluk\" in Inuktitut, translates approximately to \"big island\".\nCommon native names in Canada include \"Ujarak\" (rock), \"Nuvuk\" (headland), \"Nasak\" (hat, or hood), \"Tupiq\" or \"Tupeq\" in Kalaallisut (tent), and \"Qajaq\" (kayak). Inuit also use animal names, traditionally believing that by using those names, they took on some of the characteristics of that animal: \"Nanuq\" or \"Nanoq\" in Kalaallisut (polar-bear), \"Uqalik\" or \"Ukaleq\" in Kalaallisut (Arctic hare), and \"Tiriaq\" or \"Teriaq\" in Kalaallisut (mouse) are favourites. In other cases, Inuit are named after dead people or people in traditional tales, by naming them after anatomical traits those people are believed to have had. Examples include \"Itigaituk\" (has no feet), \"Anana\" or \"Anaana\" (mother), \"Piujuq\" (beautiful) and \"Tulimak\" (rib). Inuit may have any number of names, given by parents and other community members.\nDisc numbers and Project Surname.\nIn the 1920s, changes in lifestyle and serious epidemics such as tuberculosis made the government of Canada interested in tracking the Inuit of Canada's Arctic. Traditionally Inuit names reflect what is important in Inuit culture: environment, landscape, seascape, family, animals, birds, spirits. However these traditional names were difficult for non-Inuit to parse. Also, the agglutinative nature of Inuit language meant that names seemed long and were difficult for southern bureaucrats and missionaries to pronounce.\nThus, in the 1940s, the Inuit were given disc numbers, recorded on a special leather ID tag, similar to a dog tag. They were required to keep the tag with them always. (Some tags are now so old and worn that the number is polished out.) The numbers were assigned with a letter prefix that indicated location (E = east), community, and then the order in which the census-taker saw the individual. In some ways this state renaming was abetted by the churches and missionaries, who viewed the traditional names and their calls to power as related to shamanism and paganism.\nThey encouraged people to take Christian names. So a young woman who was known to her relatives as \"Lutaaq, Pilitaq, Palluq, or Inusiq\" and had been baptised as \"Annie\" was under this system to become Annie E7-121. People adopted the number-names, their family members' numbers, etc., and learned all the region codes (like knowing a telephone area code).\nUntil Inuit began studying in the south, many did not know that numbers were not normal parts of Christian and English naming systems. Then in 1969, the government started Project Surname, headed by Abe Okpik, to replace number-names with patrilineal \"family surnames\".\nWords for snow.\nA popular belief exists that the Inuit have an unusually large number of words for snow. This is not accurate, and results from a misunderstanding of the nature of polysynthetic languages. In fact, the Inuit have only a few base roots for snow: 'qanniq-' ('qanik-' in some dialects), which is used most often like the verb \"to snow\", and 'aput', which means \"snow\" as a substance. Parts of speech work very differently in the Inuit language than in English, so these definitions are somewhat misleading.\nThe Inuit languages can form very long words by adding more and more descriptive affixes to words. Those affixes may modify the syntactic and semantic properties of the base word, or may add qualifiers to it in much the same way that English uses adjectives or prepositional phrases to qualify nouns (e.g. \"falling snow\", \"blowing snow\", \"snow on the ground\", \"snow drift\", etc.)\nThe \"fact\" that there are many Inuit words for snow has been put forward so often that it has become a journalistic clich\u00e9.\nNumbers.\nThe Inuit use a base-20 counting system.\nWriting.\nBecause the Inuit languages are spread over such a large area, divided between different nations and political units and originally reached by Europeans of different origins at different times, there is no uniform way of writing the Inuit language.\nCurrently there are six \"standard\" ways to write the languages:\nThough all except the syllabics use a Latin-based script, the alphabets differ in use of diacritics, non-Latin letters, etc. Most Inuktitut in Nunavut and Nunavik is written using a script called Inuktitut syllabics, based on Canadian Aboriginal syllabics. The western part of Nunavut and the Northwest Territories use a Latin-script alphabet usually identified as Inuinnaqtun. In Alaska, another Latin alphabet is used, with some characters using diacritics. Nunatsiavut uses an alphabet devised with German-speaking Moravian missionaries, which includes the letter \"kra\". Greenland's Latin alphabet was originally much like the one used in Nunatsiavut, but underwent a spelling reform in 1973 to bring the orthography in line with changes in pronunciation and better reflect the phonemic inventory of the language.\nCanadian syllabics.\nInuktitut syllabics, used in Canada, is based on Cree syllabics, devised by the missionary James Evans based on Devanagari, a Brahmi script. The present form of Canadian Inuktitut syllabics was adopted by the Inuit Cultural Institute in Canada in the 1970s.\nThough presented in syllabic form, syllabics is not a true syllabary but an abugida, since syllables starting with the same consonant are written with graphically similar letters.\nAll of the characters needed for Inuktitut syllabics are available in the Unicode character repertoire, in the blocks Unified Canadian Aboriginal Syllabics.\nInuktut Qaliujaaqpait.\nThe Canadian national organization Inuit Tapiriit Kanatami adopted Inuktut Qaliujaaqpait, a unified orthography for all varieties of Inuktitut, in September 2019. It is based on the Latin alphabet without diacritics.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15229", "revid": "36336112", "url": "https://en.wikipedia.org/wiki?curid=15229", "title": "Ibn Battuta", "text": "Maghrebi traveller and scholar (1304\u20131368/1369)\nIbn Battuta (; 24 February 1304\u00a0\u2013 1368/1369) was a Maghrebi Muslim traveller, explorer and scholar. Over a period of 30 years from 1325 to 1354, he visited much of Africa, Asia, and the Iberian Peninsula. Near the end of his life, Ibn Battuta dictated an account of his journeys, titled \"A Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling\", commonly known as \"The Rihla\". Ibn Battuta travelled more than any other explorer in pre-modern history, totalling around , surpassing Zheng He with about and Marco Polo with .\nName.\n\"Ibn Battuta\" is a patronymic, literally meaning 'son of a duckling'. His most common full name is given as Abu Abdullah Muhammad ibn Battuta. In his travelogue, \"The Rihla\", he gives his full name as \"Shams al-Din Abu \u2019Abdallah Muhammad ibn \u2019Abdallah ibn Muhammad ibn Ibrahim ibn Muhammad ibn Yusuf Lawati al-Tanji ibn Battuta\".\nEarly life.\nAll that is known about Ibn Battuta's life comes from the autobiographical information included in the account of his travels, which records that he was of Berber descent, born into a family of Islamic legal scholars (known as qadis in the Muslim traditions of Morocco) in Tangier on 24 February 1304, during the reign of the Marinid dynasty. His family belonged to a Berber tribe clan known as the Lawata. As a young man, he would have studied at a Sunni Maliki school, the dominant form of education in North Africa at that time. Maliki Muslims requested that Ibn Battuta serve as their religious judge, as he was from an area where it was practised.\nJourneys.\nItinerary, 1325\u20131332.\nFirst pilgrimage.\nOn 2 Rajab 725 AH (14 June 1325 AD), Ibn Battuta set off from his home town at the age of 21 to perform a \"hajj\" (pilgrimage) to Mecca, a journey that would ordinarily take sixteen months. He was eager to learn more about far-away lands and craved adventure. He would not return to Morocco again for 24 years.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I set out alone, having neither fellow-traveler in whose companionship I might find cheer, nor caravan whose part I might join, but swayed by an overmastering impulse within me and a desire long-cherished in my bosom to visit these illustrious sanctuaries. So I braced my resolution to quit my dear ones, female and male, and forsook my home as birds forsake their nests. My parents being yet in the bonds of life, it weighed sorely upon me to part from them, and both they and I were afflicted with sorrow at this separation.\nHe travelled to Mecca overland, following the North African coast across the sultanates of Abd al-Wadid and Hafsid. The route took him through Tlemcen, B\u00e9ja\u00efa, and then Tunis, where he stayed for two months. For safety, Ibn Battuta usually joined a caravan to reduce the risk of being robbed. He took a bride in the town of Sfax, but soon left her due to a dispute with the father. That was the first in a series of marriages that would feature in his travels.\nIn the early spring of 1326, after a journey of over , Ibn Battuta arrived at the port of Alexandria, at the time part of the Bahri Mamluk empire. He met two ascetic pious men in Alexandria. One was Sheikh Burhanuddin, who is supposed to have foretold the destiny of Ibn Battuta as a world traveller and told him, \"It seems to me that you are fond of foreign travel. You must visit my brother Fariduddin in India, Rukonuddin in Sind, and Burhanuddin in China. Convey my greetings to them.\" Another pious man, Sheikh Murshidi, interpreted a dream of Ibn Battuta as being that he was meant to be a world traveller.\nHe spent several weeks visiting sites in the area, and then headed inland to Cairo, the capital of the Mamluk Sultanate. After spending about a month in Cairo, he embarked on the first of many detours within the relative safety of Mamluk territory. Of the three usual routes to Mecca, Ibn Battuta chose the least-traveled, which involved a journey up the Nile valley, then east to the Red Sea port of \u02bfAydhab. Upon approaching the town, however, a local rebellion forced him to turn back.\nIbn Battuta returned to Cairo and took a second side trip, this time to Mamluk-controlled Damascus. During his first trip he had encountered a holy man who prophesied that he would only reach Mecca by travelling through Syria. The diversion held an added advantage; because of the holy places that lay along the way, including Hebron, Jerusalem, and Bethlehem, the Mamluk authorities kept the route safe for pilgrims. Without this help many travellers would be robbed and murdered.\nAfter spending the Muslim month of Ramadan, during August, in Damascus, he joined a caravan travelling the south to Medina, site of the Mosque of the Islamic prophet Muhammad. After four days in the town, he journeyed on to Mecca while visiting holy sites along the way; upon his arrival to Mecca he completed his first pilgrimage, in November, and he took the honorific status of \"El-Hajji\". Rather than returning home, Ibn Battuta decided to continue travelling, choosing as his next destination the Ilkhanate, a Mongol Khanate, to the northeast.\nIraq and Iran.\nOn 17 November 1326, following a month spent in Mecca, Ibn Battuta joined a large caravan of pilgrims returning to Iraq across the Arabian Peninsula. The group headed north to Medina and then, travelling at night, turned northeast across the Najd plateau to Najaf, on a journey that lasted about two weeks. In Najaf, he visited the mausoleum of Ali, the Fourth Caliph.\nThen, instead of continuing to Baghdad with the caravan, Ibn Battuta started a six-month detour that took him into Iran. From Najaf, he journeyed to Wasit, then followed the river Tigris south to Basra. His next destination was the town of Isfahan across the Zagros Mountains in Iran. He then headed south to Shiraz, a large, flourishing city spared the destruction wrought by Mongol invaders on many more northerly towns. Finally, he returned across the mountains to Baghdad, arriving there in June 1327. Parts of the city were still ruined from the damage inflicted by Hulagu Khan's invading army in 1258.\nIn Baghdad, he found Abu Sa'id, the last Mongol ruler of the unified Ilkhanate, leaving the city and heading north with a large retinue. Ibn Battuta joined the royal caravan for a while, then turned north on the Silk Road to Tabriz, the first major city in the region to open its gates to the Mongols and by then an important trading centre as most of its nearby rivals had been razed by the Mongol invaders.\nIbn Battuta left again for Baghdad, probably in July, but first took an excursion northwards along the river Tigris. He visited Mosul, where he was the guest of the Ilkhanate governor, and then the towns of Cizre (Jazirat ibn 'Umar) and Mardin in modern-day Turkey. At a hermitage on a mountain near Sinjar, he met a Kurdish mystic who gave him some silver coins. Once back in Mosul, he joined a \"feeder\" caravan of pilgrims heading south to Baghdad, where they would meet up with the main caravan that crossed the Arabian Desert to Mecca. Ill with diarrhoea, he arrived in the city weak and exhausted for his second \"hajj\".\nArabia.\nIbn Battuta remained in Mecca for some time (the \"Rihla\" suggests about three years, from September 1327 until autumn 1330). Problems with chronology, however, lead commentators to suggest that he may have left after the 1328 \"hajj\".\nAfter the \"hajj\" in either 1328 or 1330, he made his way to the port of Jeddah on the Red Sea coast. From there he followed the coast in a series of boats (known as a jalbah, these were small craft made of wooden planks sewn together, lacking an established phrase) making slow progress against the prevailing south-easterly winds. Once in Yemen he visited Zab\u012bd and later the highland town of Ta'izz, where he met the Rasulid dynasty king (\"Malik\") Mujahid Nur al-Din Ali. Ibn Battuta also mentions visiting Sanaa, but whether he actually did so is doubtful. In all likelihood, he went directly from Ta'izz to the important trading port of Aden, arriving around the beginning of 1329 or 1331.\nSomalia.\nFrom Aden, Ibn Battuta embarked on a ship heading for Zeila on the coast of Somalia. He then moved on to Cape Guardafui further down the Somali seaboard, spending about a week in each location. Later he would visit Mogadishu, the then pre-eminent city of the \"Land of the Berbers\" (\u0628\u0644\u062f \u0627\u0644\u0628\u0631\u0628\u0631 \"Balad al-Barbar\", the medieval Arabic term for the Horn of Africa).\nWhen Ibn Battuta arrived in 1332, Mogadishu stood at the zenith of its prosperity. He described it as \"an exceedingly large city\" with many rich merchants, noted for its high-quality fabric that was exported to other countries, including Egypt. Battuta added that the city was ruled by a Somali sultan, Abu Bakr ibn Shaikh 'Umar. He noted that Sultan Abu Bakr had dark skin complexion and spoke in his native tongue (Somali), but was also fluent in Arabic. The Sultan also had a retinue of wazirs (ministers), legal experts, commanders, royal eunuchs, and other officials at his beck and call.\nSwahili coast.\nIbn Battuta continued by ship south to the Swahili coast, a region then known in Arabic as the \"Bilad al-Zanj\" (\"Land of the Zanj\") with an overnight stop at the island town of Mombasa. Although relatively small at the time, Mombasa would become important in the following century. After a journey along the coast, Ibn Battuta next arrived in the island town of Kilwa in present-day Tanzania, which had become an important transit centre of the gold trade. He described the city as \"one of the finest and most beautifully built towns; all the buildings are of wood, and the houses are roofed with \"d\u012bs\" reeds\".\nIbn Battuta recorded his visit to the Kilwa Sultanate in 1330, and commented favourably on the humility and religion of its ruler, Sultan al-Hasan ibn Sulaiman, a descendant of the legendary Ali ibn al-Hassan Shirazi. He further wrote that the authority of the Sultan extended from Malindi in the north to Inhambane in the south and was particularly impressed by the planning of the city, believing it to be the reason for Kilwa's success along the coast. During this period, he described the construction of the Palace of Husuni Kubwa and a significant extension to the Great Mosque of Kilwa, which was made of coral stones and was the largest mosque of its kind. With a change in the monsoon winds, Ibn Battuta sailed back to Arabia, first to Oman and the Strait of Hormuz then on to Mecca for the \"hajj\" of 1330 (or 1332).\nItinerary 1332\u20131347.\nAnatolia.\nAfter his third pilgrimage to Mecca, Ibn Battuta decided to seek employment with the Sultan of Delhi, Muhammad bin Tughluq. In the autumn of 1330 (or 1332), he set off for the Seljuk controlled territory of Anatolia to take an overland route to India. He crossed the Red Sea and the Eastern Desert to reach the Nile valley and then headed north to Cairo. From there he crossed the Sinai Peninsula to Palestine and then travelled north again through some of the towns that he had visited in 1326. From the Syrian port of Latakia, a Genoese ship took him (and his companions) to Alanya on the southern coast of modern-day Turkey.\nHe then journeyed westwards along the coast to the port of Antalya. In the town he met members of one of the semi-religious \"fityan\" associations. These were a feature of most Anatolian towns in the 13th and 14th centuries. The members were young artisans and had at their head a leader with the title of \"Akhil\". The associations specialised in welcoming travellers. Ibn Battuta was very impressed with the hospitality that he received and would later stay in their hospices in more than 25 towns in Anatolia. From Antalya Ibn Battuta headed inland to E\u011firdir which was the capital of the Hamidids. He spent Ramadan (June 1331 or May 1333) in the city.\nFrom this point his itinerary across Anatolia in the \"Rihla\" becomes confused. Ibn Battuta describes travelling westwards from E\u011firdir to Milas and then skipping eastward past E\u011firdir to Konya. He then continues travelling in an easterly direction, reaching Erzurum from where he skips back to Birgi which lies north of Milas. Historians believe that Ibn Battuta visited a number of towns in central Anatolia, but not in the order in which he describes.\nWhen Ibn Battuta arrived in \u0130znik, it had just been conquered by Orhan, sultan of the Ottoman Beylik. Orhan was away and his wife was in command of the nearby stationed soldiers, Ibn Battuta gave this account of Orhan's wife: \"A pious and excellent woman. She treated me honourably, gave me hospitality and sent gifts.\"\nIbn Battuta's account of Orhan:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The greatest of the kings of the Turkmens and the richest in wealth, lands and military forces. Of fortresses, he possesses nearly a hundred, and for most of his time, he is continually engaged in making a round of them, staying in each fortress for some days to put it in good order and examine its condition. It is said that he has never stayed for a whole month in any one town. He also fights with the infidels continually and keeps them under siege.\u2014\u200a\nIbn Battuta had also visited Bursa which at the time was the capital of the Ottoman Beylik, he described Bursa as \"a great and important city with fine bazaars and wide streets, surrounded on all sides with gardens and running springs\".\nHe also visited the Beylik of Aydin. Ibn Battuta stated that the ruler of the Beylik of Aydin had twenty Greek slaves at the entrance of his palace and Ibn Battuta was given a Greek slave as a gift. His visit to Anatolia was the first time in his travels he acquired a servant; the ruler of Aydin gifted him his first slave. Later, he purchased a young Greek girl for 40 dinars in Ephesus, was gifted another slave in \u0130zmir by the Sultan, and purchased a second girl in Balikesir. The conspicuous evidence of his wealth and prestige continued to grow.\nCentral Asia.\nFrom Sinope, he took a sea route to the Crimean Peninsula, arriving in the Golden Horde realm. He went to the port town of Azov, where he met with the emir of the Khan, then to the large and rich city of Majar. He left Majar to meet with Uzbeg Khan's travelling court (\"Orda\"), which was at the time near Mount Beshtau. From there he made a journey to Bolghar, which became the northernmost point he reached, and noted its unusually short nights in summer (by the standards of the subtropics). Then he returned to the Khan's court and with it moved to Astrakhan.\nIbn Battuta recorded that while in Bolghar he wanted to travel further north into the land of darkness. The land is snow-covered throughout (northern Siberia) and the only means of transport is dog-drawn sled. There lived a mysterious people who were reluctant to show themselves. They traded with southern people in a peculiar way. Southern merchants brought various goods and placed them in an open area on the snow in the night, then returned to their tents. Next morning they came to the place again and found their merchandise taken by the mysterious people, but in exchange they found fur-skins which could be used for making valuable coats, jackets, and other winter garments. The trade was done between merchants and the mysterious people without seeing each other. As Ibn Battuta was not a merchant and saw no benefit of going there he abandoned the travel to this land of darkness.\nWhen they reached Astrakhan, \u00d6z Beg Khan had just given permission for one of his pregnant wives, Princess Bayalun, a daughter of Byzantine emperor Andronikos III Palaiologos, to return to her home city of Constantinople to give birth. Ibn Battuta talked his way into this expedition, which would be his first beyond the boundaries of the Islamic world.\nArriving in Constantinople towards the end of 1332 (or 1334), he met the Byzantine emperor Andronikos III Palaiologos. He visited the great church of Hagia Sophia and spoke with an Eastern Orthodox priest about his travels in the city of Jerusalem. After a month in the city, Ibn Battuta returned to Astrakhan, then arrived in the capital city Sarai al-Jadid and reported the accounts of his travels to Sultan \u00d6z Beg Khan (r. 1313\u20131341). Then he continued past the Caspian and Aral Seas to Bukhara and Samarkand, the latter of which he praised as \"one of the grandest and finest cities, and the most perfect of them\". Here he visited the court of another Mongol khan, Tarmashirin (r. 1331\u20131334) of the Chagatai Khanate. He also noted the ruined state of the city walls, a result of the Mongol invasion in 1220 and subsequent infighting. From there, he journeyed south to Afghanistan, then crossed into India via the mountain passes of the Hindu Kush. In the \"Rihla\", he mentions these mountains and the history of the range in slave trading. He wrote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;After this I proceeded to the city of Barwan, in the road to which is a high mountain, covered with snow and exceedingly cold; they call it the Hindu Kush, that is Hindu-slayer, because most of the slaves brought thither from India die on account of the intenseness of the cold.\u2014\u200a\nIbn Battuta and his party reached the Indus River on 12 September 1333. From there, he made his way to Delhi and became acquainted with the sultan, Muhammad bin Tughluq.\nIndian subcontinent.\nMuhammad bin Tughluq was renowned as the wealthiest man in the Muslim world at that time. He patronised various scholars, Sufis, qadis, viziers, and other functionaries in order to consolidate his rule. On the strength of his years of study in Mecca, Ibn Battuta was appointed a \"qadi\" (judge) by the sultan. However, he found it difficult to enforce Islamic law beyond the sultan's court in Delhi, due to lack of Islamic appeal in India.\nIt is uncertain by which route Ibn Battuta entered the Indian subcontinent but it is known that he was kidnapped and robbed by rebels on his journey to the Indian coast. He may have entered via the Khyber Pass and Peshawar, or further south. He crossed the Sutlej river near the city of Pakpattan, in modern-day Pakistan, where he paid obeisance at the shrine of Baba Farid, before crossing southwest into Rajput country. From the Rajput kingdom of Sarsatti, Battuta visited Hansi in India, describing it as \"among the most beautiful cities, the best constructed and the most populated; it is surrounded with a strong wall, and its founder is said to be one of the great non-Muslim kings, called Tara\". Upon his arrival in Sindh, Ibn Battuta mentions the Indian rhinoceros that lived on the banks of the Indus.\nThe Sultan was erratic even by the standards of the time and for six years Ibn Battuta veered between living the high life of a trusted subordinate and falling under suspicion of treason for a variety of offences. His plan to leave on the pretext of taking another \"hajj\" was stymied by the Sultan. The opportunity for Battuta to leave Delhi finally arose in 1341 when an embassy arrived from the Yuan dynasty of China asking for permission to rebuild a Himalayan Buddhist temple popular with Chinese pilgrims.\nIbn Battuta was given charge of the embassy but en route to the coast at the start of the journey to China, he and his large retinue were attacked by a group of bandits. Separated from his companions, he was robbed, kidnapped, and nearly lost his life. Despite this setback, within ten days he had caught up with his group and continued on to Khambhat in the Indian state of Gujarat. From there, they sailed to Calicut (now known as Kozhikode), where Portuguese explorer Vasco da Gama would land two centuries later. While in Calicut, Battuta was the guest of the ruling Zamorin. While Ibn Battuta visited a mosque on shore, a storm arose and one of the ships of his expedition sank. The other ship then sailed without him only to be seized by a local Sumatran king a few months later.\nAfraid to return to Delhi and be seen as a failure, he stayed for a time in southern India under the protection of Jamal-ud-Din, ruler of the small but powerful Nawayath Sultanate on the banks of the Sharavathi river next to the Arabian Sea. This area is today known as Hosapattana and lies in the Honnavar Taluk of Uttara Kannada. Following the overthrow of the sultanate, Ibn Battuta had no choice but to leave India. Although determined to continue his journey to China, he first took a detour to visit the Maldive Islands where he worked as a judge.\nHe spent nine months on the islands, much longer than he had intended. When he arrived at the capital, Mal\u00e9, Ibn Battuta did not plan to stay. However, the leaders of the formerly Buddhist nation that had recently converted to Islam were looking for a chief judge, someone who knew Arabic and the Qur'an. To convince him to stay they gave him pearls, gold jewellery, and slaves, while at the same time making it impossible for him to leave by ship. Compelled into staying, he became a chief judge and married into the royal family of Omar I.\nIbn Battuta took on his duties as a judge with keenness and strived to transform local practices to conform to a stricter application of Muslim law. He commanded that men who did not attend Friday prayer be publicly whipped, and that robbers' right hand be cut off. He forbade women from being topless in public, which had previously been the custom. However, these and other strict judgements began to antagonise the island nation's rulers, and involved him in power struggles and political intrigues. Ibn Battuta resigned from his job as chief qadi, although in all likelihood it was inevitable that he would have been dismissed.\nThroughout his travels, Ibn Battuta kept close company with women, usually taking a wife whenever he stopped for any length of time at one place, and then divorcing her when he moved on. While in the Maldives, Ibn Battuta took four wives. In his \"Travels\" he wrote that in the Maldives the effect of small dowries and female non-mobility combined to, in effect, make a marriage a convenient temporary arrangement for visiting male travellers and sailors.\nFrom the Maldives, he carried on to Sri Lanka and visited Sri Pada and Tenavaram temple. Ibn Battuta's ship almost sank on embarking from Sri Lanka, only for the vessel that came to his rescue to suffer an attack by pirates. Stranded onshore, he worked his way back to the Madurai kingdom in India. Here he spent some time in the court of the short-lived Madurai Sultanate under Ghiyas-ud-Din Muhammad Damghani, from where he returned to the Maldives and boarded a Chinese junk, still intending to reach China and take up his ambassadorial post.\nHe reached the port of Chittagong in modern-day Bangladesh intending to travel to Sylhet to meet Shah Jalal, who became so renowned that Ibn Battuta, then in Chittagong, made a one-month journey through the mountains of Kamaru near Sylhet to meet him. On his way to Sylhet, Ibn Battuta was greeted by several of Shah Jalal's disciples who had come to assist him on his journey many days before he had arrived. At the meeting in 1345 CE, Ibn Battuta noted that Shah Jalal was tall and lean, fair in complexion and lived by the mosque in a cave, where his only item of value was a goat he kept for milk, butter, and yogurt. He observed that the companions of the Shah Jalal were foreign and known for their strength and bravery. He also mentions that many people would visit the Shah to seek guidance. Ibn Battuta went further north into Assam, then turned around and continued with his original plan.\nSoutheast Asia.\nIn 1345, Ibn Battuta travelled to Samudra Pasai Sultanate (called \"al-Jawa\") in present-day Aceh, Northern Sumatra, after 40 days voyage from Sunur Kawan. He notes in his travel log that the ruler of Samudra Pasai was a pious Muslim named Sultan Al-Malik Al-Zahir Jamal-ad-Din, who performed his religious duties with utmost zeal and often waged campaigns against animists in the region. The island of Sumatra, according to Ibn Battuta, was rich in camphor, areca nut, cloves, and tin.\nThe \"madh'hab\" he observed was Imam Al-Shafi\u2018i, whose customs were similar to those he had previously seen in coastal India, especially among the Mappila Muslims, who were also followers of Imam Al-Shafi\u2018i. At that time Samudra Pasai marked the end of Dar al-Islam, because no territory east of this was ruled by a Muslim. Here he stayed for about two weeks in the wooden walled town as a guest of the sultan, and then the sultan provided him with supplies and sent him on his way on one of his own junks to China.\nIbn Battuta first sailed for 21 days to a place called \"Mul Jawa\" (island of Java or Majapahit Java) which was a center of a Hindu empire. The empire spanned 2 months of travel, and ruled over the country of Qaqula and Qamara. He arrived at the walled city named Qaqula/Kakula, and observed that the city had war junks for pirate raiding and collecting tolls and that elephants were employed for various purposes. He met the ruler of Mul Jawa and stayed as a guest for three days.\nIbn Battuta then sailed to a state called Kaylukari in the land of Tawalisi, where he met Urduja, a local princess. Urduja was a brave warrior, and her people were opponents of the Yuan dynasty. She was described as an \"idolater\", but could write the phrase Bismillah in Islamic calligraphy. The locations of Kaylukari and Tawalisi are disputed. Kaylukari might referred to Po Klong Garai in Champa (now southern Vietnam), and Urduja might be an aristocrat of Champa or Dai Viet. Filipinos widely believe that Kaylukari was in present-day Pangasinan Province of the Philippines. Their opposition to the Mongols might indicate 2 possible locations: Japan and Java (Majapahit). In modern times, Urduja has been featured in Filipino textbooks and films as a national heroine. Numerous other locations have been proposed, ranging from Java to somewhere in Guangdong Province, China. However, Sir Henry Yule and William Henry Scott consider both Tawalisi and Urduja to be entirely fictitious. From Kaylukari, Ibn Battuta finally reached Quanzhou in Fujian Province, China.\nChina.\nIn the year 1345, Ibn Battuta arrived at Quanzhou in China's Fujian province, then under the rule of the Mongol-led Yuan dynasty. One of the first things he noted was that Muslims referred to the city as \"Zaitun\" (meaning olive), but Ibn Battuta could not find any olives anywhere. He mentioned local artists and their mastery in making portraits of newly arrived foreigners; these were for security purposes. Ibn Battuta praised the craftsmen and their silk and porcelain, as well as fruits such as plums and watermelons and the advantages of paper money.\nHe described the manufacturing process of large ships in the city of Quanzhou. He also mentioned Chinese cuisine and its use of animals such as frogs, pigs, and even dogs, which were sold in the markets, and noted that the chickens in China were larger than those in the west. Scholars however have pointed out numerous errors given in Ibn Battuta's account of China, for example confusing the Yellow River with the Grand Canal and other waterways, as well as believing that porcelain was made from coal.\nIn Quanzhou, Ibn Battuta was welcomed by the head of the local Muslim merchants (possibly a f\u0101nzh\u01ceng or \"Leader of Foreigners\" ) and Sheikh al-Islam (Imam), who came to meet him with flags, drums, trumpets, and musicians. Ibn Battuta noted that the Muslim populace lived within a separate portion in the city where they had their own mosques, bazaars, and hospitals. In Quanzhou, he met two prominent Iranians, Burhan al-Din of Kazerun and Sharif al-Din from Tabriz (both of whom were influential figures noted in the \"Yuan History\" as \"A-mi-li-ding\" and \"Sai-fu-ding\", respectively). While in Quanzhou he ascended the \"Mount of the Hermit\" and briefly visited a well-known Taoist monk in a cave.\nHe then travelled south along the Chinese coast to Guangzhou, where he lodged for two weeks with one of the city's wealthy merchants.\nFrom Guangzhou he went north to Quanzhou and then proceeded to the city of Fuzhou, where he took up residence with Zahir al-Din and met Kawam al-Din and a fellow countryman named Al-Bushri of Ceuta, who had become a wealthy merchant in China. Al-Bushri accompanied Ibn Battuta northwards to Hangzhou and paid for the gifts that Ibn Battuta would present to the Emperor Huizong of Yuan.\nIbn Battuta said that Hangzhou was one of the largest cities he had ever seen, and he noted its charm, describing that the city sat on a beautiful lake surrounded by gentle green hills. He mentions the city's Muslim quarter and resided as a guest with a family of Egyptian origin. During his stay at Hangzhou he was particularly impressed by the large number of well-crafted and well-painted Chinese wooden ships, with coloured sails and silk awnings, assembling in the canals. Later he attended a banquet of the Yuan administrator of the city named Qurtai, who according to Ibn Battuta, was very fond of the skills of local Chinese conjurers. Ibn Battuta also mentions locals who worshipped a solar deity.\nHe described floating through the Grand Canal on a boat watching crop fields, orchids, merchants in black silk, and women in flowered silk and priests also in silk. In Beijing, Ibn Battuta referred to himself as the long-lost ambassador from the Delhi Sultanate and was invited to the Yuan imperial court of Emperor Huizong (who according to Ibn Battuta was worshipped by some people in China). Ibn Batutta noted that the palace of Khanbaliq was made of wood and that the ruler's \"head wife\" (Empress Qi) held processions in her honour.\nIbn Battuta also wrote he had heard of \"the rampart of Yajuj and Majuj\" that was \"sixty days' travel\" from the city of Zeitun (Quanzhou); Hamilton Alexander Rosskeen Gibb notes that Ibn Battuta believed that the Great Wall of China was built by Dhul-Qarnayn to contain Gog and Magog as mentioned in the Quran. However, Ibn Battuta, who asked about the wall in China, could find no one who had either seen it or knew of anyone who had seen it.\nIbn Battuta travelled from Beijing to Hangzhou, and then proceeded to Fuzhou. Upon his return to Quanzhou, he soon boarded a Chinese junk owned by the Sultan of Samudera Pasai Sultanate heading for Southeast Asia, whereupon Ibn Battuta was unfairly charged a hefty sum by the crew and lost much of what he had collected during his stay in China.\nBattuta claimed that the Emperor Huizong of Yuan had interred with him in his grave six slave soldiers and four girl slaves. Silver, gold, weapons, and carpets were put into the grave.\nReturn.\nAfter returning to Quanzhou in 1346, Ibn Battuta began his journey back to Morocco. In Kozhikode, he once again considered throwing himself at the mercy of Muhammad bin Tughluq in Delhi, but thought better of it and decided to carry on to Mecca. On his way to Basra he passed through the Strait of Hormuz, where he learned that Abu Sa'id, last ruler of the Ilkhanate Dynasty, had died in Iran. Abu Sa'id's territories had subsequently collapsed due to a fierce civil war between the Iranians and Mongols.\nIn 1348, Ibn Battuta arrived in Damascus with the intention of retracing the route of his first \"hajj\". He then learned that his father had died 15 years earlier, and death became the dominant theme for the next year or so. The Black Death had struck, and he stopped in Homs as the plague spread through Syria, Palestine, and Arabia. He heard of terrible death tolls in Gaza but returned to Damascus that July, where the death toll had reached 2,400 victims each day. When he stopped in Gaza, he found it was depopulated, and in Egypt he stayed at Abu Sir. Reportedly deaths in Cairo had reached levels of 1,100 each day. He made hajj to Mecca, then he decided to return to Morocco, nearly a quarter of a century after leaving home. On the way he made one last detour to Sardinia, then, in 1349, returned to Tangier by way of Fez, only to discover that his mother had also died a few months before.\nItinerary 1349\u20131354.\nSpain and North Africa.\nAfter a few days in Tangier, Ibn Battuta set out for a trip to the Muslim-controlled territory of al-Andalus on the Iberian Peninsula. King Alfonso XI of Castile and Le\u00f3n had threatened to attack Gibraltar, so in 1350, Ibn Battuta joined a group of Muslims leaving Tangier with the intention of defending the port. By the time he arrived, the Black Death had killed Alfonso and the threat of invasion had receded, so he turned the trip into a sight-seeing tour ending up in Granada.\nAfter his departure from al-Andalus he decided to travel through Morocco. On his return home, he stopped for a while in Marrakesh, which was almost a ghost town following the recent plague and the transfer of the capital to Fez.\nMali and Timbuktu.\nIn the autumn of 1351, Ibn Battuta left Fez and made his way to the town of Sijilmasa on the northern edge of the Sahara in present-day Morocco. There he bought a number of camels and stayed for four months. He set out again with a caravan in February 1352 and, after 25 days, arrived at the dry salt lake bed of Taghaza with its salt mines. All of the local buildings were made from slabs of salt by the slaves of the Masufa tribe, who cut the salt in thick slabs for transport by camel. Taghaza was a commercial centre and awash with Malian gold, though Ibn Battuta did not form a favourable impression of the place, recording that it was plagued by flies and the water was brackish.\nAfter a ten-day stay in Taghaza, the caravan set out for the oasis of Tasarahla (probably Bir al-Ksaib), where it stopped for three days in preparation for the last and most difficult leg of the journey across the vast desert. From Tasarahla, a Masufa scout was sent ahead to the oasis town of Oualata, where he arranged for water to be transported a distance of four days travel where it would meet the thirsty caravan. Oualata was the southern terminus of the trans-Saharan trade route and had recently become part of the Mali Empire. Altogether, the caravan took two months to cross the of desert from Sijilmasa.\nFrom there, Ibn Battuta travelled southwest along a river he believed to be the Nile (it was actually the Niger River), until he reached the capital of the Mali Empire. There he met \"Mansa\" Suleyman, king since 1341. Ibn Battuta disapproved of the fact that female slaves, servants, and even the daughters of the sultan went about exposing parts of their bodies not befitting a Muslim. He wrote in his \"Rihla\" that black Africans were characterised by \"ill manners\" and \"contempt for white men\", and that he \"was long astonished at their feeble intellect and their respect for mean things.\" The earliest mention of the Mande xylophone (now known as \"bala\") comes from this journey: \"made from reeds and gourds, which are struck with sticks and make a pleasant sound\". He left the capital in February accompanied by a local Malian merchant and journeyed overland by camel to Timbuktu. Though in the next two centuries it would become the most important city in the region, at that time it was a small city and relatively unimportant. It was during this journey that Ibn Battuta first encountered a hippopotamus. The animals were feared by the local boatmen and hunted with lances to which strong cords were attached. After a short stay in Timbuktu, Ibn Battuta journeyed down the Niger to Gao in a canoe carved from a single tree. At the time Gao was an important commercial center.\nAfter spending a month in Gao, Ibn Battuta set off with a large caravan for the oasis of Takedda. On his journey across the desert, he received a message from the Sultan of Morocco commanding him to return home. He set off for Sijilmasa in September 1353, accompanying a large caravan transporting 600 female slaves, and arrived back in Morocco early in 1354.\nIbn Battuta's itinerary gives scholars a glimpse as to when Islam first began to spread into the heart of west Africa.\nWorks.\nAfter returning home from his travels in 1354, and at the suggestion of the Marinid ruler of Morocco, Abu Inan Faris, Ibn Battuta dictated an account in Arabic of his journeys to Ibn Juzayy, a scholar whom he had previously met in Granada. The account is the only source for Ibn Battuta's adventures. The full title of the manuscript may be translated as \"A Masterpiece to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling\" (, \"Tu\u1e25fat an-Nu\u1e93\u1e93\u0101r f\u012b Ghar\u0101\u02beib al-Am\u1e63\u0101r wa \u02bfAj\u0101\u02beib al-Asf\u0101r\"). However, it is often simply referred to as \"TheTravels\" (, \"Rihla\"), in reference to a standard form of Arabic literature.\nThere is no indication that Ibn Battuta made any notes or had any journal during his twenty-nine years of travelling. When he came to dictate an account of his experiences he had to rely on memory and manuscripts produced by earlier travellers. Ibn Juzayy did not acknowledge his sources and presented some of the earlier descriptions as Ibn Battuta's own observations. When describing Damascus, Mecca, Medina, and some other places in the Middle East, he clearly copied passages from the account by the Andalusian Ibn Jubayr which had been written more than 150 years earlier. Similarly, most of Ibn Juzayy's descriptions of places in Palestine were copied from an account by the 13th-century traveller Muhammad al-Abdari.\nScholars do not believe that Ibn Battuta visited all the places he described and argue that in order to provide a comprehensive description of places in the Muslim world, he relied on hearsay evidence and made use of accounts by earlier travellers. For example, it is considered very unlikely that Ibn Battuta made a trip up the Volga River from New Sarai to visit Bolghar and there are serious doubts about a number of other journeys such as his trip to Sanaa in Yemen, his journey from Balkh to Bistam in Khorasan, and his trip around Anatolia.\nIbn Battuta's claim that a Maghrebian called \"Abu'l Barakat the Berber\" converted the Maldives to Islam is contradicted by an entirely different story which says that the Maldives were converted to Islam after miracles were performed by a Tabrizi named Maulana Shaikh Yusuf Shams-ud-din according to the Tarikh, the official history of the Maldives.\nSome scholars have also questioned whether he really visited China. Ibn Battuta may have plagiarised entire sections of his descriptions of China lifted from works by other authors like \"Masalik al-absar fi mamalik al-amsar\" by Shihab al-Umari, Sulaiman al-Tajir, and possibly from Al Juwayni, Rashid al din, and an Alexander romance. Furthermore, Ibn Battuta's description and Marco Polo's writings share extremely similar sections and themes, with some of the same commentary, e.g. it is unlikely that the 3rd Caliph Uthman ibn Affan had someone with the identical name in China who was encountered by Ibn Battuta.\nHowever, even if the \"Rihla\" is not fully based on what its author personally witnessed, it provides an important account of much of the 14th-century world. Concubines were used by Ibn Battuta such as in Delhi. He wedded several women, divorced at least some of them, and in Damascus, Malabar, Delhi, Bukhara, and the Maldives had children by them or by concubines. Ibn Battuta insulted Greeks as \"enemies of Allah\", drunkards and \"swine eaters\", while at the same time in Ephesus he purchased and used a Greek girl who was one of his many slave girls in his \"harem\" through Byzantium, Khorasan, Africa, and Palestine. It was two decades before he again returned to find out what happened to one of his wives and child in Damascus.\nIbn Battuta often experienced culture shock in regions he visited where the local customs of recently converted peoples did not fit in with his orthodox Muslim background. Among the Turks and Mongols, he remarked that on seeing a Turkic couple in a bazaar one might assume that the man was the woman's servant when he was in fact her husband. He also felt that dress customs in the Maldives, and some sub-Saharan regions in Africa were too revealing.\nLittle is known about Ibn Battuta's life after completion of his \"Rihla\" in 1355. He was appointed a judge in Morocco and died in 1368 or 1369.\nIbn Battuta's work was unknown outside the Muslim world until the beginning of the 19th century, when the German traveller-explorer Ulrich Jasper Seetzen (1767\u20131811) acquired a collection of manuscripts in the Middle East, among which was a 94-page volume containing an abridged version of Ibn Juzayy's text. Three extracts were published in 1818 by the German Orientalist Johann Kosegarten. A fourth extract was published the following year. French scholars were alerted to the initial publication by a lengthy review published in the \"Journal de Savants\" by the Orientalist Silvestre de Sacy.\nThree copies of another abridged manuscript were acquired by the Swiss traveller Johann Burckhardt and bequeathed to the University of Cambridge. He gave a brief overview of their content in a book published posthumously in 1819. The Arabic text was translated into English by the Orientalist Samuel Lee and published in London in 1829.\nIn the 1830s, during the French occupation of Algeria, the Biblioth\u00e8que Nationale (BNF) in Paris acquired five manuscripts of Ibn Battuta's travels, two of which were complete. One manuscript containing just the second part of the work is dated 1356 and is believed to be Ibn Juzayy's autograph. The BNF manuscripts were used in 1843 by the Irish-French orientalist Baron de Slane to produce a translation into French of Ibn Battuta's visit to the Sudan. They were also studied by the French scholars Charles Defr\u00e9mery and Beniamino Sanguinetti. Beginning in 1853 they published a series of four volumes containing a critical edition of the Arabic text together with a translation into French. In their introduction Defr\u00e9mery and Sanguinetti praised Lee's annotations but were critical of his translation which they claimed lacked precision, even in straightforward passages.\nIn 1929, exactly a century after the publication of Lee's translation, the historian and orientalist Hamilton Gibb published an English translation of selected portions of Defr\u00e9mery and Sanguinetti's Arabic text. Gibb had proposed to the Hakluyt Society in 1922 that he should prepare an annotated translation of the entire \"Rihla\" into English. His intention was to divide the translated text into four volumes, each volume corresponding to one of the volumes published by Defr\u00e9mery and Sanguinetti. The first volume was not published until 1958. Gibb died in 1971, having completed the first three volumes. The fourth volume was prepared by Charles Beckingham and published in 1994. Defr\u00e9mery and Sanguinetti's printed text has now been translated into number of other languages.\nHistoricity.\nGerman Islamic studies scholar Ralph Elger views Battuta's travel account as an important literary work but doubts the historicity of much of its content, which he suspects to be a work of fiction compiled and inspired from other contemporary travel reports. Various other scholars have raised similar doubts.\nIn 1987, Ross E. Dunn similarly expressed doubts that any evidence would be found to support the narrative of the \"Rihla\", but in 2010, Tim Mackintosh-Smith completed a multi-volume field study in dozens of the locales mentioned in the \"Rihla\", in which he reports on previously unknown manuscripts of Islamic law kept in the archives of Al-Azhar University in Cairo that were copied by Ibn Battuta in Damascus in 1326, corroborating the date in the \"Rihla\" of his sojourn in Syria.\nPresent-day cultural references.\nThe largest themed mall in Dubai, UAE, the Ibn Battuta Mall is named for him and features both areas designed to recreate the exotic lands he visited on his travels and statuary tableaus depicting scenes from his life history.\nA giant semblance of Battuta, alongside two others from the history of Arab exploration, the geographer and historian Al Bakri and the navigator and cartographer Ibn Majid, is displayed at the Mobility pavilion at Expo 2020 in Dubai in a section of the exhibition designed by Weta Workshop.\nTangier Ibn Battouta Airport is an international airport located in his hometown of Tangier, Morocco. Ibn Batouta Stadium in Tangier is also named after him.\nHe appears as a leader in \"Civilization VII\", able to lead any civilization, including the Abbasid or Songhai in the Exploration Age.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15231", "revid": "50132161", "url": "https://en.wikipedia.org/wiki?curid=15231", "title": "ISDN", "text": "Set of digital telephony standards\nIntegrated Services Digital Network (ISDN) is a set of communication standards for simultaneous digital transmission of voice, video, data, and other network services over the digitalised circuits of the public switched telephone network. Work on the standard began in 1980 at Bell Labs and was formally standardized in 1988 in the CCITT \"Red Book\". By the time the standard was released, newer networking systems with much greater speeds were available, and ISDN saw relatively little uptake in the wider market. One estimate suggests ISDN use peaked at a worldwide total of 25 million subscribers at a time when 1.3 billion analog lines were in use. ISDN has largely been replaced with digital subscriber line (DSL) systems of much higher performance.\nPrior to ISDN, the telephone system consisted of digital links like T1/E1 on the long-distance lines between telephone company offices and analog signals on copper telephone wires to the customers, the \"last mile\". At the time, the network was viewed as a way to transport voice, with some special services available for data using additional equipment like modems or by providing a T1 on the customer's location. What became ISDN started as an effort to digitize the last mile, originally under the name \"Public Switched Digital Capacity\" (PSDC). This would allow call routing to be completed in an all-digital system, while also offering a separate data line. The Basic Rate Interface, or BRI, is the standard last-mile connection in the ISDN system, offering two 64 kbit/s \"bearer\" lines and a single 16 kbit/s \"data\" channel for commands and data.\nAlthough ISDN was successful in a few countries such as Germany, on a global scale the system was largely ignored and garnered the industry nickname \"innovation(s) subscribers didn't need.\" It found a use for a time for small-office digital connection, using the voice lines for data at 64 kbit/s, sometimes \"bonded\" to 128 kbit/s, but the introduction of 56 kbit/s modems undercut its value in many roles. It also found use in videoconference systems, where the direct end-to-end connection was desirable. The H.320 standard was designed around its 64 kbit/s data rate. The underlying ISDN concepts found wider use as a replacement for the T1/E1 lines it was originally intended to extend, roughly doubling the performance of those lines.\nHistory.\nDigital lines.\nSince its introduction in 1881, the twisted pair copper line has been installed for telephone use worldwide, with well over a billion individual connections installed by the year 2000. Over the first half of the 20th century, the connection of these lines to form calls was increasingly automated, culminating in the crossbar switches that had largely replaced earlier concepts by the 1950s.\nAs telephone use surged in the post-WWII era, the problem of connecting the massive number of lines became an area of significant study. Bell Labs' seminal work on digital encoding of voice led to the use of 64 kbit/s as a standard for voice lines (or 56 kbit/s in some systems). In 1962, Robert Aaron of Bell introduced the T1 system, which carried 1.544 Mbit/s of data on a pair of twisted pair lines over a distance of about one mile. This was used in the Bell network to carry traffic between local switch offices, with 24 voice lines at 64 kbit/s and a separate 8 kbit/s line for signaling commands like connecting or hanging up a call. This could be extended over long distances using repeaters in the lines. T1 used a very simple encoding scheme, alternate mark inversion (AMI), which reached only a few percent of the theoretical capacity of the line but was appropriate for 1960s electronics.\nBy the late 1970s, T1 lines and their faster counterparts, along with all-digital switching systems, had replaced the earlier analog systems for most of the western world, leaving only the customer's equipment and their local end office using analog systems. Digitizing this \"last mile\" was increasingly seen as the next problem that needed to be solved. However, these connections now represented over 99% of the total telephony network, as the upstream links had increasingly been aggregated into a smaller number of much higher performance systems, especially after the introduction of fiber optic lines. If the system was to become all-digital, a new standard would be needed that was appropriate for the existing customer lines, which might be miles long and of widely varying quality.\nStandardization.\nAround 1978, Ralph Wyndrum, Barry Bossick and Joe Lechleider of Bell Labs began one such effort to develop a last-mile solution. They studied a number of derivatives of the T1's AMI concept and concluded that a customer-side line could reliably carry about 160 kbit/s of data over a distance of . That would be enough to carry two voice-quality lines at 64 kbit/s as well as a separate 16 kbit/s line for data. At the time, modems were normally 300 bit/s and 1200 bit/s would not become common until the early 1980s and the 2400 bit/s standard would not be completed until 1984. In this market, 16 kbit/s represented a significant advance in performance in addition to being a separate channel that coexists with voice channels.\nA key problem was that the customer might only have a single twisted pair line to the location of the handset, so the solution used in T1 with separate upstream and downstream connections was not universally available. With analog connections, the solution was to use echo cancellation, but at the much higher bandwidth of the new concept, this would not be so simple. A debate broke out between teams worldwide about the best solution to this problem; some promoted newer versions of echo cancellation, while others preferred the \"ping pong\" concept where the direction of data would rapidly switch the line from send to receive at such a high rate it would not be noticeable to the user. John Cioffi had recently demonstrated echo cancellation would work at these speeds, and further suggested that they should consider moving directly to 1.5 Mbit/s performance using this concept. The suggestion was literally laughed off the table (His boss told him to \"sit down and shut up\") but the echo cancellation concept that was taken up by Joe Lechleider eventually came to win the debate.\nMeanwhile, the debate over the encoding scheme itself was also ongoing. As the new standard was to be international, this was even more contentious as several regional digital standards had emerged in the 1960s and 70s and merging them was not going to be easy. To further confuse issues, in 1984 the Bell System was broken up and the US center for development moved to the American National Standards Institute (ANSI) T1D1.3 committee. Thomas Starr of the newly formed Ameritech led this effort and eventually convinced the ANSI group to select the 2B1Q standard proposed by Peter Adams of British Telecom. This standard used an 80\u00a0kHz base frequency and encoded two bits per baud to produce the 160 kbit/s base rate. Ultimately Japan selected a different standard, and Germany selected one with three levels instead of four, but all of these could interchange with the ANSI standard.\nFrom an economic perspective, the European Commission sought to liberalize and regulate ISDN across the European Economic Community. The Council of the European Communities adopted Council Recommendation https:// in December 1986 for its coordinated introduction within the framework of CEPT. ETSI (the European Telecommunications Standards Institute) was created by CEPT in 1988 and would develop the framework.\nRollout.\nWith digital-quality voice made possible by ISDN, offering two separate lines and continuous data connectivity, there was an initial global expectation of high customer demand for such systems in both the home and office environments. This expectation was met with varying degrees of success across different regions.\nIn the United States, many changes in the market led to the introduction of ISDN being tepid. During the lengthy standardization process, new concepts rendered the system largely superfluous. In the office, multi-line digital switches like the Meridian Norstar took over telephone lines while local area networks like Ethernet provided performance around 10 Mbit/s which had become the baseline for inter-computer connections in offices. ISDN offered no real advantages in the voice role and was far from competitive in data. Additionally, modems had continued improving, introducing 9600 bit/s systems in the late 1980s and 14.4 kbit/s in 1991, which significantly eroded ISDN's value proposition for the home customer.\nConversely, in Europe, ISDN found fertile ground for deployment, driven by regulatory support, infrastructural needs, and the absence of comparable high-speed communication technologies at the time. The technology was widely embraced for its ability to digitalize the \"last mile\" of telecommunications, significantly enhancing the quality and efficiency of voice, data, and video transmission over traditional analog systems.\nMeanwhile, Lechleider had proposed using ISDN's echo cancellation and 2B1Q encoding on existing T1 connections so that the distance between repeaters could be doubled to about . Another standards war broke out, but in 1991 Lechleider's 1.6 Mbit/s \"High-Speed Digital Subscriber Line\" eventually won this process as well, after Starr drove it through the ANSI T1E1.4 group. A similar standard emerged in Europe to replace their E1 lines, increasing the sampling range from 80 to 100\u00a0kHz to achieve 2.048 Mbit/s. By the mid-1990s, these Primary Rate Interface (PRI) lines had largely replaced T1 and E1 between telephone company offices.\nReplacement by ADSL.\nLechleider also believed this higher-speed standard would be much more attractive to customers than ISDN had proven. Unfortunately, at these speeds, the systems suffered from a type of crosstalk known as \"NEXT\", for \"near-end crosstalk\". This made longer connections on customer lines difficult. Lechleider noted that NEXT only occurred when similar frequencies were being used, and could be diminished if one of the directions used a different carrier rate, but doing so would reduce the potential bandwidth of that channel. Lechleider suggested that most consumer use would be asymmetric anyway, and that providing a high-speed channel towards the user and a lower speed return would be suitable for many uses.\nThis work in the early 1990s eventually led to the ADSL concept, which emerged in 1995. An early supporter of the concept was Alcatel, who jumped on ADSL while many other companies were still devoted to ISDN. Krish Prabu stated that \"Alcatel will have to invest one billion dollars in ADSL before it makes a profit, but it is worth it.\" They introduced the first DSL Access Multiplexers (DSLAM), the large multi-modem systems used at the telephony offices, and later introduced customer ADSL modems under the Thomson brand. Alcatel remained the primary vendor of ADSL systems for well over a decade.\nADSL quickly replaced ISDN as the customer-facing solution for last-mile connectivity. ISDN has largely disappeared on the customer side, remaining in use only in niche roles like dedicated teleconferencing systems and similar legacy systems.\nDesign.\n\"Integrated services\" refers to ISDN's ability to deliver at minimum two simultaneous connections, in any combination of data, voice, video, and fax, over a single line. Multiple devices can be attached to the line, and used as needed. That means an ISDN line can take care of what were expected to be most people's complete communications needs (apart from broadband Internet access and entertainment television) at a much higher transmission rate, without forcing the purchase of multiple analog phone lines. It also refers to integrated switching and transmission in that telephone switching and carrier wave transmission are integrated rather than separate as in earlier technology.\nConfigurations.\nIn ISDN, there are two types of channels, \"B\" (for \"bearer\") and \"D\" (for \"data\"). \"B channels\" are used for data (which may include voice), and \"D channels\" are intended for signaling and control (but can also be used for data).\nThere are two ISDN implementations. Basic Rate Interface (BRI), also called basic rate access (BRA)\u00a0\u2014 consists of two B channels, each with bandwidth of 64 kbit/s, and one D channel with a bandwidth of 16 kbit/s. Together these three channels can be designated as 2B+D. Primary Rate Interface (PRI), also called primary rate access (PRA) in Europe\u00a0\u2014 contains a greater number of B channels and a D channel with a bandwidth of 64 kbit/s. The number of B channels for PRI varies according to the nation: in North America and Japan it is 23B+1D, with an aggregate bit rate of 1.544 Mbit/s (T1); in Europe, India and Australia it is 30B+2D, with an aggregate bit rate of 2.048 Mbit/s (E1). Broadband Integrated Services Digital Network (BISDN) is another ISDN implementation and it is able to manage different types of services at the same time. It is primarily used within network backbones and employs ATM.\nAnother alternative ISDN configuration can be used in which the B channels of an ISDN BRI line are bonded to provide a total duplex bandwidth of 128 kbit/s. This precludes use of the line for voice calls while the internet connection is in use. The B channels of several BRIs can be bonded, a typical use is a 384K videoconferencing channel.\nUsing bipolar with eight-zero substitution encoding technique, call data is transmitted over the data (B) channels, with the signaling (D) channels used for call setup and management. Once a call is set up, there is a simple 64 kbit/s synchronous bidirectional data channel (actually implemented as two simplex channels, one in each direction) between the end parties, lasting until the call is terminated. There can be as many calls as there are bearer channels, to the same or different end-points. Bearer channels may also be multiplexed into what may be considered single, higher-bandwidth channels via a process called B channel BONDING, or via use of Multi-Link PPP \"bundling\" or by using an H0, H11, or H12 channel on a PRI.\nThe D channel can also be used for sending and receiving X.25 data packets, and connection to X.25 packet network, this is specified in X.31. In practice, X.31 was only commercially implemented in the UK, France, Japan and Germany.\nReference points.\nA set of \"reference points\" are defined in the ISDN standard to refer to certain points between the telco and the end-user equipment.\nMost NT-1 devices can perform the functions of the NT2 as well, and so the S and T reference points are generally collapsed into the S/T reference point.\nIn North America, the NT1 device is considered customer premises equipment (CPE) and must be maintained by the customer, thus, the U interface is provided to the customer. In other locations, the NT1 device is maintained by the telco, and the S/T interface is provided to the customer. In India, service providers provide U interface and an NT1 may be supplied by Service provider as part of service offering.\nBasic Rate Interface.\nThe entry level interface to ISDN is the Basic Rate Interface (BRI), a 128 kbit/s service delivered over a pair of standard telephone copper wires. The 144 kbit/s overall payload rate is divided into two 64 kbit/s bearer channels ('B' channels) and one 16 kbit/s signaling channel ('D' channel or data channel). This is sometimes referred to as 2B+D.\nThe interface specifies the following network interfaces:\nBRI-ISDN is very popular in Europe but is much less common in North America. It is also common in Japan\u00a0\u2014 where it is known as INS64.\nPrimary Rate Interface.\nThe other ISDN access available is the Primary Rate Interface (PRI), which is carried over T-carrier (T1) with 24 time slots (channels) in North America, and over E-carrier (E1) with 32 channels in most other countries. Each channel provides transmission at a 64 kbit/s data rate.\nWith the E1 carrier, the available channels are divided into 30 bearer (\"B\") channels, one data (\"D\") channel, and one timing and alarm channel. This scheme is often referred to as 30B+2D.\nIn North America, PRI service is delivered via T1 carriers with only one data channel, often referred to as 23B+D, and a total data rate of 1544 kbit/s. Non-Facility Associated Signalling (NFAS) allows two or more PRI circuits to be controlled by a single D channel, which is sometimes called \"23B+D + n*24B\". D-channel backup allows for a second D channel in case the primary fails. NFAS is commonly used on a Digital Signal 3 (DS3/T3).\nPRI-ISDN is popular throughout the world, especially for connecting private branch exchanges to the public switched telephone network (PSTN).\nEven though many network professionals use the term \"ISDN\" to refer to the lower-bandwidth BRI circuit, in North America BRI is relatively uncommon whilst PRI circuits serving PBXs are commonplace.\nBearer channel.\nThe bearer channel (B) is a standard 64 kbit/s voice channel of 8 bits sampled at 8\u00a0kHz with G.711 encoding. B-channels can also be used to carry data, since they are nothing more than digital channels.\nEach one of these channels is known as a DS0.\nMost B channels can carry a 64 kbit/s signal, but some were limited to 56K because they traveled over RBS lines. This was commonplace in the 20th century, but has since become less so.\nX.25.\nX.25 can be carried over the B or D channels of a BRI line, and over the B channels of a PRI line. X.25 over the D channel is used at many point-of-sale (credit card) terminals because it eliminates the modem setup, and because it connects to the central system over a B channel, thereby eliminating the need for modems and making much better use of the central system's telephone lines.\nX.25 was also part of an ISDN protocol called \"Always On/Dynamic ISDN\", or AO/DI. This allowed a user to have a constant multi-link PPP connection to the internet over X.25 on the D channel, and brought up one or two B channels as needed.\nFrame Relay.\nIn theory, Frame Relay can operate over the D channel of BRIs and PRIs, but it is seldom, if ever, used.\nUses.\nTelephone industry.\nISDN is a core technology in the telephone industry. A telephone network can be thought of as a collection of wires strung between switching systems. The common electrical specification for the signals on these wires is T1 or E1. Between telephone company switches, the signaling is performed via SS7. Normally, a PBX is connected via a T1 with robbed bit signaling to indicate on-hook or off-hook conditions and MF and DTMF tones to encode the destination number. ISDN is much better because messages can be sent much more quickly than by trying to encode numbers as long (100 ms per digit) tone sequences. This results in faster call setup times. Also, a greater number of features are available and fraud is reduced.\nIn common use, ISDN is often limited to usage to Q.931 and related protocols, which are a set of signaling protocols establishing and breaking circuit-switched connections, and for advanced calling features for the user. Another usage was the deployment of videoconference systems, where a direct end-to-end connection is desirable. ISDN uses the H.320 standard for audio coding and video coding.\nISDN is also used as a smart-network technology intended to add new services to the public switched telephone network (PSTN) by giving users direct access to end-to-end circuit-switched digital services and as a backup or failsafe circuit solution for critical use data circuits.\nVideo conferencing.\nOne of ISDNs successful use-cases was in the videoconference field, where even small improvements in data rates are useful, but more importantly, its direct end-to-end connection offers lower latency and better reliability than packet-switched networks of the 1990s. The H.320 standard for audio coding and video coding was designed with ISDN in mind, and more specifically its 64 kbit/s basic data rate. including audio codecs such as G.711 (PCM) and G.728 (CELP), and discrete cosine transform (DCT) video codecs such as H.261 and H.263.\nBroadcast industry.\nISDN is used heavily by the broadcast industry as a reliable way of switching low-latency, high-quality, long-distance audio circuits. In conjunction with an appropriate codec using MPEG or various manufacturers' proprietary algorithms, an ISDN BRI can be used to send stereo bi-directional audio coded at 128 kbit/s with 20\u00a0Hz\u00a0\u2013 20\u00a0kHz audio bandwidth, although commonly the G.722 algorithm is used with a single 64 kbit/s B channel to send much lower latency mono audio at the expense of audio quality. Where very high quality audio is required multiple ISDN BRIs can be used in parallel to provide a higher bandwidth circuit switched connection. BBC Radio 3 commonly makes use of three ISDN BRIs to carry 320 kbit/s audio stream for live outside broadcasts. ISDN BRI services are used to link remote studios, sports grounds and outside broadcasts into the main broadcast studio. ISDN via satellite is used by field reporters around the world. It is also common to use ISDN for the return audio links to remote satellite broadcast vehicles.\nIn many countries, such as the UK and Australia, ISDN has displaced the older technology of equalised analogue landlines, with these circuits being phased out by telecommunications providers. Use of IP-based streaming codecs such as Comrex ACCESS and ipDTL is becoming more widespread in the broadcast sector, using broadband internet to connect remote studios.\nBackup lines.\nProviding a backup line for business's inter-office and internet connectivity was a popular use of the technology.\nInternational deployment.\nA study of the Germany's Federal Ministry of Education and Research shows the following share of ISDN-channels per 1,000 inhabitants in 2005:\nAustralia.\nTelstra provides the business customer with the ISDN services. There are five types of ISDN services which are ISDN2, ISDN2 Enhanced, ISDN10, ISDN20 and ISDN30. Telstra changed the minimum monthly charge for voice and data calls. In general, there are two group of ISDN service types; The Basic Rate services \u2013 ISDN 2 or ISDN 2 Enhanced. Another group of types are the Primary Rate services, ISDN 10/20/30. Telstra announced that the new sales of ISDN product would be unavailable as of 31 January 2018. The final exit date of ISDN service and migration to the new service was on 31 May 2022.\nFrance.\nOrange offers ISDN services under their product name Numeris (2 B+D), of which a professional Duo and home Itoo version is available. ISDN is generally known as RNIS in France and has widespread availability. The introduction of ADSL is reducing ISDN use for data transfer and Internet access, although it is still common in more rural and outlying areas, and for applications such as business voice and point-of-sale terminals. In 2023, Numeris services will enter a phase-out process. They will be replaced by VoIP services.\nGermany.\nIn Germany, ISDN was very popular with an installed base of 25 million channels (29% of all subscriber lines in Germany as of 2003 and 20% of all ISDN channels worldwide). Due to the success of ISDN, the number of installed analog lines was decreasing. Deutsche Telekom (DTAG) offered both BRI and PRI. Competing phone companies often offered ISDN only and no analog lines. However, these operators generally offered free hardware that also allows the use of POTS equipment, such as NTBAs (\"\": small devices that bridge the two-wire UK0 line to the four-wire S0 bus) with integrated terminal adapters. Because of the widespread availability of ADSL services, ISDN was primarily used for voice and fax traffic.\nUntil 2007 ISDN (BRI) and ADSL/VDSL were often bundled on the same line, mainly because the combination of DSL with an analog line had no cost advantage over a combined ISDN-DSL line. This practice turned into an issue for the operators when vendors of ISDN technology stopped manufacturing it and spare parts became hard to come by. Since then phone companies started introducing cheaper xDSL-only products using VoIP for telephony, also in an effort to reduce their costs by operating separate data &amp; voice networks.\nSince approximately 2010, most German operators have offered more and more VoIP on top of DSL lines and ceased offering ISDN lines. New ISDN lines have been no longer available in Germany since 2018, existing ISDN lines were phased out from 2016 onwards and existing customers were encouraged to move to DSL-based VoIP products.\nDeutsche Telekom intended to phase-out by 2018 but announced the complete transition in 2020. Other providers like Vodafone estimate to have their phase-out completed by 2022.\nGreece.\nOTE, the incumbent telecommunications operator, offers ISDN BRI (BRA) services in Greece. Following the launch of ADSL in 2003, the importance of ISDN for data transfer began to decrease and is today limited to niche business applications with point-to-point requirements.\nIndia.\nBharat Sanchar Nigam Limited, Reliance Communications and Bharti Airtel are the largest communication service providers, and offer both ISDN BRI and PRI services across the country. Reliance Communications and Bharti Airtel uses the DLC technology for providing these services. With the introduction of broadband technology, the load on bandwidth is being absorbed by ADSL. ISDN continues to be an important backup network for point-to-point leased line customers such as banks, e-Seva Centers, Life Insurance Corporation of India, and SBI ATMs.\nJapan.\nOn April 19, 1988, Japanese telecommunications company NTT began offering nationwide ISDN services trademarked INS Net 64, and INS Net 1500, a fruition of NTT's independent research and trial from the 1970s of what it referred to the INS (Information Network System).\nPreviously, in April 1985, Japanese digital telephone exchange hardware made by Fujitsu was used to experimentally deploy the world's first I interface ISDN. The I interface, unlike the older and incompatible Y interface, is what modern ISDN services use today.\nSince 2000, NTT's ISDN offering have been known as ISDN, incorporating the \"FLET's\" brand that NTT uses for all of its ISP offerings.\nIn Japan, the number of ISDN subscribers dwindled as alternative technologies such as ADSL, cable Internet access, and fiber to the home gained greater popularity. On November 2, 2010, NTT announced plans to migrate their backend from PSTN to the IP network from around 2020 to around 2025. For this migration, ISDN services will be retired, and fiber optic services are recommended as an alternative.\nNorway.\nOn April 19, 1988, Norwegian telecommunications company Telenor began offering nationwide ISDN services trademarked INS Net 64, and INS Net 1500, a fruition of NTT's independent research and trial from the 1970s of what it referred to the INS (Information Network System).\nUnited Kingdom.\nIn the United Kingdom, British Telecom (BT) provides ISDN2e (BRI) as well as ISDN30 (PRI). Until April 2006, they also offered services named Home Highway and Business Highway, which were BRI ISDN-based services that offered integrated analogue connectivity as well as ISDN. Later versions of the Highway products also included built-in USB sockets for direct computer access. Home Highway was bought by many home users, usually for Internet connection, although not as fast as ADSL, because it was available before ADSL and in places where ADSL does not reach.\nIn early 2015, BT announced their intention to retire the UK's ISDN infrastructure by 2025.\nUnited States and Canada.\nISDN-BRI never gained popularity as a general use telephone access technology in Canada and the US, and remains a niche product. The service was seen as \"a solution in search of a problem\", and the extensive array of options and features were difficult for customers to understand and use. ISDN has long been known by derogatory backronyms highlighting these issues, such as \"It Still Does Nothing\", \"Innovations Subscribers Don't Need\", and \"I Still Don't kNow\", or, from the supposed standpoint of telephone companies, \"I Smell Dollars Now\".\nAlthough various minimum bandwidths have been used in definitions of Broadband Internet access, ranging up from 64 kbit/s up to 1.0 Mbit/s, the 2006 OECD report is typical by defining broadband as having download data transfer rates equal to or faster than 256 kbit/s, while the United States FCC, as of 2008, defines broadband as anything above 768 kbit/s. Once the term \"broadband\" came to be associated with data rates incoming to the customer at 256 kbit/s or more, and alternatives like ADSL grew in popularity, the consumer market for BRI did not develop. Its only remaining advantage is that, while ADSL has a functional distance limitation and can use ADSL loop extenders, BRI has a greater limit and can use repeaters. As such, BRI may be acceptable for customers who are too remote for ADSL. Widespread use of BRI is further stymied by some small North American CLECs such as CenturyTel having given up on it and not providing Internet access using it. However, AT&amp;T in most states (especially the former SBC/SWB territory) will still install an ISDN BRI line anywhere a normal analog line can be placed.\nISDN-BRI is currently primarily used in industries with specialized and very specific needs. High-end videoconferencing hardware can bond up to 8 B-channels together (using a BRI circuit for every 2 channels) to provide digital, circuit-switched video connections to almost anywhere in the world. This is very expensive, and is being replaced by IP-based conferencing, but where cost concern is less of an issue than predictable quality and where a QoS-enabled IP does not exist, BRI is the preferred choice.\nMost modern non-VoIP PBXs use ISDN-PRI circuits. These are connected via T1 lines with the central office switch, replacing older analog two-way and direct inward dialing (DID) trunks. PRI is capable of delivering Calling Line Identification (CLID) in both directions so that the telephone number of an extension, rather than a company's main number, can be sent. It is still commonly used in recording studios and some radio programs, when a voice-over actor or host is in one studio conducting remote work, but the director and producer are in a studio at another location. The ISDN protocol delivers channelized, not-over-the-Internet service, powerful call setup and routing features, faster setup and tear down, superior audio fidelity as compared to plain old telephone service (POTS), lower delay and, at higher densities, lower cost.\nIn 2013, Verizon announced it would no longer take orders for ISDN service in the Northeastern United States, signalling the beginning of the end for the technology in that region.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15235", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15235", "title": "Genomic imprinting", "text": "Expression of genes depending on parentage\nGenomic imprinting is an epigenetic phenomenon that causes genes to be expressed or not, depending on whether they are inherited from the female or male parent. Genes can also be partially imprinted. Partial imprinting occurs when alleles from both parents are differently expressed rather than complete expression and complete suppression of one parent's allele. Forms of genomic imprinting have been demonstrated in fungi, plants and animals. In 2014, there were about 150 imprinted genes known in mice and about half that in humans. As of 2019, 260 imprinted genes have been reported in mice and 228 in humans.\nGenomic imprinting is an inheritance process independent of the classical Mendelian inheritance. It is an epigenetic process that involves DNA methylation and histone methylation without altering the genetic sequence. These epigenetic marks are established (\"imprinted\") in the germline (sperm or egg cells) of the parents and are maintained through mitotic cell divisions in the somatic cells of an organism.\nAppropriate imprinting of certain genes is important for normal development. Human diseases involving genomic imprinting include Angelman, Prader\u2013Willi, and Beckwith\u2013Wiedemann syndromes. Methylation defects have also been associated with male infertility.\nOverview.\nIn diploid organisms (like humans), the somatic cells possess two copies of the genome, one inherited from the male and one from the female. Each autosomal gene is therefore represented by two copies, or alleles, with one copy inherited from each parent at fertilization. The expressed allele is dependent upon its parental origin. For example, the gene encoding insulin-like growth factor 2 (IGF2/Igf2) is only expressed from the allele inherited from the male. Although imprinting accounts for a small proportion of mammalian genes, they play an important role in embryogenesis particularly in the formation of visceral structures and the nervous system.\nThe term \"imprinting\" was first used to describe events in the insect \"Pseudococcus nipae\". In Pseudococcids (mealybugs) (Hemiptera, Coccoidea) both the male and female develop from a fertilised egg. In females, all chromosomes remain euchromatic and functional. In embryos destined to become males, one haploid set of chromosomes becomes heterochromatinised after the sixth cleavage division and remains so in most tissues; males are thus functionally haploid.\nImprinted genes in mammals.\nThat imprinting might be a feature of mammalian development was suggested in breeding experiments in mice carrying reciprocal chromosomal translocations. Nucleus transplantation experiments in mouse zygotes in the early 1980s confirmed that normal development requires the contribution of both the maternal and paternal genomes. The vast majority of mouse embryos derived from parthenogenesis (called parthenogenones, with two maternal or egg genomes) and androgenesis (called androgenones, with two paternal or sperm genomes) die at or before the blastocyst/implantation stage. In the rare instances that they develop to postimplantation stages, gynogenetic embryos show better embryonic development relative to placental development, while for androgenones, the reverse is true. Nevertheless, for the latter, only a few have been described (in a 1984 paper). Nevertheless, in 2018 genome editing allowed for bipaternal and viable bimaternal mouse and even (in 2022) parthenogenesis, still this is far from full reimprinting. Finally in March 2023 viable bipaternal embryos were created.\nNo naturally occurring cases of parthenogenesis exist in mammals because of imprinted genes. However, in 2004, experimental manipulation by Japanese researchers of a paternal methylation imprint controlling the \"Igf2\" gene led to the birth of a mouse (named Kaguya) with two maternal sets of chromosomes, though it is not a true parthenogenone since cells from two different female mice were used. The researchers were able to succeed by using one egg from an immature parent, thus reducing maternal imprinting, and modifying it to express the gene Igf2, which is normally only expressed by the paternal copy of the gene.\nParthenogenetic/gynogenetic embryos have twice the normal expression level of maternally derived genes, and lack expression of paternally expressed genes, while the reverse is true for androgenetic embryos. It is now known that there are at least 80 imprinted genes in humans and mice, many of which are involved in embryonic and placental growth and development. Hybrid offspring of two species may exhibit unusual growth due to the novel combination of imprinted genes.\nVarious methods have been used to identify imprinted genes. In swine, Bischoff \"et al.\" compared transcriptional profiles using DNA microarrays to survey differentially expressed genes between parthenotes (2 maternal genomes) and control fetuses (1 maternal, 1 paternal genome). An intriguing study surveying the transcriptome of murine brain tissues revealed over 1300 imprinted gene loci (approximately 10-fold more than previously reported) by RNA-sequencing from F1 hybrids resulting from reciprocal crosses. The result however has been challenged by others who claimed that this is an overestimation by an order of magnitude due to flawed statistical analysis.\nIn domesticated livestock, single-nucleotide polymorphisms in imprinted genes influencing foetal growth and development have been shown to be associated with economically important production traits in cattle, sheep and pigs.\nGenetic mapping of imprinted genes.\nAt the same time as the generation of the gynogenetic and androgenetic embryos discussed above, mouse embryos were also being generated that contained only small regions that were derived from either a paternal or maternal source. The generation of a series of such uniparental disomies, which together span the entire genome, allowed the creation of an imprinting map. Those regions which when inherited from a single parent result in a discernible phenotype contain imprinted gene(s). Further research showed that within these regions there were often numerous imprinted genes. Around 80% of imprinted genes are found in clusters such as these, called imprinted domains, suggesting a level of co-ordinated control. More recently, genome-wide screens to identify imprinted genes have used differential expression of mRNAs from control fetuses and parthenogenetic or androgenetic fetuses hybridized to gene expression profiling microarrays, allele-specific gene expression using SNP genotyping microarrays, transcriptome sequencing, and in silico prediction pipelines.\nImprinting mechanisms.\nImprinting is a dynamic process. It must be possible to erase and re-establish imprints through each generation so that genes that are imprinted in an adult may still be expressed in that adult's offspring. (For example, the maternal genes that control insulin production will be imprinted in a male but will be expressed in any of the male's offspring that inherit these genes.) The nature of imprinting must therefore be epigenetic rather than DNA sequence dependent. In germline cells the imprint is erased and then re-established according to the sex of the individual, i.e. in the developing sperm (during spermatogenesis), a paternal imprint is established, whereas in developing oocytes (oogenesis), a maternal imprint is established. This process of erasure and reprogramming is necessary such that the germ cell imprinting status is relevant to the sex of the individual. In both plants and mammals there are two major mechanisms that are involved in establishing the imprint; these are DNA methylation and histone modifications.\nRecently, a new study has suggested a novel inheritable imprinting mechanism in humans that would be specific of placental tissue and that is independent of DNA methylation (the main and classical mechanism for genomic imprinting). This was observed in humans, but not in mice, suggesting development after the evolutionary divergence of humans and mice, ~80 Mya. Among the hypothetical explanations for this novel phenomenon, two possible mechanisms have been proposed: either a histone modification that confers imprinting at novel placental-specific imprinted \"loci\" or, alternatively, a recruitment of DNMTs to these loci by a specific and unknown transcription factor that would be expressed during early trophoblast differentiation.\nRegulation.\nThe grouping of imprinted genes within clusters allows them to share common regulatory elements, such as non-coding RNAs and differentially methylated regions (DMRs). When these regulatory elements control the imprinting of one or more genes, they are known as imprinting control regions (ICR). The expression of non-coding RNAs, such as antisense Igf2r RNA (\"Air\") on mouse chromosome 17 and KCNQ1OT1 on human chromosome 11p15.5, have been shown to be essential for the imprinting of genes in their corresponding regions.\nDifferentially methylated regions are generally segments of DNA rich in cytosine and guanine nucleotides, with the cytosine nucleotides methylated on one copy but not on the other. Contrary to expectation, methylation does not necessarily mean silencing; instead, the effect of methylation depends upon the default state of the region.\nFunctions of imprinted genes.\nThe control of expression of specific genes by genomic imprinting is unique to therian mammals (placental mammals and marsupials) and flowering plants. Imprinting of whole chromosomes has been reported in mealybugs (Genus: \"Pseudococcus\") and a fungus gnat (\"Sciara\"). It has also been established that X-chromosome inactivation occurs in an imprinted manner in the extra-embryonic tissues of mice and all tissues in marsupials, where it is always the paternal X-chromosome which is silenced.\nThe majority of imprinted genes in mammals have been found to have roles in the control of embryonic growth and development, including development of the placenta. Other imprinted genes are involved in post-natal development, with roles affecting suckling and metabolism.\nHypotheses on the origins of imprinting.\nA widely accepted hypothesis for the evolution of genomic imprinting is the \"parental conflict hypothesis\". Also known as the kinship theory of genomic imprinting, this hypothesis states that the inequality between parental genomes due to imprinting is a result of the differing interests of each parent in terms of the evolutionary fitness of their genes. The father's genes that encode for imprinting gain greater fitness through the success of the offspring, at the expense of the mother. The mother's evolutionary imperative is often to conserve resources for her own survival while providing sufficient nourishment to current and subsequent litters. Accordingly, paternally expressed genes tend to be growth-promoting whereas maternally expressed genes tend to be growth-limiting. In support of this hypothesis, genomic imprinting has been found in all placental mammals, where post-fertilisation offspring resource consumption at the expense of the mother is high; although it has also been found in oviparous birds where there is relatively little post-fertilisation resource transfer and therefore less parental conflict. A small number of imprinted genes are fast evolving under positive Darwinian selection possibly due to antagonistic co-evolution. The majority of imprinted genes display high levels of micro-synteny conservation and have undergone very few duplications in placental mammalian lineages.\nHowever, our understanding of the molecular mechanisms behind genomic imprinting show that it is the maternal genome that controls much of the imprinting of both its own and the paternally-derived genes in the zygote, making it difficult to explain why the maternal genes would willingly relinquish their dominance to that of the paternally-derived genes in light of the conflict hypothesis.\nAnother hypothesis proposed is that some imprinted genes act coadaptively to improve both fetal development and maternal provisioning for nutrition and care. In it, a subset of paternally expressed genes are co-expressed in both the placenta and the mother's hypothalamus. This would come about through selective pressure from parent-infant coadaptation to improve infant survival. Paternally expressed 3 (\"PEG3\") is a gene for which this hypothesis may apply.\nOthers have approached their study of the origins of genomic imprinting from a different side, arguing that natural selection is operating on the role of epigenetic marks as machinery for homologous chromosome recognition during meiosis, rather than on their role in differential expression. This argument centers on the existence of epigenetic effects on chromosomes that do not directly affect gene expression, but do depend on which parent the chromosome originated from. This group of epigenetic changes that depend on the chromosome's parent of origin (including both those that affect gene expression and those that do not) are called parental origin effects, and include phenomena such as paternal X inactivation in the marsupials, nonrandom parental chromatid distribution in the ferns, and even mating type switching in yeast. This diversity in organisms that show parental origin effects has prompted theorists to place the evolutionary origin of genomic imprinting before the last common ancestor of plants and animals, over a billion years ago.\nNatural selection for genomic imprinting requires genetic variation in a population. A hypothesis for the origin of this genetic variation states that the host-defense system responsible for silencing foreign DNA elements, such as genes of viral origin, mistakenly silenced genes whose silencing turned out to be beneficial for the organism. There appears to be an over-representation of retrotransposed genes, that is to say genes that are inserted into the genome by viruses, among imprinted genes. It has also been postulated that if the retrotransposed gene is inserted close to another imprinted gene, it may just acquire this imprint.\nImprinted loci phenotypic signatures.\nUnfortunately, the relationship between the phenotype and genotype of imprinted genes is solely conceptual. The idea is frameworked using two alleles on a single locus and hosts three different possible classes of genotypes. The reciprocal heterozygotes genotype class contributes to understanding how imprinting will impact genotype to phenotype relationship. Reciprocal heterozygotes have a genetically equivalent, but they are phenotypically nonequivalent. Their phenotype may not be dependent on the equivalence of the genotype. This can ultimately increase diversity in genetic classes, expanding flexibility of imprinted genes. This increase will also force a higher degree in testing capabilities and assortment of tests to determine the presences of imprinting.\nWhen a locus is identified as imprinted, two different classes express different alleles. Inherited imprinted genes of offspring are believed to be monoallelic expressions. A single locus will entirely produce one's phenotype although two alleles are inherited. This genotype class is called parental imprinting, as well as dominant imprinting. Phenotypic patterns are variant to possible expressions from paternal and maternal genotypes. Different alleles inherited from different parents will host different phenotypic qualities. One allele will have a larger phenotypic value and the other allele will be silenced. Underdominance of the locus is another possibility of phenotypic expression. Both maternal and paternal phenotypes will have a small value rather than one hosting a large value and silencing the other.\nStatistical frameworks and mapping models are used to identify imprinting effects on genes and complex traits. Allelic parent-of-origin influences the vary in phenotype that derive from the imprinting of genotype classes. These models of mapping and identifying imprinting effects include using unordered genotypes to build mapping models. These models will show classic quantitative genetics and the effects of dominance of the imprinted genes.\nHuman disorders associated with imprinting.\nImprinting may cause problems in cloning, with clones having DNA that is not methylated in the correct positions. It is possible that this is due to a lack of time for reprogramming to be completely achieved. When a nucleus is added to an egg during somatic cell nuclear transfer, the egg starts dividing in minutes, as compared to the days or months it takes for reprogramming during embryonic development. If time is the responsible factor, it may be possible to delay cell division in clones, giving time for proper reprogramming to occur.\nIn vitro fertilisation, including ICSI, is associated with an increased risk of imprinting disorders, with an odds ratio of 3.7 (95% confidence interval 1.4 to 9.7).\nMale infertility.\nEpigenetic deregulations at H19 imprinted gene in sperm have been observed associated with male infertility. Indeed, methylation loss at H19 imprinted gene has been observed associated with MTHFR gene promoter hypermethylation in semen samples from infertile males.\nPrader-Willi/Angelman.\nThe first imprinted genetic disorders to be described in humans were the reciprocally inherited Prader-Willi syndrome and Angelman syndrome. Both syndromes are associated with loss of the chromosomal region 15q11-13 (band 11 of the long arm of chromosome 15). This region contains the paternally expressed genes SNRPN and NDN and the maternally expressed gene UBE3A.\nDIRAS3 (NOEY2 or ARH1).\nDIRAS3 is a paternally expressed and maternally imprinted gene located on chromosome 1 in humans. Reduced DIRAS3 expression is linked to an increased risk of ovarian and breast cancers; in 41% of breast and ovarian cancers the protein encoded by DIRAS3 is not expressed, suggesting that it functions as a tumor suppressor gene. Therefore, if uniparental disomy occurs and a person inherits both chromosomes from the mother, the gene will not be expressed and the individual is put at a greater risk for breast and ovarian cancer.\nOther.\nOther conditions involving imprinting include Beckwith-Wiedemann syndrome, Silver-Russell syndrome, and pseudohypoparathyroidism.\nTransient neonatal diabetes mellitus can also involve imprinting.\nImprinted genes in other animals.\nIn insects, imprinting affects entire chromosomes. In some insects the entire paternal genome is silenced in male offspring, and thus is involved in sex determination. The imprinting produces effects similar to the mechanisms in other insects that eliminate paternally inherited chromosomes in male offspring, including arrhenotoky. \nIn social honey bees, the parent of origin and allele-specific genes has been studied from reciprocal crosses to explore the epigenetic mechanisms underlying aggressive behavior.\nIn placental species, parent-offspring conflict can result in the evolution of strategies, such as genomic imprinting, for embryos to subvert maternal nutrient provisioning. Despite several attempts to find it, genomic imprinting has not been found in the platypus, reptiles, birds, or fish. The absence of genomic imprinting in a placental reptile, the Pseudemoia entrecasteauxii, is interesting as genomic imprinting was thought to be associated with the evolution of viviparity and placental nutrient transport.\nStudies in domestic livestock, such as dairy and beef cattle, have implicated imprinted genes (e.g. IGF2) in a range of economic traits, including dairy performance in Holstein-Friesian cattle.\nIn sheep, the CLPG gene (\"callipyge\" from Greek, meaning \"beautiful buttocks\") produces a large buttocks consisting of muscle with very little fat. The large-buttocked phenotype only occurs when the allele is present on the copy of chromosome 18 inherited from a sheep's father and is \"not\" on the copy of chromosome 18 inherited from that sheep's mother. The CLPG locus is encompassed by Dlk1-Gtl2, an imprinted region of the mammalian genome, and the atypical presentation of this gene is a result of this imprinting.\nMouse foraging behavior.\nForaging behavior in mice studied is influenced by a sexually dimorphic allele expression implicating a cross-gender imprinting influence that varies throughout the body and may dominate expression and shape a behavior.\nImprinted genes in plants.\nA similar imprinting phenomenon has also been described in flowering plants (angiosperms). During fertilization of the egg cell, a second, separate fertilization event gives rise to the endosperm, an extraembryonic structure that nourishes the embryo in a manner analogous to the mammalian placenta. Unlike the embryo, the endosperm is often formed from the fusion of two maternal cells with a male gamete. This results in a triploid genome. The 2:1 ratio of maternal to paternal genomes appears to be critical for seed development. Some genes are found to be expressed from both maternal genomes while others are expressed exclusively from the lone paternal copy. It has been suggested that these imprinted genes are responsible for the triploid block effect in flowering plants that prevents hybridization between diploids and autotetraploids. Several computational methods to detect imprinting genes in plants from reciprocal crosses have been proposed. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15236", "revid": "50360230", "url": "https://en.wikipedia.org/wiki?curid=15236", "title": "ICANN", "text": "American nonprofit organization\nThe Internet Corporation for Assigned Names and Numbers (ICANN ) is a global multistakeholder group and nonprofit organization headquartered in the United States. Responsible for coordinating the maintenance and procedures of several databases related to the namespaces and numerical spaces of the Internet while also ensuring the Internet's smooth, secure and stable operation. ICANN performs the actual technical maintenance (work) of the Central Internet Address pools and DNS root zone registries pursuant to the Internet Assigned Numbers Authority (IANA) function contract. The contract regarding the IANA stewardship functions between ICANN and the National Telecommunications and Information Administration (NTIA) of the United States Department of Commerce ended on October 1, 2016, formally transitioning the functions to the global multistakeholder community.\nMuch of its work has concerned the Internet's global Domain Name System (DNS), including policy development for internationalization of the DNS, introduction of new generic top-level domains (TLDs), and the operation of root name servers; the numbering facilities ICANN manages include the Internet Protocol (IP) address spaces for IPv4 and v6 in addition to the assignment of address blocks to regional Internet registries (RIRs).\nICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; promoting competition; achieving broad representation of the global Internet community, and developing policies appropriate to its mission through bottom-up, consensus-based processes. The organization has often included a motto of \"One World. One Internet.\" on annual reports beginning in 2010, on less formal publications, as well as their official website.\nICANN was officially incorporated in the state of California on September 30, 1998, with entrepreneur and philanthropist Esther Dyson as founding chairwoman. Originally headquartered in Marina del Rey in the same building as the University of Southern California's Information Sciences Institute (ISI), its offices are now in the Playa Vista neighborhood of Los Angeles.\nHistory.\nBefore the establishment of ICANN, the IANA function of administering RIRs (including the distributing top-level domains and IP addresses) was performed by Jon Postel, a computer science researcher who had been involved in the creation of ARPANET, first at UCLA and then at USC-ISI. In 1997 Postel testified before Congress that this had come about as a \"side task\" to this research work. The Information Sciences Institute was funded by the U.S. Department of Defense, as was SRI International's Network Information Center, which also performed some assigned name functions.\nAs the Internet grew and expanded globally, the U.S. Department of Commerce initiated a process to establish a new organization to perform the IANA functions. On January 30, 1998, the National Telecommunications and Information Administration (NTIA), an agency of the U.S. Department of Commerce, issued for comment, \"A Proposal to Improve the Technical Management of Internet Names and Addresses.\" The proposed rule making, or \"Green Paper\", was published in the Federal Register on February 20, 1998, providing opportunity for public comment. NTIA received more than 650 comments as of March 23, 1998, when the comment period closed.\nThe Green Paper proposed certain actions designed to privatize the management of Internet names and addresses in a manner that allows for the development of competition and facilitates global participation in Internet management. The Green Paper proposed for discussion a variety of issues relating to DNS management including private sector creation of a new not-for-profit corporation (the \"new corporation\") managed by a globally and functionally representative board of directors. ICANN was formed in response to this policy. ICANN managed the Internet Assigned Numbers Authority (IANA) under contract to the United States Department of Commerce (DOC) and pursuant to an agreement with the IETF.\nICANN is a public-benefit nonprofit corporation \"organized under the California Nonprofit Public Benefit Corporation Law for charitable and public purposes.\" ICANN was established in California due to the presence of Postel, who was a founder of ICANN and was set to be its first Chief Technology Officer prior to his unexpected death. ICANN formerly operated from the same Marina del Rey building where Postel formerly worked, which is home to an office of the Information Sciences Institute at the University of Southern California. However, ICANN's headquarters is now located in the nearby Playa Vista neighborhood of Los Angeles.\nPer its original by-laws, primary responsibility for policy formation in ICANN was to be delegated to three supporting organizations (Address Supporting Organization, Domain Name Supporting Organization, and Protocol Supporting Organization), each of which was to develop and recommend substantive policies and procedures for the management of the addresses within their respective scope. They were also required to be financially independent from ICANN. As expected, the RIRs and IETF agreed to serve as the Address Supporting Organization and Protocol Supporting Organization respectively, and ICANN issued a call for interested parties to propose the structure and composition of the Domain Name Supporting Organization. In March 1999, the ICANN Board, based in part on the DNSO proposals received, decided instead on an alternate construction for the DNSO which delineated specific constituencies bodies within ICANN itself, thus adding primary responsibility for DNS policy development to ICANN's existing duties of oversight and coordination.\nOn July 26, 2006, the United States government renewed the contract with ICANN for performance of the IANA function for an additional one to five years. The context of ICANN's relationship with the U.S. government was clarified on September 29, 2006, when ICANN signed a new memorandum of understanding with the United States Department of Commerce (DOC). This document gave the DOC oversight over some of the ICANN operations.\nIn July 2008, the DOC reiterated an earlier statement that it has \"no plans to transition management of the authoritative root zone file to ICANN\". The letter also stresses the separate roles of the IANA and VeriSign.\nOn September 30, 2009, ICANN signed an agreement with the DOC (known as the \"Affirmation of Commitments\") that confirmed ICANN's commitment to a multistakeholder governance model, but did not remove it from DOC oversight and control. The Affirmation of Commitments, which aimed to create international oversight, ran into criticism.\nOn March 10, 2016, ICANN and the DOC signed a historic, culminating agreement to finally remove ICANN and IANA from the control and oversight of the DOC. On October 1, 2016, ICANN was freed from U.S. government oversight.\nSince its creation, ICANN has been the subject of criticism and controversy. In 2000, professor Michael Froomkin of the University of Miami School of Law argued that ICANN's relationship with the U.S. Department of Commerce is illegal, in violation of either the Constitution or federal statutes. On June 10, 2024, it was announced that Kurt Erik Lindqvist, who has been CEO of the London Internet Exchange since 2019, was to become the new president and CEO of ICANN on December 5, 2024.\nNotable events.\nOn March 18, 2002, publicly elected At-Large Representative for North America board member Karl Auerbach sued ICANN in the Superior Court of Los Angeles County, California, to gain access to ICANN's accounting records without restriction. Judge Dzintra Janavs ruled in Auerbach's favor on July 29, 2002.\nDuring September and October 2003, ICANN played a crucial role in the conflict over VeriSign's \"wild card\" DNS service Site Finder. After an open letter from ICANN issuing an ultimatum to VeriSign, later endorsed by the Internet Architecture Board, the company voluntarily ended the service on October 4, 2003. After this action, VeriSign filed a lawsuit against ICANN on February 27, 2004, claiming that ICANN had exceeded its authority. By this lawsuit, VeriSign sought to reduce ambiguity about ICANN's authority. The antitrust component of VeriSign's claim was dismissed during August 2004. VeriSign's challenge that ICANN overstepped its contractual rights is currently outstanding. A proposed settlement already approved by ICANN's board would resolve VeriSign's challenge to ICANN in exchange for the right to increase pricing on .com domains. At the meeting of ICANN in Rome, which took place from March 2 to 6, 2004, ICANN agreed to ask approval of the U.S. Department of Commerce for the Waiting List Service of VeriSign.\nOn May 17, 2004, ICANN published a proposed budget for the year 2004\u201305. It included proposals to increase the openness and professionalism of its operations, and increased its proposed spending from US$8.27\u00a0million to $15.83\u00a0million. The increase was to be funded by the introduction of new top-level domains, charges to domain registries, and a fee for some domain name registrations, renewals and transfers (initially US$0.20 for all domains within a country-code top-level domain, and US$0.25 for all others). The Council of European National Top Level Domain Registries (CENTR), which represents the RIRs of 39 countries, rejected the increase, accusing ICANN of a lack of financial prudence and criticizing what it describes as ICANN's \"unrealistic political and operational targets\". Despite the criticism, the registry agreement for the top-level domains jobs and travel includes a US$2 fee on every domain the licensed companies sell or renew.\nAfter a second round of negotiations during 2004, the TLDs eu, asia, travel, jobs, mobi, and cat were introduced during 2005.\nOn February 28, 2006, ICANN's board approved a settlement with VeriSign in the lawsuit resulting from SiteFinder that involved allowing VeriSign (the registry) to raise its registration fees by up to 7% a year. This was criticised by a few members of the U.S. House of Representatives' Small Business Committee.\nDuring February 2007, ICANN began procedures to end accreditation of one of their registrars, RegisterFly amid charges and lawsuits involving fraud, and criticism of ICANN's management of the situation. ICANN has been the subject of criticism as a result of its handling of RegisterFly, and the harm caused to thousands of clients as a result of what has been termed ICANN's \"laissez faire attitude toward customer allegations of fraud\".\nOn May 23, 2008, ICANN issued enforcement notices against ten accredited registrars and announced this through a press release entitled \"'Worst Spam Offenders' Notified by ICANN, Compliance system working to correct Whois and other issues.\" This was largely in response to a report issued by KnujOn, called \"The 10 Worst Registrars\" in terms of spam advertised junk product sites and compliance failure. The mention of the word \"spam\" in the title of the ICANN memo is somewhat misleading since ICANN does not address issues of spam or email abuse. Website content and usage are not within ICANN's mandate. However, the KnujOn report details how various registrars have not complied with their contractual obligations under the Registrar Accreditation Agreement (RAA). The main point of the KnujOn research was to demonstrate the relationships between compliance failure, illicit product traffic, and spam. The report demonstrated that out of 900 ICANN accredited registrars, fewer than 20 held 90% of the web domains advertised in spam. These same registrars were also most frequently cited by KnujOn as failing to resolve complaints made through the Whois Data Problem Reporting System (WDPRS).\nOn June 26, 2008, the ICANN Board started a new process of TLD naming policy to take a \"significant step forward on the introduction of new generic top-level domains.\" This program envisioned the availability of many new or already proposed domains, as well a new application and implementation process.\nOn October 1, 2008, ICANN issued breach notices against Joker and Beijing Innovative Linkage Technology Ltd. after further researching reports and complaints issued by KnujOn. These notices gave the registrars 15 days to fix their Whois investigation efforts.\nIn 2010, ICANN approved a major review of its policies with respect to accountability, transparency, and public participation by the Berkman Center for Internet and Society at Harvard University. This external review was an assistance of the work of ICANN's Accountability and Transparency Review team.\nOn February 3, 2011, ICANN announced that it had distributed the last batch of its remaining IPv4 addresses to the world's five RIRs (the organizations that manage IP addresses in different regions); these registries began assigning the final IPv4 addresses within their regions until they completely ran out.\nOn June 20, 2011, the ICANN board voted to end most restrictions on the names of generic top-level domains (gTLD). Companies and organizations became able to choose essentially arbitrary top-level Internet domain names. The use of non-Latin characters (such as Cyrillic, Arabic, Chinese, etc.) is also allowed in gTLDs. ICANN began accepting applications for new gTLDS on January 12, 2012. The initial price to apply for a new gTLD was set at $185,000 and the annual renewal fee is $25,000.\nDuring December 2011, the Federal Trade Commission stated ICANN had long failed to provide safeguards that protect consumers from online swindlers.\nFollowing the 2013 NSA spying scandal, ICANN endorsed the Montevideo Statement, although no direct connection between these could be proven.\nOn October 1, 2016, ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA) and entered the private sector.\nThe European Union's General Data Protection Regulation (active since May 25, 2018) has had an impact on ICANN's operations, which had to be fixed via some last minute changes.\nStructure.\nFrom its founding to the present, ICANN has been formally organized as a nonprofit corporation \"for charitable and public purposes\" under the California Nonprofit Public Benefit Corporation Law. It is managed by a 16-member board of directors composed of eight members selected by a nominating committee on which all the constituencies of ICANN are represented; six representatives of its Supporting Organizations, sub-groups that deal with specific sections of the policies under ICANN's purview; an at-large seat filled by an at-large organization; and the president / CEO, appointed by the board.\nThere are currently three supporting organizations: the Generic Names Supporting Organization (GNSO) which deals with policy making on generic top-level domains (gTLDs); the Country Code Names Supporting Organization (ccNSO) deals with policy making on country-code top-level domains (ccTLDs); the Address Supporting Organization (ASO) deals with policy making on IP addresses.\nICANN also relies on some advisory committees and other advisory mechanisms to receive advice on the interests and needs of stakeholders that do not directly participate in the Supporting Organizations. These include the Governmental Advisory Committee (GAC), which is composed of representatives of a large number of national governments from all over the world; the At-Large Advisory Committee (ALAC), which is composed of individual Internet users from around the world selected by each of the Regional At-Large Organizations (RALO) and Nominating Committee; the Root Server System Advisory Committee (RSSAC), which provides advice on the operation of the DNS root server system; the Security and Stability Advisory Committee (SSAC), which is composed of Internet experts who study security issues pertaining to ICANN's mandate; and the Technical Liaison Group (TLG), which is composed of representatives of other international technical organizations that focus, at least in part, on the Internet.\nGovernmental Advisory Committee.\nRepresentatives.\nThe Governmental Advisory Committee has representatives from 179 states and 38 Observer organizations, including the Holy See, Cook Islands, Niue, Taiwan, Hong Kong, Bermuda, Montserrat, the European Commission and the African Union Commission.\nObservers.\nIn addition the following organizations are GAC Observers:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nTrusted Community Representatives.\nAs the operator of the IANA domain name functions, ICANN is responsible for the DNSSEC management of the root zone. While day-to-day operations are managed by ICANN and Verisign, the trust is rooted in a group of Trusted Community Representatives. The members of this group must not be affiliated with ICANN, but are instead members of the broader DNS community, volunteering to become a Trusted Community Representative. The role of the representatives are primarily to take part in regular key ceremonies at a physical location, organized by ICANN, and to safeguard the key materials in between.\nDemocratic input.\nIn the memorandum of understanding that set up the relationship between ICANN and the U.S. government, ICANN was given a mandate requiring that it operate \"in a bottom-up, consensus-driven, democratic manner.\" However, the attempts that ICANN has made to establish an organizational structure that would allow wide input from the global Internet community did not produce results amenable to the current Board. As a result, the At-Large constituency and direct election of board members by the global Internet community were soon abandoned.\nICANN holds periodic public meetings rotated between continents for the purpose of encouraging global participation in its processes. Resolutions of the ICANN Board, preliminary reports, and minutes of the meetings are published on the ICANN website, sometimes in real-time. However, there are criticisms from ICANN constituencies including the Noncommercial Users Constituency (NCUC) and the At-Large Advisory Committee (ALAC) that there is not enough public disclosure and that too many discussions and decisions take place out of sight of the public.\nDuring the early 2000s, there had been speculation that the United Nations might assume control of ICANN, followed by a negative reaction from the U.S. government and worries about a division of the Internet. At UN's World Summit on the Information Society in Tunisia in November 2005, the world's governments agreed not to get involved in the day-to-day and technical operations of ICANN. However they also agreed to establish an international Internet Governance Forum, with a consultative role on the future governance of the Internet. ICANN's Government Advisory Committee is currently established to provide advice to ICANN regarding public policy issues and has participation by many of the world's governments.\nSome have attempted to argue that ICANN was never given the authority to decide policy, e.g., choose new TLDs or exclude other interested parties who refuse to pay ICANN's US$185,000 fee but was to be a technical caretaker. Critics suggest that ICANN should not be allowed to impose business rules on market participants and that all TLDs should be added on a first-come, first-served basis and the market should be the arbiter of who succeeds and who does not.\nActivities.\nUniform Domain-Name Dispute Resolution Policy (UDRP).\nOne task that ICANN was asked to do was to address the issue of domain name ownership resolution for generic top-level domains (gTLDs). ICANN's attempt at such a policy was drafted in close cooperation with the World Intellectual Property Organization (WIPO), and the result has now become known as the Uniform Dispute Resolution Policy (UDRP). This policy essentially attempts to provide a mechanism for rapid, cheap and reasonable resolution of domain name conflicts, avoiding the traditional court system for disputes by allowing cases to be brought to one of a set of bodies that arbitrate domain name disputes. According to ICANN policy, domain registrants must agree to be bound by the UDRP\u2014they cannot get a domain name without agreeing to this.\nExamination of the UDRP decision patterns has caused some to conclude that compulsory domain name arbitration is less likely to give a fair hearing to domain name owners asserting defenses under the First Amendment and other laws, compared to the federal courts of appeal in particular.\nProposed elimination of public DNS whois.\nIn 2013, the initial report of ICANN's Expert Working Group has recommended that the present form of Whois, a utility that allows anyone to know who has registered a domain name on the Internet, should be \"abandoned\". It recommends it be replaced with a system that keeps most registration information secret (or \"gated\") from most Internet users, and only discloses information for \"permissible purposes\". ICANN's list of permissible purposes includes domain name research, domain name sale and purchase, regulatory enforcement, personal data protection, legal actions, and abuse mitigation. Whois has been a key tool of investigative journalists interested in determining who was disseminating information on the Internet. The use of whois by journalists is not included in the list of permissible purposes in the initial report.\nProposals for reform.\nProposals have been made to internationalize ICANN's monitoring responsibilities (currently the responsibility of the US), to transform it into an international organization (under international law), and to \"establish an intergovernmental mechanism enabling governments, on an equal footing, to carry out their roles and responsibilities in international public policy issues pertaining to the Internet\".\nIBSA proposal (2011).\nOne controversial proposal, resulting from a September 2011 summit between India, Brazil, and South Africa (IBSA), sought to move Internet governance into a \"UN Committee on Internet-Related Policy\" (UN-CIRP). The action was a reaction to a perception that the principles of the 2005 Tunis Agenda for the Information Society had not been met; the statement proposed the creation of a new political organization operating as a component of the United Nations to provide policy recommendations for the consideration of technical organizations such as ICANN and international bodies such as the ITU. Subsequent to public criticism, India backed away from the proposal.\nMontevideo Statement on the Future of Internet Cooperation (2013).\nOn October 7, 2013, the Montevideo Statement on the Future of Internet Cooperation was released by the managers of a number of organizations involved in coordinating the Internet's global technical infrastructure, loosely known as the \"I*\" (or \"I-star\") group. Among other things, the statement \"expressed strong concern over the undermining of the trust and confidence of Internet users globally due to recent revelations of pervasive monitoring and surveillance\" and \"called for accelerating the globalization of ICANN and IANA functions, towards an environment in which all stakeholders, including all governments, participate on an equal footing\". This desire to reduce United States association with the internet is considered a reaction to the ongoing NSA surveillance scandal. The statement was signed by the managers of the ICANN, the Internet Engineering Task Force, the Internet Architecture Board, the World Wide Web Consortium, the Internet Society, and the five regional Internet address registries (African Network Information Center, American Registry for Internet Numbers, Asia-Pacific Network Information Centre, Latin America and Caribbean Internet Addresses Registry, and R\u00e9seaux IP Europ\u00e9ens Network Coordination Centre).\nGlobal Multistakeholder Meeting on the Future of Internet Governance (2013).\nDuring October 2013, Fadi Chehad\u00e9, former president and CEO of ICANN, met with Brazilian president Dilma Rousseff in Brasilia. Upon Chehad\u00e9's invitation, the two announced that Brazil would host an international summit on Internet governance during April 2014. The announcement came after the 2013 disclosures of mass surveillance by the U.S. government, and Rousseff's speech at the opening session of the 2013 United Nations General Assembly, where she strongly criticized the American surveillance program as a \"breach of international law\". The \"Global Multistakeholder Meeting on the Future of Internet Governance (NET mundial)\" will include representatives of government, industry, civil society, and academia. At the IGF VIII meeting in Bali in October 2013 a commenter noted that Brazil intends the meeting to be a \"summit\" in the sense that it will be high level with decision-making authority. The organizers of the \"NET mundial\" meeting decided that an online forum called \"/1net\", set up by the I* group, will be a major conduit of non-governmental input into the three committees preparing for the meeting in April.\nThe Obama administration, which had joined critics of ICANN in 2011, announced in March 2014 that they intended to transition away from oversight of the IANA functions contract. The current contract that the United States Department of Commerce has with ICANN expired in 2015, in its place the NTIA will transition oversight of the IANA functions to the 'global multistakeholder community'.\nNetMundial Initiative (2014).\nThe NetMundial Initiative is a plan for international governance of the Internet that was first proposed at the Global Multistakeholder Meeting on the Future of Internet Governance (GMMFIG) conference (April 23\u201324, 2014)\nand later developed into the NetMundial Initiative by ICANN CEO Fadi Chehad\u00e9 along with representatives of the World Economic Forum (WEF)\nand the Brazilian Internet Steering Committee (Comit\u00ea Gestor da Internet no Brasil), commonly referred to as \"CGI.br\".\nThe meeting produced a nonbinding statement in favor of consensus-based decision-making. It represented a compromise and did not harshly condemn mass surveillance or support net neutrality, despite initial endorsement for the latter from Brazil; the final resolution states that ICANN should be controlled internationally by September 2015.\nA minority of governments, including Russia, China, Iran, and India, were unhappy with the final resolution and wanted multilateral management for the Internet (such as a UN-based model), rather than broader multistakeholder management.\nA month later, the Panel on Global Internet Cooperation and Governance Mechanisms (convened by the ICANN and WEF with assistance from The Annenberg Foundation), endorsed and included the NetMundial statement in its own report.\nDuring June 2014, France vehemently lambasted ICANN, saying they are an unfit venue for Internet governance, and that alternatives should be sought.\nTLD expansion and concerns about specific top-level domains.\nDuring 2011, seventy-nine companies, including The Coca-Cola Company, Hewlett-Packard, Samsung and others, signed a petition against ICANN's new TLD program (sometimes referred to as a \"commercial landgrab\"), in a group organized by the Association of National Advertisers. As of September 2014, this group, the Coalition for Responsible Internet Domain Oversight, that opposes the rollout of ICANN's TLD expansion program, has been joined by 102 associations and 79 major companies. Partly as a response to this criticism, ICANN initiated an effort to protect trademarks in domain name registrations, which eventually culminated in the establishment of the Trademark Clearinghouse.\n.sucks domain.\nICANN has received more than $60\u00a0million from gTLD auctions, and has accepted the controversial domain name \".sucks\" (referring to the primarily US slang for being inferior or objectionable). sucks domains are owned and controlled by the Vox Populi Registry which won the rights for .sucks gTLD in November 2014.\nThe .sucks domain registrar has been described as \"predatory, exploitive and coercive\" by the Intellectual Property Constituency that advises the ICANN board. When the .sucks registry announced their pricing model, \"most brand owners were upset and felt like they were being penalized by having to pay more to protect their brands.\" Because of the low utility of the \".sucks\" domain, most fees come from \"Brand Protection\" customers registering their trademarks to prevent domains being registered.\nCanadian brands had complained that they were being charged \"exorbitant\" prices to register their trademarks as premium names. FTC chair Edith Ramirez has written to ICANN to say the agency will take action against the .sucks owner if \"we have reason to believe an entity has engaged in deceptive or unfair practices in violation of Section 5 of the FTC Act\". The Register reported that intellectual property lawyers are infuriated that \"the dot-sucks registry was charging trademark holders $2,500 for .sucks domains and everyone else $10.\"\nU.S. Representative Bob Goodlatte has said that trademark holders are \"being shaken down\" by the registry's fees. Jay Rockefeller says that .sucks is \"a predatory shakedown scheme\" and \"Approving '.sucks', a gTLD with little or no public interest value, will have the effect of undermining the credibility ICANN has slowly been building with skeptical stakeholders.\"\n.islam, .halal top level domains.\nIn a long-running dispute, ICANN has so far declined to allow a Turkish company to purchase the .islam and .halal gTLDs, after the Organisation of Islamic Cooperation objected that the gTLDs should be administered by an organization that represents all the world's 1.6 billion Muslims. After a number of attempts to resolve the issue the domains are still held \"on hold\" as of 2019.\n.org price cap removal.\nIn April 2019, ICANN proposed an end to the price cap of org domains and effectively removed it in July in spite of having received 3,252 opposing comments and only six in favor. A few months later, the owner of the domain, the Public Interest Registry, proposed to sell the domain to investment firm Ethos Capital.\n.amazon gTLD dispute.\nIn May 2019, ICANN decided in favor of granting exclusive administration rights to amazon.com for the .amazon gTLD after a 7 year long dispute with the Amazon Cooperation Treaty Organization (ACTO).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15237", "revid": "31339567", "url": "https://en.wikipedia.org/wiki?curid=15237", "title": "Iterative method", "text": "Algorithm in which each approximation of the solution is derived from prior approximations\nIn computational mathematics, an iterative method is a mathematical procedure that uses an initial value to generate a sequence of improving approximate solutions for a class of problems, in which the \"i\"-th approximation (called an \"iterate\") is derived from the previous ones. \nA specific implementation with termination criteria for a given iterative method like gradient descent, hill climbing, Newton's method, or quasi-Newton methods like BFGS, is an algorithm of an iterative method or a method of successive approximation. An iterative method is called \"convergent\" if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common.\nIn contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (for example, solving a linear system of equations formula_1 by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving many variables (sometimes on the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.\nAttractive fixed points.\nIf an equation can be put into the form \"f\"(\"x\") = \"x\", and a solution x is an attractive fixed point of the function \"f\", then one may begin with a point \"x\"1 in the basin of attraction of x, and let \"x\"\"n\"+1 = \"f\"(\"x\"\"n\") for \"n\"\u00a0\u2265\u00a01, and the sequence {\"x\"\"n\"}\"n\"\u00a0\u2265\u00a01 will converge to the solution x. Here \"x\"\"n\" is the \"n\"th approximation or iteration of \"x\" and \"x\"\"n\"+1 is the next or \"n\" + 1 iteration of \"x\". Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings. (For example, \"x\"(\"n\"+1) = \"f\"(\"x\"(\"n\")).) If the function \"f\" is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point. If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.\nLinear systems.\nIn the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.\nStationary iterative methods.\nIntroduction.\nStationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices.\nDefinition.\nAn \"iterative method\" is defined by\nformula_2\nand for a given linear system formula_3 with exact solution formula_4 the \"error\" by\nformula_5\nAn iterative method is called \"linear\" if there exists a matrix formula_6 such that\nformula_7\nand this matrix is called the \"iteration matrix\".\nAn iterative method with a given iteration matrix formula_8 is called \"convergent\" if the following holds\nformula_9\nAn important theorem states that for a given iterative method and its iteration matrix formula_8 it is convergent if and only if its spectral radius formula_11 is smaller than unity, that is,\nformula_12\nThe basic iterative methods work by splitting the matrix formula_13 into\nformula_14\nand here the matrix formula_15 should be easily invertible.\nThe iterative methods are now defined as\nformula_16\nor, equivalently,\nformula_17\nFrom this follows that the iteration matrix is given by\nformula_18\nExamples.\nBasic examples of stationary iterative methods use a splitting of the matrix formula_13 such as\nformula_20\nwhere formula_21 is only the diagonal part of formula_13, and formula_23 is the strict lower triangular part of formula_13.\nRespectively, formula_25 is the strict upper triangular part of formula_13.\nLinear stationary iterative methods are also called relaxation methods.\nKrylov subspace methods.\nKrylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual (the Krylov sequence). \nThe approximations to the solution are then formed by minimizing the residual over the subspace formed. \nThe prototypical method in this class is the conjugate gradient method (CG) which assumes that the system matrix formula_13 is symmetric positive-definite.\nFor symmetric (and possibly indefinite) formula_13 one works with the minimal residual method (MINRES).\nIn the case of non-symmetric matrices, methods such as the generalized minimal residual method (GMRES) and the biconjugate gradient method (BiCG) have been derived.\nConvergence of Krylov subspace methods.\nSince these methods form a basis, it is evident that the method converges in \"N\" iterations, where \"N\" is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice \"N\" can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.\nPreconditioners.\nThe approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.\nMethods of successive approximation.\nMathematical methods relating to successive approximation include:\nHistory.\nJamsh\u012bd al-K\u0101sh\u012b used iterative methods to calculate the sine of 1\u00b0 and \u03c0 in \"The Treatise of Chord and Sine\" to high precision. \nAn early iterative method for solving a linear system appeared in a letter of Gauss to a student of his. He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest .\nThe theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The conjugate gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15238", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15238", "title": "International judicial institution", "text": "List of international judicial institutions\nInternational judicial institutions can be divided into courts, arbitral tribunals and quasi-judicial institutions. Courts are permanent bodies, with near the same composition for each case. Arbitral tribunals, by contrast, are constituted anew for each case. Both courts and arbitral tribunals can make binding decisions. Quasi-judicial institutions, by contrast, make rulings on cases, but these rulings are not in themselves legally binding; the main example is the individual complaints mechanisms available under the various UN human rights treaties.\nInstitutions can also be divided into global and regional institutions.\nThe listing below incorporates both currently existing institutions, defunct institutions that no longer exist, institutions which never came into existence due to non-ratification of their constitutive instruments, and institutions which do not yet exist, but for which constitutive instruments have been signed. It does not include mere proposed institutions for which no instrument was ever signed.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15239", "revid": "50480375", "url": "https://en.wikipedia.org/wiki?curid=15239", "title": "International Prize Court", "text": "Proposed international court\nThe International Prize Court was an international court proposed at the beginning of the 20th century, to hear prize cases. An international agreement to create it, the \"Convention Relative to the Creation of an International Prize Court\", was made at the Second Hague Conference in 1907 but never came into force.\nThe capturing of prizes (enemy equipment, vehicles, and especially ships) during wartime is a tradition that goes back as far as organized warfare itself. The International Prize Court was established hear appeals from national courts concerning prize cases. Even as a draft, the convention was innovative for the time, in being both the first ever treaty for a truly international court (as opposed to a mere arbitral tribunal), and in providing individuals with access to the court, going against the prevailing doctrines of international law at the time, according to which only states had rights and duties under international law. The convention was opposed, particularly by elements within the United States and the United Kingdom, as a violation of national sovereignty.\nThe 1907 convention was modified by the \"Additional Protocol to the Convention Relative to the Creation of an International Prize Court\", done at the Hague on October 18, 1910. The protocol was an attempt to resolve some concerns expressed by the United States at the court, which felt it to be in violation of its constitutional provision that provides for the U.S. Supreme Court being the final judicial authority. However, neither the convention nor the subsequent protocol ever entered into force, since only Nicaragua ratified the agreements. As a result, the court never came into existence.\nOn February 15, 1911, the U.S. Senate ratified the International Prize Court Convention.\nA number of ideas from the International Prize Court proposal can be seen in present-day international courts, such as its provision for judges \"ad hoc\", later adopted in the Permanent Court of International Justice and the subsequent International Court of Justice.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nPrimary:\nSecondary:"}
{"id": "15240", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=15240", "title": "Imam", "text": "Islamic leadership position\nImam (; , '; pl.: , ') is an Islamic leadership position. For Sunni Muslims, imam is most commonly used as the title of a prayer leader of a mosque. In this context, imams may lead Islamic prayers, serve as community leaders, and provide religious guidance. Thus for Sunnis, anyone can study the basic Islamic teachings and become an imam. Its Christian equivalent/counterpart is a pastor or a priest.\nFor most Shia Muslims, the imams are absolute infallible leaders of the Islamic community after the Prophet. Shias consider the term to be only applicable to the members and descendants of the \"Ahl al-Bayt\", the family of the Islamic prophet Muhammad. In Twelver Shi'ism there are 14 infallibles, 12 of which are imams, the final being Imam Mahdi who will return at the end of times. The title was also used by the Zaidi Shia imams of Yemen, who eventually founded the Mutawakkilite Kingdom of Yemen (1918\u20131970).\nSunni imams.\nSunni Islam does not conceive of the role of imams in the same sense as Shia Islam: an important distinction often overlooked by non-Muslims. In everyday terms, an \"imam\" for Sunni Muslims is the person charged with leading formal Islamic prayers (Fard)\u2014even in locations besides the mosque\u2014whenever prayer is performed in a group of two or more. The imam leads the worship and the congregation copies his actions. Friday sermons are most often given by an appointed imam. All mosques have an imam to lead the congregational prayers\u2014even though it may sometimes just be a member from the gathered congregation rather than an officially appointed, salaried person. Women cannot be imams when men are present but are allowed to be when no men are present. An imam should be chosen, according to Hadith, based on his knowledge of the Quran and \"Sunnah\" and his moral character.\nTitle of scholarly authority.\nAnother well-known use of the term is as an honorary title for a recognized religious scholarly authority in Islam. It is especially used for a jurist (\"faqih\") and often for the founders of the four Sunni \"madhhab\"s or schools of jurisprudence (\"fiqh\"), as well as an authority on Quranic exegesis (\"tafsir\")]], such as Al-Tabari or Ibn Kathir.\nIt may also refer to the \"Muhaddith\u016bn\" or scholars who created the analytical sciences related to Hadith; due to their scholarly authority, the term may also refer to the heads of Muhammad's family in their generational times.\nThe position of imams in Turkey.\nImams are appointed by the state to work at mosques and they are required to be graduates of an \u0130mam Hatip high school or have a university degree in theology. This is an official position regulated by the Presidency of Religious Affairs in Turkey and only men are appointed to this position, whilst female officials under the same state organisation work as preachers and Qur'an course tutors, religious services experts, etc. These officials are supposed to belong to the Hanafi school of the Sunni sect.\nA central figure in an Islamic movement is also called an imam, like Imam Nawawi in Syria.\nShia imams.\nIn the Shi'a context, an imam is not only presented as the man of God \"par excellence\", but as participating fully in the names, attributes, and acts that theology usually reserves for God alone. Imams have a meaning more central to belief, referring to leaders of the community. Twelver and Ismaili Shi'a believe that these imams are chosen by God to be perfect examples for the faithful and to lead all humanity in all aspects of life. They also believe that all the imams chosen are free from committing any sin, impeccability which is called \"ismah\". These leaders must be followed since they are appointed by God.\nTwelver.\nHere follows a list of the Twelvers Shia imams:\nFatimah, also Fatimah al-Zahraa, daughter of Muhammed (615\u2013632), is also considered infallible but not an imam. The Shi'a believe that the last imam, the 12th Imam Mahdi will one day emerge on the Day of Resurrection (\"Qiyamah\").\n \"See Imamah (Ismaili doctrine) and List of Ismaili imams for Ismaili imams.\"\n\"See details under Zaidiyyah, Islamic history of Yemen and imams of Yemen.\"\nImams as secular rulers.\nAt times, imams have held both secular and religious authority. This was the case in Oman among the Kharijite or Ibadi sects. At times, the imams were elected. At other times the position was inherited, as with the Yaruba dynasty from 1624 and 1742. See List of rulers of Oman, the Rustamid dynasty: 776\u2013909, Nabhani dynasty: 1154\u20131624, the Yaruba dynasty: 1624\u20131742, the Al Said: 1744\u2013present for further information. The Imamate of Futa Jallon (1727\u20131896) was a Fulani state in West Africa where secular power alternated between two lines of hereditary imams, or \"almami\". In the Zaidi Shiite sect, imams were secular as well as spiritual leaders who held power in Yemen for more than a thousand years. In 897, a Zaidi ruler, al-Hadi ila'l-Haqq Yahya, founded a line of such imams, a theocratic form of government which survived until the second half of the 20th century (See details under Zaidiyyah, History of Yemen, Imams of Yemen). Saudi leaders were also referred to as \"imams\", until that term was retired by Ibn Saud to be replaced by \"king\".\nRuhollah Khomeini is officially referred to as imam in Iran. Several Iranian places and institutions are named \"Imam Khomeini\", including a city, an international airport, a hospital, and a university.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15242", "revid": "50709417", "url": "https://en.wikipedia.org/wiki?curid=15242", "title": "Instrument flight rules", "text": "Civil aviation regulations for flight on instruments\nIn aviation, instrument flight rules (IFR) is one of two sets of regulations governing all aspects of civil aviation aircraft operations; the other is visual flight rules (VFR).\nThe U.S. Federal Aviation Administration's (FAA) \"Instrument Flying Handbook\" defines IFR as: \"Rules and regulations established by the FAA to govern flight under conditions in which flight by outside visual reference is not safe. IFR flight depends upon flying by reference to instruments in the flight deck, and navigation is accomplished by reference to electronic signals.\" It is also a term used by pilots and controllers to indicate the type of flight plan an aircraft is flying, such as an IFR or VFR flight plan.\nBasic information.\nComparison to visual flight rules.\nIt is possible and fairly straightforward, in relatively clear weather conditions, to fly an aircraft solely by reference to outside visual cues, such as the horizon to maintain orientation, nearby buildings and terrain features for navigation, and other aircraft to maintain separation. This is known as operating the aircraft under visual flight rules (VFR), and is the most common mode of operation for small aircraft. However, it is safe to fly VFR only when these outside references can be clearly seen from a sufficient distance. When flying through or above clouds, or in fog, rain, dust or similar low-level weather conditions, these references can be obscured. Thus, cloud ceiling and flight visibility are the most important variables for safe operations during all phases of flight.&lt;ref name=\"NASA/CR-2000-210288\"&gt; &lt;/ref&gt; The minimum weather conditions for ceiling and visibility for VFR flights are defined in FAR Part 91.155, and vary depending on the type of airspace in which the aircraft is operating, and on whether the flight is conducted during daytime or nighttime. However, typical daytime VFR minimums for most airspace is 3 statute miles of flight visibility and a distance from clouds of 500 feet below, 1,000 feet above, and 2,000 feet horizontally. Flight conditions reported as equal to or greater than these VFR minimums are referred to as visual meteorological conditions (VMC).\nAny aircraft operating under VFR must have the required equipment on board, as described in FAR Part 91.205 (which includes some instruments necessary for IFR flight). VFR pilots \"may\" use cockpit instruments as secondary aids to navigation and orientation, but are not required to; the view outside of the aircraft is the primary source for keeping the aircraft straight and level (orientation), flying to the intended destination (navigation), and avoiding obstacles and hazards (separation).\nVisual flight rules are generally simpler than instrument flight rules, and require significantly less training and practice. VFR provides a great degree of freedom, allowing pilots to go where they want, when they want, and allows them a much wider latitude in determining how they get there.\nInstrument flight rules.\nWhen operation of an aircraft under VFR is not safe, because the visual cues outside the aircraft are obscured by weather, instrument flight rules must be used instead. IFR permits an aircraft to operate in instrument meteorological conditions (IMC), which is essentially any weather condition less than VMC but in which aircraft can still operate safely. Use of instrument flight rules is also required when flying in \"Class A\" airspace regardless of weather conditions. Class A airspace extends from 18,000 feet above mean sea level to flight level 600 (60,000 feet pressure altitude) above the contiguous 48 United States and overlying the waters within 12 miles thereof. Flight in Class A airspace requires pilots and aircraft to be instrument equipped and rated and to be operating under instrument flight rules (IFR). In many countries commercial airliners and their pilots must operate under IFR as the majority of flights enter Class A airspace. Procedures and training are significantly more complex compared to VFR instruction, as a pilot must demonstrate competency in conducting an entire cross-country flight solely by reference to instruments.\nInstrument pilots must carefully evaluate weather, create a detailed flight plan based around specific instrument departure, en route, and arrival procedures, and dispatch the flight.\nSeparation and clearance.\nThe distance by which an aircraft avoids obstacles or other aircraft is termed \"separation\". The most important concept of IFR flying is that separation is maintained regardless of weather conditions. In controlled airspace, air traffic control (ATC) separates IFR aircraft from obstacles and other aircraft using a flight \"clearance\" based on route, time, distance, speed, and altitude. ATC monitors IFR flights on radar, or through aircraft position reports in areas where radar coverage is not available. Aircraft position reports are sent as voice radio transmissions. In the United States, a flight operating under IFR is required to provide position reports unless ATC advises a pilot that the plane is in radar contact. The pilot must resume position reports after ATC advises that radar contact has been lost, or that radar services are terminated.\nIFR flights in controlled airspace require an ATC \"clearance\" for each part of the flight. A clearance always specifies a \"clearance limit\", which is the farthest the aircraft can fly without a new clearance. In addition, a clearance typically provides a heading or route to follow, altitude, and communication parameters, such as frequencies and transponder codes.\nIn uncontrolled airspace, ATC clearances are unavailable. In some states a form of separation is provided to certain aircraft in uncontrolled airspace as far as is practical (often known under ICAO as an advisory service in class G airspace), but separation is not mandated nor widely provided.\nDespite the protection offered by flight in controlled airspace under IFR, the ultimate responsibility for the safety of the aircraft rests with the pilot in command, who can refuse clearances.\nWeather.\nIt is essential to differentiate between flight plan type (VFR or IFR) and weather conditions (VMC or IMC). While current and forecast weather may be a factor in deciding which type of flight plan to file, weather conditions themselves do not affect one's filed flight plan. For example, an IFR flight that encounters visual meteorological conditions (VMC) en route does not automatically change to a VFR flight, and the flight must still follow all IFR procedures regardless of weather conditions. In the US, weather conditions are forecast broadly as VFR, MVFR (marginal visual flight rules), IFR, or LIFR (low instrument flight rules).\nThe main purpose of IFR is the safe operation of aircraft in instrument meteorological conditions (IMC). The weather is considered to be MVFR or IMC when it does not meet the minimum requirements for visual meteorological conditions (VMC). To operate safely in IMC (\"actual instrument conditions\"), a pilot controls the aircraft relying on flight instruments and ATC provides separation.\nIt is important not to confuse IFR with IMC. A significant amount of IFR flying is conducted in visual meteorological conditions (VMC). Anytime a flight is operating in VMC and in a volume of airspace in which VFR traffic can operate, the crew is responsible for seeing and avoiding VFR traffic; however, because the flight is conducted under instrument flight rules, ATC still provides separation services from other IFR traffic, and can in many cases also advise the crew of the location of VFR traffic near the flight path.\nAlthough dangerous and illegal, a certain amount of VFR flying is conducted in IMC. A scenario is a VFR pilot taking off in VMC conditions, but encountering deteriorating visibility while en route. Continued VFR flight into IMC can lead to spatial disorientation of the pilot which is the cause of a significant number of general aviation crashes. VFR flight into IMC is distinct from \"VFR-on-top\", an IFR procedure in which the aircraft operates in VMC using a hybrid of VFR and IFR rules, and \"VFR over the top\", a VFR procedure in which the aircraft takes off and lands in VMC but flies above an intervening area of IMC. Also possible in many countries is \"Special VFR\" flight, where an aircraft is explicitly granted permission to operate VFR within the controlled airspace of an airport in conditions technically less than VMC; the pilot asserts they have the necessary visibility to fly despite the weather, must stay in contact with ATC, and cannot leave controlled airspace while still below VMC minimums.\nDuring flight under IFR, there are no visibility requirements, so flying through clouds (or other conditions where there is zero visibility outside the aircraft) is legal and safe. However, there are still minimum weather conditions that must be present in order for the aircraft to take off or to land; these vary according to the kind of operation, the type of navigation aids available, the location and height of terrain and obstructions in the vicinity of the airport, equipment on the aircraft, and the qualifications of the crew. For example, Reno-Tahoe International Airport (KRNO) in a mountainous region has significantly different instrument approaches for aircraft landing on the same runway surface, but from opposite directions. Aircraft approaching from the north must make visual contact with the airport at a higher altitude than when approaching from the south because of rapidly rising terrain south of the airport. This higher altitude allows a flight crew to clear the obstacle if a landing is aborted. In general, each specific instrument approach specifies the minimum weather conditions to permit landing.\nAlthough large airliners, and increasingly, smaller aircraft, carry their own terrain awareness and warning system (TAWS), these are primarily backup systems providing a last layer of defense if a sequence of errors or omissions causes a dangerous situation.\nNavigation.\nBecause IFR flights often take place without visual reference to the ground, a means of navigation other than looking outside the window is required. A number of navigational aids are available to pilots, including ground-based systems such as DME/VORs and NDBs as well as the satellite-based GPS/GNSS system. Air traffic control may assist in navigation by assigning pilots specific headings (\"radar vectors\"). The majority of IFR navigation is given by ground- and satellite-based systems, while radar vectors are usually reserved by ATC for sequencing aircraft for a busy approach or transitioning aircraft from takeoff to cruise, among other things.\nProcedures.\nSpecific procedures allow IFR aircraft to transition safely through every stage of flight. These procedures specify how an IFR pilot should respond, even in the event of a complete radio failure, and loss of communications with ATC, including the expected aircraft course and altitude.\nDepartures are described in an IFR clearance issued by ATC prior to takeoff. The departure clearance may contain an assigned heading, one or more waypoints, and an initial altitude to fly. The clearance can also specify a departure procedure (DP) or standard instrument departure (SID) that should be followed unless \"NO DP\" is specified in the notes section of the filed flight plan.\nEn route flight is described by IFR charts showing navigation aids, fixes, and standard routes called \"airways\". Aircraft with appropriate navigational equipment such as GPS, are also often cleared for a \"direct-to\" routing, where only the destination, or a few navigational waypoints are used to describe the route that the flight will follow. ATC will assign altitudes in its initial clearance or amendments thereto, and navigational charts indicate minimum safe altitudes for airways.\nThe approach portion of an IFR flight may begin with a standard terminal arrival route (STAR), describing common routes to fly to arrive at an initial approach fix (IAF) from which an instrument approach commences. \nAn instrument approach terminates either by the pilot acquiring sufficient visual reference to proceed to the runway, or with a missed approach because the required visual reference is not seen in time.\nQualifications.\nPilot.\nTo fly under IFR, a pilot must have an instrument rating and must be \"current\" (meet recency of experience requirements).\nIn the United States, to file and fly under IFR, a pilot must be instrument-rated and, within the preceding six months, have flown six instrument approaches, as well as holding procedures and course interception and tracking with navaids. Flight under IFR beyond six months after meeting these requirements is not permitted; however, currency may be reestablished within the next six months by completing the requirements above. Beyond the twelfth month, examination (\"instrument proficiency check\") by an instructor is required.\nPracticing instrument approaches can be done either in the instrument meteorological conditions or in visual meteorological conditions \u2013 in the latter case, a safety pilot is required so that the pilot practicing instrument approaches can wear a view-limiting device which restricts their field of view to the instrument panel. A safety pilot's primary duty is to observe and avoid other traffic.\nIn the UK, an IR (UK restricted) - formerly the \"IMC rating\" - which permits flight under IFR in airspace classes B to G in instrument meteorological conditions, a non-instrument-rated pilot can also elect to fly under IFR in visual meteorological conditions outside controlled airspace. Compared to the rest of the world, the UK's flight crew licensing regime is somewhat unusual in its licensing for meteorological conditions and airspace, rather than flight rules.\nAircraft.\nThe aircraft must be equipped and type-certified for instrument flight, and the related navigational equipment must have been inspected or tested within a specific period of time prior to the instrument flight.\nIn the United States, instruments required for IFR flight in addition to those that are required for VFR flight are: heading indicator, sensitive altimeter adjustable for barometric pressure, clock with a sweep-second pointer or digital equivalent, attitude indicator, radios and suitable avionics for the route to be flown, alternator or generator, gyroscopic rate-of-turn indicator that is either a turn coordinator or the turn and bank indicator. From 1999 single-engine helicopters could not be FAA-certified for IFR. Recently, however, Bell and Leonardo have certified the single engine helicopters for instrument flight rules.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15244", "revid": "220260680", "url": "https://en.wikipedia.org/wiki?curid=15244", "title": "In Vitro Fertilization", "text": ""}
{"id": "15245", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15245", "title": "Ismail Khan", "text": "Afghan politician and former warlord (born 1946)\nMuhammad Ismail Khan (born 1946) is an Afghan former military officer, warlord and politician who served as Minister of Energy and Water from 2005 to 2013 and before that served as the governor of Herat Province. Originally a captain in the Afghan Army, he is widely known as a former warlord who controlled a large Afghan mujahideen force, mainly his fellow Tajiks from western Afghanistan, during the Soviet\u2013Afghan War. \nHis reputation gained him the nickname \"Lion of Herat\". Ismail Khan was a key member of the now exiled political party Jamiat-e Islami and of the now defunct United National Front party. In 2021, Ismail Khan returned to arms to help defend Herat from the Taliban's offensive, which he and the Afghan Army lost. He was captured by the Taliban forces and then reportedly fled to Iran on 16 August 2021.\nEarly life and rise to power.\nKhan was born in 1946 in the Shindand District of Herat Province in Afghanistan. An ethnic Tajik, his family is from the Chahar-Mahal neighborhood of Shindand.\nIn early 1979, Ismail Khan was a captain in the Afghan Army based in the western city of Herat. In early March of that year, there was a protest in front of the Communist governor's palace against the arrests and assassinations being carried out in the countryside by the Khalq government. The governor's troops opened fire on the demonstrators, who proceeded to storm the palace and hunt down Soviet advisers. The Herat garrison mutinied and joined the revolt in what is called the Herat uprising, with Ismail Khan and other officers distributing all available weapons to the insurgents. The government led by Nur Mohammed Taraki responded, pulverizing the city using Soviet supplied bombers and killing up to 24,000 citizens in less than a week. This event marked the opening salvo of the rebellion which led to the Soviet military intervention in Afghanistan in December 1979. Ismail Khan escaped to the countryside where he began to assemble a local rebel force.\nDuring the ensuing war, he became the leader of the western command of Burhanuddin Rabbani's Jamiat-e-Islami, political party. With Ahmad Shah Massoud, he was one of the most respected mujahideen leaders. In 1992, three years after the Soviet withdrawal from Afghanistan, the mujahideen captured Herat and Ismail Khan became governor.\nIn 1995, he successfully defended his province against the Taliban, in cooperation with defense minister Ahmad Shah Massoud. Khan even tried to attack the Taliban stronghold of Kandahar, but was repulsed. Later in September, an ally of the Jamiat, Uzbek General Abdul Rashid Dostum changed sides and attacked Herat. Ismail Khan was forced to flee to neighboring Iran with 8,000 men, and the Taliban took over Herat Province.\nTwo years later, while organizing opposition to the Taliban in Faryab area, he was betrayed and captured by Abdul Majid Rouzi who had defected to the Taliban along with Abdul Malik Pahlawan, then one of Dostum's deputies. Then in March 1999 he escaped from Kandahar prison. During the U.S. intervention in Afghanistan, he fought against the Taliban within the United Islamic Front for the Salvation of Afghanistan (Northern Alliance) and thus regained his position as Governor of Herat after they were victorious in December 2001.\nKarzai administration and return to Afghanistan.\nAfter returning to Herat, Ismail Khan quickly consolidated his control over the region. He took over control of the city from the local ulema and quickly established control over the trade route between Herat and Iran, a large source of revenue. As Emir of Herat, Ismail Khan exercised great autonomy, providing social welfare for Heratis, expanding his power into neighbouring provinces, and maintaining direct international contacts. Although hated by the educated in Herat and often accused of human rights abuses, Ismail Khan's regime provided security, paid government employees, and made investments in public services. However, during his tenure as governor, Ismail Khan was accused of ruling his province like a private fiefdom, leading to increasing tensions with the Afghan Transitional Administration. In particular, he refused to pass on to the government the revenues gained from custom taxes on goods from Iran and Turkmenistan.\nOn 13 August 2003, President Karzai removed Governor Ismail Khan from his command of the 4th Corps. This was announced as part of a programme removing the ability of officials to hold both civilian and military posts.\nIsmail Khan was ultimately removed from power in March 2004 due to pressure by neighbouring warlords and the central Afghan government. Various sources have presented different versions of the story, and the exact dynamics cannot be known with certainty. What is known is that Ismail Khan found himself at odds with a few regional commanders who, although theoretically his subordinates, attempted to remove him from power. Ismail Khan claims that these efforts began with a botched assassination attempt. Afterwards, these commanders moved their forces near Herat. Ismail Khan, unpopular with the Herati military class, was slow to mobilise his forces, perhaps waiting for the threat to Herat to become existential as a means to motivate his forces. However, the conflict was stopped with the intervention of International Security Assistance Force forces and soldiers of the Afghan National Army, freezing the conflict in its tracks. Ismail Khan's forces even fought skirmishes with the Afghan National Army, in which his son, Mirwais Sadiq was killed. Because Ismail Khan was contained by the Afghan National Army, the warlords who opposed him were quickly able to occupy strategic locations unopposed. Ismail Khan was forced to give up his governorship and to go to Kabul, where he served in Hamid Karzai's cabinet as the Minister of Energy.\nIn 2005 Ismail Khan became the Minister of Water and Energy.\nIn late 2012, the Government of Afghanistan accused Ismail Khan of illegally distributing weapons to his supporters. About 40 members of the country's Parliament requested Ismail Khan to answer their queries. The government believes that Khan is attempting to create some kind of disruption in the country.\nAssassination attempt.\nOn September 27, 2009, Ismail Khan survived a suicide blast that killed 4 of his bodyguards in Herat, in western Afghanistan. He was driving to Herat Airport when a powerful explosion occurred close to his convoy of vehicles. Taliban spokesman Zabiullah Mujahid claimed responsibility and said Khan was the target.\nTestimony requested by a Guantanamo captive.\nGuantanamo captive Abdul Razzaq Hekmati requested Ismail Khan's testimony, when he was called before a Combatant Status Review Tribunal. \nIsmail Khan, like Afghan Minister of Defense Rahim Wardak, was one of the high-profile Afghans that those conducting the Tribunals ruled were \"not reasonably available\" to give a statement on a captive's behalf because they could not be located.\nHekmati had played a key role in helping Ismail Khan escape from the Taliban in 1999.\nHekmati stood accused of helping Taliban leaders escape from the custody of Hamid Karzai's government.\nCarlotta Gall and Andy Worthington interviewed Ismail Khan for a new \"The New York Times\" article after Hekmati died of cancer in Guantanamo. \nAccording to the \"New York Times\"\nIsmail Khan said he personally buttonholed the American ambassador to tell him that Hekmati was innocent, and should be released. In contrast, Hekmati was told that the State Department had been unable to locate Khan.\n2021 Taliban offensive and capture.\nIn July 2021, Ismail Khan mobilized hundreds of his loyalists in Herat in support of the Afghan Armed Forces to defend the city from an offensive by the Taliban. Despite this, the city fell on 12 August 2021. After trying to escape by helicopter, Khan was captured by the Taliban. The Taliban interviewed him shortly after and claimed that he and his forces have joined them. After negotiating with the Taliban, he was allowed to return to his residence.\nAfter leaving Taliban custody, as of August 2021 Khan is living in Mashhad, Iran. He said that a conspiracy was responsible for Herat being captured by the Taliban.\nControversy.\nIsmail Khan is a controversial figure. Reporters Without Borders has charged him with muzzling the press and ordering attacks on journalists. Also Human Rights Watch has accused him of human rights abuses.\nNevertheless, he remains a popular figure for some in Afghanistan. Unlike other mujahideen commanders, Khan has not been linked to large-scale massacres and atrocities such as those committed after the capture of Kabul in 1992. Following news of his dismissal, rioting broke out in the streets of Herat, and President Karzai had to ask him to make a personal appeal for calm.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15247", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=15247", "title": "Improvisational theater", "text": ""}
{"id": "15250", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15250", "title": "Indigo", "text": "Color\nIndigo is a term used for a number of hues in the region of blue. The word comes from the ancient dye of the same name. The term \"indigo\" can refer to the color of the dye, various colors of fabric dyed with indigo dye, a spectral color, one of the seven colors of the rainbow as described by Isaac Newton, or a region on the color wheel, and can include various shades of blue, ultramarine, and green-blue. Since the web era, the term has also been used for various purple and violet hues identified as \"indigo\", based on use of the term \"indigo\" in HTML web page specifications.\nThe word \"indigo\" comes from the Latin word , meaning \"Indian\", as the naturally based dye was originally exported to Europe from India.\nThe Early Modern English word indigo referred to the dye, not to the color (hue) itself, and indigo is not traditionally part of the basic color-naming system.\nThe first known recorded use of indigo as a color name in English was in 1289. Due to the extensive knowledge of indigo cultivation by enslaved West Africans, indigo became a major cash crop in the American colonies.\nNewton regarded indigo as a color in the visible spectrum, as well as one of the seven colors of the rainbow: the color between blue and violet; however, sources differ as to its actual position in the electromagnetic spectrum. Later scientists have concluded that what Newton called \"blue\" was what is now called cyan or blue-green; and what Newton called \"indigo\" was what is now called blue.\nIn the 1980s, programmers produced a somewhat arbitrary list of color names for the X Window computer operating system, resulting in the HTML and CSS specifications issued in the 1990s using the term \"indigo\" for a dark purple hue. This has resulted in violet and purple hues also being associated with the term \"indigo\" since that time.\nBecause of the Abney effect, pinpointing indigo to a specific hue value in the HSV color wheel is elusive, as a higher HSV saturation value shifts the hue towards blue. However, on the new CIECAM16 standard, the hues values around 290\u00b0 may be thought of as indigo, depending on the observer.\nHistory.\nIndigo as a dye.\n\"Indigo dye\" is a blue color, obtained from several different types of plants. The indigo plant (Indigofera tinctoria) often called \"true indigo\" probably produces the best results, although several others are close in color: Japanese indigo (Polygonum tinctoria), Natal indigo (\"Indigofera arrecta\"), Guatemalan indigo (\"Indigofera suffruticosa\"), Chinese indigo (\"Persicaria tinctoria\"), and woad (\"Isatis tinctoria\").\n\"Indigofera tinctoria\" and related species were cultivated in West Africa, East Asia, Egypt, India, Bangladesh and Peru in antiquity. The earliest direct evidence for the use of indigo dates to around 4000 BC and comes from Huaca Prieta, in contemporary Peru. Pliny the Elder mentions India as the source of the dye after which it was named. It was imported from there in small quantities via the Silk Road.\nThe Ancient Greek term for the dye was (\"indikon pharmakon\", \"Indian dye\"), which, adopted to Latin as (a second declension noun) or \"indico\" (oblique case) and via Portuguese, gave rise to the modern word indigo.\nIn Ancient West Africa, \nIn early Europe, the main source was from the woad plant \"Isatis tinctoria\", also known as pastel. For a long time, woad was the main source of blue dye in Europe. Woad was replaced by \"true indigo\", as trade routes opened up. Plant sources have now been largely replaced by synthetic dyes.\nSpanish explorers discovered an American species of indigo and began to cultivate the product in Guatemala. The English and French subsequently began to encourage indigo cultivation in their colonies in the West Indies.\nIn North America, indigo is native to the coastal regions of the American South and was cultivated as early as 1622. Eliza Lucas, whose father sent her indigo seeds as a gift, is credited with introducing the plant to the southern colonies. However, it was enslaved West Africans from present-day Mali, Guinea, Ivory Coast, Senegal, and Nigeria who actually cultivated those indigo seeds and introduced indigo to North America in the form of dye, where it became the colony's second-most important cash crop (after rice). Before the Revolutionary War, indigo accounted for more than one-third of the value of exports from the American colonies.\nIsaac Newton's classification of indigo as a spectral color.\nNewton introduced indigo as one of the seven base colors of his work. In the mid-1660s, when Newton bought a pair of prisms at a fair near Cambridge, the East India Company had begun importing indigo dye into England, supplanting the homegrown woad as source of blue dye. In a pivotal experiment in the history of optics, the young Newton shone a narrow beam of sunlight through a prism to produce a rainbow-like band of colors on the wall. In describing this optical spectrum, Newton acknowledged that the spectrum had a continuum of colors, but named seven: \"The originall or primary colours are Red, yellow, Green, Blew, &amp; a violet purple; together with Orang, Indico, &amp; an indefinite varietie of intermediate gradations.\" He linked the seven prismatic colors to the seven notes of a western major scale, as shown in his color wheel, with orange and indigo as the semitones. Having decided upon seven colors, he asked a friend to repeatedly divide up the spectrum that was projected from the prism onto the wall:\nI desired a friend to draw with a pencil lines cross the image, or pillar of colours, where every one of the seven aforenamed colours was most full and brisk, and also where he judged the truest confines of them to be, whilst I held the paper so, that the said image might fall within a certain compass marked on it. And this I did, partly because my own eyes are not very critical in distinguishing colours, partly because another, to whom I had not communicated my thoughts about this matter, could have nothing but his eyes to determine his fancy in making those marks.\nIndigo is therefore counted as one of the traditional colors of the rainbow, the order of which is given by the mnemonics \"Richard of York gave battle in vain\" and \"Roy G. Biv\". James Clerk Maxwell and Hermann von Helmholtz accepted indigo as an appropriate name for the color flanking violet in the spectrum.\nLater scientists concluded that Newton named the colors differently from current usage.\nAccording to Gary Waldman, \"A careful reading of Newton's work indicates that the color he called indigo, we would normally call blue; his blue is then what we would name blue-green or cyan.\" If this is true, Newton's seven spectral colors would have been:\nThe human eye does not readily differentiate hues in the wavelengths between what are now called blue and violet. If this is where Newton meant indigo to lie, most individuals would have difficulty distinguishing indigo from its neighbors. According to Isaac Asimov, \"It is customary to list indigo as a color lying between blue and violet, but it has never seemed to me that indigo is worth the dignity of being considered a separate color. To my eyes, it seems merely deep blue.\"\n1800s.\nIn 1821, Abraham Werner published \"Werner's Nomenclature of Colours\", where indigo, called \"indigo blue\", is classified as a blue hue, and not listed among the violet hues. He writes that the color is composed of \"Berlin blue, a little black, and a small portion of apple green,\" and indicating it is the color of blue copper ore, with Berlin blue being described as the color of a blue jay's wing, a hepatica flower, or a blue sapphire.\nAccording to an article, \"Definition of the Color Indigo\" published in \"Nature\" magazine in the late 1800s, Newton's use of the term \"indigo\" referred to a spectral color between blue and violet. However, the article states that Wilhelm von Bezold, in his treatise on color, disagreed with Newton's use of the term, on the basis that the pigment indigo was a darker hue than the spectral color; and furthermore, Professor Ogden Rood points out that indigo pigment corresponds to the cyan-blue region of the spectrum, lying between blue and green, although darker in hue. Rood considers that artificial ultramarine pigment is closer to the point of the spectrum described as \"indigo\", and proposed renaming that spectral point as \"ultramarine\". The article goes on to state that comparison of the pigments, both dry and wet, with Maxwell's discs and with the spectrum, that indigo is almost identical to Prussian blue, stating that it \"certainly does not lie on the violet side of 'blue.'\" When scraped, a lump of indigo pigment appears more violet, and if powdered or dissolved, becomes greenish.\nModern spectral classification.\nSeveral modern sources place indigo in the electromagnetic spectrum between 420 and 450 nanometers, which lies on the short-wave side of color wheel (RGB) blue, towards (spectral) violet.\nThe correspondence of this definition with colors of actual indigo dyes, though, is disputed. Optical scientists Hardy and Perrin list indigo as between 445 and 464\u00a0nm wavelength, which occupies a spectrum segment from roughly the color wheel (RGB) blue extending to the long-wave side, towards azure.\nOther modern color scientists, such as Bohren and Clothiaux (2006), and J.W.G. Hunt (1980), divide the spectrum between violet and blue at about 450\u00a0nm, with no hue specifically named indigo.\nWeb era.\nOrigin of \"Indigo\" as a name for purple in web pages.\nTowards the end of the 20th century, purple colors also became referred to as \"indigo\". In the 1980s, computer programmers Jim Gettys, Paul Ravelling, John C. Thomas and Jim Fulton produced a list of colors for the X Window Operating System. The color identified as \"indigo\" was not the color indigo (as generally understood at the time), but was actually a dark purple hue; the programmers assigned it the hex code #4B0082 \u2003. This collection of color names was somewhat arbitrary: Thomas used a box of 72 Crayola crayons as a standard, whereas Ravelling used color swabs from the now-defunct Sinclair Paints company, resulting in the color list for version X11 of the operating system containing fanciful color names such as \"papaya whip\", \"blanched almond\" and \"peach puff\". The database was also criticised for its many inconsistencies, such as \"dark gray\" being lighter than \"gray\", and for the color distribution being uneven, tending towards reds and greens at the expense of blues.\nIn the 1990s, this list which came with version X11 became the basis of the HTML and CSS color rendition used in websites and web design. This resulted in the name \"Indigo\" being associated with purple and violet hues in web page design and graphic design. Physics author John Spacey writes on the website \"Simplicable\" that the X11 programmers did not have any background in color theory, and that as these names are used by web designers and graphic designers, the name \"indigo\" has since that time been strongly associated with purple or violet. Spacey writes, \"As such, a few programmers accidentally repurposed a color name that was known to civilisations for thousands of years.\"\nCrayola crayon colors.\nThe Crayola company released an indigo crayon in 1999, with the Crayola website using the hex code #4F49C6 \u2003 to approximate the crayon color. The 2001 iron indigo crayon is portrayed using hex code #184FA1 \u2003. The 2004 indigo crayon color is depicted by #5D76CB \u2003, and the 2019 iridescent indigo is portrayed by #3C32CD \u2003.\nDistinction among tones of indigo.\nLike many other colors (orange, rose, and violet are the best-known), indigo gets its name from an object in the natural world\u2014the plant named indigo once used for dyeing cloth (see also Indigo dye).\nThe color pigment indigo is equivalent to the web color indigo and approximates the color indigo that is usually reproduced in pigments and colored pencils.\nThe color of indigo dye is a different color from either spectrum indigo or pigment indigo. This is the actual color of the dye. A vat full of this dye is a darker color, approximating the web color midnight blue.\nThe color \"electric indigo\" is a bright and saturated color between the traditional indigo and violet. This is the brightest color indigo that can be approximated on a computer screen; it is a color located between the (primary) blue and the color violet of the RGB color wheel.\nThe web color blue violet or deep indigo is a tone of indigo brighter than pigment indigo, but not as bright as electric indigo.\nListed below are several indigo hues, some of which have included the word \"indigo\", with the adoption of HTML color names in the World Wide Web era.\nIndigo dye color.\n\"Indigo dye\" is a greenish dark blue color, obtained from either the leaves of the tropical Indigo plant (\"Indigofera\"), or from woad (\"Isatis tinctoria\"), or the Chinese indigo (\"Persicaria tinctoria\"). Many societies make use of the \"Indigofera\" plant for producing different shades of blue. Cloth that is repeatedly boiled in an indigo dye bath-solution (boiled and left to dry, boiled and left to dry, etc.), the blue pigment becomes darker on the cloth. After dyeing, the cloth is hung in the open air to dry.\nA Native American woman described the process used by the Cherokee Indians when extracting the dye:\nWe raised our indigo which we cut in the morning while the dew was still on it; then we put it in a tub and soaked it overnight, and the next day we foamed it up by beating it with a gourd. We let it stand overnight again, and the next day rubbed tallow on our hands to kill the foam. Afterwards, we poured the water off, and the sediment left in the bottom we would pour into a pitcher or crock to let it get dry, and then we would put it into a poke made of cloth (i.e. sack made of coarse cloth) and then when we wanted any of it to dye [there]with, we would take the dry indigo.\nIn Sa Pa, Vietnam, the tropical Indigo (\"Indigo tinctoria\") leaves are harvested and, while still fresh, placed inside a tub of room-temperature to lukewarm water where they are left to sit for 3 to 4 days and allowed to ferment, until the water turns green. Afterwards, crushed limestone (pickling lime) is added to the water, at which time the water with the leaves are vigorously agitated for 15 to 20 minutes, until the water turns blue. The blue pigment settles as sediment at the bottom of the tub. The sediment is scooped out and stored. When dyeing cloth, the pigment is then boiled in a vat of water; the cloth (usually made from yarns of hemp) is inserted into the vat for absorbing the dye. After hanging out to dry, the boiling process is repeated as often as needed to produce a darker color.\nIndigo (color wheel).\nIn a RGB color space, \"Indigo(color wheel)\" is composed of 25.1% red, 0% green and 100% blue. Whereas in a CMYK color space, it is composed of 74.9% cyan, 100% magenta, 0% yellow and 0% black. It has a hue angle of 255.1 degrees, a saturation of 100% and a lightness of 50%. Indigo(color wheel) could be obtained by blending violet with blue.\nElectric indigo.\n\"Electric indigo\" is brighter than the pigment indigo reproduced above. When plotted on the CIE chromaticity diagram, this color is at 435 nanometers, in the middle of the portion of the spectrum traditionally considered indigo, i.e., between 450 and 420 nanometers. This color is only an approximation of spectral indigo, since actual spectral colors are outside the gamut of the sRGB color system.\nDeep indigo (web color blue-violet).\nAt right is displayed the web color \"blue-violet\", a color intermediate in brightness between electric indigo and pigment indigo. It is also known as \"deep indigo\".\nWeb color indigo.\nThe color box on the right displays the web color indigo. Its hue is closer to violet than to indigo dye for which the color is named.\nTropical indigo.\n'Tropical Indigo' is the color that is called \"a\u00f1il\" in the \"Gu\u00eda de coloraciones\" (Guide to colorations) by Rosa Gallego and\nJuan Carlos Sanz, a color dictionary published in 2005 that is widely popular in the Hispanophone realm.\nIn culture.\nLiterature.\nMarina Warner's novel \"Indigo\" (1992) is a retelling of Shakespeare's \"The Tempest\" and features the production of indigo dye by Sycorax.\nMilitary.\nThe French Army adopted dark blue indigo at the time of the French Revolution, as a replacement for the white uniforms previously worn by the Royal infantry regiments. In 1806, Napoleon decided to restore the white coats because of shortages of indigo dye imposed by the British continental blockade. However, the greater practicability of the blue color led to its retention, and indigo remained the dominant color of French military coats until 1914.\nSpirituality.\nThe spiritualist applications use electric indigo, because the color is positioned between blue and violet on the spectrum.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15251", "revid": "41631503", "url": "https://en.wikipedia.org/wiki?curid=15251", "title": "International Monetary Fund", "text": "International financial institution\nThe International Monetary Fund (IMF) is an international financial institution and a specialized agency of the United Nations, headquartered in Washington, D.C. It consists of 191 member countries, and its stated mission is \"working to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world.\" The IMF acts as a lender of last resort to its members experiencing actual or potential balance of payments crises.\nEstablished in July 1944 at the Bretton Woods Conference based on the ideas of Harry Dexter White and John Maynard Keynes, the IMF came into formal existence in 1945 with 29 member countries and the goal of reconstructing the international monetary system. For its first three decades, the IMF oversaw the Bretton Woods system of fixed exchange rate arrangements. Following the collapse of this system in 1971, the Fund's role shifted to managing balance-of-payments difficulties and international financial crises, becoming a key institution in the era of globalization.\nThrough a quota system, countries contribute funds to a pool from which they can borrow if they experience balance-of-payments problems; a country's quota also determines its voting power. As a condition for loans, the IMF often requires borrowing countries to undertake policy reforms, known as structural adjustment. The organization also provides technical assistance and economic surveillance of its members' economies.\nThe IMF's loan conditions have been criticized for imposing austerity measures that can hinder economic recovery and harm the most vulnerable populations. Critics argue that the Fund's policies limit the economic sovereignty of borrowing nations and that its governance structure is dominated by Western countries, which hold a disproportionate share of voting power. The current managing director and chairperson is Bulgarian economist Kristalina Georgieva, who has held the position since 1 October 2019.\nHistory.\n20th century.\nThe IMF was originally laid out as a part of the Bretton Woods system exchange agreement in 1944. During the Great Depression, countries sharply raised barriers to trade in an attempt to improve their failing economies. This led to the devaluation of national currencies and a decline in world trade.\nThis breakdown in international monetary cooperation created a need for oversight. The representatives of 45 governments met at the Bretton Woods Conference in the Mount Washington Hotel in Bretton Woods, New Hampshire, in the United States, to discuss a framework for postwar international economic cooperation and how to rebuild Europe.\nThere were two views on the role the IMF should assume as a global economic institution. American delegate Harry Dexter White foresaw an IMF that functioned more like a bank, making sure that borrowing states could repay their debts on time. Most of White's plan was incorporated into the final acts adopted at Bretton Woods. British economist John Maynard Keynes, on the other hand, imagined that the IMF would be a cooperative fund upon which member states could draw to maintain economic activity and employment through periodic crises. This view suggested an IMF that helped governments and act as the United States government had during the New Deal to the great depression of the 1930s.\nThe agreement at Bretton Woods implied that both the IMF and the World Bank would be headquartered in the United States. US Treasury Secretary Henry Morgenthau Jr. intended them to be located in New York, but his successor Fred M. Vinson unilaterally decided that they would be in Washington, D.C. instead, noting that \"the institutions would be fatally prejudiced in American opinion if they were placed in New York, since they would then come under the taint of 'international finance'\". The IMF formally came into existence on 27 December 1945, when the first 29 countries ratified its Articles of Agreement. By the end of 1946 the IMF had grown to 39 members. On 1 March 1947, the IMF began its financial operations, and on 8 May France became the first country to borrow from it.\nThe IMF was one of the key organizations of the international economic system; its design allowed the system to balance the rebuilding of international capitalism with the maximization of national economic sovereignty and human welfare, also known as embedded liberalism. The IMF's influence in the global economy steadily increased as it accumulated more members. Its membership began to expand in the late 1950s and during the 1960s as many African countries became independent and applied for membership. But the Cold War limited the Fund's membership, with most countries in the Soviet sphere of influence not joining until 1970s and 1980s.\nThe Bretton Woods exchange rate system prevailed until 1971 when the United States government suspended the convertibility of the US$ (and dollar reserves held by other governments) into gold. This is known as the Nixon Shock. The changes to the IMF articles of agreement reflecting these changes were ratified in 1976 by the Jamaica Accords. Later in the 1970s, large commercial banks began lending to states because they were awash in cash deposited by oil exporters. The lending of the so-called money center banks led to the IMF changing its role in the 1980s after a world recession provoked a crisis that brought the IMF back into global financial governance.\nIn the mid-1980s, the IMF shifted its narrow focus from currency stabilization to a broader focus of promoting market-liberalizing reforms through structural adjustment programs. This shift occurred without a formal renegotiation of the organization's charter or operational guidelines. The Ronald Reagan administration, in particular Treasury Secretary James Baker, his assistant secretary David Mulford and deputy assistant secretary Charles Dallara, pressured the IMF to attach market-liberal reforms to the organization's conditional loans.\nDuring the 20th century, the IMF shifted its position on capital controls. Whereas the IMF permitted capital controls at its founding and throughout the 1970s, IMF staff increasingly favored free capital movement from 1980s onwards. This shift happened in the aftermath of an emerging consensus in economics on the desirability of free capital movement, retirement of IMF staff hired in the 1940s and 1950s, and the recruitment of staff exposed to new thinking in economics.\n21st century.\nThe IMF provided two major lending packages in the early 2000s to Argentina (during the 1998\u20132002 Argentine great depression) and Uruguay (after the 2002 Uruguay banking crisis). However, by the mid-2000s, IMF lending was at its lowest share of world GDP since the 1970s.\nIn May 2010, the IMF participated, in 3:11 proportion, in the first Greek bailout that totaled \u20ac110 billion, to address the great accumulation of public debt, caused by continuing large public sector deficits. As part of the bailout, the Greek government agreed to adopt austerity measures that would reduce the deficit from 11% in 2009 to \"well below 3%\" in 2014. The bailout did not include debt restructuring measures such as a haircut, to the chagrin of the Swiss, Brazilian, Indian, Russian, and Argentinian Directors of the IMF, with the Greek authorities themselves (at the time, PM George Papandreou and Finance Minister Giorgos Papakonstantinou) ruling out a haircut.\nA second bailout package of more than \u20ac100\u00a0billion was agreed upon over the course of a few months from October 2011, during which time Papandreou was forced from office. The so-called Troika, of which the IMF is part, are joint managers of this programme, which was approved by the executive directors of the IMF on 15 March 2012 for XDR 23.8\u00a0billion and saw private bondholders take a haircut of upwards of 50%. In the interval between May 2010 and February 2012 the private banks of Holland, France, and Germany reduced exposure to Greek debt from \u20ac122\u00a0billion to \u20ac66\u00a0billion.\nAs of January\u00a02012[ [update]], the largest borrowers from the IMF in order were Greece, Portugal, Ireland, Romania, and Ukraine.\nOn 25 March 2013, a \u20ac10\u00a0billion international bailout of Cyprus was agreed by the Troika, at the cost to the Cypriots of its agreement: to close the country's second-largest bank; to impose a one-time bank deposit levy on Bank of Cyprus uninsured deposits. No insured deposit of \u20ac100k or less were to be affected under the terms of a novel bail-in scheme.\nThe topic of sovereign debt restructuring was taken up by the IMF in April 2013, for the first time since 2005, in a report entitled \"Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework\". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize, and Jamaica. An explanatory interview with deputy director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of \"The Wall Street Journal\".\nIn the October 2013, Fiscal Monitor publication, the IMF suggested that a capital levy capable of reducing Euro-area government debt ratios to \"end-2007 levels\" would require a very high tax rate of about 10%.\nThe Fiscal Affairs department of the IMF, headed at the time by Acting Director Sanjeev Gupta, produced a January 2014 report entitled \"Fiscal Policy and Income Inequality\" that stated that \"Some taxes levied on wealth, especially on immovable property, are also an option for economies seeking more progressive taxation ... Property taxes are equitable and efficient, but underutilized in many economies ... There is considerable scope to exploit this tax more fully, both as a revenue source and as a redistributive instrument.\"\nIn late 2019, the IMF estimated global growth in 2020 to reach 3.4%, but due to the coronavirus, in November 2020, it expected the global economy to shrink by 4.4%.\nIn March 2020, Kristalina Georgieva announced that the IMF stood ready to mobilize $1 trillion as its response to the COVID-19 pandemic. This was in addition to the $50 billion fund it had announced two weeks earlier, of which $5 billion had already been requested by Iran. One day earlier on 11 March, the UK called to pledge \u00a3150 million to the IMF catastrophe relief fund. It came to light on 27 March that \"more than 80 poor and middle-income countries\" had sought a bailout due to the coronavirus.\nOn 13 April 2020, the IMF said that it \"would provide immediate debt relief to 25 member countries under its Catastrophe Containment and Relief Trust (CCRT)\" programme.\nGovernance and membership.\nNot all member countries of the IMF are sovereign states, and therefore not all \"member countries\" of the IMF are members of the United Nations. Amidst \"member countries\" of the IMF that are not member states of the UN are non-sovereign areas with special jurisdictions that are officially under the sovereignty of full UN member states, such as Aruba, Cura\u00e7ao, Hong Kong, and Macao, as well as Kosovo. The corporate members appoint \"ex-officio\" voting members, who are listed below. All members of the IMF are also International Bank for Reconstruction and Development (IBRD) members and vice versa.\nFormer members are Cuba (which left in 1964), and Taiwan, which was ejected from the IMF in 1980 after losing the support of the then United States President Jimmy Carter and was replaced by the People's Republic of China. However, \"Taiwan Province of China\" is still listed in the official IMF indices. Poland withdrew in 1950\u2014allegedly pressured by the Soviet Union\u2014but returned in 1986. The former Czechoslovakia was expelled in 1954 for \"failing to provide required data\" and was readmitted in 1990, after the Velvet Revolution.\nApart from Cuba, the other UN states that do not belong to the IMF are Monaco and North Korea. Liechtenstein became the 191st member on 21 October 2024.\nQualifications.\nAny country may apply to be a part of the IMF. Post-IMF formation, in the early postwar period, rules for IMF membership were left relatively loose. Members needed to make periodic membership payments towards their quota, to refrain from currency restrictions unless granted IMF permission, to abide by the Code of Conduct in the IMF Articles of Agreement, and to provide national economic information. However, stricter rules were imposed on governments that applied to the IMF for funding.\nThe countries that joined the IMF between 1945 and 1971 agreed to keep their exchange rates secured at rates that could be adjusted only to correct a \"fundamental disequilibrium\" in the balance of payments, and only with the IMF's agreement.\nBenefits.\nMember countries of the IMF have access to information on the economic policies of all member countries, the opportunity to influence other members' economic policies, technical assistance in banking, fiscal affairs, and exchange matters, financial support in times of payment difficulties, and increased opportunities for trade and investment.\nVoting power.\nVoting power in the IMF is based on a quota system. Each member has a number of basic votes, equal to 5.502% of the total votes, plus one additional vote for each special drawing right (SDR) of 100,000 of a member country's quota. The SDR is the unit of account of the IMF and represents a potential claim to currency. When SDRs were created in 1969, they were each worth 0.888671 grams of gold, roughly the equivalent of one US dollar at the time. In 1973, following the termination of the Bretton Woods agreement in 1971, the IMF redefined the SDR as equivalent to the value of a specific selection of world currencies. The basic votes generate a slight bias in favour of small countries, but the additional votes determined by SDR outweigh this bias. Changes in the voting shares require approval by a super-majority of 85% of voting power.\nIn December 2015, the United States Congress adopted a legislation authorising the 2010 Quota and Governance Reforms. As a result,\nEffects of the quota system.\nThe IMF's quota system was created to raise funds for loans. Each IMF member country is assigned a quota, or contribution, that reflects the country's relative size in the global economy. Each member's quota also determines its relative voting power. Thus, financial contributions from member governments are linked to voting power in the organization.\nThis system follows the logic of a shareholder-controlled organization: wealthy countries have more say in the making and revision of rules. Since decision making at the IMF reflects each member's relative economic position in the world, wealthier countries that provide more money to the IMF have more influence than poorer members that contribute less; nonetheless, the IMF focuses on redistribution.\nInflexibility of voting power.\nQuotas are normally reviewed every five years and can be increased when deemed necessary by the board of governors. IMF voting shares are relatively inflexible: countries that grow economically have tended to become under-represented as their voting power lags behind. Currently, reforming the representation of developing countries within the IMF has been suggested. These countries' economies represent a large portion of the global economic system but this is not reflected in the IMF's decision-making process through the nature of the quota system. Joseph Stiglitz argues, \"There is a need to provide more effective voice and representation for developing countries, which now represent a much larger portion of world economic activity since 1944, when the IMF was created.\" In 2008, a number of quota reforms were passed including shifting 6% of quota shares to dynamic emerging markets and developing countries.\nOvercoming the borrower/creditor divide.\nThe IMF's membership is divided along income lines: certain countries provide financial resources while others use these resources. Both developed country \"creditors\" and developing country \"borrowers\" are members of the IMF. The developed countries provide the financial resources but rarely enter into IMF loan agreements; they are the creditors. Conversely, the developing countries use the lending services but contribute little to the pool of money available to lend because their quotas are smaller; they are the borrowers. Thus, tension is created around governance issues because these two groups, creditors and borrowers, have fundamentally different interests, and low-income \"borrowing\" members lack meaningful representation in IMF decision-making.\nThe criticism is that the system of voting power distribution through a quota system institutionalizes borrower subordination and creditor dominance. The resulting division of the IMF's membership into borrowers and non-borrowers has increased the controversy around conditionality because the borrowers are interested in increasing loan access while creditors want to maintain reassurance that the loans will be repaid.\nOperations.\nAccording to the IMF itself, it works to foster global growth and economic stability by providing policy advice and financing to its members. It also works with developing countries to help them achieve macroeconomic stability and reduce poverty. The rationale for this is that private international capital markets function imperfectly and many countries have limited access to financial markets. Such market imperfections, together with balance-of-payments financing, provide the justification for official financing, without which many countries could only correct large external payment imbalances through measures with adverse economic consequences. The IMF provides alternate sources of financing such as the Poverty Reduction and Growth Facility.\nUpon the founding of the IMF, its three primary functions were: \nThe IMF's role was fundamentally altered by the floating exchange rates after 1971. It shifted to examining the economic policies of countries with IMF loan agreements to determine whether a shortage of capital was due to economic fluctuations or economic policy. The IMF also researched what types of government policy would ensure economic recovery. A particular concern of the IMF was to prevent financial crises, such as those in Mexico in 1982, Brazil in 1987, the 1997 Asian financial crisis, and the 1998 Russian financial crisis, from spreading and threatening the entire global financial and currency system. The challenge was to promote and implement a policy that reduced the frequency of crises among emerging market countries, especially the middle-income countries which are vulnerable to massive capital outflows. Rather than maintaining a position of oversight of only exchange rates, their function became one of surveillance of the overall macroeconomic performance of member countries. Their role became a lot more active because the IMF now manages economic policy rather than just exchange rates.\nIn addition, the IMF negotiates conditions on lending and loans under their policy of conditionality, which was established in the 1950s. Low-income countries can borrow on concessional terms, which means there is a period of time with no interest rates, through the Extended Credit Facility (ECF), the Standby Credit Facility (SCF) and the Rapid Credit Facility (RCF). Non-concessional loans, which include interest rates, are provided mainly through the Stand-By Arrangements (SBA), the Flexible Credit Line (FCL), the Precautionary and Liquidity Line (PLL), and the Extended Fund Facility. The IMF provides emergency assistance via the Rapid Financing Instrument (RFI) to members facing urgent balance-of-payments needs.\nSurveillance of the global economy.\nThe IMF is mandated to oversee the international monetary and financial system and monitor the economic and financial policies of its member countries. Accurate estimations require a degree of participatory surveillance. Market sizes and economic facts are estimated using member-state data, shared and verifiable by the organization's other member-states. This transparency is intended to facilitate international co-operation and trade. Since the demise of the Bretton Woods system of fixed exchange rates in the early 1970s, surveillance has evolved largely by way of changes in procedures rather than through the adoption of new obligations.\nThe Fund typically analyses the appropriateness of each member country's economic and financial policies for achieving orderly economic growth, and assesses the consequences of these policies for other countries and for the global economy. For instance, The IMF played a significant role in individual countries, such as Armenia and Belarus, in providing financial support to achieve stabilization financing from 2009 to 2019. The maximum sustainable debt level of a polity, which is watched closely by the IMF, was defined in 2011 by IMF economists to be 120%. Indeed, it was at this number that the Greek government-debt crisis started in 2010.\nIn 1995, the International Monetary Fund began to work on data dissemination standards with the view of guiding IMF member countries to disseminate their economic and financial data to the public. The International Monetary and Financial Committee (IMFC) endorsed the guidelines for the dissemination standards and they were split into two tiers: The General Data Dissemination System (GDDS) and the Special Data Dissemination Standard (SDDS).\nThe executive board approved the SDDS and GDDS in 1996 and 1997, respectively, and subsequent amendments were published in a revised \"Guide to the General Data Dissemination System\". The system is aimed primarily at statisticians and aims to improve many aspects of statistical systems in a country. It is also part of the World Bank Millennium Development Goals (MDG) and Poverty Reduction Strategic Papers (PRSPs).\nThe primary objective of the GDDS is to encourage member countries to build a framework to improve data quality and statistical capacity building to evaluate statistical needs, set priorities in improving timeliness, transparency, reliability, and accessibility of financial and economic data. Some countries initially used the GDDS, but later upgraded to SDDS.\nSome entities that are not IMF members also contribute statistical data to the systems:\nA 2021 study found that the IMF's surveillance activities have \"a substantial impact on sovereign debt with much greater impacts in emerging than high-income economies\".\nConditionality of loans.\nIMF conditionality is a set of policies or conditions that the IMF requires in exchange for financial resources. The IMF does require collateral from countries for loans but also requires the government seeking assistance to correct its macroeconomic imbalances in the form of policy reform. If the conditions are not met, the funds are withheld. The concept of conditionality was introduced in a 1952 executive board decision and later incorporated into the Articles of Agreement.\nConditionality is associated with economic theory as well as an enforcement mechanism for repayment. Stemming primarily from the work of Jacques Polak, the theoretical underpinning of conditionality was the \"monetary approach to the balance of payments\".\nStructural adjustment.\nSome of the conditions for structural adjustment can include:\nThese conditions are known as the Washington Consensus.\nBenefits.\nThese loan conditions ensure that the borrowing country will be able to repay the IMF and that the country will not attempt to solve their balance-of-payment problems in a way that would negatively impact the international economy. The incentive problem of moral hazard\u2014when economic agents maximise their own utility to the detriment of others because they do not bear the full consequences of their actions\u2014is mitigated through conditions rather than providing collateral; countries in need of IMF loans do not generally possess internationally valuable collateral anyway.\nConditionality also reassures the IMF that the funds lent to them will be used for the purposes defined by the Articles of Agreement and provides safeguards that the country will be able to rectify its macroeconomic and structural imbalances. In the judgment of the IMF, the adoption by the member of certain corrective measures or policies will allow it to repay the IMF, thereby ensuring that the resources will be available to support other members.\nAs of 2004[ [update]], borrowing countries have had a good track record for repaying credit extended under the IMF's regular lending facilities with full interest over the duration of the loan. This indicates that IMF lending does not impose a burden on creditor countries, as lending countries receive market-rate interest on most of their quota subscription, plus any of their own-currency subscriptions that are loaned out by the IMF, plus all of the reserve assets that they provide the IMF.\nUse.\nIn 2008, the SAIS Review of International Affairs revealed that the average overall use of IMF credit per decade increased, in real terms, by 21% between the 1970s and 1980s, and increased again by just over 22% from the 1980s to the 1991\u20132005 period. Another study has suggested that since 1950 the continent of Africa alone has received $300\u00a0billion from the IMF, the World Bank, and affiliate institutions.\nA study by Bumba Mukherjee found that developing democratic countries benefit more from IMF programs than developing autocratic countries because policy-making, and the process of deciding where loaned money is used, is more transparent within a democracy. One study done by Randall Stone found that although earlier studies found little impact of IMF programs on balance of payments, more recent studies using more sophisticated methods and larger samples \"usually found IMF programs improved the balance of payments\".\nExceptional Access Framework \u2013 sovereign debt.\nThe Exceptional Access Framework was created in 2003 when John B. Taylor was Under Secretary of the US Treasury for International Affairs. The new Framework became fully operational in February 2003 and it was applied in the subsequent decisions on Argentina and Brazil. Its purpose was to place some sensible rules and limits on the way the IMF makes loans to support governments with debt problem\u2014especially in emerging markets\u2014and thereby move away from the bailout mentality of the 1990s. Such a reform was essential for ending the crisis atmosphere that then existed in emerging markets. The reform was closely related to and put in place nearly simultaneously with the actions of several emerging market countries to place collective action clauses in their bond contracts.\nIn 2010, the framework was abandoned so the IMF could make loans to Greece in an unsustainable and political situation.\nThe topic of sovereign debt restructuring was taken up by IMF staff in April 2013 for the first time since 2005, in a report entitled \"Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework\". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize, and Jamaica. An explanatory interview with deputy director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of \"The Wall Street Journal\".\nThe staff was directed to formulate an updated policy, which was accomplished on 22 May 2014 with a report entitled \"The Fund's Lending Framework and Sovereign Debt: Preliminary Considerations\", and taken up by the executive board on 13 June. The staff proposed that \"in circumstances where a (Sovereign) member has lost market access and debt is considered sustainable ... the IMF would be able to provide Exceptional Access on the basis of a debt operation that involves an extension of maturities\", which was labeled a \"reprofiling operation\". These reprofiling operations would \"generally be less costly to the debtor and creditors\u2014and thus to the system overall\u2014relative to either an upfront debt reduction operation or a bail-out that is followed by debt reduction ... (and) would be envisaged only when both (a) a member has lost market access and (b) debt is assessed to be sustainable, but not with high probability ... Creditors will only agree if they understand that such an amendment is necessary to avoid a worse outcome: namely, a default and/or an operation involving debt reduction ... Collective action clauses, which now exist in most\u2014but not all\u2014bonds would be relied upon to address collective action problems.\"\nPersonnel.\nBoard of Governors.\nThe board of governors consists of one governor and one alternate governor for each member country. Each member country appoints its two governors. The Board normally meets once a year and is responsible for electing or appointing an executive director to the executive board. While the board of governors is officially responsible for approving quota increases, special drawing right allocations, the admittance of new members, compulsory withdrawal of members, and amendments to the Articles of Agreement and By-Laws, in practice it has delegated most of its powers to the IMF's executive board.\nThe board of governors is advised by the International Monetary and Financial Committee and the Development Committee. The International Monetary and Financial Committee has 24 members and monitors developments in global liquidity and the transfer of resources to developing countries. The Development Committee has 25 members and advises on critical development issues and on financial resources required to promote economic development in developing countries.\nThe board of governors reports directly to the managing director of the IMF, Kristalina Georgieva.\nExecutive Board.\n25 Executive Directors make up the executive board. The executive directors represent all 191 member countries in a geographically based roster. Countries with large economies have their own executive director, but most countries are grouped in constituencies representing four or more countries.\nFollowing the \"2008 Amendment on Voice and Participation\" which came into effect in March 2011, seven countries each appoint an executive director: the United States, Japan, China, Germany, France, the United Kingdom, and Saudi Arabia. The remaining 18 Directors represent constituencies consisting of 2 to 23 countries. This Board usually meets several times each week. The board membership and constituency is scheduled for periodic review every eight years.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;List of Executive Directors of the IMF, as of August 2025\nManaging Director.\nThe IMF is led by a managing director, who is head of the staff and serves as chairman of the executive board. The managing director is the most powerful position at the IMF. Historically, the IMF's managing director has been a European citizen and the president of the World Bank has been an American citizen. However, this standard is increasingly being questioned and competition for these two posts may soon open up to include other qualified candidates from any part of the world. In August 2019, the International Monetary Fund has removed the age limit which is 65 or over for its managing director position.\nIn 2011, the world's largest developing countries, the BRICS states, issued a statement declaring that the tradition of appointing a European as managing director undermined the legitimacy of the IMF and called for the appointment to be merit-based.\nList of Managing Directors.\nFormer managing director Dominique Strauss-Kahn was arrested in connection with charges of sexually assaulting a New York hotel room attendant and resigned on 18 May. The charges were later dropped. On 28 June 2011 Christine Lagarde was confirmed as managing director of the IMF for a five-year term starting on 5 July 2011. She was re-elected by consensus for a second five-year term, starting 5 July 2016, being the only candidate nominated for the post of managing director.\nFirst Deputy Managing Director.\nThe managing director is assisted by a First Deputy managing director (FDMD) who, by convention, has always been a citizen of the United States. Together, the managing director and their First Deputy lead the senior management of the IMF. Like the managing director, the First Deputy traditionally serves a five-year term.\nChief Economist.\nThe chief economist leads the research division of the IMF and is a \"senior official\" of the IMF.\nIMF staff.\nIMF staff have considerable autonomy and are known to shape IMF policy. According to Jeffrey Chwieroth, \"It is the staff members who conduct the bulk of the IMF's tasks; they formulate policy proposals for consideration by member states, exercise surveillance, carry out loan negotiations and design the programs, and collect and systematize detailed information.\" Most IMF staff are economists. According to a 1968 study, nearly 60% of staff were from English-speaking developed countries. By 2004, between 40 and 50% of staff were from English-speaking developed countries.\nA 1996 study found that 90% of new staff with a PhD obtained them from universities in the United States or Canada. A 1999 study found that none of the new staff with a PhD obtained their PhD in the Global South.\nCriticisms and controversy.\nGeneral.\nOverseas Development Institute (ODI) research undertaken in 1980 included criticisms of the IMF which support the analysis that it is a pillar of what activist Titus Alexander calls global apartheid.\nODI conclusions were that the IMF's very nature of promoting market-oriented approaches attracted unavoidable criticism. On the other hand, the IMF could serve as a scapegoat while allowing governments to blame international bankers. The ODI conceded that the IMF was insensitive to political aspirations of LDCs while its policy conditions were inflexible.\nArgentina, which had been considered by the IMF to be a model country in its compliance to policy proposals by the Bretton Woods institutions, experienced a catastrophic economic crisis in 2001, which some believe to have been caused by IMF-induced budget restrictions\u2014which undercut the government's ability to sustain national infrastructure even in crucial areas such as health, education, and security\u2014and privatisation of strategically vital national resources. Others attribute the crisis to Argentina's misdesigned fiscal federalism, which caused subnational spending to increase rapidly. The crisis added to widespread hatred of this institution in Argentina and other South American countries, with many blaming the IMF for the region's economic problems. The post-2000s trend toward moderate left-wing governments in the region and a growing concern with the development of a regional economic policy largely independent of big business pressures has been ascribed to this crisis.\nIn 2001, the Independent Evaluation Office, an autonomous body, was established to conduct independent evaluations of policies and activities of the International Monetary Fund.\nIn 2006, a senior ActionAid policy analyst Akanksha Marphatia stated that IMF policies in Africa undermine any possibility of meeting the Millennium Development Goals (MDGs) due to imposed restrictions that prevent spending on important sectors, such as education and health.\nIn an interview (2008-05-19), the former Romanian Prime Minister C\u0103lin Popescu-T\u0103riceanu claimed that \"Since 2005, IMF is constantly making mistakes when it appreciates the country's economic performances\". Former Tanzanian President Julius Nyerere, who claimed that debt-ridden African states were ceding sovereignty to the IMF and the World Bank, famously asked, \"Who elected the IMF to be the ministry of finance for every country in the world?\"\nFormer chief economist of IMF and former Reserve Bank of India (RBI) Governor Raghuram Rajan who predicted the 2008 financial crisis criticised the IMF for remaining a sideline player to the developed world. He criticised the IMF for praising the monetary policies of the US, which he believed were wreaking havoc in emerging markets. He had been critical of \"ultra-loose money policies\" of some unnamed countries.\nCountries such as Zambia have not received proper aid with long-lasting effects, leading to concern from economists. Since 2005, Zambia (as well as 29 other African countries) did receive debt write-offs, which helped with the country's medical and education funds. However, Zambia returned to a debt of over half its GDP in less than a decade. American economist William Easterly, sceptical of the IMF's methods, had initially warned that \"debt relief would simply encourage more reckless borrowing by crooked governments unless it was accompanied by reforms to speed up economic growth and improve governance\", according to \"The Economist\".\nFeminist and postcolonial scholars have critiqued the IMF\u2019s structural adjustment and austerity programs for reinforcing neocolonial economic dependencies in postcolonial states. These policy conditions often reduce public spending on health, education, and social services, disproportionately affecting women and marginalized communities, while prioritizing debt repayment and market liberalization over social welfare and national economic sovereignty.\nConditionality.\nThe IMF has been criticised for being \"out of touch\" with local economic conditions, cultures, and environments in the countries they are requiring policy reform. The economic advice the IMF gives might not always take into consideration the difference between what spending means on paper and how it is felt by citizens. Countries charge that with excessive conditionality, they do not \"own\" the programmes and the links are broken between a recipient country's people, its government, and the goals being pursued by the IMF.\nJeffrey Sachs argues that the IMF's \"usual prescription is 'budgetary belt tightening to countries who are much too poor to own belts'\". Sachs wrote that the IMF's role as a generalist institution specialising in macroeconomic issues needs reform. Conditionality has also been criticised because a country can pledge collateral of \"acceptable assets\" to obtain waivers\u2014if one assumes that all countries are able to provide \"acceptable collateral\".\nOne view is that conditionality undermines domestic political institutions. The recipient governments are sacrificing policy autonomy in exchange for funds, which can lead to public resentment of the local leadership for accepting and enforcing the IMF conditions. Political instability can result from more leadership turnover as political leaders are replaced in electoral backlashes. IMF conditions are often criticised for reducing government services, thus increasing unemployment.\nAnother criticism is that IMF policies are only designed to address poor governance, excessive government spending, excessive government intervention in markets, and too much state ownership. This assumes that this narrow range of issues represents the only possible problems; everything is standardised and differing contexts are ignored. A country may also be compelled to accept conditions it would not normally accept had they not been in a financial crisis in need of assistance.\nOn top of that, regardless of what methodologies and data sets used, it comes to same the conclusion of exacerbating income inequality. With Gini coefficient, it became clear that countries with IMF policies face increased income inequality.\nIt is claimed that conditionalities hinder social stability and hence inhibit the stated goals of the IMF, while Structural Adjustment Programmes lead to an increase in poverty in recipient countries. The IMF sometimes advocates \"austerity programmes\", cutting public spending and increasing taxes even when the economy is weak, to bring budgets closer to a balance, thus reducing budget deficits. Countries are often advised to lower their corporate tax rate. In \"Globalization and Its Discontents\", Joseph E. Stiglitz, former chief economist and senior vice-president at the World Bank, criticises these policies. He argues that by converting to a more monetarist approach, the purpose of the fund is no longer valid, as it was designed to provide funds for countries to carry out Keynesian reflations, and that the IMF \"was not participating in a conspiracy, but it was reflecting the interests and ideology of the Western financial community.\"\nStiglitz concludes, \"Modern high-tech warfare is designed to remove physical contact: dropping bombs from 50,000 feet ensures that one does not 'feel' what one does. Modern economic management is similar: from one's luxury hotel, one can callously impose policies about which one would think twice if one knew the people whose lives one was destroying.\"\nThe researchers Eric Toussaint and Damien Millet argue that the IMF's policies amount to a new form of colonisation that does not need a military presence: \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Following the exigencies of the governments of the richest companies, the IMF, permitted countries in crisis to borrow in order to avoid default on their repayments. Caught in the debt's downward spiral, developing countries soon had no other recourse than to take on new debt in order to repay the old debt. Before providing them with new loans, at higher interest rates, future leaders asked the IMF, to intervene with the guarantee of ulterior reimbursement, asking for a signed agreement with the said countries. The IMF thus agreed to restart the flow of the 'finance pump' on condition that the concerned countries first use this money to reimburse banks and other private lenders, while restructuring their economy at the IMF's discretion: these were the famous conditionalities, detailed in the Structural Adjustment Programmes. The IMF and its ultra-liberal experts took control of the borrowing countries' economic policies. A new form of colonisation was thus instituted. It was not even necessary to establish an administrative or military presence; the debt alone maintained this new form of submission.\nInternational politics play an important role in IMF decision making. The clout of member states is roughly proportional to its contribution to IMF finances. The United States has the greatest number of votes and therefore wields the most influence. Domestic politics often come into play, with politicians in developing countries using conditionality to gain leverage over the opposition to influence policy.\nAcademic Jeremy Garlick cites IMF loans to South Korea during the 1997 Asian financial crisis as widely perceived by the South Korean public as a debt-trap.89 Garlick writes that the public was generally bitter about submitting to the conditions imposed by the IMF, which required South Korea to radically restructure its economy and consult with the IMF before making economic decisions until the debt was repaid.89\nIn 2016, the IMF's research department published a report titled \"Neoliberalism: Oversold?\" which, while praising some aspects of the \"neoliberal agenda\", claims that the organisation has been \"overselling\" fiscal austerity policies and financial deregulation, which they claim has exacerbated both financial crises and economic inequality around the world.\nIn 2020 and 2021, Oxfam criticized the IMF for forcing tough austerity measures on many low income countries during the COVID-19 pandemic, despite forcing cuts to healthcare spending, would hamper the recipient's response to the pandemic.\nSupport of dictatorships.\nThe role of the Bretton Woods institutions has been controversial since the late Cold War, because of claims that the IMF policy makers supported military dictatorships friendly to American and European corporations, but also other anti-communist and communist regimes, such as the Socialist Republic of Romania. An example of IMF's support for a dictatorship was its ongoing support for Mobutu's rule in Zaire, although its own envoy Erwin Blumenthal provided a sobering report about the entrenched corruption and embezzlement and the inability of the country to pay back any loans. In 2021, the IMF approved a US$1 billion loan to the autocratic Uganda despite protests from Ugandans in Washington, London and South Africa. Critics also claim that the IMF is generally apathetic or hostile to democracy, human rights, and labour rights. The controversy has helped spark the anti-globalization movement.\nArguments in favour of the IMF supporting dictatorships is the claim that economic stability is a precursor to democracy. A 2017 study found no evidence of IMF lending programs undermining democracy in borrowing countries, it found \"evidence for modest but definitively positive conditional differences in the democracy scores of participating and non-participating countries\".\nA 2020 study found the reverse causality with democracy as a precursor to economic stability. Critics highlight various examples in which democratised countries fell after receiving IMF loans.\nParty-based autocracies and democracies can face similar incentives when considering agreements with IMF, in contrast to personalist and military regimes.\nImpact on access to food.\nA number of civil society organisations have criticised the IMF's policies for their impact on access to food, particularly in developing countries. In October 2008, former United States president Bill Clinton delivered a speech to the United Nations on World Food Day, criticising the World Bank and IMF for their policies on food and agriculture:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We need the World Bank, the IMF, all the big foundations, and all the governments to admit that, for 30 years, we all blew it, including me when I was president. We were wrong to believe that food was like some other product in international trade, and we all have to go back to a more responsible and sustainable form of agriculture. \u2014\u200a\nThe FPIF remarked that there is a recurring pattern: \"the destabilization of peasant producers by a one-two punch of IMF-World Bank structural adjustment programs that gutted government investment in the countryside followed by the massive influx of subsidized U.S. and European Union agricultural imports after the WTO's Agreement on Agriculture pried open markets.\"\nImpact on public health.\nThe structural adjustments programs (SAPs) demanded by IMF conditionality contribute to the weakening of public health services in a number of ways. Cuts in funding to public health services prompted by IMF SAPs contribute to poorer working conditions in the sector, causing repercussions for health service quality and availability. This has been shown to negatively impact vaccination rates, increase exposure to conditions with risk-factors like disability and death, and higher prevalence of tuberculosis.\nA 2009 study concluded that the strict conditions resulted in thousands of deaths in Eastern Europe by tuberculosis as public health care had to be weakened. In the 21 countries to which the IMF had given loans, tuberculosis deaths rose by 16.6%. A 2017 systematic review on studies conducted on the impact that structural adjustment programs have on child and maternal health found that these programs have a detrimental effect on maternal and child health among other adverse effects.\nStructural adjustment programs are considered a form of neo-colonialism by some academics due to their intensification of healthcare access inequality, and their promotion of external intervention and aid to meet health needs in developing countries.\nReform.\nFunction and policies.\nThe IMF is only one of many international organisations, and it is a generalist institution that deals only with macroeconomic issues; its core areas of concern in developing countries are very narrow. One proposed reform is a movement towards close partnership with other specialist agencies such as UNICEF, the Food and Agriculture Organization (FAO), and the United Nations Development Programme (UNDP).\nJeffrey Sachs argues in \"The End of Poverty\" that the IMF and the World Bank have \"the brightest economists and the lead in advising poor countries on how to break out of poverty, but the problem is development economics\". Development economics needs the reform, not the IMF. He also notes that IMF loan conditions should be paired with other reforms\u2014e.g., trade reform in developed nations, debt cancellation, and increased financial assistance for investments in basic infrastructure. IMF loan conditions cannot stand alone and produce change; they need to be partnered with other reforms or other conditions as applicable.\nU.S. influence and voting reform.\nThe scholarly consensus is that IMF decision-making is not simply technocratic, but also guided by political and economic concerns. The United States is the IMF's most powerful member, and its influence reaches even into decision-making concerning individual loan agreements. The U.S. has historically been openly opposed to losing what Treasury Secretary Jacob Lew described in 2015 as its \"leadership role\" at the IMF, and the U.S.' \"ability to shape international norms and practices\".\nEmerging markets were not well-represented for most of the IMF's history: Despite being the most populous country, China's vote share was the sixth largest; Brazil's vote share was smaller than Belgium's. Reforms to give more powers to emerging economies were agreed by the G20 in 2010. The reforms could not pass, however, until they were ratified by the United States Congress, since 85% of the Fund's voting power was required for the reforms to take effect, and the Americans held more than 16% of voting power at the time. After repeated criticism, the U.S. finally ratified the voting reforms at the end of 2015. The OECD countries maintained their overwhelming majority of voting share, and the U.S. in particular retained its share at over 16%.\nThe criticism of the American- and European-dominated IMF has led to what some consider \"disenfranchising the world\" from the governance of the IMF. Ra\u00fal Prebisch, the founding secretary-general of the UN Conference on Trade and Development (UNCTAD), wrote that one of \"the conspicuous deficiencies of the general economic theory, from the point of view of the periphery, is its false sense of universality\".\nScandals.\nManaging Director Lagarde (2011\u20132019) was convicted of giving preferential treatment to businessman-turned-politician Bernard Tapie as he pursued a legal challenge against the French government. At the time, Lagarde was the French economic minister. Within hours of her conviction, in which she escaped any punishment, the fund's 24-member executive board put to rest any speculation that she might have to resign, praising her \"outstanding leadership\" and the \"wide respect\" she commands around the world.\nFormer IMF Managing Director Rodrigo Rato was arrested in 2015 for alleged fraud, embezzlement and money laundering. In 2017, the Audiencia Nacional found Rato guilty of embezzlement and sentenced him to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;4+1\u20442 years' imprisonment. In 2018, the sentence was confirmed by the Supreme Court of Spain.\nAssessment.\nImpact.\nAccording to a 2002 study by Randall W. Stone, the academic literature on the IMF shows \"no consensus on the long-term effects of IMF programs on growth\".\nSome research has found that IMF loans can reduce the chance of a future banking crisis, while other studies have found that they can increase the risk of political crises. IMF programs can reduce the effects of a currency crisis.\nSome research has found that IMF programs are less effective in countries which possess a developed-country patron (be it by foreign aid, membership of postcolonial institutions or UN voting patterns), seemingly due to this patron allowing countries to flaunt IMF program rules as these rules are not consistently enforced. Some research has found that IMF loans reduce economic growth due to creating an economic moral hazard, reducing public investment, reducing incentives to create a robust domestic policies and reducing private investor confidence. Other research has indicated that IMF loans can have a positive impact on economic growth and that their effects are highly nuanced.\nIMF and globalization.\nGlobalization encompasses three institutions: global financial markets and transnational companies, national governments linked to each other in economic and military alliances led by the United States, and rising \"global governments\" such as World Trade Organization (WTO), IMF, and World Bank. Charles Derber argues in his book \"People Before Profit,\" \"These interacting institutions create a new global power system where sovereignty is globalized, taking power and constitutional authority away from nations and giving it to global markets and international bodies\". Titus Alexander argues that this system institutionalises global inequality between western countries and the Majority World in a form of global apartheid, in which the IMF is a key pillar.\nThe establishment of globalised economic institutions has been both a symptom of and a stimulus for globalisation. The development of the World Bank, the IMF, regional development banks such as the European Bank for Reconstruction and Development (EBRD), and multilateral trade institutions such as the WTO signals a move away from the dominance of the state as the primary actor analysed in international affairs. Globalization has thus been transformative in terms of limiting of state sovereignty over the economy.\nInternational central bank digital currency.\nIn April 2023, the IMF launched their international central bank digital currency through their Digital Currency Monetary Authority, it will be called the Universal Monetary Unit, or Units for shorthand. The ANSI character will be \u00dc and will be used to facilitate international banking and international trade between countries and currencies. It will help facilitate SWIFT transactions on cross border transactions at wholesale FX rates instantaneously with real-time settlements. In June, it announced it was working on a platform for central bank digital currencies (CBDCs) that would enable transactions between nations. IMF Managing Director Kristalina Georgieva said that if central banks did not agree on a common platform, cryptocurrency would fill the resulting vacuum.\nAlternatives.\nIn March 2011, the Ministers of Economy and Finance of the African Union proposed to establish an African Monetary Fund.\nAt the 6th BRICS summit in July 2014 the BRICS nations (Brazil, Russia, India, China, and South Africa) announced the BRICS Contingent Reserve Arrangement with an initial size of US$100\u00a0billion, a framework to provide liquidity through currency swaps in response to actual or potential short-term balance-of-payments pressures.\nIn 2014, the China-led Asian Infrastructure Investment Bank was established.\nIn the media.\n\"Life and Debt\", a documentary film, deals with the IMF's policies' influence on Jamaica and its economy from a critical point of view. \"Debtocracy\", a 2011 independent Greek documentary film, also criticises the IMF. Portuguese musician Jos\u00e9 M\u00e1rio Branco's 1982 album \"FMI\" is inspired by the IMF's intervention in Portugal through monitored stabilisation programs in 1977\u201378. In the 2015 film \"Our Brand Is Crisis\", the IMF is mentioned as a point of political contention, where the Bolivian population fears its electoral interference.\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15252", "revid": "2259739", "url": "https://en.wikipedia.org/wiki?curid=15252", "title": "Islands of the Clyde", "text": "Scottish island group\nThe Islands of the Firth of Clyde are the fifth largest of the major Scottish island groups after the Inner and Outer Hebrides, Orkney and Shetland. They are situated in the Firth of Clyde between Argyll and Bute in the west and Inverclyde, North Ayrshire and South Ayrshire in the east. There are about forty islands and skerries. Only four are inhabited, and only nine are larger than . The largest and most populous are Arran and Bute. They are served by dedicated ferry routes, as are Great Cumbrae and Holy Island. Unlike the isles in the four larger Scottish archipelagos, none of the isles in this group are connected to one another or to the mainland by bridges.\nThe geology and geomorphology of the area is complex, and the islands and the surrounding sea lochs each have distinctive features. The influence of the Atlantic Ocean and the North Atlantic Drift create a mild, damp oceanic climate. There is a diversity of wildlife, including three species of rare endemic trees.\nThe larger islands have been continuously inhabited since Neolithic times. The cultures of their inhabitants were influenced by the emergence of the kingdom of D\u00e1l Riata, beginning in 500 AD. The islands were then politically absorbed into the emerging kingdom of Alba, led by Kenneth MacAlpin. During the early Middle Ages, the islands experienced Viking incursions. In the 13th century, they became part of the Kingdom of Scotland.\nGeology and geography.\nThe Highland Boundary Fault runs past Bute and through the northern part of Arran. Therefore, from a geological perspective, some of the islands are in the Highlands and some in the Central Lowlands. As a result of Arran's geological similarity to Scotland, it is sometimes referred to as \"Scotland in miniature\" and the island is a popular destination for geologists. They come to Arran to study its intrusive igneous landforms, such as sills and dykes, as well as its sedimentary and metasedimentary rocks, which range widely in age. Visiting in 1787, the geologist James Hutton found his first example of an unconformity there. The spot where he discovered it is one of the most famous places in the history of the study of geology. The group of weakly metamorphosed rocks that form the Highland Border Complex lie discontinuously along the Highland Boundary Fault. One of the most prominent exposures is along Loch Fad on Bute. Ailsa Craig, which lies some south of Arran, has been quarried for a rare type of micro-granite containing riebeckite, known as \"Ailsite\". It is used by Kays of Scotland to make curling stones. (As of 2004, 60 to 70% of all curling stones in use globally were made from granite quarried on the island.)\nLike the rest of Scotland, the Firth of Clyde was covered by ice sheets during the Pleistocene ice ages, and the landscape has been much affected by glaciation. Back then, Arran's highest peaks may have been nunataks. Sea-level changes and the isostatic rise of land after the last retreat of the ice created clifflines behind raised beaches, which are a prominent feature of the entire coastline. The action of these forces has made charting the post glacial coastlines a complex task.\nThe various soil types on the islands reflect their diverse geology. Bute has the most productive land, and it has a pattern of deposits that is typical of the southwest of Scotland. In the eroded valleys, there is a mixture of boulder clay and other glacial deposits. Elsewhere, especially to the south and west, there are raised beach- and marine deposits, which in some places, such as Stravanan, result in a machair landscape inland from the sandy bays.\nThe Firth of Clyde, in which these islands lie, is north of the Irish Sea and has numerous branching inlets. Some of those inlets, including Loch Goil, Loch Long, Gare Loch, Loch Fyne, and the estuary of the River Clyde, have their own substantial features. In places, the effect of glaciation on the seabed is pronounced. For example, the Firth is deep between Arran and Bute, even though they are only apart. The islands all stand exposed to wind and tide. Various lighthouses, such as those on Ailsa Craig, Pladda, and Davaar, act as an aid to navigation.\nClimate.\nThe Firth of Clyde lies between 55 and 56 degrees north latitude. This is the same latitude as Labrador in Canada and north of the Aleutian Islands. However, the influence of the North Atlantic Drift\u2014the northern extension of the Gulf Stream\u2014moderates the winter weather. As a result, the area enjoys a mild, damp oceanic climate. Temperatures are generally cool, averaging about in January and in July at sea level. Snow seldom lies at sea level, and frosts are generally less frequent than they are on the mainland. In common with most islands off the west coast of Scotland, the average annual rainfall is generally high: between on Bute, in the Cumbraes, and in the south of Arran, and in the north of Arran. The Arran mountains are even wetter: Their summits receive over of rain annually. May, June and July are the sunniest months: on average, there is a total of 200 hours of bright sunshine during that 3-month period each year. Southern Bute benefits from a particularly large number of sunny days.\nHistory.\nPrehistory.\nMesolithic humans arrived in the area of the Firth of Clyde during the 4th millennium BC, probably from Ireland. This initial arrival was followed by another wave of Neolithic peoples using the same route. In fact, there is some evidence that the Firth of Clyde was a significant route through which mainland Scotland was colonised during the Neolithic period. The inhabitants of Argyll, the Clyde estuary, and elsewhere in western Scotland at that time developed a distinctive style of megalithic structure that is known today as the Clyde cairns. About 100 of these structures have been found. They were used for interment of the dead. They are rectangular or trapezoidal, with a small enclosing chamber into which the person's body was placed. They are faced with large slabs of stone set on end (sometimes subdivided into smaller compartments). They also feature a forecourt area, which may have been used for displays or rituals associated with interment. They are mostly found in Arran, Bute, and Kintyre. It is thought likely that the Clyde cairns were the earliest forms of Neolithic monument constructed by incoming settlers. However, only a few of the cairns have been radiocarbon dated. A cairn at Monamore on Arran has been dated to 3160 BC, although other evidence suggests that it was almost certainly built earlier than that, possibly around 4000 BC. The area also features numerous standing stones dating from prehistoric times, including six stone circles on Machrie Moor in Arran, and other examples on Great Cumbrae and Bute.\nLater, Bronze Age settlers also constructed megaliths at various sites. Many of them date from the 2nd millennium BC. However, instead of chambered cairns, these peoples constructed burial cists, which can be found, for example, on Inchmarnock. Evidence of settlement during this period, especially the early part of it, is scant. However, one notable artifact has been found on Bute that dates from around 2000 BC. Known today as the \u201cQueen of the Inch necklace,\u201d it is an article of jewellery made of lignite (commonly called \u201cjet\u201d).\nDuring the early Iron Age, the Brythonic culture held sway. There is no evidence that the Roman occupation of southern Scotland extended into these islands.\nEarly Scots rule.\nBeginning in the 2nd century AD, Irish influence was at work in the region, and by the 6th century, Gaels had established the kingdom of D\u00e1l Riata there. Unlike earlier inhabitants, such as the P-Celtic speaking Brythons, these Gaels spoke a form of Gaelic (a modern version of which is still spoken today in the Hebrides). During this period, through the efforts of Saint Ninian and others, Christianity slowly supplanted Druidism. The kingdom of D\u00e1l Riata flourished from the rule of Fergus M\u00f3r in the late 5th century until the Viking incursions beginning in the late 8th century. Islands close to the shores of modern Ayrshire presumably remained part of the Kingdom of Strathclyde during this period, whilst the main islands became part of the emerging Kingdom of Alba founded by Kenneth MacAlpin (Cin\u00e1ed mac Ailp\u00edn).\nViking influence.\nBeginning in the 9th century and into the 13th century, the Islands of the Clyde constituted a border zone between the Norse \"Su\u00f0reyjar\" and Scotland, and many of them were under Norse hegemony.\nBeginning in the last half of the 12th century, and then into the early 1200s, the islands may well have served as the power base of Somhairle mac Giolla Brighde and his descendants. During this time, the islands seem to have come under the sway of the Steward of Scotland\u2019s authority and to have been taken over by the expanding Stewart lordship.\nThis western extension of Scottish authority appears to have been one of the factors motivating the Norwegian invasion of the region in 1230, during which the invaders seized Rothesay Castle.\nIn 1263, Norwegian troops commanded by Haakon Haakonarson repeated the feat, but the ensuing Battle of Largs between Scots and Norwegian forces, which took place on the shores of the Firth of Clyde, was inconclusive as a military contest.\nThis battle marked an ultimately fatal weakening of Norwegian power in Scotland. Haakon retreated to Orkney, where he died in December 1263, consoled on his death bed by recitations of the old sagas. Following his death, under the 1266 Treaty of Perth, all rights that the Norwegian Crown \"had of old therein\" in relation to the islands were yielded to the Kingdom of Scotland.\nModern Scotland.\nPolitically, from the conclusion of the Treaty of Perth in 1266 to the present day, all of the islands of the Clyde have been part of Scotland.\nEcclesiastically, beginning in the early medieval period all of these isles were part of the Diocese of Sodor and Man, based at Peel, on the Isle of Man. After 1387, the seat of the Bishopric of the Isles was relocated to the north, first to Snizort on Skye and then to Iona. This arrangement continued until the Scottish Reformation in the 16th century, when Scotland broke with the Catholic Church.\nThe mid-1700s marked the beginning of a century of significant change. New forms of transport, industry, and agriculture brought an end to ways of life that had endured for centuries. The Battle of Culloden in 1746 foreshadowed the end of the clan system. These changes improved living standards for some, but came at a cost for others.\nIn the late 18th and early 19th centuries, Alexander, the 10th Duke of Hamilton (1767\u20131852), and others implemented a controversial agricultural-reform programme called the Highland Clearances that had a devastating effect on many of Arran's inhabitants. Whole villages were emptied, and the Gaelic culture of the island was dealt a terminal blow. (A memorial to the tenant farmers evicted from the island by this programme was later erected on the shore at Lamlash, funded by a Canadian descendant of some of those evicted.)\nFrom the 1850s to the late 20th century, cargo ships known as \u201cClyde Puffers\u201d (made famous by an early-20th-century story collection called the \"Vital Spark\"), were the workhorses of the islands, carrying a great deal of produce and a great variety of products to and from the islands. In May 1889, the Caledonian Steam Packet Company (CSP) was founded and began operating steamer services to and from Gourock for the Caledonian Railway. The company soon expanded by taking over rival steamer operators. David MacBrayne operated the Glasgow-to-Ardrishaig steamer service, as part of the so-called \"Royal Route\" to Oban. During the 20th century, many of the islands were developed as tourist resorts along the lines of mainland resorts such as Largs and Troon, but catering for Glaswegians who preferred to holiday \"Doon the Watter\".\nIn 1973, CSP and MacBraynes combined their Clyde and West Highland operations under the new name of Caledonian MacBrayne. A government-owned corporation, they serve Great Cumbrae, Arran, and Bute, and also run mainland-to-mainland ferries across the firth. Private companies operate services from Arran to Holy Isle, and from McInroy's Point (Gourock) to Hunter's Quay on the Cowal peninsula.\nPolitically, from 1890 to 1975, most of the islands comprised the traditional County of Bute, and its inhabitants were represented by the county council. Since the 1975 reorganization, however, the islands have been split more or less equally between two modern council authorities: Argyll and Bute, and North Ayrshire. Only Ailsa Craig and Lady Isle in South Ayrshire are not part of either of these two council areas.\nIslands.\nBelow is a table listing the nine islands of the Firth of Clyde that have an area greater than 40\u00a0hectares (approximately 100\u00a0acres), showing their population and listing the smaller uninhabited islets adjacent to them (including tidal islets separated only when the tide is higher, and skerries exposed only when the tide is lower).\nAs of 2001, six of the islands were inhabited, but that included one with only two residents (Davaar), and one with only one resident (Sanda). At the 2011 census, there was no one usually resident on either of these islands.\nOutlying islands.\nThe islets that lie remote from the larger islands are described separately below.\nThere are two islets in Gare Loch: Green Island and Perch Rock. Gare Loch is small, but it hosts the Faslane Naval Base, where the UK's Trident nuclear submarines are located. At its southern end, the loch opens into the Firth of Clyde via the Rhu narrows.\nThere are also several islets in the Kilbrannan Sound, which lies between Arran and the Kintyre peninsula. They are: An Struthlag, Cour Island, Eilean Carrach (Carradale), Eilean Carrach (Skipness), Eilean Grianain, Eilean Sunadale, Gull Isle, Island Ross and Thorn Isle.\nThere are also several islets and skerries in Loch Fyne, which extends inland from the Sound of Bute, and is the longest of Scotland's sea lochs. They are: Duncuan Island, Eilean Ardgaddan, Eilean a' Bhuic, Eilean Aoghainn, Eilean a' Chomhraig, Eilean an D\u00fanain, Eilean Buidhe (Ardmarnock), Eilean Buidhe (Portavadie), Eilean Fraoch, Eilean Math-ghamhna, Eilean M\u00f3r, Glas Eilean, Heather Island, Inverneil Island, Kilbride Island, and Liath Eilean.\nThere are several islets surrounding Horse Isle in North Ayrshire: Broad Rock, East Islet, Halftide Rock, High Rock and North Islet.\nLady Isle lies off the South Ayrshire coast near Troon. At one time it housed \"ane old chapell with an excellent spring of water\". However, in June 1821, someone set fire to the \"turf and pasture\". Once the pasture had burned away, gales blew much of the island's soil into the sea. This permanently destroyed the island's ability to support grazing.\nThere are no islands in Loch Goil or Loch Long, which are fjord-like arms in the northern part of the firth.\nNon-island areas with \u201cisland\u201d in their name.\nHere is a list of places along that shores of the Firth of Clyde that are not islands, but have names that misleadingly suggest they are islands (\"eilean\" being Gaelic for \"island\"): Eilean na Beithe, Portavadie; Eilean Beag, Cove; Eilean Dubh, Dalchenna, Loch Fyne; Eilean nan Gabhar, Melldalloch, Kyles of Bute; Barmore Island, just north of Tarbert, Kintyre; Eilean Aoidh, south of Portavadie; Eilean Leathan, Kilbrannan Sound just south of Torrisdale Bay; Island Muller, Kilbrannan Sound north of Campbeltown.\nNatural history.\nAround the Firth of Clyde, there are populations of red deer, red squirrel, badger, otter, adder, and common lizard. In the Firth itself, there are harbour porpoises, basking sharks and various species of dolphin. Davaar is home to a population of wild goats.\nOver 200 bird species have been recorded as sighted in the area, including the black guillemot, the eider, the peregrine falcon, and the golden eagle. In 1981, there were 28 ptarmigans sighted on Arran, but in 2009 it was reported that extensive surveys had been unable to find any recorded ptarmigans sightings. Similarly, the red-billed chough no longer breeds on the island.\nArran has three species of the rare endemic trees known as Arran Whitebeams: the Scottish or Arran whitebeam; the cut-leaved whitebeam; and the Catacol whitebeam. All of them are found only in Gleann Diomhan, and they are amongst the most endangered tree species in the world. (Gleann Diomhan was formerly part of a designated national nature reserve\u2014the designation was removed in 2011)- and it continues to be part of an area designated as a Site of Special Scientific Interest.) Only 283 Arran whitebeam and 236 cut-leaved whitebeam were recorded as mature trees in 1980, and it is thought that grazing pressures and insect damage are preventing regeneration of the woodland. The Catacol whitebeam was discovered in 2007, but only two specimens have been found, so steps have been taken to protect them.\nEtymology.\nThe Roman historian Tacitus refers to the \"Clota\", meaning the Clyde. The derivation is not certain but is probably from the Brythonic \"Clouta\", which became \"Clut\" in Old Welsh. The name literally means \"wash\", probably referring to a river goddess who is seen as \"the washer\" or \"the strongly flowing one\". The derivation of the word \u201cBute\u201d is also uncertain. The Norse name for it is \"B\u00f3t\" an Old Irish word for \"fire\", which might be a reference to signal fires. The etymology of \u201cArran\u201d is no clearer. Haswell-Smith (2004) suggests that it derive from a Brythonic word meaning \"high place\", although Watson (1926) suggests it may be pre-Celtic.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15253", "revid": "37005538", "url": "https://en.wikipedia.org/wiki?curid=15253", "title": "International Bank Account Number", "text": "Alphanumeric code that uniquely identifies a bank account in any participating country\nThe International Bank Account Number (IBAN) is an internationally agreed upon system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. An IBAN uniquely identifies the account of a customer at a financial institution. It was originally adopted by the European Committee for Banking Standards (ECBS) and since 1997 as the international standard ISO 13616 under the International Organization for Standardization (ISO). The current version is ISO 13616:2020, which indicates the Society for Worldwide Interbank Financial Telecommunication (SWIFT) as the formal registrar. Initially developed to facilitate payments within the European Union, it has been implemented by most European countries and numerous countries in other parts of the world, mainly in the Middle East and the Caribbean. By December 2024, 89 countries were using the IBAN numbering system.\nThe IBAN consists of up to 34 alphanumeric characters comprising a country code; two check digits; and a number that includes the domestic bank account number, branch identifier, and potential routing information. The check digits enable a check of the bank account number to confirm its integrity before submitting a transaction.\nBackground.\nBefore IBAN, differing national standards for bank account identification (i.e. bank, branch, routing codes, and account number) were confusing for some users. This often led to necessary routing information being missing from payments. Routing information as specified by ISO 9362 (also known as Business Identifier Codes (BIC), SWIFT ID or SWIFT code, and SWIFT-BIC) does not require a specific format for the transaction so the identification of accounts and transaction types is left to agreements of the transaction partners. It also does not contain check digits, so errors of transcription were not detectable and it was not possible for a sending bank to validate the routing information prior to submitting the payment. Routing errors caused delayed payments and incurred extra costs to the sending and receiving banks and often to intermediate routing banks.\nIn 1997, to overcome these difficulties, the International Organization for Standardization (ISO) published ISO 13616:1997. This proposal had a degree of flexibility that the European Committee for Banking Standards (ECBS) believed would make it unworkable, and they produced a \"slimmed down\" version of the standard which, amongst other things, permitted only upper-case letters and required that the IBAN for each country have a fixed length. ISO 13616:1997 was subsequently withdrawn and replaced by ISO 13616:2003. The standard was revised again in 2007 when it was split into two parts. ISO 13616-1:2007 \"specifies the elements of an international bank account number (IBAN) used to facilitate the processing of data internationally in data interchange, in financial environments as well as within and between other industries\" but \"does not specify internal procedures, file organization techniques, storage media, languages, etc. to be used in its implementation\". ISO 13616-2:2007 describes \"the Registration Authority (RA) responsible for the registry of IBAN formats that are compliant with ISO 13616-1 [and] the procedures for registering ISO 13616-compliant IBAN formats\". The official IBAN registrar under ISO 13616-2:2007 is SWIFT.\nIBAN imposes a flexible but regular format sufficient for account identification and contains validation information to avoid errors in transcription. It carries all the routing information needed to get a payment from one bank to another wherever it may be; it contains key bank account details such as country code, branch codes (known as sort codes in the UK and Ireland) and account numbers, and it contains check digits which can be validated at the source according to a single standard procedure.\nStructure.\nThe IBAN consists of up to 34 alphanumeric characters, as follows:\nThe check digits represent the checksum of the bank account number which is used by banking systems to confirm that the number contains no simple errors.\nIn order to facilitate reading by humans, IBANs are traditionally expressed in groups of four characters separated by spaces, the last group being of variable length as shown in the example below; when transmitted electronically however spaces are omitted. Current exceptions to this formatting are Burundi (4, 5, 5, 11, 2), Egypt (no spaces), Libya (4, 3, 3, 15), and El Salvador (2, 2, 4, 20).\nPermitted IBAN characters are the digits \"0\" to \"9\" and the 26 Latin alphabetic characters \"A\" to \"Z\". This applies even in countries where these characters are not used in the national language (e.g. Greece).\nBasic Bank Account Number.\nThe Basic Bank Account Number (BBAN) format is decided by the national central bank or designated payment authority of each country. There is no consistency between the formats adopted. The national authority may register its BBAN format with SWIFT but is not obliged to do so. It may adopt IBAN without registration. SWIFT also acts as the registration authority for the SWIFT system, which is used by most countries that have not adopted IBAN. A major difference between the two systems is that under SWIFT there is no requirement that BBANs used within a country be of a pre-defined length.\nThe BBAN must be of a fixed length for the country and comprise case-insensitive alphanumeric characters. It includes the domestic bank account number, branch identifier, and potential routing information. Each country can have a different national routing/account numbering system, up to a maximum of 30 alphanumeric characters.\nCheck digits.\nThe check digits enable the sending bank (or its customer) to perform a sanity check of the routing destination and account number from a single string of data at the time of data entry. This check is guaranteed to detect any instances where a single character has been omitted, duplicated, mistyped or where two characters have been transposed.\nProcessing.\nOne of the design aims of the IBAN was to enable as much validation as possible to be done at the point of data entry. In particular, the computer program that accepts an IBAN will be able to validate:\nThe check digits are calculated using MOD-97-10 as per ISO/IEC 7064:2003 (abbreviated to \"mod-97\" in this article), which specifies a set of check character systems capable of protecting strings against errors which occur when people copy or key data. In particular, the standard states that the following can be detected:\nThe underlying rules for IBANs is that the account-servicing financial institution should issue an IBAN, as there are a number of areas where different IBANs could be generated from the same account and branch numbers that would satisfy the generic IBAN validation rules. In particular cases where codice_8 is a valid check digit, codice_9 will not be a valid check digit, likewise, if codice_10 is a valid check digit, codice_11 will not be a valid check digit, similarly with codice_12 and codice_13.\nThe UN CEFACT TBG5 has published a free IBAN validation service in 32 languages for all 57 countries that have adopted the IBAN standard. They have also published the JavaScript source code of the verification algorithm.\nAn English language IBAN checker for ECBS member country bank accounts is available on its website.\nAlgorithms.\nValidating the IBAN.\nAn IBAN is validated by converting it into an integer and performing a basic \"mod-97\" operation (as described in ISO 7064) on it. If the IBAN is valid, the remainder equals 1. The algorithm of IBAN validation is as follows:\nIf the remainder is 1, the check digit test is passed and the IBAN might be valid.\nExample (fictitious United Kingdom bank, sort code 12-34-56, account number 98765432):\nGenerating IBAN check digits.\nAccording to the ECBS \"generation of the IBAN shall be the exclusive responsibility of the bank/branch servicing the account\". The ECBS document replicates part of the ISO/IEC 7064:2003 standard as a method for generating check digits in the range 02 to 98. Check digits in the ranges 00 to 96, 01 to 97, and 03 to 99 will also provide validation of an IBAN, but the standard is silent as to whether or not these ranges may be used.\nThe preferred algorithm is:\nModulo operation on IBAN.\nAny computer programming language or software package that is used to compute \"D\" mod \"97\" directly must have the ability to handle integers of more than 60 digits. In practice, this can only be done by software that either supports arbitrary-precision arithmetic or that can handle 219-bit (unsigned) integers, features that are often not standard. If the application software in use does not provide the ability to handle integers of this size, the modulo operation can be performed in a piece-wise manner (as is the case with the UN CEFACT TBG5 JavaScript program).\nPiece-wise calculation \"D\" mod \"97\" can be done in many ways. One such way is as follows:\nThe result of the final calculation in step 2 will be \"D\" mod 97 = \"N\" mod \"97\".\nExample.\nIn this example, the above algorithm for \"D\" mod 97 will be applied to \"D\" = 3214282912345698765432161182. (The digits are colour-coded to aid the description below.) If the result is one, the IBAN corresponding to \"D\" passes the check digit test.\nFrom step 8, the final result is \"D\" mod 97 = 1 and the IBAN has passed this check digit test.\nNational check digits.\nIn addition to the IBAN check digits, many countries have their own national check digits used within the BBAN, as part of their national account number formats. Each country determines its own algorithm used for assigning and validating the national check digits - some relying on international standards, some inventing their own national standard, and some allowing each bank to decide if or how to implement them. Some algorithms apply to the entire BBAN, and others to one or more of the fields within it. The check digits may be considered an integral part of the account number, or an external field separate from the account number, depending on the country's rules.\nMost of the variations used are based on two categories of algorithms:\n- \"ISO 7064 MOD-97-10\": Treat the account number as a large integer, divide it by 97 and use the remainder or its complement as the check digit(s).\n- \"Weighted sum\": Treat the account number as a series of individual numbers, multiply each number by a weight value according to its position in the string, sum the products, divide the sum by a modulus (10, 11 or 26) and use the remainder or its complement as the check digit or letter.\nIn both cases, there may first be a translation from alphanumeric characters to numbers using conversion tables. The complement, if used, means the remainder r is subtracted from a fixed value, usually the modulus or the modulus plus one (with the common exception that a remainder of 0 results in 0, denoted as 0 \u2192 0, as opposed to e.g. 0 \u2192 97 meaning that if the remainder is zero the checksum is 97). Some national specifications define the weights order from right to left, but since the BBAN length in the IBAN is fixed, they can be used from left to right as well.\nAdoption.\nInternational bank transactions use either an IBAN or the ISO 9362 Business Identifier Code system (BIC or SWIFT code) in conjunction with the BBAN (Basic Bank Account Number).\nEEA and territories.\nThe banks of most countries in Europe publish account numbers using both the IBAN format and the nationally recognised identifiers, this being mandatory within the European Economic Area.\nDay-to-day administration of banking in British Overseas Territories varies from territory to territory; some, such as South Georgia and the South Sandwich Islands, have too small a population to warrant a banking system while others, such as Bermuda, have a thriving financial sector. The use of the IBAN is up to the local government\u2014Gibraltar, formerly part of the European Union is required to use the IBAN, as are the Crown Dependencies, which use the British clearing system, and the British Virgin Islands have chosen to do so. As of \u00a02013[ [update]], no other British Overseas Territories have chosen to use the IBAN. Banks in the Caribbean Netherlands also do not use the IBAN.\nSingle Euro Payments Area.\nThe IBAN designation scheme was chosen as the foundation for electronic straight-through processing in the European Economic Area. The European Parliament mandated that a bank charge needs to be the same amount for domestic credit transfers as for cross-border credit transfers regulated in decision 2560/2001 (updated in 924/2009). This regulation took effect in 2003. Only payments in euro up to \u20ac12,500 to a bank account designated by its IBAN were covered by the regulation, not payments in other currencies.\nThe Euro Payments regulation was the foundation for the decision to create a Single Euro Payments Area (SEPA). The European Central Bank has created the TARGET Services network that unifies the technical infrastructure of the 26 central banks of the European Union (although Sweden has opted out). SEPA is a self-regulatory initiative by the banking sector of Europe as represented in the European Payments Council (EPC). The European Union made the scheme mandatory through the Payment Services Directive published in 2007. Since January 2008, all countries were required to support SEPA credit transfer, and SEPA direct debit was required to be supported since November 2009. The regulation on SEPA payments increased the charge cap (same price for domestic payments as for cross-border payments) to \u20ac50,000.\nWith a further decision of the European Parliament, the IBAN scheme for bank accounts fully replaced the domestic numbering schemes from 31 December 2012. On 16 December 2010, the European Commission published regulations that made IBAN support mandatory for domestic credit transfer by 2013 and for domestic direct debit by 2014 (with a 12 and 24 months transition period respectively). Some countries had already replaced their traditional bank account scheme by IBAN. This included Switzerland where IBAN was introduced for national credit transfer on 1 January 2006 and the support for the old bank account numbers was not required from 1 January 2010.\nBased on a 20 December 2011 memorandum, the EU parliament resolved the mandatory dates for the adoption of the IBAN on 14 February 2012. On 1 February 2014, all national systems for credit transfer and direct debit were abolished and replaced by an IBAN-based system. This was then extended to all cross-border SEPA transactions on 1 February 2016 (Article 5 Section 7). After these dates the IBAN is sufficient to identify an account for home and foreign financial transactions in SEPA countries and banks are no longer permitted to require that the customer supply the BIC for the beneficiary's bank.\nIn the run-up to the 1 February 2014 deadline, it became apparent that many old bank account numbers had not been allocated IBANs\u2014an issue that was addressed on a country-by-country basis. In Germany, for example, Deutsche Bundesbank and the German Banking Industry Committee required that all holders of German bank codes (\"Bankleitzahl\") published the specifics of their IBAN generation format taking into account not only the generation of check digits but also the handling of legacy bank codes, thereby enabling third parties to generate IBANs independently of the bank. The first such catalogue was published in June 2013 as a variant of the old bank code catalog (\"Bankleitzahlendatei\").\nNon-EEA.\nBanks in numerous non-European countries including most states of the Middle East, North Africa and the Caribbean have implemented the IBAN format for account identification. In some countries the IBAN is used on an \"ad hoc\" basis, an example was Ukraine where account numbers used for international transfers by some domestic banks had additional aliases that followed the IBAN format as a precursor to formal SWIFT registration. This practice in Ukraine ended on 1 November 2019 when all Ukrainian banks had fully switched to the IBAN standard.\nThe degree to which a bank verifies the validity of a recipient's bank account number depends on the configuration of the transmitting bank's software\u2014many major software packages supply bank account validation as a standard function. Some banks outside Europe may not recognize IBAN, though this is expected to diminish with time. Non-European banks usually accept IBANs for accounts in Europe, although they might not treat IBANs differently from other foreign bank account numbers. In particular, they might not check the IBAN's validity prior to sending the transfer.\nBanks in the United States do not use IBAN as account numbers for U.S. accounts and use ABA routing transit numbers. Any adoption of the IBAN standard by U.S. banks would likely be initiated by ANSI ASC X9, the U.S. financial services standards development organization: a working group (X9B20) was established as an X9 subcommittee to generate an IBAN construction for U.S. bank accounts.\nCanadian financial institutions have not adopted IBAN and use routing numbers issued by Payments Canada for domestic transfers, and SWIFT for international transfers. There is no formal governmental or private sector regulatory requirement in Canada for the major banks to use IBAN.\nAustralia and New Zealand do not use IBAN. They use Bank State Branch codes for domestic transfers and SWIFT for international transfers.\nIBAN formats by country.\nThis table summarises the IBAN formats by country:\nIn addition to the above, the IBAN is under development in countries below but has not yet been catalogued for general international use.\nIn this list\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15254", "revid": "50727176", "url": "https://en.wikipedia.org/wiki?curid=15254", "title": "Infinitive", "text": "Grammatical form\nInfinitive (abbreviated INF) is a term in linguistics for certain verb forms existing in many languages, most often used as non-finite verbs that do not show a tense. As with many linguistic concepts, there is not a single definition applicable to all languages. The name is derived from Late Latin [] , a derivative of meaning \"unlimited\".\nIn traditional descriptions of English, the infinitive is the basic dictionary form of a verb when used non-finitely, with or without the particle to. Thus to go is an infinitive, as is \"go\" in a sentence like \"I must go there\" (but not in \"I go there\", where it is a finite verb). The form without \"to\" is called the bare infinitive, and the form with \"to\" is called the full infinitive or to-infinitive.\nIn many other languages the infinitive is a distinct single word, often with a characteristic inflective ending, like (\"[to] sing\") in Portuguese, (\"[to] die\") in Spanish, (\"[to] eat\") in French, (\"[to] carry\") in Latin and Italian, (\"[to] love\") in German, (', \"[to] read\") in Russian, etc. However, some languages have no infinitive forms. Many Native American languages, Arabic, Asian languages such as Japanese, and some languages in Africa and Australia do not have direct equivalents to infinitives or verbal nouns. Instead, they use finite verb forms in ordinary clauses or various special constructions.\nBeing a verb, an infinitive may take objects and other complements and modifiers to form a verb phrase (called an infinitive phrase). Like other non-finite verb forms (like participles, converbs, gerunds and gerundives), infinitives do not generally have an expressed subject; thus an infinitive verb phrase also constitutes a complete non-finite clause, called an infinitive (infinitival) clause. Such phrases or clauses may play a variety of roles within sentences, often being nouns (for example being the subject of a sentence or being a complement of another verb), and sometimes being adverbs or other types of modifier. Many verb forms known as infinitives differ from gerunds (verbal nouns) in that they do not inflect for case or occur in adpositional phrases. Instead, infinitives often originate in earlier inflectional forms of verbal nouns. Unlike finite verbs, infinitives are not usually inflected for tense, person, etc. either, although some degree of inflection sometimes occurs; for example Latin has distinct active and passive infinitives.\nPhrases and clauses.\nAn \"infinitive phrase\" is a verb phrase constructed with the verb in infinitive form. This consists of the verb together with its objects and other complements and modifiers. Some examples of infinitive phrases in English are given below \u2013 these may be based on either the full infinitive (introduced by the particle \"to\") or the bare infinitive (without the particle \"to\").\nInfinitive phrases often have an implied grammatical subject making them effectively clauses rather than phrases. Such \"infinitive clauses\" or \"infinitival clauses\", are one of several kinds of non-finite clause. They can play various grammatical roles like a constituent of a larger clause or sentence; for example it may form a noun phrase or adverb. Infinitival clauses may be embedded within each other in complex ways, like in the sentence:\nHere the infinitival clause \"to get married\" is contained within the finite dependent clause \"that John Welborn is going to get married to Blair\"; this in turn is contained within another infinitival clause, which is contained in the finite independent clause (the whole sentence).\nThe grammatical structure of an infinitival clause may differ from that of a corresponding finite clause. For example, in German, the infinitive form of the verb usually goes to the end of its clause, whereas a finite verb (in an independent clause) typically comes in second position.\nClauses with implicit subject in the objective case.\nFollowing certain verbs or prepositions, infinitives commonly \"do\" have an implicit subject, e.g.,\nAs these examples illustrate, the implicit subject of the infinitive occurs in the objective case (them, him) in contrast to the nominative case that occurs with a finite verb, e.g., \"They ate their dinner.\" \nSuch accusative and infinitive constructions are present in Latin and Ancient Greek, as well as many modern languages. The atypical case regarding the implicit subject of an infinitive is an example of exceptional case-marking. As shown in the above examples, the object of the transitive verb \"want\" and the preposition \"for\" allude to their respective pronouns' subjective role within the clauses.\nMarking for tense, aspect and voice.\nIn some languages, infinitives may be marked for grammatical categories like voice, aspect, and to some extent tense. This may be done by inflection, as with the Latin perfect and passive infinitives, or by periphrasis (with the use of auxiliary verbs), as with the Latin future infinitives or the English perfect and progressive infinitives.\nLatin has present, perfect and future infinitives, with active and passive forms of each. For details see .\nEnglish has infinitive constructions that are marked (periphrastically) for aspect: perfect, progressive (continuous), or a combination of the two (perfect progressive). These can also be marked for passive voice (as can the plain infinitive):\nFurther constructions can be made with other auxiliary-like expressions, like \"(to) be going to eat\" or \"(to) be about to eat\", which have future meaning. For more examples of the above types of construction, see .\nPerfect infinitives are also found in other European languages that have perfect forms with auxiliaries similarly to English. For example, \"avoir mang\u00e9\" means \"(to) have eaten\" in French.\nEnglish.\nThe term \"infinitive\" is traditionally applied to the unmarked form of the verb (the \"plain form\") when it forms a non-finite verb, whether or not introduced by the particle \"to\". Hence \"sit\" and \"to sit\", as used in the following sentences, would each be considered an infinitive:\nThe form without \"to\" is called the \"bare infinitive\"; the form introduced by \"to\" is called the \"full infinitive\" or \"to-infinitive\".\nThe other non-finite verb forms in English are the gerund or present participle (the \"-ing\" form), and the past participle \u2013 these are not considered infinitives. Moreover, the unmarked form of the verb is not considered an infinitive when it forms a finite verb: like a present indicative (\"I \"sit\" every day\"), subjunctive (\"I suggest that he \"sit\"), or imperative (\"Sit\" down!\"). (For some irregular verbs the form of the infinitive coincides additionally with that of the past tense and/or past participle, like in the case of \"put\".)\nCertain auxiliary verbs are modal verbs (such as \"can\", \"must\", etc., which defective verbs lacking an infinitive form or any truly inflected non-finite form) are complemented by a bare infinitive verb. periphrastic items, such as (1) had better or ought to as substitutes for \"should,\" (2) used to as a substitute for did, and (3) \"(to) be able to\" for \"can\", are similarly complemented by a bare infinitive verb. Infinitives are negated by simply preceding them with \"not\". Of course the verb \"do\", when complementing a finite verb, occurs as an infinitive. However, the auxiliary verbs \"have\" (used to form the perfect) and \"be\" (used to form the passive voice and continuous aspect) often occur as an infinitive: \"I should have finished by now\"; \"It's thought to have been a burial site\"; \"Let him be released\"; \"I hope to be working tomorrow.\"\nHuddleston and Pullum's \"Cambridge Grammar of the English Language\" (2002) does not use the notion of the \"infinitive\" (\"there is no form in the English verb paradigm called 'the infinitive'\"), only that of the \"infinitival clause\", noting that English uses the same form of the verb, the \"plain form\", in infinitival clauses that it uses in imperative and present-subjunctive clauses.\nA matter of controversy among prescriptive grammarians and style writers has been the appropriateness of separating the two words of the \"to\"-infinitive (as in \"I expect \"to\" happily \"sit\" here\"). For details of this, see split infinitive. Opposing linguistic theories typically do not consider the \"to\"-infinitive a distinct constituent, instead regarding the scope of the particle \"to\" as an entire verb phrase; thus, \"to buy a car\" is parsed like \"to [buy [a car]]\", not like \"[to buy] [a car]\".\nUses of the infinitive.\nThe bare infinitive and the \"to\"-infinitive have a variety of uses in English. The two forms are mostly in complementary distribution \u2013 certain contexts call for one, and certain contexts for the other; they are not normally interchangeable, except in occasional instances like after the verb \"help\", where either can be used.\nThe main uses of infinitives (or infinitive phrases) are varied:\n# to complement a modal auxiliary verb, \"I can't breathe\" or \"I can see clearly now.\"\n# to complement a direct object that \u2013\na. follows a verb of perception such as \"see\", \"watch\" or \"hear\", e.g. \"We saw it fall\" or \"I can hear the birds sing.\"\nb. follows a verb of causation such as \"make\", \"bid\", or \"have\", e.g. \"Make it stop or \"We'll have them call you.\"\nc. follows a verb of permission, e.g. \"Let me ask you something.\"\n# the subject of a clause: \"To err is human\" or \"To know me is to love me.\" \n# the object of a predicative expression: \"What you should do is make a list\" or \"To know me is to love me\".\n# to express purpose, intent or result, as the \"to\"-infinitive can have the meaning of \"in order to\", e.g. \"I closed the door [in order] to block out any noise.\" \n# to characterize an adjective, e.g., \"keen to get on\" or \"nice to listen to\".\n# the bare infinitive is used after \"why\", e.g., \"Why reveal it?\"\n# the \"to-\" infinitive is used:\na. after \"whom\", e.g., \"Whom to believe?\"\nb. after \"what\", e.g., \"What to do?\"\nc. after \"when\", e.g., \"When to surrender?\"\nd. after \"where\", e.g., \"Where to go?\"\ne. after \"how\", e.g., \"How to know?\"\nThe infinitive typically is the dictionary form or citation form of a verb. The form listed in a dictionary entry is the bare infinitive, but the \"to\"-infinitive is often used when defining other verbs, e.g.\namble (verb)\nambled; ambling\n\"intransitive verb\"\n#to walk slowly\n#to stroll without a particular aim\nFor further detail and examples of the uses of infinitives in English, see Bare infinitive and \"To\"-infinitive in the article on uses of English verb forms.\nOther Germanic languages.\nThe original Proto-Germanic ending of the infinitive was \"-an\", with verbs derived from other words ending in \"-jan\" or \"-janan\".\nIn German it is \"-en\" (\"sagen\"), with \"-eln\" or \"-ern\" endings on a few words based on -l or -r roots (\"segeln\", \"\u00e4ndern\"). The use of \"zu\" with infinitives is similar to English \"to\", but is less frequent than in English. German infinitives can form nouns, often expressing abstractions of the action, in which case they are of neuter gender: \"das Essen\" means \"the eating\", but also \"the food\".\nIn Dutch infinitives also end in \"-en\" (\"zeggen\" \u2014 \"to say\"), sometimes used with \"te\" similar to English \"to\", e.g., \"Het is niet moeilijk te begrijpen\" \u2192 \"It is not hard to understand.\" The few verbs with stems ending in \"-a\" have infinitives in -n (\"gaan\" \u2014 \"to go\", \"slaan\" \u2014 \"to hit\"). Afrikaans has lost the distinction between the infinitive and present forms of verbs, with the exception of the verbs \"wees\" (to be), which admits the present form \"is\", and the verb \"h\u00ea\" (to have), whose present form is \"het\".\nIn North Germanic languages the final \"-n\" was lost from the infinitive as early as 500\u2013540 AD, reducing the suffix to \"-a\". Later it has been further reduced to \"-e\" in Danish and some Norwegian dialects (including the written majority language bokm\u00e5l). In the majority of Eastern Norwegian dialects and a few bordering Western Swedish dialects the reduction to \"-e\" was only partial, leaving some infinitives in \"-a\" and others in \"-e\" (\u00e5 laga vs. \u00e5 kaste). In northern parts of Norway the infinitive suffix is completely lost (\u00e5 lag\u2019 vs. \u00e5 kast\u2019) or only the \"-a\" is kept (\u00e5 laga vs. \u00e5 kast\u2019). The infinitives of these languages are inflected for passive voice through the addition of \"-s\" or \"-st\" to the active form. This suffix appeared in Old Norse as a contraction of \"mik\" (\"me\", forming \"-mk\") or \"sik\" (reflexive pronoun, forming \"-sk\") and originally expressed reflexive actions: (hann) \"kallar\" (\"[he] calls\") + \"-sik\" (\"himself\") &gt; (hann) \"kallask\" (\"[he] calls himself\"). The suffixes \"-mk\" and \"-sk\" later merged into \"-s\", which evolved to \"-st\" in the western dialects. The loss or reduction of \"-a\" in the active voice in Norwegian did not occur in the passive forms (\"-ast\", \"-as\"), except for some dialects that have \"-es\". The other North Germanic languages have the same vowel in both forms.\nLatin and Romance languages.\nThe formation of the infinitive in the Romance languages reflects that in their ancestor, Latin, almost all verbs had an infinitive ending with \"-re\" (preceded by one of various thematic vowels). For example, in Italian infinitives end in \"-are\", \"-ere\", \"-rre\" (rare), or \"-ire\" (which is still identical to the Latin forms), and in \"-arsi\", \"-ersi\", \"-rsi\", \"-irsi\" for the reflexive forms. In Spanish and Portuguese, infinitives end in \"-ar\", \"-er\", or \"-ir\" (Spanish also has reflexive forms in \"-arse\", \"-erse\", \"-irse\"), while similarly in French they typically end in \"-re\", \"-er\", \"oir\", and \"-ir\". In Romanian, both short and long-form infinitives exist; the so-called \"long infinitives\" end in \"-are, -ere, -ire\" and in modern speech are used exclusively as verbal nouns, while there are a few verbs that cannot be converted into the nominal long infinitive. The \"short infinitives\" used in verbal contexts (e.g., after an auxiliary verb) have the endings \"-a\",\"-ea\", \"-e\", and \"-i\" (basically removing the ending in \"-re\"). In Romanian, the infinitive is usually replaced by a clause containing the conjunction \"s\u0103\" plus the subjunctive mood. The only verb that is modal in common modern Romanian is the verb \"a putea\", to be able to. However, in popular speech the infinitive after \"a putea\" is also increasingly replaced by the subjunctive.\nIn all Romance languages, infinitives can also form nouns.\nLatin infinitives challenged several of the generalizations about infinitives. They did inflect for voice (\"amare\", \"to love\", \"amari\", to be loved) and for tense (\"amare\", \"to love\", \"amavisse\", \"to have loved\"), and allowed for an overt expression of the subject (\"video Socratem currere\", \"I see Socrates running\"). See .\nRomance languages inherited from Latin the possibility of an overt expression of the subject (as in Italian \"vedo Socrate correre\"). Moreover, the \"inflected infinitive\" (or \"personal infinitive\") found in Portuguese and Galician inflects for person and number. These, alongside some dialects of Logudorese Sardinian, Old Neapolitan and some modern Southern Italian languages are the only Indo-European languages that allow infinitives to take person and number endings. This helps to make infinitive clauses very common in these languages; for example, the English finite clause \"in order that you/she/we have...\" would be translated to Portuguese like \"para teres/ela ter/termos...\" (Portuguese is a null-subject language). The Portuguese personal infinitive has no proper tenses, only aspects (imperfect and perfect), but tenses can be expressed using periphrastic structures. For instance, \"even though you sing/have sung/are going to sing\" could be translated to \"apesar de cantares/teres cantado/ires cantar\".\nOther Romance languages (including Spanish, Romanian, Catalan, and some Italian dialects) allow uninflected infinitives to combine with overt nominative subjects. For example, Spanish \"al abrir yo los ojos\" (\"when I opened my eyes\") or \"sin yo saberlo\" (\"without my knowing about it\").\nHellenic languages.\nAncient Greek.\nIn Ancient Greek the infinitive has four tenses (present, future, aorist, perfect) and three voices (active, middle, passive). Present and perfect have the same infinitive for both middle and passive, while future and aorist have separate middle and passive forms.\nThematic verbs form present active infinitives by adding to the stem the thematic vowel and the infinitive ending , and contracts to , e.g., . Athematic verbs, and perfect actives and aorist passives, add the suffix instead, e.g., . In the middle and passive, the present middle infinitive ending is , e.g., and most tenses of thematic verbs add an additional between the ending and the stem, e.g., .\nModern Greek.\nThe infinitive \"per se\" does not exist in Modern Greek. To see this, consider the ancient Greek \"\u1f10\u03b8\u03ad\u03bb\u03c9 \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\" \"I want to write\". In modern Greek this becomes \"\u03b8\u03ad\u03bb\u03c9 \u03bd\u03b1 \u03b3\u03c1\u03ac\u03c8\u03c9\" \"I want that I write\". In modern Greek, the infinitive has thus changed form and function and is used mainly in the formation of periphrastic tense forms and not with an article or alone. Instead of the Ancient Greek infinitive system \"\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd, \u03b3\u03c1\u03ac\u03c8\u03b5\u03b9\u03bd, \u03b3\u03c1\u03ac\u03c8\u03b1\u03b9, \u03b3\u03b5\u03b3\u03c1\u03b1\u03c6\u03ad\u03bd\u03b1\u03b9\", Modern Greek uses only the form \"\u03b3\u03c1\u03ac\u03c8\u03b5\u03b9\", a development of the ancient Greek aorist infinitive \"\u03b3\u03c1\u03ac\u03c8\u03b1\u03b9\". This form is also invariable. The modern Greek infinitive has only two forms according to voice: for example, \"\u03b3\u03c1\u03ac\u03c8\u03b5\u03b9\" for the active voice and \"\u03b3\u03c1\u03b1\u03c6(\u03c4)\u03b5\u03af\" for the passive voice (coming from the ancient passive aorist infinitive \"\u03b3\u03c1\u03b1\u03c6\u1fc6\u03bd\u03b1\u03b9\").\nBalto-Slavic languages.\nThe infinitive in Russian usually ends in \"-t\u2019\" (\u0442\u044c) preceded by a thematic vowel, or \"-ti\" (\u0442\u0438), if not preceded by one; some verbs have a stem ending in a consonant and change the \"t\" to \"\u010d\u2019\", like \"*mogt\u2019 \u2192 mo\u010d\u2019\" (*\u043c\u043e\u0433\u0442\u044c \u2192 \u043c\u043e\u0447\u044c) \"can\". Some other Balto-Slavic languages have the infinitive typically ending in, for example, \"-\u0107\" (sometimes \"-c\") in Polish, \"-\u0165\" in Slovak, \"-t\" (formerly \"-ti\") in Czech and Latvian (with a handful ending in -s on the latter), \"-ty\" (-\u0442\u0438) in Ukrainian, -\u0446\u044c (\"-ts\"') in Belarusian. Lithuanian infinitives end in -\"ti\", Serbo-Croatian in -\"ti\" or -\"\u0107i,\" and Slovenian in -\"ti\" or -\"\u010di.\"\nSerbian officially retains infinitives -\"ti\" or -\"\u0107i\", but is more flexible than the other Slavic languages in breaking the infinitive through a clause. The infinitive nevertheless remains the dictionary form.\nBulgarian and Macedonian have lost the infinitive altogether except in a handful of frozen expressions where it is the same as the 3rd person singular aorist form. Almost all expressions where an infinitive may be used in Bulgarian are listed here; nevertheless in all cases a subordinate clause is the more usual form. For that reason, the present first-person singular conjugation is the dictionary form in Bulgarian, while Macedonian uses the third person singular form of the verb in present tense.\nHebrew.\nHebrew has \"two\" infinitives, the infinitive absolute (\u05d4\u05b7\u05de\u05b8\u05bc\u05e7\u05d5\u05b9\u05e8 \u05d4\u05b7\u05de\u05bb\u05bc\u05d7\u05b0\u05dc\u05b8\u05d8, \"ham-m\u0101q\u014dr ham-mu\u1e25l\u0101\u1e6d\") and the infinitive construct (\u05d4\u05b7\u05de\u05b8\u05bc\u05e7\u05d5\u05b9\u05e8 \u05d4\u05b7\u05e0\u05b8\u05bc\u05d8\u05d5\u05bc\u05d9, \"ham-m\u0101q\u014dr han-n\u0101\u1e6d\u016by\", or \u05e9\u05b5\u05c1\u05dd \u05d4\u05b7\u05e4\u05b9\u05bc\u05e2\u05b7\u05dc, \"\u0161\u0113m hap-p\u014d\u0295al\"). The infinitive construct is used after prepositions and is inflected with pronominal endings to indicate its subject or object: \u05d1\u05b4\u05bc\u05db\u05b0\u05ea\u05b9\u05d1 \u05d4\u05b7\u05e1\u05bc\u05d5\u05b9\u05e4\u05b5\u05e8 (\"bi\u1e35\u1e6f\u014d\u1e07 has-s\u014dp\u0304\u0113r\", \"when the scribe wrote\"), \u05d0\u05b7\u05d7\u05b2\u05e8\u05b5\u05d9 \u05dc\u05b6\u05db\u05b0\u05ea\u05bc\u05d5\u05b9 (\"\u0294a\u1e25\u0103r\u0113 le\u1e35t\u014d\", \"after his going\"). When the infinitive construct is preceded by the dative preposition \u05dc\u05be, it has a similar meaning to the English \"to\"-infinitive, and this is its most frequent use in Modern Hebrew. The infinitive absolute is used for verb focus and emphasis, as in \u05de\u05d5\u05b9\u05ea \u05d9\u05b8\u05de\u05d5\u05bc\u05ea \"m\u014d\u1e6f y\u0101m\u016b\u1e6f\" (literally \"a dying he will die\", figuratively \"he shall indeed/surely die\"). This usage is commonplace in the Hebrew Bible. In Modern Hebrew it is restricted to high-register literary works.\nNote, however, that the Hebrew \"to\"-infinitive is not the dictionary form; instead, verbs are traditionally cited in the third-person masculine singular of the suffix conjugation (the Modern Hebrew past tense), which is the least marked form.\nFinnish.\nThe Finnish grammatical tradition includes many non-finite forms that are generally labeled as (numbered) infinitives although many of these are functionally converbs. To form the so-called first infinitive, the strong form of the root (without consonant gradation or epenthetic 'e') is used, and these changes occur:\nAs such, it is inconvenient for dictionary use, because the imperative would be closer to the root word. Nevertheless, dictionaries use the first infinitive.\nThere are also four other infinitives, plus a \"long\" form of the first:\nNote that all of these must change to reflect vowel harmony, so the fifth infinitive (with a third-person suffix) of \"hyp\u00e4t\u00e4\" \"jump\" is \"hypp\u00e4\u00e4m\u00e4isill\u00e4\u00e4n\" \"he was about to jump\", not \"*hypp\u00e4\u00e4maisillaan\".\nSeri.\nThe Seri language of northwestern Mexico has infinitival forms used in two constructions (with the verb meaning 'want' and with the verb meaning 'be able'). The infinitive is formed by adding a prefix to the stem: either \"iha-\" (plus a vowel change of certain vowel-initial stems) if the complement clause is transitive, or \"ica-\" (and no vowel change) if the complement clause is intransitive. The infinitive shows agreement in number with the controlling subject. Examples are: \"icatax ihmiimzo\" 'I want to go', where \"icatax\" is the singular infinitive of the verb 'go' (singular root is \"-atax\"), and \"icalx hamiimcajc\" 'we want to go', where \"icalx\" is the plural infinitive. Examples of the transitive infinitive: \"ihaho\" 'to see it/him/her/them' (root \"-aho\"), and \"ihacta\" 'to look at it/him/her/them' (root \"-oocta\").\nTranslation to languages without an infinitive.\nIn languages without an infinitive, the infinitive is translated either as a \"that\"-clause or as a verbal noun. For example, in Literary Arabic the sentence \"I want to write a book\" is translated as either \"ur\u012bdu an aktuba kit\u0101ban\" (lit. \"I want that I write a book\", with a verb in the subjunctive mood) or \"ur\u012bdu kit\u0101bata kit\u0101bin\" (lit. \"I want the writing of a book\", with the \"masdar\" or verbal noun), and in Levantine Colloquial Arabic \"biddi aktub kit\u0101b\" (subordinate clause with verb in subjunctive).\nEven in languages that have infinitives, similar constructions are sometimes necessary where English would allow the infinitive. For example, in French the sentence \"I want you to come\" translates to \"Je veux que vous veniez\" (lit. \"I want that you come\", \"come\" being in the subjunctive mood). However, \"I want to come\" is simply \"Je veux venir\", using the infinitive, just as in English. In Russian, sentences such as \"I want you to leave\" do not use an infinitive. Rather, they use the conjunction \u0447\u0442\u043e\u0431\u044b \"in order to/so that\" with the past tense form (most probably remnant of subjunctive) of the verb: \"\u042f \u0445\u043e\u0447\u0443, \u0447\u0442\u043e\u0431\u044b \u0432\u044b \u0443\u0448\u043b\u0438\" (literally, \"I want so that you left\").\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15255", "revid": "25859633", "url": "https://en.wikipedia.org/wiki?curid=15255", "title": "Intellectual Property law", "text": ""}
{"id": "15256", "revid": "51009183", "url": "https://en.wikipedia.org/wiki?curid=15256", "title": "Immaculate Conception", "text": "Teaching that Mary was conceived free from original sin\nThe Immaculate Conception is the doctrine that the Virgin Mary was free of original sin from the moment of her conception. It is one of the four Marian dogmas of the Catholic Church. Debated by medieval theologians, it was not defined as a dogma until 1854, by Pope Pius IX in the papal bull \"Ineffabilis Deus\". While the Immaculate Conception asserts Mary's freedom from original sin, the Council of Trent, held between 1545 and 1563, had previously non-dogmatically affirmed her freedom from personal sin.\nThe Immaculate Conception became a popular subject in literature, but its abstract nature meant it was late in appearing as a subject in works of art. The iconography of Our Lady of the Immaculate Conception shows Mary standing, with arms outstretched or hands clasped in prayer. The feast day of the Immaculate Conception is December 8.\nMany Protestant churches rejected the doctrine of the Immaculate Conception as unscriptural, though some Anglicans accept it as a pious devotion. The teaching on the Immaculate Conception among Oriental Orthodoxy varies: Shenouda III, Pope of the Coptic Orthodox Church, and the Patriarch Ignatius Zakka I of the Syriac Orthodox Church opposed the teaching, while the Eritrean Orthodox Tewahedo Church and Ethiopian Orthodox Tewahedo Church accept it.\nHistory.\nAnne, mother of Mary, and original sin.\nAnne, the mother of Mary, first appears in the 2nd-century apocryphal Gospel of James. The author of the gospel borrowed from Greek tales of the childhood of heroes. For Jesus' grandmother the author drew on the more benign biblical story of Hannah\u2014hence Anna\u2014who conceived Samuel in her old age, thus reprising the miraculous birth of Jesus with a merely remarkable one for his mother. Anne and her husband, Joachim, are infertile, but God hears their prayers and Mary is conceived. According to Stephen J. Shoemaker, within the Gospel of James, the conception occurs without sexual intercourse between Anne and Joachim, which fits well with the Gospel of James' persistent emphasis on Mary's sacred purity, but the story does not advance the idea of an immaculate conception. The author of the Gospel of James may have based this account of Mary's conception on that of John the Baptist as recounted in the Gospel of Luke. The Eastern Orthodox Church holds that \"Mary is conceived by her parents as we are all conceived\".\nChurch Fathers.\nAccording to church historian Frederick Holweck, writing in the \"Catholic Encyclopedia\", Justin Martyr, Irenaeus, and Cyril of Jerusalem developed the idea of Mary as the New Eve, drawing comparison to Eve, while yet immaculate and incorrupt\u00a0\u2013 that is to say, not subject to original sin. Holweck adds that Ephrem the Syrian said she was as innocent as Eve before the Fall.\nAmbrose asserted Mary's incorruptibility, attributing her virginity to grace and immunity from sin. Severus, Bishop of Antioch, concurred affirming Mary's purity and immaculateness. John Damascene extended the supernatural influence of God to Mary's parents, suggesting they were purified by the Holy Spirit during her generation. According to Damascene, even the material of Mary's origin was deemed pure and holy. This perspective, which emphasized an immaculate active generation and the sanctity of the \"conceptio carnis\", found resonance among some Western authors. Notably, the Greek Fathers did not explicitly discuss the Immaculate Conception.\nMedieval formulation.\nBy the 4th century the idea that Mary was free from sin was generally more widespread, but original sin raised the question of whether she was also free of the sin passed down from Adam. The question became acute when the feast of her conception began to be celebrated in England in the 11th century, and the question of inherited sin was raised in regard to Mary's state. The feast of Mary's conception originated in the Eastern Church in the 7th century, reached England in the 11th, and from there spread to Europe, where it was given official approval in 1477 and extended to the whole church in 1693; the word \"immaculate\" was not officially added to the name of the feast until 1854.\nThe doctrine of the Immaculate Conception caused a virtual civil war between Franciscans and Dominicans during the Middle Ages, with Franciscan 'Scotists' in its favour and Dominican 'Thomists' against it. The English ecclesiastic and scholar Eadmer (c.\u20091060\u00a0\u2013 c.\u20091126) reasoned that it was possible that Mary was conceived without original sin in view of God's omnipotence, and that it was also appropriate in view of her role as Mother of God: \"Potuit, decuit, fecit\", \"it was possible, it was fitting, therefore it was done\". Others, including Bernard of Clairvaux (1090\u20131153) and Thomas Aquinas (1225\u20131274), objected that if Mary were free of original sin at her conception then she would have no need of redemption, making Christ's saving redemption superfluous; they were answered by Duns Scotus (1264\u20131308), who \"developed the idea of preservative redemption as being a more perfect one: to have been preserved free from original sin was a greater grace than to be set free from sin\". \nIn 1439, the Council of Basel, in schism with Pope Eugene IV who resided at the Council of Florence, declared the Immaculate Conception a \"pious opinion\" consistent with faith and Scripture; the Council of Trent, held in several sessions in the early 1500s, made no explicit declaration on the subject but exempted her from the universality of original sin, affirming she remained during all her life free from all stain of sin, even the venial one; by 1571 the revised Roman Breviary set out an elaborate celebration of the Feast of the Immaculate Conception on 8 December.\nPopular devotion and \"Ineffabilis Deus\".\nAccording to Patrizia Granziera, the creation of the dogma was slow and elaborate and it was more the fruit of popular devotion than scholarly. The Immaculate Conception became a popular subject in literature and art, and some devotees went so far as to hold that Anne had conceived Mary by kissing her husband Joachim, and that Anne's father and grandmother had likewise been conceived without sexual intercourse, although Bridget of Sweden (c.\u20091303\u20131373) told how Mary herself had revealed to her that Anne and Joachim conceived their daughter through a sexual union which was sinless because it was pure and free of sexual lust.\nIn the 16th and especially the 17th centuries there was a proliferation of Immaculatist devotion in Spain, leading the Habsburg monarchs to demand that the papacy elevate the belief to the status of dogma. In France in 1830 Catherine Labour\u00e9 (May 2, 1806 \u2013 December 31, 1876) saw a vision of Mary standing on a globe while a voice commanded her to have a medal made in imitation of what she saw. The medal said \"O Mary, conceived without sin, pray for us who have recourse to thee\", which was a confirmation from Mary herself that she was conceived without sin. Labour\u00e9's vision marked the beginning of a great 19th-century Marian revival.\nIn 1849 Pope Pius IX issued the encyclical \"Ubi primum\" soliciting the bishops of the church for their views on whether the doctrine should be defined as dogma; ninety percent of those who responded were supportive, although the Archbishop of Paris, Marie-Dominique-Auguste Sibour, warned that the Immaculate Conception \"could be proved neither from the Scriptures nor from tradition\". In 1854 the Immaculate Conception dogma was proclaimed with the bull \"Ineffabilis Deus\".\nWe declare, pronounce, and define that the doctrine which holds that the most Blessed Virgin Mary, in the first instance of her conception, by a singular grace and privilege granted by Almighty God, in view of the merits of Jesus Christ, the Saviour of the human race, was preserved free from all stain of original sin, is a doctrine revealed by God and therefore to be believed firmly and constantly by all the faithful.\nDom Prosper Gu\u00e9ranger, Abbot of Solesmes Abbey, who had been one of the main promoters of the dogmatic statement, wrote \"M\u00e9moire sur l'Immacul\u00e9e Conception\", explaining what he saw as its basis:\n For the belief to be defined as a dogma of faith [...] it is necessary that the Immaculate Conception form part of Revelation, expressed in Scripture or Tradition, or be implied in beliefs previously defined. Needed, afterward, is that it be proposed to the faith of the faithful through the teaching of the ordinary magisterium. Finally, it is necessary that it be attested by the liturgy, and the Fathers and Doctors of the Church.\nGu\u00e9ranger maintained that these conditions were met and that the definition was therefore possible. \"Ineffabilis Deus\" found the Immaculate Conception in the Ark of Salvation (Noah's Ark), Jacob's Ladder, the Burning Bush at Sinai, the Enclosed Garden from the Song of Songs, and many more passages. From this wealth of support the pope's advisors singled out Genesis 3:15: \"The most glorious Virgin ... was foretold by God when he said to the serpent: 'I will put enmity between you and the woman,'\" a prophecy which reached fulfilment in the figure of the Woman in the Revelation of John, crowned with stars and trampling the Dragon underfoot. Luke 1:28, and specifically the phrase \"full of grace\" by which Gabriel greeted Mary, was another reference to her Immaculate Conception: \"she was never subject to the curse and was, together with her Son, the only partaker of perpetual benediction\".\n\"Ineffabilis Deus\" was one of the pivotal events of the papacy of Pius, pope from 16 June 1846 to his death on 7 February 1878. Four years after the proclamation of the dogma, in 1858, the young Bernadette Soubirous said that Mary appeared to her at Lourdes in southern France, to announce that she was the Immaculate Conception; the Catholic Church later endorsed the apparition as authentic. There are other (approved) Marian apparitions in which Mary identified herself as the Immaculate Conception, for example Our Lady of Gietrzwald in 1877, Poland.\nFeast, patronages and disputes.\nThe feast day of the Immaculate Conception is December 8. The Roman Missal and the Roman Rite Liturgy of the Hours include references to Mary's immaculate conception in the feast of the Immaculate Conception. Its celebration seems to have begun in the Eastern church in the 7th century and may have spread to Ireland by the 8th, although the earliest well-attested record in the Western church is from England early in the 11th. It was suppressed there after the Norman Conquest (1066), and the first thorough exposition of the doctrine was a response to this suppression. It continued to spread through the 15th century despite accusations of heresy from the Thomists and strong objections from several prominent theologians. \nBeginning around 1140 Bernard of Clairvaux, a Cistercian monk, wrote to Lyons Cathedral to express his surprise and dissatisfaction that it had recently begun to be observed there, but in 1477 Pope Sixtus IV, a Franciscan Scotist and devoted Immaculist, placed it on the Roman calendar (i.e., list of church festivals and observances) via the bull \"Cum praexcelsa\". Thereafter in 1481 and 1483, in response to the polemic writings of the prominent Thomist, Vincenzo Bandello, Pope Sixtus IV published two more bulls which forbade anybody to preach or teach against the Immaculate Conception, or for either side to accuse the other of heresy, on pains of excommunication. \nPope Pius V kept the feast on the tridentine calendar which was issued in 1568. Gregory XV in 1622 prohibited any public or private assertion that Mary was conceived in sin. Urban VIII in 1624 allowed the Franciscans to establish a military order dedicated to the Virgin of the Immaculate Conception. Following the promulgation of \"Ineffabilis Deus\" the typically Franciscan phrase \"immaculate conception\" reasserted itself in the title and euchology (prayer formulae) of the feast. Pius IX solemnly promulgated a mass formulary drawn chiefly from one composed 400 years by a papal chamberlain at the behest of Sixtus IV, beginning \"O God who by the Immaculate Conception of the Virgin\".\nPrayers and hymns.\nThe Roman Rite liturgical books, including the Roman Missal and the Liturgy of the Hours, included offices venerating Mary's immaculate conception on the feast of the Immaculate Conception. An example is the antiphon that begins: \"Tota pulchra es, Maria, et macula originalis non est in te\" (\"You are all beautiful, Mary, and the original stain [of sin] is not in you\". It continues: \"Your clothing is white as snow, and your face is like the sun. You are all beautiful, Mary, and the original stain [of sin] is not in you. You are the glory of Jerusalem, you are the joy of Israel, you give honour to our people. You are all beautiful, Mary\".) On the basis of the original Gregorian chant music, polyphonic settings have been composed by Anton Bruckner, Pablo Casals, Maurice Durufl\u00e9, Grzegorz Gerwazy Gorczycki, Ola Gjeilo, Jos\u00e9 Maur\u00edcio Nunes Garcia, and Nikolaus Schapfl.\nOther prayers honouring Mary's immaculate conception are in use outside the formal liturgy. The Immaculata prayer, composed by Maximillian Kolbe, is a prayer of entrustment to Mary as the Immaculata. A novena of prayers, with a specific prayer for each of the nine days has been composed under the title of the .\nAve Maris Stella is the vesper hymn of the feast of the Immaculate Conception. The hymn \"Immaculate Mary\", addressed to Mary as the Immaculately Conceived One, is closely associated with Lourdes.\nThe \"Loreto Litanies\" included the official Latin Marian title of \"Regina sine labe originali concepta\" (Queen conceived without original sin), which had been granted by Pope Gregory XVI (1831-1846) from 1839 onwards to some dioceses, thus several years before the proclamation of the dogma.\nArtistic representation.\nThe Immaculate Conception became a popular subject in literature, but its abstract nature meant it was late in appearing as a subject in art. During the Medieval period it was depicted as \"Joachim and Anne Meeting at the Golden Gate\", meaning Mary's conception through the chaste kiss of her parents at the Golden Gate in Jerusalem; the 14th and 15th centuries were the heyday for this scene, after which it was gradually replaced by more allegorical depictions featuring an adult Mary.\nThe definitive iconography for the depiction of \"Our Lady of the Immaculate Conception\" seems to have been finally established by the painter and theorist Francisco Pacheco in his \"El arte de la pintura\" of 1649: a beautiful young girl of 12 or 13, wearing a white tunic and blue mantle, rays of light emanating from her head ringed by twelve stars and crowned by an imperial crown, the Sun behind her and the Moon beneath her feet. Pacheco's iconography influenced other Spanish artists or artists active in Spain such as El Greco, Bartolom\u00e9 Murillo, Diego Vel\u00e1zquez, and Francisco Zurbar\u00e1n, who each produced a number of artistic masterpieces based on the use of these same symbols. The popularity of this particular representation of \"The Immaculate Conception\" spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The Moon is under her feet and a halo of twelve stars surround her head, possibly a reference to \"a woman clothed with the sun\" from . Additional imagery may include clouds, a golden light, and putti. In some paintings the putti are holding lilies and roses, flowers often associated with Mary.\nOther denominations.\nOriental Orthodoxy.\nThe Ethiopian Orthodox Tewahedo and Eritrean Orthodox Tewahedo Churches believe in the Immaculate Conception of the Theotokos. The Ethiopic phrase used to express that the Blessed Virgin Mary is free from original sin is \"\u1218\u122d\u1308\u1218 \u1235\u130b \u1218\u122d\u1308\u1218 \u1290\u134d\u1235 \u12e8\u120c\u1208\u1263\u1275\". The Ethiopian Orthodox Tewahedo Church celebrates the Feast of the Immaculate Conception on Nehasie 7 (August 13).\nThe proposition of Ethiopian Orthodox reads: \"\"Our Lady, the Virgin Mary, who conceived and gave birth to Jesus Christ in virginity is free from the original sin derived from the descendants of Adam, clean from any sins of the flesh and soul; embedded in the conscience of God before the time of her birth, free and protected from human desires and frailties, and the choicest from among the chosen. Such is the Virgin Mary - Pure and Holy of Holies\". (Song 4.7)\". This is synodal statement.\nEastern Orthodoxy.\nEastern Orthodoxy never accepted Augustine's specific ideas on original sin, and in consequence did not become involved in the later developments that took place in the Catholic Church, including the Immaculate Conception, although Eastern Orthodoxy affirms Mary's purity and preservation from sin.\nIn 1894, when Pope Leo XIII addressed the Eastern church in his encyclical \"Praeclara gratulationis\", Ecumenical Patriarch Anthimos, in 1895, replied with an encyclical approved by the Constantinopolitan Synod in which he stigmatised the dogmas of the Immaculate Conception and papal infallibility as \"Roman novelties\" and called on the Roman church to return to the faith of the early centuries. Eastern Orthodox Bishop Kallistos Ware comments that \"the Latin dogma seems to us not so much erroneous as superfluous\".\nOld Catholics.\nIn the mid-19th century, some Catholics who were unable to accept the doctrine of papal infallibility left the Roman Church and formed the Old Catholic Church. This movement rejects the Immaculate Conception.\nProtestantism.\nProtestants overwhelmingly condemned the promulgation of \"Ineffabilis Deus\" as an exercise in papal power, and the doctrine itself as unscriptural, for it denied that all had sinned and rested on the Latin translation of Luke 1:28 (the \"full of grace\" passage) that the original Greek did not support. Protestants, therefore, teach that Mary was a sinner saved through grace, like all believers.\nThe Catholic\u2013Lutheran dialogue's statement \"The One Mediator, the Saints, and Mary\", issued in 1990 after seven years of study and discussion, conceded that Lutherans and Catholics remained separated \"by differing views on matters such as the invocation of saints, the Immaculate Conception and the Assumption of Mary\"; the final report of the Anglican\u2013Roman Catholic International Commission (ARCIC), created in 1969 to further ecumenical progress between the Catholic Church and the Anglican Communion, similarly recorded the disagreement of the Anglicans with the doctrine, although Anglo-Catholics may hold the Immaculate Conception as an optional pious belief.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15257", "revid": "19277499", "url": "https://en.wikipedia.org/wiki?curid=15257", "title": "Scilly Isles", "text": ""}
{"id": "15260", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=15260", "title": "Islands of the North Atlantic", "text": "Acronym for the British Isles\nIONA (Islands of the North Atlantic) is an acronym suggested in 1980 by Sir John Biggs-Davison to refer to a loose linkage of the Channel Islands (Guernsey and Jersey), Great Britain (England, Scotland, and Wales), Ireland (Northern Ireland and the Republic of Ireland), and the Isle of Man, similar to the present day British\u2013Irish Council. Its intended purpose was as a more politically acceptable alternative to the British Isles, which is disliked by some people in Ireland.\nThe neologism has been criticised on the grounds that it excludes most of the islands in the North Atlantic, and also that the only island referred to by the term that is actually in the North Atlantic Ocean is Ireland (Great Britain is in fact in between the Irish Sea and The North Sea). In the context of the Northern Irish peace process, during the negotiation of the Good Friday Agreement, IONA was unsuccessfully proposed as a neutral name for the proposed council.\nOne feature of this name is that IONA has the same spelling as the island of Iona which is off the coast of Scotland, but with which Irish people have strong cultural associations. It is therefore a name with which people of both main islands might identify. Taoiseach Bertie Ahern noted the symbolism in a 2006 address in Edinburgh:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIn a D\u00e1il \u00c9ireann debate, Proinsias De Rossa was less enthusiastic:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The acronym IONA is a useful way of addressing the coming together of these two islands. However, the island of Iona is probably a green heaven in that nobody lives on it and therefore it cannot be polluted in any way.\nThe term IONA is used by the World Universities Debating Championship. IONA is one of the regions that appoint a representative onto the committee of the World Universities Debating Council. Greenland, the Faroe Islands and Iceland are included in the definition of IONA used in this context, while Newfoundland and Prince Edward Island are in the North American region. However, none of these islands have yet participated in the World Universities Debating Championships. Otherwise, the term has achieved very little popular usage in any context.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15261", "revid": "473593", "url": "https://en.wikipedia.org/wiki?curid=15261", "title": "Intel DX4", "text": "4th generation x86 CPU\nIntelDX4 is a clock-tripled i486 microprocessor with 16\u00a0KB level\u00a01 cache. Intel named it DX4 (rather than \"DX3\") as a consequence of litigation with Advanced Micro Devices over trademarks. The product was officially named IntelDX4, but OEMs continued using the i486 naming convention.\nIntel produced IntelDX4s with two clock speed steppings: A 75-MHz version (3\u00d7 25\u00a0MHz multiplier), and a 100-MHz version (3\u00d7 33.3\u00a0MHz or 2x 50\u00a0MHz). Both chips were released in March 1994. A version of IntelDX4 featuring write-back cache was released in October 1994. The original write-through versions of the chip are marked with a laser-embossed \u201c&amp;E,\u201d while the write-back-enabled versions are marked \u201c&amp;EW.\u201d i486 OverDrive editions of IntelDX4 had locked multipliers, and therefore can only run at 3\u00d7 the external clock speed. The 100-MHz model of the processor had an iCOMP rating of 435, while the 75-MHz processor had a rating of 319. IntelDX4 was an OEM-only product, but the DX4 Overdrive could be purchased at a retail store.\nThe IntelDX4 microprocessor is mostly pin-compatible with the i486, but requires a lower 3.3-V supply. Normal i486DX and DX2 processors use a 5-V supply; plugging a DX4 into an unmodified socket will destroy the processor. Motherboards lacking support for the 3.3-V CPUs can sometimes make use of them using a voltage regulator module (VRM) that fits between the socket and the CPU. The i486 DX4 OverDrive CPUs have voltage regulator modules built in.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15263", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=15263", "title": "Intel 80486DX", "text": ""}
{"id": "15264", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=15264", "title": "Iapetus (disambiguation)", "text": "Iapetus (; also \"Japetus\", \"Iapetos\") is a Titan god in Greek mythology.\nIapetus or Japetus, may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15266", "revid": "1318605612", "url": "https://en.wikipedia.org/wiki?curid=15266", "title": "Interactive Fiction Competition", "text": "Annual competition for interactive fiction\nThe Interactive Fiction Competition (also known as IFComp) is one of several annual competitions for works of interactive fiction. It has been held since 1995. It is intended for fairly short games, as judges are only allowed to spend two hours playing a game before deciding how many points to award it, but longer games are allowed entry. The competition has been described as the \"Super Bowl\" of interactive fiction.\nSince 2016 it is operated by the Interactive Fiction Technology Foundation (IFTF).\nOrganization.\nIn 2016, operation of the competition was taken over by the Interactive Fiction Technology Foundation.\nThe lead organizer 2014\u20132017 was Jason McIntosh, and in 2018 it was Jacqueline Ashwell.\nCategories.\nAlthough the first competition had separate sections for Inform and TADS games, subsequent competitions have not been divided into sections and are open to games produced by any method, provided that the software used to play the game is freely available.\nIn addition to the main competition, the entries take part in the Miss Congeniality contest, where the participating authors vote for three games (not including their own). This was started in 1998 to distribute that year's surplus prizes; this additional contest has remained unchanged since then, even without the original reason for its existence.\nThere is also a 'Golden Banana of Discord' side contest; the distinction is given to the entry with scores with the highest standard deviation.\nEligibility.\nThe competition differs from the XYZZY Awards, as authors must specifically submit games to the Interactive Fiction Competition, but all games released in the past year are eligible for the XYZZY Awards. Many games win awards in both competitions.\nJudging.\nAnyone can judge the games. Because anyone can judge and participate in the competition, there is a rule that \"All entries must cost nothing for judges to play\".\nRules.\nThe competition has rules for judges, authors and everyone to ensure that everyone agrees on the purpose, scope, and spirit of the competition.\nPrizes.\nAnyone can donate a prize. Almost always, there are enough prizes donated that anyone who enters will get one.\nWinners.\nThe following is a list of first place winners to date:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOnly two competitors have won more than once: Paul O'Brian, winning in 2002 and 2004, and Steph Cherrywell, winning in 2015 and 2019.\nReception.\nA reviewer for \"The A.V. Club\" said of the 2008 competition, \"Once again, the IF Competition delivers some of the best writing in games.\" The 2008 competition was described as containing \"some real standouts both in quality of puzzles and a willingness to stretch the definition of text adventures/interactive fiction.\""}
{"id": "15267", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=15267", "title": "Immunity", "text": "Immunity may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15268", "revid": "48536272", "url": "https://en.wikipedia.org/wiki?curid=15268", "title": "Inquests in England and Wales", "text": "Judicial inquiry of unexplained deaths in England and Wales\nInquests in England and Wales are held into sudden or unexplained deaths and also into the circumstances of and discovery of a certain class of valuable artefacts known as \"treasure trove\". In England and Wales, inquests are the responsibility of a coroner, who operates under the jurisdiction of the Coroners and Justice Act 2009. In some circumstances where an inquest cannot view or hear all the evidence, it may be suspended and a public inquiry held with the consent of the Home Secretary.\nWhere an inquest is needed.\nThere is a general duty upon every person to report a death to the coroner if an inquest is likely to be required. However, this duty is largely unenforceable in practice and the duty falls on the responsible registrar. The registrar must report a death where:\nThe coroner must hold an inquest where the death is:\nWhere the cause of death is unknown, the coroner may order a post mortem examination in order to determine whether the death was violent. If the death is found to be non-violent, an inquest is unnecessary.\nIn 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury. In 2014 the Royal College of Pathologists claimed that up to 10,000 deaths a year recorded as being from natural causes should have been investigated by inquests. They were particularly concerned about people whose death occurred as a result of medical errors. \"We believe a medical examiner would have been alerted to what was going on in Mid-Staffordshire long before this long list of avoidable deaths reached the total it did,\" said Archie Prentice, the pathologists' president.\nJuries.\nA coroner must summon a jury for an inquest if the death was not a result of natural causes and occurred when the deceased was in state custody (for example in prison, police custody, or whilst detained under the Mental Health Act 1983); or if it was the result of an act or omission of a police officer; or if it was a result of a notifiable accident, poisoning or disease. The senior coroner can also call a jury at his or her own discretion. This discretion has been heavily litigated in light of the Human Rights Act 1998, which means that juries are required now in a broader range of situations than expressly required by statute.\nScope of inquest.\nThe purpose of the inquest is to answer four questions:\nEvidence must be solely for the purpose of answering these questions and no other evidence is admitted. It is not for the inquest to ascertain \"how the deceased died\" or \"in what broad circumstances\", but \"how the deceased came by his death\", a more limited question. Moreover, it is not the purpose of the inquest to determine, or appear to determine, criminal or civil liability, to apportion guilt or attribute blame. For example, where a prisoner hanged himself in a cell, he came by his death by hanging and it was not the role of the inquest to enquire into the broader circumstances such as the alleged neglect of the prison authorities that might have contributed to his state of mind or given him the opportunity. However, the inquest should set out as many of the facts as the public interest requires.\nUnder the terms of article 2 of the European Convention of Human Rights, governments are required to \"establish a framework of laws, precautions, procedures and means of enforcement which will, to the greatest extent reasonably practicable, protect life\". The European Court of Human Rights has interpreted this as mandating independent official investigation of any death where public servants may be implicated. Since the Human Rights Act 1998 came into force, in those cases alone, the inquest is now to consider the broader question \"by what means and in what circumstances\".\nIn disasters, such as the 1987 King's Cross fire, a single inquest may be held into several deaths. Some inquests, such as the John Lawler inquest, result in a prevention of future deaths report.\nProcedure.\nInquests are governed by the Coroners Rules. The coroner gives notice to near relatives, those entitled to examine witnesses and those whose conduct is likely to be scrutinised. Inquests are held in public except where there are real issues and substantial of national security but only the portions which relate to national security will be held behind closed doors.\nIndividuals with an interest in the proceedings, such as relatives of the deceased, individuals appearing as witnesses, and organisations or individuals who may face some responsibility in the death of the individual, may be represented by a legal professional be that a solicitor or barrister at the discretion of the coroner. Witnesses may be compelled to testify subject to the privilege against self-incrimination.\nIf there are matters of national security or matters which relate to sensitive matters then under Schedule 1 of the Coroners and Justice Act 2009 an inquest may be suspended and replaced by a public inquiry under s.2 of the Inquiries Act 2005. This can only be ordered by the Home Secretary and must be announced to Parliament with the coroner in charge being informed and the next of kin being informed. The next of kin and coroner can appeal the decision of the Home Secretary.\nVerdict or conclusions.\nThe following conclusions (formerly called verdicts) are not mandatory but are strongly recommended:\nIn 2004, 37% of inquests recorded an outcome of death by accident / misadventure, 21% by natural causes, 13% suicide, 10% open verdicts, and 19% other outcomes.\nSince 2004 it has been possible for the coroner to record a narrative verdict, recording the circumstances of a death without apportioning blame or liability. Since 2009, other possible verdicts have included \"alcohol/drug related death\" and \"road traffic collision\". The civil standard of proof, on the balance of probabilities, is used for all conclusions. The standard of proof for suicide and unlawful killing changed in 2018 from beyond all reasonable doubt to the balance of probabilities following a case in the courts of appeal.\nModernisation.\nOwing in particular to the failures to notice the serial murder committed by Harold Shipman, the Coroners and Justice Act 2009 modernised the system with:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15270", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=15270", "title": "Index", "text": "Index (pl.: indexes or indices) may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15271", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=15271", "title": "Information retrieval", "text": "Finding information for an information need\nInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Cross-modal retrieval implies retrieval across modalities.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications.\nOverview.\nAn information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance.\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.\nDepending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.\nHistory.\n&lt;templatestyles src=\"Rquote/styles.css\"/&gt;{ class=\"rquote pullquote floatright\" role=\"presentation\" style=\"display:table; border-collapse:collapse; border-style:none; float:right; margin:0.5em 0.75em; width:33%; \"\nThe idea of using computers to search for relevant pieces of information was popularized in the article \"As We May Think\" by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' \u2013 filed by Emanuel Goldberg in the 1920s and 1930s \u2013 that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy \"Desk Set\". In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\nBy the late 1990s, the rise of the World Wide Web fundamentally transformed information retrieval. While early search engines such as AltaVista (1995) and Yahoo! (1994) offered keyword-based retrieval, they were limited in scale and ranking refinement. The breakthrough came in 1998 with the founding of Google, which introduced the PageRank algorithm, using the web's hyperlink structure to assess page importance and improve relevance ranking.\nDuring the 2000s, web search systems evolved rapidly with the integration of machine learning techniques. These systems began to incorporate user behavior data (e.g., click-through logs), query reformulation, and content-based signals to improve search accuracy and personalization. In 2009, Microsoft launched Bing, introducing features that would later incorporate semantic web technologies through the development of its Satori knowledge base. Academic analysis have highlighted Bing's semantic capabilities, including structured data use and entity recognition, as part of a broader industry shift toward improving search relevance and understanding user intent through natural language processing.\nA major leap occurred in 2018, when Google deployed BERT (Bidirectional Encoder Representations from Transformers) to better understand the contextual meaning of queries and documents. This marked one of the first times deep neural language models were used at scale in real-world retrieval systems. BERT's bidirectional training enabled a more refined comprehension of word relationships in context, improving the handling of natural language queries. Because of its success, transformer-based models gained traction in academic research and commercial search applications.\nSimultaneously, the research community began exploring neural ranking models that outperformed traditional lexical-based methods. Long-standing benchmarks such as the Text REtrieval Conference (TREC), initiated in 1992, and more recent evaluation frameworks Microsoft MARCO(MAchine Reading COmprehension) (2019) became central to training and evaluating retrieval systems across multiple tasks and domains. MS MARCO has also been adopted in the TREC Deep Learning Tracks, where it serves as a core dataset for evaluating advances in neural ranking models within a standardized benchmarking environment.\nAs deep learning became integral to information retrieval systems, researchers began to categorize neural approaches into three broad classes: sparse, dense, and hybrid models. Sparse models, including traditional term-based methods and learned variants like SPLADE, rely on interpretable representations and inverted indexes to enable efficient exact term matching with added semantic signals. Dense models, such as dual-encoder architectures like ColBERT, use continuous vector embeddings to support semantic similarity beyond keyword overlap. Hybrid models aim to combine the advantages of both, balancing the lexical (token) precision of sparse methods with the semantic depth of dense models. This way of categorizing models balances scalability, relevance, and efficiency in retrieval systems.\nAs IR systems increasingly rely on deep learning, concerns around bias, fairness, and explainability have also come to the picture. Research is now focused not just on relevance and efficiency, but on transparency, accountability, and user trust in retrieval algorithms.\nApplications.\nAreas where information retrieval techniques are employed include (the entries are in alphabetical order within each category):\nOther retrieval methods.\nMethods/Techniques in which information retrieval techniques are employed include:\nModel types.\nIn order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\nThird Dimension: representational approach-based classification.\nIn addition to the theoretical distinctions, modern information retrieval models are also categorized on how queries and documents are represented and compared, using a practical classification distinguishing between sparse, dense and hybrid models.\nThis classification has become increasingly common in both academic and the real world applications and is getting widely adopted and used in evaluation benchmarks for Information Retrieval models.\nPerformance and correctness measures.\nThe evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15272", "revid": "6051258", "url": "https://en.wikipedia.org/wiki?curid=15272", "title": "List of Italian-language poets", "text": "List of poets who wrote in Italian (or Italian dialects).\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\n * See also* References\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15273", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=15273", "title": "ICTY", "text": ""}
{"id": "15274", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=15274", "title": "International Criminal Tribunal for the former Yugoslavia", "text": "1993\u20132017 Netherlands-based United Nations ad hoc court\nThe International Criminal Tribunal for the former Yugoslavia (ICTY) was an \"ad hoc\" court of the United Nations that was established to prosecute the war crimes that had been committed during the Yugoslav Wars and to try their perpetrators. The tribunal was located in The Hague, Netherlands and operated between 1993 and 2017.\nIt was established by Resolution 827 of the United Nations Security Council, which was passed on 25 May 1993. It had jurisdiction over four clusters of crimes committed on the territory of the former Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crimes against humanity. The maximum sentence that it could impose was life imprisonment. Various countries signed agreements with the United Nations to carry out custodial sentences.\nA total of 161 persons were indicted; the final indictments were issued in December 2004, the last of which were confirmed and unsealed in the spring of 2005. The final fugitive, Goran Had\u017ei\u0107, was arrested on 20 July 2011. The final judgment was issued on 29 November 2017 and the institution formally ceased to exist on 31 December 2017.\nResidual functions of the ICTY, including the oversight of sentences and consideration of any appeal proceedings initiated since 1 July 2013, are under the jurisdiction of a successor body, the International Residual Mechanism for Criminal Tribunals (IRMCT).\nHistory.\nCreation.\nUnited Nations Security Council Resolution 808 of 22 February 1993 decided that an \"international tribunal shall be established for the prosecution of persons responsible for serious violations of international humanitarian law committed in the territory of the former Yugoslavia since 1991\", and called on the Secretary-General to \"submit for consideration by the Council ... a report on all aspects of this matter, including specific proposals and where appropriate options ... taking into account suggestions put forward in this regard by Member States\".\nThe court was originally proposed by German Foreign Minister Klaus Kinkel.\nResolution 827 of 25 May 1993 approved the http:// of the secretary-general and adopted the Statute of the International Tribunal annexed to it, formally creating the ICTY. It was to have jurisdiction over four clusters of crimes committed on the territory of the former SFR Yugoslavia since 1991:\nThe maximum sentence the ICTY could impose for these crimes was life imprisonment.\nImplementation.\nIn 1993 the internal infrastructure of the ICTY was built. 17 states had signed an agreement with the ICTY to carry out custodial sentences.\n1993\u20131994: In the first year of its existence, the tribunal laid the foundations for its existence as a judicial organ. It established the legal framework for its operations by adopting the rules of procedure and evidence, as well as its rules of detention and directive for the assignment of defence counsel. Together, these rules established a legal aid system for the tribunal. As the ICTY was a part of the United Nations and was the first \"international\" court for \"criminal\" justice, the development of a juridical infrastructure was considered quite a challenge. However, after the first year, the first ICTY judges had drafted and adopted all the rules for court proceedings.\n1994\u20131995: The ICTY established its offices within the Aegon Insurance Building in The Hague (which was, at the time, still partially in use by Aegon) and detention facilities in Scheveningen in The Hague (the Netherlands). The ICTY hired many staff members and by July 1994, the Office of the Prosecutor had sufficient staff to begin field investigations. By November 1994, the first indictments were presented to the court and confirmed, and in 1995, the staff numbered over 200 persons from all over the world.\nOperation.\nIn 1994 the first indictment was issued against the Bosnian-Serb concentration camp commander Dragan Nikoli\u0107. This was followed on 13 February 1995 by two indictments comprising 21 individuals which were issued against a group of 21 Bosnian-Serbs charged with committing atrocities against Muslim and Croat civilian prisoners. While the war in the former Yugoslavia was still raging, the ICTY prosecutors showed that an international court was viable. However, no accused was arrested.\nThe court confirmed eight indictments against 46 individuals and issued arrest warrants. Bosnian Serb indictee Du\u0161ko Tadi\u0107 became the subject of the tribunal's first trial. Tadi\u0107 was arrested by German police in Munich in 1994 for his alleged actions in the Prijedor region in Bosnia-Herzegovina (especially his actions in the Omarska, Trnopolje and Keraterm detention camps). He made his first appearance before the ICTY Trial Chamber on 26 April 1995, and pleaded not guilty to all of the charges in the indictment.\n1995\u20131996: Between June 1995 and June 1996, 10 public indictments had been confirmed against a total of 33 individuals. Six of the newly indicted persons were transferred in the tribunal's detention unit. In addition to Du\u0161ko Tadic, by June 1996 the tribunal had Tihomir Bla\u0161ki\u0107, Dra\u017een Erdemovi\u0107, Zejnil Delali\u0107, Zdravko Muci\u0107, Esad Land\u017eo and Hazim Deli\u0107 in custody. Erdemovi\u0107 became the first person to enter a guilty plea before the tribunal's court. Between 1995 and 1996, the ICTY dealt with miscellaneous cases involving several detainees, which never reached the trial stage.\nIndictees and accomplishments.\nThe tribunal indicted 161 individuals between 1997 and 2004 and completed proceedings with them as follows:\nThe indictees ranged from common soldiers to generals and police commanders all the way to prime ministers. Slobodan Milo\u0161evi\u0107 was the first sitting head of state indicted for war crimes. Other \"high level\" indictees included Milan Babi\u0107, former president of the Republika Srpska Krajina; Ramush Haradinaj, former Prime Minister of Kosovo; Radovan Karad\u017ei\u0107, former President of the Republika Srpska; Ratko Mladi\u0107, former Commander of the Bosnian Serb Army; and Ante Gotovina (acquitted), former General of the Croatian Army.\nThe very first hearing at the ICTY was a referral request in the Tadi\u0107 case on 8 November 1994. Croat Serb General and former president of the Republic of Serbian Krajina Goran Had\u017ei\u0107 was the last fugitive wanted by the tribunal to be arrested on 20 July 2011.\nAn additional 23 individuals have been the subject of contempt proceedings.\nIn 2004, the ICTY published a list of five accomplishments \"in justice and law\":\nClosure.\nThe United Nations Security Council passed resolutions 1503 in August 2003 and 1534 in March 2004, which both called for the completion of all cases at both the ICTY and its sister tribunal, the International Criminal Tribunal for Rwanda (ICTR) by 2010.\nIn December 2010, the Security Council adopted Resolution 1966, which established the International Residual Mechanism for Criminal Tribunals (IRMCT), a body intended to gradually assume residual functions from both the ICTY and the ICTR as they wound down their mandate. Resolution 1966 called upon the tribunal to finish its work by 31 December 2014 to prepare for its closure and the transfer of its responsibilities.\nIn a \"Completion Strategy Report\" issued in May 2011, the ICTY indicated that it aimed to complete all trials by the end of 2012 and complete all appeals by 2015, with the exception of Radovan Karad\u017ei\u0107 whose trial was expected to end in 2014 and Ratko Mladi\u0107 and Goran Had\u017ei\u0107, who were still at large at that time and were not arrested until later that year.\nThe IRMCT's ICTY branch began functioning on 1 July 2013. Per the Transitional Arrangements adopted by the UN Security Council, the ICTY was to conduct and complete all outstanding first-instance trials, including those of Karad\u017ei\u0107, Mladi\u0107 and Had\u017ei\u0107. The ICTY would also conduct and complete all appeal proceedings for which the notice of appeal against the judgement or sentence was filed before 1 July 2013. The IRMCT will handle any appeals for which notice is filed after that date.\nThe final ICTY trial to be completed in the first instance was that of Ratko Mladi\u0107, who was convicted on 22 November 2017. The final case to be considered by the ICTY was an appeal proceeding encompassing six individuals, whose sentences were upheld on 29 November 2017.\nOrganization.\nWhile operating, the tribunal employed around 900 staff. Its organizational components were Chambers, Registry and the Office of the Prosecutor (OTP).\nProsecutors.\nThe Prosecutor was responsible for investigating crimes, gathering evidence and prosecutions and was head of the Office of the Prosecutor (OTP). The Prosecutor was appointed by the UN Security Council upon nomination by the UN secretary-general.\nThe last prosecutor was Serge Brammertz. Previous Prosecutors have been Ram\u00f3n Escovar Salom of Venezuela (1993\u20131994), however, he never took up that office, Richard Goldstone of South Africa (1994\u20131996), Louise Arbour of Canada (1996\u20131999), and Carla Del Ponte of Switzerland (1999\u20132007). Richard Goldstone, Louise Arbour and Carla Del Ponte also simultaneously served as the Prosecutor of the International Criminal Tribunal for Rwanda until 2003. Graham Blewitt of Australia served as the Deputy Prosecutor from 1994 until 2004. David Tolbert, the president of the International Center for Transitional Justice, was also appointed Deputy Prosecutor of the ICTY in 2004.\nChambers.\nChambers encompassed the judges and their aides. The tribunal operated three Trial Chambers and one Appeals Chamber. The president of the tribunal was also the presiding judge of the Appeals Chamber.\nJudges.\nAt the time of the court's dissolution, there were seven permanent judges and one \"ad hoc\" judge who served on the tribunal. A total of 86 judges have been appointed to the tribunal from 52 United Nations member states. Of those judges, 51 were permanent judges, 36 were \"ad litem\" judges, and one was an \"ad hoc\" judge. Note that one judge served as both a permanent and \"ad litem\" judge, and another served as both a permanent and \"ad hoc\" judge.\nUN member and observer states could each submit up to two nominees of different nationalities to the UN secretary-general. The UN secretary-general submitted this list to the UN Security Council which selected from 28 to 42 nominees and submitted these nominees to the UN General Assembly. The UN General Assembly then elected 14 judges from that list. Judges served for four years and were eligible for re-election. The UN secretary-general appointed replacements in case of vacancy for the remainder of the term of office concerned.\nOn 21 October 2015, Judge Carmel Agius of Malta was elected president of the ICTY and Liu Daqun of China was elected vice-president; they assumed their positions on 17 November 2015. Agius's predecessors were Antonio Cassese of Italy (1993\u20131997), Gabrielle Kirk McDonald of the United States (1997\u20131999), Claude Jorda of France (1999\u20132002), Theodor Meron of the United States (2002\u20132005), Fausto Pocar of Italy (2005\u20132008), Patrick Robinson of Jamaica (2008\u20132011), and Theodor Meron (2011\u20132015).\nRegistry.\nThe Registry was responsible for handling the administration of the tribunal; activities included keeping court records, translating court documents, transporting and accommodating those who appear to testify, operating the Public Information Section, and such general duties as payroll administration, personnel management and procurement. It was also responsible for the Detention Unit for indictees being held during their trial and the Legal Aid program for indictees who cannot pay for their own defence. The Registry was headed by the Registrar, a position occupied over the years by Theo van Boven of the Netherlands (February 1994 to December 1994), Doroth\u00e9e de Sampayo Garrido-Nijgh of the Netherlands (1995\u20132000), Hans Holthuis of the Netherlands (2001\u20132008), and John Hocking of Australia (January 2009 to December 2017).\nDetention facilities.\nThose defendants on trial and those who were denied a provisional release were detained at the United Nations Detention Unit on the premises of the Penitentiary Institution Haaglanden, location Scheveningen in Belgisch Park, a suburb of The Hague, located some 3\u00a0km by road from the courthouse. The indicted were housed in private cells which had a toilet, shower, radio, satellite TV, personal computer (without internet access) and other luxuries. They were allowed to phone family and friends daily and could have conjugal visits. There was also a library, a gym and various rooms used for religious observances. The inmates were allowed to cook for themselves. All of the inmates mixed freely and were not segregated on the basis of nationality. As the cells were more akin to a university residence instead of a jail, some had derisively referred to the ICT as the \"Hague Hilton\". The reason for this luxury relative to other prisons is that the first president of the court wanted to emphasise that the indictees were innocent until proven guilty.\nCriticism.\nThe critique concerning the ICTY can be divided into claims pertaining to 6 main sections:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15275", "revid": "6046731", "url": "https://en.wikipedia.org/wiki?curid=15275", "title": "ISO 216", "text": "International standard for paper sizes, including A4\nISO 216 is an international standard for paper sizes, used around the world except in North America and parts of Latin America. The standard defines the \"A\", \"B\" and \"C\" series of paper sizes, which includes the A4, the most commonly available paper size worldwide. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 \"C\" series is commonly listed alongside the A and B sizes.\nAll ISO 216, ISO 217 and ISO 269 paper sizes (except some envelopes) have the same aspect ratio, , within rounding to millimetres. This ratio has the unique property that when cut or folded in half widthways, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size in the same series.\nHistory.\nThe oldest known mention of the advantages of basing a paper size on an aspect ratio of formula_1 is found in a letter written on 25 October 1786 by the German scientist Georg Christoph Lichtenberg to Johann Beckmann, both at the University of G\u00f6ttingen. Early variants of the formats that would become ISO paper sizes A2, A3, B3, B4, and B5 then evolved in France, where they were listed in a 1798 French law on taxation of publications () that was based in part on page sizes.\nSearching for a standard system of paper formats on a scientific basis at the Bridge association (), as a replacement for the vast variety of other paper formats that had been used before, in order to make paper stocking and document reproduction cheaper and more efficient, in 1911 Wilhelm Ostwald proposed, over a hundred years after the 1798 French law, a global standard\u00a0\u2013 a world format ()\u00a0\u2013 for paper sizes based on the ratio formula_1, referring to the argument advanced by Lichtenberg's 1786 letter, but linking this to the metric system using as the width of the base format. Walter Porstmann argued in a long article published in 1918, that a firm basis for the system of paper formats, which deal with surfaces, ought not be the length but the area; that is, linking the system of paper formats to the metric system using the square metre rather than the centimetre, constrained by formula_3 and area formula_4 square metre, where formula_5 is the length of the shorter side and formula_6 is the length of the longer side, for the second equation both in metres. Porstmann also argued that formats for \"containers\" of paper, such as envelopes, should be 10% larger than the paper format itself.\nIn 1921, after a long discussion and another intervention by Porstmann, the Standardisation Committee of German Industry (, or NADI in short), which is the German Institute for Standardisation (, or DIN in short) today, published German standard \"DI Norm 476\" the specification of four series of paper formats with ratio formula_1, with series A as the always preferred formats and basis for the other series. All measures are rounded to the nearest millimetre. A0 has a surface area of up to a rounding error, with a width of and height of , so an actual area of ; A4 is recommended as standard paper size for business, administrative and government correspondence; and A6 for postcards. Series B is based on B0 with width of , C0 is , and D0 . Series C is the basis for envelope formats.\nThe DIN paper-format concept was soon introduced as a national standard in many other countries, for example, Belgium (1924), Netherlands (1925), Norway (1926), Switzerland (1929), Sweden (1930), Soviet Union (1934), Hungary (1938), Italy (1939), Finland (1942), Uruguay (1942), Argentina (1943), Brazil (1943), Spain (1947), Austria (1948), Romania (1949), Japan (1951), Denmark (1953), Czechoslovakia (1953), Israel (1954), Portugal (1954), Yugoslavia (1956), India (1957), Poland (1957), United Kingdom (1959), Venezuela (1962), New Zealand (1963), Iceland (1964), Mexico (1965), South Africa (1966), France (1967), Peru (1967), Turkey (1967), Chile (1968), Greece (1970), Zimbabwe (1970), Singapore (1970), Bangladesh (1972), Thailand (1973), Barbados (1973), Australia (1974), Ecuador (1974), Colombia (1975) and Kuwait (1975).\nIt finally became both an international standard (ISO 216) as well as the official United Nations document format in 1975, and it is today used in almost all countries in the world, with the exception of several countries in the Americas.\nIn 1977, a large German car manufacturer performed a study of the paper formats found in their incoming mail and concluded that out of 148 examined countries, 88 already used the A series formats.\nAdvantages.\nThe main advantage of this system is its scaling. Rectangular paper with an aspect ratio of formula_1 has the unique property that, when cut in two across the midpoints of the longer sides, each half has the same formula_1 aspect ratio as the whole sheet before it was divided. Equivalently, if one lays two same-sized sheets of paper with an aspect ratio of formula_1 side by side along their longer side, they form a larger rectangle with the aspect ratio of formula_1 and double the area of each individual sheet.\nThe ISO system of paper sizes exploits these properties of the formula_1 aspect ratio. In each series of sizes (for example, series A), the largest size is numbered 0 (so in this case A0), and each successive size (A1, A2, etc.) has half the area of the preceding sheet and can be cut by halving the length of the preceding size sheet. The new measurement is rounded down to the nearest millimetre. A folded brochure can be made by using a sheet of the next larger size (for example, an A4 sheet is folded in half to make a brochure with size A5 pages). An office photocopier or printer can be designed to reduce a page from A4 to A5 or to enlarge a page from A4 to A3. Similarly, two sheets of A4 can be scaled down to fit one A4 sheet without excess empty paper.\nThis system also simplifies calculating the weight of paper. Under ISO 536, paper's grammage is defined as a sheet's mass in grams (g) per area in square metres (unit symbol g/m2; the nonstandard abbreviation \"gsm\" is also used). One can derive the weight of other sizes by arithmetic division. A standard A4 sheet made from 80 g/m2 paper weighs , as it is &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204416 (four halvings, ignoring rounding) of an A0 page. Thus the weight, and the associated postage rate, can be approximated easily by counting the number of sheets used.\nISO 216 and its related standards were first published between 1975 and 1995:\nProperties.\nA series.\nPaper in the A series format has an aspect ratio of (\u2248 1.414, when rounded). A0 is defined so that it has an area of before rounding to the nearest . Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the area of the preceding paper size and rounding down, so that the long side of A(\"n\" + 1) is the same length as the short side of A\"n\". Hence, each next size is nearly exactly half the area of the prior size. So, an A1 page can fit two A2 pages inside the same area.\nThe most used of this series is the size A4, which is and thus almost exactly in area. For comparison, the letter paper size commonly used in North America () is about 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4. Then, the size of A5 paper is half of A4, i.e. 148 mm \u00d7 210 mm (5.8 in \u00d7 8.3 in).\nThe geometric rationale for using the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A-series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, \"x\", and a shorter side, \"y\", ensuring that its aspect ratio, , will be the same as that of a rectangle half its size, , which means that =, which reduces to = \u221a2; in other words, an aspect ratio of .\nAny A\"n\" paper can be defined as A\"n\" = \"S\" \u00d7 \"L\", where (measuring in metres)\nformula_13\nTherefore\nformula_14\nformula_15\nformula_16\netc.\nB series.\nThe B series is defined in the standard as follows: \"A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence.\" The use of the geometric mean makes each step in size: B0, A0, B1, A1, B2 ... smaller than the previous one by the same factor. As with the A series, the lengths of the B series have the ratio , and folding one in half (and rounding down to the nearest millimetre) gives the next in the series. The shorter side of B0 is exactly 1 metre.\nThere is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are \u2248 1.22 times those of A-series paper. By comparison, the lengths of ISO B series paper are \u2248 1.19 times those of A-series paper (and \u2248 1.41 times the area).\nAny B\"n\" paper (according to the ISO standard) can be defined as B\"n\" = \"S\" \u00d7 \"L\", where (measuring in metres)\nformula_17\nTherefore\nformula_18\nformula_19\nformula_20\netc.\nC series.\nThe C series formats are geometric means between the B series and A series formats with the same number (e.g. C2 is the geometric mean between B2 and A2). The width to height ratio of C series formats is as in the A and B series. A, B, and C series of paper fit together as part of a geometric progression, with ratio of successive side lengths of , though there is no size half-way between B\"n\" and A(\"n\" \u2212 1): A4, C4, B4, \"D4\", A3, ...; there is such a D-series in the Swedish extensions to the system. The lengths of ISO C series paper are therefore \u2248 1.09 times those of A-series paper.\nThe C series formats are used mainly for envelopes. An unfolded A4 page will fit into a C4 envelope. Due to same width to height ratio, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half).\nAny C\"n\" paper can be defined as C\"n\" = \"S\" \u00d7 \"L\", where (measuring in metres)\nformula_21\nTherefore\nformula_22\nformula_23\nformula_24\netc.\nTolerances.\nThe tolerances specified in the standard are:\nThese are related to comparison between series A, B and C.\nApplication.\nThe ISO 216 formats are organized around the ratio 1:; two sheets next to each other together have the same ratio, sideways. \nIn scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet; in each case, there is neither waste nor want.\nThe principal countries not generally using the ISO paper sizes are the United States and Canada, which use North American paper sizes. Although many Latin American countries have also officially adopted the ISO 216 paper format, Mexico, Panama, Peru, Colombia, the Philippines, and Chile also use mostly U.S. paper sizes.\nRectangular sheets of paper with the ratio 1: are popular in paper folding, such as origami, where they are sometimes called \"A4 rectangles\" or \"silver rectangles\". In other contexts, the term \"silver rectangle\" can also refer to a rectangle in the proportion 1:(1\u00a0+\u00a0), known as the silver ratio.\nMatching technical pen widths.\nAn adjunct to the ISO paper sizes, particularly the A series, are the technical drawing line widths specified in ISO 128. For example, line type A (\"Continuous - thick\", used for \"visible outlines\") has a standard thickness of 0.7\u00a0mm on an A0-sized sheet, 0.5\u00a0mm on an A1 sheet, and 0.35\u00a0mm on A2, A3, or A4.\nThe matching technical pen widths are 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.40, and 2.0\u00a0mm, as specified in ISO 9175-1. Colour codes are assigned to each size to facilitate easy recognition by the drafter. Like the paper sizes, these pen widths increase by a factor of , so that particular pens can be used on particular sizes of paper, and then the next smaller or larger size can be used to continue the drawing after it has been reduced or enlarged, respectively.\n The earlier DIN 6775 standard upon which ISO 9175-1 is based also specified a term and symbol for easy identification of pens and drawing templates compatible with the standard, called \"Micronorm\", which may still be found on some technical drafting equipment.\nOverformats.\nDIN 476 provides for formats larger than A0, denoted by a prefix factor. In particular, it lists the formats 2A0 and 4A0, which are twice and four times the size of A0 respectively:\nWhile not formally defined, ISO 216:2007 notes them in the table of \"Main series of trimmed sizes\" (ISO A series) as well: \"The rarely used sizes [2A0 and 4A0] which follow also belong to this series.\" 2A0 is also known by other unofficial names like \"A00\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15276", "revid": "43007828", "url": "https://en.wikipedia.org/wiki?curid=15276", "title": "ISO 3864", "text": "Technical standard for safety symbols\nISO 3864 is an International Organization for Standardization technical standard for safety signs and markings in workplaces and public facilities. These labels are graphical, to overcome language barriers. The standard is split into four parts.\nParts.\nISO 3864 consists of four parts, that provide more specific and situation specific guidance depending on the application.\nPart 1 explains how to layout the components of safety signage, dictate the color scheme and sizing information. Part 2 covers the same concepts as part one, but specifically for labels applied on machinery, vehicles and consumer goods. Part 3 contains guidance for designing new safety symbols. Part 4 specifies the standards for phosphorescent material and colours of a sign, as well as testing to confirm these signs meets required standards.\nComponents of ISO 3864.\nColours.\nThese are the colours specified in ISO Standard 3864-4 in RAL colour standard.\nIn addition, ISO standard 3864-2:2016 lays out the following colours that correspond to levels of risk. This standard adds \"Orange\" as an incremental colour to the pallette above.\nArrows.\nISO 3864-3 defines four types of arrow designs, and specifies what situations each type should be used in.\nSafety markings.\nPart 1 also provides design standards for 'safety markings', which are safety colors combined with a contrasting color in an alternating 45\u00b0 stripe pattern, intended to increase the visibility of an object, location or safety message.\nSignage design.\nIn addition to prescribing colours for safety signage, ISO 3864 also specifies how to layout the elements of the sign: A symbol and optional 'supplemental sign' which contains the supplementary text message.\nMulti-message signs.\nFor situations where more than one message needs to be communicated, ISO 3864 also provides guidance for \"multiple signs\", which consist of two or more symbol and text messages combined into a single sign. Additionally, fire protection and safe condition signs, which mark the location of equipment or exits can be combined with an arrow to indicate the direction to the item depicted on the sign.\nRelated standards.\nThe corresponding American standard is ANSI Z535. ANSI Z535.1 also explicitly uses multiple levels of hazard, including Yellow (Pantone 109) for 'caution' messages, and Orange (Pantone 151) for stronger 'warning' messages. Like ISO 3864, ANSI Z535 includes multiple sections: ANSI Z535.6-2006 defines an optional accompanying text in one or more languages.\nISO 3864 is extended by ISO 7010, which provides a set of symbols based on the principles and properties specified in ISO 3864.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15281", "revid": "48889163", "url": "https://en.wikipedia.org/wiki?curid=15281", "title": "Isaac Abendana", "text": "Spanish-born Jewish scholar (died 1699)\nIsaac Abendana (c.\u20091640\u20131699) was a Spanish-born Jewish scholar, translator, and religious leader who became a pioneering figure in Hebrew studies at English universities during the Restoration period. Born into a Marrano family that had been forced to convert from Judaism to Christianity during the Spanish Inquisition, he was the younger brother of Jacob Abendana and the grandson of David Abendana, one of the founders of Amsterdam's first synagogue. He became \"hakam\" of the Spanish Portuguese Synagogue in London after his brother died.\nEarly life.\nIsaac Abendana was born in Spain around 1640. He was the younger brother of Jacob Abendana. Abendana arrived in England in 1662 before his brother, making him one of the first practicing Jews to teach at English universities since the medieval expulsion of 1290. \nAcademic career.\nFrom 1663, Isaac taught Hebrew at both Cambridge and Oxford universities, receiving an annual retaining fee of \u00a36 from Trinity College, Cambridge during 1664-66. His most significant scholarly achievement was producing the first complete Latin translation of the Mishnah in 1671, a six-volume manuscript work commissioned by Cambridge University that remained unpublished but was later used by Christian scholar Guilielmus Surenhusius for his own Latin edition. While he was at Cambridge, Abendana sold Hebrew books to the Bodleian Library of Oxford. After relocating to Oxford in 1689, Abendana taught Hebrew at Magdalen College.\nReligious leadership.\nFollowing his brother Jacob's death in 1695, Isaac served as hakham (chief rabbi) of London's Spanish and Portuguese Synagogue until his own death on July 17, 1699. \nScholarly work and legacy.\nAt Oxford, Isaac compiled annual Jewish calendars (almanacs) for Christian readers from 1692 to 1699, which he later republished as Discourses on the Ecclesiastical and Civil Polity of the Jews (1706)\u2014one of the first comprehensive explanations of Judaism written in English. He maintained extensive correspondence with leading Christian scholars, including with the philosopher Ralph Cudworth, master of Christ's College, Cambridge. Abendana contributed significantly to Jewish-Christian intellectual dialogue during a formative period in Anglo-Jewish history.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15284", "revid": "173030", "url": "https://en.wikipedia.org/wiki?curid=15284", "title": "List of intelligence agencies", "text": "&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nThis is a list of intelligence agencies by country. It includes only currently operational institutions which are in the public domain. The list is not intended to be exhaustive.\nAn intelligence agency is a government agency responsible for the collection, analysis, and exploitation of information in support of law enforcement, national security, military, and foreign policy objectives.\nPeople's Republic of China.\nCentral Committee of the Chinese Communist Party (CCCPC)\nPeople's Liberation Army (PLA)\nCroatia.\nInternal and Foreign Intelligence\nMilitary Intelligence\nCuba.\nCommunist Party of Cuba (PCC)\nMinistry of the FAR (MINFAR)\nMinistry of the Interior\nIndia.\nCabinet Secretariat\nMinistry of Home Affairs\nMinistry of Defence\nMinistry of Finance\nMinistry of Communications\nIraq.\nPrime Minister's Office\nMinistry of Defence\nMinistry of Interior\nIreland.\nForeign &amp; Domestic Military Intelligence (Defence Forces)\nDomestic Police Intelligence (\"Garda S\u00edoch\u00e1na\")\nItaly.\nDipartimento delle Informazioni per la Sicurezza (DIS) - Department of Information for Security\nPakistan.\nNational Intelligence Coordination Committee (NICC)\nPalestine.\nOffice of the President of Palestine\nMinistry of Interior\nSerbia.\nCivil Intelligence\nMilitary Intelligence\nSingapore.\nMinistry of Defense (MINDEF)\nMinistry of Home Affairs (MHA)\nUnited Kingdom.\nDomestic intelligence\nForeign intelligence\nSignals intelligence\nCriminal Intelligence and Protected Persons"}
{"id": "15285", "revid": "25213224", "url": "https://en.wikipedia.org/wiki?curid=15285", "title": "Internet Engineering Task Force", "text": "Open internet standards organization\nThe Internet Engineering Task Force (IETF) is a standards organization for the Internet and is responsible for the technical standards that make up the Internet protocol suite (TCP/IP). It has no formal membership roster or requirements and all its participants are volunteers. Their work is usually funded by employers or other sponsors.\nThe IETF was initially supported by the federal government of the United States but since 1993 has operated under the auspices of the Internet Society, a non-profit organization with local chapters around the world.\nOrganization.\nThere is no membership in the IETF. Anyone can participate by signing up to a working group mailing list, or registering for an IETF meeting.\nThe process for developing IETF standards is open and all-inclusive. Anyone can participate by joining a working group and attending meetings. Each working group normally has appointed two co-chairs (occasionally three); a charter that describes its focus; and what it is expected to produce, and when. It is open to all who want to participate and holds discussions on an open mailing list. Working groups hold open sessions at IETF meetings, where the onsite registration fee in 2024 was between (early registration) and $1200 per person for the week. Significant discounts are available for students and remote participants. As working groups do not make decisions at IETF meetings, with all decisions taken later on the working group mailing list, meeting attendance is not required for contributors.\nRough consensus is the primary basis for decision making. There are no formal voting procedures. Each working group is intended to complete work on its topic and then disband. In some cases, the working group will instead have its charter updated to take on new tasks as appropriate.\nThe working groups are grouped into areas by subject matter. Each area is overseen by an area director (AD), with most areas having two ADs. The ADs are responsible for appointing working group chairs. The area directors, together with the IETF Chair, form the Internet Engineering Steering Group (IESG), which is responsible for the overall operation of the IETF.\nThe Internet Architecture Board (IAB) oversees the IETF's external relationships. The IAB provides long-range technical direction for Internet development. The IAB also manages the Internet Research Task Force (IRTF), with which the IETF has a number of cross-group relations.\nA nominating committee (NomCom) of ten randomly chosen volunteers who participate regularly at meetings, a non-voting chair and 4-5 liaisons, is vested with the power to appoint, reappoint, and remove members of the IESG, IAB, IETF Trust and the IETF LLC. To date, no one has been removed by a NomCom, although several people have resigned their positions, requiring replacements.\nIn 1993 the IETF changed from an activity supported by the US federal government to an independent, international activity associated with the Internet Society, a US-based 501(c)(3) organization. In 2018 the Internet Society created a subsidiary, the IETF Administration LLC, to be the corporate, legal and financial home for the IETF. IETF activities are funded by meeting fees, meeting sponsors and by the Internet Society via its organizational membership and the proceeds of the Public Interest Registry.\nIn December 2005, the IETF Trust was established to manage the copyrighted materials produced by the IETF.\nSteering group.\nThe Internet Engineering Steering Group (IESG) is a body composed of the Internet Engineering Task Force (IETF) chair and area directors. It provides the final technical review of Internet standards and is responsible for day-to-day management of the IETF. It receives appeals of the decisions of the working groups, and the IESG makes the decision to progress documents in the standards track.\nThe chair of the IESG is the area director of the general area, who also serves as the overall IETF chair. Members of the IESG include the two directors, sometimes three, of each of the following areas:\nLiaison and \"ex officio\" members include:\nEarly leadership and administrative history.\nThe Gateway Algorithms and Data Structures (GADS) Task Force was the precursor to the IETF. Its chairman was David L. Mills of the University of Delaware.\nIn January 1986, the Internet Activities Board (IAB; now called the Internet Architecture Board) decided to divide GADS into two entities: an Internet Architecture (INARC) Task Force chaired by Mills to pursue research goals, and the IETF to handle nearer-term engineering and technology transfer issues. The first IETF chair was Mike Corrigan, who was then the technical program manager for the Defense Data Network (DDN). Also in 1986, after leaving DARPA, Robert E. Kahn founded the Corporation for National Research Initiatives (CNRI), which began providing administrative support to the IETF.\nIn 1987, Corrigan was succeeded as IETF chair by Phill Gross.\nEffective March 1, 1989, but providing support dating back to late 1988, CNRI and NSF entered into a cooperative agreement, No. NCR-8820945, wherein CNRI agreed to create and provide a \"secretariat\" for the \"overall coordination, management and support of the work of the IAB, its various task forces and, particularly, the IETF\".\nIn 1992, CNRI supported the formation and early funding of the Internet Society, which took on the IETF as a fiscally sponsored project, along with the IAB, the IRTF, and the organization of annual INET meetings. Gross continued to serve as IETF chair throughout this transition. Cerf, Kahn, and Lyman Chapin announced the formation of ISOC as \"a professional society to facilitate, support, and promote the evolution and growth of the Internet as a global research communications infrastructure\". At the first board meeting of the Internet Society, Cerf, representing CNRI, offered, \"In the event a deficit occurs, CNRI has agreed to contribute up to USD$102,000 to offset it.\" In 1993, Cerf continued to support the formation of ISOC while working for CNRI, and the role of ISOC in \"the official procedures for creating and documenting Internet Standards\" was codified in the IETF's .\nIn 1995, IETF's describes ISOC's role in the IETF as being purely administrative, and ISOC as having \"no influence whatsoever on the Internet Standards process, the Internet Standards or their technical content\".\nIn 1998, CNRI established Foretec Seminars, Inc. (Foretec), a for-profit subsidiary to take over providing secretariat services to the IETF. Foretec provided these services until at least 2004. By 2013, Foretec was dissolved.\nIn 2003, IETF's described IETFs role in appointing three board members to the ISOC's board of directors.\nIn 2018, ISOC established The IETF Administration LLC, a separate LLC to handle the administration of the IETF. In 2019, the LLC issued a call for proposals to provide secretariat services to the IETF.\nMeetings.\nThe first IETF meeting was attended by 21 US federal government-funded researchers on 16 January 1986. It was a continuation of the work of the earlier GADS Task Force. Representatives from non-governmental entities (such as gateway vendors) were invited to attend starting with the fourth IETF meeting in October 1986. Since that time all IETF meetings have been open to the public.\nInitially, the IETF met quarterly, but from 1991, it has been meeting three times a year. The initial meetings were very small, with fewer than 35 people in attendance at each of the first five meetings. The maximum attendance during the first 13 meetings was only 120 attendees. This occurred at the twelfth meeting, held during January 1989. These meetings have grown in both participation and scope a great deal since the early 1990s; it had a maximum attendance of 2810 at the December 2000 IETF held in San Diego, California. Attendance declined with industry restructuring during the early 2000s, and is currently around 1200.\nThe locations for IETF meetings vary greatly. A list of past and future meeting locations is on the IETF meetings page. The IETF strives to hold its meetings near where most of the IETF volunteers are located. IETF meetings are held three times a year, with one meeting each in Asia, Europe and North America. An occasional exploratory meeting is held outside of those regions in place of one of the other regions.\nThe IETF also organizes hackathons during the IETF meetings. The focus is on implementing code that will improve standards in terms of quality and interoperability.\nBirds of a feather.\nIn an IETF context, BoF (birds of a feather) can refer to:\nThe first use of this term among computer specialists is uncertain, but it was employed during DECUS conferences and may have been used at SHARE user group meetings in the 1960s.\nBoFs can facilitate networking and partnership formation among subgroups, including functionally oriented groups such as CEOs or geographically oriented groups. BoFs generally allow for more audience interaction than the panel discussions typically seen at conventions; the discussions are not completely unguided, though, as there is still a discussion leader.\nThe term is derived from the proverb \"birds of a feather flock together\". The (idiomatic) phrase \"birds of a feather\" meaning \"people having similar characters, backgrounds, interests, or beliefs\". In old poetic English, \"birds of a feather\" means birds which have the same kind of feathers, so the proverb refers to the fact that birds congregate with birds of their own species.\nOperations.\nThe details of IETF operations have changed considerably as the organization has grown, but the basic mechanism remains publication of proposed specifications, development based on the proposals, review and independent testing by participants, and republication as a revised proposal, a draft proposal, or eventually as an Internet Standard. IETF standards are developed in an open, all-inclusive process in which any interested individual can participate. All IETF documents are freely available over the Internet and can be reproduced at will. Multiple, working, useful, interoperable implementations are the chief requirement before an IETF proposed specification can become a standard. Most specifications are focused on single protocols rather than tightly interlocked systems. This has allowed the protocols to be used in many different systems, and its standards are routinely re-used by bodies which create full-fledged architectures (e.g. 3GPP IMS).\nBecause it relies on volunteers and uses \"rough consensus and running code\" as its touchstone, results can be slow whenever the number of volunteers is either too small to make progress, or so large as to make consensus difficult, or when volunteers lack the necessary expertise. For protocols like SMTP, which is used to transport e-mail for a user community in the many hundreds of millions, there is also considerable resistance to any change that is not fully backward compatible, except for IPv6. Work within the IETF on ways to improve the speed of the standards-making process is ongoing but, because the number of volunteers with opinions on it is very great, consensus on improvements has been slow to develop.\nThe IETF cooperates with the W3C, ISO/IEC, ITU, and other standards bodies.\nStatistics are available that show who the top contributors by RFC publication are. While the IETF only allows for participation by individuals, and not by corporations or governments, sponsorship information is available from these statistics.\nCritisism.\nIn 2025, criticism intensified within the IETF that intelligence agencies were exerting undue influence, sparking concerns on a lack of transparency in the decision-making process.\nChairs.\nThe IETF chairperson is selected by the NomCom process for a two-year renewable term. Before 1993, the IETF Chair was selected by the IAB.\nA list of the past and current chairs of the IETF:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nTopics of interest.\nThe IETF works on a broad range of networking technologies which provide foundation for the Internet's growth and evolution.\nAutomated network management.\nIt aims to improve the efficiency in management of networks as they grow in size and complexity. The IETF is also standardizing protocols for autonomic networking that enables networks to be self managing.\nInternet of things.\nIt is a network of physical objects or things that are embedded with electronics, sensors, software and also enables objects to exchange data with operator, manufacturer and other connected devices. Several IETF working groups are developing protocols that are directly relevant to IoT.\nNew transport technology.\nIts development provides the ability of internet applications to send data over the Internet. There are some well-established transport protocols such as TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) which are continuously getting extended and refined to meet the needs of the global Internet.\nGeneric Security Service API.\nThe IETF defines a \"Generic Security Services Application Programming Interface\" (GSSAPI) which provides security services to callers in a generic fashion. \nAmong these are various implementations. Java provides these features in its standard library package codice_1.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15286", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=15286", "title": "ISM radio band", "text": "Radio frequency allocations\nThe ISM radio bands are portions of the radio spectrum reserved internationally for \"industrial, scientific and medical\" (ISM) purposes, excluding applications in telecommunications. \nExamples of applications for the use of radio frequency (RF) energy in these bands include RF heating, microwave ovens, and medical diathermy machines. The powerful emissions of these devices can create electromagnetic interference and disrupt radio communication using the same frequency, so these devices are limited to certain bands of frequencies. In general, communications equipment operating in ISM bands must tolerate any interference generated by ISM applications, and users have no regulatory protection from ISM device operation in these bands.\nDespite the intent of the original allocations, in recent years the fastest-growing use of these bands has been for short-range, low-power wireless communications systems, since these bands are often approved for such devices, which can be used without a government license, as would otherwise be required for transmitters; ISM frequencies are often chosen for this purpose as they already must tolerate interference issues. Cordless phones, Bluetooth devices, near-field communication (NFC) devices, garage door openers, baby monitors, and wireless computer networks (Wi-Fi) may all use the ISM frequencies, although these low-power transmitters are not considered to be ISM devices.\nDefinition.\nThe ISM bands are defined by the ITU Radio Regulations (article 5) in footnotes 5.138, 5.150, and 5.280 of the Radio Regulations. Individual countries' use of the bands designated in these sections may differ due to variations in national radio regulations. Because communication devices using the ISM bands must tolerate any interference from ISM equipment, unlicensed operations are typically permitted to use these bands, since unlicensed operation typically needs to be tolerant of interference from other devices anyway. The ISM bands share allocations with unlicensed and licensed operations; however, due to the high likelihood of harmful interference, licensed use of the bands is typically low. In the United States, uses of the ISM bands are governed by https:// of the Federal Communications Commission (FCC) rules, while Part 15 contains the rules for unlicensed communication devices, even those that share ISM frequencies. In Europe, the ETSI develops standards for the use of short-range devices, some of which operate in ISM bands. The use of the ISM bands is regulated by the national spectrum regulation authorities that are members of the CEPT.\nFrequency allocations.\nThe allocation of radio frequencies is provided according to \"Article 5\" of the ITU Radio Regulations (edition 2012).\nIn order to improve harmonisation in spectrum utilisation, the majority of service allocations stipulated in this document were incorporated in national tables of frequency allocations and utilisations which are within the responsibilities of the appropriate national administrations. The allocation might be primary, secondary, exclusive, or shared. Exclusive or shared utilization is within the responsibility of administrations.\nType A (footnote 5.138) = frequency bands are designated for \"ISM applications\". The use of these frequency bands for ISM applications shall be subject to special authorization by the administration concerned, in agreement with other administrations whose radiocommunication services might be affected. In applying this provision, administrations shall have due regard to the latest relevant ITU-R Recommendations.\nType B (footnote 5.150) = frequency bands are also designated for ISM applications. Radiocommunication services operating within these bands must accept harmful interference which may be caused by these applications.\nITU RR, (Footnote 5.280) = In Germany, Austria, Bosnia and Herzegovina, Croatia, North Macedonia, Liechtenstein, Montenegro, Portugal, Serbia, Slovenia and Switzerland, the band 433.05\u2013434.79\u00a0MHz (center frequency 433.92\u00a0MHz) is designated for \"ISM applications\". Radio communication services of these countries operating within this band must accept harmful interference which may be caused by these applications.\nHistory.\nThe ISM bands were first established at the International Telecommunications Conference of the ITU in Atlantic City, 1947. The American delegation specifically proposed several bands, including the now commonplace 2.4\u00a0GHz band, to accommodate the then nascent process of microwave heating; however, FCC annual reports of that time suggest that much preparation was done ahead of these presentations.\nThe report of the August 9th 1947 meeting of the Allocation of Frequencies committee includes the remark:\n\"The delegate of the United States, referring to his request that the frequency 2450 Mc/s be allocated for I.S.M., indicated that there was in existence in the United States, and working on this frequency a diathermy machine and an electronic cooker, and that the latter might eventually be installed in transatlantic ships and airplanes. There was therefore some point in attempting to reach world agreement on this subject.\"\nRadio frequencies in the ISM bands have been used for communication purposes, although such devices may experience interference from non-communication sources. In the United States, as early as 1958 Class D Citizens Band, a Part 95 service, was allocated to frequencies that are also allocated to ISM. [1]\nIn the U.S., the FCC first made unlicensed spread spectrum available in the ISM bands in rules adopted on May 9, 1985. The FCC action was proposed by Michael Marcus of the FCC staff in 1980 and the subsequent regulatory action took five more years. It was part of a broader proposal to allow civil use of spread spectrum technology and was opposed at the time by mainstream equipment manufacturers and many radio system operators.\nMany other countries later developed similar regulations, enabling use of this technology.\nApplications.\n Industrial, scientific and medical (ISM) applications (of radio frequency energy) (short: ISM applications) are \u2013 according to \"article 1.15\" of the International Telecommunication Union's (ITU) ITU Radio Regulations (RR) \u2013 defined as \u00ab\"Operation of equipment or appliances designed to generate and use locally radio frequency energy for industrial, scientific, medical, domestic or similar purposes, excluding applications in the field of telecommunications\".\u00bb\nThe original ISM specifications envisioned that the bands would be used primarily for noncommunication purposes, such as heating. The bands are still widely used for these purposes. For many people, the most commonly encountered ISM device is the home microwave oven operating at 2.45\u00a0GHz which uses microwaves to cook food. Industrial heating is another big application area; such as induction heating, microwave heat treating, plastic softening, and plastic welding processes. In medical settings, shortwave and microwave diathermy machines use radio waves in the ISM bands to apply deep heating to the body for relaxation and healing. More recently hyperthermia therapy uses microwaves to heat tissue to kill cancer cells.\nHowever, as detailed below, the increasing congestion of the radio spectrum, the increasing sophistication of microelectronics, and the attraction of unlicensed use, has in recent decades led to an explosion of uses of these bands for short range communication systems for wireless devices, which are now by far the largest uses of these bands. These are sometimes called \"non ISM\" uses since they do not fall under the originally envisioned \"industrial\", \"scientific\", and \"medical\" application areas. One of the largest applications has been wireless networking (Wi-Fi). The IEEE 802.11 wireless networking protocols, the standards on which almost all wireless systems are based, use the ISM bands. Virtually all laptops, tablet computers, computer printers and cellphones now have 802.11 wireless modems using the 2.4 and 5.7\u00a0GHz ISM bands. Bluetooth is another networking technology using the 2.4\u00a0GHz band, which can be problematic given the probability of interference. Near-field communication (NFC) devices such as proximity cards and contactless smart cards use the lower-frequency 13 and 27\u00a0MHz ISM bands. Other short-range devices using the ISM bands are: wireless microphones, baby monitors, garage door openers, wireless doorbells, keyless entry systems for vehicles, radio control channels for UAVs (drones), wireless surveillance systems, RFID systems for merchandise, and wild animal tracking systems.\nSome electrodeless lamp designs are ISM devices, which use RF emissions to excite fluorescent tubes. Sulfur lamps are commercially available plasma lamps, which use 2.45\u00a0GHz magnetrons to heat sulfur into a brightly glowing plasma.\nLong-distance wireless power systems have been proposed and experimented with which would use high-power transmitters and rectennas, in lieu of overhead transmission lines and underground cables, to send power to remote locations. NASA has studied using microwave power transmission on 2.45\u00a0GHz to send energy collected by solar power satellites back to the ground.\nAlso in space applications, a helicon double-layer ion thruster is a prototype spacecraft propulsion engine which uses a 13.56\u00a0MHz transmission to break down and heat gas into plasma.\nCommon non-ISM uses.\nIn recent years ISM bands have also been shared with (non-ISM) license-free error-tolerant communications applications such as wireless sensor networks in the 915\u00a0MHz and 2.450\u00a0GHz bands, as well as wireless LANs and cordless phones in the 915\u00a0MHz, 2.450\u00a0GHz, and 5.800\u00a0GHz bands. Because unlicensed devices are required to be tolerant of ISM emissions in these bands, unlicensed low-power users are generally able to operate in these bands without causing problems for ISM users. ISM equipment does not necessarily include a radio receiver in the ISM band (e.g. a microwave oven does not have a receiver).\nIn the United States, according to 47 CFR Part 15.5, low power communication devices must accept interference from licensed users of that frequency band, and the Part 15 device must not cause interference to licensed users. Note that the 915\u00a0MHz band should not be used in countries outside Region 2, except those that specifically allow it, such as Australia and Israel, especially those that use the GSM-900 band for cellphones. The ISM bands are also widely used for radio-frequency identification (RFID) applications with the most commonly used band being the 13.56\u00a0MHz band used by systems compliant with ISO/IEC 14443 including those used by biometric passports and contactless smart cards.\nIn Europe, the use of the ISM band is covered by Short Range Device regulations issued by European Commission, based on technical recommendations by CEPT and standards by ETSI. In most of Europe, LPD433 band is allowed for license-free voice communication in addition to PMR446.\nWireless network devices use wavebands as follows:\nWireless LANs and cordless phones can also use bands other than those shared with ISM, but such uses require approval on a country by country basis. DECT phones use allocated spectrum outside the ISM bands that differs in Europe and North America. Ultra-wideband LANs require more spectrum than the ISM bands can provide, so the relevant standards such as IEEE 802.15.4a are designed to make use of spectrum outside the ISM bands. Despite the fact that these additional bands are outside the official ITU-R ISM bands, because they are used for the same types of low power personal communications, they are sometimes incorrectly referred to as ISM bands as well.\nSeveral brands of radio control equipment use the 2.4 GHz band range for low power remote control of toys, from gas powered cars to miniature aircraft.\nWorldwide Digital Cordless Telecommunications or WDCT is a technology that uses the 2.4 GHz radio spectrum.\nGoogle's Project Loon used ISM bands (specifically 2.4 and 5.8\u00a0GHz bands) for balloon-to-balloon and balloon-to-ground communications.\nPursuant to 47 CFR Part 97 some ISM bands are used by licensed amateur radio operators for communication\u00a0\u2013 including amateur television.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15287", "revid": "49685307", "url": "https://en.wikipedia.org/wiki?curid=15287", "title": "Series (mathematics)", "text": "Infinite sum\nIn mathematics, a series is, roughly speaking, an addition of infinitely many terms, one after the other. The study of series is a major part of calculus and its generalization, mathematical analysis. Series are used in most areas of mathematics, even for studying finite structures in combinatorics through generating functions. The mathematical properties of infinite series make them widely applicable in other quantitative disciplines such as physics, computer science, statistics and finance.\nAmong the Ancient Greeks, the idea that a potentially infinite summation could produce a finite result was considered paradoxical, most famously in Zeno's paradoxes. Nonetheless, infinite series were applied practically by Ancient Greek mathematicians including Archimedes, for instance in the quadrature of the parabola. The mathematical side of Zeno's paradoxes was resolved using the concept of a limit during the 17th century, especially through the early calculus of Isaac Newton. The resolution was made more rigorous and further improved in the 19th century through the work of Carl Friedrich Gauss and Augustin-Louis Cauchy, among others, answering questions about which of these sums exist via the completeness of the real numbers and whether series terms can be rearranged or not without changing their sums using absolute convergence and conditional convergence of series. \nIn modern terminology, any ordered infinite sequence formula_1 of terms, whether those terms are numbers, functions, matrices, or anything else that can be added, defines a series, which is the addition of the &amp;NoBreak;&amp;NoBreak; one after the other. To emphasize that there are an infinite number of terms, series are often also called infinite series to contrast with finite series, a term sometimes used for finite sums. Series are represented by an expression like\nformula_2\nor, using capital-sigma summation notation,\nformula_3\nThe infinite sequence of additions expressed by a series cannot be explicitly performed in sequence in a finite amount of time. However, if the terms and their finite sums belong to a set that has limits, it may be possible to assign a value to a series, called the sum of the series. This value is the limit as &amp;NoBreak;&amp;NoBreak; tends to infinity of the finite sums of the &amp;NoBreak;&amp;NoBreak; first terms of the series if the limit exists. These finite sums are called the &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;partial sums of the series. Using summation notation,\nformula_4\nif it exists. When the limit exists, the series is convergent or summable and also the sequence formula_1 is summable, and otherwise, when the limit does not exist, the series is divergent.\nThe expression formula_6 denotes both the series\u2014the implicit process of adding the terms one after the other indefinitely\u2014and, if the series is convergent, the sum of the series\u2014the explicit limit of the process. This is a generalization of the similar convention of denoting by formula_7 both the addition\u2014the process of adding\u2014and its result\u2014the \"sum\" of &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak;.\nCommonly, the terms of a series come from a ring, often the field formula_8 of the real numbers or the field formula_9 of the complex numbers. If so, the set of all series is also itself a ring, one in which the addition consists of adding series terms together term by term and the multiplication is the Cauchy product.\nDefinition.\nSeries.\nA \"series\" or, redundantly, an \"infinite series\", is an infinite sum. It is often represented as\nformula_10\nwhere the terms formula_11 are the members of a sequence of numbers, functions, or anything else that can be added. A series may also be represented with capital-sigma notation:\nformula_12\nIt is also common to express series using a few first terms, an ellipsis, a general term, and then a final ellipsis, the general term being an expression of the &amp;NoBreak;&amp;NoBreak;th term as a function of &amp;NoBreak;&amp;NoBreak;:\nformula_13\nFor example, Euler's number can be defined with the series\nformula_14 \nwhere formula_15 denotes the product of the formula_16 first positive integers, and formula_17 is conventionally equal to formula_18\nPartial sum of a series.\nGiven a series formula_19, its &amp;NoBreak;&amp;NoBreak;th \"partial sum\" is\nformula_20\nSome authors directly identify a series with its sequence of partial sums. Either the sequence of partial sums or the sequence of terms completely characterizes the series, and the sequence of terms can be recovered from the sequence of partial sums by taking the differences between consecutive elements,\nformula_21\nPartial summation of a sequence is an example of a linear sequence transformation, and it is also known as the prefix sum in computer science. The inverse transformation for recovering a sequence from its partial sums is the finite difference, another linear sequence transformation. \nPartial sums of series sometimes have simpler closed form expressions, for instance an arithmetic series has partial sums\nformula_22\nand a geometric series has partial sums\nformula_23\nif &amp;NoBreak;&amp;NoBreak; or simply &amp;NoBreak;&amp;NoBreak; if &amp;NoBreak;&amp;NoBreak;.\nSum of a series.\nStrictly speaking, a series is said to \"converge\", to be \"convergent\", or to be \"summable\" when the sequence of its partial sums has a limit. When the limit of the sequence of partial sums does not exist, the series \"diverges\" or is \"divergent\". When the limit of the partial sums exists, it is called the \"sum of the series\" or \"value of the series\":\nformula_24\nA series with only a finite number of nonzero terms is always convergent. Such series are useful for considering finite sums without taking care of the numbers of terms. When the sum exists, the difference between the sum of a series and its formula_16th partial sum, formula_26 is known as the formula_16th \"truncation error\" of the infinite series.\nAn example of a convergent series is the geometric series\nformula_28\nIt can be shown by algebraic computation that each partial sum formula_29 is \nformula_30 As one has \nformula_31\nthe series is convergent and converges to &amp;NoBreak;&amp;NoBreak; with truncation errors formula_32.\nBy contrast, the geometric series\nformula_33\nis divergent in the real numbers. However, it is convergent in the extended real number line, with formula_34 as its limit and formula_34 as its truncation error at every step.\nWhen a series's sequence of partial sums is not easily calculated and evaluated for convergence directly, convergence tests can be used to prove that the series converges or diverges.\nGrouping and rearranging terms.\nGrouping.\nIn ordinary finite summations, terms of the summation can be grouped and ungrouped freely without changing the result of the summation as a consequence of the associativity of addition. formula_36formula_37formula_38 Similarly, in a series, any finite groupings of terms of the series will not change the limit of the partial sums of the series and thus will not change the sum of the series. However, if an infinite number of groupings is performed in an infinite series, then the partial sums of the grouped series may have a different limit than the original series and different groupings may have different limits from one another; the sum of formula_39 may not equal the sum of formula_40formula_41 \nFor example, Grandi's series &amp;NoBreak;&amp;NoBreak; has a sequence of partial sums that alternates back and forth between &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; and does not converge. Grouping its elements in pairs creates the series formula_42formula_43 which has partial sums equal to zero at every term and thus sums to zero. Grouping its elements in pairs starting after the first creates the series formula_44formula_45formula_46 which has partial sums equal to one for every term and thus sums to one, a different result.\nIn general, grouping the terms of a series creates a new series with a sequence of partial sums that is a subsequence of the partial sums of the original series. This means that if the original series converges, so does the new series after grouping: all infinite subsequences of a convergent sequence also converge to the same limit. However, if the original series diverges, then the grouped series do not necessarily diverge, as in this example of Grandi's series above. However, divergence of a grouped series does imply the original series must be divergent, since it proves there is a subsequence of the partial sums of the original series which is not convergent, which would be impossible if it were convergent. This reasoning was applied in Oresme's proof of the divergence of the harmonic series, and it is the basis for the general Cauchy condensation test.\nRearrangement.\nIn ordinary finite summations, terms of the summation can be rearranged freely without changing the result of the summation as a consequence of the commutativity of addition. formula_36formula_48formula_49 Similarly, in a series, any finite rearrangements of terms of a series does not change the limit of the partial sums of the series and thus does not change the sum of the series: for any finite rearrangement, there will be some term after which the rearrangement did not affect any further terms: any effects of rearrangement can be isolated to the finite summation up to that term, and finite summations do not change under rearrangement.\nHowever, as for grouping, an infinitary rearrangement of terms of a series can sometimes lead to a change in the limit of the partial sums of the series. Series with sequences of partial sums that converge to a value but whose terms could be rearranged to a form a series with partial sums that converge to some other value are called conditionally convergent series. Those that converge to the same value regardless of rearrangement are called unconditionally convergent series.\nFor series of real numbers and complex numbers, a series formula_39 is unconditionally convergent if and only if the series summing the absolute values of its terms, formula_51 is also convergent, a property called absolute convergence. Otherwise, any series of real numbers or complex numbers that converges but does not converge absolutely is conditionally convergent. Any conditionally convergent sum of real numbers can be rearranged to yield any other real number as a limit, or to diverge. These claims are the content of the Riemann series theorem.\nA historically important example of conditional convergence is the alternating harmonic series,\nformula_52\nwhich has a sum of the natural logarithm of 2, while the sum of the absolute values of the terms is the harmonic series,\nformula_53\nwhich diverges per the divergence of the harmonic series, so the alternating harmonic series is conditionally convergent. For instance, rearranging the terms of the alternating harmonic series so that each positive term of the original series is followed by two negative terms of the original series rather than just one yields \nformula_54\nwhich is formula_55 times the original series, so it would have a sum of half of the natural logarithm of 2. By the Riemann series theorem, rearrangements of the alternating harmonic series to yield any other real number are also possible.\nOperations.\nSeries addition.\nThe addition of two series formula_56 and formula_57 is given by the termwise sum formula_58, or, in summation notation,\nformula_59 \nUsing the symbols formula_60 and formula_61 for the partial sums of the added series and formula_62 for the partial sums of the resulting series, this definition implies the partial sums of the resulting series follow formula_63 Then the sum of the resulting series, i.e., the limit of the sequence of partial sums of the resulting series, satisfies\nformula_64\nwhen the limits exist. Therefore, first, the series resulting from addition is summable if the series added were summable, and, second, the sum of the resulting series is the addition of the sums of the added series. The addition of two divergent series may yield a convergent series: for instance, the addition of a divergent series with a series of its terms times formula_65 will yield a series of all zeros that converges to zero. However, for any two series where one converges and the other diverges, the result of their addition diverges.\nFor series of real numbers or complex numbers, series addition is associative, commutative, and invertible. Therefore series addition gives the sets of convergent series of real numbers or complex numbers the structure of an abelian group and also gives the sets of all series of real numbers or complex numbers (regardless of convergence properties) the structure of an abelian group.\nScalar multiplication.\nThe product of a series formula_56 with a constant number formula_67, called a scalar in this context, is given by the termwise product formula_68, or, in summation notation,\nformula_69\nUsing the symbols formula_60 for the partial sums of the original series and formula_71 for the partial sums of the series after multiplication by formula_67, this definition implies that formula_73 for all formula_74 and therefore also formula_75when the limits exist. Therefore if a series is summable, any nonzero scalar multiple of the series is also summable and vice versa: if a series is divergent, then any nonzero scalar multiple of it is also divergent.\nScalar multiplication of real numbers and complex numbers is associative, commutative, invertible, and it distributes over series addition. \nIn summary, series addition and scalar multiplication gives the set of convergent series and the set of series of real numbers the structure of a real vector space. Similarly, one gets complex vector spaces for series and convergent series of complex numbers. All these vector spaces are infinite dimensional.\nSeries multiplication.\nThe multiplication of two series formula_76 and formula_77 to generate a third series formula_78, called the Cauchy product, can be written in summation notation\nformula_79\nwith each formula_80formula_81 Here, the convergence of the partial sums of the series formula_78 is not as simple to establish as for addition. However, if both series formula_76 and formula_77 are absolutely convergent series, then the series resulting from multiplying them also converges absolutely with a sum equal to the product of the two sums of the multiplied series,\nformula_85\nSeries multiplication of absolutely convergent series of real numbers and complex numbers is associative, commutative, and distributes over series addition. Together with series addition, series multiplication gives the sets of absolutely convergent series of real numbers or complex numbers the structure of a commutative ring, and together with scalar multiplication as well, the structure of a commutative algebra; these operations also give the sets of all series of real numbers or complex numbers the structure of an associative algebra.\nExamples of numerical series.\nPi.\nformula_106\nformula_107\nNatural logarithm of 2.\nformula_108\nformula_109\nNatural logarithm base e.\nformula_110\nformula_111\nConvergence testing.\nOne of the simplest tests for convergence of a series, applicable to all series, is the \"vanishing condition\" or \"&amp;NoBreak;&amp;NoBreak;th-term test\": If formula_112, then the series diverges; if formula_113, then the test is inconclusive.\nAbsolute convergence tests.\nWhen every term of a series is a non-negative real number, for instance when the terms are the absolute values of another series of real numbers or complex numbers, the sequence of partial sums is non-decreasing. Therefore a series with non-negative terms converges if and only if the sequence of partial sums is bounded, and so finding a bound for a series or for the absolute values of its terms is an effective way to prove convergence or absolute convergence of a series. \nFor example, the series formula_114is convergent and absolutely convergent because formula_115 for all formula_116 and a telescoping sum argument implies that the partial sums of the series of those non-negative bounding terms are themselves bounded above by 2. The exact value of this series is formula_117; see Basel problem.\nThis type of bounding strategy is the basis for general series comparison tests. First is the general \"direct comparison test\": For any series formula_118, If formula_119 is an absolutely convergent series such that formula_120 for some positive real number formula_121 and for sufficiently large formula_16, then formula_118 converges absolutely as well. If formula_124 diverges, and formula_125 for all sufficiently large formula_16, then formula_118 also fails to converge absolutely, although it could still be conditionally convergent, for example, if the formula_128 alternate in sign. Second is the general \"limit comparison test\": If formula_119 is an absolutely convergent series such that formula_130 for sufficiently large formula_16, then formula_118 converges absolutely as well. If formula_133 diverges, and formula_134 for all sufficiently large formula_16, then formula_118 also fails to converge absolutely, though it could still be conditionally convergent if the formula_128 vary in sign.\nUsing comparisons to geometric series specifically, those two general comparison tests imply two further common and generally useful tests for convergence of series with non-negative terms or for absolute convergence of series with general terms. First is the \"ratio test\": if there exists a constant formula_138 such that formula_139 for all sufficiently large\u00a0formula_16, then formula_141 converges absolutely. When the ratio is less than formula_142, but not less than a constant less than formula_142, convergence is possible but this test does not establish it. Second is the \"root test\": if there exists a constant formula_138 such that formula_145 for all sufficiently large\u00a0formula_16, then formula_141 converges absolutely.\nAlternatively, using comparisons to series representations of integrals specifically, one derives the \"integral test\": if formula_148 is a positive monotone decreasing function defined on the interval formula_149 then for a series with terms formula_150 for all\u00a0formula_16, formula_141 converges if and only if the integral formula_153 is finite. Using comparisons to flattened-out versions of a series leads to Cauchy's condensation test: if the sequence of terms formula_154 is non-negative and non-increasing, then the two series formula_141 and formula_156 are either both convergent or both divergent.\nConditional convergence tests.\nA series of real or complex numbers is said to be \"conditionally convergent\" (or \"semi-convergent\") if it is convergent but not absolutely convergent. Conditional convergence is tested for differently than absolute convergence.\nOne important example of a test for conditional convergence is the \"alternating series test\" or \"Leibniz test\": A series of the form formula_157 with all formula_158 is called \"alternating\". Such a series converges if the non-negative sequence formula_154 is monotone decreasing and converges to\u00a0formula_160. The converse is in general not true. A famous example of an application of this test is the alternating harmonic series\nformula_52\nwhich is convergent per the alternating series test (and its sum is equal to\u00a0formula_162), though the series formed by taking the absolute value of each term is the ordinary harmonic series, which is divergent.\nThe alternating series test can be viewed as a special case of the more general \"Dirichlet's test\": if formula_163 is a sequence of terms of decreasing nonnegative real numbers that converges to zero, and formula_164 is a sequence of terms with bounded partial sums, then the series formula_165 converges. Taking formula_166 recovers the alternating series test.\n\"Abel's test\" is another important technique for handling semi-convergent series. If a series has the form formula_167 where the partial sums of the series with terms formula_168, formula_169 are bounded, formula_170 has bounded variation, and formula_171 exists: if formula_172 formula_173 and formula_174converges, then the series formula_141 is convergent.\nOther specialized convergence tests for specific types of series include the Dini test for Fourier series.\nEvaluation of truncation errors.\nThe evaluation of truncation errors of series is important in numerical analysis (especially validated numerics and computer-assisted proof). It can be used to prove convergence and to analyze rates of convergence.\nAlternating series.\nWhen conditions of the alternating series test are satisfied by formula_176, there is an exact error evaluation. Set formula_29 to be the partial sum formula_178 of the given alternating series formula_179. Then the next inequality holds:\nformula_180\nHypergeometric series.\nBy using the ratio, we can obtain the evaluation of the error term when the hypergeometric series is truncated.\nMatrix exponential.\nFor the matrix exponential:\nformula_181\nthe following error evaluation holds (scaling and squaring method):\nformula_182\nSums of divergent series.\nUnder many circumstances, it is desirable to assign generalized sums to series which fail to converge in the strict sense that their sequences of partial sums do not converge. A \"summation method\" is any method for assigning sums to divergent series in a way that systematically extends the classical notion of the sum of a series. Summation methods include Ces\u00e0ro summation, generalized Ces\u00e0ro &amp;NoBreak;&amp;NoBreak; summation, Abel summation, and Borel summation, in order of applicability to increasingly divergent series. These methods are all based on sequence transformations of the original series of terms or of its sequence of partial sums.\nA variety of general results concerning possible summability methods are known. The Silverman\u2013Toeplitz theorem characterizes \"matrix summation methods\", which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients. The most general methods for summing a divergent series are non-constructive and concern Banach limits.\nSeries of functions.\nA series of real- or complex-valued functions\nformula_183\nis pointwise convergent to a limit &amp;NoBreak;&amp;NoBreak; on a set &amp;NoBreak;&amp;NoBreak; if the series converges for each &amp;NoBreak;&amp;NoBreak; in &amp;NoBreak;&amp;NoBreak; as a series of real or complex numbers. Equivalently, the partial sums\nformula_184\nconverge to &amp;NoBreak;&amp;NoBreak; as &amp;NoBreak;&amp;NoBreak; goes to infinity for each &amp;NoBreak;&amp;NoBreak; in &amp;NoBreak;&amp;NoBreak;.\nA stronger notion of convergence of a series of functions is uniform convergence. A series converges uniformly in a set formula_185 if it converges pointwise to the function &amp;NoBreak;&amp;NoBreak; at every point of formula_185 and the supremum of these pointwise errors in approximating the limit by the &amp;NoBreak;&amp;NoBreak;th partial sum,\nformula_187\nconverges to zero with increasing &amp;NoBreak;&amp;NoBreak;, independently of &amp;NoBreak;&amp;NoBreak;.\nUniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the &amp;NoBreak;&amp;NoBreak; are integrable on a closed and bounded interval &amp;NoBreak;&amp;NoBreak; and converge uniformly, then the series is also integrable on &amp;NoBreak;&amp;NoBreak; and can be integrated term by term. Tests for uniform convergence include Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.\nMore sophisticated types of convergence of a series of functions can also be defined. In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a set of measure zero. Other modes of convergence depend on a different metric space structure on the space of functions under consideration. For instance, a series of functions converges in mean to a limit function &amp;NoBreak;&amp;NoBreak; on a set &amp;NoBreak;&amp;NoBreak; if\nformula_188\nPower series.\nA power series is a series of the form\nformula_189\nThe Taylor series at a point &amp;NoBreak;&amp;NoBreak; of a function is a power series that, in many cases, converges to the function in a neighborhood of &amp;NoBreak;&amp;NoBreak;. For example, the series\nformula_190\nis the Taylor series of formula_191 at the origin and converges to it for every &amp;NoBreak;&amp;NoBreak;.\nUnless it converges only at &amp;NoBreak;&amp;NoBreak;, such a series converges on a certain open disc of convergence centered at the point &amp;NoBreak;&amp;NoBreak; in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients &amp;NoBreak;&amp;NoBreak;. The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.\nHistorically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required.\nFormal power series.\nWhile many uses of power series refer to their sums, it is also possible to treat power series as \"formal sums\", meaning that no addition operations are actually performed, and the symbol \"+\" is an abstract symbol of conjunction which is not necessarily interpreted as corresponding to addition. In this setting, the sequence of coefficients itself is of interest, rather than the convergence of the series. Formal power series are used in combinatorics to describe and study sequences that are otherwise difficult to handle, for example, using the method of generating functions. The Hilbert\u2013Poincar\u00e9 series is a formal power series used to study graded algebras.\nEven if the limit of the power series is not considered, if the terms support appropriate structure then it is possible to define operations such as addition, multiplication, derivative, antiderivative for power series \"formally\", treating the symbol \"+\" as if it corresponded to addition. In the most common setting, the terms come from a commutative ring, so that the formal power series can be added term-by-term and multiplied via the Cauchy product. In this case the algebra of formal power series is the total algebra of the monoid of natural numbers over the underlying term ring. If the underlying term ring is a differential algebra, then the algebra of formal power series is also a differential algebra, with differentiation performed term-by-term.\nLaurent series.\nLaurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form\nformula_192\nIf such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence.\nDirichlet series.\nA Dirichlet series is one of the form\nformula_193\nwhere &amp;NoBreak;&amp;NoBreak; is a complex number. For example, if all &amp;NoBreak;&amp;NoBreak; are equal to &amp;NoBreak;&amp;NoBreak;, then the sum of the Dirichlet series is the Riemann zeta function\nformula_194\nLike the zeta function, Dirichlet series in general play an important role in analytic number theory. Generally a Dirichlet series converges if the real part of &amp;NoBreak;&amp;NoBreak; is greater than a number called the abscissa of convergence. In many cases, a function defined by a Dirichlet series is an analytic function that can be extended outside the domain of convergence of the series by analytic continuation. For example, the Dirichlet series for the zeta function converges absolutely when &amp;NoBreak;&amp;NoBreak;, but the zeta function can be extended to a holomorphic function defined on formula_195 with a simple pole at\u00a0&amp;NoBreak;&amp;NoBreak;.\nThis series can be directly generalized to general Dirichlet series.\nTrigonometric series.\nA series of functions in which the terms are trigonometric functions is called a trigonometric series:\nformula_196\nThe most important example of a trigonometric series is the Fourier series of a function.\nAsymptotic series.\nAsymptotic series, typically called asymptotic expansions, are infinite series whose terms are functions of a sequence of different asymptotic orders and whose partial sums are approximations of some other function in an asymptotic limit. In general they do not converge, but they are still useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. They are crucial tools in perturbation theory and in the analysis of algorithms.\nAn asymptotic series cannot necessarily be made to produce an answer as exactly as desired away from the asymptotic limit, the way that an ordinary convergent series of functions can. In fact, a typical asymptotic series reaches its best practical approximation away from the asymptotic limit after a finite number of terms; if more terms are included, the series will produce less accurate approximations.\nHistory of the theory of infinite series.\nDevelopment of infinite series.\nInfinite series play an important role in modern analysis of Ancient Greek philosophy of motion, particularly in Zeno's paradoxes. The paradox of Achilles and the tortoise demonstrates that continuous motion would require an actual infinity of temporal instants, which was arguably an absurdity: Achilles runs after a tortoise, but when he reaches the position of the tortoise at the beginning of the race, the tortoise has reached a second position; when he reaches this second position, the tortoise is at a third position, and so on. Zeno is said to have argued that therefore Achilles could \"never\" reach the tortoise, and thus that continuous movement must be an illusion. Zeno divided the race into infinitely many sub-races, each requiring a finite amount of time, so that the total time for Achilles to catch the tortoise is given by a series. The resolution of the purely mathematical and imaginative side of the paradox is that, although the series has an infinite number of terms, it has a finite sum, which gives the time necessary for Achilles to catch up with the tortoise. However, in modern philosophy of motion the physical side of the problem remains open, with both philosophers and physicists doubting, like Zeno, that spatial motions are infinitely divisible: hypothetical reconciliations of quantum mechanics and general relativity in theories of quantum gravity often introduce quantizations of spacetime at the Planck scale.\nGreek mathematician Archimedes produced the first known summation of an infinite series with a\nmethod that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of \u03c0.\nMathematicians from the Kerala school were studying infinite series c.\u20091350 CE.\nIn the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.\nConvergence criteria.\nThe investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series\nformula_197\non which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.\nCauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms \"convergence\" and \"divergence\" had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.\nAbel (1826) in his memoir on the binomial series\nformula_198\ncorrected certain of Cauchy's conclusions, and gave a completely scientific summation of the series for complex values of formula_199 and formula_200. He showed the necessity of considering the subject of continuity in questions of convergence.\nCauchy's methods led to special rather than general criteria, and\nthe same may be said of Raabe (1832), who made the first elaborate investigation of the subject, of De Morgan (from 1842), whose\nlogarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have\nshown to fail within a certain region; of Bertrand (1842), Bonnet\n(1843), Malmsten (1846, 1847, the latter without integration); Stokes (1847), Paucker (1852), Chebyshev (1852), and Arndt\n(1853).\nGeneral criteria began with Kummer (1835), and have been studied by Eisenstein (1847), Weierstrass in his various\ncontributions to the theory of functions, Dini (1867),\nDuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.\nUniform convergence.\nThe theory of uniform convergence was treated by Cauchy (1821), his limitations being pointed out by Abel, but the first to attack it\nsuccessfully were Seidel and Stokes (1847\u201348). Cauchy took up the\nproblem again (1853), acknowledging Abel's criticism, and reaching\nthe same conclusions which Stokes had already found. Thomae used the\ndoctrine (1866), but there was great delay in recognizing the importance of distinguishing between uniform and non-uniform\nconvergence, in spite of the demands of the theory of functions.\nSemi-convergence.\nA series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.\nSemi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834), who attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schl\u00f6milch (\"Zeitschrift\", Vol.I, p.\u00a0192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function\nformula_201\nGenocchi (1852) has further contributed to the theory.\nAmong the early writers was Wronski, whose \"loi supr\u00eame\" (1815) was hardly recognized until Cayley (1873) brought it into\nprominence.\nFourier series.\nFourier series were being investigated\nas the result of physical considerations at the same time that\nGauss, Abel, and Cauchy were working out the theory of infinite\nseries. Series for the expansion of sines and cosines, of multiple\narcs in powers of the sine and cosine of the arc had been treated by\nJacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still\nearlier by Vieta. Euler and Lagrange simplified the subject,\nas did Poinsot, Schr\u00f6ter, Glaisher, and Kummer.\nFourier (1807) set for himself a different problem, to\nexpand a given function of &amp;NoBreak;&amp;NoBreak; in terms of the sines or cosines of\nmultiples of &amp;NoBreak;&amp;NoBreak;, a problem which he embodied in his \"Th\u00e9orie analytique de la chaleur\" (1822). Euler had already given the formulas for determining the coefficients in the series;\nFourier was the first to assert and attempt to prove the general\ntheorem. Poisson (1820\u201323) also attacked the problem from a\ndifferent standpoint. Fourier did not, however, settle the question\nof convergence of his series, a matter left for Cauchy (1826) to\nattempt and for Dirichlet (1829) to handle in a thoroughly\nscientific manner (see convergence of Fourier series). Dirichlet's treatment (\"Crelle\", 1829), of trigonometric series was the subject of criticism and improvement by\nRiemann (1854), Heine, Lipschitz, Schl\u00e4fli, and\ndu Bois-Reymond. Among other prominent contributors to the theory of\ntrigonometric and Fourier series were Dini, Hermite, Halphen,\nKrause, Byerly and Appell.\nSummations over general index sets.\nDefinitions may be given for infinitary sums over an arbitrary index set formula_202 This generalization introduces two main differences from the usual notion of series: first, there may be no specific order given on the set formula_203; second, the set formula_203 may be uncountable. The notions of convergence need to be reconsidered for these, then, because for instance the concept of conditional convergence depends on the ordering of the index set.\nIf formula_205 is a function from an index set formula_203 to a set formula_207 then the \"series\" associated to formula_208 is the formal sum of the elements formula_209 over the index elements formula_210 denoted by the\nformula_211\nWhen the index set is the natural numbers formula_212 the function formula_213 is a sequence denoted by formula_214 A series indexed on the natural numbers is an ordered formal sum and so we rewrite formula_215 as formula_216 in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers\nformula_217\nFamilies of non-negative numbers.\nWhen summing a family formula_218 of non-negative real numbers over the index set formula_203, define\nformula_220\nAny sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.\nWhen the supremum is finite then the set of formula_221 such that formula_222 is countable. Indeed, for every formula_223 the cardinality formula_224 of the set formula_225 is finite because\nformula_226\nHence the set formula_227 is countable.\nIf formula_203 is countably infinite and enumerated as formula_229 then the above defined sum satisfies\nformula_230\nprovided the value formula_231 is allowed for the sum of the series.\nAbelian topological groups.\nLet formula_232 be a map, also denoted by formula_233 from some non-empty set formula_203 into a Hausdorff abelian topological group formula_235 \nLet formula_236 be the collection of all finite subsets of formula_237 with formula_236 viewed as a directed set, ordered under inclusion formula_239 with union as join. \nThe family formula_233 is said to be unconditionally summable if the following limit, which is denoted by formula_241 and is called the sum of formula_233 exists in formula_243\nformula_244\nSaying that the sum formula_245 is the limit of finite partial sums means that for every neighborhood formula_246 of the origin in formula_247 there exists a finite subset formula_248 of formula_203 such that\nformula_250\nBecause formula_236 is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.\nFor every neighborhood formula_252 of the origin in formula_247 there is a smaller neighborhood formula_246 such that formula_255 It follows that the finite partial sums of an unconditionally summable family formula_233 form a Cauchy net, that is, for every neighborhood formula_252 of the origin in formula_247 there exists a finite subset formula_248 of formula_203 such that\nformula_261\nwhich implies that formula_262 for every formula_263 (by taking formula_264 and formula_265).\nWhen formula_266 is complete, a family formula_267 is unconditionally summable in formula_266 if and only if the finite sums satisfy the latter Cauchy net condition. When formula_266 is complete and formula_233 is unconditionally summable in formula_247 then for every subset formula_272 the corresponding subfamily formula_273 is also unconditionally summable in formula_235\nWhen the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group formula_275\nIf a family formula_267 in formula_266 is unconditionally summable then for every neighborhood formula_252 of the origin in formula_247 there is a finite subset formula_280 such that formula_262 for every index formula_282 not in formula_283 If formula_266 is a first-countable space then it follows that the set of formula_221 such that formula_286 is countable. This need not be true in a general abelian topological group (see examples below).\nUnconditionally convergent series.\nSuppose that formula_287 If a family formula_288 is unconditionally summable in a Hausdorff abelian topological group formula_247 then the series in the usual sense converges and has the same sum,\nformula_290\nBy nature, the definition of unconditional summability is insensitive to the order of the summation. When formula_291 is unconditionally summable, then the series remains convergent after any permutation formula_292 of the set formula_293 of indices, with the same sum,\nformula_294\nConversely, if every permutation of a series formula_291 converges, then the series is unconditionally convergent. When formula_266 is complete then unconditional convergence is also equivalent to the fact that all subseries are convergent; if formula_266 is a Banach space, this is equivalent to say that for every sequence of signs formula_298, the series\nformula_299\nconverges in formula_235\nSeries in topological vector spaces.\nIf formula_266 is a topological vector space (TVS) and formula_302 is a (possibly uncountable) family in formula_266 then this family is summable if the limit formula_304 of the net formula_305 exists in formula_247 where formula_236 is the directed set of all finite subsets of formula_203 directed by inclusion formula_239 and formula_310\nIt is called absolutely summable if in addition, for every continuous seminorm formula_311 on formula_247 the family formula_313 is summable.\nIf formula_266 is a normable space and if formula_302 is an absolutely summable family in formula_247 then necessarily all but a countable collection of formula_317\u2019s are zero. Hence, in normed spaces, it is usually only ever necessary to consider series with countably many terms.\nSummable families play an important role in the theory of nuclear spaces.\nSeries in Banach and seminormed spaces.\nThe notion of series can be easily extended to the case of a seminormed space. \nIf formula_318 is a sequence of elements of a normed space formula_266 and if formula_320 then the series formula_321 converges to formula_200 in formula_266 if the sequence of partial sums of the series formula_324 converges to formula_200 in formula_266; to wit,\nformula_327\nMore generally, convergence of series can be defined in any abelian Hausdorff topological group. \nSpecifically, in this case, formula_321 converges to formula_200 if the sequence of partial sums converges to formula_330\nIf formula_331 is a seminormed space, then the notion of absolute convergence becomes: \nA series formula_332 of vectors in formula_266 converges absolutely if\nformula_334\nin which case all but at most countably many of the values formula_335 are necessarily zero.\nIf a countable series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of ).\nWell-ordered sums.\nConditionally convergent series can be considered if formula_203 is a well-ordered set, for example, an ordinal number formula_337\nIn this case, define by transfinite recursion:\nformula_338\nand for a limit ordinal formula_339\nformula_340\nif this limit exists. If all limits exist up to formula_341 then the series converges.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15289", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=15289", "title": "Interrupt", "text": "Signal to a computer processor emitted by hardware or software\nIn digital computers, an interrupt is a request for the processor to \"interrupt\" currently executing code (when permitted), so that the event can be processed in a timely manner. If the request is accepted, the processor will suspend its current activities, save its state, and execute a function called an \"interrupt handler\" (or an \"interrupt service routine\", ISR) to deal with the event. This interruption is often temporary, allowing the software to resume normal activities after the interrupt handler finishes, although the interrupt could instead indicate a fatal error.\nInterrupts are commonly used by hardware devices to indicate electronic or physical state changes that require time-sensitive attention. Interrupts are also commonly used to implement computer multitasking and system calls, especially in real-time computing. Systems that use interrupts in these ways are said to be interrupt-driven.\nHistory.\nHardware interrupts were introduced as an optimization, eliminating unproductive waiting time in polling loops, waiting for external events. The first system to use this approach was the DYSEAC, completed in 1954, although earlier systems provided error trap functions.\nThe UNIVAC 1103A computer is generally credited with the earliest use of interrupts in 1953. Earlier, on the UNIVAC I (1951) \"Arithmetic overflow either triggered the execution of a two-instruction fix-up routine at address 0, or, at the programmer's option, caused the computer to stop.\" The IBM 650 (1954) incorporated the first occurrence of interrupt masking. The National Bureau of Standards DYSEAC (1954) was the first to use interrupts for I/O. The IBM 704 was the first to use interrupts for debugging, with a \"transfer trap\", which could invoke a special routine when a branch instruction was encountered. The MIT Lincoln Laboratory TX-2 system (1957) was the first to provide multiple levels of priority interrupts.\nTypes.\nInterrupt signals may be issued in response to hardware or software events. These are classified as hardware interrupts or software interrupts, respectively. For any particular processor, the number of interrupt types is limited by the architecture.\nHardware interrupts.\nA hardware interrupt is a condition related to the state of the hardware that may be signaled by an external hardware device, e.g., an interrupt request (IRQ) line on a PC, or detected by devices embedded in processor logic (e.g., the CPU timer in IBM System/370), to communicate that the device needs attention from the operating system (OS) or, if there is no OS, from the bare metal program running on the CPU. Such external devices may be part of the computer (e.g., disk controller) or they may be external peripherals. For example, pressing a keyboard key or moving a mouse plugged into a PS/2 port triggers hardware interrupts that cause the processor to read the keystroke or mouse position.\nHardware interrupts can arrive asynchronously with respect to the processor clock, and at any time during instruction execution. Consequently, all incoming hardware interrupt signals are conditioned by synchronizing them to the processor clock, and acted upon only at instruction execution boundaries.\nIn many systems, each device is associated with a particular IRQ signal. This makes it possible to quickly determine which hardware device is requesting service, and to expedite servicing of that device.\nOn some older systems, such as the 1964 CDC 3600, all interrupts went to the same location, and the OS used a specialized instruction to determine the highest-priority outstanding unmasked interrupt. On contemporary systems, there is generally a distinct interrupt routine for each type of interrupt (or for each interrupt source), often implemented as one or more interrupt vector tables.\nMasking.\nTo \"mask\" an interrupt is to disable it, so it is deferred or ignored by the processor, while to \"unmask\" an interrupt is to enable it.\nProcessors typically have an internal \"interrupt mask\" register, which allows selective enabling (and disabling) of hardware interrupts. Each interrupt signal is associated with a bit in the mask register. On some systems, the interrupt is enabled when the bit is set, and disabled when the bit is clear. On others, the reverse is true, and a set bit disables the interrupt. When the interrupt is disabled, the associated interrupt signal may be ignored by the processor, or it may remain pending. Signals which are affected by the mask are called \"maskable interrupts\".\nSome interrupt signals are not affected by the interrupt mask and therefore cannot be disabled; these are called \"non-maskable interrupts\" (NMIs). These indicate high-priority events which cannot be ignored under any circumstances, such as the timeout signal from a watchdog timer. With regard to SPARC, the Non-Maskable Interrupt (NMI), despite having the highest priority among interrupts, can be prevented from occurring through the use of an interrupt mask.\nMissing interrupts.\nOne failure mode is when the hardware does not generate the expected interrupt for a change in state, causing the operating system to wait indefinitely. Depending on the details, the failure might affect only a single process or might have global impact. Some operating systems have code specifically to deal with this.\nAs an example, IBM Operating System/360 (OS/360) relies on a not-ready to ready device-end interrupt when a tape has been mounted on a tape drive, and will not read the tape label until that interrupt occurs or is simulated. IBM added code in OS/360 so that the VARY ONLINE command will simulate a device end interrupt on the target device.\nSpurious interrupts.\nA \"spurious interrupt\" is a hardware interrupt for which no source can be found. The term \"phantom interrupt\" or \"ghost interrupt\" may also be used to describe this phenomenon. Spurious interrupts tend to be a problem with a wired-OR interrupt circuit attached to a level-sensitive processor input. Such interrupts may be difficult to identify when a system misbehaves.\nIn a wired-OR circuit, parasitic capacitance charging/discharging through the interrupt line's bias resistor will cause a small delay before the processor recognizes that the interrupt source has been cleared. If the interrupting device is cleared too late in the interrupt service routine (ISR), there will not be enough time for the interrupt circuit to return to the quiescent state before the current instance of the ISR terminates. The result is the processor will think another interrupt is pending, since the voltage at its interrupt request input will be not high or low enough to establish an unambiguous internal logic 1 or logic 0. The apparent interrupt will have no identifiable source, hence the \"spurious\" moniker.\nA spurious interrupt may also be the result of electrical anomalies due to faulty circuit design, high noise levels, crosstalk, timing issues, or more rarely, device errata.\nA spurious interrupt may result in system deadlock or other undefined operation if the ISR does not account for the possibility of such an interrupt occurring. As spurious interrupts are mostly a problem with wired-OR interrupt circuits, good programming practice in such systems is for the ISR to check all interrupt sources for activity and take no action (other than possibly logging the event) if none of the sources is interrupting.\nSoftware interrupts.\nA software interrupt is requested by the processor itself upon executing particular instructions or when certain conditions are met. Every software interrupt signal is associated with a particular interrupt handler.\nA software interrupt may be intentionally caused by executing a special instruction which, by design, invokes an interrupt when executed. Such instructions function similarly to subroutine calls and are used for a variety of purposes, such as requesting operating system services and interacting with device drivers (e.g., to read or write storage media). Software interrupts may also be triggered by program execution errors or by the virtual memory system.\nTypically, the operating system kernel will catch and handle software interrupts. Some interrupts are handled transparently to the program - for example, the normal resolution of a page fault is to make the required page accessible in physical memory. But in other cases such as a segmentation fault the operating system executes a process callback. On Unix-like operating systems this involves sending a signal such as SIGSEGV, SIGBUS, SIGILL or SIGFPE, which may either call a signal handler or execute a default action (terminating the program). On Windows the callback is made using Structured Exception Handling with an exception code such as STATUS_ACCESS_VIOLATION or STATUS_INTEGER_DIVIDE_BY_ZERO. Intentional software interrupts for system calls result in calls to routines in the kernel to perform the function requested by the system call.\nIn a kernel process, it is often the case that some types of software interrupts are not supposed to happen. If they occur nonetheless, an operating system crash may result.\nTerminology.\nThe terms \"interrupt\", \"trap\", \"exception\", \"fault\", and \"abort\" are used to distinguish types of interrupts, although \"there is no clear consensus as to the exact meaning of these terms\". The term \"trap\" may refer to any interrupt, to any software interrupt, to any synchronous software interrupt, or only to interrupts caused by instructions with \"trap\" in their names. In some usages, the term \"trap\" refers specifically to a breakpoint intended to initiate a context switch to a monitor program or debugger. It may also refer to a synchronous interrupt caused by an exceptional condition (e.g., division by zero, invalid memory access, illegal opcode), although the term \"exception\" is more common for this.\nx86 divides interrupts into (hardware) \"interrupts\" and software \"exceptions\", and identifies three types of exceptions: faults, traps, and aborts. (Hardware) interrupts are interrupts triggered asynchronously by an I/O device, and allow the program to be restarted with no loss of continuity. A fault is restartable as well but is tied to the synchronous execution of an instruction - the return address points to the faulting instruction. A trap is similar to a fault except that the return address points to the instruction to be executed after the trapping instruction; one prominent use is to implement system calls. An abort is used for severe errors, such as hardware errors and illegal values in system tables, and often does not allow a restart of the program.\nARM uses the term \"exception\" to refer to all types of interrupts, and divides exceptions into (hardware) \"interrupts\", \"aborts\", \"reset\", and exception-generating instructions. Aborts correspond to x86 exceptions and may be prefetch aborts (failed instruction fetches) or data aborts (failed data accesses), and may be synchronous or asynchronous. Asynchronous aborts may be precise or imprecise. MMU aborts (page faults) are synchronous.\nRISC-V uses interrupt as the overall term as well as for the external subset; internal interrupts are called exceptions.\nTriggering methods.\nEach interrupt signal input is designed to be triggered by either a logic signal level or a particular signal edge (level transition). Level-sensitive inputs continuously request processor service so long as a particular (high or low) logic level is applied to the input. Edge-sensitive inputs react to signal edges: a particular (rising or falling) edge will cause a service request to be latched; the processor resets the latch when the interrupt handler executes.\nLevel-triggered.\nA \"level-triggered interrupt\" is requested by holding the interrupt signal at its particular (high or low) active logic level. A device invokes a level-triggered interrupt by driving the signal to and holding it at the active level. It negates the signal when the processor commands it to do so, typically after the device has been serviced.\nThe processor samples the interrupt input signal during each instruction cycle. The processor will recognize the interrupt request if the signal is asserted when sampling occurs.\nLevel-triggered inputs allow multiple devices to share a common interrupt signal via wired-OR connections. The processor polls to determine which devices are requesting service. After servicing a device, the processor may again poll and, if necessary, service other devices before exiting the ISR.\u00a0 As previously described, a processor whose level-sensitive interrupt input is connected to a wired-OR circuit is susceptible to spurious interrupts, which should they occur, may cause deadlock or some other potentially-fatal system fault.\nEdge-triggered.\nAn \"edge-triggered interrupt\" is an interrupt signaled by a level transition on the interrupt line, either a falling edge (high to low) or a rising edge (low to high). A device wishing to signal an interrupt drives a pulse onto the line and then releases the line to its inactive state. \nThe important part of edge triggering is that the signal must transition to trigger the interrupt; for example, if the transition was high-low, there would only be one falling edge interrupt triggered, and the continued low level would not trigger a further interrupt. The signal must return to the high level and fall again in order to trigger a further interrupt. This contrasts with a level trigger where the low level would continue to create interrupts (if they are enabled) until the signal returns to its high level.\nComputers with edge-triggered interrupts may include an \"interrupt register\" that retains the status of pending interrupts. Systems with interrupt registers generally have interrupt mask registers as well.\nProcessor response.\nThe processor samples the interrupt trigger signals or interrupt register during each instruction cycle, and will process the highest priority enabled interrupt found.\nRegardless of the triggering method, the processor will begin interrupt processing at the next instruction boundary following a detected trigger, thus ensuring:\nThere are several different architectures for handling interrupts. In some, there is a single interrupt handler that must scan for the highest priority enabled interrupt. In others, there are separate interrupt handlers for separate interrupt types, separate I/O channels or devices, or both. Several interrupt causes may have the same interrupt type and thus the same interrupt handler, requiring the interrupt handler to determine the cause.\nSystem implementation.\nInterrupts may be fully handled in hardware by the CPU, or may be handled by both the CPU and another component such as a programmable interrupt controller or a southbridge.\nIf an additional component is used, that component would be connected between the interrupting device and the processor's interrupt pin to multiplex several sources of interrupt onto the one or two CPU lines typically available. If implemented as part of the memory controller, interrupts are mapped into the system's memory address space.\nIn systems on a chip (SoC) implementations, interrupts come from different blocks of the chip and are usually aggregated in an interrupt controller attached to one or several processors (in a multi-core system).\nShared IRQs.\nMultiple devices may share an edge-triggered interrupt line if they are designed to. The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to its inactive state, which is the default state of it. Devices signal an interrupt by briefly driving the line to its non-default state, and let the line float (do not actively drive it) when not signaling an interrupt. This type of connection is also referred to as open collector. The line then carries all the pulses generated by all the devices. (This is analogous to the pull cord on some buses and trolleys that any passenger can pull to signal the driver that they are requesting a stop.) However, interrupt pulses from different devices may merge if they occur close in time. To avoid losing interrupts the CPU must trigger on the trailing edge of the pulse (e.g. the rising edge if the line is pulled up and driven low). After detecting an interrupt the CPU must check all the devices for service requirements.\nEdge-triggered interrupts do not suffer the problems that level-triggered interrupts have with sharing. Service of a low-priority device can be postponed arbitrarily, while interrupts from high-priority devices continue to be received and get serviced. If there is a device that the CPU does not know how to service, which may raise spurious interrupts, it will not interfere with interrupt signaling of other devices. However, it is easy for an edge-triggered interrupt to be missed - for example, when interrupts are masked for a period - and unless there is some type of hardware latch that records the event it is impossible to recover. This problem caused many \"lockups\" in early computer hardware because the processor did not know it was expected to do something. More modern hardware often has one or more interrupt status registers that latch interrupts requests; well-written edge-driven interrupt handling code can check these registers to ensure no events are missed.\nThe Industry Standard Architecture (ISA) bus uses edge-triggered interrupts, without mandating that devices be able to share IRQ lines, but all mainstream ISA motherboards include pull-up resistors on their IRQ lines, so well-behaved ISA devices sharing IRQ lines should just work fine. The parallel port also uses edge-triggered interrupts. Many older devices assume that they have exclusive use of IRQ lines, making it electrically unsafe to share them.\nThere are three ways multiple devices \"sharing the same line\" can be raised. First is by exclusive conduction (switching) or exclusive connection (to pins). Next is by bus (all connected to the same line listening): cards on a bus must know when they are to talk and not talk (i.e., the ISA bus). Talking can be triggered in two ways: by accumulation latch or by logic gates. Logic gates expect a continual data flow that is monitored for key signals. Accumulators only trigger when the remote side excites the gate beyond a threshold, thus no negotiated speed is required. Each has its speed versus distance advantages. A trigger, generally, is the method in which excitation is detected: rising edge, falling edge, threshold (oscilloscope can trigger a wide variety of shapes and conditions).\nTriggering for software interrupts must be built into the software (both in OS and app). A 'C' app has a trigger table (a table of functions) in its header, which both the app and OS know of and use appropriately that is not related to hardware. However do not confuse this with hardware interrupts which signal the CPU (the CPU enacts software from a table of functions, similarly to software interrupts).\nDifficulty with sharing interrupt lines.\nMultiple devices sharing an interrupt line (of any triggering style) all act as spurious interrupt sources with respect to each other. With many devices on one line, the workload in servicing interrupts grows in proportion to the number of devices. It is therefore preferred to spread devices evenly across the available interrupt lines. Shortage of interrupt lines is a problem in older system designs where the interrupt lines are distinct physical conductors. Message-signaled interrupts, where the interrupt line is virtual, are favored in new system architectures (such as PCI Express) and relieve this problem to a considerable extent.\nSome devices with a poorly designed programming interface provide no way to determine whether they have requested service. They may lock up or otherwise misbehave if serviced when they do not want it. Such devices cannot tolerate spurious interrupts, and so also cannot tolerate sharing an interrupt line. ISA cards, due to often cheap design and construction, are notorious for this problem. Such devices are becoming much rarer, as hardware logic becomes cheaper and new system architectures mandate shareable interrupts.\nHybrid.\nSome systems use a hybrid of level-triggered and edge-triggered signaling. The hardware not only looks for an edge, but it also verifies that the interrupt signal stays active for a certain period of time.\nA common use of a hybrid interrupt is for the NMI (non-maskable interrupt) input. Because NMIs generally signal major \u2013 or even catastrophic \u2013 system events, a good implementation of this signal tries to ensure that the interrupt is valid by verifying that it remains active for a period of time. This 2-step approach helps to eliminate false interrupts from affecting the system.\nMessage-signaled.\nA \"message-signaled interrupt\" does not use a physical interrupt line. Instead, a device signals its request for service by sending a short message over some communications medium, typically a computer bus. The message might be of a type reserved for interrupts, or it might be of some pre-existing type such as a memory write.\nMessage-signalled interrupts behave very much like edge-triggered interrupts, in that the interrupt is a momentary signal rather than a continuous condition. Interrupt-handling software treats the two in much the same manner. Typically, multiple pending message-signaled interrupts with the same message (the same virtual interrupt line) are allowed to merge, just as closely spaced edge-triggered interrupts can merge.\nMessage-signalled interrupt vectors can be shared, to the extent that the underlying communication medium can be shared. No additional effort is required.\nBecause the identity of the interrupt is indicated by a pattern of data bits, not requiring a separate physical conductor, many more distinct interrupts can be efficiently handled. This reduces the need for sharing. Interrupt messages can also be passed over a serial bus, not requiring any additional lines.\nPCI Express, a serial computer bus, uses message-signaled interrupts exclusively.\nDoorbell.\nIn a push button analogy applied to computer systems, the term \"doorbell\" or \"doorbell interrupt\" is often used to describe a mechanism whereby a software system can signal or notify a computer hardware device that there is some work to be done. Typically, the software system will place data in some well-known and mutually agreed upon memory locations, and \"ring the doorbell\" by writing to a different memory location. This different memory location is often called the doorbell region, and there may even be multiple doorbells serving different purposes in this region. It is this act of writing to the doorbell region of memory that \"rings the bell\" and notifies the hardware device that the data are ready and waiting. The hardware device would now know that the data are valid and can be acted upon. It would typically write the data to a hard disk drive, or send them over a network, or encrypt them, etc.\nThe term \"doorbell interrupt\" is usually a misnomer. It is similar to an interrupt, because it causes some work to be done by the device; however, the doorbell region is sometimes implemented as a polled region, sometimes the doorbell region writes through to physical device registers, and sometimes the doorbell region is hardwired directly to physical device registers. When either writing through or directly to physical device registers, this may cause a real interrupt to occur at the device's central processor unit (CPU), if it has one.\nDoorbell interrupts can be compared to Message Signaled Interrupts, as they have some similarities.\nMultiprocessor IPI.\nIn multiprocessor systems, a processor may send an interrupt request to another processor via inter-processor interrupts (IPI).\nPerformance.\nInterrupts provide low overhead and good latency at low load, but degrade significantly at high interrupt rate unless care is taken to prevent several pathologies. The phenomenon where the overall system performance is severely hindered by excessive amounts of processing time spent handling interrupts is called an interrupt storm.\nThere are various forms of livelocks, when the system spends all of its time processing interrupts to the exclusion of other required tasks.\nUnder extreme conditions, a large number of interrupts (like very high network traffic) may completely stall the system. To avoid such problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution.\nWith multi-core processors, additional performance improvements in interrupt handling can be achieved through receive-side scaling (RSS) when multiqueue NICs are used. Such NICs provide multiple receive queues associated to separate interrupts; by routing each of those interrupts to different cores, processing of the interrupt requests triggered by the network traffic received by a single NIC can be distributed among multiple cores. Distribution of the interrupts among cores can be performed automatically by the operating system, or the routing of interrupts (usually referred to as \"IRQ affinity\") can be manually configured.\nA purely software-based implementation of the receiving traffic distribution, known as \"receive packet steering\" (RPS), distributes received traffic among cores later in the data path, as part of the interrupt handler functionality. Advantages of RPS over RSS include no requirements for specific hardware, more advanced traffic distribution filters, and reduced rate of interrupts produced by a NIC. As a downside, RPS increases the rate of inter-processor interrupts (IPIs). \"Receive flow steering\" (RFS) takes the software-based approach further by accounting for application locality; further performance improvements are achieved by processing interrupt requests by the same cores on which particular network packets will be consumed by the targeted application.\nTypical uses.\nInterrupts are commonly used to service hardware timers, transfer data to and from storage (e.g., disk I/O) and communication interfaces (e.g., UART, Ethernet), handle keyboard and mouse events, and to respond to any other time-sensitive events as required by the application system. Non-maskable interrupts are typically used to respond to high-priority requests such as watchdog timer timeouts, power-down signals and traps.\nHardware timers are often used to generate periodic interrupts. In some applications, such interrupts are counted by the interrupt handler to keep track of absolute or elapsed time, or used by the OS task scheduler to manage execution of running processes, or both. Periodic interrupts are also commonly used to invoke sampling from input devices such as analog-to-digital converters, incremental encoder interfaces, and GPIO inputs, and to program output devices such as digital-to-analog converters, motor controllers, multiplexed displays, and GPIO outputs.\nA disk interrupt signals the completion of a data transfer from or to the disk peripheral; this may cause a process to run which is waiting to read or write. A power-off interrupt predicts imminent loss of power, allowing the computer to perform an orderly shut-down while there still remains enough power to do so. Keyboard interrupts typically cause keystrokes to be buffered so as to implement typeahead.\nInterrupts are sometimes used to emulate instructions which are unimplemented on some computers in a product family. For example floating point instructions may be implemented in hardware on some systems and emulated on lower-cost systems. In the latter case, execution of an unimplemented floating point instruction will cause an \"illegal instruction\" exception interrupt. The interrupt handler will implement the floating point function in software and then return to the interrupted program as if the hardware-implemented instruction had been executed. This provides application software portability across the entire line.\nInterrupts are similar to signals, the difference being that signals are used for inter-process communication (IPC), mediated by the kernel (possibly via system calls) and handled by processes, while interrupts are mediated by the processor and handled by the kernel. The kernel may pass an interrupt as a signal to the process that caused it (typical examples are SIGSEGV, SIGBUS, SIGILL and SIGFPE).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15290", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=15290", "title": "Intercalation (timekeeping)", "text": "Insertion of a leap day, week, or month\nIntercalation or embolism in timekeeping is the insertion of a leap day, week, or month into some calendar years to make the calendar follow the seasons or moon phases. Lunisolar calendars may require intercalations of days or months.\nSolar calendars.\nThe solar or tropical year does not have a whole number of days (it is about 365.24 days), but a calendar year must have a whole number of days. The most common way to reconcile the two is to vary the number of days in the calendar year.\nIn solar calendars, this is done by adding an extra day (\"leap day\" or \"intercalary day\") to a common year of 365 days, about once every four years, creating a leap year that has 366 days (Julian, Gregorian and Indian national calendars).\nThe Decree of Canopus, issued by the pharaoh Ptolemy III Euergetes of Ancient Egypt in 239 BC, decreed a solar leap day system; an Egyptian leap year was not adopted until 25 BC, when the Roman Emperor Augustus instituted a reformed Alexandrian calendar.\nIn the Julian calendar, as well as in the Gregorian calendar, which improved upon it, intercalation is done by adding an extra day to February in each leap year. In the Julian calendar this was done every four years. In the Gregorian, years divisible by 100 but not 400 were exempted in order to improve accuracy. Thus, 2000 was a leap year; 1700, 1800, and 1900 were not.\nEpagomenal days are days within a solar calendar that are outside any regular month. Usually five epagomenal days are included within every year (Egyptian, Coptic, Ethiopian, Mayan Haab' and French Republican Calendars), but a sixth epagomenal day is intercalated every four years in some (Coptic, Ethiopian and French Republican calendars).\nThe Solar Hijri calendar, used in Iran, is based on solar calculations and is similar to the Gregorian calendar in its structure, and hence the intercalation, with the exception that its epoch is the Hijrah.\nThe Bah\u00e1'\u00ed calendar includes enough epagomenal days (usually 4 or 5) before the last month (, \"\u02bfal\u0101\u02be\") to ensure that the following year starts on the March equinox. These are known as the Ayy\u00e1m-i-H\u00e1.\nLunisolar calendars.\nThe solar year does not have a whole number of lunar months (it is about 365/29.5 = 12.37 lunations), so a lunisolar calendar must have a variable number of months per year. Regular years have 12 months, but embolismic years insert a 13th \"intercalary\" or \"leap\" month or \"embolismic\" month every second or third year. Whether to insert an intercalary month in a given year may be determined using regular cycles such as the 19-year Metonic cycle (Hebrew calendar and in the determination of Easter) or using calculations of lunar phases (Hindu lunisolar and Chinese calendars). The Buddhist calendar adds both an intercalary day and month on a usually regular cycle.\nLunar calendars.\nIn principle, lunar calendars do not employ intercalation because they do not seek to synchronise with the seasons, and the motion of the moon is astronomically predictable. But religious lunar calendars rely on actual observation.\nThe Lunar Hijri calendar, the purely lunar calendar observed by most of Islam for religious use, depends on actual observation of the first crescent of the moon and thus has no intercalation. Each month still has either 29 or 30 days, but due to the variable method of observations employed, there is usually no discernible order in the sequencing of 29- or 30-day month lengths. Traditionally, the first day of each month is the day (beginning at sunset) of the first sighting of the \"hilal\" (crescent moon) shortly after sunset. If the \"hilal\" is not observed immediately after the 29th day of a month (either because clouds block its view or because the western sky is still too bright when the moon sets), then the day that begins at that sunset is the 30th. \nThe tabular Islamic calendar, used to predict the new moon and for historical date conversions, has 12 lunar months that usually alternate between 30 and 29 days every year, but the algorithm adds an intercalary day as the last month of the year 12 times in a 33-year cycle. Some historians also linked the pre-Islamic practice of Nasi' to intercalation.\nLeap seconds.\nThe International Earth Rotation and Reference Systems Service can insert or remove leap seconds from the last day of any month (June and December are preferred). These are sometimes described as intercalary.\nOther uses.\nISO 8601 includes a specification for a 52/53-week year. Any year that has 53 Thursdays has 53 weeks; this extra week may be regarded as intercalary, i.e. a leap week.\nThe \"xiuhp\u014dhualli\" (year count) system of the Aztec calendar had five intercalary days after the eighteenth and final month, the \"n\u0113mont\u0113mi\", in which the people fasted and reflected on the past year.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15291", "revid": "41264804", "url": "https://en.wikipedia.org/wiki?curid=15291", "title": "Intercourse", "text": "Intercourse may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15292", "revid": "50704754", "url": "https://en.wikipedia.org/wiki?curid=15292", "title": "Ink", "text": "Liquid or paste that contains pigments or dyes\nInk is a gel, sol, or solution that contains at least one colorant, such as a dye or pigment, and is used to color a surface to produce an image, text, or design. Ink is used for drawing or writing with a pen, brush, reed pen, or quill. Thicker inks, in paste form, are used extensively in letterpress and lithographic printing.\nInk can be a complex medium, composed of solvents, pigments, dyes, resins, lubricants, solubilizers, surfactants, particulate matter, fluorescents, and other materials. The components of inks serve multiple purposes; the ink's carrier, colorants, and other additives affect the flow and thickness of the ink and its dry appearance.\nHistory.\n Many ancient cultures around the world have independently discovered and formulated inks due to the need to write and draw. The recipes and techniques for the production of ink are derived from archaeological analyses or from written texts themselves. The earliest inks from all civilizations are believed to have been made with \"lampblack\", a kind of soot, easily collected as a by-product of fire.\nInk was used in Ancient Egypt for writing and drawing on papyrus from at least the 26th century BC. Egyptian red and black inks included iron and ocher as pigments, in addition to phosphate, sulfate, chloride, and carboxylate ions, with lead also used as a drier.\nThe earliest Chinese inks may date to four millennia ago, to the Chinese Neolithic Period. These included plant, animal, and mineral inks, based on such materials as graphite; these were ground with water and applied with ink brushes. Direct evidence for the earliest Chinese inks, similar to modern inksticks, is found around 256 BC, in the end of the Warring States period; being produced from soot and animal glue. The preferred inks for drawing or painting on paper or silk are produced from the resin of the pine trees between 50 and 100 years old. The Chinese inkstick is produced with a fish glue, whereas Japanese glue (\u81a0 \"nikawa\") is from cow or stag.\nIndia ink was invented in China, though materials were often traded from India, hence the name. The traditional Chinese method of making the ink was to grind a mixture of hide glue, carbon black, lampblack, and bone black pigment with a pestle and mortar, then pour it into a ceramic dish to dry. To use the dry mixture, a wet brush would be applied until it reliquified. The manufacture of India ink was well-established by the Cao Wei dynasty (220\u2013265 AD). Indian documents written in Kharosthi with ink have been unearthed in Xinjiang. The practice of writing with ink and a sharp pointed needle was common in early South India. Several Buddhist and Jain sutras in India were compiled in ink.\nCephalopod ink, known as sepia, turns from dark blue-black to brown on drying, and was used as an ink in the Graeco-Roman period and subsequently. Black atramentum was also used in ancient Rome; in an article for \"The Christian Science Monitor\", Sharon J. Huntington describes these other historical inks:\nAbout 1,600 years ago, a popular ink recipe was created. The recipe was used for centuries. Iron salts, such as ferrous sulfate (made by treating iron with sulfuric acid), were mixed with tannin from gallnuts (they grow on trees) and a thickener. When first put to paper, this ink is bluish-black. Over time it fades to a dull brown.\nScribes in medieval Europe (about AD 800 to 1500) wrote principally on parchment or vellum. One 12th century ink recipe called for hawthorn branches to be cut in the spring and left to dry. Then the bark was pounded from the branches and soaked in water for eight days. The water was boiled until it thickened and turned black. Wine was added during boiling. The ink was poured into special bags and hung in the sun. Once dried, the mixture was mixed with wine and iron salt over a fire to make the final ink.\nThe reservoir pen, which may have been the first fountain pen, dates back to 953, when Ma'\u0101d al-Mu'izz, the caliph of Egypt, demanded a pen that would not stain his hands or clothes, and was provided with a pen that held ink in a reservoir.\nIn the 15th century, a new type of ink had to be developed in Europe for the printing press by Johannes Gutenberg. According to Martyn Lyons in his book \"Books: A Living History\", Gutenberg's dye was indelible, oil-based, and made from the soot of lamps (lamp-black) mixed with varnish and egg white. Two types of ink were prevalent at the time: the Greek and Roman writing ink (soot, glue, and water) and the 12th century variety composed of ferrous sulfate, gall, gum, and water. Neither of these handwriting inks could adhere to printing surfaces without creating blurs. Eventually an oily, varnish-like ink made of soot, turpentine, and walnut oil was created specifically for the printing press.\nTypes.\nInk formulas vary, but commonly involve two components:\nInks generally fall into four classes:\nColorants.\nPigments.\nPigment inks are used more frequently than dyes because they are more color-fast, but they are also more expensive, less consistent in color, and have less of a color range than dyes.\nPigments are solid, opaque particles suspended in ink to provide color. Pigment molecules typically link together in crystalline structures that are 0.1\u20132 \u03bcm in size and comprise 5\u201330 percent of the ink volume. Qualities such as hue, saturation, and lightness vary depending on the source and type of pigment. Solvent-based inks are widely used for high-speed printing and applications that require quick drying times. And the inclusion of TiO2 powder provides superior coverage and vibrant colors.\nDyes.\nDye-based inks are generally much stronger than pigment-based inks and can produce much more color of a given density per unit of mass. However, because dyes are dissolved in the liquid phase, they have a tendency to soak into paper, potentially allowing the ink to bleed at the edges of an image.\nTo circumvent this problem, dye-based inks are made with solvents that dry rapidly or are used with quick-drying methods of printing, such as blowing hot air on the fresh print. Other methods include harder paper sizing and more specialized paper coatings. The latter is particularly suited to inks used in non-industrial settings (which must conform to tighter toxicity and emission controls), such as inkjet printer inks. Another technique involves coating the paper with a charged coating. If the dye has the opposite charge, it is attracted to and retained by this coating, while the solvent soaks into the paper. Cellulose, the wood-derived material most paper is made of, is naturally charged, and so a compound that complexes with both the dye and the paper's surface aids retention at the surface. Such a compound is commonly used in ink-jet printing inks.\nAn additional advantage of dye-based ink systems is that the dye molecules can interact with other ink ingredients, potentially allowing greater benefit as compared to pigmented inks from optical brighteners and color-enhancing agents designed to increase the intensity and appearance of dyes.\nDye-based inks can be used for anti-counterfeit purposes and can be found in some gel inks, fountain pen inks, and inks used for paper currency. These inks react with cellulose to bring about a permanent color change. Dye based inks are used to color hair.\nFunctional colorants.\nFunctional colorants represent a distinct class of color-producing materials used in inks that provide effects beyond simple absorption, including fluorescence, phosphorescence, thermochromism, and photochromism. These colorants are commonly used in security printing, smart packaging, and specialty applications, where their ability to change appearance in response to light, temperature, or other stimuli is advantageous. Modern formulations employ microencapsulated dyes, rare-earth phosphors, or polymer-based nanoparticles that allow these materials to be incorporated into liquid and inkjet-compatible systems.\nHealth and environmental aspects.\nThere is a misconception that ink is non-toxic even if swallowed. Once ingested, ink can be hazardous to one's health. Certain inks, such as those used in digital printers, and even those found in a common pen, can be harmful. Though ink does not easily cause death, repeated skin contact or ingestion can cause effects such as severe headaches, skin irritation, or nervous system damage. These effects can be caused by solvents, or by pigment ingredients such as \"p\"-Anisidine, which helps create some inks' color and shine.\nThree main environmental issues with ink are:\nSome regulatory bodies have set standards for the amount of heavy metals in ink. There is a trend toward vegetable oils rather than petroleum oils in recent years in response to a demand for better environmental sustainability performance.\nInk uses up non-renewable oils and metals, which has a negative impact on the environment.\nCarbon.\nCarbon inks were commonly made from lampblack or soot and a binding agent such as gum arabic or animal glue. The binding agent keeps carbon particles in suspension and adhered to paper. Carbon particles do not fade over time even when bleached or when in sunlight. One benefit is that carbon ink does not harm paper. Over time, the ink is chemically stable and therefore does not threaten the paper's strength. Despite these benefits, carbon ink is not ideal for permanence and ease of preservation. Carbon ink tends to smudge in humid environments and can be washed off surfaces. The best method of preserving a document written in carbon ink is to store it in a dry environment (Barrow 1972).\nRecently, carbon inks made from carbon nanotubes have been successfully created. They are similar in composition to traditional inks in that they use a polymer to suspend the carbon nanotubes. These inks can be used in inkjet printers and produce electrically conductive patterns.\nIron gall (common ink).\nIron gall inks became prominent in the early 12th century; they were used for centuries and were widely thought to be the best type of ink. However, iron gall ink is corrosive and damages paper over time (Waters 1940). Items containing this ink can become brittle and the writing fades to brown. The original scores of Johann Sebastian Bach are threatened by the destructive properties of iron gall ink. The majority of his works are held by the German State Library, and about 25% of those are in advanced stages of decay (American Libraries 2000). The rate at which the writing fades is based on several factors, such as proportions of ink ingredients, amount deposited on the paper, and paper composition (Barrow 1972:16). Corrosion is caused by acid catalyzed hydrolysis and iron(II)-catalysed oxidation of cellulose (Rouchon-Quillet 2004:389).\nTreatment for preservation is a controversial subject. No treatment undoes damage already caused by acidic ink. Deterioration can only be stopped or slowed. Some think it best not to treat the item at all for fear of the consequences. Others believe that non-aqueous procedures are the best solution. Yet others think an aqueous procedure may preserve items written with iron gall ink. Aqueous treatments include distilled water at different temperatures, calcium hydroxide, calcium bicarbonate, magnesium carbonate, magnesium bicarbonate, and calcium hyphenate. There are a number of possible side effects from these treatments. There can be mechanical damage, which further weakens the paper. Paper color or ink color may change, and ink may bleed. Other consequences of aqueous treatment are a change of ink texture or formation of plaque on the surface of the ink (Reibland &amp; de Groot 1999).\nIron gall inks require storage in a stable environment, because fluctuating relative humidity increases the rate that formic acid, acetic acid, and furan derivatives form in the material the ink was used on. Sulfuric acid acts as a catalyst to cellulose hydrolysis, and iron (II) sulfate acts as a catalyst to cellulose oxidation. These chemical reactions physically weaken the paper, causing brittleness.\nIndelible ink.\n\"Indelible\" means \"un-removable\". Some types of indelible ink have a very short shelf life because of the quickly evaporating solvents used. India, Mexico, Indonesia, Malaysia and other developing countries have used indelible ink in the form of electoral stain to prevent electoral fraud. Election ink based on silver nitrate was first applied in the 1962 Indian general election, after being developed at the National Physical Laboratory of India.\nThe election commission in India has used indelible ink for a number of elections. Indonesia used it in its election in 2014. In Mali, the ink is applied to the fingernail. The technique is not infallible and can itself be used in other types of fraud, as rather than bolstering one's own votes it can be used to eliminate opponent voters by marking them before they have chances to cast their votes. There are also reports of \"indelible\" ink washing off voters' fingers in Afghanistan.\nInkblots.\nIn 19th century German poet Justinus Kerner invented the art of klecksography, making images from inkblots (German \"Tinten-Klecks\") and in 1857 he published a book of klecksographs an poetry titled \"Klecksographien\". In 1896, a similar book titled \"Gobolinks, or Shadow-Pictures for Young and Old\" was published in the United States.\nIn his childhood, Hermann Rorschach had a klecksography hobby, and this eventually led to the development of his Rorschach test.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15293", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=15293", "title": "Balochistan (Iran)", "text": ""}
{"id": "15294", "revid": "40561892", "url": "https://en.wikipedia.org/wiki?curid=15294", "title": "Islamabad Capital Territory", "text": "Federal territory of Pakistan\nThe Islamabad Capital Territory is a federal territory of Pakistan, centred around Islamabad, the capital of Pakistan. It is located on the northern edge of the Pothohar Plateau, at the foot of the Margalla Hills, in the northwestern Punjab region. The Territory shares borders with the province of Khyber Pakhtunkhwa in the west and the province of Punjab in the remaining directions. It covers an area of and, according to 2023 census, has a population of over 2.3 million.\nThe area was separated from Rawalpindi District in 1967 to form a separate territory administered by the federal government. The territory is represented in the National Assembly by NA-52, NA-53, and NA-54 constituencies and by four seats in the Senate.\nHistory.\nIn 1960, land was transferred from the Rawalpindi District of Punjab province to replace the Karachi Federal Capital Territory and establish Pakistan's new capital. According to the 1960s master plan, the territory included much of what was Rawalpindi, and composed of the following parts:\nHowever, Rawalpindi was eventually excluded from the Islamabad master plan in the 1980s.\nGeography.\nThe Islamabad Capital Territory is composed of eight types of areas: administrative, commercial, diplomatic, educational, green, industrial, residential, and rural.\nZones.\nThe territory is divided into five zones in two groups:\nZone I consists mainly of all the developed residential sectors, while Zone II consists of the under-developed residential sectors. Each residential sector is identified by a letter of the alphabet and a number, and covers an area of approximately 4 square kilometres. The sectors are lettered from A to I, and each sector is divided into four numbered sub-sectors. Zone IV is the largest in area.\nSectors.\nSeries A, B, and C are still underdeveloped. The D series has seven sectors (D-11 to D-17), of which only sector D-12 is completely developed. This series is located at the foot of Margalla Hills. The E Sectors are named from E-7 to E-17. Many foreigners and diplomatic personnel are housed in these sectors. In the revised Master Plan of the city, CDA has decided to develop a park on the pattern of Fatima Jinnah Park in sector E-14. Sectors E-8 and E-9 contain the campuses of Bahria University, Air University, and the National Defence University. The F and G series contains the most developed sectors. F series contains sectors F-5 to F-17; some sectors are still under-developed. F-5 is an important sector for the software industry in Islamabad, as the two software technology parks are located here. The entire F-9 sector is covered with Fatima Jinnah Park. The Centaurus complex will be one of the major landmarks of the F-8 sector. G sectors are numbered G-5 through G-17. Some important places include the Jinnah Convention Center and Serena Hotel in G-5, the Red Mosque in G-6, and the Pakistan Institute of Medical Sciences, the largest medical complex in the capital, located in G-8.\nThe H sectors are numbered H-8 through H-17. The H sectors are mostly dedicated to educational and health institutions. National University of Sciences and Technology covers a major portion of sector H-12. The I sectors are numbered from I-8 to I-18. With the exception of I-8, which is a well-developed residential area, these sectors are primarily part of the industrial zone. Currently two sub-sectors of I-9 and one sub-sector of I-10 are used as industrial areas. CDA is planning to set up Islamabad Railway Station in Sector I-18 and Industrial City in sector I-17. Zone III consists primarily of the Margalla Hills and Margalla Hills National Park. Rawal Lake is in this zone. Zone IV and V consist of Islamabad Park, and rural areas of the city. The Soan River flows into the city through Zone V.\nClimate.\nSeasons.\nThe climate of Islamabad has a humid subtropical climate (K\u00f6ppen: Cwa), with five seasons: winter (November\u2013February), spring (March and April), summer (May and June), rainy monsoon (July and August), and autumn (September and October).\nTemperatures.\nThe temperatures range from in January to in June. The hottest month is June, where average highs routinely exceed , while the coolest month is January. The highest recorded temperature was on 23 June 2005 while the lowest temperature was on 17 January 1967. Winters generally feature dense fog in the mornings and sunny afternoons. In the city, temperatures stay mild, with snowfall over the higher elevations points on nearby hill stations, notably Murree and Nathia Gali.\nPrecipitation.\nThe wettest month is July, with heavy rainfalls and evening thunderstorms with the possibility of cloudburst and flooding. Highest monthly rainfall of was recorded during July 1995. On 23 July 2001, Islamabad received a record breaking of rainfall in just 10 hours. It was the heaviest rainfall in Islamabad in the past 100 years and the highest rainfall in 24 hours as well. The city has also experienced snowfall on a number of occasions.\nIslamabad's micro-climate is regulated by three artificial reservoirs: Rawal, Simli, and Khanpur Dam. The latter is located on the Haro River near the town of Khanpur, about from Islamabad. Simli Dam is north of Islamabad.\nVegetation.\nAround of the city consists of the Margalla Hills National Park, while the Loi Bher Forest is situated along the Islamabad Highway, covering an area of . \nGovernment.\nThe government of the Islamabad Capital Territory is provided by the federal Government of Pakistan and by a number of territorial bodies. Article 258 of the Constitution of Pakistan stipulates that the President of Pakistan is responsible for establishing the government of the federal capital and the Parliament of Pakistan makes laws for the territory. Although the federal government retains strategic power over the territory, the day to day government is carried out by a number of bodies, which have been established over the years.\nThe Capital Development Authority (CDA), established on 14 June 1960, was responsible for building the capital and continues to have some government roles. The Islamabad Capital Territory Administration (ICTA) was established in 1980, and is led by a Chief Commissioner, who holds powers similar to a provincial government. The Islamabad High Court, established in 2007 (dissolved in 2008, re-established in 2010) is the senior court of the territory and is equivalent to the four provincial high courts. The Islamabad Metropolitan Corporation (MCI), established in 2015, is the municipal authority and is equivalent to a district government, as the upper tier of the local government in the territory. The CDA has been gradually transferring many of its roles to the MCI.\nAt the most local level, there are 101 union councils, though this is due to increase to 125 councils.\nUnion councils.\nAt the most local level, the Territory comprises 101 union councils, though this is due to increase to 125 Union councils.\nDemographics.\nPopulation.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nAccording to the 2023 Census, Islamabad Capital Territory had a population of 2,363,863.\nLanguage.\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\nAccording to 2023 Pakistani census, there are 1,154,540 Punjabi, 415,838 Pashto, 358,922 Urdu, 140,780 Hindko, 51,920 Kashmiri, 46,270 Saraiki, 21,362 Sindhi, 10,315 Balti, 7,099 Shina, 5,016 Koshistani, 4,503 Balochi, 1,095 Mewati, 668 Brahvi, 182 Kalasha and 64,734 others, of total 2,283,244 speakers.\nAfter Islamabad's establishment, people from all around Pakistan shifted here which led to a diverse demographics. Punjabis constitute the largest ethnolinguistic population in the territory, with many having settled here from Punjab province as well. Pothwari dialect, which has been regarded by many as the dialect of Punjabi, has historically been the indigenous language of the region; and Punjabi still continues to be the most-widely spoken first language in Islamabad, with Punjabi-speakers forming a majority of the population. Punjabi is spoken in the form of many dialects and varieties, including the Majhi, Dhani, Shahpuri, Jhangvi, and Doabi dialects.\nReligion.\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\nIslam is the largest and most practiced religion in the territory, with just over 97% of the population adhering to the faith. Christianity is the second largest religion and forms 2.8% of the population. Ahmadis make up 0.11% of the population. Other minorities, like Sikhs and Hindus, form just 0.2% combined.\nHindu population in Islamabad is very small as per census, around 900, but other sources estimates it to be around 3000. The territory also hosts the historic important and sacred Ram mandir.\nIslamabad-Rawalpindi metropolitan area.\nWhen the master plan for Islamabad was drawn up in 1960, Islamabad and Rawalpindi, along with the adjoining areas, was to be integrated to form a large metropolitan area called Islamabad/Rawalpindi Metropolitan Area. The area would consist of the developing Islamabad, the old colonial cantonment city of Rawalpindi, and Margalla Hills National Park, including surrounding rural areas. However, Islamabad city is part of the Islamabad Capital Territory, while Rawalpindi is part of Rawalpindi District, which is part of province of Punjab.\nEconomy.\nIslamabad is a net contributor to the Pakistani economy, as whilst having only 0.8% of the country's population, it contributes 1% to the country's GDP. Islamabad Stock Exchange, founded in 1989, is Pakistan's third largest stock exchange after Karachi Stock Exchange and Lahore Stock Exchange. The exchange has 118 members with 104 corporate bodies and 18 individual members. The average daily turnover of the stock exchange is over one million shares. As of 2012, Islamabad LTU (Large Tax Unit) was responsible for Rs 371 billion in tax revenue, which amounts to 20% of all the revenue collected by Federal Board of Revenue.\nIslamabad has seen an expansion in information and communications technology with the addition two Software Technology Parks, which house numerous national and foreign technological and information technology companies. The tech parks are located in Evacuee Trust Complex and Awami Markaz. Awami Markaz houses 36 IT companies while Evacuee Trust house 29 companies. Call centres for foreign companies have been targeted as another significant area of growth, with the government making efforts to reduce taxes by as much as 10% to encourage foreign investments in the information technology sector. Most of Pakistan's state-owned companies like PIA, PTV, PTCL, OGDCL, and Zarai Taraqiati Bank Ltd. are based in Islamabad. Headquarters of all major telecommunication operators such as PTCL, Mobilink, Telenor, Ufone, and China Mobile are located in Islamabad. Being an expensive city, the prices of most of fruits, vegetable and poultry items increased in Islamabad during the year 2015-2020\nTransport.\nAirport.\nIslamabad is connected to major destinations around the world through the Islamabad International Airport. The airport is the largest in Pakistan, handling 9 million passengers per annum. The airport was built at a cost of $400\u00a0million and opened on 3 May 2018, replacing the former Benazir Bhutto International Airport. It is the first greenfield airport in Pakistan with an area of .\nMetrobus.\nThe Rawalpindi-Islamabad Metrobus is a bus rapid transit system that serves the twin cities of Rawalpindi and Islamabad in Pakistan. It uses dedicated bus lanes for all of its route covering 24 bus stations.\nMotorways.\nAll major cities and towns are accessible through regular trains and bus services running mostly from the neighbouring city of Rawalpindi. Lahore, Yarik, Dera Ismail Khan and Peshawar are linked to Islamabad through a network of motorways, which has significantly reduced travelling times between these cities. M-2 Motorway is long and connect Islamabad and Lahore. M-1 Motorway connects Islamabad with Peshawar and is long. Islamabad is linked to Rawalpindi through the Faizabad Interchange, which has a daily traffic volume of about 48,000 vehicles. M-14 Motorway connects Islamabad to Dera Ismail Khan and Yarik. It is 285km (177mi) long. \nEducation.\nIslamabad has the highest literacy rate of Pakistan at 95%. Islamabad also has some of Pakistan's major universities, including Quaid-i-Azam University, the International Islamic University, and the National University of Sciences and Technology and Pakistan Institute of Engineering and Applied Sciences\nQuaid-e-Azam University has several faculties. The institute is located in a semi-hilly area, east of the Secretariat buildings and near the base of Margalla Hills. This Post-Graduate institute is spread over . The nucleus of the campus has been designed as an axial spine with a library as its center.\nOther universities include the following:\nSports.\nIslamabad United became the first ever team to win Pakistan Super League in 2016. And now the federal team Is participating in the Pakistan Cup.\nIn 2025, Islamabad United has won 3 Pakistan Super Leagues and earned 3 titles\nThey won PSL: 1, PSL: 3, PSL: 9 which solidifies their number 1 rank as the team with the most victories\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15295", "revid": "22831189", "url": "https://en.wikipedia.org/wiki?curid=15295", "title": "Intelligent design", "text": "Pseudoscientific argument for the existence of God\nIntelligent design (ID) is a pseudoscientific argument for the existence of God, presented by its proponents as \"an evidence-based scientific theory about life's origins\". Proponents claim that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" ID is a form of creationism that lacks empirical support and offers no testable or tenable hypotheses, and is therefore not science. The leading proponents of ID are associated with the Discovery Institute, a Christian, politically conservative think tank based in the United States.\nAlthough the phrase \"intelligent design\" had featured previously in theological discussions of the argument from design, its first publication in its present use as an alternative term for creationism was in \"Of Pandas and People\", a 1989 creationist textbook intended for high school biology classes. The term was substituted into drafts of the book, directly replacing references to \"creation science\" and \"creationism\", after the 1987 Supreme Court's \"Edwards v. Aguillard\" decision barred the teaching of creation science in public schools on constitutional grounds. From the mid-1990s, the intelligent design movement (IDM), supported by the Discovery Institute, advocated inclusion of intelligent design in public school biology curricula. This led to the 2005 \"Kitzmiller v. Dover Area School District\" trial, which found that intelligent design was not science, that it \"cannot uncouple itself from its creationist, and thus religious, antecedents\", and that the public school district's promotion of it therefore violated the Establishment Clause of the First Amendment to the United States Constitution.\nID presents two main arguments against evolutionary explanations: irreducible complexity and specified complexity, asserting that certain biological and informational features of living things are too complex to be the result of natural selection. Detailed scientific examination has rebutted several examples for which evolutionary explanations are claimed to be impossible.\nID seeks to challenge the methodological naturalism inherent in modern science, though proponents concede that they have yet to produce a scientific theory. As a positive argument against evolution, ID proposes an analogy between natural systems and human artifacts, a version of the theological argument from design for the existence of God. ID proponents then conclude by analogy that the complex features, as defined by ID, are evidence of design. Critics of ID find a false dichotomy in the premise that evidence against evolution constitutes evidence for design.\nHistory.\nOrigin of the concept.\nIn 1910, evolution was not a topic of major religious controversy in America, but in the 1920s, the fundamentalist\u2013modernist controversy in theology resulted in fundamentalist Christian opposition to teaching evolution and resulted in the origins of modern creationism. As a result, teaching of evolution was effectively suspended in US public schools until the 1960s, and when evolution was then reintroduced into the curriculum, there was a series of court cases in which attempts were made to get creationism taught alongside evolution in science classes. Young Earth creationists (YECs) promoted creation science as \"an alternative scientific explanation of the world in which we live\". This frequently invoked the argument from design to explain complexity in nature as supposedly demonstrating the existence of God.\nThe argument from design, also known as the teleological argument or \"argument from intelligent design\", has been presented by theologists for centuries. Thomas Aquinas presented ID in his fifth proof of God's existence as a syllogism. In 1802, William Paley's \"Natural Theology\" presented examples of intricate purpose in organisms. His version of the watchmaker analogy argued that a watch has evidently been designed by a craftsman and that it is just as evident that the complexity and adaptation seen in nature must have been designed. He went on to argue that the perfection and diversity of these designs shows the designer to be omnipotent and that this can only be the Christian god. Like creation science, intelligent design centers on Paley's religious argument from design, but while Paley's natural theology was open to deistic design through God-given laws, intelligent design seeks scientific confirmation of repeated supposedly miraculous interventions in the history of life. Creation science prefigured the intelligent design arguments of irreducible complexity, even featuring the bacterial flagellum. In the United States, attempts to introduce creation science into schools led to court rulings that it is religious in nature and thus cannot be taught in public school science classrooms. Intelligent design is also presented as science and shares other arguments with creation science but avoids literal Biblical references to such topics as the biblical flood story or using Bible verses to estimate the age of the Earth.\nBarbara Forrest writes that the intelligent design movement began in 1984 with the book \"The Mystery of Life's Origin: Reassessing Current Theories\", co-written by the creationist and chemist Charles B. Thaxton and two other authors and published by Jon A. Buell's Foundation for Thought and Ethics.\nIn March 1986, Stephen C. Meyer published a review of this book, discussing how information theory could suggest that messages transmitted by DNA in the cell show \"specified complexity\" and must have been created by an intelligent agent. He also argued that science is based upon \"foundational assumptions\" of naturalism that were as much a matter of faith as those of \"creation theory\". In November of that year, Thaxton described his reasoning as a more sophisticated form of Paley's argument from design. At a conference that Thaxton held in 1988 (\"Sources of Information Content in DNA\"), he said that his intelligent cause view was compatible with both metaphysical naturalism and supernaturalism.\nIntelligent design avoids identifying or naming the intelligent designer\u2014it merely states that one (or more) must exist\u2014but leaders of the movement have said the designer is the Christian God. Whether this lack of specificity about the designer's identity in public discussions is a genuine feature of the concept \u2013 or just a posture taken to avoid alienating those who would separate religion from the teaching of science \u2013 has been a matter of great debate between supporters and critics of intelligent design. The Kitzmiller v. Dover Area School District court ruling held the latter to be the case.\nOrigin of the term.\nSince the Middle Ages, discussion of the religious \"argument from design\" or \"teleological argument\" in theology, with its concept of \"intelligent design\", has persistently referred to the theistic Creator God. Although ID proponents chose this provocative label for their proposed alternative to evolutionary explanations, they have de-emphasized their religious antecedents and denied that ID is natural theology, while still presenting ID as supporting the argument for the existence of God.\nWhile intelligent design proponents have pointed out past examples of the phrase \"intelligent design\" that they said were not creationist and faith-based, they have failed to show that these usages had any influence on those who introduced the label in the intelligent design movement.\nVariations on the phrase appeared in Young Earth creationist publications: a 1967 book co-written by Percival Davis referred to \"design according to which basic organisms were created\". In 1970, A. E. Wilder-Smith published \"The Creation of Life: A Cybernetic Approach to Evolution\". The book defended Paley's design argument with computer calculations of the improbability of genetic sequences, which he said could not be explained by evolution but required \"the abhorred necessity of divine intelligent activity behind nature\", and that \"the same problem would be expected to beset the relationship between the designer behind nature and the intelligently designed part of nature known as man.\" In a 1984 article as well as in his affidavit to \"Edwards v. Aguillard\", Dean H. Kenyon defended creation science by stating that \"biomolecular systems require intelligent design and engineering know-how\", citing Wilder-Smith. Creationist Richard B. Bliss used the phrase \"creative design\" in \"Origins: Two Models: Evolution, Creation\" (1976), and in \"Origins: Creation or Evolution\" (1988) wrote that \"while evolutionists are trying to find non-intelligent ways for life to occur, the creationist insists that an intelligent design must have been there in the first place.\"\n\"Of Pandas and People\".\nThe most common modern use of the words \"intelligent design\" as a term intended to describe a field of inquiry began after the United States Supreme Court ruled in June 1987 in the case of \"Edwards v. Aguillard\" that it is unconstitutional for a state to require the teaching of creationism in public school science curricula.\nA Discovery Institute report says that Charles B. Thaxton, editor of \"Pandas\", had picked the phrase up from a NASA scientist. In two successive 1987 drafts of the book, over one hundred uses of the root word \"creation\", such as \"creationism\" and \"Creation Science\", were changed, almost without exception, to \"intelligent design\", while \"creationists\" was changed to \"design proponents\" or, in one instance, \"cdesign proponentsists\" [\"sic\"]. In June 1988, Thaxton held a conference titled \"Sources of Information Content in DNA\" in Tacoma, Washington. Stephen C. Meyer was at the conference, and later recalled that \"The term \"intelligent design\" came up...\" In December 1988 Thaxton decided to use the label \"intelligent design\" for his new creationist movement.\n\"Of Pandas and People\" was published in 1989, and in addition to including all the current arguments for ID, was the first book to make systematic use of the terms \"intelligent design\" and \"design proponents\" as well as the phrase \"design theory\", defining the term \"intelligent design\" in a glossary and representing it as not being creationism. It thus represents the start of the modern intelligent design movement. \"Intelligent design\" was the most prominent of around fifteen new terms it introduced as a new lexicon of creationist terminology to oppose evolution without using religious language. It was the first place where the phrase \"intelligent design\" appeared in its primary present use, as stated both by its publisher Jon A. Buell, and by William A. Dembski in his expert witness report for \"Kitzmiller v. Dover Area School District\".\nThe National Center for Science Education (NCSE) has criticized the book for presenting all of the basic arguments of intelligent design proponents and being actively promoted for use in public schools before any research had been done to support these arguments. Although presented as a scientific textbook, philosopher of science Michael Ruse considers the contents \"worthless and dishonest\". An American Civil Liberties Union lawyer described it as a political tool aimed at students who did not \"know science or understand the controversy over evolution and creationism\". One of the authors of the science framework used by California schools, Kevin Padian, condemned it for its \"sub-text\", \"intolerance for honest science\" and \"incompetence\".\nConcepts.\nIrreducible complexity.\nThe term \"irreducible complexity\" was introduced by biochemist Michael Behe in his 1996 book \"Darwin's Black Box\", though he had already described the concept in his contributions to the 1993 revised edition of \"Of Pandas and People\". Behe defines it as \"a single system which is composed of several well-matched interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning\".\nBehe uses the analogy of a mousetrap to illustrate this concept. A mousetrap consists of several interacting pieces\u2014the base, the catch, the spring and the hammer\u2014all of which must be in place for the mousetrap to work. Removal of any one piece destroys the function of the mousetrap. Intelligent design advocates assert that natural selection could not create irreducibly complex systems, because the selectable function is present only when all parts are assembled. Behe argued that irreducibly complex biological mechanisms include the bacterial flagellum of \"E. coli\", the blood clotting cascade, cilia, and the adaptive immune system.\nCritics point out that the irreducible complexity argument assumes that the necessary parts of a system have always been necessary and therefore could not have been added sequentially. They argue that something that is at first merely advantageous can later become necessary as other components change. Furthermore, they argue, evolution often proceeds by altering preexisting parts or by removing them from a system, rather than by adding them. This is sometimes called the \"scaffolding objection\" by an analogy with scaffolding, which can support an \"irreducibly complex\" building until it is complete and able to stand on its own.\nIn the case of Behe's mousetrap analogy, it has been shown that a mousetrap can be created with increasingly fewer parts and that even a single part is sufficient.\nBehe has acknowledged using \"sloppy prose\", and that his \"argument against Darwinism does not add up to a logical proof.\" Irreducible complexity has remained a popular argument among advocates of intelligent design; in the Dover trial, the court held that \"Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large.\"\nSpecified complexity.\nIn 1986, Charles B. Thaxton, a physical chemist and creationist, used the term \"specified complexity\" from information theory when claiming that messages transmitted by DNA in the cell were specified by intelligence, and must have originated with an intelligent agent.\nThe intelligent design concept of \"specified complexity\" was developed in the 1990s by mathematician, philosopher, and theologian William A. Dembski. Dembski states that when something exhibits specified complexity (i.e., is both complex and \"specified\", simultaneously), one can infer that it was produced by an intelligent cause (i.e., that it was designed) rather than being the result of natural processes. He provides the following examples: \"A single letter of the alphabet is specified without being complex. A long sentence of random letters is complex without being specified. A Shakespearean sonnet is both complex and specified.\" He states that details of living things can be similarly characterized, especially the \"patterns\" of molecular sequences in functional biological molecules such as DNA.\nDembski defines complex specified information (CSI) as anything with a less than 1 in 10150 chance of occurring by (natural) chance. Critics say that this renders the argument a tautology: complex specified information cannot occur naturally because Dembski has defined it thus, so the real question becomes whether or not CSI actually exists in nature.\nThe conceptual soundness of Dembski's specified complexity/CSI argument has been discredited in the scientific and mathematical communities. Specified complexity has yet to be shown to have wide applications in other fields, as Dembski asserts. John Wilkins and Wesley R. Elsberry characterize Dembski's \"explanatory filter\" as \"eliminative\" because it eliminates explanations sequentially: first regularity, then chance, finally defaulting to design. They argue that this procedure is flawed as a model for scientific inference because the asymmetric way it treats the different possible explanations renders it prone to making false conclusions.\nRichard Dawkins, evolutionary biologist and religion critic, argues in \"The God Delusion\" (2006) that allowing for an intelligent designer to account for unlikely complexity only postpones the problem, as such a designer would need to be at least as complex. Other scientists have argued that evolution through selection is better able to explain the observed complexity, as is evident from the use of selective evolution to design certain electronic, aeronautic and automotive systems that are considered problems too complex for human \"intelligent designers\".\nFine-tuned universe.\nIntelligent design proponents have also occasionally appealed to broader teleological arguments outside of biology, most notably an argument based on the fine-tuning of universal constants that make matter and life possible and that are argued not to be solely attributable to chance. These include the values of fundamental physical constants, the relative strength of nuclear forces, electromagnetism, and gravity between fundamental particles, as well as the ratios of masses of such particles. Intelligent design proponent and Center for Science and Culture fellow Guillermo Gonzalez argues that if any of these values were even slightly different, the universe would be dramatically different, making it impossible for many chemical elements and features of the Universe, such as galaxies, to form. Thus, proponents argue, an intelligent designer of life was needed to ensure that the requisite features were present to achieve that particular outcome.\nScientists have generally responded that these arguments are poorly supported by existing evidence. Victor J. Stenger and other critics say both intelligent design and the weak form of the anthropic principle are essentially a tautology; in his view, these arguments amount to the claim that life is able to exist because the Universe is able to support life. The claim of the improbability of a life-supporting universe has also been criticized as an argument by lack of imagination for assuming no other forms of life are possible: life as we know it might not exist if things were different, but a different sort of life might exist in its place. A number of critics also suggest that many of the stated variables appear to be interconnected and that calculations made by mathematicians and physicists suggest that the emergence of a universe similar to ours is quite probable.\nIntelligent designer.\nThe contemporary intelligent design movement formulates its arguments in secular terms and intentionally avoids identifying the intelligent agent (or agents) they posit. Although they do not state that God is the designer, the designer is often implicitly hypothesized to have intervened in a way that only a god could intervene. Dembski, in \"The Design Inference\" (1998), speculates that an alien culture could fulfill these requirements. \"Of Pandas and People\" proposes that the search for extraterrestrial intelligence illustrates an appeal to intelligent design in science. In 2000, philosopher of science Robert T. Pennock suggested the Ra\u00eblian UFO religion as a real-life example of an extraterrestrial intelligent designer view that \"make[s] many of the same bad arguments against evolutionary theory as creationists\". The authoritative description of intelligent design, however, explicitly states that the \"Universe\" displays features of having been designed. Acknowledging the paradox, Dembski concludes that \"no intelligent agent who is strictly physical could have presided over the origin of the universe or the origin of life.\" The leading proponents have made statements to their supporters that they believe the designer to be the Christian God, to the exclusion of all other religions.\nBeyond the debate over whether intelligent design is scientific, a number of critics argue that existing evidence makes the design hypothesis appear unlikely, irrespective of its status in the world of science. For example, Jerry Coyne asks why a designer would \"give us a pathway for making vitamin C, but then destroy it by disabling one of its enzymes\" (see pseudogene) and why a designer would not \"stock oceanic islands with reptiles, mammals, amphibians, and freshwater fish, despite the suitability of such islands for these species\". Coyne also points to the fact that \"the flora and fauna on those islands resemble that of the nearest mainland, even when the environments are very different\" as evidence that species were not placed there by a designer. Previously, in \"Darwin's Black Box\", Behe had argued that we are simply incapable of understanding the designer's motives, so such questions cannot be answered definitively. Odd designs could, for example, \"...have been placed there by the designer for a reason\u2014for artistic reasons, for variety, to show off, for some as-yet-undetected practical purpose, or for some unguessable reason\u2014or they might not.\" Coyne responds that in light of the evidence, \"either life resulted not from intelligent design, but from evolution; or the intelligent designer is a cosmic prankster who designed everything to make it look as though it had evolved.\"\nIntelligent design proponents such as Paul Nelson avoid the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design. Behe cites Paley as his inspiration, but he differs from Paley's expectation of a perfect Creation and proposes that designers do not necessarily produce the best design they can. Behe suggests that, like a parent not wanting to spoil a child with extravagant toys, the designer can have multiple motives for not giving priority to excellence in engineering. He says that \"Another problem with the argument from imperfection is that it critically depends on a psychoanalysis of the unidentified designer. Yet the reasons that a designer would or would not do anything are virtually impossible to know unless the designer tells you specifically what those reasons are.\" This reliance on inexplicable motives of the designer makes intelligent design scientifically untestable. Retired UC Berkeley law professor, author and intelligent design advocate Phillip E. Johnson puts forward a core definition that the designer creates for a purpose, giving the example that in his view AIDS was created to punish immorality and is not caused by HIV, but such motives cannot be tested by scientific methods.\nAsserting the need for a designer of complexity also raises the question \"What designed the designer?\" Intelligent design proponents say that the question is irrelevant to or outside the scope of intelligent design. Richard Wein counters that \"...scientific explanations often create new unanswered questions. But, in assessing the value of an explanation, these questions are not irrelevant. They must be balanced against the improvements in our understanding which the explanation provides. Invoking an unexplained being to explain the origin of other beings (ourselves) is little more than question-begging. The new question raised by the explanation is as problematic as the question which the explanation purports to answer.\" Richard Dawkins sees the assertion that the designer does not need to be explained as a thought-terminating clich\u00e9. In the absence of observable, measurable evidence, the question \"What designed the designer?\" leads to an infinite regression from which intelligent design proponents can only escape by resorting to religious creationism or logical contradiction.\nMovement.\nThe intelligent design movement is a direct outgrowth of the creationism of the 1980s. The scientific and academic communities, along with a US federal court, view intelligent design as either a form of creationism or as a direct descendant that is closely intertwined with traditional creationism; and several authors explicitly refer to it as \"intelligent design creationism\".\nThe movement is headquartered in the Center for Science and Culture, established in 1996 as the creationist wing of the Discovery Institute to promote a religious agenda calling for broad social, academic and political changes. The Discovery Institute's intelligent design campaigns have been staged primarily in the United States, although efforts have been made in other countries to promote intelligent design. Leaders of the movement say intelligent design exposes the limitations of scientific orthodoxy and of the secular philosophy of naturalism. Intelligent design proponents allege that science should not be limited to naturalism and should not demand the adoption of a naturalistic philosophy that dismisses out-of-hand any explanation that includes a supernatural cause. The overall goal of the movement is to \"reverse the stifling dominance of the materialist worldview\" represented by the theory of evolution in favor of \"a science consonant with Christian and theistic convictions\".\nPhillip E. Johnson stated that the goal of intelligent design is to cast creationism as a scientific concept. All leading intelligent design proponents are fellows or staff of the Discovery Institute and its Center for Science and Culture. Nearly all intelligent design concepts and the associated movement are the products of the Discovery Institute, which guides the movement and follows its wedge strategy while conducting its \"teach the controversy\" campaign and their other related programs.\nLeading intelligent design proponents have made conflicting statements regarding intelligent design. In statements directed at the general public, they say intelligent design is not religious; when addressing conservative Christian supporters, they state that intelligent design has its foundation in the Bible. Recognizing the need for support, the Institute affirms its Christian, evangelistic orientation:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nBarbara Forrest, an expert who has written extensively on the movement, describes this as being due to the Discovery Institute's obfuscating its agenda as a matter of policy. She has written that the movement's \"activities betray an aggressive, systematic agenda for promoting not only intelligent design creationism, but the religious worldview that undergirds it.\"\nReligion and leading proponents.\nAlthough arguments for intelligent design by the intelligent design movement are formulated in secular terms and intentionally avoid positing the identity of the designer, the majority of principal intelligent design advocates are publicly religious Christians who have stated that, in their view, the designer proposed in intelligent design is the Christian conception of God. Stuart Burgess, Phillip E. Johnson, William A. Dembski, and Stephen C. Meyer are evangelical Protestants; Michael Behe is a Roman Catholic; Paul Nelson supports young Earth creationism; and Jonathan Wells is a member of the Unification Church. Non-Christian proponents include David Klinghoffer, who is Jewish, Michael Denton and David Berlinski, who are agnostic, and Muzaffar Iqbal, a Pakistani-Canadian Muslim. Phillip E. Johnson has stated that cultivating ambiguity by employing secular language in arguments that are carefully crafted to avoid overtones of theistic creationism is a necessary first step for ultimately reintroducing the Christian concept of God as the designer. Johnson explicitly calls for intelligent design proponents to obfuscate their religious motivations so as to avoid having intelligent design identified \"as just another way of packaging the Christian evangelical message.\" Johnson emphasizes that \"...the first thing that has to be done is to get the Bible out of the discussion. ...This is not to say that the biblical issues are unimportant; the point is rather that the time to address them will be after we have separated materialist prejudice from scientific fact.\"\nThe strategy of deliberately disguising the religious intent of intelligent design has been described by William A. Dembski in \"The Design Inference\". In this work, Dembski lists a god or an \"alien life force\" as two possible options for the identity of the designer; however, in his book \"Intelligent Design: The Bridge Between Science and Theology\" (1999), Dembski states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Christ is indispensable to any scientific theory, even if its practitioners don't have a clue about him. The pragmatics of a scientific theory can, to be sure, be pursued without recourse to Christ. But the conceptual soundness of the theory can in the end only be located in Christ.\nDembski also stated, \"ID is part of God's general revelation ... Not only does intelligent design rid us of this ideology [materialism], which suffocates the human spirit, but, in my personal experience, I've found that it opens the path for people to come to Christ.\" Both Johnson and Dembski cite the Bible's Gospel of John as the foundation of intelligent design.\nBarbara Forrest contends such statements reveal that leading proponents see intelligent design as essentially religious in nature, not merely a scientific concept that has implications with which their personal religious beliefs happen to coincide. She writes that the leading proponents of intelligent design are closely allied with the ultra-conservative Christian Reconstructionism movement. She lists connections of (current and former) Discovery Institute Fellows Phillip E. Johnson, Charles B. Thaxton, Michael Behe, Richard Weikart, Jonathan Wells and Francis J. Beckwith to leading Christian Reconstructionist organizations, and the extent of the funding provided the Institute by Howard Ahmanson, Jr., a leading figure in the Reconstructionist movement.\nReaction from other creationist groups.\nNot all creationist organizations have embraced the intelligent design movement. According to Thomas Dixon, \"Religious leaders have come out against ID too. An open letter affirming the compatibility of Christian faith and the teaching of evolution, first produced in response to controversies in Wisconsin in 2004, has now been signed by over ten thousand clergy from different Christian denominations across America.\" Hugh Ross of Reasons to Believe, a proponent of Old Earth creationism, believes that the efforts of intelligent design proponents to divorce the concept from Biblical Christianity make its hypothesis too vague. In 2002, he wrote: \"Winning the argument for design without identifying the designer yields, at best, a sketchy origins model. Such a model makes little if any positive impact on the community of scientists and other scholars. ... the time is right for a direct approach, a single leap into the origins fray. Introducing a biblically based, scientifically verifiable creation model represents such a leap.\"\nLikewise, two of the most prominent YEC organizations in the world have attempted to distinguish their views from those of the intelligent design movement. Henry M. Morris of the Institute for Creation Research (ICR) wrote, in 1999, that ID, \"even if well-meaning and effectively articulated, will not work! It has often been tried in the past and has failed, and it will fail today. The reason it won't work is because it is not the Biblical method.\" According to Morris: \"The evidence of intelligent design ... must be either followed by or accompanied by a sound presentation of true Biblical creationism if it is to be meaningful and lasting.\" In 2002, Carl Wieland, then of Answers in Genesis (AiG), criticized design advocates who, though well-intentioned, \"'left the Bible out of it'\" and thereby unwittingly aided and abetted the modern rejection of the Bible. Wieland explained that \"AiG's major 'strategy' is to boldly, but humbly, call the church back to its Biblical foundations ... [so] we neither count ourselves a part of this movement nor campaign against it.\"\nReaction from the scientific community.\nThe unequivocal consensus in the scientific community is that intelligent design is not science and has no place in a science curriculum. The US National Academy of Sciences has stated that \"creationism, intelligent design, and other claims of supernatural intervention in the origin of life or of species are not science because they are not testable by the methods of science.\" The US National Science Teachers Association and the American Association for the Advancement of Science have termed it pseudoscience. Others in the scientific community have denounced its tactics, accusing the ID movement of manufacturing false attacks against evolution, of engaging in misinformation and misrepresentation about science, and marginalizing those who teach it. More recently, in September 2012, Bill Nye warned that creationist views threaten science education and innovations in the United States.\nIn 2001, the Discovery Institute published advertisements under the heading \"A Scientific Dissent From Darwinism\", with the claim that listed scientists had signed this statement expressing skepticism:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We are skeptical of claims for the ability of random mutation and natural selection to account for the complexity of life. Careful examination of the evidence for Darwinian theory should be encouraged.\nThe ambiguous statement did not exclude other known evolutionary mechanisms, and most signatories were not scientists in relevant fields, but starting in 2004 the Institute claimed the increasing number of signatures indicated mounting doubts about evolution among scientists. The statement formed a key component of Discovery Institute campaigns to present intelligent design as scientifically valid by claiming that evolution lacks broad scientific support, with Institute members continuing to cite the list through at least 2011. As part of a strategy to counter these claims, scientists organised Project Steve, which gained more signatories named Steve (or variants) than the Institute's petition, and a counter-petition, \"A Scientific Support for Darwinism\", which quickly gained similar numbers of signatories.\nPolls.\nSeveral surveys were conducted prior to the December 2005 decision in \"Kitzmiller v. Dover School District\", which sought to determine the level of support for intelligent design among certain groups. According to a 2005 Harris poll, 10% of adults in the United States viewed human beings as \"so complex that they required a powerful force or intelligent being to help create them.\" Although Zogby polls commissioned by the Discovery Institute show more support, these polls suffer from considerable flaws, such as having a low response rate (248 out of 16,000), being conducted on behalf of an organization with an expressed interest in the outcome of the poll, and containing leading questions.\nA series of Gallup polls in the United States from 1982 through 2014 on \"Evolution, Creationism, Intelligent Design\" found support for \"human beings have developed over millions of years from less advanced forms of life, but God guided the process\" of between 31% and 40%, support for \"God created human beings in pretty much their present form at one time within the last 10,000 years or so\" varied from 40% to 47%, and support for \"human beings have developed over millions of years from less advanced forms of life, but God had no part in the process\" varied from 9% to 19%. The polls also noted answers to a series of more detailed questions.\nThe 2017 Gallup creationism survey found that 38% of adults in the United States hold the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years.\nAllegations of discrimination against ID proponents.\nThere have been allegations that ID proponents have met discrimination, such as being refused tenure or being harshly criticized on the Internet. In the documentary film \"\", released in 2008, host Ben Stein presents five such cases. The film contends that the mainstream science establishment, in a \"scientific conspiracy to keep God out of the nation's laboratories and classrooms\", suppresses academics who believe they see evidence of intelligent design in nature or criticize evidence of evolution. Investigation into these allegations turned up alternative explanations for perceived persecution.\nThe film portrays intelligent design as motivated by science, rather than religion, though it does not give a detailed definition of the phrase or attempt to explain it on a scientific level. Other than briefly addressing issues of irreducible complexity, \"Expelled\" examines it as a political issue. The scientific theory of evolution is portrayed by the film as contributing to fascism, the Holocaust, communism, atheism, and eugenics.\n\"Expelled\" has been used in private screenings to legislators as part of the Discovery Institute intelligent design campaign for Academic Freedom bills. Review screenings were restricted to churches and Christian groups, and at a special pre-release showing, one of the interviewees, PZ Myers, was refused admission. The American Association for the Advancement of Science describes the film as dishonest and divisive propaganda aimed at introducing religious ideas into public school science classrooms, and the Anti-Defamation League has denounced the film's allegation that evolutionary theory influenced the Holocaust. The film includes interviews with scientists and academics who were misled into taking part by misrepresentation of the topic and title of the film. Skeptic Michael Shermer describes his experience of being repeatedly asked the same question without context as \"surreal\".\nCriticism.\nScientific criticism.\nAdvocates of intelligent design seek to keep God and the Bible out of the discussion, and present intelligent design in the language of science as though it were a scientific hypothesis. For a theory to qualify as scientific, it is expected to be:\nFor any theory, hypothesis, or conjecture to be considered scientific, it must meet most, and ideally all, of these criteria. The fewer criteria are met, the less scientific it is; if it meets only a few or none at all, then it cannot be treated as scientific in any meaningful sense of the word. Typical objections to defining intelligent design as science are that it lacks consistency, violates the principle of parsimony, is not scientifically useful, is not falsifiable, is not empirically testable, and is not correctable, dynamic, progressive, or provisional.\nIntelligent design proponents seek to change this fundamental basis of science by eliminating \"methodological naturalism\" from science and replacing it with what the leader of the intelligent design movement, Phillip E. Johnson, calls \"theistic realism\". Intelligent design proponents argue that naturalistic explanations fail to explain certain phenomena and that supernatural explanations provide a simple and intuitive explanation for the origins of life and the universe. Many intelligent design followers believe that \"scientism\" is itself a religion that promotes secularism and materialism in an attempt to erase theism from public life, and they view their work in the promotion of intelligent design as a way to return religion to a central role in education and other public spheres.\nIt has been argued that methodological naturalism is not an \"assumption\" of science, but a \"result\" of science well done: the God explanation is the least parsimonious, so according to Occam's razor, it cannot be a scientific explanation.\nThe failure to follow the procedures of scientific discourse and the failure to submit work to the scientific community that withstands scrutiny have weighed against intelligent design being accepted as valid science. The intelligent design movement has not published a properly peer-reviewed article supporting ID in a scientific journal, and has failed to publish supporting peer-reviewed research or data. The only article published in a peer-reviewed scientific journal that made a case for intelligent design was quickly withdrawn by the publisher for having circumvented the journal's peer-review standards. The Discovery Institute says that a number of intelligent design articles have been published in peer-reviewed journals, but critics, largely members of the scientific community, reject this claim and state intelligent design proponents have set up their own journals with peer review that lack impartiality and rigor, consisting entirely of intelligent design supporters.\nFurther criticism stems from the fact that the phrase \"intelligent\" design makes use of an assumption of the quality of an observable intelligence, a concept that has no scientific consensus definition. The characteristics of intelligence are assumed by intelligent design proponents to be observable without specifying what the criteria for the measurement of intelligence should be. Critics say that the design detection methods proposed by intelligent design proponents are radically different from conventional design detection, undermining the key elements that make it possible as legitimate science. Intelligent design proponents, they say, are proposing both searching for a designer without knowing anything about that designer's abilities, parameters, or intentions (which scientists do know when searching for the results of human intelligence), as well as denying the distinction between natural/artificial design that allows scientists to compare complex designed artifacts against the background of the sorts of complexity found in nature.\nAmong a significant proportion of the general public in the United States, the major concern is whether conventional evolutionary biology is compatible with belief in God and in the Bible, and how this issue is taught in schools. The Discovery Institute's \"teach the controversy\" campaign promotes intelligent design while attempting to discredit evolution in United States public high school science courses. The scientific community and science education organizations have replied that there is no scientific controversy regarding the validity of evolution and that the controversy exists solely in terms of religion and politics.\nArguments from ignorance.\nEugenie C. Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent design proponents are arguments from ignorance.\nIn the argument from ignorance, a lack of evidence for one view is erroneously argued to constitute proof of the correctness of another view. Scott and Branch say that intelligent design is an argument from ignorance because it relies on a lack of knowledge for its conclusion: lacking a natural explanation for certain specific aspects of evolution, we assume intelligent cause. They contend most scientists would reply that the unexplained is not unexplainable, and that \"we don't know yet\" is a more appropriate response than invoking a cause outside science. Particularly, Michael Behe's demands for ever more detailed explanations of the historical evolution of molecular systems seem to assume a false dichotomy, where either evolution or design is the proper explanation, and any perceived failure of evolution becomes a victory for design. Scott and Branch also contend that the supposedly novel contributions proposed by intelligent design proponents have not served as the basis for any productive scientific research.\nMoreover, philosophical criticism of intelligent design has extended beyond traditional demarcation arguments that attempt to distinguish science from non-science. Some philosophers have argued that intelligent design faces a more fundamental problem of intelligibility when presented as a non-theological doctrine. According to this view, intelligent design cannot be coherently characterized without theological commitments, making it conceptually incoherent rather than merely unscientific. This philosophical position suggests that demarcation criteria\u2014which have themselves been subject to philosophical controversy\u2014are unnecessary for rejecting intelligent design, as the doctrine fails to achieve logical coherence when stripped of its theological foundations. Critics argue this approach provides a more robust philosophical foundation for dismissing intelligent design without relying on potentially contested boundaries between scientific and non-scientific inquiry.\nIn his conclusion to the Kitzmiller trial, Judge John E. Jones III wrote that \"ID is at bottom premised upon a false dichotomy, namely, that to the extent evolutionary theory is discredited, ID is confirmed.\" This same argument had been put forward to support creation science at the \"McLean v. Arkansas\" (1982) trial, which found it was \"contrived dualism\", the false premise of a \"two model approach\". Behe's argument of irreducible complexity puts forward negative arguments against evolution but does not make any positive scientific case for intelligent design. It fails to allow for scientific explanations continuing to be found, as has been the case with several examples previously put forward as supposed cases of irreducible complexity.\nPossible theological implications.\nIntelligent design proponents often insist that their claims do not require a religious component. However, various philosophical and theological issues are naturally raised by the claims of intelligent design.\nIntelligent design proponents attempt to demonstrate scientifically that features such as irreducible complexity and specified complexity could not arise through natural processes, and therefore required repeated direct miraculous interventions by a Designer (often a Christian concept of God). They reject the possibility of a Designer who works merely through setting natural laws in motion at the outset, in contrast to theistic evolution (to which even Charles Darwin was open). Intelligent design is distinct because it asserts repeated miraculous interventions in addition to designed laws. This contrasts with other major religious traditions of a created world in which God's interactions and influences do not work in the same way as physical causes. The Roman Catholic tradition makes a careful distinction between ultimate metaphysical explanations and secondary, natural causes.\nThe concept of direct miraculous intervention raises other potential theological implications. If such a Designer does not intervene to alleviate suffering even though capable of intervening for other reasons, some imply the designer is not omnibenevolent (see problem of evil and related theodicy).\nFurther, repeated interventions imply that the original design was not perfect and final, and thus pose a problem for any who believe that the Creator's work had been both perfect and final. Intelligent design proponents seek to explain the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design (for example, proposing that vestigial organs have unknown purposes), or by proposing that designers do not necessarily produce the best design they can, and may have unknowable motives for their actions.\nIn 2005, the director of the Vatican Observatory, the Jesuit astronomer George Coyne, set out theological reasons for accepting evolution in an August 2005 article in \"The Tablet\", and said that \"Intelligent design isn't science even though it pretends to be. If you want to teach it in schools, intelligent design should be taught when religion or cultural history is taught, not science.\" In 2006, he \"condemned ID as a kind of 'crude creationism' which reduced God to a mere engineer.\"\nCritics state that the wedge strategy's \"ultimate goal is to create a theocratic state\".\nGod of the gaps.\nIntelligent design has also been characterized as a God-of-the-gaps argument, which has the following form:\nA God-of-the-gaps argument is the theological version of an argument from ignorance. A key feature of this type of argument is that it merely answers outstanding questions with explanations (often supernatural) that are unverifiable and ultimately themselves subject to unanswerable questions. Historians of science observe that the astronomy of the earliest civilizations, although astonishing and incorporating mathematical constructions far in excess of any practical value, proved to be misdirected and of little importance to the development of science because they failed to inquire more carefully into the mechanisms that drove the heavenly bodies across the sky. It was the Greek civilization that first practiced science, although not yet as a formally defined experimental science, but nevertheless an attempt to rationalize the world of natural experience without recourse to divine intervention. In this historically motivated definition of science any appeal to an intelligent creator is explicitly excluded for the paralysing effect it may have on scientific progress.\nLegal challenges in the United States.\nKitzmiller trial.\n\"Kitzmiller v. Dover Area School District\" was the first direct challenge brought in the United States federal courts against a public school district that required the presentation of intelligent design as an alternative to evolution. The plaintiffs successfully argued that intelligent design is a form of creationism, and that the school board policy thus violated the Establishment Clause of the First Amendment to the United States Constitution.\nEleven parents of students in Dover, Pennsylvania, sued the Dover Area School District over a statement that the school board required be read aloud in ninth-grade science classes when evolution was taught. The plaintiffs were represented by the American Civil Liberties Union (ACLU), Americans United for Separation of Church and State (AU) and Pepper Hamilton LLP. The National Center for Science Education acted as consultants for the plaintiffs. The defendants were represented by the Thomas More Law Center. The suit was tried in a bench trial from September 26 to November 4, 2005, before Judge John E. Jones III. Kenneth R. Miller, Kevin Padian, Brian Alters, Robert T. Pennock, Barbara Forrest and John F. Haught served as expert witnesses for the plaintiffs. Michael Behe, Steve Fuller and Scott Minnich served as expert witnesses for the defense.\nOn December 20, 2005, Judge Jones issued his 139-page findings of fact and decision, ruling that the Dover mandate was unconstitutional, and barring intelligent design from being taught in Pennsylvania's Middle District public school science classrooms. On November 8, 2005, there had been an election in which the eight Dover school board members who voted for the intelligent design requirement were all defeated by challengers who opposed the teaching of intelligent design in a science class, and the current school board president stated that the board did not intend to appeal the ruling.\nIn his finding of facts, Judge Jones made the following condemnation of the \"Teach the Controversy\" strategy:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Moreover, ID's backers have sought to avoid the scientific scrutiny which we have now determined that it cannot withstand by advocating that the \"controversy\", but not ID itself, should be taught in science class. This tactic is at best disingenuous, and at worst a . The goal of the IDM is not to encourage critical thought, but to foment a revolution which would supplant evolutionary theory with ID.\nReaction to Kitzmiller ruling.\nJudge Jones himself anticipated that his ruling would be criticized, saying in his decision that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Those who disagree with our holding will likely mark it as the product of an activist judge. If so, they will have erred as this is manifestly not an activist Court.\nRather, this case came to us as the result of the activism of an ill-informed faction on a school board, aided by a national public interest law firm eager to find a constitutional test case on ID, who in combination drove the Board to adopt an imprudent and ultimately unconstitutional policy. The breathtaking inanity of the Board's decision is evident when considered against the factual backdrop which has now been fully revealed through this trial. The students, parents, and teachers of the Dover Area School District deserved better than to be dragged into this legal maelstrom, with its resulting utter waste of monetary and personal resources.\nAs Jones had predicted, John G. West, Associate Director of the Center for Science and Culture, said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Dover decision is an attempt by an activist federal judge to stop the spread of a scientific idea and even to prevent criticism of Darwinian evolution through government-imposed censorship rather than open debate, and it won't work. He has conflated Discovery Institute's position with that of the Dover school board, and he totally misrepresents intelligent design and the motivations of the scientists who research it.\nNewspapers have noted that the judge is \"a Republican and a churchgoer\".\nThe decision has been examined in a search for flaws and conclusions, partly by intelligent design supporters aiming to avoid future defeats in court. In its Winter issue of 2007, the \"Montana Law Review\" published three articles.\nIn the first, David K. DeWolf, John G. West and Casey Luskin, all of the Discovery Institute, argued that intelligent design is a valid scientific theory, the Jones court should not have addressed the question of whether it was a scientific theory, and that the Kitzmiller decision will have no effect at all on the development and adoption of intelligent design as an alternative to standard evolutionary theory. In the second Peter H. Irons responded, arguing that the decision was extremely well reasoned and spells the death knell for the intelligent design efforts to introduce creationism in public schools, while in the third, DeWolf, \"et al.\", answer the points made by Irons. However, fear of a similar lawsuit has resulted in other school boards abandoning intelligent design \"teach the controversy\" proposals.\nAnti-evolution legislation.\nA number of anti-evolution bills have been introduced in the United States Congress and State legislatures since 2001, based largely upon language drafted by the Discovery Institute for the Santorum Amendment. Their aim has been to expose more students to articles and videos produced by advocates of intelligent design that criticise evolution. They have been presented as supporting \"academic freedom\", on the supposition that teachers, students, and college professors face intimidation and retaliation when discussing scientific criticisms of evolution, and therefore require protection. Critics of the legislation have pointed out that there are no credible scientific critiques of evolution, and an investigation in Florida of allegations of intimidation and retaliation found no evidence that it had occurred. The vast majority of the bills have been unsuccessful, with the one exception being Louisiana's Louisiana Science Education Act, which was enacted in 2008.\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K\u201312 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, these worldviews as well as others \"that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\nStatus outside the United States.\nEurope.\nIn June 2007, the Council of Europe's Committee on Culture, Science and Education issued a report, \"The dangers of creationism in education\", which states \"Creationism in any of its forms, such as 'intelligent design', is not based on facts, does not use any scientific reasoning and its contents are pathetically inadequate for science classes.\" In describing the dangers posed to education by teaching creationism, it described intelligent design as \"anti-science\" and involving \"blatant scientific fraud\" and \"intellectual deception\" that \"blurs the nature, objectives and limits of science\" and links it and other forms of creationism to denialism. On October 4, 2007, the Council of Europe's Parliamentary Assembly approved a resolution stating that schools should \"resist presentation of creationist ideas in any discipline other than religion\", including \"intelligent design\", which it described as \"the latest, more refined version of creationism\", \"presented in a more subtle way\". The resolution emphasises that the aim of the report is not to question or to fight a belief, but to \"warn against certain tendencies to pass off a belief as science\".\nIn the United Kingdom, public education includes religious education, and there are many faith schools that teach the ethos of particular denominations. When it was revealed that a group called Truth in Science had distributed DVDs produced by Illustra Media featuring Discovery Institute fellows making the case for design in nature, and claimed they were being used by 59 schools, the Department for Education and Skills (DfES) stated that \"Neither creationism nor intelligent design are taught as a subject in schools, and are not specified in the science curriculum\" (part of the National Curriculum, which does not apply to private schools or to education in Scotland). The DfES subsequently stated that \"Intelligent design is not a recognised scientific theory; therefore, it is not included in the science curriculum\", but left the way open for it to be explored in religious education in relation to different beliefs, as part of a syllabus set by a local Standing Advisory Council on Religious Education. In 2006, the Qualifications and Curriculum Authority produced a \"Religious Education\" model unit in which pupils can learn about religious and nonreligious\nviews about creationism, intelligent design and evolution by natural selection.\nOn June 25, 2007, the UK Government responded to an e-petition by saying that creationism and intelligent design should not be taught as science, though teachers would be expected to answer pupils' questions within the standard framework of established scientific theories. Detailed government \"Creationism teaching guidance\" for schools in England was published on September 18, 2007. It states that \"Intelligent design lies wholly outside of science\", has no underpinning scientific principles, or explanations, and is not accepted by the science community as a whole. Though it should not be taught as science, \"Any questions about creationism and intelligent design which arise in science lessons, for example as a result of media coverage, could provide the opportunity to explain or explore why they are not considered to be scientific theories and, in the right context, why evolution is considered to be a scientific theory.\" However, \"Teachers of subjects such as RE, history or citizenship may deal with creationism and intelligent design in their lessons.\"\nThe British Centre for Science Education lobbying group has the goal of \"countering creationism within the UK\" and has been involved in government lobbying in the UK in this regard. Northern Ireland's Department for Education says that the curriculum provides an opportunity for alternative theories to be taught. The Democratic Unionist Party (DUP)\u00a0\u2013 which has links to fundamentalist Christianity\u00a0\u2013 has been campaigning to have intelligent design taught in science classes. A DUP former Member of Parliament, David Simpson, has sought assurances from the education minister that pupils will not lose marks if they give creationist or intelligent design answers to science questions. In 2007, Lisburn city council voted in favor of a DUP recommendation to write to post-primary schools asking what their plans are to develop teaching material in relation to \"creation, intelligent design and other theories of origin\".\nPlans by Dutch Education Minister Maria van der Hoeven to \"stimulate an academic debate\" on the subject in 2005 caused a severe public backlash. After the 2006 elections, she was succeeded by Ronald Plasterk, described as a \"molecular geneticist, staunch atheist and opponent of intelligent design\". As a reaction to this situation in the Netherlands, the Director General of the Flemish Secretariat of Catholic Education (VSKO) in Belgium, Mieke Van Hecke, declared that: \"Catholic scientists already accepted the theory of evolution for a long time and that intelligent design and creationism doesn't belong in Flemish Catholic schools. It's not the tasks of the politics to introduce new ideas, that's task and goal of science.\"\nAustralia.\nThe status of intelligent design in Australia is somewhat similar to that in the UK (see Education in Australia). In 2005, the Australian Minister for Education, Science and Training, Brendan Nelson, raised the notion of intelligent design being taught in science classes. The public outcry caused the minister to quickly concede that the correct forum for intelligent design, if it were to be taught, is in religion or philosophy classes. The Australian chapter of Campus Crusade for Christ distributed a DVD of the Discovery Institute's documentary \"Unlocking the Mystery of Life\" (2002) to Australian secondary schools. Tim Hawkes, the head of The King's School, one of Australia's leading private schools, supported use of the DVD in the classroom at the discretion of teachers and principals.\nRelation to Islam.\nMuzaffar Iqbal, a notable Pakistani-Canadian Muslim, signed \"A Scientific Dissent From Darwinism\", a petition from the Discovery Institute. Ideas similar to intelligent design have been considered respected intellectual options among Muslims, and in Turkey many intelligent design books have been translated. In Istanbul in 2007, public meetings promoting intelligent design were sponsored by the local government, and David Berlinski of the Discovery Institute was the keynote speaker at a meeting in May 2007.\nRelation to ISKCON.\nIn 2011, the International Society for Krishna Consciousness (ISKCON) Bhaktivedanta Book Trust published an intelligent design book titled \"Rethinking Darwin: A Vedic Study of Darwinism and Intelligent Design\". The book included contributions from intelligent design advocates William A. Dembski, Jonathan Wells and Michael Behe as well as from Hindu creationists Leif A. Jensen and Michael Cremo.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15301", "revid": "7436027", "url": "https://en.wikipedia.org/wiki?curid=15301", "title": "I think, therefore I am", "text": ""}
{"id": "15302", "revid": "18174831", "url": "https://en.wikipedia.org/wiki?curid=15302", "title": "Integrin", "text": "Instance of a defined set in Homo sapiens with Reactome ID (R-HSA-374573)\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nIntegrins are transmembrane receptors that help cell\u2013cell and cell\u2013extracellular matrix (ECM) adhesion. Upon ligand binding, integrins activate signal transduction pathways that mediate cellular signals such as regulation of the cell cycle, organization of the intracellular cytoskeleton, and movement of new receptors to the cell membrane. The presence of integrins allows rapid and flexible responses to events at the cell surface (\"e.g\". signal platelets to initiate an interaction with coagulation factors).\nSeveral types of integrins exist, and one cell generally has multiple different types on its surface. Integrins are found in all animal cells while integrin-like receptors are found in plant cells.\nIntegrins work alongside other proteins such as cadherins, the immunoglobulin superfamily cell adhesion molecules, selectins and syndecans, to mediate cell\u2013cell and cell\u2013matrix interaction. Ligands for integrins include fibronectin, vitronectin, collagen and laminin.\nStructure.\nIntegrins are obligate heterodimers composed of \u03b1 and \u03b2 subunits. Several genes code for multiple isoforms of these subunits, which gives rise to an array of unique integrins with varied activity. In mammals, integrins are assembled from eighteen \u03b1 and eight \u03b2 subunits, in \"Drosophila\" five \u03b1 and two \u03b2 subunits, and in \"Caenorhabditis\" nematodes two \u03b1 subunits and one \u03b2 subunit. The \u03b1 and \u03b2 subunits are both class I transmembrane proteins, so each penetrates the plasma membrane once, and can possess several cytoplasmic domains.\nVariants of some subunits are formed by differential RNA splicing; for example, four variants of the beta-1 subunit exist. Through different combinations of the \u03b1 and \u03b2 subunits, 24 unique mammalian integrins are generated, excluding splice- and glycosylation variants.\nIntegrin subunits span the cell membrane and have short cytoplasmic domains of 40\u201370 amino acids. The exception is the beta-4 subunit, which has a cytoplasmic domain of 1,088 amino acids, one of the largest of any membrane protein. Outside the cell membrane, the \u03b1 and \u03b2 chains lie close together along a length of about 23\u00a0nm; the final 5\u00a0nm N-termini of each chain forms a ligand-binding region for the ECM. They have been compared to lobster claws, although they don't actually \"pinch\" their ligand, they chemically interact with it at the insides of the \"tips\" of their \"pinchers\".\nThe molecular mass of the integrin subunits can vary from to . Beta subunits have four cysteine-rich repeated sequences. Both \u03b1 and \u03b2 subunits bind several divalent cations. The role of divalent cations in the \u03b1 subunit is unknown, but may stabilize the folds of the protein. The cations in the \u03b2 subunits are more interesting: they are directly involved in coordinating at least some of the ligands that integrins bind.\nIntegrins can be categorized in multiple ways. For example, some \u03b1 chains have an additional structural element (or \"domain\") inserted toward the N-terminal, the alpha-A domain (so called because it has a similar structure to the A-domains found in the protein von Willebrand factor; it is also termed the \u03b1-I domain). Integrins carrying this domain either bind to collagens (e.g. integrins \u03b11 \u03b21, and \u03b12 \u03b21), or act as cell-cell adhesion molecules (integrins of the \u03b22 family). This \u03b1-I domain is the binding site for ligands of such integrins. Those integrins that don't carry this inserted domain also have an A-domain in their ligand binding site, but \"this\" A-domain is found on the \u03b2 subunit.\nIn both cases, the A-domains carry up to three divalent cation binding sites. One is permanently occupied in physiological concentrations of divalent cations, and carries either a calcium or magnesium ion, the principal divalent cations in blood at median concentrations of 1.4\u00a0mM (calcium) and 0.8\u00a0mM (magnesium). The other two sites become occupied by cations when ligands bind\u2014at least for those ligands involving an acidic amino acid in their interaction sites. An acidic amino acid features in the integrin-interaction site of many ECM proteins, for example as part of the amino acid sequence Arginine-Glycine-Aspartic acid (\"RGD\" in the one-letter amino acid code).\nStructure.\nDespite many years of effort, discovering the high-resolution structure of integrins proved to be challenging, as membrane proteins are classically difficult to purify, and as integrins are large, complex and highly glycosylated with many sugar 'trees' attached to them. Low-resolution images of detergent extracts of intact integrin GPIIbIIIa, obtained using electron microscopy, and even data from indirect techniques that investigate the solution properties of integrins using ultracentrifugation and light scattering, were combined with fragmentary high-resolution crystallographic or NMR data from single or paired domains of single integrin chains, and molecular models postulated for the rest of the chains.\nThe X-ray crystal structure obtained for the complete extracellular region of one integrin, \u03b1v\u03b23, shows the molecule to be folded into an inverted V-shape that potentially brings the ligand-binding sites close to the cell membrane. Perhaps more importantly, the crystal structure was also obtained for the same integrin bound to a small ligand containing the RGD-sequence, the drug cilengitide. As detailed above, this finally revealed why divalent cations (in the A-domains) are critical for RGD-ligand binding to integrins. The interaction of such sequences with integrins is believed to be a primary switch by which ECM exerts its effects on cell behaviour.\nThe structure poses many questions, especially regarding ligand binding and signal transduction. The ligand binding site is directed towards the C-terminal of the integrin, the region where the molecule emerges from the cell membrane. If it emerges orthogonally from the membrane, the ligand binding site would apparently be obstructed, especially as integrin ligands are typically massive and well cross-linked components of the ECM. In fact, little is known about the angle that membrane proteins subtend to the plane of the membrane; this is a problem difficult to address with available technologies. The default assumption is that they emerge rather like little lollipops, but there is little evidence for this. The integrin structure has drawn attention to this problem, which may have general implications for how membrane proteins work. It appears that the integrin transmembrane helices are tilted (see \"Activation\" below), which hints that the extracellular chains may also not be orthogonal with respect to the membrane surface.\nAlthough the crystal structure changed surprisingly little after binding to cilengitide, the current hypothesis is that integrin function involves changes in shape to move the ligand-binding site into a more accessible position, away from the cell surface, and this shape change also triggers intracellular signaling. There is a wide body of cell-biological and biochemical literature that supports this view. Perhaps the most convincing evidence involves the use of antibodies that only recognize integrins when they have bound to their ligands, or are activated. As the \"footprint\" that an antibody makes on its binding target is roughly a circle about 3\u00a0nm in diameter, the resolution of this technique is low. Nevertheless, these so-called LIBS (Ligand-Induced-Binding-Sites) antibodies unequivocally show that dramatic changes in integrin shape routinely occur. However, how the changes detected with antibodies look on the structure is still unknown.\nActivation.\nWhen released into the cell membrane, newly synthesized integrin dimers are speculated to be found in the same \"bent\" conformation revealed by the structural studies described above. One school of thought claims that this bent form prevents them from interacting with their ligands, although bent forms can predominate in high-resolution EM structures of integrin bound to an ECM ligand. Therefore, at least in biochemical experiments, integrin dimers must apparently not be 'unbent' in order to prime them and allow their binding to the ECM. In cells, the priming is accomplished by a protein talin, which binds to the \u03b2 tail of the integrin dimer and changes its conformation. The \u03b1 and \u03b2 integrin chains are both class-I transmembrane proteins: they pass the plasma membrane as single transmembrane alpha-helices. Unfortunately, the helices are too long, and recent studies suggest that, for integrin gpIIbIIIa, they are tilted with respect both to one another and to the plane of the membrane. Talin binding alters the angle of tilt of the \u03b23 chain transmembrane helix in model systems and this may reflect a stage in the process of inside-out signalling which primes integrins. Moreover, talin proteins are able to dimerize and thus are thought to intervene in the clustering of integrin dimers which leads to the formation of a focal adhesion. Recently, the Kindlin-1 and Kindlin-2 proteins have also been found to interact with integrin and activate it.\nFunction.\nIntegrins have two main functions, attachment of the cells to the ECM and signal transduction from the ECM to the cells. They are also involved in a wide range of other biological activities, including extravasation, cell-to-cell adhesion, cell migration, and as receptors for certain viruses, such as adenovirus, echovirus, hantavirus, foot-and-mouth disease, polio virus and other viruses. Recently, the importance of integrins in the progress of autoimmune disorders is also gaining attention of the scientists. These mechanoreceptors seem to regulate autoimmunity by dictating various intracellular pathways to control immune cell adhesion to endothelial cell layers followed by their trans-migration. This process might or might not be dependent on the sheer force faced by the extracellular parts of different integrins.\nA prominent function of the integrins is seen in the molecule GpIIb/IIIa, an integrin on the surface of blood platelets (thrombocytes) responsible for attachment to fibrin within a developing blood clot. This molecule dramatically increases its binding affinity for fibrin/fibrinogen through association of platelets with exposed collagens in the wound site. Upon association of platelets with collagen, GPIIb/IIIa changes shape, allowing it to bind to fibrin and other blood components to form the clot matrix and stop blood loss.\nAttachment of cell to the ECM.\nIntegrins couple the cell-extracellular matrix (ECM) outside a cell to the cytoskeleton (in particular, the microfilaments) inside the cell. Which ligand in the ECM the integrin can bind to is defined by which \u03b1 and \u03b2 subunits the integrin is made of. Among the ligands of integrins are fibronectin, vitronectin, collagen, and laminin. The connection between the cell and the ECM may help the cell to endure pulling forces without being ripped out of the ECM. The ability of a cell to create this kind of bond is also of vital importance in ontogeny.\nCell attachment to the ECM is a basic requirement to build a multicellular organism. Integrins are not simply hooks, but give the cell critical signals about the nature of its surroundings. Together with signals arising from receptors for soluble growth factors like VEGF, EGF, and many others, they enforce a cellular decision on what biological action to take, be it attachment, movement, death, or differentiation. Thus integrins lie at the heart of many cellular biological processes. The attachment of the cell takes place through formation of cell adhesion complexes, which consist of integrins and many cytoplasmic proteins, such as talin, vinculin, paxillin, and alpha-actinin. These act by regulating kinases such as FAK (focal adhesion kinase) and Src kinase family members to phosphorylate substrates such as p130CAS thereby recruiting signaling adaptors such as CRK. These adhesion complexes attach to the actin cytoskeleton. The integrins thus serve to link two networks across the plasma membrane: the extracellular ECM and the intracellular actin filamentous system. Integrin \u03b16\u03b24 is an exception: it links to the keratin intermediate filament system in epithelial cells.\nFocal adhesions are large molecular complexes, which are generated following interaction of integrins with ECM, then their clustering. The clusters likely provide sufficient intracellular binding sites to permit the formation of stable signaling complexes on the cytoplasmic side of the cell membrane. So the focal adhesions contain integrin ligand, integrin molecule, and associate plaque proteins. Binding is propelled by changes in free energy. As previously stated, these complexes connect the extracellular matrix to actin bundles. Cryo-electron tomography reveals that the adhesion contains particles on the cell membrane with diameter of 25 +/- 5\u00a0nm and spaced at approximately 45\u00a0nm. Treatment with Rho-kinase inhibitor Y-27632 reduces the size of the particle, and it is extremely mechanosensitive.\nOne important function of integrins on cells in tissue culture is their role in cell migration. Cells adhere to a substrate through their integrins. During movement, the cell makes new attachments to the substrate at its front and concurrently releases those at its rear. When released from the substrate, integrin molecules are taken back into the cell by endocytosis; they are transported through the cell to its front by the endocytic cycle, where they are added back to the surface. In this way they are cycled for reuse, enabling the cell to make fresh attachments at its leading front. The cycle of integrin endocytosis and recycling back to the cell surface is important for migrating cells and also during animal development.\nSignal transduction.\nIntegrins play an important role in cell signaling by modulating the cell signaling pathways of transmembrane protein kinases such as receptor tyrosine kinases (RTK). While the interaction between integrin and receptor tyrosine kinases originally was thought of as uni-directional and supportive, recent studies indicate that integrins have additional, multi-faceted roles in cell signaling. \nIntegrins can regulate the receptor tyrosine kinase signaling by recruiting specific adaptors to the plasma membrane. For example, \u03b21c integrin recruits Gab1/Shp2 and presents Shp2 to IGF1R, resulting in dephosphorylation of the receptor. In a reverse direction, when a receptor tyrosine kinase is activated, integrins co-localise at focal adhesion with the receptor tyrosine kinases and their associated signaling molecules.\nThe repertoire of integrins expressed on a particular cell can specify the signaling pathway due to the differential binding affinity of ECM ligands for the integrins. The tissue stiffness and matrix composition can initiate specific signaling pathways regulating cell behavior. Clustering and activation of the integrins/actin complexes strengthen the focal adhesion interaction and initiate the framework for cell signaling through assembly of adhesomes.\nDepending on the integrin's regulatory impact on specific receptor tyrosine kinases, the cell can experience:\nKnowledge of the relationship between integrins and receptor tyrosine kinase has laid a foundation for new approaches to cancer therapy. Specifically, targeting integrins associated with RTKs is an emerging approach for inhibiting angiogenesis.\nIntegrins and nerve repair.\nIntegrins have an important function in neuroregeneration after injury of the peripheral nervous system (PNS). Integrins are present at the growth cone of damaged PNS neurons and attach to ligands in the ECM to promote axon regeneration. It is unclear whether integrins can promote axon regeneration in the adult central nervous system (CNS). There are two obstacles that prevent integrin-mediated regeneration in the CNS: 1) integrins are not localised in the axon of most adult CNS neurons and 2) integrins become inactivated by molecules in the scar tissue after injury.\nVertebrate integrins.\nThe following are 16 of the ~24 integrins found in vertebrates:\nBeta-1 integrins interact with many alpha integrin chains. Gene knockouts of integrins in mice are not always lethal, which suggests that during embryonal development, one integrin may substitute its function for another in order to allow survival. Some integrins are on the cell surface in an inactive state, and can be rapidly primed, or put into a state capable of binding their ligands, by cytokines. Integrins can assume several different well-defined shapes or \"conformational states\". Once primed, the conformational state changes to stimulate ligand binding, which then activates the receptors \u2014 also by inducing a shape change \u2014 to trigger outside-in signal transduction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "15303", "revid": "1319066455", "url": "https://en.wikipedia.org/wiki?curid=15303", "title": "Ion channel", "text": "Pore-forming membrane protein\nIon channels are pore-forming membrane proteins that allow ions to pass through the channel pore. Their functions include establishing a resting membrane potential, shaping action potentials and other electrical signals by gating the flow of ions across the cell membrane, controlling the flow of ions across secretory and epithelial cells, and regulating cell volume. Ion channels are present in the membranes of all cells. Ion channels are one of the two classes of ionophoric proteins, the other being ion transporters.\nThe study of ion channels often involves biophysics, electrophysiology, and pharmacology, while using techniques including voltage clamp, patch clamp, immunohistochemistry, X-ray crystallography, fluoroscopy, and RT-PCR. Their classification as molecules is referred to as channelomics.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nBasic features.\nThere are two distinctive features of ion channels that differentiate them from other types of ion transporter proteins:\nIon channels are located within the membrane of all excitable cells, and of many intracellular organelles. They are often described as narrow, water-filled tunnels that allow only ions of a certain size and/or charge to pass through. This characteristic is called selective permeability. The archetypal channel pore is just one or two atoms wide at its narrowest point and is selective for specific species of ion, such as sodium or potassium. However, some channels may be permeable to the passage of more than one type of ion, typically sharing a common charge: positive (cations) or negative (anions). Ions often move through the segments of the channel pore in a single file nearly as quickly as the ions move through the free solution. In many ion channels, passage through the pore is governed by a \"gate\", which may be opened or closed in response to chemical or electrical signals, temperature, or mechanical force.\nIon channels are integral membrane proteins, typically formed as assemblies of several individual proteins. Such \"multi-subunit\" assemblies usually involve a circular arrangement of identical or homologous proteins closely packed around a water-filled pore through the plane of the membrane or lipid bilayer. For most voltage-gated ion channels, the pore-forming subunit(s) are called the \u03b1 subunit, while the auxiliary subunits are denoted \u03b2, \u03b3, and so on.\nBiological role.\nBecause channels underlie the nerve impulse and because \"transmitter-activated\" channels mediate conduction across the synapses, channels are especially prominent components of the nervous system. Indeed, numerous toxins that organisms have evolved for shutting down the nervous systems of predators and prey (e.g., the venoms produced by spiders, scorpions, snakes, fish, bees, sea snails, and others) work by modulating ion channel conductance and/or kinetics. In addition, ion channels are key components in a wide variety of biological processes that involve rapid changes in cells, such as cardiac, skeletal, and smooth muscle contraction, epithelial transport of nutrients and ions, T-cell activation, and pancreatic beta-cell insulin release. In the search for new drugs, ion channels are a frequent target.\nDiversity.\nThere are over 300 types of ion channels just in the cells of the inner ear. Ion channels may be classified by the nature of their gating, the species of ions passing through those gates, the number of gates (pores), and localization of proteins.\nFurther heterogeneity of ion channels arises when channels with different constitutive subunits give rise to a specific kind of current. Absence or mutation of one or more of the contributing types of channel subunits can result in loss of function and, potentially, underlie neurologic diseases.\nClassification by gating.\nIon channels may be classified by gating, i.e. what opens and closes the channels. For example, voltage-gated ion channels open or close depending on the voltage gradient across the plasma membrane, while ligand-gated ion channels open or close depending on binding of ligands to the channel.\nVoltage-gated.\nVoltage-gated ion channels open and close in response to membrane potential.\nLigand-gated (neurotransmitter).\nAlso known as ionotropic receptors, this group of channels open in response to specific ligand molecules binding to the extracellular domain of the receptor protein. Ligand binding causes a conformational change in the structure of the channel protein that ultimately leads to the opening of the channel gate and subsequent ion flux across the plasma membrane. Examples of such channels include the cation-permeable nicotinic acetylcholine receptors, ionotropic glutamate-gated receptors, acid-sensing ion channels (ASICs), ATP-gated P2X receptors, and the anion-permeable \u03b3-aminobutyric acid-gated GABAA receptor.\nIon channels activated by second messengers may also be categorized in this group, although ligands and second messengers are otherwise distinguished from each other.\nLipid-gated.\nThis group of channels opens in response to specific lipid molecules binding to the channel's transmembrane domain typically near the inner leaflet of the plasma membrane. Phosphatidylinositol 4,5-bisphosphate (PIP2) and phosphatidic acid (PA) are the best-characterized lipids to gate these channels. Many of the leak potassium channels are gated by lipids including the inward-rectifier potassium channels and two pore domain potassium channels TREK-1 and TRAAK. KCNQ potassium channel family are gated by PIP2. The voltage activated potassium channel (Kv) is regulated by PA. Its midpoint of activation shifts +50 mV upon PA hydrolysis, near resting membrane potentials. This suggests Kv could be opened by lipid hydrolysis independent of voltage and may qualify this channel as dual lipid and voltage gated channel.\nOther gating.\nGating also includes activation and inactivation by second messengers from the inside of the cell membrane \u2013 rather than from outside the cell, as in the case for ligands.\nClassification by cellular localization.\nIon channels are also classified according to their subcellular localization. The plasma membrane accounts for around 2% of the total membrane in the cell, whereas intracellular organelles contain 98% of the cell's membrane. The major intracellular compartments are endoplasmic reticulum, Golgi apparatus, and mitochondria. On the basis of localization, ion channels are classified as:\nOther classifications.\nSome ion channels are classified by the duration of their response to stimuli:\nDetailed structure.\nChannels differ with respect to the ion they let pass (for example, Na+, K+, Cl\u2212), the ways in which they may be regulated, the number of subunits of which they are composed and other aspects of structure. Channels belonging to the largest class, which includes the voltage-gated channels that underlie the nerve impulse, consist of four or sometimes five subunits with six transmembrane helices each. On activation, these helices move about and open the pore. Two of these six helices are separated by a loop that lines the pore and is the primary determinant of ion selectivity and conductance in this channel class and some others.\nThe existence and mechanism for ion selectivity was first postulated in the late 1960s by Bertil Hille and Clay Armstrong. The idea of the ionic selectivity for potassium channels was that the carbonyl oxygens of the protein backbones of the \"selectivity filter\" (named by Bertil Hille) could efficiently replace the water molecules that normally shield potassium ions, but that sodium ions were smaller and cannot be completely dehydrated to allow such shielding, and therefore could not pass through. This mechanism was finally confirmed when the first structure of an ion channel was elucidated. A bacterial potassium channel KcsA, consisting of just the selectivity filter, \"P\" loop, and two transmembrane helices was used as a model to study the permeability and the selectivity of ion channels in the Mackinnon lab. The determination of the molecular structure of KcsA by Roderick MacKinnon using X-ray crystallography won a share of the 2003 Nobel Prize in Chemistry.\nBecause of their small size and the difficulty of crystallizing integral membrane proteins for X-ray analysis, it is only very recently that scientists have been able to directly examine what channels \"look like.\" Particularly in cases where the crystallography required removing channels from their membranes with detergent, many researchers regard images that have been obtained as tentative. An example is the long-awaited crystal structure of a voltage-gated potassium channel, which was reported in May 2003. One inevitable ambiguity about these structures relates to the strong evidence that channels change conformation as they operate (they open and close, for example), such that the structure in the crystal could represent any one of these operational states. Most of what researchers have deduced about channel operation so far they have established through electrophysiology, biochemistry, gene sequence comparison and mutagenesis.\nChannels can have single (CLICs) to multiple transmembrane (K channels, P2X receptors, Na channels) domains which span plasma membrane to form pores. Pore can determine the selectivity of the channel. Gate can be formed either inside or outside the pore region.\nPharmacology.\nChemical substances can modulate the activity of ion channels, for example by blocking or activating them.\nIon channel blockers.\nA variety of ion channel blockers (inorganic and organic molecules) can modulate ion channel activity and conductance.\nSome commonly used blockers include:\nIon channel activators.\nSeveral compounds are known to promote the opening or activation of specific ion channels. These are classified by the channel on which they act:\nDiseases.\nThere are a number of disorders which disrupt normal functioning of ion channels and have disastrous consequences for the organism. Genetic and autoimmune disorders of ion channels and their modifiers are known as channelopathies. See for a full list.\nHistory.\nThe fundamental properties of currents mediated by ion channels were analyzed by the British biophysicists Alan Hodgkin and Andrew Huxley as part of their Nobel Prize-winning research on the action potential, published in 1952. They built on the work of other physiologists, such as Cole and Baker's research into voltage-gated membrane pores from 1941. The existence of ion channels was confirmed in the 1970s by Bernard Katz and Ricardo Miledi using noise analysis . It was then shown more directly with an electrical recording technique known as the \"patch clamp\", which led to a Nobel Prize to Erwin Neher and Bert Sakmann, the technique's inventors. Hundreds if not thousands of researchers continue to pursue a more detailed understanding of how these proteins work. In recent years the development of automated patch clamp devices helped to increase significantly the throughput in ion channel screening.\nThe Nobel Prize in Chemistry for 2003 was awarded to Roderick MacKinnon for his studies on the physico-chemical properties of ion channel structure and function, including x-ray crystallographic structure studies.\nCulture.\nRoderick MacKinnon commissioned \"Birth of an Idea\", a tall sculpture based on the KcsA potassium channel. The artwork contains a wire object representing the channel's interior with a blown glass object representing the main cavity of the channel structure.\nIon channels and stochastic processes.\nThe behavior of ion channels can be usefully modeled using mathematics and probability. Stochastic processes are mathematical models of systems and phenomena that appear to vary in a random manner. A very simple example is flipping a coin; each flip has an equal chance to be heads or tails, the chances are not influenced by the outcome of past flips, and we can say that pheads = 0.5 and ptails = 0.5.\nA particularly relevant form of stochastic processes in the study of ion channels is Markov chains. In a Markov chain, there are multiple states, each of which has given chances to transition to different states over a particular period of time. Ion channels undergo state transitions (e.g. open, closed, inactive) that behave like Markov chains. Markov chain analysis can be used to make conclusions regarding the nature of a given ion channel, including the likely number of open and closed states. We can also use Markov chain analysis to produce models that accurately simulate the insertion of ion channels into cell membranes.\nMarkov chains can also be used in combination with the stochastic matrix to determine the stable distribution matrix by solving the equation PX=X, where P is the stochastic matrix and X is the stable distribution matrix. This stable distribution matrix tells us the relative frequencies of each state after a long time, which in the context of ion channels can be the frequencies of the open, closed, and inactive states for an ion channel. Note that Markov chain assumptions apply, including that (1) all transition probabilities for each state sum to one, (2) the probability model applies to all possible states, and (3) that the probability of transitions are constant over time. Therefore, Markov chains have limited applicability in some situations.\nThere are a variety of other stochastic processes that are utilized in the study of ion channels, but are too complex to relate here and can be examined more closely elsewhere.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15304", "revid": "38004052", "url": "https://en.wikipedia.org/wiki?curid=15304", "title": "IDE", "text": "IDE, iDE, or Ide may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15305", "revid": "25895", "url": "https://en.wikipedia.org/wiki?curid=15305", "title": "Integrated development environment", "text": "Software engineering toolkit\nAn integrated development environment (IDE) is a software application that provides comprehensive facilities for software development. An IDE normally consists of at least a source-code editor, build automation tools, and a debugger. Some IDEs, such as IntelliJ IDEA, Eclipse and Lazarus contain the necessary compiler, interpreter or both; others, such as SharpDevelop and NetBeans, do not.\nThe boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.\nOverview.\nIntegrated development environments are designed to maximize programmer productivity by providing tight-knit components with similar user interfaces. IDEs present a single program in which all development is done. This program typically provides many features for authoring, modifying, compiling, deploying and debugging software. This contrasts with software development using unrelated tools, such as vi, GDB, GNU Compiler Collection, or make.\nOne aim of the IDE is to reduce the configuration necessary to piece together multiple development utilities. Instead, it provides the same set of capabilities as one cohesive unit. Reducing setup time can increase developer productivity, especially in cases where learning to use the IDE is faster than manually integrating and learning all of the individual tools. Tighter integration of all development tasks has the potential to improve overall productivity beyond just helping with setup tasks. For example, code can be continuously parsed while it is being edited, providing instant feedback when syntax errors are introduced, thus allowing developers to debug code much faster and more easily with an IDE.\nSome IDEs are dedicated to a specific programming language, allowing a feature set that most closely matches the programming paradigms of the language. However, there are many multiple-language IDEs.\nWhile most modern IDEs are graphical, text-based IDEs such as Turbo Pascal were in popular use before the availability of windowing systems like Microsoft Windows and the X Window System (X11). They commonly use function keys or hotkeys to execute frequently used commands or macros.\nHistory.\nIDEs initially became possible when developing via a console or terminal. Early systems could not support one, since programs were submitted to a compiler or assembler via punched cards, paper tape, etc. Dartmouth BASIC was the first language to be created with an IDE (and was also the first to be designed for use while sitting in front of a console or terminal). Its IDE (part of the Dartmouth Time-Sharing System) was command-based, and therefore did not look much like the menu-driven, graphical IDEs popular after the advent of the graphical user interface. However it integrated editing, file management, compilation, debugging and execution in a manner consistent with a modern IDE.\nMaestro I is a product from Softlab Munich and was the world's first integrated development environment for software. Maestro I was installed for 22,000 programmers worldwide. Until 1989, 6,000 installations existed in the Federal Republic of Germany. Maestro was arguably the world leader in this field during the 1970s and 1980s. Today one of the last Maestro I can be found in the Museum of Information Technology at Arlington in Texas.\nOne of the first IDEs with a plug-in concept was Softbench. In 1995 \"Computerwoche\" commented that the use of an IDE was not well received by developers since it would fence in their creativity.\nAs of \u00a02023[ [update]], the most commonly searched for IDEs on Google Search were Visual Studio, Visual Studio Code, and Eclipse.\nTopics.\nSyntax highlighting.\nThe IDE editor usually provides syntax highlighting, it can show both the structures, the language keywords and the syntax errors with visually distinct colors and font effects.\nCode completion.\nCode completion is an important IDE feature, intended to speed up programming. Modern IDEs even have intelligent code completion.\nRefactoring.\nAdvanced IDEs provide support for automated refactoring.\nVersion control.\nAn IDE is expected to provide integrated version control, in order to interact with source repositories.\nDebugging.\nIDEs are also used for debugging, using an integrated debugger, with support for setting breakpoints in the editor, visual rendering of steps, etc.\nCode search.\nIDEs may provide support for code search. Code search has two different meanings. First, it means searching for class and function declarations, usages, variable and field read/write, etc. IDEs can use different kinds of user interface for code search, for example form-based widgets and natural-language based interfaces.\nSecond, it means searching for a concrete implementation of some specified functionality.\nVisual programming.\nVisual programming is a usage scenario in which an IDE is generally required. Visual Basic allows users to create new applications by moving programming, building blocks, or code nodes to create flowcharts or structure diagrams that are then compiled or interpreted. These flowcharts often are based on the Unified Modeling Language.\nThis interface has been popularized with the Lego Mindstorms system and is being actively perused by a number of companies wishing to capitalize on the power of custom browsers like those found at Mozilla. KTechlab supports flowcode and is a popular open-source IDE and Simulator for developing software for microcontrollers. Visual programming is also responsible for the power of distributed programming (cf. LabVIEW and EICASLAB software). An early visual programming system, Max, was modeled after an analog synthesizer design and has been used to develop real-time music performance software since the 1980s. Another early example was Prograph, a dataflow-based system originally developed for the Macintosh. The graphical programming environment \"GRAPE\" is used to program qfix robot kits.\nThis approach is also used in specialist software such as Openlab, where the end-users want the flexibility of a full programming language, without the traditional learning curve associated with one.\nLanguage support.\nSome IDEs support multiple languages, such as GNU Emacs, IntelliJ IDEA, Eclipse, MyEclipse, NetBeans, MonoDevelop, JDoodle or PlayCode.\nSupport for alternative languages is often provided by plugins, allowing them to be installed on the same IDE at the same time. For example, Flycheck is a modern on-the-fly syntax checking extension for GNU Emacs 24 with support for 39 languages. Another example is JDoodle, an online cloud-based IDE that supports 88 languages.https:// Eclipse, and Netbeans have plugins for C/C++, Ada, GNAT (for example AdaGIDE), Perl, Python, Ruby, and PHP, which are selected between automatically based on file extension, environment or project settings.\nImplementation.\nIDEs can be implemented in various languages, for example:\nAttitudes across different computing platforms.\nUnix programmers can combine command-line POSIX tools into a complete development environment, capable of developing large programs such as the Linux kernel and its environment. In this sense, the entire Unix system functions as an IDE. The free software GNU toolchain (including GNU Compiler Collection (GCC), GNU Debugger (GDB), and GNU make) is available on many platforms, including Windows. The pervasive Unix philosophy of \"everything is a text stream\" enables developers who favor command-line oriented tools to use editors with support for many of the standard Unix and GNU build tools, building an IDE with programs like\nEmacs\nor Vim. Data Display Debugger is intended to be an advanced graphical front-end for many text-based debugger standard tools. Some programmers prefer managing makefiles and their derivatives to the similar code building tools included in a full IDE. For example, most contributors to the PostgreSQL database use make and GDB directly to develop new features. Even when building PostgreSQL for Microsoft Windows using Visual C++, Perl scripts are used as a replacement for make rather than relying on any IDE features. Some Linux IDEs such as Geany attempt to provide a graphical front end to traditional build operations.\nOn the various Microsoft Windows platforms, command-line tools for development are seldom used. Accordingly, there are many commercial and non-commercial products. However, each has a different design commonly creating incompatibilities. Most major compiler vendors for Windows still provide free copies of their command-line tools, including Microsoft (Visual C++, Platform SDK, .NET Framework SDK, nmake utility).\nIDEs have always been popular on the Apple Macintosh's classic Mac OS and macOS, dating back to Macintosh Programmer's Workshop, Turbo Pascal, THINK Pascal and THINK C environments of the mid-1980s. Currently macOS programmers can choose between native IDEs like Xcode and open-source tools such as Eclipse and Netbeans. ActiveState Komodo is a proprietary multilanguage IDE supported on macOS.\nOnline.\nAn online integrated development environment, also known as a web IDE or cloud IDE, is a browser based IDE that allows for software development or web development. An online IDE can be accessed from a web browser, allowing for a portable work environment. An online IDE does not usually contain all of the same features as a traditional or desktop IDE although all of the basic IDE features, such as syntax highlighting, are typically present.\nA Mobile-Based Integrated Development Environment (IDE) is a software application that provides a comprehensive suite of tools for software development on mobile platforms. Unlike traditional desktop IDEs, mobile-based IDEs are designed to run on smartphones and tablets, allowing developers to write, debug, and deploy code directly from their mobile devices.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15306", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=15306", "title": "Integrated Drive Electronics", "text": ""}
{"id": "15307", "revid": "220928", "url": "https://en.wikipedia.org/wiki?curid=15307", "title": "Injection system", "text": ""}
{"id": "15308", "revid": "43080961", "url": "https://en.wikipedia.org/wiki?curid=15308", "title": "Ian McKellen", "text": "English actor (born 1939)\nSir Ian Murray McKellen (born 25 May 1939) is an English actor. He has played roles on the screen and stage in genres ranging from Shakespearean dramas and modern theatre to popular fantasy and science fiction. He is regarded as a British cultural icon and was knighted by Queen Elizabeth II in 1991. He has received numerous accolades, including a Tony Award, six Olivier Awards, and a Golden Globe Award as well as nominations for two Academy Awards, five BAFTA Awards and five Emmy Awards.\nMcKellen made his stage debut in 1961 at the Belgrade Theatre as a member of its repertory company, and in 1965 made his first West End appearance. In 1969, he was invited to join the Prospect Theatre Company to play the lead parts in Shakespeare's \"Richard II\" and Marlowe's \"Edward II\". In the 1970s McKellen became a stalwart of the Royal Shakespeare Company and the National Theatre of Great Britain. He has earned five Olivier Awards for his roles in \"Pillars of the Community\" (1977), \"The Alchemist\" (1978), \"Bent\" (1979), \"Wild Honey\" (1984), and \"Richard III\" (1995). McKellen made his Broadway debut in \"The Promise\" (1965). He went on to receive the Tony Award for Best Actor in a Play for his role as Antonio Salieri in \"Amadeus\" (1980). He was further nominated for \"Ian McKellen: Acting Shakespeare\" (1984). He returned to Broadway in \"Wild Honey\" (1986), \"Dance of Death\" (1990), \"No Man's Land\" (2013), and \"Waiting for Godot\" (2013), the latter two being a joint production with Patrick Stewart.\nMcKellen achieved worldwide fame for his film roles, including the titular King in \"Richard III\" (1995), James Whale in \"Gods and Monsters\" (1998), Magneto in the \"X-Men\" films, Cogsworth in \"Beauty and the Beast\" (2017) and Gandalf in \"The Lord of the Rings\" (2001\u20132003) and \"The Hobbit\" (2012\u20132014) trilogies. Other notable film roles include \"A Touch of Love\" (1969), \"Plenty\" (1985), \"Six Degrees of Separation\" (1993), \"Restoration\" (1995), \"Flushed Away\" (2006), \"Mr. Holmes\" (2015), and \"The Good Liar\" (2019).\nMcKellen came out as gay in 1988, and has since championed LGBT social movements worldwide. He was awarded the Freedom of the City of London in October 2014. McKellen is a cofounder of Stonewall, an LGBT rights lobby group in the United Kingdom, named after the Stonewall riots. He is patron of LGBT History Month, Pride London, Oxford Pride, GayGlos, LGBT Foundation and FFLAG.\nEarly life and education.\nMcKellen was born on 25 May 1939 in Burnley, Lancashire, the son of Margery Lois (n\u00e9e Sutcliffe) and Denis Murray McKellen. He was their second child, with a sister, Jean, five years his senior. At four months old, shortly before the outbreak of the Second World War in September 1939, his family moved to Wigan. They lived there until Ian was twelve years old, before relocating to Bolton in 1951 after his father had been promoted. The experience of living through the war as a young child had a lasting impact on him, and he later said that \"only after peace resumed\u00a0... did I realise that war wasn't normal\". When an interviewer remarked that he seemed quite calm in the aftermath of the 11 September attacks, McKellen said: \"Well, darling, you forget\u2014I slept under a steel plate until I was four years old\".\nMcKellen's father was a civil engineer and lay preacher, and was of Protestant Irish and Scottish descent. Both of McKellen's grandfathers were preachers, and his great-great-grandfather, James McKellen, was a \"strict, evangelical Protestant minister\" in Ballymena, County Antrim. His home environment was strongly Christian, but non-orthodox. \"My upbringing was of low nonconformist Christians who felt that you led the Christian life in part by behaving in a Christian manner to everybody you met\". When he was 12, his mother died of breast cancer; his father died when he was 25. After his coming out as gay to his stepmother, Gladys McKellen, who was a Quaker, he said, \"Not only was she not fazed, but as a member of a society which declared its indifference to people's sexuality years back, I think she was just glad for my sake that I wasn't lying any more\". His great-great-grandfather Robert J. Lowes was an activist and campaigner in the ultimately successful campaign for a Saturday half-holiday in Manchester, the forerunner to the modern five-day work week, thus making Lowes a \"grandfather of the modern weekend\".\nMcKellen attended Bolton School (Boys' Division), of which he is still a supporter, attending regularly to talk to pupils. McKellen's acting career started at Bolton Little Theatre, of which he is now the patron. An early fascination with the theatre was encouraged by his parents, who took him on a family outing to \"Peter Pan\" at the Manchester Opera House when he was three. When he was nine, his main Christmas present was a fold-away wood and bakelite Victorian theatre from Pollocks Toy Theatres, with cardboard scenery and wires to push on the cut-outs of Cinderella and of Laurence Olivier's reenactment of Shakespeare's \"Hamlet\".\nHis sister took him to his first Shakespeare play, \"Twelfth Night\", by the amateurs of Wigan's Little Theatre, shortly followed by their \"Macbeth\" and Wigan High School for Girls' production of \"A Midsummer Night's Dream\", with music by Mendelssohn, with the role of Bottom played by Jean McKellen, who continued to act, direct, and produce amateur theatre until her death.\nIn 1958, McKellen, at the age of 18, won a scholarship to the University of Cambridge where he studied English literature as an undergraduate student of St Catharine's College, Cambridge. He has since been made an Honorary Fellow of the college. While at Cambridge, McKellen was a member of the Marlowe Society, where he appeared in 23 plays over the course of 3 years. At that young age he was already giving performances that have since become legendary such as his Justice Shallow in \"Henry IV\" alongside Trevor Nunn and Derek Jacobi (March 1959), \"Cymbeline\" (as Posthumus, opposite Margaret Drabble as Imogen) and \"Doctor Faustus\". During this period McKellen had already been directed by Peter Hall, John Barton and Dadie Rylands, all of whom would have a significant impact on McKellen's future career.\nCareer.\n1965\u20131985: National Theatre acclaim.\nMcKellen made his first professional appearance in 1961 at the Belgrade Theatre in Coventry, as Roper in \"A Man for All Seasons\", although an audio recording of the Marlowe Society's \"Cymbeline\" had gone on commercial sale as part of the Argo Shakespeare series. After four years in regional repertory theatres, McKellen made his first West End appearance, in \"A Scent of Flowers\", regarded as a \"notable success\". In 1965 he was a member of Laurence Olivier's National Theatre Company at the Old Vic, which led to roles at the Chichester Festival. With the Prospect Theatre Company, McKellen made his breakthrough performances of Shakespeare's \"Richard II\" (directed by Richard Cottrell) and Christopher Marlowe's \"Edward II\" (directed by Toby Robertson) at the Edinburgh Festival in 1969, the latter causing a storm of protest over the enactment of the homosexual Edward's lurid death.\nOne of his first major roles on television was as the title character in the BBC's 1966 adaptation of \"David Copperfield\", which achieved 12 million viewers on its initial airings. After some rebroadcasting in the late 60s, the master videotapes for the serial were wiped, and only four scattered episodes (3, 8, 9 and 11) survive as telerecordings, three of which feature McKellen as adult David. McKellen had taken film roles throughout his career\u2014beginning in 1969 with his role of George Matthews in \"A Touch of Love\", and his first leading role was in 1980 as D. H. Lawrence in \"Priest of Love\", but it was not until the 1990s that he became more widely recognised in this medium after several roles in blockbuster Hollywood films. In 1969, McKellen starred in three films, Michael Hayes's \"The Promise\", Clive Donner's epic film \"Alfred the Great\", and Waris Hussein's \"A Touch of Love\" (1969).\nIn the 1970s, McKellen became a well-known figure in British theatre, performing frequently at the Royal Shakespeare Company and the Royal National Theatre, where he played several leading Shakespearean roles. From 1973 to 1974, McKellen toured the United Kingdom and Brooklyn Academy of Music portraying Lady Wishfort's Footman, Kruschov, and Edgar in the William Congreve comedy \"The Way of the World\", Anton Chekhov's comedic three-act play \"The Wood Demon\" and William Shakespeare tragedy \"King Lear\". The following year, he starred in Shakespeare's \"King John\", George Colman's \"The Clandestine Marriage\", and George Bernard Shaw's \"Too True to Be Good\". From 1976 to 1977 he portrayed Romeo in the Shakespeare romance \"Romeo &amp; Juliet\" at the Royal Shakespeare Theatre. The following year he played King Leontes in \"The Winter's Tale\".\nIn 1976, he played the title role in William Shakespeare's \"Macbeth\" at Stratford in a \"gripping\u00a0... out of the ordinary\" production, with Judi Dench, and Iago in \"Othello\", in award-winning productions directed by Trevor Nunn. Both of these productions were adapted into television films, also directed by Nunn. From 1978 to 1979 he toured in a double feature production of Shakespeare's \"Twelfth Night\", and Anton Chekov's \"Three Sisters\" portraying Sir Toby Belch and Andrei, respectively. In 1979, McKellen gained acclaim for his role as Antonio Salieri in the Broadway transfer production of Peter Shaffer's play \"Amadeus\". It was an immensely popular play produced by the National Theatre originally starring Paul Scofield. The transfer starred McKellen, Tim Curry as Wolfgang Amadeus Mozart, and Jane Seymour as Constanze Mozart. \"The New York Times\" theatre critic Frank Rich wrote of McKellen's performance \"In Mr. McKellen's superb performance, Salieri's descent into madness was portrayed in dark notes of almost bone-rattling terror\". For his performance, McKellen received the Tony Award for Best Actor in a Play.\nIn 1981, McKellen portrayed writer and poet D. H. Lawrence in the Christopher Miles directed biographical film, \"Priest of Love\". He followed up with Michael Mann's horror film \"The Keep\" (1983). In 1985, he starred in \"Plenty\", the film adaptation of the David Hare play of the same name. The film was directed by Fred Schepisi and starred Meryl Streep, Charles Dance, John Gielgud, and Sting. The film spans nearly 20 years from the early 1940s to the 1960s, around an Englishwoman's experiences as a fighter for the French Resistance during World War II when she has a one-night stand with a British intelligence agent. The film received mixed reviews with Roger Ebert of \"The Chicago Sun-Times\" praising the film's ensemble cast writing, \"The performances in the movie supply one brilliant solo after another; most of the big moments come as characters dominate the scenes they are in\".\n1986\u20132000: Established actor.\nIn 1986, he returned to Broadway in the revival of Anton Chekhov's first play \"Wild Honey\" alongside Kim Cattrall and Kate Burton. The play concerned a local Russian schoolteacher who struggles to remain faithful to his wife, despite the attention of three other women. McKellen received mixed reviews from critics in particular Frank Rich of \"The New York Times\" who praised him for his \"bravura and athletically graceful technique that provides everything except, perhaps, the thing that matters most\u2014sustained laughter\". He later wrote, \"Mr. McKellen finds himself in the peculiar predicament of the star who strains to carry a frail supporting cast\". In 1989 he played Iago in production of \"Othello\" by the Royal Shakespeare Company. McKellen starred in the British drama \"Scandal\" (1989) a fictionalised account of the Profumo affair that rocked the government of British prime minister Harold Macmillan. McKellen portrayed John Profumo. The film starred Joanne Whalley, and John Hurt. The film premiered at the 1989 Cannes Film Festival and competed for the Palme d'Or. When his friend and colleague, Patrick Stewart, decided to accept the role of Captain Jean-Luc Picard in the American television series, \"\", McKellen strongly advised him not to throw away his respected theatrical career to work in television. However, McKellen later conceded that Stewart had been prudent in accepting the role, which made him a global star and later followed his example such as co-starring with Stewart in the \"X-Men\" superhero film series.\nFrom 1990 to 1992, he acted in a world tour of a lauded revival of \"Richard III\", playing the title character. The production played at the Brooklyn Academy of Music for two weeks before continuing its tour where Frank Rich of \"New York Times\" was able to review it. In his piece, he praised McKellen's performance writing, \"Mr McKellen's highly sophisticated sense of theatre and fun drives him to reveal the secrets of how he pulls his victims' strings whether he is addressing the audience in a soliloquy or not\". For his performance he received the Laurence Olivier Award for Best Actor.\nIn 1992, he acted in Pam Gems's revival of Chekov's \"Uncle Vanya\" at the Royal National Theatre alongside Antony Sher, and Janet McTeer. In 1993, he starred in the film \"Six Degrees of Separation\" based on the Pulitzer Prize and Tony Award nominated play of the same name. McKellen starred alongside Will Smith, Donald Sutherland and Stockard Channing. The film was a critical success. That same year, he appeared in the western \"The Ballad of Little Jo\" opposite Bob Hoskins and the action comedy \"Last Action Hero\" starring Arnold Schwarzenegger. The following year, he appeared in the superhero film \"The Shadow\" with Alec Baldwin and the James L. Brooks directed comedy \"I'll Do Anything\" starring Nick Nolte.\nIn 1995, McKellen made his screenwriting debut with \"Richard III\", an ambitious adaptation of William Shakespeare's play of the same name, directed by Richard Loncraine. The film reimagines the play's story and characters to a setting based on 1930s Britain, with Richard depicted as a fascist plotting to usurp the throne. McKellen stars in the title role alongside an ensemble cast including Annette Bening, Robert Downey Jr., Jim Broadbent, Kristen Scott Thomas, Nigel Hawthorne and Dame Maggie Smith. As executive producer he returned his \u00a350,000 fee to complete the filming of the final battle. In his review of the film, \"The Washington Post\" film critic Hal Hinson called McKellen's performance a \"lethally flamboyant incarnation\" and said his \"florid mastery\u00a0... dominates everything\". Film critic Roger Ebert of the \"Chicago Sun-Times\" praised McKellen's adaptation and his performance in his four star review writing, \"McKellen has a deep sympathy for the playwright\u00a0... Here he brings to Shakespeare's most tortured villain a malevolence we are moved to pity. No man should be so evil, and know it. Hitler and others were more evil, but denied out to themselves. There is no escape for Richard. He is one of the first self-aware characters in the theatre, and for that distinction he must pay the price\". His performance in the title role garnered BAFTA and Golden Globe nominations for Best Actor and won the European Film Award for Best Actor. His screenplay was nominated for the BAFTA Award for Best Adapted Screenplay. That same year, he appeared in the historical drama \"Restoration\" (1995) also starring Downey Jr., as well as Meg Ryan, Hugh Grant, and David Thewlis. He appeared in the British romantic comedy \"Jack and Sarah\" (1995) starring Richard E. Grant, Samantha Mathis, and Judi Dench.\nIn 1993, he appeared in minor roles in the television miniseries \"Tales of the City\", based on the novel by his friend Armistead Maupin. Later that year, McKellen appeared in the HBO television film \"And the Band Played On\" based on the acclaimed novel of the same name about the discovery of HIV. For his performance as gay rights activist Bill Kraus, McKellen received the CableACE Award for Supporting Actor in a Movie or Miniseries and was nominated for the Primetime Emmy Award for Outstanding Supporting Actor in a Miniseries or a Movie. From 1993 to 1997 McKellen toured in a one-man show entitled, \"A Knights Out\", about coming out as a gay man. Laurie Winer from \"The Los Angeles Times\" wrote, \"Even if he is preaching to the converted, McKellen makes us aware of the vast and powerful intolerance outside the comfortable walls of the theatre. Endowed with a rare technique, he is a natural storyteller, an admirable human being and a hands-on activist\". From 1997 to 1998, he starred as Dr. Tomas Stockmann in a revival of Henrik Ibsen's \"An Enemy of the People\". Later that year he played Garry Essendine in the No\u00ebl Coward comedy \"Present Laughter\" at the West Yorkshire Playhouse. In 1998, he appeared in the modestly acclaimed psychological thriller \"Apt Pupil\", which was directed by Bryan Singer and based on a story by Stephen King. McKellen portrayed a fugitive Nazi officer living under a false name in the US who is befriended by a curious teenager (Brad Renfro) who threatens to expose him unless he tells his story in detail. That same year, he played James Whale, the director of \"Frankenstein\" in the Bill Condon directed period drama \"Gods and Monsters\", a role for which he was subsequently nominated for the Academy Award for Best Actor, losing it to Roberto Benigni in \"Life is Beautiful\" (1998).\nIn 1995, he appeared in the BBC television comedy film \"Cold Comfort Farm\" starring Kate Beckinsale, Rufus Sewell, and Stephen Fry. The following year he starred as Tsar Nicholas II in the HBO made-for-television movie \"\" (1996) starring Alan Rickman as Rasputin. For his performance, McKellen earned a Primetime Emmy Award for Outstanding Supporting Actor in a Limited Series or Movie nomination and received a Golden Globe Award for Best Supporting Actor \u2013 Series, Miniseries or Television Film win. McKellen appeared as Mr Creakle in the BBC series \"David Copperfield\" (1999) based on the Charles Dickens classic novel. The miniseries starred a pre-\"Harry Potter\" Daniel Radcliffe, Bob Hoskins, and Dame Maggie Smith.\n2000\u20132011: International stardom.\nIn 1999, McKellen was cast, again under the direction of Bryan Singer, to play the comic book supervillain Magneto in the 2000 film \"X-Men\" and its sequels ' (2003) and ' (2006). He later reprised his role of Magneto in 2014's ', sharing the role with Michael Fassbender, who played a younger version of the character in 2011's '.\nWhile filming the first \"X-Men\" film in 1999, McKellen was cast as the wizard Gandalf in Peter Jackson's film trilogy adaptation of \"The Lord of the Rings\" (consisting of ', ', and \"\"), released between 2001 and 2003. He won the Screen Actors Guild Award for Best Supporting Actor in a Motion Picture for his work in \"The Fellowship of the Ring\" and was nominated for the Academy Award for Best Supporting Actor for the same role. He provided the voice of Gandalf for several video game adaptations of the \"Lord of the Rings\" films.\nMcKellen returned to the Broadway stage in 2001 in an August Strindberg play \"The Dance of Death\" alongside Helen Mirren and David Strathairn at the Broadhurst Theatre. \"The New York Times\" Theatre critic Ben Brantley praised McKellen's performance writing, \"[McKellen] returns to Broadway to serve up an Elysian concoction we get to sample too little these days: a mixture of heroic stage presence, actorly intelligence, and rarefied theatrical technique\". McKellen toured with the production at the Lyric Theatre in London's West End and to the Sydney Art's Festival in Australia. On 16 March 2002, he hosted \"Saturday Night Live\".\nIn 2002 McKellen appeared in a solo performance at the Beverly Hills Canon Theatre, where he performed his personally written scene from a Shakespeare annex piece.\nIn 2003 McKellen made a guest appearance as himself on the American cartoon show \"The Simpsons\" in a special British-themed episode entitled \"The Regina Monologues\", along with the then UK Prime Minister Tony Blair and author J. K. Rowling. In April and May 2005, he played the role of Mel Hutchwright in Granada Television's long-running British soap opera, \"Coronation Street\", fulfilling a lifelong ambition, where in 2015 he was gifted a cobble from the soap's exterior set for his seventy-sixth birthday. He narrated Richard Bell's film \"Eighteen\" as a grandfather who leaves his World War II memoirs on audio-cassette for his teenage grandson.\nHe has appeared in limited release films, such as \"Emile\" (which was shot in three weeks following the \"X2\" shoot), \"Neverwas\" and \"Asylum\". In 2006, he appeared as Sir Leigh Teabing in \"The Da Vinci Code\" opposite Tom Hanks as Robert Langdon. During a 17 May 2006 interview on \"The Today Show\" with the \"Da Vinci Code\" cast and director Ron Howard, Matt Lauer posed a question to the group about how they would have felt if the film had borne a prominent disclaimer that it is a work of fiction, as some religious groups wanted. McKellen responded, \"I've often thought the Bible should have a disclaimer in the front saying 'This is fiction'. I mean, walking on water? It takes\u00a0... an act of faith. And I have faith in this movie\u2014not that it's true, not that it's factual, but that it's a jolly good story\". He continued, \"And I think audiences are clever enough and bright enough to separate out fact and fiction, and discuss the thing when they've seen it\".\nMcKellen appeared in the 2006 BBC series of Ricky Gervais's comedy series \"Extras\", where he played himself directing Gervais's character Andy Millman in a play about gay lovers. McKellen received a 2007 Primetime Emmy Award for Outstanding Guest Actor \u2013 Comedy Series nomination for his performance. In 2007, McKellen narrated the romantic fantasy adventure film \"Stardust\" starring Charlie Cox and Claire Danes, which was a critical and financial success. That same year, he lent his voice to the armoured bear Iorek Byrnison in the Chris Weitz-directed fantasy film \"The Golden Compass\" based on the acclaimed Philip Pullman novel \"Northern Lights\" and starred Nicole Kidman and Daniel Craig. The film received mixed reviews but was a financial success.\nIn 2007, he returned to the Royal Shakespeare Company, in productions of \"King Lear\" and \"The Seagull\", both directed by Trevor Nunn. In 2009 he portrayed Number Two in \"The Prisoner\", a remake of the 1967 cult series \"The Prisoner\". In 2009, he appeared in a very popular revival of \"Waiting for Godot\" at London's Haymarket Theatre, directed by Sean Mathias, and playing opposite Patrick Stewart. From 2013 to 2014, McKellen and Stewart starred in a double production of Samuel Beckett's \"Waiting for Godot\" and Harold Pinter's \"No Man's Land\" on Broadway at the Cort Theatre. \"Variety\" theatre critic Marilyn Stasio praised the dual production writing, \"McKellen and Stewart find plenty of consoling comedy in two masterpieces of existential despair\". In both productions of Stasio claims, \"the two thespians play the parts they were meant to play\". He is Patron of English Touring Theatre and also President and Patron of the Little Theatre Guild of Great Britain, an association of amateur theatre organisations throughout the UK. In late August 2012, he took part in the opening ceremony of the London Paralympics, portraying Prospero from \"The Tempest\".\nSince 2012: Career expansion.\nMcKellen reprised the role of Gandalf on screen in Peter Jackson's three-part film adaptation of \"The Hobbit\" starting with ' (2012), followed by ' (2013), and finally \"\" (2014). Despite the series receiving mixed reviews, it emerged as a financial success. McKellen \nreprised his role as Erik Lehnsherr/Magneto in James Mangold's \"The Wolverine\" (2013), and Singer's \"\" (2014). In November 2013, McKellen appeared in the \"Doctor Who\" 50th anniversary comedy homage \"The Five(ish) Doctors Reboot\". From 2013 to 2016, McKellen co-starred in the ITV sitcom \"Vicious\" as Freddie Thornhill, alongside Derek Jacobi. The series revolves around an elderly gay couple who have been together for 50 years. The show's original title was \"Vicious Old Queens\". There are ongoing jokes about McKellen's career as a relatively unsuccessful character actor who owns a tux because he stole it after doing a guest spot on \"Downton Abbey\" and that he holds the title of \"10th Most Popular 'Doctor Who' Villain\". Liz Shannon Miller of \"IndieWire\" noted that while the concept seemed \"weird as hell\", \"Once you come to accept McKellen and Jacobi in a multi-camera format, there is a lot to respect about their performances; specifically, the way that those decades of classical training adapt themselves to the sitcom world. Much has been written before about how the tradition of the multi-cam, filmed in front of a studio audience, relates to theatre, and McKellen and Jacobi know how to play to a live crowd\".\nIn 2015, McKellen reunited with director Bill Condon playing an elderly Sherlock Holmes in the mystery film \"Mr. Holmes\" alongside Laura Linney. In the film based on the novel \"A Slight Trick of the Mind\" (2005), Holmes now 93, struggles to recall the details of his final case because his mind is slowly deteriorating. The film premiered at the 65th Berlin International Film Festival with McKellen receiving acclaim for his performance. \"Rolling Stone\" film critic Peter Travers praised his performance writing, \"Don't think you can take another Hollywood version of Sherlock Holmes? Snap out of it. Apologies to Robert Downey Jr. and Benedict Cumberbatch, but what Ian McKellen does with Arthur Conan Doyle's fictional detective in Mr Holmes is nothing short of magnificent\u00a0... Director Bill Condon, who teamed superbly with McKellen on the Oscar-winning Gods and Monsters, brings us a riveting character study of a lion not going gentle into winter\". In October 2015, McKellen appeared as Norman to Anthony Hopkins's Sir in a BBC Two production of Ronald Harwood's \"The Dresser\", alongside Edward Fox, Vanessa Kirby, and Emily Watson. Television critic Tim Goodman of \"The Hollywood Reporter\" praised the film and the central performances writing, \"there's no escaping that Hopkins and McKellen are the central figures here, giving wonderfully nuanced performances, onscreen together for their first time in their acclaimed careers\". For his performance McKellen received a British Academy Television Award nomination for his performance.\nIn 2017, McKellen portrayed in a supporting role as Cogsworth (originally voiced by David Ogden Stiers in the 1991 animated film) in the live-action adaptation of Disney's \"Beauty and the Beast\", directed by Bill Condon (which marked the third collaboration between Condon and McKellen, after \"Gods and Monsters\" and \"Mr. Holmes\") and co-starred alongside Emma Watson and Dan Stevens. The film was released to positive reviews and grossed $1.2billion worldwide, making it the highest-grossing live-action musical film, the second highest-grossing film of 2017, and the 17th highest-grossing film of all time. In 2017, McKellen appeared in the documentary \"McKellen: Playing the Part\", directed by director Joe Stephenson. The documentary explores McKellen's life and career as an actor.\nIn October 2017, McKellen played King Lear at the Chichester Festival Theatre, a role which he said was likely to be his \"last big Shakespearean part\". He performed the play at the Duke of York's Theatre in London's West End during the summer of 2018. McKellen voiced Dr. Cecil Pritchfield the child psychiatrist for Stewie Griffin in the \"Family Guy\" episode \"Send in Stewie, Please\" in 2018. He appeared in Kenneth Branagh's historical drama \"All is True\" (2018) portraying Henry Wriothesley, 3rd Earl of Southampton, opposite Branagh and Judi Dench. Peter Bradshaw of \"The Guardian\" wrote that Judi Dench and he \"offer solid support\" and added that his role is a \"colossal, emphatically wigged cameo\". To celebrate his 80th birthday, in 2019 McKellen performed in a one-man stage show titled \"\" celebrating the various performances throughout his career. The show toured across the UK and Ireland (raising money for each venue and organisation's charity) before a West End run at the Harold Pinter Theatre and was performed for one night only on Broadway at the Hudson Theatre.\nIn 2019, he reunited with Condon for a fourth time in the mystery thriller \"The Good Liar\" opposite Helen Mirren, who received praise for their onscreen chemistry. That same year, he appeared as Gus the Theatre Cat in the movie musical adaptation of \"Cats\" directed by Tom Hooper. The film featured performances from Jennifer Hudson, James Corden, Rebel Wilson, Idris Elba, and Judi Dench. The film was widely panned for its poor visual effects, editing, performances, screenplay, and was a box office disaster. In 2021, he played the title role in an age-blind production of \"Hamlet\" (having previously played the part in a UK and European tour in 1971), followed by the role of Firs in Chekov's \"The Cherry Orchard\" at the Theatre Royal, Windsor. Since November 2021, McKellen and ABBA member Bj\u00f6rn Ulvaeus have posted Instagram videos featuring the pair knitting Christmas jumpers and other festive attire. In 2023, it was revealed that Ulvaeus and McKellen would be knitting stagewear for Kylie Minogue as part of her \"More Than Just a Residency\" concert residency at Voltaire at The Venetian Las Vegas.\nIn 2023, he starred in the period thriller \"The Critic\" directed by Anand Tucker. The film is written by Patrick Marber adapted off the 2015 novel \"Curtain Call\" by Anthony Quinn. The film premiered at the 2023 Toronto International Film Festival.\nIn April 2024, McKellen starred as John Falstaff in \"Player Kings (\"an adaptation of Shakespeare's \"Henry IV Parts 1 and 2)\" opposite Richard Coyle and Toheeb Jimoh at the No\u00ebl Coward Theatre in London's West End and received rave reviews (following runs at New Wimbledon Theatre and Manchester Opera House). The production was scheduled to run until 22 June before touring to Bristol, Birmingham, Norwich and Newcastle upon Tyne, however during the performance on 17 June, McKellen fell off the front of the stage during a fight scene and called for assistance; the performance was cancelled and the audience dismissed. He was later reported to have recovered and to be \"in good spirits.\" He subsequently pulled out of the remaining West End and tour performances on medical advice.\nThe 2025 Christmas pantomime at the Pleasance Theatre, Islington, features a cameo appearance on video of McKellen as the dog Toto in \"The Wonderful Wizard of Oz-lington\", a show derived from \"Wicked\" and \"The Wizard of Oz\".\nMcKellen is set to reprise his role as Magneto in \"\" (2026).\nPersonal life.\nMcKellen and his first partner, Brian Taylor, a history teacher from Bolton, began their relationship in 1964. Their relationship lasted for eight years, ending in 1972. They lived in Earls Terrace, Kensington, London, where McKellen continued to pursue his career as an actor. In 1978, he met his second partner, Sean Mathias, at the Edinburgh Festival. This relationship lasted until 1988, and according to Mathias, it was tempestuous, with conflicts over McKellen's success in acting versus Mathias's somewhat less-successful career. The two remained friends, with Mathias later directing McKellen in \"Waiting for Godot\" at the Theatre Royal Haymarket in 2009. The pair entered into a business partnership with Evgeny Lebedev, purchasing the lease of The Grapes public house in Narrow Street. As of 2005, McKellen had been living in Narrow Street, Limehouse, for more than 25 years, more than a decade of which had been spent in a five-storey Victorian house.\nHe is an atheist. In the late 1980s, he lost his appetite for every kind of meat but fish, and has since followed a mainly pescetarian diet. In 2001, he received the Artist Citizen of the World Award (France). He has a tattoo of the Elvish number nine, written using J. R. R. Tolkien's constructed script of Tengwar, on his shoulder in reference to his involvement in the \"Lord of the Rings\" and the fact that his character was one of the original nine companions of the Fellowship of the Ring. All but one of the other actors of \"The Fellowship\" (Elijah Wood, Sean Astin, Orlando Bloom, Billy Boyd, Sean Bean, Dominic Monaghan and Viggo Mortensen) have the same tattoo (John Rhys-Davies did not get the tattoo, but his stunt double Brett Beattie did).\nHe was diagnosed with prostate cancer in 2006. In 2012, he stated on his blog that \"There is no cause for alarm. I am examined regularly and the cancer is contained. I've not needed any treatment\". He registered as a marriage officiant in early 2013 to preside over the marriage of his friend and \"X-Men\" co-star Patrick Stewart to the singer Sunny Ozell.\nMcKellen was awarded an honorary Doctorate of Letters by the University of Cambridge on 18 June 2014. He was made a Freeman of the City of London on 30 October 2014. The ceremony took place at Guildhall in London. He was nominated by London's Lord Mayor Fiona Woolf, who said he was an \"exceptional actor\" and \"tireless campaigner for equality\". He is an emeritus Fellow of St Catherine's College, Oxford.\nActivism.\nLGBT rights.\nWhile McKellen had made his sexual orientation known to fellow actors early on in his stage career, it was not until 1988 that he came out to the general public while appearing on the BBC Radio programme \"Third Ear\" hosted by conservative journalist Peregrine Worsthorne. The context that prompted McKellen's decision, overriding any concerns about a possible negative effect on his career, was that the controversial Section 28 of the \"Local Government Act 1988\", was then under consideration in the British Parliament. Section 28 proposed prohibiting local authorities from promoting homosexuality \"... as a kind of pretended family relationship\". McKellen has stated that he was influenced in his decision by the advice and support of his friends, among them noted gay author Armistead Maupin. In a 1998 interview that discusses the 29th anniversary of the Stonewall riots, McKellen commented, I have many regrets about not having come out earlier, but one of them might be that I didn't engage myself in the politicking. He has said of this period: My own participating in that campaign was a focus for people [to] take comfort that if Ian McKellen was on board for this, perhaps it would be all right for other people to be as well, gay and straight.\nIn 2003, during an appearance on \"Have I Got News For You\", McKellen claimed when he visited Michael Howard, then Environment Secretary (responsible for local government), in 1988 to lobby against Section 28, Howard refused to change his position but did ask him to leave an autograph for his children. McKellen agreed, but wrote, \"Fuck off, I'm gay\". McKellen described Howard's junior ministers, Conservatives David Wilshire and Jill Knight, who were the architects of Section 28, as the 'ugly sisters' of a political pantomime.\nMcKellen has continued to be very active in LGBT rights efforts. In a statement on his website regarding his activism, the actor commented:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I have been reluctant to lobby on other issues I most care about\u2014nuclear weapons (against), religion (atheist), capital punishment (anti), AIDS (fund-raiser) because I never want to be forever spouting, diluting the impact of addressing my most urgent concern: legal and social equality for gay people worldwide.\nMcKellen is a co-founder of Stonewall, an LGBT rights lobby group in the United Kingdom, named after the Stonewall riots. McKellen is also patron of LGBT History Month, Pride London, Oxford Pride, GAY-GLOS, LGBT Foundation and FFLAG where he appears in their video \"Parents Talking\".\nIn 1994, at the closing ceremony of the Gay Games, he briefly took the stage to address the crowd, saying, \"I'm Sir Ian McKellen, but you can call me Serena\": This nickname, given to him by Stephen Fry, had been circulating within the gay community since McKellen's knighthood was conferred. In 2002, he was the Celebrity Grand Marshal of the San Francisco Pride Parade and he attended the Academy Awards with his then-boyfriend, New Zealander Nick Cuthell. In 2006, McKellen spoke at the pre-launch of the 2007 LGBT History Month in the UK, lending his support to the organisation and its founder, Sue Sanders. In 2007, he became a patron of The Albert Kennedy Trust, an organisation that provides support to young, homeless and troubled LGBT people.\nIn 2006, he became a patron of Oxford Pride, stating:I send my love to all members of Oxford Pride, their sponsors and supporters, of which I am proud to be one\u00a0... Onlookers can be impressed by our confidence and determination to be ourselves and gay people, of whatever age, can be comforted by the occasion to take the first steps towards coming out and leaving the closet forever behind.\nMcKellen has taken his activism internationally, and caused a major stir in Singapore, where he was invited to do an interview on a morning show and shocked the interviewer by asking if they could recommend him a gay bar; the programme immediately ended. In December 2008, he was named in \"Out\"'s annual Out 100 list.\nIn 2010, McKellen extended his support for Liverpool's Homotopia festival in which a group of gay and lesbian Merseyside teenagers helped to produce an anti-homophobia campaign pack for schools and youth centres across the city. In May 2011, he called Sergey Sobyanin, Moscow's mayor, a \"coward\" for refusing to allow gay parades in the city.\nIn 2014, he was named in the top 10 on the World Pride Power list.\nCharity work.\nIn April 2010, along with actors Brian Cox and Eleanor Bron, McKellen appeared in a series of TV advertisements to support Age UK, the charity recently formed from the merger of Age Concern and Help the Aged. All three actors gave their time free of charge.\nA cricket fan since childhood, McKellen umpired in March 2011 for a charity cricket match in New Zealand to support earthquake victims of the February 2011 Christchurch earthquake.\nMcKellen is an honorary board member for the New York City- and Washington, D.C.\u2013based organisation Only Make Believe. Only Make Believe creates and performs interactive plays in children's hospitals and care facilities. He was honoured by the organisation in 2012 and hosted their annual Make Believe on Broadway Gala in November 2013.\nMcKellen also has a history of supporting individual theatres. While in New Zealand filming \"The Hobbit\" in 2012, he announced a special New Zealand tour \"Shakespeare, Tolkien and You!\", with proceeds going to help save the Isaac Theatre Royal, which suffered extensive damage during the 2011 Christchurch earthquake. McKellen said he opted to help save the building as it was the last theatre he played in New Zealand (\"Waiting for Godot\" in 2010) and the locals' love for it made it a place worth supporting. In July 2017, he performed a new one-man show for a week at Park Theatre (London), donating the proceeds to the theatre.\nTogether with a number of his \"Lord of the Rings\" co-stars (plus writer Philippa Boyens and director Peter Jackson), on 1 June 2020 McKellen joined Josh Gad's YouTube series \"Reunited Apart\" which reunites the cast of popular movies through video-conferencing, and promotes donations to non-profit charities.\nOther work.\nA friend of Ian Charleson and an admirer of his work, McKellen contributed an entire chapter to \"For Ian Charleson: A Tribute\". A recording of McKellen's voice is heard before performances at the Royal Festival Hall, reminding patrons to ensure their mobile phones and watch alarms are switched off and to keep coughing to a minimum. He took part in the 2012 Summer Paralympics opening ceremony in London as Prospero from Shakespeare's \"The Tempest\".\nAccolades and honours.\nMcKellen has received two Academy Award nominations for his performances in \"Gods and Monsters\" (1999), and ' (2001). He has received five Primetime Emmy Award nominations. McKellen has received two Tony Award nominations, winning for Best Actor in a Play for his performance in \"Amadeus\" in 1981. He has received 12 Olivier Award nominations, winning six awards for his performances in \"Pillars of the Community\" (1977), \"The Alchemist\" (1978), \"Bent\" (1979), \"Wild Honey\" (1984), \"Richard III\" (1991), and ' (2020).\nHe has received honorary awards including Pride International Film Festival's Lifetime Achievement &amp; Distinction Award in 2004 and the Olivier Awards' Society Special Award in 2006. He received The Lebedev Special Award in the 2009 \"Evening Standard\" Theatre Awards. The following year he received the Empire Awards' Empire Icon Award. In 2017 he received the Honorary Award from the Istanbul International Film Festival. McKellen was awarded an Honorary Fellowship of the British Shakespeare Association in 2020.\nMcKellen was appointed a Commander of the Order of the British Empire (CBE) in the 1979 Birthday Honours, then knighted in the 1991 New Year Honours for services to the performing arts, and made a Member of the Order of the Companions of Honour (CH) in the 2008 New Year Honours for services to drama and to equality.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15309", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15309", "title": "Intellivision", "text": "Home video game console\nThe Intellivision (a portmanteau of intelligent television) is a home video game console released by Mattel Electronics in 1979. It distinguished itself from competitors with more realistic sports and strategic games. By 1981, Mattel Electronics had close to 20% of the domestic video game market, selling more than 3.75 million consoles and 20 million cartridges through 1983. At its peak, Mattel Electronics had about 1,800 employees in several countries, including 110 videogame developers. In 1984, Mattel sold its video game assets to a former Mattel Electronics executive and investors, eventually becoming INTV Corporation. Game development ran from 1978 to 1990, when the Intellivision was discontinued.\nIn 2009, IGN ranked the Intellivision No. 14 on their list of the greatest video game consoles of all time.\nHistory.\nThe Intellivision was developed at Mattel in Hawthorne, California. By 1969, multiple research and development groups came together as the Preliminary Design department on the third floor of the head office. Mattel had a history with technology R&amp;D as design engineer Jack Ryan, who joined the company in 1955 from Raytheon, led a group of engineers, chemists, and sculptors. With a large budget they were expected to be forward thinking, dubbed the blue-sky group.\nMaster Component.\nIn 1975, mechanical engineer Richard Chang, a director under Ryan, contacted MOS Technology for a demonstration of their new 6502 microprocessor in a video game application. MOS arranged for their client Glenn Hightower of APh Technological Consulting and teacher at CalTech University to do the demonstration. Shortly after, Dave James, an industrial engineer under Chang, wrote a memo dated January 26, 1976, documenting two product concepts. First, a microprocessor programmed video system with \"plug-in\" ROM modules or cassettes, and a list of applications that include war games, gambling games, strategy and board games, video Etch-a-Sketch, driving simulator, pinball; and football with 10 player a side, defense/offense patterns and floating field background. Second, calculator based games. With Mattel executives skeptical, Chang's group moved forward with handheld electronic games enlisting Hightower's help with a prototype.\nMattel hired Michael Katz as Marketing Manager for New Product Categories in 1975, Katz asked Chang to prototype a calculator sized electronic game for 1976. In Fall 1976, Mattel hired Ed Krakauer as Vice President of New Business Development, who hired Jeff Rochlis as Director of New Business Development. In an October 1977 newspaper article, Rochlis was quoted saying, \"Basically these things are fore-runners of the home computer. There's a logical transition involved. One way to get into the home-computer market is to sell games.\"\nIn April 1977, David Chandler, with a doctoral degree in Electrical Engineering, a career in Aerospace, also having prototyped an early word processor as well as an arcade video game, joined Prelimanary Design under Chang. Chandler shared Chang's vision for a video game system with rich graphics and long-lasting gameplay to distinguish itself from its competitors and took over responsibility for its engineering. Prior to Chandler\u2019s arrival, Chang's group had already met with National Semiconductor about their new, although expensive, chip set. Chandler negotiated better pricing for a simpler design. At the Consumer Electronics Show in June 1977, Chandler saw two more chipsets. One from MOS Technology lacked moving objects. The other from General Instrument, listed as the Gimini programmable set in the GI 1977 catalog. The GI chipset lacked programmable graphics and Mattel worked with GI to implement changes. GI published an updated chipset in its 1978 catalog. Mattel initially chose National Semiconductor, who advised Mattel to postpone the project, turning them to GI. Mattel corporate management reacted by putting a halt to video game development for several months. On November 9, 1977, Mattel, GI, and Magnavox (their initial contract manufacturer) met to plan contracts and production.\nWith the 1977 success of their Mattel Electronics branded handheld electronic games, Mattel Electronics became a division within the company, with separate marketing, finance, and engineering. In September, Krakauer made Rochlis its president. Chang, becoming director of its new Design and Development department, responsible for Intellivision software. Chandler, became director of Product Engineering led a team engineering the hardware, including the hand controllers. In 1978, David Rolfe of APh developed the onboard executive control software named Exec, and with a group of Caltech summer student employees programmed the first Intellivision games. Hal Finney of APh contributed sound and music processing routines to the Exec. Graphics were designed by a group of artists at Mattel led by Dave James. James also creating detailed game proposal documents.\nDuring June 1978 CES, Mattel privately showed a prototype to retailers, leading to a Christmas release. Delays at GI pushed that into 1979. Magnavox backed out as manufacturer, replaced with Sylvania. Chandler considered replacing the GI chipset and working with Texas Instruments and their new TMS9918 video processor. The TI chip had more moving objects but half the number on a horizontal line compared with the GI &lt;templatestyles src=\"Template:Tooltip/styles.css\" /&gt;, it also lacked hardware scrolling that the GI &lt;templatestyles src=\"Template:Tooltip/styles.css\" /&gt; provides. Further, the TI chip requires more RAM and software already developed would have to be reworked.\nThe Intellivision was introduced at the 1979 Las Vegas CES in January as a modular home computer with the Master Component priced at US$ and a soon-to-follow Keyboard Component also at $. At Chicago CES in June, prices were revised to $ for each component. A shortage of key chips from manufacturer General Instrument resulted in a limited number of Intellivision Master Components produced that year. In Fall 1979, Sylvania marketed its own branded Intellivision at $ in its GTE stores at Philadelphia, Baltimore, and Washington, D.C. On December 3, Mattel delivered consoles to the Gottschalks department store chain headquartered in Fresno, California, with a suggested list price of $. The Intellivision was also listed in the nationally distributed JCPenney Christmas 1979 catalog along with seven cartridges. By April 1980, markets expanded to Los Angeles, New York, and Chicago. It was in stores nationwide by mid-1980 with the pack-in game \"Las Vegas Poker &amp; Blackjack\" and a library of ten cartridges.\nBy September 1980, there was internal debate about the effectiveness of marketing the Intellivision as a home computer and the direction of Mattel Electronics questioned. Krakauer and Rochlis resigned, and Josh Denham became the new president of Mattel Electronics. The Keyboard Component was no longer promoted in advertising. A series of advertisements starring George Plimpton used side-by-side game comparisons to demonstrate the superior graphics and sound of Intellivision over the Atari 2600. One slogan called Intellivision \"the closest thing to the real thing\". One such example compared golf games; where the 2600's games had a blip sound and cruder graphics, the Intellivision featured a realistic swing sound and striking of the ball and a more 3D look. In 1980, Mattel sold out its 190,000 stock of Intellivision Master Components, along with one million cartridges. In 1981, more than one million Intellivision consoles were sold, more than five times the amount of the previous year. Mattel Electronics became a subsidiary and relocated to another building to accommodate their growth. In 1982, they sold 1.8 million Intellivisions.\nThe Intellivision Master Component was branded and distributed by various companies. Before Mattel shifted manufacturing to Hong Kong, Mattel Intellivision consoles were manufactured by GTE Sylvania. \"GTE Sylvania\" Intellivision consoles were produced along with Mattel's, differing only by the brand name. The Sears Super Video Arcade, manufactured by Mattel in Hong Kong, has a restyled beige top cover and detachable controllers. Its default title screen lacks the \"Mattel Electronics\" captioning. In 1982, Radio Shack marketed the Tandyvision One, similar to the original console but with the gold plates replaced with more wood trim. In Japan, Intellivision consoles were branded for Bandai in 1982, and in Brazil there were Digimed and Digiplay consoles manufactured by Sharp in 1983.\nSoftware.\nInside every Intellivision console is 4K of ROM containing the Exec software. It provides two benefits: reusable code that can effectively make a 4K cartridge an 8K game and a software framework for new programmers to develop games more easily and quickly. It also allows other programmers to more easily review and continue another's project. Under the supervision of David Rolfe at APh, and with graphics from Mattel artist Dave James, APh was able to quickly create the Intellivision launch game library using mostly summer students. The drawback is that to be flexible and handle many different types of games, the Exec runs less efficiently than a dedicated program. Intellivision games that leverage the Exec run at a 20\u00a0Hz frame rate instead of the 60\u00a0Hz frame rate for which the Intellivision was designed. Using the Exec framework is optional, but almost all Intellivision games released by Mattel Electronics use it and thus run at 20\u00a0Hz. The limited ROM space in the early years of Intellivision game releases also means there is no space for a computer player, so many early multiplayer games require two human players.\nInitially, all Intellivision games were programmed by an outside firm, APh Technological Consulting, with 19 cartridges produced before Christmas 1980. Once the Intellivision project became successful, software development was brought in-house. Mattel formed its own software development group and began hiring programmers. The original five members of that Intellivision team were Mike Minkoff, Rick Levine, John Sohl, Don Daglow, and manager Gabriel Baum. Levine and Minkoff, a long-time Mattel Toys veteran, both transferred from the hand-held Mattel game engineering team. During 1981, Mattel hired programmers as fast as possible. Early in 1982 Mattel Electronics relocated from Mattel headquarters to an unused industrial building. Offices were renovated as new staff moved in. To keep these programmers from being hired away by rival Atari, their identities and work location was kept a closely guarded secret. In public, the programmers were referred to collectively as the Blue Sky Rangers.\nMost of the early games are based on traditional real-world concepts such as sports, with an emphasis on realism and depth of play within the technology of the time. The Intellivision was not marketed as a toy; as such, games such as \"Sea Battle\" and \"B-17 Bomber\" are not made in the pick-up-and-play format like arcade games. Reading the instructions is often a prerequisite. Every cartridge produced by Mattel Electronics includes two plastic controller overlays to help navigate the 12-button keypad, although not every game uses it. Game series, or networks, are \"Major League Sports\", \"Action\", \"Strategy\", \"Gaming\", \"Children's Learning\", and later \"Space Action\" and \"Arcade\". The network concept was dropped in 1983, as was the convenient gatefold-style box for storing the cartridge, instructions, and overlays.\nStarting in 1981, programmers looking for credit and royalties on sales began leaving both APh and Mattel Electronics to create Intellivision cartridges for third-party publishers. They helped form Imagic in 1981, and in 1982 others joined Activision and Atari. Cheshire Engineering was formed by a few senior APh programmers including David Rolfe, author of the Exec, and Tom Loughry, creator of one of the most popular Intellivision games, \"\". Cheshire created Intellivision games for Activision. Third-party developers Activision, Imagic, and Coleco started producing Intellivision cartridges in 1982, and Atari, Parker Brothers, Sega, and Interphase followed in 1983. The third-party developers, not having legal access to Exec knowledge, often bypassed the Exec framework to create smooth 30\u00a0Hz and 60\u00a0Hz Intellivision games such as \"The Dreadnaught Factor\". Cheaper ROM prices also allowed for progressively larger games as 8K, 12K, and 16K cartridges became common. The first Mattel Electronics Intellivision game to run at 60\u00a0Hz was \"Masters of the Universe\" in 1983. Marketing dubbed the term \"Super Graphics\" on the game's packaging and marketing.\nMattel Electronics had a competitive advantage in its team of experienced and talented programmers. As competitors often depended on licensing well known trademarks to sell video games, Mattel focused on original ideas. Don Daglow was a key early programmer at Mattel and became director of Intellivision game development. Daglow created \"Utopia\", a precursor to the sim genre and, with Eddie Dombrower, the ground-breaking sports simulation \"World Series Major League Baseball\". Daglow was also involved with the popular Intellivision games \"Tron Deadly Discs\" and \"Shark! Shark!\". After Mattel Electronics closed in 1984, its programmers continued to make significant contributions to the videogame industry. Don Daglow and Eddie Dombrower went on to Electronic Arts to create \"Earl Weaver Baseball\", and Don Daglow founded Stormfront Studios. Bill Fisher, Steve Roney, and Mike Breen founded Quicksilver Software, and David Warhol founded Realtime Associates.\nKeyboard Component.\nThe Intellivision was designed as a modular home computer; so, from the beginning, its packaging, promotional materials, and television commercials promised the addition of a forthcoming accessory called the Keyboard Component. The Master Component was packaged as a stand-alone video game system to which the Keyboard Component could be added, providing the computer keyboard and tape drive. Not meant to be a hobbyist or business computer, the Intellivision home computer was meant to run pre-programmed software and bring \"data flow\" (Videotex) into the home.\nThe Keyboard Component adds an 8-bit 6502 processor, making the Intellivision a dual-processor computer. It has 16K 10-bit shared RAM that can load and execute both Intellivision CP1610 and 6502 program code from tape, which is a large amount as typical contemporary cartridges are 4K. The cassettes have two tracks of digital data and two tracks of analog audio, completely controlled by the computer. Two tracks are read-only for the software, and two tracks are for user data. The tape drive is block addressed with high speed indexing. A high resolution 40\u00d724 monochrome text display can overlay regular Intellivision graphics. There is a microphone port and two expansion ports for peripherals and RAM. The Microsoft BASIC programming cartridge uses one of these ports. Expanded memory cartridges support 1,000 pages of each. A third pass-through cartridge port is for regular Intellivision cartridges. It uses the Intellivision's power supply.\nDavid Rolfe of APh wrote a control program for the Keyboard Component called PicSe (Picture Sequencer) specifically for the development of multimedia applications. PicSe synchronizes the graphics and analog audio while concurrently saving or loading tape data. Productivity software for home finances, personal improvement, and self education were planned. Subject experts were consulted and their voices recorded and used in the software. Only two applications using the PicSe system were released on cassette tape: \"Conversational French\" and \"Jack Lalanne's Physical Conditioning\". Cassettes in development include \"Super Football\", \"Spelling Challenge\", \"Chartcraft Stock Analysis\", and \"Jeanne Dixon Astrology\".\nPrograms written in BASIC do not have access to Intellivision graphics and were sold at a lower price. Five BASIC applications were released on tape: \"Family Budgeting\", \"Geography Challenge\", and \"Crosswords I, II, and III\".\nThe Keyboard Component was an ambitious piece of engineering for its time, and it was repeatedly delayed as engineers tried to reduce manufacturing costs. In August 1979, a breadboard form of the Component was successfully entered into the Sears Market Research Program. In December 1979, Mattel had production design working units but decided on a significant internal design change to consolidate circuit boards. In September 1980, it was test marketed in Fresno, California, but without software, except for the BASIC programming cartridge. In late 1981, design changes were finally implemented and the Keyboard Component was released at $ in Seattle and New Orleans only. Customers who complained in writing could buy a Keyboard Component directly from Mattel. The printer, a rebadged Alphacom Sprinter 40, was only available by mail order.\nThe Keyboard Component's repeated delays became so notorious around Mattel headquarters that comedian Jay Leno, when performing at Mattel's 1981 Christmas party, got his biggest response of the evening with the line: \"You know what the three big lies are, don't you? 'The check is in the mail', 'I'll still respect you in the morning', and 'The keyboard will be out in spring.'\"\nComplaints from consumers who had chosen to buy the Intellivision specifically on the promise of a \"coming soon\" personal-computer upgrade eventually caught the attention of the Federal Trade Commission (FTC), who started investigating Mattel Electronics for fraud and false advertising. Mattel explained to the FTC that the Keyboard Component was a failed product, avoiding fines. Mattel subsequently cancelled the product in August 1982, and offered to buy back all of the existing Keyboard Components from customers. Mattel provided a full refund, but customers without a receipt received $ for the Keyboard Component, $ for the BASIC cartridge, and $ for each cassette software. Any customer who opted to keep the products was required to sign a waiver with the understanding that no more software would be written for the system and absolving Mattel of any future responsibility for technical support. They were also compensated with $ worth of Mattel Electronics products.\nThough approximately 4,000 Keyboard Components were manufactured, it is not clear how many of them were sold and they are rare. Many of the units were dismantled for parts. Others were used by Mattel Electronics programmers as part of their development system. A Keyboard Component could be interfaced with an Intellivision development system in place of the hand-built Magus board RAM cartridge. Data transfer to the Keyboard Component RAM is done serially and is slower than the Magus board parallel interface.\nThe keyboard component debacle was ranked as No. 11 on \"GameSpy\"'s \"25 dumbest moments in gaming\".\nEntertainment Computer System (ECS).\nIn mid-1981, Mattel's upper management was becoming concerned that the Keyboard Component group would never be able to produce a sellable product. As a result, Design and Development set up a competing engineering team whose stated mission was to produce an inexpensive add-on called the \"Basic Development System\", or BDS, to be sold as an educational device to introduce kids to the concepts of computer programming.\nThe rival BDS engineering group eventually came up with a much less expensive alternative. Originally dubbed the \"Lucky\", from LUCKI: Low User-Cost Keyboard Interface, it lacked many of the sophisticated features envisioned for the original Keyboard Component. Gone, for example, was the 16K (8MB max) of RAM, the secondary CPU, and high resolution text; instead, the ECS offered a mere 2KB RAM expansion, a built-in BASIC that was marginally functional, plus a much-simplified cassette and printer interface. Ultimately, this fulfilled the original promise of turning the Intellivision into a computer, making it possible to write programs and store them to tape as well as interfacing with a printer. It even offered, via an additional sound chip (AY-3-8917) inside the ECS module and an optional 49-key music synthesizer keyboard, the possibility of turning the Intellivision into a multi-voice synthesizer which could be used to play or learn music.\nIn the fall of 1982, the LUCKI, now renamed the Entertainment Computer System (ECS), was presented at the annual sales meeting, officially ending the ill-fated keyboard component project. A new advertising campaign was aired in time for the 1982 Christmas season, and the ECS itself was shown to the public at the January 1983 Consumer Electronics Show (CES) in Las Vegas. However, it would not see release until late December as the \"Intellivision Computer Module\".\nPrior to release, an internal shake-up at the top levels of Mattel Electronics' management had caused the company's focus to shift away from hardware add-ons in favor of software, and the ECS received very little in terms of furthering the marketing push. Further hardware developments, including a planned Program Expander that would have added another 16K of RAM and a more intricate, fully featured Extended-BASIC to the system, were halted. In the end, six games were released for the ECS; a few more were completed but not released.\nThe ECS Computer Module also offered four player game-play with the optional addition of two extra hand controllers. Four player games were in development when Mattel Electronics closed in 1984. \"World Cup Soccer\" was later completed and released in 1985 by Dextel in Europe and then INTV Corporation in North America. The documentation does not mention it but when the ECS Computer Adapter is used, \"World Cup Soccer\" can be played with one to four players, or two players cooperatively against the computer.\nIntellivoice.\nIn 1982, Mattel introduced the Intellivoice Voice Synthesis Module, a speech synthesizer for compatible cartridges. The Intellivoice was novel in two respects: human sounding male and female voices with distinct accents, and speech-supporting games designed with speech as an integral part of the gameplay.\nLike the Intellivision chipset, the Intellivoice chipset was developed by General Instrument. The SP0256-012 orator chip has 2KB ROM inside and is used to store the speech for numerical digits, some common words, and the phrase \"Mattel Electronics presents\". Speech can also be processed from the Intellivoice's SP650 buffer chip, stored and loaded from cartridge memory. That buffer chip has its own I/O and the Intellivoice has a 30-pin expansion port under a removable top plate. Mattel Electronics planned to use that connector for wireless hand controllers.\nMattel Electronics built a state of the art voice processing lab to produce the phrases used in Intellivoice games. However, the amount of speech that could be compressed into an 8K or 12K cartridge and still leave room for a game was limited. Intellivoice cartridges \"Space Spartans\" and \"B-17 Bomber\" did sell about 300,000 copies each, priced a few dollars more than regular Intellivision cartridges. However, at $79, the Intellivoice did not sell as well as Mattel expected; Intellivoices were later offered free with the purchase of a Master Component. In August 1983, the Intellivoice system was quietly phased out. A children's title called \"Magic Carousel\" and foreign-language versions of \"Space Spartans\" were completed but shelved. Additional games \"Woody Woodpecker\" and \"Space Shuttle\" went unfinished with the voice recordings unused.\nFour Intellivoice games were released: \"Space Spartans\", \"B-17 Bomber\", \"Bomb Squad\", and \"\".\nA fifth game, \"Intellivision World Series Major League Baseball\", developed as part of the Entertainment Computer System series, also supports the Intellivoice if both the ECS and Intellivoice are connected concurrently. Unlike the Intellivoice-specific games, however, \"World Series Major League Baseball\" is also playable without the Intellivoice module (but not without the ECS).\nIntellivision II.\nIn the spring of 1983, Mattel introduced the \"Intellivision II\", a cheaper, more compact redesign of the original, that was designed to be less expensive to manufacture and service, with updated styling. It also had longer controller cords. The Intellivision II was initially released without a pack-in game but was later packaged with \"BurgerTime\" in the United States and \"Lock 'n' Chase\" in Canada. In 1984, the Digiplay Intellivision II was introduced in Brazil. Brazil was the only country outside North America to have the redesigned Intellivision II.\nUsing an external AC Adapter (16.2V AC), consolidating some ICs, and taking advantage of relaxed FCC emission standards, the Intellivision II has a significantly smaller footprint than the original. The controllers, now detachable, have a different feel, with plastic rather than rubber side buttons and a flat membrane keypad. Users of the original Intellivision missed the ability to find keypad buttons by the tactile feel of the original controller bubble keypad.\nOne functional difference was the addition of a video input to the cartridge port, added specifically to support the System Changer, an accessory also released in 1983 by Mattel that played Atari 2600 cartridges through the Intellivision. The Intellivision hand controllers could be used to play Atari 2600 games. The System Changer also had two controller ports compatible with Atari joysticks. The original Intellivision required a hardware modification, a service provided by Mattel, to work with the \"System Changer\". Otherwise the Intellivision II was promoted to be compatible with the original.\nIt was discovered that a few Coleco Intellivision games did not work on the Intellivision II. Mattel secretly changed the Exec internal ROM program in an attempt to lock out third-party games. A few of Coleco's early games were affected but the 3rd party developers quickly figured out how to get around it. Mattel's own \"Electric Company Word Fun\", however, will not run on the Intellivision II due to this change. In an unrelated issue but also due to Exec changes, Super Pro Football experiences a minor glitch where the quarterback does not appear until after the ball is hiked. There were also some minor changes to the sound chip (AY-3-8914A/AY-3-8916) affecting sound effects in some games. Programmers at Mattel discovered the audio differences and avoided the problem in future games.\nDecade.\nAs early as 1981, Dave Chandler's group began designing what would have been Mattel's next-generation console, codenamed \"Decade\" and now referred to as the \"Intellivision IV\". It would have been based on the 32-bit MC68000 processor and a 16-bit custom designed advanced graphic interface chip. Specifications called for dual-display support, 240\u00d7192 bitmap resolution, 16 programmable 12-bit colors (4096 colors), antialiasing, 40\u00d724 tiled graphics modes, four colors per tile (16 with shading), text layer and independent scrolling, 16 multicolored 16\u00d716 sprites per scan-line, 32 level hardware sprite scaling. Line interrupts for reprogramming sprite and color registers would allow for many more sprites and colors on screen at the same time. It was intended as a machine that could lead Mattel Electronics into the 1990s; however, on August 4, 1983, most hardware people at Mattel Electronics were laid off.\nIntellivision III.\nAlso in 1981, Mattel Electronics executives indicated to APh, interest in a successor system for 1983. Although planned for some time, APh redirected staff efforts on the Intellivision III hardware around summer 1982. Based on a faster CP1610 for backward compatibility, APh developed an updated graphics STIC chip with 4x the resolution, more sprites, and more colors. Mattel Electronics programmers developing the EXEC software. When Mattel Electronics cancelled the project in mid-1983, Toshiba was laying out the new graphics chip, consoles expected to be in production by Christmas, cartridges to be ready by January 1984, according to Glenn Hightower of APh. A\u00a0Mattel\u00a0document titled\u00a0Target Specification Intellivision III\u00a0has the following.\nCompetition and market crash.\nAccording to the company's 1982 Form 10-K, Mattel had almost 20% of the domestic video-game market. Mattel Electronics provided 25% of revenue and 50% of operating income in fiscal 1982. Although the Atari 2600 had more third-party development, \"Creative Computing Video &amp; Arcade Games\" reported after visiting the summer 1982 Consumer Electronics Show that \"the momentum is tremendous\". Activision and Imagic began releasing games for the Intellivision, as did hardware rival Coleco. Mattel created \"M Network\" branded games for Atari's system. The company's advertisement budget increased to over $ for the year. In its October 1982 stockholders' report Mattel announced that \"Electronics\" had, so far that year, posted a nearly $ profit on nearly $ sales; a threefold increase over October 1981.\nHowever, the same report predicted a loss for the upcoming quarter. Hiring still continued, as did the company's optimism that the investment in software and hardware development would pay off. The \"M Network\" brand expanded to personal computers. An office in Taiwan was opened to handle Apple II programming. The original five-person Mattel game development team had grown to 110 people under new vice president Baum, while Daglow led Intellivision development and top engineer Minkoff directed all work on all other platforms. In February 1983, Mattel Electronics opened an office in the south of France to provide European input to Intellivision games and develop games for the ColecoVision. At its peak Mattel Electronics employed 1800 people.\nAmid the flurry of new hardware and software development, there was trouble for the Intellivision. New game systems (ColecoVision and Atari 5200) introduced in 1982 took advantage of falling RAM prices to offer graphics closer to arcade quality. In 1983, the price of home computers, particularly the Commodore 64, came down drastically to compete with video game system sales. The market became flooded with hardware and software, and retailers were ill-equipped to cope. In spring 1983, hiring at Mattel Electronics came to a halt.\nAt the June 1983 Consumer Electronics Show in Chicago, Mattel Electronics had the opportunity to show off all their new products. The response was underwhelming. Several people in top management positions were replaced due to massive losses. On July 12, 1983, Mattel Electronics President Josh Denham was replaced with outsider Mack Morris. Morris brought in former Mattel Electronics president and marketing director Jeff Rochlis as a consultant and all projects were under review. The Intellivision III was cancelled and then all new hardware development was stopped when 660 jobs were cut on August 4. The price of the Intellivision II (which launched at $ earlier that year) was lowered to $, and Mattel Electronics was to be a software company. However, by October 1983, Electronics' losses were over $ for the year and one third of the programming staff were laid off. Another third were gone by November, and, on January 20, 1984, the remaining programming staff were laid off. The Taiwan and French offices continued a little while longer due to contract and legal obligations. On February 4, 1984, Mattel sold the Intellivision business for $. In 1983, 750,000 Intellivision Master Components were sold, compared to 1.8 million in 1982.\nINTV Corporation (1984\u20131990).\nFormer Mattel Electronics Senior Vice President of Marketing, Terrence Valeski, understood that although losses were huge, the demand for video games increased in 1983. Valeski found investors and purchased the rights to Intellivision, the games, and inventory from Mattel. A new company, Intellivision Inc, was formed and by the end of 1984 Valeski bought out the other investors and changed the name to INTV Corporation. They continued to supply the large toy stores and sold games through direct mail order. At first they sold the existing inventory of games and Intellivision II systems. When the inventory of games sold out they produced more, but without the Mattel name or unnecessary licenses on the printed materials. To lower costs, the boxes, instructions, and overlays were produced at lower quality compared to Mattel.\nIn France, the Mattel Electronics office found investors and became Nice Ideas in April 1984. They continued to work on Intellivision, Colecovision, and other computer games. They produced Intellivision \"World Cup Soccer\" and \"Championship Tennis\", both released in 1985 by European publisher Dextel.\nIn 1985, INTV Corporation introduced the \"INTV System III\", also branded as the \"Intellivision Super Pro System\", using the same design as the original Intellivision model but in black and silver. That same year INTV Corp introduced two new games that were completed at Mattel but not released: \"Thunder Castle\" and \"World Championship Baseball\". With their early success INTV Corp decided to produce new games and in 1986 introduced \"Super Pro Football\", an update of Mattel \"NFL Football\". INTV Corp continued a relationship that Mattel had with Data East and produced all new cartridges such as \"Commando\" in 1987 and \"Body Slam Wrestling\" in 1988. Also in 1987, INTV Corp released \"Dig Dug\", purchased from Atari where the game was completed but not released in 1984. They also got into producing next-generation games with the production of \"Monster Truck Rally\" for Nintendo Entertainment System (NES) in 1991, also released as \"Stadium Mud Buggies\" for Intellivision in 1989.\nLicensing agreements with Nintendo and Sega required INTV Corporation to discontinue the Intellivision in 1990. INTV Corporation did publish 21 new Intellivision cartridges bringing the Intellivision library to a total of 124 cartridges plus one compilation cartridge.\nTutorvision.\nIn 1989, INTV Corp and World Book Encyclopedia entered into an agreement to manufacture an educational video game system called Tutorvision. It is a modified Intellivision, the case molded in light beige with gold and blue trim. The Exec ROM expanded, system RAM increased to 1.75K, and graphics RAM increased to 2KB. That is enough graphics RAM to define unique graphic tiles for the entire screen.\nGames were designed by World Book, \"J. Hakansson Associates\", and programmed by Realtime Associates. Sixteen games were in production, plus one Canadian variation. However, the cartridges and the Tutorvision were never released; instead World Book and INTV Corporation sued each other. In 1990, INTV Corporation filed for bankruptcy protection and closed in 1991.\nAn unknown number of later Intellivision SuperPro systems have Tutorvision hardware inside. A subset of these units contain the full Tutorvision EXEC and can play Tutorvision games.\nHardware specifications.\nMaster Component.\n\"Intellivision, Super Video Arcade, Tandyvision One, Intellivision II, INTV System III, Super Pro System\"\nGame controller.\nThe Intellivision controller features:\nThe directional pad was called a \"control disc\" and marketed as having the \"functionality of both a joystick and a paddle\". The controller was ranked the fourth worst video game controller by IGN editor Craig Harris.\nReception.\nA July 1980 article in Video magazine said \"Now, arcade addicts can revel in the most sophisticated games this side of the complex simulations designed for high-level computers right in their own livingrooms.\", \"It may not be perfect but it's certainly the best unit offered so far to players of electronic video games.\", \"Those used to joysticks will have to endure a short period of adjustment, but even finicky players will be forced to agree that the company has developed a truly elegant solution to the controller problem.\"\nKen Uston published \"Ken Uston's Guide to Buying and Beating the Home Video Games\" in 1982 as a guide to potential buyers of console systems/cartridges, as well as a brief strategy guide to numerous cartridge games then in existence. He described Intellivision as \"the most mechanically reliable of the systems\u2026 The controller (used during \"many hours of experimentation\") worked with perfect consistency. The unit never had overheating problems, nor were loose wires or other connections encountered.\" However, Uston rated the controls and control system as \"below average\" and the worst of the consoles he tested (including Atari 2600, Magnavox Odyssey\u00b2, Astrovision, and Fairchild Channel F).\nJeff Rovin lists \"Intellivision\" as one of the seven major suppliers of videogames in 1982 and mentions it as \"the unchallenged king of graphics\", but says the controllers can be \"difficult to operate\", mentions the fact that if a controller breaks the entire unit must be shipped off for repairs (since they did not detach at first), and explains that the overlays \"are sometimes so stubborn as to tempt one's patience\" .\nA 1996 article in \"Next Generation\" said the Intellivision \"had greater graphics power than the dominant Atari 2600. It was slower than the 2600 and had less software available, but it was known for its superior sports titles.\" A year later, \"Electronic Gaming Monthly\" assessed the Intellivision in an overview of older gaming consoles, remarking that the controllers \"were as comfortable as they were practical. The unique disk-shaped directional pad provided unprecedented control for the time, and the numeric keypad opened up new options previously unavailable in console gaming.\" They praised the breadth of the software library but said there was a lack of genuinely stand-out games.\nLegacy.\nIntellivision Lives!\nIntellivision games became readily available again when Keith Robinson and Stephen Roney, both former Intellivision programmers at Mattel Electronics, obtained exclusive rights to the Intellivision and games in 1997. That year they formed a new company, Intellivision Productions, and made \"Intellivision for PC Volume 1\" available as a free download. Intellivision games could be played on a modern computer for the first time. That download includes three Intellivision games and an MS-DOS Intellivision emulator that plays original game code. It was followed by \"Volume 2\" and another three games including \"Deep Pockets Super Pro Pool &amp; Billiards\"; a game completed in 1990 but never released until this download in 1997. In 2000, the \"Intellipack 3\" download was available with another four Intellivision games and emulators for Windows or Macintosh.\nIntellivision Productions released \"Intellivision Lives!\" and \"Intellivision Rocks\" on compact disc in 1998 and 2001. These compilation CDs play the original game code through emulators for MS-DOS, Windows, and Macintosh computers. Together they have over 100 Intellivision games including never before released \"King of the Mountain, Takeover, Robot Rubble\", \"League of Light\", and others. Intellivision Rocks includes Intellivision games made by Activision and Imagic. Some games could not be included due to licensing, others simply used different titles to avoid trademarked names. The CDs are also a resource for development history, box art, hidden features, programmer biographies, video interviews, and original commercials.\nAlso in 1997, Intellivision Productions announced they would sell development tools allowing customers to program their own Intellivision games. They were to provide documentation, PC compatible cross-assemblers, and the \"Magus II\" PC Intellivision cartridge interface. Unfortunately, the project was cancelled but they did provide copies of \"Your Friend the EXEC\", the programmers guide to the Intellivision Executive control software. By 2000 Intellivision hobbyists ultimately created their own development tools, including Intellivision memory cartridges.\nIn 2005, Intellivision Productions announced that new Intellivision cartridges were to be produced. \"Deep Pockets and Illusions will be the first two releases in a series of new cartridges for the Intellivision. The printed circuit boards, the cartridge casings, the boxes are all being custom manufactured for this special series.\" \"Illusions\" was completed at Mattel Electronics' French office in 1983 but never released. \"Deep Pockets Super Pro Pool &amp; Billiards\" was programmed for INTV Corporation in 1990 and only released as a ROM file in 1998. However, no cartridges were produced. Previously, in 2000, Intellivision Productions did release new cartridges for the Atari 2600 and Colecovision. \"Sea Battle\" and \"Swordfight\" were Atari 2600 games created by Mattel Electronics in the early 1980s but not previously released. \"Steamroller\" (Colecovision) was developed for Activision in 1984 and not previously released.\nLicensing Intellivision Games.\nAlso in 1999, Activision released \"A Collection of Intellivision Classic Games\" for PlayStation. Also known as \"Intellivision Classics\", it has 30 emulated Intellivision games as well as video interviews of some of the original programmers. All of the games were licensed from Intellivision Productions and none of the Activision or Imagic Intellivision games were included. In 2003, Crave Entertainment released a PlayStation 2 version of Intellivision Lives! followed by versions for the Xbox and GameCube in 2004. In 2010, Virtual Play Games released Intellivision Lives! for the Nintendo DS including one never before released game, \"Blow Out\". In 2008 Microsoft made Intellivision Lives! an available download on the Xbox Live Marketplace as an Xbox Original and playable on the Xbox 360.\nIn 2003, the Intellivision 25 and Intellivision 10 direct-to-TV systems were released by Techno Source Ltd. These are an all-in-one single controller design that plugs directly into a television. One includes 25 games the other ten. These Intellivision games were not emulated but rewritten for the native processor (Famiclone-based hardware) and adapted to a contemporary controller. As such they look and play differently than Intellivision. In 2005 they were updated for two-player play as the Intellivision X2 with 15 games. They were commercially very successful altogether selling about 4 million units by end of 2006.\nSeveral licensed Intellivision games became available to Windows computers through the GameTap subscription gaming service in 2005 including \"Astrosmash, Buzz Bombers, Hover Force, Night Stalker, Pinball, Shark! Shark!, Skiing and Snafu\". Installation of the GameTap Player software was required to access the emulator and games. The VH1 Online Arcade made nine Intellivision games available in 2007. Using a Shockwave emulator these Intellivision games could be played directly through a web browser with Shockwave Player. In 2010, VH1 Classic and MTV Networks released 6 Intellivision games to iOS. Intellivision games were first adapted to mobile phones and published by THQ Wireless in 2001. On March 24, 2010, Microsoft launched the Game Room service for Xbox Live and Games for Windows Live. This service includes support for Intellivision games and allows players to compete for high scores via online leaderboards. At the 2011 Consumer Electronics Show, Microsoft announced a version of Game Room for Windows Phone, promising a catalog of 44 Intellivision games. AtGames and its Direct2Drive digital store has Windows compatible Intellivision compilations available for download purchase.\nIntellivision Flashback.\nThe number of Intellivision games that can be played effectively with contemporary game controllers is limited. On October 1, 2014, AtGames Digital Media, Inc., under license from Intellivision Productions, Inc., released the Intellivision Flashback classic game console. It is a miniature sized Intellivision console with two original sized Intellivision controllers. While adapters have been available to interface original Intellivision controllers to personal computers, the Intellivision Flashback includes two new Intellivision controllers identical in layout and function to the originals. It comes with 60 (61 at Dollar General) emulated Intellivision games built into ROM and a sample set of plastic overlays for 10 games. The Advanced Dungeons &amp; Dragons games were included as \"Crown of Kings\" and \"Minotaur\". As with many of the other Intellivision compilations, no games requiring third-party licensing were included.\nIntellivision Entertainment.\nIn May 2018, Tommy Tallarico announced that he acquired the rights to the Intellivision brand and games with plans to launch a new home video game console, the Intellivision Amico. A new company, Intellivision Entertainment, was formed with Tallarico serving as president. Intellivision Productions has been renamed Blue Sky Rangers Inc. and their video game intellectual property has been transferred to Intellivision Entertainment.\nIn 2021 Blaze Entertainment released a collection of twelve emulated Intellivision games for the Evercade systems. They released a second collection of twelve emulated Intellivision games in 2022.\nIntellivision Sprint.\nOn May 23, 2024, Atari SA announced the acquisition of the Intellivision brand and library from Intellivision Entertainment. The deal did not include the unreleased Intellivision Amico console nor the Intellivision Entertainment company itself, both of which would be renamed. However, that company would secure a licensing deal with Atari to continue to release newer versions of Intellivision titles for the Amico.\nAtari announced the Intellivision Sprint for pre-orders in October 2025, and shipping on December 5, 2025 for $150. As a retro console based on the original Intellivision design, the Intellivision Sprint will include 45 games from the original Intellivision. It will include two controllers based on the original design of the Intellivision, though both will be wireless and charged by USB ports on the console. Overlays for each of the built in games will be included. The console also supports HDMI video output and USB-A connections.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15311", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=15311", "title": "Intelligent Design Theory", "text": ""}
{"id": "15315", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=15315", "title": "Interfaith organizations", "text": ""}
{"id": "15316", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=15316", "title": "Imperialism", "text": "Extension of rule over foreign nations\nImperialism is the maintaining and extending of power over foreign nations, particularly through expansionism, employing both hard power (military and economic power) and soft power (diplomatic power and cultural imperialism). Imperialism focuses on establishing or maintaining hegemony and a more formal empire.\nWhile related to the concept of colonialism, imperialism is a distinct concept that can apply to other forms of expansion and many forms of government.\nEtymology and usage.\nThe word \"imperialism\" was derived from the Latin word , which means 'to command', 'to be sovereign', or simply 'to rule'. It was coined in the 19th century to decry Napoleon III's despotic militarism and his attempts at obtaining political support through foreign military interventions. The term became common in the current sense in Great Britain during the 1870s; by the 1880s it was used with a positive connotation. By the end of the 19th century, the term was used to describe the behavior of empires at all times and places. Hannah Arendt and Joseph Schumpeter defined imperialism as expansion for the sake of expansion.\n\"Imperialism\" was and is mainly used to refer to Western and Japanese political and economic dominance, especially in Asia and Africa, in the 19th and 20th centuries. Its precise meaning continues to be debated by scholars. Some writers, such as Edward Said, use the term more broadly to describe any system of domination and subordination organized around an imperial core and a periphery. This definition encompasses both nominal empires and neocolonialism. Imperialism has also been identified in newer phenomena like space development and its governing context.\nVersus colonialism.\nThe term \"imperialism\" is often conflated with \"colonialism\"; however, many scholars have argued that each has its own distinct definition. Imperialism and colonialism have been used in order to describe one's influence upon a person or group of people. Robert Young writes that imperialism operates from the centre as a state policy and is developed for ideological as well as financial reasons, while colonialism is simply the development for settlement or commercial intentions; however, colonialism still includes invasion. Colonialism in modern usage also tends to imply a degree of geographic separation between the colony and the imperial power. Particularly, Edward Said distinguishes between imperialism and colonialism by stating: \"imperialism involved 'the practice, the theory and the attitudes of a dominating metropolitan center ruling a distant territory', while colonialism refers to the 'implanting of settlements on a distant territory.'\" Contiguous land empires such as the Russian, Chinese or Ottoman have traditionally been excluded from discussions of colonialism, though this is beginning to change, since it is accepted that they also sent populations into the territories they ruled.\nImperialism and colonialism both dictate the political and economic advantage over a land and the indigenous populations they control, yet scholars sometimes find it difficult to illustrate the difference between the two. Although imperialism and colonialism focus on the suppression of \"another\", if colonialism refers to the process of a country taking physical control of another, imperialism refers to the political and monetary dominance, either formally or informally. Colonialism is seen to be the architect deciding how to start dominating areas and then imperialism can be seen as creating the idea behind conquest cooperating with colonialism. Colonialism is when the imperial nation begins a conquest over an area and then eventually is able to rule over the areas the previous nation had controlled. Colonialism's core meaning is the exploitation of the valuable assets and supplies of the nation that was conquered and the conquering nation then gaining the benefits from the spoils of the war. The meaning of imperialism is to create an empire, by conquering the other state's lands and therefore increasing its own dominance. Colonialism is the builder and preserver of the colonial possessions in an area by a population coming from a foreign region. Colonialism can completely change the existing social structure, physical structure, and economics of an area; it is not unusual that the characteristics of the conquering peoples are inherited by the conquered indigenous populations.\nThe Soviet leader Vladimir Lenin suggested that \"imperialism was the highest form of capitalism\", claiming that \"imperialism developed after colonialism, and was distinguished from colonialism by monopoly capitalism\".\nAge of Imperialism.\nImperialism has been present and prominent since the beginning of history, and its most intensive phase occurred in the Axial Age. But the concept of the \"Age of Imperialism\" refers to the period pre-dating World War I. While the end of the period is commonly fixed in 1914, the date of the beginning varies between 1760 and 1870. The latter date makes the Age of Imperialism identical with the New Imperialism. According to Historians Daniel Hedinger and , the widespread use of the term \"Age of Empire\" for this specific period reflects a Eurocentric bias in terms of time.\nThe Age saw European nations, helped by industrialization, intensifying the process of colonizing, influencing, and annexing other parts of the world. In the late 19th century, they were joined by the United States and Japan. Other 19th century episodes included the Scramble for Africa and Great Game.\nIn the 1970s British historians John Gallagher (1919\u20131980) and Ronald Robinson (1920\u20131999) argued that European leaders rejected the notion that \"imperialism\" required formal, legal control by one government over a colonial region. Much more important was informal control of independent areas. According to Wm. Roger Louis, \"In their view, historians have been mesmerized by formal empire and maps of the world with regions colored red. The bulk of British emigration, trade, and capital went to areas outside the formal British Empire. Key to their thinking is the idea of empire 'informally if possible and formally if necessary.'\" Oron Hale says that Gallagher and Robinson looked at the British involvement in Africa where they \"found few capitalists, less capital, and not much pressure from the alleged traditional promoters of colonial expansion. Cabinet decisions to annex or not to annex were made, usually on the basis of political or geopolitical considerations.\"\nLooking at the main empires from 1875 to 1914, there was a mixed record in terms of profitability. At first, planners expected that colonies would provide an excellent captive market for manufactured items. Apart from the Indian subcontinent, this was seldom true. By the 1890s, imperialists saw the economic benefit primarily in the production of inexpensive raw materials to feed the domestic manufacturing sector. Overall, Great Britain did very well in terms of profits from India, especially Mughal Bengal, but not from most of the rest of its empire. According to Indian Economist Utsa Patnaik, the scale of the wealth transfer out of India, between 1765 and 1938, was an estimated $45 Trillion. The Netherlands did very well in the East Indies. Germany and Italy got very little trade or raw materials from their empires. France did slightly better. The Belgian Congo was notoriously profitable when it was a capitalistic rubber plantation owned and operated by King Leopold II as a private enterprise. However, scandal after scandal regarding atrocities in the Congo Free State led the international community to force the government of Belgium to take it over in 1908, and it became much less profitable. The Philippines cost the United States much more than expected because of military action against rebels.\nBecause of the resources made available by imperialism, the world's economy grew significantly and became much more interconnected in the decades before World War I, making the many imperial powers rich and prosperous.\nEurope's expansion into territorial imperialism was largely focused on economic growth by collecting resources from colonies, in combination with assuming political control by military and political means. The colonization of India in the mid-18th century offers an example of this focus: there, the \"British exploited the political weakness of the Mughal state, and, while military activity was important at various times, the economic and administrative incorporation of local elites was also of crucial significance\" for the establishment of control over the subcontinent's resources, markets, and manpower. Although a substantial number of colonies had been designed to provide economic profit and to ship resources to home ports in the 17th and 18th centuries, D. K. Fieldhouse suggests that in the 19th and 20th centuries in places such as Africa and Asia, this idea is not necessarily valid:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nDuring this time, European merchants had the ability to \"roam the high seas and appropriate surpluses from around the world (sometimes peaceably, sometimes violently) and to concentrate them in Europe\".\nEuropean expansion greatly accelerated in the 19th century. To obtain raw materials, Europe expanded imports from other countries and from the colonies. European industrialists sought raw materials such as dyes, cotton, vegetable oils, and metal ores from overseas. Concurrently, industrialization was quickly making Europe the centre of manufacturing and economic growth, driving resource needs.\nCommunication became much more advanced during European expansion. With the invention of railroads and telegraphs, it became easier to communicate with other countries and to extend the administrative control of a home nation over its colonies. Steam railroads and steam-driven ocean shipping made possible the fast, cheap transport of massive amounts of goods to and from colonies.\nAlong with advancements in communication, Europe also continued to advance in military technology. European chemists made new explosives that made artillery much more deadly. By the 1880s, the machine gun had become a reliable battlefield weapon. This technology gave European armies an advantage over their opponents, as armies in less-developed countries were still fighting with arrows, swords, and leather shields (e.g. the Zulus in Southern Africa during the Anglo-Zulu War of 1879). Some exceptions of armies that managed to get nearly on par with the European expeditions and standards include the Ethiopian armies at the Battle of Adwa, and the Japanese Imperial Army of Japan, but these still relied heavily on weapons imported from Europe and often on European military advisors.\nTheories of imperialism.\nAnglophone academic studies often base their theories regarding imperialism on the British Empire. The term \"imperialism\" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British Prime Minister Benjamin Disraeli. Supporters of \"imperialism\" such as Joseph Chamberlain quickly appropriated the concept. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed.\nHistorians and political theorists have long debated the correlation between capitalism, class, and imperialism. Much of the debate was pioneered by such theorists as John A. Hobson (1858\u20131940), Joseph Schumpeter (1883\u20131950), Thorstein Veblen (1857\u20131929), and Norman Angell (1872\u20131967). While these non-Marxist writers were at their most prolific before World War I, they remained active in the interwar years. Their combined work informed the study of imperialism and its impact on Europe, and contributed to reflections on the rise of the military-industrial complex in the United States from the 1950s.\nIn \"Imperialism: A Study\" (1902), Hobson developed a highly influential interpretation of imperialism that expanded on his belief that free-enterprise capitalism had a harmful effect on the majority of the population. In \"Imperialism,\" he argued that the financing of overseas empires drained money that was needed at home. It was invested abroad because lower wages paid to workers overseas made for higher profits and higher rates of return, compared to domestic wages. So, although domestic wages remained higher, they did not grow nearly as fast as they might otherwise have. Exporting capital, he concluded, put a lid on the growth of domestic wages and the domestic standard of living. Hobson theorized that domestic social reforms could cure the international disease of imperialism by removing its economic foundation, while state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order.\nEuropean Marxists picked up Hobson's ideas incorporated them their own theory of imperialism, most notably in Vladimir Lenin's \"Imperialism, the Highest Stage of Capitalism\" (1916). Lenin portrayed imperialism as monopoly capitalism at the global stage, resulting in colonial expansion in order to secure capitalist economic growth and profits. Later Marxist theoreticians echo this conception of imperialism as a structural feature of capitalism, which explained the world wars as the battle between imperialists for control of external markets. Lenin's treatise became a standard textbook that flourished until the collapse of the Soviet Union in 1989\u201391.\nSome theoreticians on the non-Communist left have emphasized the structural or systemic character of \"imperialism\". Such writers have expanded the period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Colonization and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect\u2014among other shifts in sensibility\u2014a growing unease, even great distaste, with the pervasiveness of such power, specifically, Western power.\nBy the 1970s, historians such as David K. Fieldhouse, David Landes, and Oron Hale argued that the Hobsonian conception of imperialism was no longer supported. They advocated that modern imperialism was primarily a political product caused by the national mass hysteria rather than by the capitalists. The British experience failed to support it.\nWalter Rodney, in his 1972 \"How Europe Underdeveloped Africa\", proposes the idea that imperialism is a phase of capitalism \"in which Western European capitalist countries, the US, and Japan established political, economic, military and cultural hegemony over other parts of the world which were initially at a lower level and therefore could not resist domination.\" As a result, Imperialism \"for many years embraced the whole world \u2013 one part being the exploiters and the other the exploited, one part being dominated and the other acting as overlords, one part making policy and the other being dependent.\"\nJustification and issues.\nExpansionism.\nImperialism was common in the form of expansionism through vassalage, irredentism, and conquest.\nOrientalism and imaginative geography.\nImperial control, territorial and cultural, is justified through discourses about the imperialists' understanding of different spaces. Conceptually, imagined geographies explain the limitations of the imperialist understanding of the societies of the different spaces inhabited by the non\u2013European Other.\nIn \"Orientalism\" (1978), Edward Said said that the West developed the concept of The Orient\u2014an imagined geography of the Eastern world\u2014which functions as an essentializing discourse that represents neither the ethnic diversity nor the social reality of the Eastern world. That by reducing the East into cultural essences, the imperial discourse uses place-based identities to create cultural difference and psychologic distance between \"We, the West\" and \"They, the East\" and between \"Here, in the West\" and \"There, in the East\".\nThat cultural differentiation was especially noticeable in the books and paintings of early Oriental studies, the European examinations of the Orient, which misrepresented the East as irrational and backward, the opposite of the rational and progressive West. Defining the East as a negative vision of the Western world, as its inferior, not only increased the sense-of-self of the West, but also was a way of ordering the East, and making it known to the West, so that it could be dominated and controlled. Therefore, Orientalism was the ideological justification of early Western imperialism\u2014a body of knowledge and ideas that rationalized social, cultural, political, and economic control of other, non-white peoples.\nCartography.\nOne of the main tools used by imperialists was cartography. Cartography is \"the art, science and technology of making maps\" but this definition is problematic. It implies that maps are objective representations of the world when in reality they serve very political means. For Harley, maps serve as an example of Foucault's power and knowledge concept.\nTo better illustrate this idea, Bassett focuses his analysis of the role of 19th-century maps during the \"Scramble for Africa\". He states that maps \"contributed to empire by promoting, assisting, and legitimizing the extension of French and British power into West Africa\". During his analysis of 19th-century cartographic techniques, he highlights the use of blank space to denote unknown or unexplored territory. This provided incentives for imperial and colonial powers to obtain \"information to fill in blank spaces on contemporary maps\".\nAlthough cartographic processes advanced through imperialism, further analysis of their progress reveals many biases linked to eurocentrism. According to Bassett, \"[n]ineteenth-century explorers commonly requested Africans to sketch maps of unknown areas on the ground. Many of those maps were highly regarded for their accuracy\" but were not printed in Europe unless Europeans verified them.\nCultural imperialism.\nThe concept of cultural imperialism refers to the cultural influence of one dominant culture over others, i.e. a form of soft power, which changes the moral, cultural, and societal worldview of the subordinate culture. This means more than just \"foreign\" music, television or film becoming popular with young people; rather that a populace changes its own expectations of life, desiring for their own country to become more like the foreign country depicted. For example, depictions of opulent American lifestyles in the soap opera \"Dallas\" during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama-series in North Korea. The importance of soft power is not lost on authoritarian regimes, which may oppose such influence with bans on foreign popular culture, control of the internet and of unauthorized satellite dishes, etc. Nor is such a usage of culture recent \u2013 as part of Roman imperialism, local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants.\nImperialism has been subject to moral or immoral censure by its critics, and thus the term \"imperialism\" is frequently used in international propaganda as a pejorative for expansionist and aggressive foreign policy.\nReligious imperialism.\nAspects of imperialism motivated by religious supremacism can be described as religious imperialism. Religious imperialism involves the extension of religious authority modeled on colonial power structures. In 1932, scholar Clifford Manshardt defined religious imperialism as \"the attitude of mind which says that that which I believe should be believed by everyone else; and which is willing to undergo hardships and make sacrifices for the extension of that belief.\"\nPsychological imperialism.\nAn empire mentality may build on and bolster views contrasting \"primitive\" and \"advanced\" peoples and cultures, thus justifying and encouraging imperialist practices among participants.\nAssociated psychological tropes include the White Man's Burden and the idea of civilizing mission ().\nSocial imperialism.\nThe political concept social imperialism is a Marxist expression first used in the early 20th century by Lenin as \"socialist in words, imperialist in deeds\" describing the Fabian Society and other socialist organizations.\nLater, in a split with the Soviet Union, Mao Zedong criticized its leaders as social imperialists.\nSocial Darwinism.\nStephen Howe has summarized his view on the beneficial effects of the colonial empires:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;At least some of the great modern empires\u2014the British, French, Austro-Hungarian, Russian, and even the Ottoman\u2014have virtues that have been too readily forgotten. They provided stability, security, and legal order for their subjects. They constrained, and at their best, tried to transcend, the potentially savage ethnic or religious antagonisms among the peoples. And the aristocracies which ruled most of them were often far more liberal, humane, and cosmopolitan than their supposedly ever more democratic successors.\nA controversial aspect of imperialism is the defense and justification of empire-building based on seemingly rational grounds. In ancient China, Tianxia denoted the lands, space, and area divinely appointed to the Emperor by universal and well-defined principles of order. The center of this land was directly apportioned to the Imperial court, forming the center of a world view that centered on the Imperial court and went concentrically outward to major and minor officials and then the common citizens, tributary states, and finally ending with the fringe \"barbarians\". Tianxia's idea of hierarchy gave Chinese a privileged position and was justified through the promise of order and peace.\nThe purportedly scientific nature of \"Social Darwinism\" and a theory of races formed a supposedly rational justification for imperialism. Under this doctrine, the French politician Jules Ferry could declare in 1883 that \"Superior races have a right, because they have a duty. They have the duty to civilize the inferior races.\" J. A. Hobson identifies this justification on general grounds as: \"It is desirable that the earth should be peopled, governed, and developed, as far as possible, by the races which can do this work best, i.e. by the races of highest 'social efficiency'\". The Royal Geographical Society of London and other geographical societies in Europe had great influence and were able to fund travelers who would come back with tales of their discoveries. These societies also served as a space for travellers to share these stories. Political geographers such as Friedrich Ratzel of Germany and Halford Mackinder of Britain also supported imperialism. Ratzel believed expansion was necessary for a state's survival and this argument dominated the discipline of geopolitics for decades. British imperialism in some sparsely-inhabited regions applied a principle now termed Terra nullius (Latin expression which stems from Roman law meaning 'no man's land'). The British settlement in Australia in the 18th century was arguably premised on \"terra nullius\", as its settlers considered it unused by its original inhabitants. The rhetoric of colonizers being racially superior appears still to have its impact. For example, throughout Latin America \"whiteness\" is still prized today and various forms of blanqueamiento (whitening) are common.\nImperial peripheries benefited from economic efficiency improved through the building of roads, other infrastructure and introduction of new technologies. Herbert L\u00fcthy notes that ex-colonial peoples themselves show no desire to undo the basic effects of this process. Hence moral self-criticism in respect of the colonial past is out of place.\nEnvironmental determinism.\nThe concept of environmental determinism served as a moral justification for the domination of certain territories and peoples. The environmental determinist school of thought held that the environment in which certain people lived determined those persons' behaviours; and thus validated their domination. Some geographic scholars under colonizing empires divided the world into climatic zones. These scholars believed that Northern Europe and the Mid-Atlantic temperate climate produced a hard-working, moral, and upstanding human being. In contrast, tropical climates allegedly yielded lazy attitudes, sexual promiscuity, exotic culture, and moral degeneracy. The tropical peoples were believed to be \"less civilized\" and in need of European guidance, therefore justifying colonial control as a civilizing mission. For instance, American geographer Ellen Churchill Semple argued that even though human beings originated in the tropics they were only able to become fully human in the temperate zone. Across the three major waves of European colonialism (the first in the Americas, the second in Asia and the last in Africa), environmental determinism served to place categorically indigenous people in a racial hierarchy. Tropicality can be paralleled with Edward Said's Orientalism as the west's construction of the east as the \"other\". According to Said, orientalism allowed Europe to establish itself as the superior and the norm, which justified its dominance over the essentialized Orient. Orientalism is a view of a people based on their geographical location.\nFormer imperialism.\nBritish.\nEngland.\nEngland's imperialist ambitions can be seen as early as the 16th century as the Tudor conquest of Ireland began in the 1530s. In 1599 the British East India Company was established and was chartered by Queen Elizabeth in the following year. With the establishment of trading posts in India, the British were able to maintain strength relative to other empires such as the Portuguese who already had set up trading posts in India.\nScotland.\nBetween 1621 and 1699, the Kingdom of Scotland authorised several colonies in the Americas. Most of these colonies were either closed down or collapsed quickly for various reasons.\nUnited Kingdom.\nUnder the Acts of Union 1707, the English and Scottish kingdoms were merged, and their colonies collectively became subject to Great Britain (also known as the United Kingdom). The empire Great Britain would go on to found was the largest empire that the world has ever seen both in terms of landmass and population. Its power, both military and economic, remained unmatched for a few decades.\nIn 1767, the Anglo-Mysore Wars and other political activity caused exploitation of the East India Company causing the plundering of the local economy, almost bringing the company into bankruptcy. By the year 1670 Britain's imperialist ambitions were well off as she had colonies in Virginia, Massachusetts, Bermuda, Honduras, Antigua, Barbados, Jamaica and Nova Scotia.\nDue to the vast imperialist ambitions of European countries, Britain had several clashes with France. This competition was evident in the colonization of what is now known as Canada. John Cabot claimed Newfoundland for the British while the French established colonies along the St. Lawrence River and claiming it as \"New France\". Britain continued to expand by colonizing countries such as New Zealand and Australia, both of which were not empty land as they had their own locals and cultures. Britain's nationalistic movements were evident with the creation of the commonwealth countries where there was a shared nature of national identity.\nFollowing the proto-industrialization, the \"First\" British Empire was based on mercantilism, and involved colonies and holdings primarily in North America, the Caribbean, and India. Its growth was reversed by the loss of the American colonies in 1776. Britain made compensating gains in India, Australia, and in constructing an informal economic empire through control of trade and finance in Latin America after the independence of Spanish and Portuguese colonies in about 1820. By the 1840s, the United Kingdom had adopted a highly successful policy of free trade that gave it dominance in the trade of much of the world. After losing its first Empire to the Americans, Britain then turned its attention towards Asia, Africa, and the Pacific. Following the defeat of Napoleonic France in 1815, the United Kingdom enjoyed a century of almost unchallenged dominance and expanded its imperial holdings around the globe. Unchallenged at sea, British dominance was later described as \"Pax Britannica\" (\"British Peace\"), a period of relative peace in Europe and the world (1815\u20131914) during which the British Empire became the global hegemon and adopted the role of global policeman. However, this peace was mostly a perceived one from Europe, and the period was still an almost uninterrupted series of colonial wars and disputes. The British Conquest of India, its intervention against Mehemet Ali, the Anglo-Burmese Wars, the Crimean War, the Opium Wars and the Scramble for Africa to name the most notable conflicts mobilised ample military means to press Britain's lead in the global conquest Europe led across the century.\nIn the early 19th century, the Industrial Revolution began to transform Britain; by the time of the Great Exhibition in 1851 the country was described as the \"workshop of the world\". The British Empire expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During this century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions.\nA resurgence came in the late 19th century with the Scramble for Africa and major additions in Asia and the Middle East. The British spirit of imperialism was expressed by Joseph Chamberlain and Lord Rosebury, and implemented in Africa by Cecil Rhodes. The pseudo-sciences of Social Darwinism and theories of race formed an ideological underpinning and legitimation during this time. Other influential spokesmen included Lord Cromer, Lord Curzon, General Kitchener, Lord Milner, and the writer Rudyard Kipling. After the First Boer War, the South African Republic and Orange Free State were recognised by the United Kingdom but eventually re-annexed after the Second Boer War. But British power was fading, as the reunited German state founded by the Kingdom of Prussia posed a growing threat to Britain's dominance. As of 1913, the United Kingdom was the world's fourth economy, behind the U.S., Russia and Germany.\nIrish War of Independence in 1919\u20131921 led to the \u0441reation of the Irish Free State. But the United Kingdom gained control of former German and Ottoman colonies with the League of Nations mandate. The United Kingdom now had a practically continuous line of controlled territories from Egypt to Burma and another one from Cairo to Cape Town. However, this period was also one of emergence of independence movements based on nationalism and new experiences the colonists had gained in the war.\nWorld War II decisively weakened Britain's position in the world, especially financially. Decolonization movements arose nearly everywhere in the Empire, resulting in Indian independence and partition in 1947, the self-governing dominions break away from the empire in 1949, and the establishment of independent states in the 1950s. British imperialism showed its frailty in Egypt during the Suez Crisis in 1956. However, with the United States and Soviet Union emerging from World War II as the sole superpowers, Britain's role as a worldwide power declined significantly and rapidly.\nCaliphate.\nThe Early Muslim conquests and the pan-islamic Caliphate have been described as religious imperialism motivated by Islamic supremacism.\nCanada.\nIn Canada, the \"imperialism\" (and the related term \"colonialism\") has had a variety of contradictory meanings since the 19th century. In the late 19th and early 20th, to be an \"imperialist\" meant thinking of Canada as a part of the British nation not a separate nation. The older words for the same concepts were \"loyalism\" or \"unionism\", which continued to be used as well. In mid-twentieth century Canada, the words \"imperialism\" and \"colonialism\" were used in English Canadian discourse to instead portray Canada as a victim of economic and cultural penetration by the United States. In twentieth century French-Canadian discourse the \"imperialists\" were all the Anglo-Saxon countries including Canada who were oppressing French-speakers and the province of Quebec. By the early 21st century, \"colonialism\" was used to highlight supposed anti-indigenous attitudes and actions of Canada inherited from the British period.\nChina.\nChina was home to some of the world's oldest empires. Due to its long history of imperialist expansion, China has been seen by its neighboring countries as a threat due to its large population, giant economy, large military force as well as its territorial evolution throughout history. Starting with the unification of China under the Qin dynasty, later Chinese dynasties continued to follow its form of expansions.\nThe most successful Chinese imperial dynasties in terms of territorial expansion were the Han, Tang, Yuan, and Qing dynasties.\nDenmark.\nDenmark\u2013Norway (Denmark after 1814) possessed overseas colonies from 1536 until 1953. At its apex there were colonies on four continents: Europe, North America, Africa and Asia. In the 17th century, following territorial losses on the Scandinavian Peninsula, Denmark-Norway began to develop colonies, forts, and trading posts in West Africa, the Caribbean, and the Indian subcontinent. Christian IV first initiated the policy of expanding Denmark-Norway's overseas trade, as part of the mercantilist wave that was sweeping Europe. Denmark-Norway's first colony was established at Tranquebar on India's southern coast in 1620. Admiral Ove Gjedde led the expedition that established the colony. After 1814, when Norway was ceded to Sweden, Denmark retained what remained of Norway's great medieval colonial holdings. One by one the smaller colonies were lost or sold. Tranquebar was sold to the British in 1845. The United States purchased the Danish West Indies in 1917. Iceland became independent in 1944. Today, the only remaining vestiges are two originally Norwegian colonies that are currently within the Danish Realm, the Faroe Islands and Greenland; the Faroes were a Danish county until 1948, while Greenland's colonial status ceased in 1953. They are now autonomous territories.\nDutch.\nThe most notable example of Dutch imperialism is regarding Indonesia.\nFrance.\nDuring the 16th century, the French colonization of the Americas began with the creation of New France. It was followed by French East India Company's trading posts in Africa and Asia in the 17th century. France had its \"First colonial empire\" from 1534 until 1814, including New France (Canada, Acadia, Newfoundland and Louisiana), French West Indies (Saint-Domingue, Guadeloupe, Martinique), French Guiana, Senegal (Gor\u00e9e), Mascarene Islands (Mauritius Island, R\u00e9union) and French India.\nIts \"Second colonial empire\" began with the seizure of Algiers in 1830 and came for the most part to an end with the granting of independence to Algeria in 1962. The French imperial history was marked by numerous wars, large and small, and also by significant help to France itself from the colonials in the world wars. France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa (French North Africa, French West Africa, French Equatorial Africa), as well as South-East Asia (French Indochina), with other conquests in the South Pacific (New Caledonia, French Polynesia). France also twice attempted to make Mexico a colony in 1838\u201339 and in 1861\u201367 (see Pastry War and Second French intervention in Mexico).\nFrench Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars. It became a moral justification to lift the world up to French standards by bringing Christianity and French culture. In 1884 the leading exponent of colonialism, Jules Ferry declared France had a civilising mission: \"The higher races have a right over the lower races, they have a duty to civilize the inferior\". Full citizenship rights \u2013 \"assimilation\" \u2013 were offered, although in reality assimilation was always on the distant horizon. Contrasting from Britain, France sent small numbers of settlers to its colonies, with the only notable exception of Algeria, where French settlers nevertheless always remained a small minority.\nThe French colonial empire of extended over at its height in the 1920s and had a population of 110 million people on the eve of World War II.\nIn World War II, Charles de Gaulle and the Free French used the overseas colonies as bases from which they fought to liberate France. However, after 1945 anti-colonial movements began to challenge the Empire. France fought and lost a bitter war in Vietnam in the 1950s. Whereas they won the war in Algeria, de Gaulle decided to grant Algeria independence anyway in 1962. French settlers and many local supporters relocated to France. Nearly all of France's colonies gained independence by 1960, but France retained significant financial and diplomatic influence. It has repeatedly sent troops to assist its former colonies in Africa in suppressing insurrections and coups d'\u00e9tat.\nEducation policy.\nFrench colonial officials, influenced by the revolutionary ideal of equality, standardized schools, curricula, and teaching methods as much as possible. They did not establish colonial school systems with the idea of furthering the ambitions of the local people, but rather simply exported the systems and methods in vogue in the mother nation. Having a moderately trained lower bureaucracy was of great use to colonial officials. The emerging French-educated indigenous elite saw little value in educating rural peoples. After 1946 the policy was to bring the best students to Paris for advanced training. The result was to immerse the next generation of leaders in the growing anti-colonial diaspora centered in Paris. Impressionistic colonials could mingle with studious scholars or radical revolutionaries or so everything in between. Ho Chi Minh and other young radicals in Paris formed the French Communist party in 1920.\nTunisia was exceptional. The colony was administered by Paul Cambon, who built an educational system for colonists and indigenous people alike that was closely modeled on mainland France. He emphasized female and vocational education. By independence, the quality of Tunisian education nearly equalled that in France.\nAfrican nationalists rejected such a public education system, which they perceived as an attempt to retard African development and maintain colonial superiority. One of the first demands of the emerging nationalist movement after World War II was the introduction of full metropolitan-style education in French West Africa with its promise of equality with Europeans.\nIn Algeria, the debate was polarized. The French set up schools based on the scientific method and French culture. The Pied-Noir (Catholic migrants from Europe) welcomed this. Those goals were rejected by the Moslem Arabs, who prized mental agility and their distinctive religious tradition. The Arabs refused to become patriotic and cultured Frenchmen and a unified educational system was impossible until the Pied-Noir and their Arab allies went into exile after 1962.\nIn South Vietnam from 1955 to 1975 there were two competing powers in education, as the French continued their work and the Americans moved in. They sharply disagreed on goals. The French educators sought to preserving French culture among the Vietnamese elites and relied on the Mission Culturelle \u2013 the heir of the colonial Direction of Education \u2013 and its prestigious high schools. The Americans looked at the great mass of people and sought to make South Vietnam a nation strong enough to stop communism. The Americans had far more money, as USAID coordinated and funded the activities of expert teams, and particularly of academic missions. The French deeply resented the American invasion of their historical zone of cultural imperialism.\nGermany.\nGerman expansion into Slavic lands begins in the 12th\u201313th-century (see Drang Nach Osten). The concept of Drang Nach Osten was a core element of German nationalism and a major element of Nazi ideology. However, the German involvement in the seizure of overseas territories was negligible until the end of the 19th century. Prussia unified the other states into the second German Empire in 1871. Its Chancellor, Otto von Bismarck (1862\u201390), long opposed colonial acquisitions, arguing that the burden of obtaining, maintaining, and defending such possessions would outweigh any potential benefits. He felt that colonies did not pay for themselves, that the German bureaucratic system would not work well in the tropics and the diplomatic disputes over colonies would distract Germany from its central interest, Europe itself.\nHowever, public opinion and elite opinion in Germany demanded colonies for reasons of international prestige, so Bismarck was forced to oblige. In 1883\u201384 Germany began to build a colonial empire in Africa and the South Pacific. The establishment of the German colonial empire started with German New Guinea in 1884. Within 25 years, German South West Africa had committed the Herero and Namaqua genocide in modern-day Namibia, the first genocide of the 20th century.\nGerman colonies included the present territories of in Africa: Tanzania, Rwanda, Burundi, Namibia, Cameroon, Ghana and Togo; in Oceania: New Guinea, Solomon Islands, Nauru, Marshall Islands, Mariana Islands, Caroline Islands and Samoa; and in Asia: Qingdao, Yantai and the Jiaozhou Bay. The Treaty of Versailles made them mandates under the control the Allied victors.\nGermany also lost the portions of its Eastern territories that had Polish majorities to independent Poland as a result of the Treaty of Versailles in 1919. The Eastern territories inhabited by a German majority since the Middle Ages were torn from Germany and became part of both Poland and the USSR as a result of the territorial reorganization established by the Potsdam Conference of the Allied powers in 1945.\nItaly.\nThe Italian Empire (\"Impero italiano\") comprised the overseas possessions of the Kingdom of Italy primarily in northeast Africa. It began with the purchase in 1869 of Assab Bay on the Red Sea by an Italian navigation company which intended to establish a coaling station at the time the Suez Canal was being opened to navigation. This was taken over by the Italian government in 1882, becoming modern Italy's first overseas territory. By the start of the First World War in 1914, Italy had acquired in Africa the colony of Eritrea on the Red Sea coast, a large protectorate and later colony in Somalia, and authority in formerly Ottoman Tripolitania and Cyrenaica (gained after the Italo-Turkish War) which were later unified in the colony of Libya.\nOutside Africa, Italy possessed the Dodecanese Islands off the coast of Turkey (following the Italo-Turkish War) and a small concession in Tianjin in China following the Boxer War of 1900. During the First World War, Italy occupied southern Albania to prevent it from falling to Austria-Hungary. In 1917, it established a protectorate over Albania, which remained in place until 1920. The Fascist government that came to power with Benito Mussolini in 1922 sought to increase the size of the Italian empire and to satisfy the claims of Italian irredentists.\nAfter its second invasion of Ethiopia in 1936, Italy occupied Ethiopia until 1941. In 1939, Italy invaded Albania and incorporated it into the Fascist state. During the Second World War (1939\u20131945), Italy occupied British Somaliland, parts of south-eastern France, western Egypt and most of Greece, but then lost those conquests and its African colonies, including Ethiopia, to the invading allied forces by 1943. It was forced in the peace treaty of 1947 to relinquish sovereignty over all its colonies. It was granted a trust to administer former Italian Somaliland under United Nations supervision in 1950. When Somalia became independent in 1960, Italy's eight-decade experiment with colonialism ended.\nJapan.\nFor over 200 years, Japan maintained a feudal society during a period of relative isolation from the rest of the world. However, in the 1850s, military pressure from the United States and other world powers coerced Japan to open itself to the global market, resulting in an end to the country's isolation. A period of conflicts and political revolutions followed due to socioeconomic uncertainty, ending in 1868 with the reunification of political power under the Japanese Emperor during the Meiji Restoration. This sparked a period of rapid industrialization driven in part by a Japanese desire for self-sufficiency. By the early 1900s, Japan was a naval power that could hold its own against an established European power as it defeated Russia.\nDespite its rising population and increasingly industrialized economy, Japan lacked significant natural resources. As a result, the country turned to imperialism and expansionism in part as a means of compensating for these shortcomings, adopting the national motto \"Fukoku ky\u014dhei\" (, \"Enrich the state, strengthen the military\").\nAnd Japan was eager to take every opportunity. In 1869 they took advantage of the defeat of the rebels of the Republic of Ezo to formally incorporate the island of Hokkaido into the Japanese Empire. For centuries, Japan viewed the Ryukyu Islands as one of its provinces. In 1871 the Mudan incident happened: Taiwanese aborigines murdered 54 Ry\u016bky\u016ban sailors that became shipwrecked. At that time the Ryukyu Islands were claimed by both Qing China and Japan, and the Japanese interpreted the incident as an attack on their citizens. They took steps to bring the islands in their jurisdiction: in 1872 the Japanese Ryukyu Domain was declared, and in 1874 a retaliatory incursion to Taiwan was sent, which was a success. The success of this expedition emboldened the Japanese: not even the Americans could defeat the Taiwanese in the Formosa Expedition of 1867. Very few gave it much thought at the time, but this was the first move in the Japanese expansionism series. Japan occupied Taiwan for the rest of 1874 and then left owing to Chinese pressures, but in 1879 it finally annexed the Ryukyu Islands. In 1875 Qing China sent a 300-men force to subdue the Taiwanese, but unlike the Japanese the Chinese were routed, ambushed and 250 of their men were killed; the failure of this expedition exposed once more the failure of Qing China to exert effective control in Taiwan, and acted as another incentive for the Japanese to annex Taiwan. Eventually, the spoils for winning the First Sino-Japanese War in 1894 included Taiwan.\nIn 1875 Japan took its first operation against Joseon Korea, another territory that for centuries it coveted; the Ganghwa Island incident made Korea open to international trade. Korea was annexed in 1910. As a result of winning the Russo-Japanese War in 1905, Japan took part of Sakhalin Island from Russia. Precisely, the victory against the Russian Empire shook the world: never before had an Asian nation defeated a European power, and in Japan it was seen as a feat. Japan's victory against Russia would act as an antecedent for Asian countries in the fight against the Western powers for Decolonization. During World War I, Japan took German-leased territories in China's Shandong Province, as well as the Mariana, Caroline, and Marshall Islands, and kept the islands as League of nations mandates. At first, Japan was in good standing with the victorious Allied powers of World War I, but different discrepancies and dissatisfaction with the rewards of the treaties cooled the relations with them, for example American pressure forced it to return the Shandong area. By the '30s, economic depression, urgency of resources and a growing distrust in the Allied powers made Japan lean to a hardened militaristic stance. Through the decade, it would grow closer to Germany and Italy, forming together the Axis alliance. In 1931 Japan took Manchuria from China. International reactions condemned this move, but Japan's already strong skepticism against Allied nations meant that it nevertheless carried on.\nDuring the Second Sino-Japanese War in 1937, Japan's military invaded central China. Also, in 1938\u20131939 Japan made an attempt to seize the territory of Soviet Russia and Mongolia, but suffered a serious defeats (see Battle of Lake Khasan, Battles of Khalkhin Gol). By now, relations with the Allied powers were at the bottom, and an international boycott against Japan to deprive it of natural resources was enforced. A military move to gain access to them was deemed necessary, and so Japan attacked Pearl Harbor, bringing the United States to World War II. Using its superior technological advances in naval aviation and its modern doctrines of amphibious and naval warfare, Japan achieved one of the fastest maritime expansions in history. By 1942 Japan had conquered much of East Asia and the Pacific, including the east of China, Hong Kong, Thailand, Vietnam, Cambodia, Burma (Myanmar), Malaysia, the Philippines, Indonesia, part of New Guinea and many islands of the Pacific Ocean. Just as Japan's late industrialization success and victory against the Russian Empire was seen as an example among underdeveloped Asia-Pacific nations, the Japanese took advantage of this and promoted among its conquered the goal to jointly create an anti-European \"Greater East Asia Co-Prosperity Sphere\". This plan helped the Japanese gain support from native populations during its conquests especially in Indonesia. However, the United States had a vastly stronger military and industrial base and defeated Japan, stripping it of conquests and returning its settlers back to Japan.\nOttoman.\nThe Ottoman Empire was an imperial state that lasted from 1299 to 1922. In 1453, Mehmed the Conqueror captured Constantinople and made it his capital. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire, which invaded and colonized much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. Its repeated invasions, and brutal treatment of Slavs led to the Great Migrations of the Serbs to escape persecution. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\nFollowing a long period of military setbacks against European powers, the Ottoman Empire gradually declined, losing control of much of its territory in Europe and Africa.\nBy 1810 Egypt was effectively independent. In 1821\u20131829 the Greeks in the Greek War of Independence were assisted by Russia, Britain and France. In 1815 to 1914 the Ottoman Empire could exist only in the conditions of acute rivalry of the great powers, with Britain its main supporter, especially in the Crimean War 1853\u20131856, against Russia. After Ottoman defeat in the Russo-Turkish War (1877\u20131878), Bulgaria, Serbia and Montenegro gained independence and Britain took colonial control of Cyprus, while Bosnia and Herzegovina were occupied and annexed by Austro-Hungarian Empire in 1908.\nThe empire allied with Germany in World War I, aiming to recover lost territories, but dissolved following its decisive defeat. The Turkish national movement, supported by Soviet Russia, achieved victory in the course of the Turkish War of Independence, and the parties signed and ratified the Treaty of Lausanne in 1923 and 1924. The Republic of Turkey was established.\nRome.\nThe Roman Empire was the post-Republican period of ancient Rome. As a polity, it included large territorial holdings around the Mediterranean Sea in Europe, North Africa, and Western Asia, ruled by emperors.\nRussia.\nRussian Empire.\nBy the 18th century, the Russian Empire extended its control to the Pacific, peacefully forming a common border with the Qing Empire and Empire of Japan. This took place in a large number of military invasions of the lands east, west, and south of it. The Polish\u2013Russian War of 1792 took place after Polish nobility from the Polish\u2013Lithuanian Commonwealth wrote the Constitution of 3 May 1791. The war resulted in eastern Poland being conquered by Imperial Russia as a colony until 1918. The southern campaigns involved a series of Russo-Persian Wars, which began with the Persian Expedition of 1796, resulting in the acquisition of Georgia as a protectorate. Between 1800 and 1864, Imperial armies invaded south in the Russian conquest of the Caucasus, the Murid War, and the Russo-Circassian War. This last conflict led to the ethnic cleansing of Circassians from their lands. The Russian conquest of Siberia over the Khanate of Sibir took place in the 16th and 17th centuries, and resulted in the slaughter of various indigenous tribes by Russians, including the Daur, the Koryaks, the Itelmens, Mansi people and the Chukchi. The Russian colonization of Central and Eastern Europe and Siberia and treatment of the resident indigenous peoples has been compared to European colonization of the Americas, with similar negative impacts on the indigenous Siberians as upon the indigenous peoples of the Americas. The extermination of indigenous Siberian tribes was so complete that a relatively small population of only 180,000 are said to exist today. The Russian Empire exploited and suppressed Cossacks hosts during this period, before turning them into the special military estate Sosloviye in the late 18th century. Cossacks were then used in Imperial Russian campaigns against other tribes.\nThe acquisition of Ukraine by Russia commenced in 1654 with the Pereiaslav Agreement. Georgia's accession to Russia in 1783 was marked by the Treaty of Georgievsk.\nSoviet Union.\nBolshevik leaders had effectively reestablished a polity with roughly the same extent as that empire by 1921, however with an internationalist ideology: Lenin in particular asserted the right to limited self-determination for national minorities within the new territory. Beginning in 1923, the policy of \"Indigenization\" [korenizatsiya] was intended to support non-Russians develop their national cultures within a socialist framework. Never formally revoked, it stopped being implemented after 1932. After World War II, the Soviet Union installed socialist regimes modeled on those it had installed in 1919\u201320 in the old Russian Empire, in areas its forces occupied in Eastern Europe. The Soviet Union and later the People's Republic of China supported revolutionary and communist movements in foreign nations and colonies to advance their own interests, but were not always successful. The USSR provided great assistance to Kuomintang in 1926\u20131928 in the formation of a unified Chinese government (see Northern Expedition). Although then relations with the USSR deteriorated, but the USSR was the only world power that provided military assistance to China against Japanese aggression in 1937\u20131941 (see Sino-Soviet Non-Aggression Pact). The victory of the Chinese Communists in the civil war of 1946\u20131949 relied on the great help of the USSR (see Chinese Civil War).\nAlthough the Soviet Union declared itself anti-imperialist, critics argue that it exhibited traits common to historic empires. Some scholars hold that the Soviet Union was a hybrid entity containing elements common to both multinational empires and nation-states. Some also argued that the USSR practiced colonialism as did other imperial powers and was carrying on the old Russian tradition of expansion and control. Mao Zedong once argued that the Soviet Union had itself become an imperialist power while maintaining a socialist fa\u00e7ade. Moreover, the ideas of imperialism were widely spread in action on the higher levels of government. Josip Broz Tito and Milovan Djilas have referred to the Stalinist USSR's foreign policies, such as the occupation and economic exploitations of Eastern Europe and its aggressive and hostile policy towards Yugoslavia as Soviet imperialism. Some Marxists within the Russian Empire and later the USSR, like Sultan Galiev and Vasyl Shakhrai, considered the Soviet regime a renewed version of the Russian imperialism and colonialism. The crushing of the Hungarian Revolution of 1956 and Soviet\u2013Afghan War have been cited as examples.\nSpain.\nSpanish imperialism in the colonial era corresponds with the rise and decline of the Spanish Empire, conventionally recognized as emerging in 1402 with the conquest of the Canary Islands. Following the successes of exploratory maritime voyages conducted during the Age of Discovery, Spain committed considerable financial and military resources towards developing a robust navy capable of conducting large-scale, transatlantic expeditionary operations in order to establish and solidify a firm imperial presence across large portions of North America, South America, and the geographic regions comprising the Caribbean basin. Concomitant with Spanish endorsement and sponsorship of transatlantic expeditionary voyages was the deployment of \"Conquistadors\", which further expanded Spanish imperial boundaries through the acquisition and development of territories and colonies.\nImperialism in the Caribbean basin.\nIn congruence with the colonialist activities of competing European imperial powers throughout the 15th \u2013 19th centuries, the Spanish were equally engrossed in extending geopolitical power. The Caribbean basin functioned as a key geographic focal point for advancing Spanish imperialism. Similar to the strategic prioritization Spain placed towards achieving victory in the conquests of the Aztec Empire and Inca Empire, Spain placed equal strategic emphasis on expanding the nation's imperial footprint within the Caribbean basin.\nEchoing the prevailing ideological perspectives regarding colonialism and imperialism embraced by Spain's European rivals during the colonial era, including the English, French, and the Dutch, the Spanish used colonialism as a means of expanding imperial geopolitical borders and securing the defense of maritime trade routes in the Caribbean basin.\nWhile leveraging colonialism in the same geographic operating theater as its imperial rivals, Spain maintained distinct imperial objectives and instituted a unique form of colonialism in support of its imperial agenda. Spain placed significant strategic emphasis on the acquisition, extraction, and exportation of precious metals (primarily gold and silver). A second objective was the evangelization of subjugated indigenous populations residing in mineral-rich and strategically favorable locations. Notable examples of these indigenous groups include the Ta\u03afno populations inhabiting Puerto Rico and segments of Cuba. Compulsory labor and slavery were widely institutionalized across Spanish-occupied territories and colonies, with an initial emphasis on directing labor towards mining activity and related methods of procuring semi-precious metals. The emergence of the \"Encomienda\" system during the 16th\u201317th centuries in occupied colonies within the Caribbean basin reflects a gradual shift in imperial prioritization, increasingly focusing on large-scale production and exportation of agricultural commodities.\nScholarly debate and controversy.\nThe scope and scale of Spanish participation in imperialism within the Caribbean basin remains a subject of scholarly debate among historians. A fundamental source of contention stems from the inadvertent conflation of theoretical conceptions of imperialism and colonialism. Furthermore, significant variation exists in the definition and interpretation of these terms as expounded by historians, anthropologists, philosophers, and political scientists.\nAmong historians, there is substantial support in favor of approaching imperialism as a conceptual theory emerging during the 18th\u201319th centuries, particularly within Britain, propagated by key exponents such as Joseph Chamberlain and Benjamin Disraeli. In accordance with this theoretical perspective, the activities of the Spanish in the Caribbean are not components of a preeminent, ideologically driven form of imperialism. Rather, these activities are more accurately classified as representing a form of colonialism.\nFurther divergence among historians can be attributed to varying theoretical perspectives regarding imperialism that are proposed by emerging academic schools of thought. Noteworthy examples include cultural imperialism, whereby proponents such as John Downing and Annabelle Sreberny-Modammadi define imperialism as \"...the conquest and control of one country by a more powerful one.\" Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force.\" Moreover, colonialism is understood as \"...the form of imperialism in which the government of the colony is run directly by foreigners.\"\nIn spite of diverging perspectives and the absence of a unilateral scholarly consensus regarding imperialism among historians, within the context of Spanish expansion in the Caribbean basin during the colonial era, imperialism can be interpreted as an overarching ideological agenda that is perpetuated through the institution of colonialism. In this context, colonialism functions as an instrument designed to achieve specific imperialist objectives.\nUnited States.\nMade up of former colonies itself, the early United States expressed its opposition to imperialism, at least in a form distinct from its own Manifest Destiny, through policies such as the Monroe Doctrine. However the US may have unsuccessfully attempted to capture Canada in the War of 1812. The United States achieved very significant territorial concessions from Mexico during the Mexican\u2013American War. Beginning in the late 19th and early 20th century, policies such as Theodore Roosevelt's interventionism in Central America and Woodrow Wilson's mission to \"make the world safe for democracy\" changed all this. They were often backed by military force, but were more often affected from behind the scenes. This is consistent with the general notion of hegemony and imperium of historical empires. In 1898, Americans who opposed imperialism created the Anti-Imperialist League to oppose the US annexation of the Philippines and Cuba. One year later, a war erupted in the Philippines causing business, labor and government leaders in the US to condemn America's occupation in the Philippines as they also denounced them for causing the deaths of many Filipinos. American foreign policy was denounced as a \"racket\" by Smedley Butler, a former American general who had become a spokesman for the far left.\nAt the start of World War II, President Franklin D. Roosevelt was opposed to European colonialism, especially in India. He pulled back when Britain's Winston Churchill demanded that victory in the war be the first priority. Roosevelt expected that the United Nations would take up the problem of decolonization.\nSome have described the internal strife between various people groups as a form of imperialism or colonialism. This internal form is distinct from informal U.S. imperialism in the form of political and financial hegemony. It also showed difference in the United States' formation of \"colonies\" abroad. Through the treatment of its indigenous peoples during westward expansion, the United States took on the form of an imperial power prior to any attempts at external imperialism. This internal form of empire has been referred to as \"internal colonialism\". Participation in the African slave trade and the subsequent treatment of its 12 to 15\u00a0million Africans is viewed by some to be a more modern extension of America's \"internal colonialism\". However, this internal colonialism faced resistance, as external colonialism did, but the anti-colonial presence was far less prominent due to the nearly complete dominance that the United States was able to assert over both indigenous peoples and African-Americans. In a lecture on April 16, 2003, Edward Said described modern imperialism in the United States as an aggressive means of attack towards the contemporary Orient stating that \"due to their backward living, lack of democracy and the violation of women's rights. The western world forgets during this process of converting the other that enlightenment and democracy are concepts that not all will agree upon\".\nContemporary.\nRussia under Putin.\nSince the 2010s, Russia under Vladimir Putin has been described as neo-imperialist. Russia occupies parts of neighboring countries and has engaged in expansionism, most notably with the 2008 Russian invasion of Georgia, the 2014 annexation of Crimea, and the 2022 invasion of Ukraine and annexation of its southeast. Russia has also established domination over Belarus. Four months into the invasion of Ukraine, Putin compared himself to Russian emperor Peter the Great. He said that Tsar Peter had returned \"Russian land\" to the empire, and that \"it is now also our responsibility to return (Russian) land\". Kseniya Oksamytna wrote that in Russia media, the invasion was accompanied by discourses of Russian \"supremacy\". She says that this likely fuelled war crimes against Ukrainians and that \"the behavior of Russian forces bore all hallmarks of imperial violence\". The Putin regime has revived imperial ideas such as the \"Russian world\" and the ideology of Eurasianism. It has used disinformation and the Russian diaspora to undermine the sovereignty of other countries. Russia is also accused of neo-colonialism in Africa, mainly through the activities of the Wagner Group and Africa Corps.\nAnti-imperialism.\nAnti-imperialism gained a wide currency after the Second World War and at the onset of the Cold War as political movements in colonies of European powers promoted national sovereignty. Some anti-imperialist groups who opposed the United States supported the power of the Soviet Union, such as in Guevarism, while in Maoism this was criticized as social imperialism.\nPan-Africanism is a movement across Africa and the world that came as a result of imperial ideas splitting apart African nations and pitting them against each other. The Pan-African movement instead tried to reverse those ideas by uniting Africans and creating a sense of brotherhood among all African people. The Pan-African movement helped with the eventual end of Colonialism in Africa.\nRepresentatives at the 1900 Pan African Conference demanded moderate reforms for colonial African nations. The conference also discussed African populations in the Caribbean and the United States and their rights. A total of six Pan-African conferences that were held, and these allowed the African people to have a voice in ending colonial rule.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nPrimary sources"}
{"id": "15317", "revid": "39635320", "url": "https://en.wikipedia.org/wiki?curid=15317", "title": "IPv4", "text": "Fourth version of the Internet Protocol\nInternet Protocol version 4 (IPv4) is the first version of the Internet Protocol (IP) as a standalone specification. It is one of the core protocols of standards-based internetworking methods in the Internet and other packet-switched networks. IPv4 was the first version deployed for production on SATNET in 1982 and on the ARPANET in January 1983. It is still used to route most Internet traffic today, even with the ongoing deployment of Internet Protocol version 6 (IPv6), its successor.\nIPv4 uses a 32-bit address space which provides 4,294,967,296 (232) unique addresses, but large blocks are reserved for special networking purposes. This quantity of unique addresses is not large enough to meet the needs of the global Internet, which has caused a significant issue known as IPv4 address exhaustion during the ongoing transition to IPv6.\nPurpose.\nThe Internet Protocol (\"IP\") is the protocol that defines and enables internetworking at the internet layer of the Internet Protocol Suite. It gives the Internet a global-scale logical addressing system which allows the routing of IP data packets from a source host to the next router that is one hop closer to the intended destination host on another network.\nIPv4 is a connectionless protocol, and operates on a best-effort delivery model, in that it does not guarantee delivery, nor does it assure proper sequencing or avoidance of duplicate delivery. These aspects may be addressed by upper layer transport protocols, such as the Transmission Control Protocol (TCP) or the QUIC protocol.\nHistory.\nEarlier versions of TCP/IP were a combined specification through TCP/IPv3. With IPv4, the Internet Protocol became a separate specification.\nInternet Protocol version 4 is described in IETF publication RFC 791 (September 1981), replacing an earlier definition of January 1980 (RFC 760). In March 1982, the US Department of Defense decided on the Internet Protocol Suite (TCP/IP) as the standard for all military computer networking.\nAddress space exhaustion.\nIn the late 1980s, it became apparent that the pool of available IPv4 addresses was depleting at a rate that was not initially anticipated in the original design of the network. The main market forces that accelerated address depletion beginning in the 1990s included the rapidly growing number of Internet users, who increasingly used mobile computing devices, such as laptop computers, and personal digital assistants (PDAs), and smart phones with IP data services. In addition, high-speed Internet access was based on always-on devices. The threat of exhaustion motivated the introduction of a number of remedial technologies, such as:\nBy the mid-1990s, NAT was used pervasively in network access provider systems, along with strict usage-based allocation policies at the regional and local Internet registries.\nThe primary address pool of the Internet, maintained by IANA, was exhausted on 3 February 2011, when the last five blocks were allocated to the five RIRs. APNIC was the first RIR to exhaust its regional pool on 15 April 2011, except for a small amount of address space reserved for the transition technologies to IPv6, which is to be allocated under a restricted policy.\nThe long-term solution to address exhaustion was the 1998 specification of a new version of the Internet Protocol, IPv6. It provides a vastly increased address space, but also allows improved route aggregation across the Internet, and offers large subnetwork allocations of a minimum of 264 host addresses to end users. However, IPv4 is not directly interoperable with IPv6, so that IPv4-only hosts cannot directly communicate with IPv6-only hosts. With the phase-out of the 6bone experimental network starting in 2004, permanent formal deployment of IPv6 commenced in 2006. Completion of IPv6 deployment is expected to take considerable time, so that intermediate transition technologies are necessary to permit hosts to participate in the Internet using both versions of the protocol.\nAddressing.\nIPv4 uses 32-bit addresses which limits the address space to (232) addresses.\nIPv4 reserves special address blocks for private networks (224\u00a0+\u00a0220\u00a0+\u00a0216\u00a0\u2248\u00a018 million addresses) and multicast addresses (228\u00a0\u2248\u00a0268 million addresses).\nAddress representations.\nIPv4 addresses may be represented in any notation expressing a 32-bit integer value. They are most often written in dot-decimal notation, which consists of four octets of the address expressed individually in decimal numbers (without any extra leading zeros) and separated by periods.\nFor example, the quad-dotted IP address in the illustration () represents the 32-bit decimal number 2886794753, which in hexadecimal format is 0xAC10FE01.\nCIDR notation combines the address with its routing prefix in a compact format, in which the address is followed by a slash character (/) and the count of leading consecutive \"1\" bits in the routing prefix (subnet mask).\nOther address representations were in common use when classful networking was practiced. For example, the loopback address was commonly written as , given that it belongs to a class-A network with eight bits for the network mask and 24 bits for the host number. When fewer than four numbers were specified in the address in dotted notation, the last value was treated as an integer of as many bytes as are required to fill out the address to four octets. Thus, the address is equivalent to .\nAllocation.\nIn the original design of IPv4, an IP address was divided into two parts: the network identifier was the most significant octet of the address, and the host identifier was the rest of the address. The latter was also called the \"rest field\". This structure permitted a maximum of 256 network identifiers, which was quickly found to be inadequate.\nTo overcome this limit, the most-significant address octet was redefined in 1981 to create \"network classes\", in a system which later became known as \"classful\" networking. The revised system defined five classes. Classes A, B, and C had different bit lengths for network identification. The rest of the address was used as previously to identify a host within a network. Because of the different sizes of fields in different classes, each network class had a different capacity for addressing hosts. In addition to the three classes for addressing hosts, Class D was defined for multicast addressing and Class E was reserved for future applications.\nDividing existing classful networks into subnets began in 1985 with the publication of . This division was made more flexible with the introduction of variable-length subnet masks (VLSM) in in 1987. In 1993, based on this work, introduced Classless Inter-Domain Routing (CIDR), which expressed the number of bits (from the most significant) as, for instance, , and the class-based scheme was dubbed \"classful\", by contrast. CIDR was designed to permit repartitioning of any address space so that smaller or larger blocks of addresses could be allocated to users. The hierarchical structure created by CIDR is managed by the Internet Assigned Numbers Authority (IANA) and the regional Internet registries (RIRs). Each RIR maintains a publicly searchable WHOIS database that provides information about IP address assignments.\nSpecial-use addresses.\nThe Internet Engineering Task Force (IETF) and IANA have restricted from general use various reserved IP addresses for special purposes. Notably these addresses are used for multicast traffic and to provide addressing space for unrestricted uses on private networks.\n&lt;section begin=IPv4-special-address-blocks/&gt;\n&lt;section end=IPv4-special-address-blocks/&gt;\nPrivate networks.\nOf the approximately four billion addresses defined in IPv4, about 18 million addresses from three ranges are reserved for use in private networks as outlined by . Packets addresses in these ranges are not routable in the public Internet; they are ignored by all public routers. Therefore, private hosts cannot directly communicate with public networks, but require network address translation at a routing gateway for this purpose.\n&lt;section begin=IPv4-private-networks/&gt;\n&lt;section end=IPv4-private-networks/&gt;\nSince two private networks, e.g., two branch offices, cannot directly interoperate via the public Internet, the two networks must be bridged across the Internet via a virtual private network (VPN) or an IP tunnel, which encapsulates packets, including their headers containing the private addresses, in a protocol layer during transmission across the public network. Additionally, encapsulated packets may be encrypted for transmission across public networks to secure the data.\nLink-local addressing.\nRFC 3927 defines the special address block 169.254.0.0/16 for link-local addressing. These addresses are only valid on the link (such as a local network segment or point-to-point connection) directly connected to a host that uses them. These addresses are not routable. Like private addresses, these addresses cannot be the source or destination of packets traversing the internet. These addresses are primarily used for address autoconfiguration (Zeroconf) when a host cannot obtain an IP address from a DHCP server or other internal configuration methods.\nWhen the address block was reserved, no standards existed for address autoconfiguration. Microsoft created an implementation called Automatic Private IP Addressing (APIPA), which was deployed on millions of machines and became a de facto standard. Many years later, in May 2005, the IETF defined a formal standard in RFC 3927, entitled \"Dynamic Configuration of IPv4 Link-Local Addresses\".\nLoopback.\nThe class A network (classless network ) is reserved for loopback. IP packets whose source addresses belong to this network should never appear outside a host. Packets received on a non-loopback interface with a loopback source or destination address must be dropped.\nFirst and last subnet addresses.\nIn every subnet, both the all-zeros and all-ones host addresses are reserved. The all-zeros host address is used to identify a given subnet. The highest address of every subnet, with all host bits set to \"1\", is the local broadcast address for sending messages to all devices on the subnet simultaneously. For networks of size or larger, the broadcast address in dot-decimal notation always ends in .\nFor example, in the subnet (subnet mask ) the identifier is used to refer to the entire subnet. The broadcast address of the network is .\nHowever, this does not mean that every address ending in 0 or 255 cannot be used as a host address. For example, in the subnet , which is equivalent to the address range \u2013, the broadcast address is . One can use the following addresses for hosts, even though they end with 255: , , etc. Also, is the network identifier and must not be assigned to an interface. The addresses , , etc., may be assigned, despite ending with 0.\nIn the past, conflict between network addresses and broadcast addresses arose because some software used non-standard broadcast addresses with zeros instead of ones.\nIn networks smaller than , broadcast addresses do not necessarily end with 255. For example, a CIDR subnet has the broadcast address .\nAs a special case, a network has capacity for just two hosts. These networks are typically used for point-to-point connections. There is no network identifier or broadcast address for these networks.\nAddress resolution.\nHosts on the Internet are usually known by names, e.g., www.example.com, not primarily by their IP address, which is used for routing and network interface identification. The use of domain names requires translating, called \"resolving\", them to addresses and vice versa. This is analogous to looking up a phone number in a phone book using the recipient's name.\nThe translation between addresses and domain names is performed by the Domain Name System (DNS), a hierarchical, distributed naming system that allows for the subdelegation of namespaces to other DNS servers.\nUnnumbered interface.\nAn unnumbered point-to-point (PtP) link, also called a transit link, is a link that does not have an IP network or subnet number associated with it, but still has an IP address. First introduced in 1993, Phil Karn from Qualcomm is credited as the original designer.\nThe purpose of a transit link is to route datagrams. They are used to free IP addresses from a scarce IP address space or to reduce the management of assigning IP and configuration of interfaces. Previously, every link needed to dedicate a or subnet using 2 or 4 IP addresses per point-to-point link. When a link is unnumbered, a \"router-id\" is used, a single IP address borrowed from a defined (normally a loopback) interface. The same \"router-id\" can be used on multiple interfaces.\nOne of the disadvantages of unnumbered interfaces is that it is harder to do remote testing and management.\nPacket structure.\nAn IP packet consists of a header section and a data section. An IP packet has no data checksum or any other footer after the data section.\nTypically the link layer encapsulates IP packets in frames with a CRC footer that detects most errors. Many transport-layer protocols carried by IP also have their own error checking.\nHeader.\nThe IPv4 packet header consists of 14 fields, of which 13 are required. The 14th field is optional and aptly named: options. The fields in the header are packed with the most significant byte first (network byte order), and for the diagram and discussion, the most significant bits are considered to come first (MSB 0 bit numbering). The most significant bit is numbered 0, so the version field is actually found in the four most significant bits of the first byte, for example.\nAgain, the data size is preserved: 1,480 + 1,000 = 2,480, and 1,480 + 540 = 2,020.\nAlso in this case, the \"More Fragments\" bit remains 1 for all the fragments that came with 1 in them and for the last fragment that arrives, it works as usual, that is the MF bit is set to 0 only in the last one. And of course, the Identification field continues to have the same value in all re-fragmented fragments. This way, even if fragments are re-fragmented, the receiver knows they have initially all started from the same packet.\nThe last offset and last data size are used to calculate the total data size: formula_1.\nReassembly.\nA receiver knows that a packet is a fragment, if at least one of the following conditions is true:\nThe receiver identifies matching fragments using the source and destination addresses, the protocol ID, and the identification field. The receiver reassembles the data from fragments with the same ID using both the fragment offset and the more fragments flag. When the receiver receives the last fragment, which has the \"more fragments\" flag set to 0, it can calculate the size of the original data payload, by multiplying the last fragment's offset by eight and adding the last fragment's data size. In the given example, this calculation was formula_2 bytes. When the receiver has all fragments, they can be reassembled in the correct sequence according to the offsets to form the original datagram.\nAssistive protocols.\nIP addresses are not tied in any permanent manner to networking hardware and, indeed, in modern operating systems, a network interface can have multiple IP addresses. In order to properly deliver an IP packet to the destination host on a link, hosts and routers need additional mechanisms to make an association between the hardware address of network interfaces and IP addresses. The Address Resolution Protocol (ARP) performs this IP-address-to-hardware-address translation for IPv4. In addition, the reverse correlation is often necessary. For example, unless an address is preconfigured by an administrator, when an IP host is booted or connected to a network it needs to determine its IP address. Protocols for such reverse correlations include Dynamic Host Configuration Protocol (DHCP), Bootstrap Protocol (BOOTP) and, infrequently, reverse ARP.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15318", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=15318", "title": "IPv6", "text": "Version 6 of the Internet Protocol\nInternet Protocol version 6 (IPv6) is the most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion, and was intended to replace IPv4. In December 1998, IPv6 became a Draft Standard for the IETF, which subsequently ratified it as an Internet Standard on 14 July 2017.\nDevices on the Internet are assigned a unique IP address for identification and location definition. With the rapid growth of the Internet after commercialization in the 1990s, it became evident that far more addresses would be needed to connect devices than the 4,294,967,296 (232) IPv4 address space had available. By 1998, the IETF had formalized the successor protocol, IPv6 which uses 128-bit addresses, theoretically allowing 2128, or 340,282,366,920,938,463,463,374,607,431,768,211,456 total addresses. The actual number is slightly smaller, as multiple ranges are reserved for special usage or completely excluded from general use. The two protocols are not designed to be interoperable, and thus direct communication between them is impossible, complicating the move to IPv6. However, several transition mechanisms have been devised to rectify this.\nIPv6 provides other technical benefits in addition to a larger addressing space. In particular, it permits hierarchical address allocation methods that facilitate route aggregation across the Internet, and thus limit the expansion of routing tables. The use of multicast addressing is expanded and simplified, and provides additional optimization for the delivery of services. Device mobility, security, and configuration aspects have been considered in the design of the protocol.\nIPv6 addresses are represented as eight groups of four hexadecimal digits each, separated by colons. The full representation may be shortened according to specific rules; for example, becomes .\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nMain features.\nIPv6 is an Internet Layer protocol for packet-switched internetworking and provides end-to-end datagram transmission across multiple IP networks, closely adhering to the design principles developed in the previous version of the protocol, Internet Protocol Version 4 (IPv4).\nIn addition to offering more addresses, IPv6 also implements features not present in IPv4. It simplifies aspects of address configuration, network renumbering, and router announcements when changing network connectivity providers. It simplifies packet processing in routers by placing the responsibility for packet fragmentation in the end points. The IPv6 subnet size is standardized by fixing the size of the host identifier portion of an address to 64 bits.\nThe addressing architecture of IPv6 allows three different types of transmission: unicast, anycast and multicast. IPv6 does not implement broadcast, and therefore has no notion of a broadcast address.\nMotivation and origin.\nIPv4 address exhaustion.\nInternet Protocol Version 4 (IPv4) was the first publicly used version of the Internet Protocol. IPv4 was developed as a research project by the Defense Advanced Research Projects Agency (DARPA), a United States Department of Defense agency, before becoming the foundation for the Internet and the World Wide Web. IPv4 includes an addressing system that uses numerical identifiers consisting of 32 bits. These addresses are typically displayed in dot-decimal notation as decimal values of four octets, each in the range 0 to 255, or 8 bits per number. Thus, IPv4 provides an addressing capability of 232 or approximately 4.3 billion addresses. Address exhaustion was not initially a concern in IPv4 as this version was originally presumed to be a test of DARPA's networking concepts. During the first decade of operation of the Internet, it became apparent that methods had to be developed to conserve address space. In the early 1990s, even after the redesign of the addressing system using a classless network model, it became clear that this would not suffice to prevent IPv4 address exhaustion, and that further changes to the Internet infrastructure were needed.\nThe last unassigned top-level address blocks of 16 million IPv4 addresses were allocated in February 2011 by the Internet Assigned Numbers Authority (IANA) to the five regional Internet registries (RIRs). However, each RIR still has available address pools and is expected to continue with standard address allocation policies until one Classless Inter-Domain Routing (CIDR) block remains. After that, only blocks of 1,024 addresses (/22) will be provided from the RIRs to a local Internet registry (LIR). As of April 2025, all of Asia-Pacific Network Information Centre (APNIC), the R\u00e9seaux IP Europ\u00e9ens Network Coordination Centre (RIPE NCC), Latin America and Caribbean Network Information Centre (LACNIC), African Network Information Centre (AFRINIC), and American Registry for Internet Numbers (ARIN) have reached this stage.\nRIPE NCC announced that it had fully run out of IPv4 addresses on 25 November 2019, and called for greater progress on the adoption of IPv6.\nComparison with IPv4.\nOn the Internet, data is transmitted in the form of network packets. IPv6 specifies a new packet format, designed to minimize packet header processing by routers. Because the headers of IPv4 packets and IPv6 packets are significantly different, the two protocols are not interoperable. However, most transport and application-layer protocols need little or no change to operate over IPv6; exceptions are application protocols that embed Internet-layer addresses, such as File Transfer Protocol (FTP) and Network Time Protocol (NTP), where the new address format may cause conflicts with existing protocol syntax.\nLarger address space.\nThe main advantage of IPv6 over IPv4 is its larger address space. The size of an IPv6 address is 128 bits, compared to 32 bits in IPv4. The address space therefore has 2128=340,282,366,920,938,463,463,374,607,431,768,211,456 addresses (340 undecillion, approximately ). Some blocks of this space and some specific addresses are reserved for special uses.\nWhile this address space is very large, it was not the intent of the designers of IPv6 to assure geographical saturation with usable addresses. Rather, the longer addresses simplify allocation of addresses, enable efficient route aggregation, and allow implementation of special addressing features. In IPv4, complex Classless Inter-Domain Routing (CIDR) methods were developed to make the best use of the small address space. The standard size of a subnet in IPv6 is 264 addresses, about four billion times the size of the entire IPv4 address space. Thus, actual address space utilization will be small in IPv6, but network management and routing efficiency are improved by the large subnet space and hierarchical route aggregation.\nMulticasting.\nMulticasting, the transmission of a packet to multiple destinations in a single send operation, is part of the base specification in IPv6. In IPv4 this is an optional (although commonly implemented) feature. IPv6 multicast addressing has features and protocols in common with IPv4 multicast, but also provides changes and improvements by eliminating the need for certain protocols. IPv6 does not implement traditional IP broadcast, i.e. the transmission of a packet to all hosts on the attached link using a special \"broadcast address\", and therefore does not define broadcast addresses. In IPv6, the same result is achieved by sending a packet to the link-local \"all nodes\" multicast group at address , which is analogous to IPv4 multicasting to address . IPv6 also provides for new multicast implementations, including embedding rendezvous point addresses in an IPv6 multicast group address, which simplifies the deployment of inter-domain solutions.\nIn IPv4 it is very difficult for an organization to get even one globally routable multicast group assignment, and the implementation of inter-domain solutions is arcane. Unicast address assignments by a local Internet registry for IPv6 have at least a 64-bit routing prefix, yielding the smallest subnet size available in IPv6 (also 64 bits). With such an assignment it is possible to embed the unicast address prefix into the IPv6 multicast address format, while still providing a 32-bit block, the least significant bits of the address, or approximately 4.2 billion multicast group identifiers. Thus each user of an IPv6 subnet automatically has available a set of globally routable source-specific multicast groups for multicast applications.\nStateless address autoconfiguration (SLAAC).\nIPv6 hosts configure themselves automatically. Every interface has a self-generated link-local address and, when connected to a network, conflict resolution is performed and routers provide network prefixes via router advertisements. Stateless configuration of routers can be achieved with a special router renumbering protocol. When necessary, hosts may configure additional stateful addresses via Dynamic Host Configuration Protocol version 6 (DHCPv6) or static addresses manually.\nLike IPv4, IPv6 supports globally unique IP addresses. The design of IPv6 intended to re-emphasize the end-to-end principle of network design that was originally conceived during the establishment of the early Internet by rendering network address translation obsolete. Therefore, every device on the network is globally addressable directly from any other device.\nA stable, unique, globally addressable IP address would facilitate tracking a device across networks. Therefore, such addresses are a particular privacy concern for mobile devices, such as laptops and cell phones. To address these privacy concerns, the SLAAC protocol includes what are typically called \"privacy addresses\" or, more correctly, \"temporary addresses\". Temporary addresses are random and unstable. A typical consumer device generates a new temporary address daily and will ignore traffic addressed to an old address after one week. Temporary addresses are used by default by Windows since XP SP1, macOS since (Mac\u00a0OS\u00a0X) 10.7, Android since 4.0, and iOS since version 4.3. Use of temporary addresses by Linux distributions varies.\nRenumbering an existing network for a new connectivity provider with different routing prefixes is a major effort with IPv4. With IPv6, however, changing the prefix announced by a few routers can in principle renumber an entire network, since the host identifiers (the least-significant 64 bits of an address) can be independently self-configured by a host.\nThe SLAAC address generation method is implementation-dependent. IETF recommends that addresses be deterministic but semantically opaque.\nIPsec.\nInternet Protocol Security (IPsec) was originally developed for IPv6, but found widespread deployment first in IPv4, for which it was re-engineered. IPsec was a mandatory part of all IPv6 protocol implementations, and Internet Key Exchange (IKE) was recommended, but with RFC 6434 the inclusion of IPsec in IPv6 implementations was downgraded to a recommendation because it was considered impractical to require full IPsec implementation for all types of devices that may use IPv6. However, as of RFC 4301 IPv6 protocol implementations that do implement IPsec need to implement IKEv2 and need to support a minimum set of cryptographic algorithms. This requirement will help to make IPsec implementations more interoperable between devices from different vendors. The IPsec Authentication Header (AH) and the Encapsulating Security Payload header (ESP) are implemented as IPv6 extension headers.\nSimplified processing by routers.\nThe packet header in IPv6 is simpler than the IPv4 header. Many rarely used fields have been moved to optional header extensions. The IPv6 packet header has simplified the process of packet forwarding by routers. Although IPv6 packet headers are at least twice the size of IPv4 packet headers, processing of packets that only contain the base IPv6 header by routers may, in some cases, be more efficient, because less processing is required in routers due to the headers being aligned to match common word sizes. However, many devices implement IPv6 support in software (as opposed to hardware), thus resulting in very bad packet processing performance. Additionally, for many implementations, the use of Extension Headers causes packets to be processed by a router's CPU, leading to poor performance or even security issues.\nMoreover, an IPv6 header does not include a checksum. The IPv4 header checksum is calculated for the IPv4 header, and has to be recalculated by routers every time the time to live (called hop limit in the IPv6 protocol) is reduced by one. The absence of a checksum in the IPv6 header furthers the end-to-end principle of Internet design, which envisioned that most processing in the network occurs in the leaf nodes. Integrity protection for the data that is encapsulated in the IPv6 packet is assumed to be assured by both the link layer or error detection in higher-layer protocols, namely the Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP) on the transport layer. Thus, while IPv4 allowed UDP datagram headers to have no checksum (indicated by 0 in the header field), IPv6 requires a checksum in UDP headers.\nIPv6 routers do not perform IP fragmentation. IPv6 hosts are required to do one of the following: perform Path MTU Discovery, perform end-to-end fragmentation, or send packets no larger than the default maximum transmission unit (MTU), which is 1280 octets.\nMobility.\nUnlike mobile IPv4, mobile IPv6 avoids triangular routing and is therefore as efficient as native IPv6. IPv6 routers may also allow entire subnets to move to a new router connection point without renumbering.\nExtension headers.\nThe IPv6 packet header has a minimum size of 40 octets (320 bits). Options are implemented as extensions. This provides the opportunity to extend the protocol in the future without affecting the core packet structure. However, RFC 7872 notes that some network operators drop IPv6 packets with extension headers when they traverse transit autonomous systems.\nJumbograms.\nIPv4 limits packets to 65,535 (216 \u2212 1) octets of payload. An IPv6 node can optionally handle packets over this limit, referred to as jumbograms, which can be as large as 4,294,967,295 (232 \u2212 1) octets. The use of jumbograms may improve performance over high-MTU links. The use of jumbograms is indicated by the Jumbo Payload Option extension header.\nIPv6 packets.\nAn IPv6 packet has two parts: a header and payload.\nThe header consists of a fixed portion with minimal functionality required for all packets and may be followed by optional extensions to implement special features.\nThe fixed header occupies the first 40\u00a0octets (320 bits) of the IPv6 packet. It contains the source and destination addresses, traffic class, hop count, and the type of the optional extension or payload which follows the header. This \"Next Header\" field tells the receiver how to interpret the data which follows the header. If the packet contains options, this field contains the option type of the next option. The \"Next Header\" field of the last option points to the upper-layer protocol that is carried in the packet's payload.\nThe current use of the IPv6 Traffic Class field divides this between a 6 bit Differentiated Services Code Point. and a 2-bit Explicit Congestion Notification field.\nExtension headers carry options that are used for special treatment of a packet in the network, e.g., for routing, fragmentation, and for security using the IPsec framework.\nWithout special options, a payload must be less than . With a Jumbo Payload option (in a \"Hop-By-Hop Options\" extension header), the payload must be less than 4\u00a0GB.\nUnlike with IPv4, routers never fragment a packet. Hosts are expected to use Path MTU Discovery to make their packets small enough to reach the destination without needing to be fragmented. See IPv6 packet fragmentation.\nAddressing.\nIPv6 addresses have 128 bits. The design of the IPv6 address space implements a different design philosophy than in IPv4, in which subnetting was used to improve the efficiency of utilization of the small address space. In IPv6, the address space is deemed large enough for the foreseeable future, and a local area subnet always uses 64 bits for the host portion of the address, designated as the interface identifier, while the most-significant 64 bits are used as the routing prefix. While the myth has existed regarding IPv6 subnets being impossible to scan, notes that patterns resulting from some IPv6 address configuration techniques and algorithms allow address scanning in many real-world scenarios.\nAddress representation.\nThe 128 bits of an IPv6 address are represented in 8 groups of 16 bits each. Each group is written as four hexadecimal digits (sometimes called \"hextets\" or more formally \"hexadectets\" and informally a \"quibble\" or \"quad-nibble\") and the groups are separated by colons (:). An example of this representation is .\nFor convenience and clarity, the representation of an IPv6 address may be shortened with the following rules:\nAn example of application of these rules:\nInitial address: .\nAfter removing all leading zeros in each group: .\nAfter omitting consecutive sections of zeros: .\nThe loopback address is defined as and is abbreviated to by using both rules.\nAs an IPv6 address may have more than one representation, the IETF has issued a proposed standard for representing them in text.\nBecause IPv6 addresses contain colons, and URLs use colons to separate the host from the port number, an IPv6 address used as the host-part of a URL should be enclosed in square brackets, e.g. http://[2001:db8:4006:812::200e] or http://[2001:db8:4006:812::200e]:8080/path/page.html.\nLink-local address.\nAll interfaces of IPv6 hosts require a link-local address, which have the prefix . This prefix is followed by 54 bits that can be used for subnetting, although they are typically set to zeros, and a 64-bit interface identifier. The host can compute and assign the Interface identifier by itself without the presence or cooperation of an external network component like a DHCP server, in a process called \"link-local address autoconfiguration\".\nThe lower 64 bits of the link-local address (the suffix) were originally derived from the MAC address of the underlying network interface card. As this method of assigning addresses would cause undesirable address changes when faulty network cards were replaced, and as it also suffered from a number of security and privacy issues, has replaced the original MAC-based method with the hash-based method specified in .\nAddress uniqueness and router solicitation.\nIPv6 uses a new mechanism for mapping IP addresses to link-layer addresses (e.g. MAC addresses), because it does not support the broadcast addressing method, on which the functionality of the Address Resolution Protocol (ARP) in IPv4 is based. IPv6 implements the Neighbor Discovery Protocol (NDP, ND) in the link layer, which relies on ICMPv6 and multicast transmission. IPv6 hosts verify the uniqueness of their IPv6 addresses in a local area network (LAN) by sending a neighbor solicitation message asking for the link-layer address of the IP address. If any other host in the LAN is using that address, it responds.\nA host bringing up a new IPv6 interface first generates a unique link-local address using one of several mechanisms designed to generate a unique address. Should a non-unique address be detected, the host can try again with a newly generated address. Once a unique link-local address is established, the IPv6 host determines whether the LAN is connected on this link to any router interface that supports IPv6. It does so by sending out an ICMPv6 router solicitation message to the all-routers multicast group with its link-local address as source. If there is no answer after a predetermined number of attempts, the host concludes that no routers are connected. If it does get a response, known as a router advertisement, from a router, the response includes the network configuration information to allow establishment of a globally unique address with an appropriate unicast network prefix. There are also two flag bits that tell the host whether it should use DHCP to get further information and addresses:\nGlobal addressing.\nThe assignment procedure for global addresses is similar to local-address construction. The prefix is supplied from router advertisements on the network. Multiple prefix announcements cause multiple addresses to be configured.\nStateless address autoconfiguration (SLAAC) requires a address block. Local Internet registries are assigned at least blocks, which they divide among subordinate networks. The initial recommendation of none }} stated assignment of a subnet to end-consumer sites. In none }} this recommendation was refined: The IETF \"recommends giving home sites significantly more than a single , but does not recommend that every home site be given a either\". Blocks of s are specifically considered. It remains to be seen whether ISPs will honor this recommendation. For example, during initial trials, Comcast customers were given a single network.\nIPv6 in the Domain Name System.\nIn the Domain Name System (DNS), hostnames are mapped to IPv6 addresses by AAAA (\"quad-A\") resource records. For reverse resolution, the IETF reserved the domain ip6.arpa, where the name space is hierarchically divided by the 1-digit hexadecimal representation of nibble units (4 bits) of the IPv6 address.\nWhen a dual-stack host queries a DNS server to resolve a fully qualified domain name (FQDN), the DNS client of the host sends two DNS requests, one querying AAAA records and the other querying A records, in that order, by default. If both types of addresses are returned by the DNS, and there is a route for it, the IPv6 address is preferred over the IPv4 address. However, the host operating system may be configured with an alternate preference for address selection.\nAn alternative record type was used in early DNS implementations for IPv6, designed to facilitate network renumbering. The \"A6\" resource record was used for the forward lookup, completed with a number of other innovations such as \"bit-string labels\" and \"DNAME\" records. After a discussion of the pros and cons of both schemes,) the use of A6 resource records has been deprecated to experimental status.\nTransition mechanisms.\nIPv6 is not foreseen to supplant IPv4 instantaneously. Both protocols will continue to operate simultaneously for some time. Therefore, IPv6 transition mechanisms are needed to enable IPv6 hosts to reach IPv4 services and to allow isolated IPv6 hosts and networks to reach each other over IPv4 infrastructure.\nAccording to Silvia Hagen, a dual-stack implementation of the IPv4 and IPv6 on devices is the easiest way to migrate to IPv6. Many other transition mechanisms use tunneling to encapsulate IPv6 traffic within IPv4 networks and vice versa. This is an imperfect solution, which reduces the maximum transmission unit (MTU) of a link and therefore complicates Path MTU Discovery, and may increase latency.\nDual-stack IP implementation.\nDual-stack IP implementations provide complete IPv4 and IPv6 protocol stacks in the operating system of a computer or network device on top of the common physical layer implementation, such as Ethernet. This permits dual-stack hosts to participate in IPv6 and IPv4 networks simultaneously.\nA device with dual-stack implementation in the operating system has an IPv4 and IPv6 address, and can communicate with other nodes in the LAN or the Internet using either IPv4 or IPv6. The DNS protocol is used by both IP protocols to resolve fully qualified domain names and IP addresses, but dual stack requires that the resolving DNS server can resolve both types of addresses. Such a dual-stack DNS server holds IPv4 addresses in the A records and IPv6 addresses in the AAAA records. Depending on the destination that is to be resolved, a DNS name server may return an IPv4 or IPv6 IP address, or both. A default address selection mechanism, or preferred protocol, needs to be configured either on hosts or the DNS server. The IETF has published Happy Eyeballs to assist dual-stack applications, so that they can connect using both IPv4 and IPv6, but prefer an IPv6 connection if it is available. However, dual-stack also needs to be implemented on all routers between the host and the service for which the DNS server has returned an IPv6 address. Dual-stack clients should be configured to prefer IPv6 only if the network is able to forward IPv6 packets using the IPv6 versions of routing protocols. When dual-stack network protocols are in place the application layer can be migrated to IPv6.\nWhile dual-stack is supported by major operating system and network device vendors, legacy networking hardware and servers do not support IPv6.\nISP customers with public-facing IPv6.\nInternet service providers (ISPs) are increasingly providing their business and private customers with public-facing IPv6 global unicast addresses. If IPv4 is still used in the local area network (LAN), however, and the ISP can only provide one public-facing IPv6 address, the IPv4 LAN addresses are translated into the public facing IPv6 address using NAT64, a network address translation (NAT) mechanism. Some ISPs cannot provide their customers with public-facing IPv4 and IPv6 addresses, thus supporting dual-stack networking, because some ISPs have exhausted their globally routable IPv4 address pool. Meanwhile, ISP customers are still trying to reach IPv4 web servers and other destinations.\nA significant percentage of ISPs in all regional Internet registry (RIR) zones have obtained IPv6 address space. This includes many of the world's major ISPs and mobile network operators, such as Verizon Wireless, StarHub Cable, Chubu Telecommunications, Kabel Deutschland, Swisscom, T-Mobile, Internode and Telef\u00f3nica.\nWhile some ISPs still allocate customers only IPv4 addresses, many ISPs allocate their customers only an IPv6 or dual-stack IPv4 and IPv6. ISPs report the share of IPv6 traffic from customers over their network to be anything between 20% and 40%, but by mid-2017 IPv6 traffic still only accounted for a fraction of total traffic at several large Internet exchange points (IXPs). AMS-IX reported it to be 2% and SeattleIX reported 7%. A 2017 survey found that many DSL customers that were served by a dual stack ISP did not request DNS servers to resolve fully qualified domain names into IPv6 addresses. The survey also found that the majority of traffic from IPv6-ready web-server resources were still requested and served over IPv4, mostly due to ISP customers that did not use the dual stack facility provided by their ISP and to a lesser extent due to customers of IPv4-only ISPs.\nTunneling.\nThe technical basis for tunneling, or encapsulating IPv6 packets in IPv4 packets, is outlined in RFC 4213. When the Internet backbone was IPv4-only, one of the frequently used tunneling protocols was 6to4. Teredo tunneling was also frequently used for integrating IPv6 LANs with the IPv4 Internet backbone. Teredo is outlined in RFC 4380 and allows IPv6 local area networks to tunnel over IPv4 networks, by encapsulating IPv6 packets within UDP. The Teredo relay is an IPv6 router that mediates between a Teredo server and the native IPv6 network. It was expected that 6to4 and Teredo would be widely deployed until ISP networks would switch to native IPv6, but by 2014 Google Statistics showed that the use of both mechanisms had dropped to almost 0.\nIPv4-mapped IPv6 addresses.\nHybrid dual-stack IPv6/IPv4 implementations recognize a special class of addresses, the IPv4-mapped IPv6 addresses. These addresses are typically written with a 96-bit prefix in the standard IPv6 format, and the remaining 32 bits are written in the customary dot-decimal notation of IPv4.\nAddresses in this group consist of an 80-bit prefix of zeros, the next 16 bits are ones, and the remaining, least-significant 32 bits contain the IPv4 address. For example, represents the IPv4 address . A previous format, called \"IPv4-compatible IPv6 address\", was ; however, this method is deprecated.\nBecause of the significant internal differences between IPv4 and IPv6 protocol stacks, some of the lower-level functionality available to programmers in the IPv6 stack does not work the same when used with IPv4-mapped addresses. Some common IPv6 stacks do not implement the IPv4-mapped address feature, either because the IPv6 and IPv4 stacks are separate implementations (e.g., Microsoft Windows 2000, XP, and Server 2003), or because of security concerns (OpenBSD). On these operating systems, a program must open a separate socket for each IP protocol it uses. On some systems, e.g., the Linux kernel, NetBSD, and FreeBSD, this feature is controlled by the socket option IPV6_V6ONLY.22\nThe address prefix is a class of IPv4-embedded IPv6 addresses for use in NAT64 transition methods. For example, represents the IPv4 address .\nSecurity.\nA number of security implications may arise from the use of IPv6. Some of them may be related with the IPv6 protocols themselves, while others may be related with implementation flaws.\nShadow networks.\nThe addition of nodes having IPv6 enabled by default by the software manufacturer may result in the inadvertent creation of \"shadow networks\", causing IPv6 traffic flowing into networks having only IPv4 security management in place. This may also occur with operating system upgrades, when the newer operating system enables IPv6 by default, while the older one did not. Failing to update the security infrastructure to accommodate IPv6 can lead to IPv6 traffic bypassing it. Shadow networks have occurred on business networks in which enterprises are replacing Windows XP systems that do not have an IPv6 stack enabled by default, with Windows 7 systems, that do. Some IPv6 stack implementors have therefore recommended disabling IPv4 mapped addresses and instead using a dual-stack network where supporting both IPv4 and IPv6 is necessary.\nIPv6 packet fragmentation.\nResearch has shown that the use of fragmentation could be leveraged to evade network security controls, similar to IPv4. As a result, it is now required that the first fragment of an IPv6 packet contains the entire IPv6 header chain, such that some very pathological fragmentation cases are forbidden. Additionally, as a result of research on the evasion of RA-Guard, the use of fragmentation is deprecated with Neighbor Discovery, and discouraged with Secure Neighbor Discovery (SEND).\nStandardization through RFCs.\nWorking-group proposals.\nDue to the anticipated global growth of the Internet, the Internet Engineering Task Force (IETF) in the early 1990s started an effort to develop a next generation IP protocol. By the beginning of 1992, several proposals appeared for an expanded Internet addressing system and by the end of 1992 the IETF announced a call for white papers. In September 1993, the IETF created a temporary, ad hoc \"IP Next Generation\" (IPng) area to deal specifically with such issues. The new area was led by Allison Mankin and Scott Bradner, and had a directorate with 15 engineers from diverse backgrounds for direction-setting and preliminary document review: The working-group members were J. Allard (Microsoft), Steve Bellovin (AT&amp;T), Jim Bound (Digital Equipment Corporation), Ross Callon (Wellfleet), Brian Carpenter (CERN), Dave Clark (MIT), John Curran (NEARNET), Steve Deering (Xerox), Dino Farinacci (Cisco), Paul Francis (NTT), Eric Fleischmann (Boeing), Mark Knopper (Ameritech), Greg Minshall (Novell), Rob Ullmann (Lotus), and Lixia Zhang (Xerox).\nThe Internet Engineering Task Force adopted the IPng model on 25 July 1994, with the formation of several IPng working groups. By 1996, a series of RFCs was released defining Internet Protocol version 6 (IPv6), starting with . (Version 5 was used by the experimental Internet Stream Protocol.)\nRFC standardization.\nThe first RFC to standardize IPv6 was the in 1995. In 1998 became the RFC for IPv6. In July 2017 was superseded by , which elevated IPv6 to \"Internet Standard\" (the highest maturity level for IETF protocols).\nDeployment.\nThe 1993 introduction of Classless Inter-Domain Routing (CIDR) in the routing and IP address allocation for the Internet, and the extensive use of network address translation (NAT), delayed IPv4 address exhaustion to allow for IPv6 deployment, which began in the mid-2000s.\nUniversities were among the early adopters of IPv6. Virginia Tech deployed IPv6 at a trial location in 2004 and later expanded IPv6 deployment across the campus network. By 2016, 82% of the traffic on their network used IPv6. Imperial College London began experimental IPv6 deployment in 2003 and by 2016 the IPv6 traffic on their networks averaged between 20% and 40%. A significant portion of this IPv6 traffic was generated through their high energy physics collaboration with CERN, which relies entirely on IPv6.\nThe Domain Name System (DNS) has supported IPv6 since 2008. In the same year, IPv6 was first used in a major world event during the Beijing 2008 Summer Olympics.\nBy 2011, all major operating systems in use on personal computers and server systems had production-quality IPv6 implementations. Cellular telephone systems presented a large deployment field for Internet Protocol devices as mobile telephone service made the transition from 3G to 4G technologies, in which voice is provisioned as a voice over IP (VoIP) service that would leverage IPv6 enhancements. In 2009, the US cellular operator Verizon released technical specifications for devices to operate on its \"next-generation\" networks. The specification mandated IPv6 operation according to the \"3GPP Release 8 Specifications (March 2009)\", and deprecated IPv4 as an optional capability.\nThe deployment of IPv6 in the Internet backbone continued. In 2018 only 25.3% of the about 54,000 autonomous systems advertised both IPv4 and IPv6 prefixes in the global Border Gateway Protocol (BGP) routing database. A further 243 networks advertised only an IPv6 prefix. Internet backbone transit networks offering IPv6 support existed in every country globally, except in parts of Africa, the Middle East and China. By mid-2018 some major European broadband ISPs had deployed IPv6 for the majority of their customers. Sky UK provided over 86% of its customers with IPv6, Deutsche Telekom had 56% deployment of IPv6, XS4ALL in the Netherlands had 73% deployment and in Belgium the broadband ISPs VOO and Telenet had 73% and 63% IPv6 deployment respectively. In the United States the broadband ISP Xfinity had an IPv6 deployment of about 66%. In 2018 Xfinity reported an estimated 36.1 million IPv6 users, while AT&amp;T reported 22.3 million IPv6 users.\nPeering issues.\nThere is a peering dispute going on between Hurricane Electric and Cogent Communications on IPv6, with the two network providers refusing to peer.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15319", "revid": "48181213", "url": "https://en.wikipedia.org/wiki?curid=15319", "title": "Inca Empire", "text": "1438\u20131533 empire in South America\nThe Inca Empire, officially known as the Realm of the Four Parts ( , lit.\u2009'land of four parts'), was the largest empire in pre-Columbian America. The administrative, political, and military center of the empire was in the city of Cusco. The Inca civilisation rose from the Peruvian highlands sometime in the early 13th century. The Portuguese explorer Aleixo Garcia was the first European to reach the Inca Empire in 1524. Later, in 1532, the Spanish began the conquest of the Inca Empire, and by 1572 the last Inca state was fully conquered.\nFrom 1438 to 1533, the Incas incorporated a large portion of western South America, centered on the Andean Mountains, using conquest and peaceful assimilation, among other methods. At its largest, the empire joined modern-day Peru with what are now western Ecuador, western and south-central Bolivia, northwest Argentina, the southwesternmost tip of Colombia and a large portion of modern-day Chile, forming a state comparable to the historical empires of Eurasia. Its official language was Quechua.\nThe Inca Empire was unique in that it lacked many of the features associated with civilization in the Old World. The anthropologist Gordon McEwan wrote that the Incas were able to construct \"one of the greatest imperial states in human history\" without the use of the wheel, draft animals, knowledge of iron or steel, or even a system of writing. Notable features of the Inca Empire included its monumental architecture, especially stonework, extensive road network () reaching all corners of the empire, finely-woven textiles, use of knotted strings ( or \"khipu\") for record keeping and communication, agricultural innovations and production in a difficult environment, and the organization and management fostered or imposed on its people and their labor.\nThe Inca Empire functioned largely without money and without markets. Instead, exchange of goods and services was based on reciprocity between individuals and among individuals, groups, and Inca rulers. \"Taxes\" consisted of a labour obligation of a person to the Empire. The Inca rulers (who theoretically owned all the means of production) reciprocated by granting access to land and goods and providing food and drink in celebratory feasts for their subjects.\nMany local forms of worship persisted in the empire, most of them concerning local sacred \"huacas\" or \"wak'a\", but the Inca leadership encouraged the sun worship of Inti \u2013 their sun god \u2013 and imposed its sovereignty above other religious groups, such as that of Pachamama. The Incas considered their king, the Sapa Inca, to be the \"son of the Sun\".\nThe Inca economy has been the subject of scholarly debate. Darrell E. La Lone, in his work \"The Inca as a Nonmarket Economy\", noted that scholars have previously described it as \"feudal, slave, [or] socialist\", as well as \"a system based on reciprocity and redistribution; a system with markets and commerce; or an Asiatic mode of production.\"\nEtymology.\nThe Inca referred to their empire as \"Tawantinsuyu\", \"the suyu of four [parts]\". In Quechua, \"tawa\" is four and \u2013 \"ntin\" is a suffix naming a group, so that a \"tawantin\" is a quartet, a group of four things taken together, in this case the four \"suyu\" (\"regions\" or \"provinces\") whose corners met at the capital. The four \"suyu\" were: Chinchaysuyu (north), Antisuyu (east; the Amazon jungle), Qullasuyu (south) and Kuntisuyu (west). The name \"Tawantinsuyu\" was, therefore, a descriptive term indicating a union of provinces. The Spanish normally transliterated the name as \"Tahuatinsuyo\".\nWhile the term nowadays is translated as \"ruler\" or \"lord\" in Quechua, this term does not simply refer to the \"king\" of the Tawantinsuyu or \"Sapa Inca\" but also to the Inca nobles, and some theorize its meaning could be broader. In that sense, the Inca nobles were a small percentage of the total population of the empire, probably numbering only 15,000 to 40,000, but ruling a population of around 10million people.\nWhen the Spanish arrived in the Empire of the Incas, they gave the name \"Peru\" to what the natives knew as Tawantinsuyu. The name \"Inca Empire\" originated from the Chronicles of the 16th century.\nHistory.\nAntecedents.\nThe Inca Empire was the last chapter of thousands of years of Andean civilizations. The Andean civilisation is one of at least five civilisations in the world deemed by scholars to be \"pristine\". The concept of a pristine civilisation refers to a civilisation that has developed independently of external influences and is not a derivative of other civilisations.\nThe Inca Empire was preceded by two large-scale empires in the Andes: the Tiwanaku (c.\u2009300\u20131100 AD), based around Lake Titicaca, and the Wari or Huari (c.\u2009600\u20131100 AD), centered near the city of Ayacucho. The Wari occupied the Cuzco area for about 400 years. Thus, many of the characteristics of the Inca Empire derived from earlier multi-ethnic and expansive Andean cultures. To those earlier civilizations may be owed some of the accomplishments cited for the Inca Empire: \"thousands of kilometres/miles of roads and dozens of large administrative centers with elaborate stone construction... terraced mountainsides and filled in valleys\", and the production of \"vast quantities of goods\".\nCarl Troll has argued that the development of the Inca state in the central Andes was aided by conditions that allow for the elaboration of the staple food \"chu\u00f1o\". Chu\u00f1o, which can be stored for long periods, is made of potato dried at the freezing temperatures that are common at nighttime in the southern Andean highlands. Such a link between the Inca state and chu\u00f1o has been questioned, as other crops such as maize can also be dried with only sunlight.\nTroll also argued that llamas, the Incas' pack animal, can be found in their largest numbers in this very same region. The maximum extent of the Inca Empire roughly coincided with the distribution of llamas and alpacas, the only large domesticated animals in Pre-Hispanic America.\nAs a third point Troll pointed out irrigation technology as advantageous to Inca state-building. While Troll theorized concerning environmental influences on the Inca Empire, he opposed environmental determinism, arguing that culture lay at the core of the Inca civilization.\nOrigin.\nThe Inca people were a pastoral tribe in the Cusco area around the 12th century. Indigenous Andean oral history tells two main origin stories: the legends of Manco Capac and Mama Ocllo, and that of the Ayar brothers.\nThe Legend of the Ayar Brothers.\nThe center cave at Tambo Tocco (Tampu T'uqu) was named Capac Tocco (Qhapaq T'uqu, \"principal niche\"). The other caves were Maras Tocco (Maras T'uqu) and Sutic Tocco (Sutiq T'uqu). Four brothers and four sisters stepped out of the middle cave. They were: Ayar Manco (Ayar Manqu), Ayar Cachi (Ayar Kachi), Ayar Auca (Ayar Awka) and Ayar Uchu (Ayar Uchi); and Mama Ocllo (Mama Uqllu), Mama Raua (Mama Rawa), Mama Huaco (Mama Waqu) and Mama Coea (Mama Qura). Out of the side caves came the people who were to be the ancestors of all the Inca clans.\nAyar Manco carried a magic staff made of the finest gold. Where this staff landed, the people would live. They traveled for a long time. On the way, Ayar Cachi boasted about his strength and power. His siblings tricked him into returning to the cave to get a sacred llama. When he went into the cave, they trapped him inside to get rid of him.\nAyar Uchu decided to stay on the top of the cave to look over the Inca people. The minute he proclaimed that, he turned to stone. They built a shrine around the stone and it became a sacred object. Ayar Auca grew tired of all this and decided to travel alone. Only Ayar Manco and his four sisters remained.\nFinally, they reached Cusco. The staff sank into the ground. Before they arrived, Mama Ocllo had already borne Ayar Manco a child, Sinchi Roca. The people who were already living in Cusco fought hard to keep their land, but Mama Huaca was a good fighter. When the enemy attacked, she threw her bolas (several stones tied together that spun through the air when thrown) at a soldier (gualla) and killed him instantly. The other people became afraid and ran away.\nAfter that, Ayar Manco became known as Manco Capac, the founder of the Inca. It is said that he and his sisters built the first Inca homes in the valley with their own hands. When the time came, Manco Capac turned to stone like his brothers before him. His son, Sinchi Roca, became the second emperor of the Inca.\nThe Legend of Manco C\u00e1pac and Mama Ocllo.\nLegend collected by the chronicler Inca Garcilaso de la Vega in his work \"Los Comentarios Reales de los Incas\" (transl.\u2009The Royal Commentaries of the Inca). It narrates the adventure of a couple, Manco Capac and Mama Ocllo, who were sent by the Sun God and emerged from the depths of Lake Titicaca (\"pacarina\" ~ \"paqarina\" \"sacred place of origin\") and marched north. They carried a golden staff, given by the Sun God; the message was clear: in the place where the golden staff sank, they would establish a city and settle there. The staff sank at Mount Guanacaure in the Acamama Valley; therefore, the couple decided to remain there and informed the inhabitants of the area that they were sent by the Sun God. They then proceeded to teach them agriculture and weaving. Thus, the Inca civilization began.\nKingdom of Cuzco.\nIn the early 1200s, under the leadership of Manco Capac, the Inca formed the small city-state Kingdom of Cuzco (Quechua \"Qusqu\"). There Manco Capac built a temple to the Sun God, called Inticancha, in the current location of Coricancha. Over the successive Inca rulers, they expanded their influence beyond Cusco and into the Sacred Valley through a series of battles, marriages, and alliances.\nIn 1438, they began a far-reaching expansion under the command of the 9th Sapa Inca (\"paramount leader\"), Pachacuti Cusi Yupanqui (Pachakutiy Kusi Yupanki), whose epithet \"Pachacuti\" means \"the turn of the world\". The name of Pachacuti was given to him after he conquered the tribe of the Chancas during the Chanka\u2013Inca War (in modern-day Apur\u00edmac). During his reign, he and his son Topa Yupanqui (Tupa Yupanki) brought much of the modern-day territory of Peru under Inca control.\nReorganisation and formation.\nPachacuti reorganised the kingdom of Cusco into the Tahuantinsuyu, which consisted of a central government with the Inca at its head and four provincial governments with strong leaders: Chinchaysuyu (NW), Antisuyu (NE), Kuntisuyu (SW) and Qullasuyu (SE). Pachacuti is thought to have built Machu Picchu, either as a family home or summer retreat, although it may have been an agricultural station.\nPachacuti sent spies to regions he wanted in his empire and they brought to him reports on political organization, military strength and wealth. He then sent messages to their leaders extolling the benefits of joining his empire, offering them presents of luxury goods such as high quality textiles and promising that they would be materially richer as his subjects.\nMost accepted the rule of the Inca as a \"fait accompli\" and acquiesced peacefully. Refusal to accept Inca rule resulted in military conquest. Following conquest the local rulers were executed. The ruler's children were brought to Cuzco to learn about Inca administration systems, then return to rule their native lands. This allowed the Inca to indoctrinate them into the Inca nobility and, with luck, marry their daughters into families at various corners of the empire.\nExpansion and consolidation.\nPachacuti had named his favorite son, Amaru Yupanqui, as his co-ruler and successor. However, as co-ruler Amaru showed little interest in military affairs. Due to this lack of military talent, he faced much opposition from the Inca nobility, who began to plot against him. Despite this, Pachacuti decided to take a blind eye to his son's lack of capability. Following a revolt during which Amaru almost led the Inca forces to defeat, the Sapa Inca decided to replace the co-ruler with another one of his sons, Topa Inca Yupanqui. T\u00fapac Inca Yupanqui began conquests to the north in 1463 and continued them as Inca ruler after Pachacuti's death in 1471. T\u00fapac Inca's most important conquest was the Kingdom of Chimor, the Inca's only serious rival for the coast. T\u00fapac Inca's empire then stretched north into what are today Ecuador and Colombia. Topa Inca's son Huayna Capac added a small portion of land to the north in what is today Ecuador. At its height, the Inca Empire included modern-day Peru, what are today western and south central Bolivia, southwest Ecuador and Colombia and a large portion of modern-day Chile, at the north of the Maule River. Traditional historiography claims the advance south halted after the Battle of the Maule where they met determined resistance from the Mapuche.\nThis view is challenged by historian Osvaldo Silva who argues instead that it was the social and political framework of the Mapuche that posed the main difficulty in imposing imperial rule. Silva does accept that the battle of the Maule was a stalemate, but argues the Incas lacked the incentives for conquest they had when fighting more complex societies such as the Chim\u00fa Empire.\nSilva also disputes the date given by traditional historiography for the battle: the late 15th century during the reign of Topa Inca Yupanqui (1471\u20131493). Instead, he places it in 1532 during the Inca Civil War. Nevertheless, Silva agrees on the claim that the bulk of the Inca conquests were made during the late 15th century. At the time of the Inca Civil War an Inca army was, according to Diego de Rosales, subduing a revolt among the Diaguitas of Copiap\u00f3 and Coquimbo.\nThe empire's push into the Amazon Basin near the Chinchipe River was stopped by the Shuar in 1527. The empire extended into corners of what are today the north of Argentina and part of the southern Colombia. However, most of the southern portion of the Inca empire, the portion denominated as Qullasuyu, was located in the Altiplano.\nThe Inca Empire was an amalgamation of languages, cultures and peoples. The components of the empire were not all uniformly loyal, nor were the local cultures all fully integrated. The Inca empire as a whole had an economy based on exchange and taxation of luxury goods and labour. The following quote describes a method of taxation:\nFor as is well known to all, not a single village of the highlands or the plains failed to pay the tribute levied on it by those who were in charge of these matters. There were even provinces where, when the natives alleged that they were unable to pay their tribute, the Inca ordered that each inhabitant should be obliged to turn in every four months a large quill full of live lice, which was the Inca's way of teaching and accustoming them to pay tribute.\nFirst contact.\nAleixo Garcia (died 1525) was a Portuguese explorer and conquistador. He was a castaway who lived in Brazil and explored Paraguay and Bolivia. On a raiding expedition with a Guaran\u00ed army, Garcia and a few colleagues were the first Europeans known to have come into contact with the Inca Empire.\nInca Civil War and Spanish conquest.\nSpanish conquistadors led by Francisco Pizarro and his brothers explored south from what is today Panama, reaching Inca territory by 1526. It was clear that they had reached a wealthy land with prospects of great treasure, and after another expedition in 1529 Pizarro traveled to Spain and received royal approval to conquer the region and be its viceroy. This approval was received as detailed in the following quote: \"In July 1529 the Queen of Spain signed a charter allowing Pizarro to conquer the Incas. Pizarro was named governor and captain of all conquests in Peru, or New Castile, as the Spanish now called the land\".\nWhen the conquistadors returned to Peru in 1532, a war of succession between the sons of Sapa Inca Huayna Capac, Hu\u00e1scar and Atahualpa, and unrest among newly conquered territories weakened the empire. Perhaps more importantly smallpox, influenza, typhus and measles had potentially spread from Central America. The first epidemic of European disease in the Inca Empire possibly happened in the 1520s, killing Huayna Capac, his designated heir Ninan Cuyochi, and an unknown, probably large, number of other Inca subjects. This claim has been disputed, with the earliest written accounts of Huayan Capac's death not fully agreeing on the cause, early chroniclers like Francisco de Xerez having simply describing it as \"that disease\".\nThe forces led by Pizarro consisted of 168 men, along with one cannon and 27 horses. The conquistadors were armed with lances, arquebuses, steel armor and long swords. In contrast, the Inca used weapons made out of wood, stone, copper and bronze, while using an Alpaca fiber based armor, putting them at significant technological disadvantage \u2013 none of their weapons could pierce the Spanish steel armor. In addition, due to the absence of horses in Peru, the Inca did not develop tactics to fight cavalry. However, the Inca were still effective warriors, being able to successfully fight the Mapuche, who later would strategically defeat and reverse Spanish colonisation in southern Chile.\nThe first engagement between the Inca and the Spanish was the Battle of Pun\u00e1, near present-day Guayaquil, Ecuador, on the Pacific Coast; Pizarro then founded the city of Piura in July 1532. Hernando de Soto was sent inland to explore the interior and returned with an invitation to meet the Inca, Atahualpa, who had defeated his brother in the civil war and was resting at Cajamarca with his army of 80,000 troops, that were at the moment armed only with hunting tools (knives and lassos for hunting llamas).\nPizarro and some of his men, most notably a friar named Vincente de Valverde, met with the Inca, who had brought only a small retinue. The Inca offered them ceremonial chicha in a golden cup, which the Spanish rejected. The Spanish interpreter, Friar Vincente, read the \"Requerimiento\" that demanded that he and his empire accept the rule of King Charles I of Spain and convert to Christianity. Atahualpa dismissed the message and asked them to leave. After this, the Spanish began their attack against the mostly unarmed Inca, captured Atahualpa as hostage, and forced the Inca to collaborate.\nAtahualpa offered the Spaniards enough gold to fill the room he was imprisoned in and twice that amount of silver. The Inca fulfilled this ransom, but Pizarro deceived them, refusing to release the Inca afterwards. During Atahualpa's imprisonment, Huascar was assassinated elsewhere. The Spaniards maintained that this was at Atahualpa's orders; this was used as one of the charges against Atahualpa when the Spaniards finally executed him in August 1533.\nAlthough \"defeat\" often implies an unwanted loss in battle, many of the diverse ethnic groups ruled by the Inca \"welcomed the Spanish invaders as liberators and willingly settled down with them to share rule of Andean farmers and miners\". Many regional leaders, known as kurakas, continued to serve the Spanish overlords, called encomenderos, as they had served the Inca overlords. Other than efforts to spread the religion of Christianity, the Spanish benefited from and made little effort to change the society and culture of the former Inca Empire until the rule of Francisco de Toledo as viceroy from 1569 to 1581.\nEnd of the Inca Empire.\nThe Spanish installed Atahualpa's brother Manco Inca Yupanqui in power; for some time Manco cooperated with the Spanish while they fought to put down resistance in the north. Meanwhile, an associate of Pizarro, Diego de Almagro, attempted to claim Cusco. Manco tried to use this intra-Spanish feud to his advantage, recapturing Cusco in 1536, but the Spanish retook the city afterwards. Manco Inca then retreated to the mountains of Vilcabamba and established the small Neo-Inca State, where he and his successors ruled for another 36 years, sometimes raiding the Spanish or inciting revolts against them. In 1572 the last Inca stronghold was conquered and the last ruler, Topa Amaru, Manco's son, was captured and executed. This ended resistance to the Spanish conquest under the political authority of the Inca state.\nAfter the fall of the Inca Empire many aspects of Inca culture were systematically destroyed, including their sophisticated farming system, known as the vertical archipelago model of agriculture. Spanish colonial officials used the Inca mita corv\u00e9e labor system for colonial aims, sometimes brutally. One member of each family was forced to work in the gold and silver mines, the foremost of which was the titanic silver mine at Potos\u00ed. When a family member died, which would usually happen within a year or two, the family was required to send a replacement.\nAlthough smallpox is usually presumed to have spread through the Empire before the arrival of the Spaniards, the devastation is also consistent with other theories. Beginning in Colombia, smallpox spread rapidly before the Spanish invaders first arrived in the empire. The spread was probably aided by the efficient Inca road system. Smallpox was only the first epidemic. Other diseases, including a probable typhus outbreak in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, and measles in 1618, all ravaged the Inca people.\nThere would be periodic attempts by indigenous leaders to expel the Spanish colonists and re-create the Inca Empire until the late 18th century. See Juan Santos Atahualpa and T\u00fapac Amaru II.\nSociety.\nPopulation.\nThe number of people inhabiting Tawantinsuyu at its peak is uncertain, with estimates ranging from 4\u201337million. Most population estimates are in the range of 6 to 14million. In spite of the fact that the Inca kept excellent census records using their quipus, knowledge of how to read them was lost as almost all fell into disuse and disintegrated over time or were destroyed by the Spaniards.\nLanguages.\nThe empire was linguistically diverse. Some of the most important languages were Quechua, Aymara, Puquina and Mochica, respectively mainly spoken in the Central Andes, the Altiplano (Qullasuyu), the south coast (Kuntisuyu), and the area of the north coast (Chinchaysuyu) around Chan Chan, today Trujillo. Other languages included Quignam, Jaqaru, Leco, Uru-Chipaya languages, Kunza, Humahuaca, Cac\u00e1n, Mapudungun, Culle, Chachapoya, Catacao languages, Manta, Barbacoan languages, and Ca\u00f1ari\u2013Puruh\u00e1 as well as numerous Amazonian languages on the frontier regions. The exact linguistic topography of the pre-Columbian and early colonial Andes remains incompletely understood, owing to the extinction of several languages and the loss of historical records.\nIn order to manage this diversity, the Inca lords promoted the usage of Quechua, especially , as the official language or lingua franca. Defined by mutual intelligibility, Quechua is actually a family of languages rather than one single language, parallel to the Romance or Slavic languages in Europe. Most communities within the empire, even those resistant to Inca rule, learned to speak a variety of Quechua (forming new regional varieties with distinct phonetics) in order to communicate with the Inca lords and mitma colonists, as well as the wider integrating society, but largely retained their native languages as well. The Incas also had their own ethnic language, which is thought to have been closely related to or a dialect of Puquina.\nThere are several common misconceptions about the history of Quechua, as it is frequently identified as the \"Inca language\". Quechua did not originate with the Incas, had been a lingua franca in multiple areas before the Inca expansions, was diverse before the rise of the Incas, and it was not the native or original language of the Incas. However, the Incas left a linguistic legacy in that they introduced Quechua to many areas where it is still widely spoken today, including Ecuador, southern Bolivia, southern Colombia, and parts of the Amazon basin. The Spanish conquerors continued the official usage of Quechua during the early colonial period and transformed it into a literary language.\nThe Incas were not known to develop a written form of language; however, they visually recorded narratives through paintings on vases and cups (qirus). These paintings are usually accompanied by geometric patterns known as toqapu, which are also found in textiles. Researchers have speculated that toqapu patterns could have served as a form of written communication (e.g. heraldry or glyphs), however, this remains unclear. The Incas also kept records by using quipus.\nAge and defining gender.\nThe high infant mortality rates that plagued the Inca Empire caused all newborn infants to be given the term \"wawa\" when they were born. Most families did not invest very much into their child until they reached the age of two or three years old. Once the child reached the age of three, a \"coming of age\" ceremony occurred, called the \"rutuchikuy\". For the Incas, this ceremony indicated that the child had entered the stage of \"ignorance\". During this ceremony, the family would invite all relatives to their house for food and dance, and then each member of the family would receive a lock of hair from the child. After each family member had received a lock, the father would shave the child's head. This stage of life was categorized by a stage of \"ignorance, inexperience, and lack of reason, a condition that the child would overcome with time\". For Inca society, in order to advance from the stage of ignorance to development the child must learn the roles associated with their gender.\nThe next important ritual was to celebrate the maturity of a child. Unlike the coming of age ceremony, the celebration of maturity signified the child's sexual potency. This celebration of puberty was called \"warachikuy\" for boys and \"qikuchikuy\" for girls. The \"warachikuy\" ceremony included dancing, fasting, tasks to display strength, and family ceremonies. The boy would also be given new clothes and taught how to act as an unmarried man. The \"qikuchikuy\" signified the onset of menstruation, upon which the girl would go into the forest alone and return only once the bleeding had ended. In the forest she would fast, and, once returned, the girl would be given a new name, adult clothing, and advice. This \"folly\" stage of life was the time young adults were allowed to have sex without being a parent.\nBetween the ages of 20 and 30, people were considered young adults, \"ripe for serious thought and labor\". Young adults were able to retain their youthful status by living at home and assisting in their home community. Young adults only reached full maturity and independence once they had married.\nAt the end of life, the terms for men and women denote loss of sexual vitality and humanity. Specifically, the \"decrepitude\" stage signifies the loss of mental well-being and further physical decline.\n present in his book Daily Life in Peru Under the Last Incas another classification based on the ability to work for each age:\nThe category of \"The sleepy old man only able to give advices\" included also men non capable to work.\nMarriage.\nIn the Inca Empire, the age of marriage differed for men and women: men typically married at the age of 20, while women usually got married about four years earlier at the age of 16. Men who were highly ranked in society could have multiple wives, but those lower in the ranks could only take a single wife. Marriages were typically within classes and resembled a more business-like agreement. Once married, the women were expected to cook, collect food and watch over the children and livestock. Girls and mothers would also work around the house to keep it orderly to please the public inspectors. These duties remained the same even after wives became pregnant and with the added responsibility of praying and making offerings to Kanopa, who was the god of pregnancy. It was typical for marriages to begin on a trial basis with both men and women having a say in the longevity of the marriage. If the man felt that it would not work out or if the woman wanted to return to her parents' home the marriage would end. Once the marriage was final, the only way the two could be divorced was if they did not have a child together. Marriage within the Empire was crucial for survival. A family was considered disadvantaged if there was not a married couple at the center because everyday life centered around the balance of male and female tasks.\nGender roles.\nAccording to some historians, such as Terence N. D'Altroy, male and female roles were considered equal in Inca society. The \"indigenous cultures saw the two genders as complementary parts of a whole\". In other words, there was not a hierarchical structure in the domestic sphere for the Incas. Within the domestic sphere, women came to be known as weavers, although there is significant evidence to suggest that this gender role did not appear until colonizing Spaniards realized women's productive talents in this sphere and used it to their economic advantage. There is evidence to suggest that both men and women contributed equally to the weaving tasks in pre-Hispanic Andean culture. Women's everyday tasks included: spinning, watching the children, weaving cloth, cooking, brewing chichi, preparing fields for cultivation, planting seeds, bearing children, harvesting, weeding, hoeing, herding, and carrying water. Men on the other hand, \"weeded, plowed, participated in combat, helped in the harvest, carried firewood, built houses, herded llama and alpaca, and spun and wove when necessary\". This relationship between the genders may have been complementary. Onlooking Spaniards believed women were treated like slaves, because women did not work in Spanish society to the same extent, and certainly did not work in fields. Women were sometimes allowed to own land and herds because inheritance was passed down from both the mother's and father's side of the family. Kinship within the Inca society followed a parallel line of descent. In other words, women descended from women and men descended from men. Due to the parallel descent, a woman had access to land and other assets through her mother.\nEducation.\nAccess to formal education in Incan society was limited to children of the central nobility and certain levels of the curacal ( curaca). They attended the (house of knowledge) in Cusco to learn from the (wises) and the (poets). They learned languages, accounting, astronomy, about wars and political application strategies. The non-formalized education for the was given in daily life, in practice; it was also given in the assemblies of the ayllu or , where they were taught the three moral and legal principles: (don't be lazy), (don't steal) and (don't lie).\nBurial customs.\nDue to the dry climate that extends from modern-day Peru to what is now Chile's Norte Grande, mummification occurred naturally by desiccation. It is believed that the ancient Incas learned to mummify their dead to show reverence to their leaders and representatives. Mummification was chosen to preserve the body and to give others the opportunity to worship them in their death. The ancient Inca believed in reincarnation, so preservation of the body was vital for passage into the afterlife. Since mummification was reserved for royalty, this entailed preserving power by placing the deceased's valuables with the body in places of honor. The bodies remained accessible for ceremonies where they would be removed and celebrated with. The ancient Inca mummified their dead with various tools. Chicha corn beer was used to delay decomposition and the effects of bacterial activity on the body. The bodies were then stuffed with natural materials such as vegetable matter and animal hair. Sticks were used to maintain their shape and poses. In addition to the mummification process, the Inca would bury their dead in the fetal position inside a vessel intended to mimic the womb for preparation of their new birth. A ceremony would be held that included music, food, and drink for the relatives and loved ones of the deceased.\nDuality.\nThe basic organizational principle of Inca society was duality or , which was based on kinship relationships. The ayllus were divided into two parts that could be Hanan or Hurin, Alaasa or Massaa, Uma or Urco, Allauca or Ichoc; according to Franklin Pease, these terms were understood as \"high or low,\" \"right or left,\" \"male or female,\" \"inside or outside,\" \"near or far,\" and \"front or back.\" Though the specific functions of each part are unclear, it is documented that one leader was subordinate to the other, with Mar\u00eda Rostworowski noting that in Cuzco, the upper half was more important, while in Ica, the lower half held more significance. Pease also points out that both halves were integrated through reciprocity. In Cuzco, \"Hanan\" and \"Hurin\" were opposites yet complementary, like human hands in the .\nReligion.\nInca myths were transmitted orally until early Spanish colonists recorded them; however, some scholars claim that they were recorded on quipus, Andean knotted string records.\nThe Inca believed in reincarnation. After death, the passage to the next world was fraught with difficulties. The spirit of the dead, \"camaquen,\" would need to follow a long road and during the trip the assistance of a black dog that could see in the dark was required. Most Incas imagined the after world to be like an earthly paradise with flower-covered fields and snow-capped mountains.\nIt was important to the Inca that they not die as a result of burning or that the body of the deceased not be incinerated. Burning would cause their vital force to disappear and threaten their passage to the after world. The Inca nobility practiced cranial deformation. They wrapped tight cloth straps around the heads of newborns to shape their soft skulls into a more conical form, thus distinguishing the nobility from other social classes.\nThe Incas made human sacrifices. As many as 4,000 servants, court officials, favorites and concubines were killed upon the death of the Inca Huayna Capac in 1527. The Incas performed child sacrifices around important events, such as the death of the Sapa Inca or during a famine. These sacrifices were known as \"capacocha\" or \"qhapaq hucha\".\nThe Incas were polytheists who worshipped many gods. These included:\nAccording to Inca mythology, there were three different worlds created by Viracocha:\nEconomy.\nThe Inca Empire employed central planning. Coastal chiefdoms within the Inca Empire punctually traded with outside regions, although they did not operate a substantial internal market economy. While axe-monies were used along the northern coast, where the custom of reciprocity was not in place, presumably by the provincial \"mindal\u00e1e\" trading class, most households in the empire lived in a traditional economy in which households were required to pay tributes, usually in the form of the mit\u2019a corv\u00e9e labor, and military obligations, though barter (or \"trueque\") was present in some areas. In return, the state provided security, food in times of hardship through the supply of emergency resources, agricultural projects (e.g. aqueducts and terraces) to increase productivity and occasional feasts hosted by Inca officials for their subjects. While mit\u2019a was used by the state to obtain labor, individual villages had a pre-Inca system of communal work known as mink'a. This system survives to the modern day, known as \"mink'a\" or \"faena\". The economy rested on the material foundations of the vertical archipelago, a system of ecological complementarity in accessing resources and the cultural foundation of \"ayni\", or reciprocal exchange.\nAgriculture.\nIt was the main economic activity in the Tawantinsuyu, followed by livestock raising. It was a mixed economy with agrarian technology based on ancestral knowledge such as the \"andenes\" (terraces), (sunken fields), (raised fields), \"qucha\" (artificial lakes); and the improvement of cultivation tools, like the \"chaquitaclla\" and the . The potato was the staple food with over 200 species and 5000 different varieties while corn and coca were considered sacred plants.\nThey also built agrobiological experimentation centers such as Moray (Cuzco), Castrovirreyna (Huancavelica) and Carania (Yauyos), through circular terraces where the products of the entire empire were reproduced.\nAnimal husbandry.\nIn pre-Hispanic Andes, camelids played a crucial role in the economy. The domesticated species, llama and alpaca, were raised in large herds and used for various purposes within the Inca production system. Additionally, two wild camelid species, vicu\u00f1a and guanaco, were also utilized. Vicu\u00f1as were hunted through collective drives (chacos), sheared with tools like stones, knives, and metal axes, and then released to maintain their population. Guanacos were hunted for their highly valued meat. Chronicles indicate that all camelid meat was consumed, but due to restrictions on slaughter, its consumption was likely considered a luxury. Fresh meat was probably accessible mainly to the military or during ceremonial occasions involving widespread distribution of sacrificed animals. During the colonial period, pastures diminished or degraded due to the massive presence of introduced Spanish animals and their feeding habits, significantly altering the Andean environment.\nGovernment.\nBeliefs.\nThe Sapa Inca, the head of upper Cusco, was conceptualized as divine and was effectively head of the state religion. The \"Willaq Umu\" (or Chief Priest), the head of lower Cusco, was second to the emperor. Local religious traditions continued and in some cases such as the Oracle at Pachacamac on the coast, were officially venerated. Following Pachacuti, the Sapa Inca claimed descent from Inti, who placed a high value on imperial blood; by the end of the empire, it was common to incestuously wed brother and sister. He was \"son of the sun\", and his people the \"Intip churin\", or \"children of the sun\", and both his right to rule and mission to conquer derived from his holy ancestor. The Sapa Inca also presided over ideologically important festivals, notably during the \"Inti Raymi\" or \"Sun festival\" attended by soldiers, mummified rulers, nobles, clerics and the general population of Cusco beginning on the June solstice and culminating nine days later with the ritual breaking of the earth using a foot plow by the Inca. Moreover, Cusco was considered cosmologically central, loaded as it was with \"huacas\" and radiating \"ceque\" lines as the geographic center of the Four-Quarters; Inca Garcilaso de la Vega called it \"the navel of the universe\".\nOrganization of the empire.\nThe Inca Empire was a decentralized government consisting of a central government with the Inca at its head and four regional quarters, or \"suyu\":\nThe four corners of these quarters met at the center, Cuzco. These \"suyu\" were likely created around 1460 during the reign of Pachacuti before the empire reached its largest territorial extent. At the time the \"suyu\" were established they were roughly of equal size and only later changed their proportions as the empire expanded north and south along the Andes.\nCuzco was likely not organized as a \"wamani\" or province. Rather, it was probably somewhat akin to a modern federal district, like Washington, DC or Mexico City. The city sat at the center of the four \"suyu\" and served as the preeminent center of politics and religion. While Cusco was essentially governed by the Sapa Inca, his relatives and the royal \"panaqa\" lineages, each \"suyu\" was governed by an \"Apu\" a term of esteem used for men of high status and for venerated mountains. Both Cuzco as a district and the four \"suyu\" as administrative regions were grouped into upper \"hanan\" and lower \"hurin\" divisions. As the Inca did not have written records, it is impossible to exhaustively list the constituent \"wamani\". However, colonial records allow us to reconstruct a partial list. There were likely more than 86 \"wamani\", with more than 48 in the highlands and more than 38 on the coast.\nSuyu.\nThe most populous \"suyu\" was Chinchaysuyu, which encompassed the former Chim\u00fa Empire and much of the northern Andes. At its largest extent, it extended through much of what are now Ecuador and Colombia.\nThe largest \"suyu\" by area was Qullasuyu, named after the Aymara-speaking Qulla people. It encompassed what is now the Bolivian Altiplano and much of the southern Andes, reaching what is now Argentina and as far south as the Maipo or Maule river in modern Central Chile. Historian Jos\u00e9 Bengoa singled out Quillota as likely being the foremost Inca settlement in Chile.\nThe second smallest \"suyu\", Antisuyu, was northwest of Cusco in the high Andes. Its name is the root of the word \"Andes\".\nKuntisuyu was the smallest \"suyu\" located along the southern coast of modern Peru, extending into the highlands towards Cusco.\nLaws.\nThe Inca state had no separate judiciary or codified laws. Customs, expectations and traditional local power holders governed behavior. The state had legal force, such as through \"tukuy rikuq\" (lit.\u2009'he who sees all') or inspectors. The highest such inspector, typically a blood relative to the Sapa Inca, acted independently of the conventional hierarchy, providing a point of view for the Sapa Inca free of bureaucratic influence.\nThe Inca had three moral precepts that governed their behavior:\nAdministration.\nColonial sources are not entirely clear or in agreement about Inca government structure, such as exact duties and functions of government positions. But the basic structure can be broadly described. The top was the \"Sapa Inca\", who wore the as a symbol of power. Below that may have been the \"Willaq Umu\", literally the \"priest who recounts\", the High Priest of the Sun. However, beneath the \"Sapa Inca\" also sat the \"Inkap rantin\", who was a confidant and assistant to the \"Sapa Inca\", perhaps similar to a Prime Minister. Starting with Topa Inca Yupanqui, a \"Council of the Realm\" was composed of 16 nobles: 2 from \"hanan\" Cusco; 2 from \"hurin\" Cusco; 4 from Chinchaysuyu; 2 from Cuntisuyu; 4 from Collasuyu; and 2 from Antisuyu. This weighting of representation balanced the \"hanan\" and \"hurin\" divisions of the empire, both within Cuzco and within the Quarters (\"hanan suyu\" and \"hurin suyu\").\nWhile provincial bureaucracy and government varied greatly, the basic organization was decimal. Taxpayers \u2013 male heads of household of a certain age range \u2013 were organized into corv\u00e9e labor units (often doubling as military units) that formed the state's muscle as part of mit'a service. Each unit of more than 100 tax-payers were headed by a \"kuraka\", while smaller units were headed by a \"kamayuq\", a lower, non-hereditary status. However, while \"kuraka\" status was hereditary and typically served for life, the position of a \"kuraka\" in the hierarchy was subject to change based on the privileges of superiors in the hierarchy; a \"pachaka kuraka\" could be appointed to the position by a \"waranqa kuraka\". Furthermore, one \"kuraka\" in each decimal level could serve as the head of one of the nine groups at a lower level, so that a \"pachaka kuraka\" might also be a \"waranqa kuraka\", in effect directly responsible for one unit of 100 tax-payers and less directly responsible for nine other such units.\nCulture.\nMonumental architecture.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nWe can assure your majesty that it is so beautiful and has such fine buildings that it would even be remarkable in Spain.\n Francisco Pizarro\nArchitecture was the most important of the Inca arts, with textiles reflecting architectural motifs. The most notable example is Machu Picchu, which was constructed by Inca engineers. The prime Inca structures were made of stone blocks that fit together so well that a knife could not be fitted through the stonework. These constructs have survived for centuries, with no use of mortar to sustain them.\nThis process was first used on a large scale by the Pucara (c.\u2009300 BC\u2013AD 300) peoples to the south in Lake Titicaca and later in the city of Tiwanaku (c.\u2009AD 400\u20131100) in what is now Bolivia. The rocks were sculpted to fit together exactly by repeatedly lowering a rock onto another and carving away any sections on the lower rock where the dust was compressed. The tight fit and the concavity on the lower rocks made them extraordinarily stable, despite the ongoing challenge of earthquakes and volcanic activity.\nTunics.\nTunics were created by skilled Inca textile-makers as a piece of warm clothing, but they also symbolized cultural and political status and power. \"Cumbi\" was the fine, tapestry-woven woolen cloth that was produced and necessary for the creation of tunics. \"Cumbi\" was produced by specially-appointed women and men. Generally, textile-making was practiced by both men and women. As emphasized by certain historians, only with European conquest was it deemed that women would become the primary weavers in society, as opposed to Inca society where specialty textiles were produced by men and women equally.\nComplex patterns and designs were meant to convey information about order in Andean society as well as the Universe. Tunics could also symbolize one's relationship to ancient rulers or important ancestors. These textiles were frequently designed to represent the physical order of a society, for example, the flow of tribute within an empire. Many tunics have a \"checkerboard effect\" which is known as the \"collcapata\". According to historians Kenneth Mills, William B. Taylor, and Sandra Lauderdale Graham, the \"collcapata\" patterns \"seem to have expressed concepts of commonality, and, ultimately, unity of all ranks of people, representing a careful kind of foundation upon which the structure of Inkaic universalism was built.\" Rulers wore various tunics throughout the year, switching them out for different occasions and feasts.\nThe symbols present within the tunics suggest the importance of \"pictographic expression\" within Inca and other Andean societies far before the iconographies of the Spanish Christians.\nUncu.\nUncu was a men's garment similar to a tunic. It was an upper-body garment of knee-length; Royals wore it with a mantle cloth called \"yacolla\".\nCeramics, precious metals and textiles.\nCeramics were painted using the polychrome technique portraying numerous motifs including animals, birds, waves, felines (popular in the Chavin culture) and geometric patterns found in the Nazca style of ceramics. In a culture without a written language, ceramics portrayed the basic scenes of everyday life, including the smelting of metals, relationships and scenes of tribal warfare. The most distinctive Inca ceramic objects are the (Cuzco bottles or \"aryballos\"), mainly used for the production of chicha. Many of these pieces are on display in Lima in the Larco Archaeological Museum and the National Museum of Archaeology, Anthropology and History.\nAlmost all of the gold and silver work of the Inca empire was melted down by the conquistadors and shipped back to Spain.\nCoca.\nThe Incas revered the coca plant as sacred/magical. Its leaves were used in moderate amounts to lessen hunger and pain during work but were mostly used for religious and health purposes. The Spaniards took advantage of the effects of chewing coca leaves. The chasquis, messengers who ran throughout the empire to deliver messages, chewed coca leaves for extra energy. Coca leaves were also used as an anaesthetic during surgeries.\nBanner of the Inca.\nChronicles and references from the 16th and 17th centuries support the idea of a banner. However, it represented the Inca (emperor), not the empire.\nFrancisco L\u00f3pez de Jerez wrote in 1534:\nChronicler Bernab\u00e9 Cobo wrote:\n()-\nGuaman Poma's 1615 book, \"El primer nueva cor\u00f3nica y buen gobierno\", shows numerous line drawings of Inca flags. In his 1847 book \"A History of the Conquest of Peru\", William H. Prescott says that in the Inca army each company had its particular banner and that the imperial standard, high above all, displayed the glittering device of the rainbow, the armorial ensign of the Incas.\" A 1917 world flags book says the Inca \"heir-apparent ... was entitled to display the royal standard of the rainbow in his military campaigns.\"\nIn modern times, the rainbow flag has been wrongly associated with the Tawantinsuyu and displayed as a symbol of Inca heritage by some groups in Peru and Bolivia. The city of Cusco also flies the Rainbow Flag, but as an official flag of the city. The Peruvian president Alejandro Toledo (2001\u20132006) flew the Rainbow Flag in Lima's presidential palace. However, according to the Peruvian historiography, the Inca Empire never had a flag. Peruvian historian Mar\u00eda Rostworowski said, \"I bet my life, the Inca never had that flag, it never existed, no chronicler mentioned it\". Also, to the Peruvian newspaper \"El Comercio\", the flag dates to the first decades of the 20th century, and even the Congress of the Republic of Peru has determined that the flag is a fake by citing the conclusion of the National Academy of Peruvian History:\n\"The official use of the wrongly called 'Tawantinsuyu flag' is a mistake. In the Pre-Hispanic Andean World there did not exist the concept of a flag, it did not belong to their historic context\".\nNational Academy of Peruvian History\nMusic and Dance.\nAncient Andean inhabitants shared their experiences through singing and dancing with \"aqa\" (chicha de jora), though these practices reflected social inequalities, as some dances and songs were reserved for nobles.\nIncaic Andean music was pentatonic (using notes re, fa, sol, la, and do). They composed \"taki\" (\"songs\") with wind and percussion instruments, lacking string instruments. Key wind instruments included the quena (made of cane and bone), zampo\u00f1a, pututo or , (a five-voice whistle), and (a long flute). Percussion instruments included (a simple small drum), (a large drum with a stick), silver rattles, and (bells).\nDances were categorized as nobiliary dances for the sapa inca and the panacas, such as and , as well as for young nobles; masked men's war dances, such as ; and collective dances for laborers (), shepherds (), and the ayllu in their tasks ().\nScience and technology.\nMeasures, calendrics and mathematics.\nPhysical measures used by the Inca were based on human body parts. Units included fingers, the distance from thumb to forefinger, palms, cubits and wingspans. The most basic distance unit was \"thatkiy\" or \"thatki\" or one pace. The next largest unit was reported by Cobo to be the \"topo\" or \"tupu\", measuring 6,000 \"thatkiy\"s, or about ; careful study has shown that a range of is likely. Next was the \"wamani\", composed of 30 \"topo\"s (roughly ). To measure area, 25 by 50 wingspans were used, reckoned in \"topo\"s (roughly ). It seems likely that distance was often interpreted as one day's walk; the distance between \"tambo\" way-stations varies widely in terms of distance, but far less in terms of time to walk that distance.\nInca calendars were strongly tied to astronomy. Inca astronomers understood equinoxes, solstices and zenith passages, along with the Venus cycle. They could not, however, predict eclipses. The Inca calendar was essentially lunisolar, as two calendars were maintained in parallel, one solar and one lunar. As 12 lunar months fall 11 days short of a full 365-day solar year, those in charge of the calendar had to adjust every winter solstice. Each lunar month was marked with festivals and rituals. Apparently, the days of the week were not named and days were not grouped into weeks. Similarly, months were not grouped into seasons. Time during a day was not measured in hours or minutes, but in terms of how far the sun had travelled or in how long it had taken to perform a task.\nThe sophistication of Inca administration, calendrics and engineering required facility with numbers. Numerical information was stored in the knots of \"quipu\" strings, allowing for compact storage of large numbers. These numbers were stored in base-10 digits, the same base used by the Quechua language and in administrative and military units. These numbers, stored in \"quipu\", could be calculated on \"yupanas\", grids with squares of positionally varying mathematical values, perhaps functioning as an abacus. Calculation was facilitated by moving piles of tokens, seeds or pebbles between compartments of the \"yupana\". It is likely that Inca mathematics at least allowed division of integers into integers or fractions and multiplication of integers and fractions.\nAccording to mid-17th-century Jesuit chronicler Bernab\u00e9 Cobo, the Inca designated officials to perform accounting-related tasks. These officials were called quipo camayos. Study of khipu sample VA 42527 (Museum f\u00fcr V\u00f6lkerkunde, Berlin) revealed that the numbers arranged in calendrically significant patterns were used for agricultural purposes in the \"farm account books\" kept by the khipukamayuq (accountant or warehouse keeper) to facilitate the closing of accounting books.\nCommunication and medicine.\nThe Inca recorded information on assemblages of knotted strings, known as quipu, although they can no longer be decoded. Originally, it was thought that Quipu were used only as mnemonic devices or to record numerical data. Quipus are also believed to record history and literature.\nThe Inca made many discoveries in medicine. They performed successful skull surgery, by cutting holes in the skull to alleviate fluid buildup and inflammation caused by head wounds. Many skull surgeries performed by Inca surgeons were successful. Survival rates were 80\u201390%, compared to about 30% before Inca times. According to chronicler Bernab\u00e9 Cobo, they also had a deep knowledge of herbalism, and the Spanish soldiers trusted the hands of an indigenous surgeon more than one of the barbers who accompanied them.\nWeapons, armor and warfare.\nThe Inca army was the most powerful at that time, because any ordinary villager or farmer could be recruited as a soldier as part of the \"mit'a\" system of mandatory public service. Every able bodied male Inca of fighting age had to take part in war in some capacity at least once and to prepare for warfare again when needed. By the time the empire reached its largest size, every section of the empire contributed in setting up an army for war.\nThe Incas had no iron or steel and their weapons were not much more effective than those of their opponents so they often defeated opponents by sheer force of numbers, or else by persuading them to surrender beforehand by offering generous terms. Inca weaponry included \"hardwood spears launched using throwers, arrows, javelins, slings, the bolas, clubs, and maces with star-shaped heads made of copper or bronze\". Rolling rocks downhill onto the enemy was a common strategy, taking advantage of the hilly terrain. Fighting was sometimes accompanied by drums and trumpets made of wood, shell or bone. Armor included:\nRoads allowed quick movement (on foot) for the Inca army. Shelters called \"tambo\" and storage silos called qullqas were built one day's travelling distance from each other, so an army on campaign could be fed and rested. This can be seen in names of ruins such as Ollantaytambo or \"the storehouse of Ollantay\". These were set up so the Inca and his entourage would always have supplies (and possibly shelter) ready as they traveled.\nAdaptations to altitude.\nThe people of the Andes, including the Incas, were able to adapt to high-altitude living through successful acclimatization, which is characterized by increasing oxygen supply to the blood tissues. For the native living in the Andean highlands, this was achieved through the development of a larger lung capacity and an increase in red blood cell counts, hemoglobin concentration, and capillary beds.\nCompared to other humans, the Andeans had slower heart rates, almost one-third larger lung capacity, about 2 L (4 pints) more blood volume and double the amount of hemoglobin, which transfers oxygen from the lungs to the rest of the body. While the Conquistadors may have been taller, the Inca had the advantage of coping with the extraordinary altitude. The Tibetans in Asia living in the Himalayas are also adapted to living in high-altitudes, although the adaptation is different from that of the Andeans.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15320", "revid": "37052726", "url": "https://en.wikipedia.org/wiki?curid=15320", "title": "Inka", "text": ""}
{"id": "15321", "revid": "42330296", "url": "https://en.wikipedia.org/wiki?curid=15321", "title": "Inca (disambiguation)", "text": "The Inca Empire was the largest empire in pre-Columbian America.\nInca, Inka, or \u0130nc\u0259 may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15322", "revid": "75150", "url": "https://en.wikipedia.org/wiki?curid=15322", "title": "Incans", "text": ""}
{"id": "15323", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=15323", "title": "Internet Protocol", "text": "Communication protocol that allows connections between networks\nThe Internet Protocol (IP) is the network layer communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. Its routing function enables internetworking, and essentially establishes the Internet.\nIP has the task of delivering packets from the source host to the destination host solely based on the IP addresses in the packet headers. For this purpose, IP defines packet structures that encapsulate the data to be delivered. It also defines addressing methods that are used to label the datagram with source and destination information.\nIP was the connectionless datagram service in the original \"Transmission Control Program\" introduced by Vint Cerf and Bob Kahn in 1974, which was complemented by a connection-oriented service that became the basis for the Transmission Control Protocol (TCP). The Internet protocol suite is therefore often referred to as \"TCP/IP\".\nThe first major version of IP, Internet Protocol version 4 (IPv4), is the dominant protocol of the Internet. Its successor is Internet Protocol version 6 (IPv6), which has been in increasing deployment on the public Internet since around 2006.\nFunction.\nThe Internet Protocol is responsible for addressing host interfaces, encapsulating data into datagrams (including fragmentation and reassembly) and routing datagrams from a source host interface to a destination host interface across one or more IP networks. For these purposes, the Internet Protocol defines the format of packets and provides an addressing system.\nEach datagram has two components: a header and a payload. The IP header includes a source IP address, a destination IP address, and other metadata needed to route and deliver the datagram. The payload is the data that is transported. This method of nesting the data payload in a packet with a header is called encapsulation.\nIP addressing entails the assignment of IP addresses and associated parameters to host interfaces. The address space is divided into subnets, involving the designation of network prefixes. IP routing is performed by all hosts, as well as routers, whose main function is to transport packets across network boundaries. Routers communicate with one another via specially designed routing protocols, either interior gateway protocols or exterior gateway protocols, as needed for the topology of the network.\nAddressing methods.\nThere are four principal addressing methods in the Internet Protocol:\nVersion history.\nIn May 1974, the Institute of Electrical and Electronics Engineers (IEEE) published a paper entitled \"A Protocol for Packet Network Intercommunication\". The paper's authors, Vint Cerf and Bob Kahn, described an internetworking protocol for sharing resources using packet switching among network nodes. A central control component of this model was the Transmission Control Program that incorporated both connection-oriented links and datagram services between hosts. The monolithic Transmission Control Program was later divided into a modular architecture consisting of the Transmission Control Protocol and User Datagram Protocol at the transport layer and the Internet Protocol at the internet layer. The model became known as the \"Department of Defense (DoD) Internet Model\" and \"Internet protocol suite\", and informally as \"TCP/IP\".\nThe following Internet Experiment Note (IEN) documents describe the evolution of the Internet Protocol into the modern version of IPv4:\nIP versions 1 to 3 were experimental versions, designed between 1973 and 1978. Versions 2 and 3 supported variable-length addresses ranging between 1 and 16 octets (between 8 and 128 bits). An early draft of version 4 supported variable-length addresses of up to 256 octets (up to 2048 bits) but this was later abandoned in favor of a fixed-size 32-bit address in the final version of IPv4. This remains the dominant internetworking protocol in use in the Internet Layer; the number 4 identifies the protocol version, carried in every IP datagram. IPv4 is defined in (1981).\nVersion number 5 was used by the Internet Stream Protocol, an experimental streaming protocol that was not adopted.\nThe successor to IPv4 is IPv6. IPv6 was a result of several years of experimentation and dialog during which various protocol models were proposed, such as TP/IX (), PIP () and TUBA (TCP and UDP with Bigger Addresses, ). Its most prominent difference from version 4 is the size of the addresses. While IPv4 uses 32 bits for addressing, yielding c. 4.3 billion () addresses, IPv6 uses 128-bit addresses providing c. addresses. Although adoption of IPv6 has been slow, as of \u00a02023[ [update]], most countries in the world show significant adoption of IPv6, with over 41% of Google's traffic being carried over IPv6 connections.\nThe assignment of the new protocol as IPv6 was uncertain until due diligence assured that IPv6 had not been used previously. Other Internet Layer protocols have been assigned version numbers, such as 7 (\"IP/TX\"), 8 and 9 (\"historic\"). Notably, on April 1, 1994, the IETF published an April Fools' Day RfC about IPv9. IPv9 was also used in an alternate proposed address space expansion called TUBA. A 2004 Chinese proposal for an IPv9 protocol appears to be unrelated to all of these, and is not endorsed by the IETF.\nIP version numbers.\nAs the version number is carried in a 4-bit field, only numbers 0\u201315 can be assigned.\nReliability.\nThe design of the Internet protocol suite adheres to the end-to-end principle, a concept adapted from the CYCLADES project. Under the end-to-end principle, the network infrastructure is considered inherently unreliable at any single network element or transmission medium and is dynamic in terms of the availability of links and nodes. No central monitoring or performance measurement facility exists that tracks or maintains the state of the network. For the benefit of reducing network complexity, the intelligence in the network is located in the end nodes.\nAs a consequence of this design, the Internet Protocol only provides best-effort delivery and its service is characterized as unreliable. In network architectural parlance, it is a connectionless protocol, in contrast to connection-oriented communication. Various fault conditions may occur, such as data corruption, packet loss and duplication. Because routing is dynamic, meaning every packet is treated independently, and because the network maintains no state based on the path of prior packets, different packets may be routed to the same destination via different paths, resulting in out-of-order delivery to the receiver.\nAll fault conditions in the network must be detected and compensated for by the participating end nodes. The upper layer protocols of the Internet protocol suite are responsible for resolving reliability issues. For example, a host may buffer network data to ensure correct ordering before the data is delivered to an application.\nIPv4 provides safeguards to ensure that the header of an IP packet is error-free. A routing node discards packets that fail a header checksum test. Although the Internet Control Message Protocol (ICMP) provides notification of errors, a routing node is not required to notify either end node of errors. IPv6, by contrast, operates without header checksums, since current link layer technology is assumed to provide sufficient error detection.\nLink capacity and capability.\nThe dynamic nature of the Internet and the diversity of its components provide no guarantee that any particular path is actually capable of, or suitable for, performing the data transmission requested. One of the technical constraints is the size of data packets possible on a given link. Facilities exist to examine the maximum transmission unit (MTU) size of the local link and Path MTU Discovery can be used for the entire intended path to the destination.\nThe IPv4 internetworking layer automatically fragments a datagram into smaller units for transmission when the link MTU is exceeded. IP provides re-ordering of fragments received out of order. An IPv6 network does not perform fragmentation in network elements, but requires end hosts and higher-layer protocols to avoid exceeding the path MTU.\nThe Transmission Control Protocol (TCP) is an example of a protocol that adjusts its segment size to be smaller than the MTU. The User Datagram Protocol (UDP) and ICMP disregard MTU size, thereby forcing IP to fragment oversized datagrams.\nSecurity.\nDuring the design phase of the ARPANET and the early Internet, the security aspects and needs of a public, international network were not adequately anticipated. Consequently, many Internet protocols exhibited vulnerabilities highlighted by network attacks and later security assessments. In 2008, a thorough security assessment and proposed mitigation of problems was published. The IETF has been pursuing further studies.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15327", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=15327", "title": "I think therefore I am", "text": ""}
{"id": "15328", "revid": "48069756", "url": "https://en.wikipedia.org/wiki?curid=15328", "title": "Impeachment", "text": "Process for charging a public official with legal offenses by the legislature(s)\nImpeachment is a process by which a legislative body or other legally constituted tribunal initiates charges against a public official for misconduct. Impeachment tends to be confined to ministerial officials as the unique nature of their positions may place ministers beyond the reach of the law to prosecute, or their misconduct is not codified into law as an offense except through the unique expectations of their high office. Both \"peers and commoners\" have been subject to the process, however. \nFrom 1990 to 2020, there have been at least 272 impeachment charges against 132 different heads of state in 63 countries. In Latin America, which includes almost 40% of the world's presidential systems, ten presidents from seven countries were removed from office by their national legislatures via impeachments or declarations of incapacity between 1978 and 2019. \nMost democracies (with the notable exception of the United States) involve the courts (often a national constitutional court) in some way. National legislations differ regarding both the consequences and definition of impeachment, but the intent is nearly always to expeditiously vacate the office. Most commonly, an official is considered impeached after the commencement of the charges, and a trial of some kind is required to remove the official from office.\nImpeachment is distinct from the motion of no confidence procedure available in some countries whereby a motion of censure can be used to remove a government and its ministers from office. Such a procedure is not applicable in countries with presidential forms of government like the United States. Because impeachment involves a departure from the normal constitutional procedures by which individuals achieve high office (election, ratification, or appointment) and because it generally requires a supermajority, it is usually reserved for those deemed to have committed serious abuses of their office. In the United States, for example, impeachment at the federal level is limited to those who may have committed \"Treason, Bribery, or other high crimes and misdemeanors\". Under the United States Constitution, the House of Representatives has the sole power of impeachments while the Senate has the sole power to try impeachments (\"i.e.\", to acquit or convict); the validity of an impeachment trial is a political question that is nonjusticiable (\"i.e.\", is not reviewable by the courts).\nEtymology and history.\nThe word \"impeachment\" likely derives from Old French from Latin word expressing the idea of catching or ensnaring by the 'foot' (), and has analogues in the modern French verb (to prevent) and the modern English \"impede\". Medieval popular etymology also associated it (wrongly) with derivations from the Latin (to attack).\nThe process was first used by the English \"Good Parliament\" against William Latimer, 4th Baron Latimer in the second half of the 14th century. Following the English example, the constitutions of Virginia (1776), Massachusetts (1780) and other states thereafter adopted the impeachment mechanism, but they restricted the punishment to removal of the official from office, in contrast to the English Parliament's broad power to punish impeachments.\nIn West Africa, rulers of the Ashanti Empire who violated any oaths taken during their enstoolment were destooled by Kingmakers. Reasons include punishing citizens arbitrarily or being exposed as corrupt. The process involved Kingmakers forcibly removing the sandals of the guilty party, then bumping his buttocks on the ground three times. Once destooled, a king automatically lost sanctity and honours as he could not exercise royal powers such as being chief administrator, judge, and military commander. Also withdrawn from him were the Golden Stool (a throne functionally equivalent to crowns), swords, and other regalia. While a deposed king no longer held custodianship of the realm, he remained a member of the royal family from which he was elected.\nIn Korea, the Goryeo dynasty installed Sahundae (\u53f8\u61b2\u81fa) in 983 AD, which oversaw the impeachment of officials.\nIn various jurisdictions.\nBrazil.\nIn Brazil, as in most other Latin American countries, \"impeachment\" refers to the definitive removal from office. The president of Brazil may be provisionally removed from office by the Chamber of Deputies and then tried and definitely removed from office by the Federal Senate. The Brazilian Constitution requires that two-thirds of the Deputies vote in favor of the opening of the impeachment process of the president and that two-thirds of the senators vote for impeachment. State governors and municipal mayors can also be impeached by the respective legislative bodies. Article 2 of Law no. 1.079, from 10 April 1950, or \"The Law of Impeachment\", states that \"The crimes defined in this law, even when simply attempted, are subject to the penalty of loss of office, with disqualification for up to five years for the exercise of any public function, to be imposed by the Federal Senate in proceedings against the president of the republic, ministers of state, ministers of the Supreme Federal Tribunal, or the attorney general.\"\nInitiation: An accusation of a responsibility crime against the president may be brought by any Brazilian citizen; however, the president of the Chamber of Deputies holds prerogative to accept the charge, which if accepted will be read at the next session and reported to the president of the republic.\nExtraordinary Committee: An extraordinary committee is established, consisting of members from each political party in proportion to their party's membership. The committee is responsible for assessing the need for impeachment proceedings. The president is given ten parliamentary sessions to present their defense. Following this, two legislative sessions are held to allow for the formulation of a legal opinion by a rapporteur regarding whether or not impeachment proceedings should be initiated and brought to trial in the Senate.\nThe rapporteur's opinion is subject to a vote within the committee. If the majority accepts the rapporteur's opinion, it is deemed adopted. However, if the majority rejects the rapporteur's opinion, the committee adopts an alternative opinion proposed by the majority. For instance, if the rapporteur recommends against impeachment but fails to secure majority support, the committee will adopt the opinion to proceed with impeachment. Conversely, if the rapporteur advises impeachment but does not obtain majority approval, the committee will adopt the opinion not to impeach.\nIf the committee vote is successful, the rapporteur's opinion is considered adopted, thereby determining the course of action regarding impeachment.\nChamber of Deputies: The chamber issues a call-out vote to accept the opinion of the committee, requiring a supermajority of two thirds in favor of an impeachment opinion (or a supermajority of two thirds against a dismissal opinion) of the committee, in order to authorize the Senate impeachment proceedings. The president is suspended (provisionally removed) from office as soon as the Senate receives and accepts from the Chamber of Deputies the impeachment charges and decides to proceed with a trial.\nThe Senate: The process in the Senate had been historically lacking in procedural guidance until 1992, when the Senate published in the Official Diary of the Union the step-by-step procedure of the Senate's impeachment process, which involves the formation of another special committee and closely resembles the lower house process, with time constraints imposed on the steps taken. The committee's opinion must be presented within 10 days, after which it is put to a call-out vote at the next session. The vote must proceed within a single session; the vote on President Rousseff took over 20 hours. A simple majority vote in the Senate begins formal deliberation on the complaint, immediately suspends the president from office, installs the vice president as acting president, and begins a 20-day period for written defense as well as up to 180-days for the trial. In the event the trial proceeds slowly and exceeds 180 days, the Brazilian Constitution determines that the president is entitled to return and stay provisionally in office until the trial comes to its decision.\nSenate plenary deliberation: The committee interrogates the accused or their counsel, from which they have a right to abstain, and also a probative session which guarantees the accused rights to contradiction, or \"audiatur et altera pars,\" allowing access to the courts and due process of law under Article 5 of the constitution. The accused has 15 days to present written arguments in defense and answer to the evidence gathered, and then the committee shall issue an opinion on the merits within ten days. The entire package is published for each senator before a single plenary session issues a call-out vote, which shall proceed to trial on a simple majority and close the case otherwise.\nSenate trial: A hearing for the complainant and the accused convenes within 48 hours of notification from deliberation, from which a trial is scheduled by the president of the Supreme Court no less than ten days after the hearing. The senators sit as judges, while witnesses are interrogated and cross-examined; all questions must be presented to the president of the Supreme Court, who, as prescribed in the Constitution, presides over the trial. The president of the Supreme Court allots time for debate and rebuttal, after which time the parties leave the chamber and the senators deliberate on the indictment. The president of the Supreme Court reads the summary of the grounds, the charges, the defense and the evidence to the Senate. The senators in turn issue their judgement. On conviction by a supermajority of two thirds, the president of the Supreme Court pronounces the sentence and the accused is immediately notified. If there is no supermajority for conviction, the accused is acquitted.\nUpon conviction, the officeholder has his or her political rights revoked for eight years, which bars them from running for any office during that time.\nFernando Collor de Mello, the 32nd president of Brazil, resigned in 1992 amidst impeachment proceedings. Despite his resignation, the Senate nonetheless voted to convict him and bar him from holding any office for eight years, due to evidence of bribery and misappropriation.\nIn 2016, the Chamber of Deputies initiated an impeachment case against President Dilma Rousseff on allegations of budgetary mismanagement, a crime of responsibility under the Constitution. On 12 May 2016, after 20 hours of deliberation, the admissibility of the accusation was approved by the Senate with 55 votes in favor and 22 against (an absolute majority would have been sufficient for this step) and Vice President Michel Temer was notified to assume the duties of the president pending trial. On 31 August, 61 senators voted in favor of impeachment and 20 voted against it, thus achieving the &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2\u20443 majority needed for Rousseff's definitive removal. A vote to disqualify her for five years was taken and failed (in spite of the Constitution not separating disqualification from removal) having less than two thirds in favor.\nCroatia.\nThe process of impeaching the president of Croatia can be initiated by a two-thirds majority vote in favor in the Sabor and is thereafter referred to the Constitutional Court, which must accept such a proposal with a two-thirds majority vote in favor in order for the president to be removed from office. This has never occurred in the history of the Republic of Croatia. In case of a successful impeachment motion a president's constitutional term of five years would be terminated and an election called within 60 days of the vacancy occurring. During the period of vacancy the presidential powers and duties would be carried out by the speaker of the Croatian Parliament in his/her capacity as Acting President of the Republic.\nCzech Republic.\nIn 2013, the constitution was changed. Since 2013, the process can be started by at least three-fifths of present senators, and must be approved by at least three-fifths of all members of the Chamber of Deputies within three months. Also, the President can be impeached for high treason (newly defined in the Constitution) or any serious infringement of the Constitution.\nThe process starts in the Senate of the Czech Republic which has the right to only impeach the president. After the approval by the Chamber of Deputies, the case is passed to the Constitutional Court of the Czech Republic, which has to decide the verdict against the president. If the Court finds the President guilty, then the President is removed from office and is permanently barred from being elected President of the Czech Republic again.\nNo Czech president has ever been impeached, though members of the Senate sought to impeach President V\u00e1clav Klaus in 2013. This case was dismissed by the court, which reasoned that his mandate had expired. The Senate also proposed to impeach president Milo\u0161 Zeman in 2019 but the Chamber of Deputies did not vote on the issue in time and thus the case did not even proceed to the Court.\nDenmark.\nIn Denmark the possibility for current and former ministers being impeached was established with the Danish Constitution of 1849. Unlike many other countries Denmark does not have a Constitutional Court who would normally handle these types of cases. Instead Denmark has a special Court of Impeachment (In Danish: Rigsretten) which is called upon every time a current and former minister have been impeached. The role of the Impeachment Court is to process and deliver judgments against current and former ministers who are accused of unlawful conduct in office. The legal content of ministerial responsibility is laid down in the Ministerial Accountability Act which has its background in section 13 of the Danish Constitution, according to which the ministers' accountability is determined in more detail by law. In Denmark the normal practice in terms of impeachment cases is that it needs to be brought up in the Danish Parliament (Folketing) first for debate between the different members and parties in the parliament. After the debate the members of the Danish Parliament vote on whether a current or former minister needs to be impeached. If there is a majority in the Danish Parliament for an impeachment case against a current or former minister, an Impeachment Court is called into session. In Denmark the Impeachment Court consists of up to 15 Supreme Court judges and 15 parliament members appointed by the Danish Parliament. The members of the Impeachment Court in Denmark serve a six-year term in this position.\nIn 1995 the former Minister of Justice Erik Ninn-Hansen from the Conservative People's Party was impeached in connection with the Tamil Case. The case was centered around the illegal processing of family reunification applications. From September 1987 to January 1989 applications for family reunification of Tamil refugees from civil war-torn Sri Lanka were put on hold in violation of Danish and International law. On 22 June 1995, Ninn-Hansen was found guilty of violating paragraph five subsection one of the Danish Ministerial Responsibility Act which says: A minister is punished if he intentionally or through gross negligence neglects the duties incumbent on him under the constitution or legislation in general or according to the nature of his post. A majority of the judges in that impeachment case voted for former Minister of Justice Erik Ninn-Hansen to receive a suspended sentence of four months with one year of probation. The reason why the sentence was made suspended was especially in relation to Ninn-Hansen's personal circumstances, in particular, his health and age \u2013 Ninn-Hansen was 73 years old when the sentence was handed down. After the verdict, Ninn-Hansen complained to the European Court of Human Rights and complained, among other things, that the Court of Impeachment was not impartial. The European Court of Human Rights dismissed the complaint on 18 May 1999. As a direct result and consequence of this case, the Conservative-led government and Prime Minister at that time Poul Schl\u00fcter was forced to step down from power.\nIn February 2021 the former Minister for Immigration and Integration Inger St\u00f8jberg at that time member of the Danish Liberal Party was impeached when it was discovered that she had possibly against both Danish and International law tried to separate couples in refugee centres in Denmark, as the wives of the couples were under legal age. According to a commission report Inger St\u00f8jberg had also lied in the Danish Parliament and failed to report relevant details to the Parliamentary Ombudsman The decision to initiate an impeachment case was adopted by the Danish Parliament with a 141\u201330 vote and decision (In Denmark 90 members of the parliament need to vote for impeachment before it can be implemented). On 13 December 2021 former Minister for Immigration and Integration Inger St\u00f8jberg was convicted by the special Court of Impeachment of separating asylum seeker families illegally according to Danish and international law and sentenced to 60 days in prison. The majority of the judges in the special Court of Impeachment (25 out of 26 judges) found that it had been proven that Inger St\u00f8jberg on 10 February 2016 decided that an accommodation scheme should apply without the possibility of exceptions, so that all asylum-seeking spouses and cohabiting couples where one was a minor aged 15\u201317, had to be separated and accommodated separately in separate asylum centers. On 21 December, a majority in the Folketing voted that the sentence means that she is no longer worthy of sitting in the Folketing and she therefore immediately lost her seat.\nFrance.\nIn France the comparable procedure is called \"destitution\". The president of France can be impeached by the French Parliament for willfully violating the Constitution or the national laws. The process of impeachment is written in the 68th article of the French Constitution. Either the National Assembly or the Senate can begin the process. Then, the impeachment proposal must be transmitted to the other house, which must accept or reject the impeachment process within 15 days. After the upper and lower houses' agreement, they unite in joint session to form the High Court.\nThe High Court must decide whether or not to declare the removal from office of the president. The impeachment procedure in front of the National Assembly and the Senate, as well as the removal from office by the High Court require a majority of two thirds of the members of the House involved or of the High Court by secret ballot; no proxy voting is allowed.\nGermany.\nThe federal president of Germany can be impeached both by the Bundestag and by the Bundesrat for willfully violating federal law. Once the Bundestag or the Bundesrat impeaches the president, the Federal Constitutional Court decides whether the President is guilty as charged and, if this is the case, whether to remove him or her from office. The Federal Constitutional Court also has the power to remove federal judges from office for willfully violating core principles of the federal constitution or a state constitution. The impeachment procedure is regulated in Article 61 of the Basic Law for the Federal Republic of Germany.\nThere is no formal impeachment process for the chancellor of Germany; however, the Bundestag can replace the chancellor at any time by voting for a new chancellor (constructive vote of no confidence, Article 67 of the Basic Law).\nThere has never been an impeachment against the President so far. Constructive votes of no confidence against the chancellor occurred in 1972 and 1982, with only the second one being successful.\nHong Kong.\nThe chief executive of Hong Kong can be impeached by the Legislative Council. A motion for investigation, initiated jointly by at least one-fourth of all the legislators charging the Chief Executive with \"serious breach of law or dereliction of duty\" and refusing to resign, shall first be passed by the council. An independent investigation committee, chaired by the chief justice of the Court of Final Appeal, will then carry out the investigation and report back to the council. If the Council find the evidence sufficient to substantiate the charges, it may pass a motion of impeachment by a two-thirds majority.\nHowever, the Legislative Council does not have the power to actually remove the chief executive from office, as the chief executive is appointed by the Central People's Government (State Council of China). The council can only report the result to the Central People's Government for its decision.\nHungary.\nArticle 13 of Hungary's Fundamental Law (constitution) provides for the process of impeaching and removing the president. The president enjoys immunity from criminal prosecution while in office, but may be charged with crimes committed during his term afterwards. Should the president violate the constitution while discharging his duties or commit a willful criminal offense, he may be removed from office. Removal proceedings may be proposed by the concurring recommendation of one-fifth of the 199 members of the country's unicameral Parliament. Parliament votes on the proposal by secret ballot, and if two thirds of all representatives agree, the president is impeached. Once impeached, the president's powers are suspended, and the Constitutional Court decides whether or not the President should be removed from office.\nIndia.\nThe president and judges, including the chief justice of the supreme court and high courts, can be impeached by the parliament before the expiry of the term for violation of the Constitution. Other than impeachment, no other penalty can be given to a president in position for the violation of the Constitution under of the constitution. However, a president after his/her removal can be punished for her/his already proven unlawful activity under disrespecting the constitution, etc. No president has faced impeachment proceedings. Hence, the provisions for impeachment have never been tested. The sitting president cannot be charged and needs to step down in order for that to happen.\nIreland.\nThe Constitution of Ireland states the President of Ireland may be impeached for \"stated misbehaviour\". The president may not be otherwise removed from office or made answerable for his actions, although if five judges of the Supreme Court rule that he has become \"permanently incapacitated\" then a new presidential election must be held within 60 days and the Presidential Commission will deputise in the interim. Impeachment is controlled by the Oireachtas (parliament) with one house (D\u00e1il or Seanad) preferring a charge and the other directing the ensuing investigation and final vote. The charge requires a motion signed by at least thirty members and consequent resolution supported by at least two-thirds of the total membership. The investigation may be made by the house itself or delegated to another \"court, tribunal or body\". The president is removed from office only if at least two-thirds of the total membership of the investigating house support an ensuing resolution that, not only has the charge been sustained, but also the misbehaviour was serious enough to render the president \"unfit to continue in office\".\nAs of \u00a02024[ [update]] no impeachment of a president has ever taken place. The dignity of what is a largely ceremonial office is considered important, so it is likely that a president would resign from office long before undergoing formal conviction or impeachment. In 1976, after being criticised by a minister, Cearbhall \u00d3 D\u00e1laigh resigned \"to protect the dignity and independence of the presidency as an institution\", although there was no question of impeachment.\nWhile the Constitution also states that Comptroller and Auditor General and justices of the superior courts can be removed from office for \"stated misbehaviour\", it does not describe this as \"impeachment\" and the requirement in each case is simple resolution by each house of the Oireachtas. The process is nevertheless informally called \"impeachment\".\nItaly.\nIn Italy, according to Article 90 of the Constitution, the President of Italy can be impeached through a majority vote of the Parliament in joint session for high treason and for attempting to overthrow the Constitution. If impeached, the president of the Republic is then tried by the Constitutional Court integrated with sixteen citizens older than forty chosen by lot from a list compiled by the Parliament every nine years.\nItalian press and political forces made use of the term \"impeachment\" for the attempt by some members of parliamentary opposition to initiate the procedure provided for in Article 90 against Presidents Francesco Cossiga (1991), Giorgio Napolitano (2014) and Sergio Mattarella (2018).\nJapan.\nBy Article 78 of the Constitution of Japan, judges can be impeached. The voting method is specified by laws. The National Diet has two organs, namely () and (), which is established by Article 64 of the Constitution. The former has a role similar to prosecutor and the latter is analogous to Court. Seven judges were removed by them.\nLiechtenstein.\nMembers of the Liechtenstein Government can be impeached before the State Court for breaches of the Constitution or of other laws. As a hereditary monarchy the Sovereign Prince cannot be impeached as he \"is not subject to the jurisdiction of the courts and does not have legal responsibility\". The same is true of any member of the Princely House who exercises the function of head of state should the Prince be temporarily prevented or in preparation for the Succession.\nLithuania.\nIn the Republic of Lithuania, the president may be impeached by a three-fifths majority in the Seimas. President Rolandas Paksas was removed from office by impeachment on 6 April 2004 after the Constitutional Court of Lithuania found him guilty of having violated his oath and the constitution. He was the first European head of state to have been impeached.\nNorway.\nMembers of government, representatives of the national assembly (Stortinget) and Supreme Court judges can be impeached for criminal offenses tied to their duties and committed in office, according to the Constitution of 1814, \u00a7\u00a7 86 and 87. The procedural rules were modeled after the U.S. rules and are quite similar to them. Impeachment has been used eight times since 1814, last in 1927. Many argue that impeachment has fallen into desuetude. In cases of impeachment, an appointed court (Riksrett) takes effect.\nPeru.\nThe first impeachment process against Pedro Pablo Kuczynski, then the incumbent President of Peru since 2016, was initiated by the Congress of Peru on 15 December 2017. According to Luis Galarreta, the President of the Congress, the whole process of impeachment could have taken as little as a week to complete. This event was part of the second stage of the political crisis generated by the confrontation between the Government of Pedro Pablo Kuczynski and the Congress, in which the opposition Popular Force has an absolute majority. The impeachment request was rejected by the congress on 21 December 2017, for failing to obtain sufficient votes for the deposition.\nSince Kuczynski's failed impeachment, there have been three successful impeachments between 2020 and 2025, all of which have resulted in the removal of Mart\u00edn Vizcarra, Pedro Castillo, and Dina Boluarte from office.\nPhilippines.\nImpeachment in the Philippines follows procedures similar to the United States. Under Sections2 and 3, Article XI, Constitution of the Philippines, the House of Representatives of the Philippines has the exclusive power to initiate all cases of impeachment against the president, vice president, members of the Supreme Court, members of the Constitutional Commissions (Commission on Elections, Civil Service Commission and the Commission on Audit), and the ombudsman. When a third of its membership has endorsed article(s) of impeachment, it is then transmitted to the Senate of the Philippines which tries and decide, as impeachment tribunal, the impeachment case.\nA main difference from U.S. proceedings, however, is that only one third of House members are required to approve the motion to impeach the president (as opposed to a simple majority of those present and voting in their U.S. counterpart). In the Senate, selected members of the House of Representatives act as the prosecutors and the senators act as judges with the Senate president presiding over the proceedings (the chief justice jointly presides with the Senate president if the president is on trial). Like the United States, to convict the official in question requires that a minimum of two thirds (i.e. 16 of 24 members) of all the members of the Senate vote in favor of conviction. If an impeachment attempt is unsuccessful or the official is acquitted, no new cases can be filed against that impeachable official for at least one full year.\nPoland.\nIn Poland, referral to the State Tribunal is used instead of the process of impeachment, which is traditionally used in some other nations as a way of addressing similar allegations against persons holding analogous offices.\nRomania.\nThe president can be impeached by Parliament and is then suspended. A referendum then follows to determine whether the suspended president should be removed from office. President Traian B\u0103sescu was impeached twice by the Parliament: in 2007 and then again in July 2012. A referendum was held on 19 May 2007 and a large majority of the electorate voted against removing the president from office. For the most recent suspension a referendum was held on 29 July 2012; voters overwhelmingly approved B\u0103sescu's impeachment, but the referendum was invalidated due to low turnout.\nRussia.\nIn 1999, members of the State Duma of Russia, led by the Communist Party of the Russian Federation, unsuccessfully attempted to impeach President Boris Yeltsin on charges relating to his role in the 1993 Russian constitutional crisis and launching the First Chechen War (1995\u201396); efforts to launch impeachment proceedings failed.\nSingapore.\nThe Constitution of Singapore allows the impeachment of a sitting president on charges of treason, violation of the Constitution, corruption, or attempting to mislead the Presidential Elections Committee for the purpose of demonstrating eligibility to be elected as president. The prime minister or at least one-quarter of all members of Parliament (MPs) can pass an impeachment motion, which can succeed only if at least half of all MPs (excluding nominated members) vote in favor, whereupon the chief justice of the Supreme Court will appoint a tribunal to investigate allegations against the president. If the tribunal finds the president guilty, or otherwise declares that the president is \"permanently incapable of discharging the functions of his office by reason of mental or physical infirmity\", Parliament will hold a vote on a resolution to remove the president from office, which requires a three-quarters majority to succeed. No president has ever been removed from office in this fashion.\nSouth Africa.\nWhen the Union of South Africa was established in 1910, the only officials who could be impeached (though the term itself was not used) were the chief justice and judges of the Supreme Court of South Africa. The scope was broadened when the country became a republic in 1961, to include the state president. It was further broadened in 1981 to include the new office of vice state president; and in 1994 to include the executive deputy presidents, the public protector and the Auditor-General. Since 1997, members of certain commissions established by the Constitution can also be impeached. The grounds for impeachment, and the procedures to be followed, have changed several times over the years.\nSouth Korea.\nAccording to the Article 65(1) of Constitution of South Korea, the President, the Prime Minister, members of the State Council, heads of Executive Ministries, Justices of the Constitutional Court, judges, members of the National Election Commission, the chairperson and members of the Board of Audit and Inspection can be impeached by the National Assembly if they violate the Constitution or other statutory duties. By article 65(2) of the Constitution, proposal of an impeachment motion requires at least a third of the members, while passage needs an absolute majority of votes among the entire membership of the National Assembly. However, exceptionally, the impeachment of a president needs an absolute majority to propose and a two-thirds supermajority of votes to pass among the entire membership of the National Assembly. When the impeachment proposal is passed in the National Assembly, it is reviewed by the Constitutional Court of Korea, according to article 111(1) of the Constitution. During this process, the impeached officeholder is suspended from exercising power by article 65(3) of the Constitution.\nSince the establishment of the Republic of Korea in 1948, the National Assembly has impeached three presidents: Roh Moo-hyun in 2004, Park Geun-hye in 2016, and Yoon Suk Yeol in 2024 following his declaration of martial law. Although Roh's impeachment was rejected by the Constitutional Court, Park and Yoon were later removed from office by the final Constitutional Court ruling.\nIn February 2021, Judge Lim Seong-geun of the Busan High Court was impeached by the National Assembly for meddling in politically sensitive trials, the first ever impeachment of a judge in Korean history. Unlike presidential impeachments, only a simple majority is required to impeach. Judge Lim's term expired before the Constitutional Court could render a verdict, leading the court to dismiss the case without ruling on it merits.\nTurkey.\nIn Turkey, according to the Constitution, the Grand National Assembly may initiate an investigation of the president, the vice president or any member of the Cabinet upon the proposal of simple majority of its total members, and within a period less than a month, the approval of three-fifths of the total members. The investigation would be carried out by a commission of fifteen members of the Assembly, each nominated by the political parties in proportion to their representation therein. The commission would submit its report indicating the outcome of the investigation to the speaker within two months. If the investigation is not completed within this period, the commission's time may be renewed for another month. Within ten days of its submission to the speaker, the report would be distributed to all members of the Assembly, and ten days after its distribution, the report would be discussed on the floor. Upon the approval of two thirds of the total number of the Assembly by secret vote, the person or persons, about whom the investigation was conducted, may be tried before the Constitutional Court. The trial would be finalized within three months, and if not, a one-time additional period of three months shall be granted.\nThe president, about whom an investigation has been initiated, may not call for an election. The president, who is convicted by the Court, would be removed from office.\nThe provision of this article shall also apply to the offenses for which the president allegedly worked during his term of office.\nUnited Kingdom.\nIn the United Kingdom, in principle, anybody may be prosecuted and tried by the two Houses of Parliament for any crime. The first recorded impeachment is that of William Latimer, 4th Baron Latimer during the Good Parliament of 1376. The latest was that of Henry Dundas, 1st Viscount Melville which started in 1805 and which ended with his acquittal in June 1806. Over the centuries, the procedure has been supplemented by other forms of oversight including select committees, confidence motions, and judicial review, while the privilege of peers to trial only in the House of Lords was abolished in 1948 (see ), and thus impeachment, which has not kept up with modern norms of democracy or procedural fairness, is generally considered obsolete.\nUnited States.\nIn the federal system, Article One of the United States Constitution provides that the House of Representatives has the \"sole Power of Impeachment\" and the Senate has \"the sole Power to try all Impeachments\". Article Two provides that \"The President, Vice President and all civil Officers of the United States, shall be removed from Office on Impeachment for, and Conviction of, Treason, Bribery, or other high Crimes and Misdemeanors.\" In the United States, impeachment is the first of two stages; an official may be impeached by a majority vote of the House, but conviction and removal from office in the Senate requires \"the concurrence of two thirds of the members present\". Impeachment is analogous to an indictment.\nAccording to the House practice manual, \"Impeachment is a constitutional remedy to address serious offenses against the system of government. It is the first step in a remedial process\u00a0\u2013 that of removal from public office and possible disqualification from holding further office. The purpose of impeachment is not punishment; rather, its function is primarily to maintain constitutional government.\" Impeachment may be understood as a unique process involving both political and legal elements. The Constitution provides that \"Judgment in Cases of Impeachment shall not extend further than to removal from Office, and disqualification to hold and enjoy any Office of honor, Trust or Profit under the United States: but the Party convicted shall nevertheless be liable and subject to Indictment, Trial, Judgment and Punishment, according to Law.\" It was generally accepted that \"a former President may be prosecuted for crimes of which he was acquitted by the Senate,\" though that standard has been challenged in a recent case which held that the president has immunity for all official acts.\nAs of 2025, the U.S. House of Representatives had impeached an official 22 times since 1789: four times for presidents, fifteen times for federal judges, twice for a Cabinet secretary, and once for a US senator. Of the 22, the Senate voted to remove 8 officials impeached by the House of Representatives (all federal judges) from office. The four impeachment trials in the Senate of presidents were: Andrew Johnson in 1868, Bill Clinton in 1998, and Donald Trump in 2019 and again in 2021. All four impeachments were followed by acquittal in the Senate. An impeachment process was also commenced against Richard Nixon, but he resigned in 1974 to avoid an impeachment vote.\nAlmost all state constitutions set forth parallel impeachment procedures for state governments, allowing the state legislature to impeach officials of the state government. From 1789 through 2008, 14 governors have been impeached (including two who were impeached twice), of whom seven governors were convicted.\nOther.\nArmenia.\nThe process for removing a prime minister of Armenia by a vote of non-confidence (outlined in Article 115 of the Constitution of the Republic of Armenia) is often referred to as \"impeachment\", despite this term not being used by the constitution itself.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15334", "revid": "50052602", "url": "https://en.wikipedia.org/wiki?curid=15334", "title": "Ibizan Hound", "text": "The Ibizan Hound (, ) is a lean, agile dog of the hound family. There are two hair types of the breed: smooth and wire. The more commonly seen type is the smooth.\nAppearance.\nThe Ibizan Hound is an elegant and agile breed, with an athletic and attractive outline and a ground-covering springy trot. Though graceful in appearance, it has good bone girth and is a rugged/hardy breed. Its large upright ears \u2014 a hallmark of the breed \u2014 are broad at the base and frame a long and elegant headpiece. The neck is long and lean. It has a unique front assembly with well laid-back shoulders and relatively straight upper arm. Coming in both smooth and wire-coated varieties, their coat is a combination of red and white with the nose, ears, eye rims, and pads of feet being a light tan color. Its eyes are a striking amber color and have an alert and intelligent expression. The Ibizan may range in height, from 23 to 27 inches at the withers for males while females are 22 to 26 inches at the withers. The average weight of males is 50 lbs (22.6 kg) while the average weight of females is 45 lbs (20.4 kg).\nTemperament.\nIbizan Hounds are intelligent, active, and engaging by nature. They rank 53rd in Stanley Coren's book \"The Intelligence of Dogs\", considered average working/obedience intelligence, but many Ibizan owners enjoy recounting a multitude of examples of their problem-solving abilities. They are true \"clowns\" of the dog world, delighting in entertaining their people with their antics. Though somewhat independent and stubborn at times, they do take well to training if positive methods are used, but they will balk at punitive training methods. They are generally quiet but will alarm bark if necessary, so they make good watch dogs. They are sensitive hounds, and very good around children and other dogs alike. They generally make good house dogs but are active and athletic, therefore need a lot of daily exercise. They do not make good kennel dogs. Ibizan hounds are sweet, but they are very stubborn and independent.\nIbizan Hounds are \"escapologists\": they are able to jump incredible heights from a standstill, so they need very tall fences. They also have been known to climb, and many can escape from crates and can open baby gates and even locks. They have a strong prey drive, therefore they cannot be trusted off leash unless in a safely enclosed area. Once off the leash, they might not come back for a long time. A hound that knows where its home is and the surrounding area will usually return unscathed.\nHealth.\nThe Ibizan Hound is typical of the hound group in that it rarely suffers from hereditary illness. Minor health concerns for the breed include seizures and allergies; very rarely, one will see axonal dystrophy, cataract, retinal dysplasia and deafness in the breed. Ibizan Hound owners should have their dogs' eyes tested by a veterinarian before breeding. CERF and BAER testing is recommended for the breed. Ibizan Hounds are sensitive to barbiturate anesthesia, and typically live between 12 and 14 years.\nHistory.\nDNA analysis indicates that the breed was formed recently from other breeds.\nThe Ibizan Hound is similar in function and type to several breeds, such as the Pharaoh Hound, the Cirneco dell'Etna, the Portuguese Podengo, and the Podenco Canario. The Ibizan Hound is the largest of these breeds, classified by the F\u00e9d\u00e9ration Cynologique Internationale as primitive types.\nUse.\nThis breed originates in the island of Ibiza and has been traditionally used in the Catalan-speaking areas of Spain, and France where it was known under the name of \"le charnigue\", to hunt rabbits and other small game. The Ibizan Hound is a fast dog that can hunt on all types of terrain, working by scent, sound and sight. Hunters run these dogs in mostly female packs, with perhaps a male or two, as the female is considered the better hunter.\nTraditionally a farmer may have one dog and a very well off farmer two dogs to catch rabbits for food. However, in the last twenty years it is seen as a sport where between five and fifteen dogs can be seen in the chase of one rabbit.\nThe Ibizan Hound authority Miquel Rossell\u00f3 has provided a detailed description of a working trial which characterises their typical hunting technique and action, strikingly illustrated with action photos by Charles Camberoque which demonstrate hunt behaviour and typical hunt terrain. \nWhile local hunters will at times use one dog or a brace, and frequently packs of six to eight or as many as fifteen, the working trial requires an evaluation of one or two braces. A brace is called a \"colla\". The couples should be tested on at least two to five rabbits (not hares), without the use of any other hunting aid. An inspection and evaluation of the exterior, fitness, character and obedience of the dogs is recommended prior to the hunt. \nThe trial is qualified as having 5 parts. The dogs should show: (1) careful tracking and scenting of the rabbit, without being distracted in the least, 0-30 points; (2) correct signalling of the game, patient stand, strong jump into the air, obedience 0-10 points; (3) chase, giving tongue, speed, sureness, anticipation 0-30 points; (4) putting the game to cover at close quarters, listening, waiting, obedience, correct attack 0-10 point; and (5) good catch, or correct indication of the game's location, retrieval, obedience 0-20 points.\nIndividual dogs are expected to show a great degree of discipline, obedience and co-operation. They should be extremely agile, have good speed and a powerful vertical jump from a stationary position in rough and often heavily covered ground. They should have excellent scent-tracking abilities, give tongue at the right time when approaching the game closely, and otherwise be silent so that they can locate the game by sound.\nIn the United States, the Ibizan Hound is frequently competed in lure coursing through the AKC and ASFA, and also competes in LGRA straight racing and NOTRA oval track racing. Some parts of the country also use them for coursing live prey, generally jackrabbits.\nThe Ibizan Hound breed is recognized by the F\u00e9d\u00e9ration Cynologique Internationale, Continental Kennel Club, American Kennel Club, United Kennel Club, Kennel Club of Great Britain, Canadian Kennel Club, National Kennel Club, New Zealand Kennel Club, Australian National Kennel Council, America's Pet Registry, and American Canine Registry. It was fully recognized by the American Kennel Club in 1979.\nIn folk culture.\nAccording to journalist Norman Lewis, when an owner no longer wants to own one of these dogs (having too much of an appetite, for instance), it is considered very bad luck to kill the dog. Instead, they release the dog on the other side of the island, so that someone else might 'adopt' the animal.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15335", "revid": "318484", "url": "https://en.wikipedia.org/wiki?curid=15335", "title": "Irish Wolfhound", "text": "The Irish Wolfhound (Irish: \"C\u00fa Faoil\") is a breed of large sighthound that has, by its presence and substantial size, inspired literature, poetry and mythology. One of the largest of all breeds of dog, the breed is used by coursing hunters who have prized it for its ability to dispatch game caught by other, swifter sighthounds. In 1902, the Irish Wolfhound was declared the regimental mascot of the Irish Guards.\nHistory.\nPre-19th century.\nIn 391, there is a reference to large dogs by Quintus Aurelius Symmachus, a Roman consul who got seven \"canes Scotici\" as a gift to be used for fighting lions and bears, and who wrote \"all Rome viewed (them) with wonder\". Scoti is a Latin name for the Gaels (ancient Irish). Dansey, the early 19th century translator of the first complete version of Arrian's work in English, \"On Coursing\", suggested the Irish and Scottish \"greyhounds\" were derived from the same ancestor, the \"vertragus\", and had expanded with the Scoti from Ireland across the Western Isles and into what is today Scotland.\nWolfhounds were used as hunting dogs by the Gaels, who called them \"C\u00fa Faoil\" ( , composed of the elements \"hound\" and \"wolf\", i.e. \"wolfhound\"). Dogs are mentioned as \"c\u00fa\" in Irish laws and literature dating from the sixth century or, in the case of the Sagas, from the old Irish period, AD 600\u2013900. C\u00fa Chulainn, a mythical warrior whose name means \"hound of Culann\", is supposed to have gained this name as a child when he slew the ferocious guard dog of Culann. As recompense he offered himself as a replacement.\nIn discussing the systematic evidence of historic dog sizes in Ireland, the Irish zooarchaeologist Finbar McCormick stressed that no dogs of Irish Wolfhound size are known from sites of the Iron Age period of 1000 BC through to the early Christian period to 1200 AD. On the basis of the historic dog bones available, dogs of current Irish Wolfhound size seem to be a relatively modern development: \"it must be concluded that the dog of C\u00fa Chulainn was no larger than an Alsatian and not the calf-sized beast of the popular imagination\".\nIn his \"Historie of Ireland\", written in 1571, Edmund Campion gives a description of the hounds used for hunting wolves in the Dublin and Wicklow mountains. He says: \"They (the Irish) are not without wolves and greyhounds to hunt them, bigger of bone and limb than a colt\". Due to their popularity overseas many were exported to European royal houses leaving numbers in Ireland depleted. This led to a declaration by Oliver Cromwell being published in Kilkenny on 27 April 1652 to ensure that sufficient numbers remained to control the wolf population.\nReferences to the Irish Wolfhound in the 18th century tell of its great size, strength and greyhound shape as well as its scarcity. Writing in 1790, Thomas Bewick described it as the largest and most beautiful of the dog kind; about 36\u00a0inches high, generally of a white or cinnamon colour, somewhat like the Greyhound but more robust. He said that their aspect was mild, disposition peaceful, and strength so great that in combat the Mastiff or Bulldog was far from being an equal to them.\nThe last wolf in Ireland was killed in County Carlow in 1786. It is thought to have been killed at Myshall, on the slopes of Mount Leinster, by a pack of wolfdogs kept by a Mr Watson of Ballydarton. The wolfhounds that remained in the hands of a few families, who were mainly descendants of the old Irish chieftains, were now symbols of status rather than used as hunters, and these were said to be the last of their race.\nThomas Pennant (1726\u20131798) reported that he could find no more than three wolfdogs when he visited Ireland. At the 1836 meeting of the Geological Society of Dublin, John Scouler presented a paper titled \"Notices of Animals which have disappeared from Ireland\", including mention of the wolfdog.\nModern wolfhound.\nCaptain George Augustus Graham (1833\u20131909), of Rednock House, Dursley, Gloucestershire, was responsible for reviving the Irish wolfhound breed. He stated that he could not find the breed \"in its original integrity\" to work with:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;That we are in possession of the breed in its original integrity is not pretended; at the same time it is confidently believed that there are strains now existing that tracing back, more or less clearly, to the original breed; and it appears to be tolerably certain that our Deerhound is descended from that noble animal, and gives us a fair idea of what he was, though undoubtedly considerably his inferior in size and power.\u2014\u200a\nIn Ireland, Graham acquired \"Faust\" of Kilfane and \"Old Donagh\" of Ballytobin, County Kilkenny; these were the respective progenitors of Graham's breeding program and said to descend from original Irish wolfhound strains. Based on the writings of others, he had concluded that the Scottish Deerhound and Great Dane were derived earlier from the wolfhound. As a result, said breeds were heavily emphasized in his breeding program. For an outbreed, a Borzoi and \"Tibetan wolfdog\" may also have been included. It has been suggested that the latter was a Tibetan Kyi Apso.\nIn 1885, Captain Graham founded the Irish Wolfhound Club, and the Breed Standard of Points to establish and agree the ideal to which breeders should aspire. In 1902, the Irish Wolfhound was declared the regimental mascot of the Irish Guards.\nDNA analysis.\nGenomic analysis indicates that although there has been some DNA sharing between the Irish wolfhound with the Deerhound, Whippet, and Greyhound, there has been significant sharing of DNA between the Irish Wolfhound and the Great Dane. One writer has stated that for the Irish Wolfhound, \"the Great Dane appearance is strongly marked too prominently before the 20th Century\". George Augustus Graham created the modern Irish wolfhound breed by retaining the appearance of the original form, but not its genetic ancestry.\nCharacteristics.\nThe Irish Wolfhound is characterised by its large size. According to the FCI standard, the expected range of heights at the withers is ; minimum heights and weights are / and / for dogs and bitches respectively. It is more massively built than the Scottish Deerhound, but less so than the Great Dane.\nThe coat is hard and rough on the head, body and legs, with the beard and the hair over the eyes particularly wiry. It may be black, brindle, fawn, grey, red, pure white, or any colour seen in the Deerhound.\nThe Irish Wolfhound is a sighthound, and hunts by visual perception alone. The neck is muscular and fairly long, and the head is carried high. It should appear to be longer than it is tall,\nand to be capable of catching and killing a wolf.\nTemperament.\nIrish Wolfhounds have a varied range of personalities and are most often noted for their personal quirks and individualism. An Irish Wolfhound, however, is rarely mindless, and, despite its large size, is rarely found to be destructive in the house or boisterous. This is because the breed is generally introverted, intelligent, and reserved in character. An easygoing animal, the Irish Wolfhound is quiet by nature. Wolfhounds often create a strong bond with their family and can become quite destructive or morose if left alone for long periods of time.\nThe Irish Wolfhound makes for an effective and imposing guardian. The breed becomes attached to both owners and other dogs they are raised with and is therefore not the most adaptable of breeds. Bred for independence, an Irish Wolfhound is not necessarily keen on defending spaces. A wolfhound is most easily described by its historical motto, \"gentle when stroked, fierce when provoked\".\nIrish Wolfhounds are often favored for their loyalty, affection, patience, and devotion. Although at some points in history they have been used as watchdogs, unlike some breeds, the Irish Wolfhound is usually unreliable in this role as they are often friendly toward strangers, although their size can be a natural deterrent. However, when protection is required this dog is never found wanting. When they or their family are in any perceived danger they display a fearless nature. Author and Irish Wolfhound breeder Linda Glover believes the dogs' close affinity with humans makes them acutely aware and sensitive to ill will or malicious intentions leading to their excelling as a guardian rather than guard dog.\nHealth.\nLike many large dog breeds, Irish Wolfhounds have a relatively short lifespan. Published lifespan estimations vary between 4.95 and 8.75 years. More recently a 2024 UK study found a life expectancy of 9.9 years for the breed compared to an average of 12.7 for purebreeds and 12 for crossbreeds. A 2005 Swedish study of insurance data found 91% of Irish Wolfhounds died by the age of 10, higher than the overall rate of 35% of dogs dying by the age of 10.\nThe most frequently reported diseases are dilated cardiomyopathy, bone cancer, gastric dilatation volvulus, and osteochondrosis. Different studies have reported a rate of dilated cardiomyopathy in the breed between 12.1% and 44.7% of Irish Wolfhounds. In a study that compared multiple breeds the Irish Wolfhound had a 3.4 odds ratio of the condition compared to overall. The condition is likely hereditary in the breed.\nOne study found the Irish Wolfhound to be 27.5 times more likely to contract osteogenic sarcoma than the overall dog population.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15336", "revid": "46807820", "url": "https://en.wikipedia.org/wiki?curid=15336", "title": "Italian Greyhound", "text": "Italian breed of sighthound\nThe Italian Greyhound or Italian Sighthound () is an Italian breed of small sighthound. It was bred to hunt hare and rabbit, but is kept mostly as a companion dog.\nHistory.\nSmall dogs of sighthound type have long been popular with nobility and royalty. Among those believed to have kept them are Frederick II, Duke of Swabia; members of the D'Este, Medici and Visconti families; the French kings Louis XI, Charles VIII, Charles IX, Louis XIII and Louis XIV; Frederick the Great of Prussia;519 Anne of Denmark; Catherine the Great; and Queen Victoria. Dogs of this type have often been represented in sculpture \u2013 including a second-century Roman statue now in the Vatican Museums \u2013 and paintings, notably by Giotto, Sassetta and Tiepolo.\nDogs of this kind were taken in the first half of the nineteenth century to the United Kingdom, where they were known as Italian Greyhounds;44 the first volume of \"The Kennel Club Calendar and Stud Book\", published in 1874, lists forty of them.597 A breed association, the Italian Greyhound Club, was established in Britain in 1900.157 Registrations by the American Kennel Club began in 1886.\nThe history of the modern Piccolo Levriero goes back to the last years of the nineteenth century. A total of six of the dogs were shown in 1901 in Milan and Novara, two in Turin in 1902, and one in Udine in 1903. Numbers began to increase only after the First World War, partly as a result of the work of two individual breeders, Emilio Cavallini and Giulia Aj\u00f2 Montecuccoli degli Erri. In this post-War period the Piccolo Levriero was bred principally in Italy, France and Germany, and some Italian breeders imported dogs from outside the country. Of the forty-five of the dogs registered in 1926\u20131927 by the Kennel Club Italiano (as it was then known), twenty-eight were born in Italy and seventeen were imported.\nThe events of the Second World War brought the Piccolo Levriero close to extinction, and numbers began to recover only in the 1950s, particularly after 1951, when Maria Luisa Incontri Lotteringhi della Stufa brought the influential bitch Komtesse von Gastuna from Austria. The breed was definitively accepted by the F\u00e9d\u00e9ration Cynologique Internationale in October 1956, and in November of that year a breed society, the , was formed under the auspices of the Ente Nazionale della Cinofilia Italiana; it was later renamed the .\nIn the nine years from 2011 to 2019, the Ente Nazionale della Cinofilia Italiana recorded a total of 2557 new registrations of the Piccolo Levriero, with a minimum of 213 and a maximum of 333 per year.\nCharacteristics.\nThe Italian Greyhound is the smallest of the sighthounds. It weighs no more than and stands at the withers. It is deep in the chest, with a tucked-up abdomen, long slender legs and a long neck. The head is small, elongated and narrow. The gait should be high-stepping and well-sprung, with good forward extension in the trot, and a fast gallop. The coat may be solid black, or grey or isabelline in any shade; white markings are accepted on the chest and feet only.\nMedian longevity is about 14 years, compared to an average of 12.5 for all dogs.127127\nThe dogs may be affected by breed-related neurological abnormalities including congenital deafness and cervical intervertebral disc disease.291 In the United States, the Ortheopedic Foundation for Animals has found the Italian Greyhound to be the least affected by hip dysplasia of 157 breeds studied, with an incidence of 0.\nUse.\nThe original function of the Piccolo Levriero was to hunt hare and rabbit; it is capable of bursts of speed up to . Although assigned to the sighthound or hare-coursing groups by the F\u00e9d\u00e9ration Cynologique Internationale and the Ente Nazionale della Cinofilia Italiana, the Italian Sighthound is \u2013 as it was in the past \u2013 kept mostly as a companion dog. It is classified as a toy breed by the American Kennel Club and the Kennel Club of the United Kingdom.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15337", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=15337", "title": "ILink", "text": ""}
{"id": "15339", "revid": "27158663", "url": "https://en.wikipedia.org/wiki?curid=15339", "title": "IT", "text": ""}
