{"id": "14914", "revid": "50468286", "url": "https://en.wikipedia.org/wiki?curid=14914", "title": "Industrial Revolution", "text": "1760\u20131840 agrarian to industrial era shift\nThe Industrial Revolution, sometimes divided into the First Industrial Revolution and Second Industrial Revolution, was a transitional period of the global economy toward more widespread, efficient and stable manufacturing processes, succeeding the Second Agricultural Revolution. Beginning in Great Britain around 1760, the Industrial Revolution had spread to continental Europe and the United States by about 1840. This transition included going from hand production methods to machines; new chemical manufacturing and iron production processes; the increasing use of water power and steam power; the development of machine tools; and rise of the mechanised factory system. Output greatly increased, and the result was an unprecedented rise in population and population growth. The textile industry was the first to use modern production methods, and textiles became the dominant industry in terms of employment, value of output, and capital invested.\nMany technological and architectural innovations were British. By the mid-18th century, Britain was the leading commercial nation, controlled a global trading empire with colonies in North America and the Caribbean, and had military and political hegemony on the Indian subcontinent. The development of trade and rise of business were among the major causes of the Industrial Revolution. Developments in law facilitated the revolution, such as courts ruling in favour of property rights. An entrepreneurial spirit and consumer revolution helped drive industrialisation.\nThe Industrial Revolution influenced almost every aspect of life. In particular, average income and population began to exhibit unprecedented sustained growth. Economists note the most important effect was that the standard of living for most in the Western world began to increase consistently for the first time, though others have said it did not begin to improve meaningfully until the 20th century. GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, afterwards saw an era of per-capita economic growth in capitalist economies. Economic historians agree that the onset of the Industrial Revolution is the most important event in human history, comparable only to the adoption of agriculture with respect to material advancement.\nThe precise start and end of the Industrial Revolution is debated among historians, as is the pace of economic and social changes. According to Leigh Shaw-Taylor, Britain was already industrialising in the 17th century. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s, while T. S. Ashton held that it occurred between 1760 and 1830. Rapid adoption of mechanized textiles spinning occurred in Britain in the 1780s, and high rates of growth in steam power and iron production occurred after 1800. Mechanised textile production spread from Britain to continental Europe and the US in the early 19th century.\nA recession occurred from the late 1830s when the adoption of the Industrial Revolution's early innovations, such as mechanised spinning and weaving, slowed as markets matured despite increased adoption of locomotives, steamships, and hot blast iron smelting. New technologies such as the electrical telegraph, widely introduced in the 1840s in the UK and US, were not sufficient to drive high rates of growth. Rapid growth reoccurred after 1870, springing from new innovations in the Second Industrial Revolution. These included steel-making processes, mass production, assembly lines, electrical grid systems, large-scale manufacture of machine tools, and use of advanced machinery in steam-powered factories.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nEtymology.\nThe earliest recorded use of \"Industrial Revolution\" was in 1799 by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. Raymond Williams states: \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811\u201318, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\" The term \"Industrial Revolution\" applied to technological change became more common by the 1830s, as in J\u00e9r\u00f4me-Adolphe Blanqui's description in 1837 of . Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which...changed the whole of civil society\". His book was not translated into English until the late 19th century, and the expression did not enter everyday language till then. Credit for its popularisation is given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.\nEconomic historians such as Mendels, Pomeranz, and Kridte argue proto-industrialisation in parts of Europe, the Islamic world, Mughal India, and China created the social and economic conditions that led to the Industrial Revolution, thus causing the Great Divergence. Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and that \"revolution\" is a misnomer.\nRequirements.\nSeveral key factors enabled industrialisation. High agricultural productivity\u2014exemplified by the British Agricultural Revolution\u2014freed up labor and ensured food surpluses. The presence of skilled managers and entrepreneurs, an extensive network of ports, rivers, canals, and roads for efficient transport, and abundant natural resources such as coal, iron, and water power further supported industrial growth. Political stability, a legal system favorable to business, and access to financial capital also played crucial roles. Once industrialisation began in Britain in the 18th century, its spread was facilitated by the eagerness of British entrepreneurs to export industrial methods and the willingness of other nations to adopt them. By the early 19th century, industrialisation had reached Western Europe and the United States, and by the late 19th century, Japan.\nImportant technological developments.\nThe commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s, the following gains had been made in important technologies:\nTextile manufacture.\nBritish textile industry.\nIn 1750, Britain imported 2.5\u00a0million pounds of raw cotton, most of which was spun and woven by the cottage industry in Lancashire. The work was done by hand in workers' homes or master weavers' shops. Wages were six times those in India in 1770 when productivity in Britain was three times higher. In 1787, raw cotton consumption was 22\u00a0million pounds, most of which was cleaned, carded, and spun on machines. The British textile industry used 52\u00a0million pounds of cotton in 1800, and 588\u00a0million pounds in 1850.\nThe share of value added by the cotton industry in Britain was 2.6% in 1760, 17% in 1801, and 22% in 1831. Value added by the woollen industry was 14% in 1801. Cotton factories numbered about 900 in 1797. In 1760, approximately one-third of cotton cloth manufactured was exported, rising to two-thirds by 1800. In 1781, cotton spun amounted to 5\u00a0million pounds, which increased to 56\u00a0million pounds by 1800. In 1800, less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788, there were 50,000 spindles in Britain, rising to 7\u00a0million over the next 30 years.\nWool.\nThe earliest European attempts at mechanised spinning were with wool; however, wool spinning proved more difficult to mechanise than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant, but less than cotton.\nSilk.\nArguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. To promote manufacturing, the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.\nCotton.\nParts of India, China, Central America, South America, and the Middle East have a history of hand-manufacturing cotton textiles, which became a major industry after 1000 AD. Most cotton was grown by small farmers alongside food and spun in households for domestic consumption. In the 1400s, China began to require households to pay part of their taxes in cotton cloth. By the 17th century, almost all Chinese wore cotton clothing, and it could be used as a medium of exchange. In India, cotton textiles were manufactured for distant markets, often produced by professional weavers.\nCotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations. Spanish explorers found Native Americans growing sea island (\"Gossypium barbadense\") and upland cotton (\"Gossypium hirsutum\"). Sea island cotton was exported from Barbados from the 1650s. Upland cotton was uneconomical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi, in 1806 became the parent genetic material for 90% of world production today; it produced bolls three to four times faster to pick.\nTrade and textiles.\nThe Age of Discovery was followed by colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the British founded the East India Company, and other countries founded companies, which established trading posts throughout the Indian Ocean region.\nA large segment of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago where spices were purchased for sale to Southeast Asia and Europe. By the 1760s, cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in Europe, where previously only wool and linen were available; however, cotton goods consumed in Europe was minor until the early 19th century.\nPre-mechanized European textile production.\nBy 1600, Flemish refugees began weaving cotton in English towns where cottage spinning and weaving of wool and linen was established. They were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th-century Italy and 15th-century southern Germany, but these ended when the supply of cotton was cut off.\nBritish cloth could not compete with Indian cloth because India's labour cost was approximately one-fifth that of Britain's. In 1700 and 1721, the British government passed Calico Acts to protect domestic woollen and linen industries from cotton fabric imported from India. The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton had insufficient strength, the resulting blend was not as soft as 100% cotton and more difficult to sew.\nOn the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption, and as a cottage industry under the putting-out system. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off-season, the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took 4\u20138 spinners to supply one handloom weaver.\nInvention of textile machinery.\nThe flying shuttle, patented in 1733 by John Kay, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the dropbox, which facilitated changing thread colors.\nLewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with John Wyatt of Birmingham. In 1743, a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. A similar mill was built by Daniel Bourn. Paul and Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill.\nIn 1764, in Oswaldtwistle, Lancashire, James Hargreaves invented the spinning jenny. It was the first practical spinning frame with multiple spindles. The jenny worked similarly to the spinning wheel, by first clamping down on the fibres, then drawing them out, followed by twisting. It was a simple, wooden-framed machine that only cost \u00a36 for a 40-spindle model in 1792 and was used mainly by home spinners.\nThe water frame, was developed by Richard Arkwright, who patented it in 1769. The design was partly based on a spinning machine built by Kay, hired by Arkwright. The water frame could produce a hard, medium-count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. Arkwright used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name. Samuel Crompton invented the spinning mule in 1779, so called because it is a hybrid of Arkwright's water frame and James Hargreaves's spinning jenny. Crompton's mule could produce finer thread than hand spinning, at lower cost. Mule-spun thread was of suitable strength to be used as a warp and allowed Britain to produce highly competitive yarn in large quantities.\nRealising expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. Samuel Horrocks patented a loom in 1813, which was improved by Richard Roberts in 1822, and these were produced in large numbers by Roberts, Hill &amp; Co. Roberts was a maker of high-quality machine tools and pioneer in the use of jigs and gauges for precision workshop measurement.\nThe demand for cotton presented an opportunity to planters in the US, who thought upland cotton would be profitable if a better way could be found to remove the seed. Eli Whitney responded by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed in one day, which previously took two months.\nThese advances were capitalised on by entrepreneurs, of whom the best known is Arkwright. He is credited with a list of inventions, but these were developed by such people as Kay and Thomas Highs. Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and developed the use of power, which made cotton manufacture a mechanised industry. Other inventors increased the efficiency of spinning, so the supply of yarn increased greatly. Steam power was then applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.\nThough mechanisation dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, destroying the Indian industry.\nIron industry.\nBritish iron production.\nIn the UK in 1720, there were 20,500 tons of charcoal iron and 400 tons with coke. In 1806, charcoal iron production had dropped to 7,800 tons and coke cast iron was 250,000 tons. In 1750, the UK imported 31,000 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron, using charcoal and 100 tons using coke. In 1796, the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.\nIron process innovations.\nA major change in the iron industries, during the Industrial Revolution, was the replacement of wood and other bio-fuels with coal. For a given amount of heat, mining coal required much less labour than cutting wood and converting it to charcoal, and coal was more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century.\nIn 1709, Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron made was not suitable for making wrought iron and was used mostly for the production of cast iron goods. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper.\nIn 1750, coke had replaced charcoal in the smelting of copper and lead and was in widespread use in glass production. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Another factor limiting the iron industry was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.\nUse of coal in iron smelting started before the Industrial Revolution, based on innovations by Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities in the coal do not migrate into the metal. This technology was applied to lead in 1678, copper in 1687, and iron foundries in the 1690s, but in this case the reverberatory furnace was known as an air furnace.\nCoke pig iron was hardly used to produce wrought iron until 1755, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available, and not far from Coalbrookdale. These furnaces were equipped with water-powered bellows, the water being pumped by Newcomen atmospheric engines. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.\nSteam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, ironmaster John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768, designed by John Smeaton.\nCast iron cylinders for use with a piston were difficult to manufacture. James Watt had difficulty trying to have a cylinder made for his first steam engine. In 1774 Wilkinson invented a machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.\nIn addition to lower cost and greater availability, coke had other advantages over charcoal in that it was harder and made the column of materials flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.\nAs cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example is The Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron. Conversion of cast iron had long been done in a finery forge. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Puddling became widely used after 1800. British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, by the 1790s Britain eliminated imports and became a net exporter of bar iron.\nHot blast, patented by the Scottish inventor James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. The amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; the efficiency gains continued as the technology improved. Hot blast raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.\nShortly before the Industrial Revolution, an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire, and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.\nSteam power.\nThe development of the stationary steam engine was important in the Industrial Revolution; however, during its early period, most industrial power was supplied by water and wind. In Britain, by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000\u00a0hp.\nThe first commercially successful industrial use of steam power was patented by Thomas Savery in 1698. He constructed in London a low-lift combined vacuum and pressure water pump that generated about one horsepower (hp) and was used in waterworks and a few mines. The first successful piston steam engine was introduced by Thomas Newcomen before 1712. Newcomen engines were installed for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital, and produced upwards of . They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, they opened up a great expansion in coal mining by allowing mines to go deeper. The engines spread to Hungary in 1722, then Germany and Sweden; 110 were built by 1733. In the 1770s John Smeaton built large examples and introduced improvements. 1,454 engines had been built by 1800. Despite their disadvantages, Newcomen engines were reliable, easy to maintain and continued to be used in coalfields until the early 19th century.\nA fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated radical improvements, notably closing the upper part of the cylinder making the low-pressure steam drive the top of the piston instead of the atmosphere and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. These improvements increased engine efficiency so Boulton and Watt's engines used only 20\u201325% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.\nIn 1783, the Watt steam engine had been fully developed into a double-acting rotative type, which meant it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially successful, and by 1800 the firm Boulton and Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .\nUntil about 1800, the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon self-contained rotative engines were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steamboats.\nSmall industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the 20th century. These included crank-powered, treadle-powered and horse-powered workshop, and light industrial machinery.\nMachine tools.\nOver time it was shown that wooden components had the disadvantage of changing dimensions with temperature and humidity, and the joints tended to work loose. As the Industrial Revolution progressed machines with metal parts and frames, making them more common. Other uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts, and nuts. There was need for precision in making parts, to allow better working machinery, interchangeability of parts, and standardization of threaded fasteners.\nThe demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by clock and scientific instrument makers, to enable them to batch-produce small mechanisms. Before machine tools, metal was worked manually using the basic hand tools: hammers, files, scrapers, saws, and chisels. Consequently, use of metal machine parts was kept to a minimum. Hand methods of production were laborious and costly, and precision was difficult to achieve.\nThe first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It was designed to bore the large cylinders on steam engines. Wilkinson's machine was the first to use the principle of line-boring, where the tool is supported on both ends. The planing machine, the milling machine and the shaping machine were developed. Though the milling machine was invented at this time, it was not developed as a serious workshop tool until later. James Fox and Matthew Murray were manufacturers of machine tools who found success in exports and developed the planer around the same time as Richard Roberts.\nHenry Maudslay, who trained a school of machine tool makers, was a mechanic who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice under Jan Verbruggen, who, in 1774, installed a horizontal boring machine which was the first industrial size lathe in the UK. Maudslay was hired by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe with similarities to the slide rest lathe, Maudslay perfected this lathe, which cut machine screws of different thread pitches. Before its invention, screws could not be cut with precision. The slide rest lathe was called one of history's most important inventions. Although it was not Maudslay's idea, he was the first to build a functional lathe using innovations of the lead screw, slide rest, and change gears. Maudslay set up a shop, and built the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and the first for mass production and making components with interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and he trained men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.\nThe techniques to make mass-produced metal parts of sufficient precision to be interchangeable is attributed to the U.S. Department of War which perfected interchangeable parts for firearms. In the half-century following the invention of the fundamental machine tools, the machine industry became the largest industrial sector of the U.S. economy.\nChemicals.\nLarge-scale production of chemicals was an important development. The first of these was the production of sulphuric acid by the lead chamber process, invented by John Roebuck in 1746. He was able to increase the scale of the manufacture by replacing expensive glass vessels with larger, cheaper chambers made of riveted sheets of lead. Instead of a small amount, he was able to make around in each chamber, a tenfold increase.\nThe production of an alkali on a large scale became an important goal, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate (soda ash). The Leblanc process was a reaction of sulfuric acid with sodium chloride to give sodium sulfate and hydrochloric acid. The sodium sulfate was heated with calcium carbonate and coal to give a mixture of sodium carbonate and calcium sulfide. Adding water separated the soluble sodium carbonate from the calcium sulfide. The process produced significant pollution, nonetheless, this synthetic soda ash proved economical compared to that from burning plants, and to potash (potassium carbonate) produced from hardwood ashes. Soda ash and sulphuric acid were important because they enabled the introduction of other inventions, replacing small-scale operations with more cost-effective and controllable processes. Sodium carbonate had uses in the glass, textile, soap, and paper industries. Early uses for sulfuric acid included pickling (removing rust from) iron and steel, and for bleaching cloth.\nThe development of bleaching powder (calcium hypochlorite) by chemist Charles Tennant in 1800, based on the discoveries of Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by reducing the time required for the traditional process then in use: repeated exposure to the sun in fields after soaking the textiles with alkali or sour milk. Tennant's St Rollox Chemical Works, Glasgow, became the world's largest chemical plant.\nAfter 1860 the focus on chemical innovation was in dyestuffs, and Germany took leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in 1860\u20131914 to learn the latest techniques. British scientists lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.\nConcrete.\nIn 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement, an important advance in the building trades. This process involves sintering clay and limestone to about , then grinding it into a fine powder which is mixed with water, sand and gravel to produce concrete. In the 1840s, Joseph's son William Aspdin developed his father's invention.\nPortland cement concrete was used by English engineer Marc Isambard Brunel when constructing the Thames Tunnel, the world's first underwater tunnel. Portland cement concrete was used on a large scale in the construction of the London sewer system a generation later.\nGas lighting.\nThough others made a similar innovation, the large-scale introduction of gas lighting was the work of William Murdoch, an employee of Boulton &amp; Watt. The process consisted of the large-scale gasification of coal in furnaces, purification of the gas, and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.\nGlass making.\nGlass was made in ancient Greece and Rome. A new method of glass production, known as the cylinder process, was developed in Europe during the 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass; they became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is a significant example of the use of sheet glass in a new and innovative structure.\nPaper machine.\nA machine for making a continuous sheet of paper, on a loop of wire fabric, was patented in 1798 by Louis-Nicolas Robert in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. The Fourdrinier machine is the predominant means of production today. The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron, steel and other continuous production processes.\nAgriculture.\nThe British Agricultural Revolution raised crop yields and released labour for industrial employment, although per-capita food supply in much of Europe remained stagnant until the late 18th century. Key innovations included Jethro Tull's early 18th-century mechanical seed drill (1701), which ensured more even sowing and depth control, Joseph Foljambe's iron Rotherham plough (c. 1730) and Andrew Meikle's threshing machine (1784), which reduced manual labour requirements. Hand threshing with a flail, was a laborious job that had taken about one-quarter of agricultural labour, lower labour requirements resulted in lower wages and fewer labourers, who faced near starvation, leading to the 1830 Swing Riots.\nMining.\nCoal mining in Britain started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. If the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets up the shaft or to a sough (a tunnel driven into a hill to drain a mine). The water had to be discharged into a stream or ditch at a level where it could flow away.\nIntroduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 facilitated removal of water and enabled deeper shafts, enabling more coal to be extracted. These developments had begun before the Industrial Revolution, but the adoption of Smeaton's improvements to the Newcomen engine, followed by Watt's steam engines from the 1770s, reduced the fuel costs, making mines more profitable. The Cornish engine, developed in the 1810s, was more efficient than the Watt engine.\nCoal mining was dangerous owing to the presence of firedamp in coal seams. A degree of safety was provided by the safety lamp invented in 1816 by Sir Humphry Davy, and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe quickly and provided weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the 19th century. Conditions were very poor, with a high casualty rate from rock falls.\nTransportation.\nAt the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all motive power on land, with sails providing motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives introduced in the early 19th century. Improving sailing technologies boosted speed by 50% between 1750 and 1830.\nThe Industrial Revolution improved Britain's transport infrastructure with turnpike road, waterway and rail networks. Raw materials and finished products could be moved quicker and cheaper than before. Improved transport allowed ideas to spread quickly.\nCanals and improved waterways.\nBefore and during the Industrial Revolution navigation on British rivers was improved by removing obstructions, straightening curves, widening and deepening, and building navigation locks. Britain had over of navigable rivers and streams by 1750. Canals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a tens of times larger than could be drawn in a cart.\nCanals began to be built in the UK in the late 18th century to link major manufacturing centres. Known for its huge commercial success, the Bridgewater Canal in North West England, was opened in 1761 and mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost \u00a3168,000 (\u00a3 as of 2013[ [update]]), but its advantages over land and river transport meant that within one year, the coal price in Manchester fell by half. This success inspired Canal Mania, canals were hastily built with the aim of replicating the commercial success of Bridgewater, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.\nBy the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods used to construct the railways. They were largely superseded by the railways from the 1840s. The last major canal built in the UK was the Manchester Ship Canal, which upon opening in 1894 was the world's largest ship canal, and opened Manchester as a port. However, it never achieved the commercial success its sponsors hoped for and signalled canals as a dying transport mode in an age dominated by railways, which were quicker and often cheaper. Britain's canal network, and its mill buildings, is one of the most enduring features of the Industrial Revolution to be seen in Britain.\nRoads.\nFrance was known for having an excellent road system at this time; however, most roads on the European continent and in the UK were in bad condition, dangerously rutted. Much of the original British road system was poorly maintained by local parishes, but from the 1720s turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s: almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and John McAdam, with the first 'macadam' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The first macadam road in the U.S. was the \"Boonsborough Turnpike Road\" between Hagerstown and Boonsboro, Maryland in 1823.\nThe major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by slow, broad-wheeled carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or teams of packhorse. Stagecoaches carried the rich, and the less wealthy rode on carriers carts. Productivity of road transport increased greatly during the Industrial Revolution, and the cost of travel fell dramatically. Between 1690 and 1840 productivity tripled for long-distance carrying and increased four-fold in stage coaching.\nRailways.\nRailways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine. Reduced friction was a major reason for the success of railways compared to wagons. This was demonstrated on an iron plate-covered wooden tramway in 1805 at Croydon, England.\nA good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.\nWagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement. These were horse-drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of steam locomotive were on wagon or plate ways. Horse-drawn public railways begin in the early 19th century when improvements to pig and wrought iron production lowered costs.\nSteam locomotives began being built after the introduction of high-pressure steam engines, after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were much lighter and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.\nThe rapid introduction of railways followed the 1829 Rainhill trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity of the blast furnace. On 15 September 1830, the Liverpool and Manchester Railway, the first inter-city railway in the world, was opened. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port of Liverpool. The railway became highly successful, transporting passengers and freight.\nThe success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania. Construction of major railways connecting the larger cities and towns began in the 1830s, but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to the countryside but remained in the cities, providing additional workers for the factories.\nSocial effects.\nThe Industrial Revolution effectively asked the social question, demanding new ideas for managing large groups. Visible poverty, growing population and materialistic wealth, caused tensions between the richest and poorest. These tensions were sometimes violently released and led to philosophical ideas such as socialism, communism and anarchism.\nFactory system.\nPrior to the Industrial Revolution, most were employed in agriculture as self-employed farmers, tenants, landless agricultural labourers. It was common for families to spin yarn, weave cloth and make their clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution, India, China, and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth, while Europeans produced wool and linen goods.\nIn Great Britain in the 16th century, the putting-out system was practised, by which farmers and townspeople produced goods for a market in their homes, often described as \"cottage industry\". Merchant capitalists typically provided the raw materials, paid workers by the piece, and were responsible for sales. Embezzlement of supplies by workers and poor quality were common. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations.\nSome early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive, giving rise to capitalist ownership of factories.\nMost textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They worked for 12\u201314 hours with only Sundays off. It was common for women to take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours, and poor pay made it difficult to recruit and retain workers. The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx; however, he recognized the increase in productivity from technology.\nStandards of living.\nSome economists, such as Robert Lucas Jr., say the real effect of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth\u00a0... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\"\nOthers argue that while growth of the economy was unprecedented, living standards for most did not grow meaningfully until the late 19th century and workers' living standards declined under early capitalism. Some studies estimate that wages in Britain only increased 15% between the 1780s and 1850s and life expectancy did not dramatically increase until the 1870s. Average height declined during the Industrial Revolution, because nutrition was decreasing. Life expectancy of children increased dramatically: the percentage of Londoners who died before the age of five decreased from 75% in 1730\u201349, to 32% in 1810\u201329. The effects on living conditions have been controversial and were debated by historians from the 1950s to the 1980s. Between 1813 and 1913, there was a significant increase in wages.\nFood and nutrition.\nChronic hunger and malnutrition were the norms for most, including in Britain and France, until the late 19th century. Until about 1750, malnutrition limited life expectancy in France to 35, and 40 in Britain. The US population was adequately fed, taller, and had a life expectancy of 45\u201350, though this slightly declined by the mid 19th century. Food consumption per person also declined during an episode known as the Antebellum Puzzle. Food supply in Great Britain was adversely affected by the Corn Laws (1815\u201346) which imposed tariffs on imported grain. The laws were enacted to keep prices high to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.\nThe initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution with better agricultural practices; however, population grew as well.\nHousing.\nRapid population growth included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure, this was usually of advantage to tenants. People moved in so rapidly there was not enough capital to build adequate housing, so low-income newcomers squeezed into overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such that which devastated Ireland in the 1840s.\nA large expos\u00e9 literature grew up condemning the unhealthy conditions. The most famous publication was by a founder of the socialist movement. In \"The Condition of the Working Class in England\" in 1844, Friedrich Engels describes backstreets of Manchester and other mill towns, where people lived in shanties and shacks, some not enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. However, not everyone lived in such poor conditions. The Industrial Revolution created a middle class of businessmen, clerks, foremen, and engineers who lived in much better conditions.\nConditions improved over the 19th century with new public health acts regulating things such as sewage, hygiene, and home construction. In the introduction of his 1892 edition, Engels noted most of the conditions had greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.\nWater and sanitation.\nPre-industrial water supply relied on gravity systems, pumping water was done by water wheels, and wipes were made of wood. Steam-powered pumps and iron pipes allowed widespread piping of water to horse watering troughs and households.\nEngels' book describes how untreated sewage created awful odours and turned the rivers green in industrial cities. In 1854 John Snow traced a cholera outbreak in Soho, London to fecal contamination of a public water well by a home cesspit. Snow's finding that cholera could be spread by contaminated water took years to be accepted, but led to fundamental changes in the design of public water and waste systems. In 1855 the chemist Michael Faraday wrote a letter to \"The Times\" on the subject of the foul condition of the River Thames (raw sewerage went directly into the Thames), and in response to the exacerbation of sanitary conditions brought on by heavy industrialisation and urbanisation (London's population more than doubled between 1800 and 1850, making it by far the largest in the world), the modern sewage system was built in London by the Metropolitan Board of Works led by its chief engineer Joseph Bazalgette. The London sewer system began construction in 1859 and included of main and of street sewers that diverted waste to the Thames Estuary, and by the 1890s it would feature the revolutionary biological treatment of sewage to oxidise the waste.\nLiteracy.\nIn the 18th century, there was relatively high literacy among farmers in England and Scotland. This permitted the recruitment of literate craftsmen, skilled workers, foremen, and managers who supervised textile factories and coal mines. Much of the labour was unskilled, and especially in textile mills children as young as eight proved useful in handling chores and adding to family income. Children were taken out of school to work alongside their parents in the factories. However, by the mid-19th century, unskilled labour forces were common in Western Europe, and British industry moved upscale, needing more engineers and skilled workers who could handle technical instructions and handle complex situations. Literacy was essential to be hired. A senior government official told Parliament in 1870:\nUpon the speedy provision of elementary education depends are industrial prosperity. It is of no use trying to give technical teaching to our citizens without elementary education; uneducated labourers\u2014and many of our labourers are utterly uneducated\u2014are, for the most part, unskilled labourers, and if we leave our work\u2013folk any longer unskilled, notwithstanding their strong sinews and determined energy, they will become overmatched in the competition of the world.\nThe invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and pamphlet publishing, which contributed to rising literacy and demands for mass political participation.\nClothing and consumer goods.\nConsumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco, and chocolate became affordable to many in Europe. The consumer revolution in England from the 17th to the mid-18th century had seen a marked increase in the consumption and variety of luxury goods and products by individuals from different economic and social backgrounds. With improvements in transport and manufacturing technology, opportunities for buying and selling became faster and more efficient. The expanding textile trade in the north of England meant the three-piece suit became affordable to the masses. Founded by potter and retail entrepreneur Josiah Wedgwood in 1759, Wedgwood fine china and porcelain tableware was became a common feature on dining tables. Rising prosperity and social mobility in the 18th century increased those with disposable income for consumption, and the marketing of goods for individuals, as opposed households, started to appear.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;With the rapid growth of towns and cities, shopping became an important part of everyday life. Window shopping and the purchase of goods became a cultural activity...and many exclusive shops were opened in elegant urban districts: in the Strand and Piccadilly in London, for example, and in spa towns such as Bath and Harrogate. Prosperity and expansion in manufacturing industries such as pottery and metalware increased consumer choice dramatically. Where once labourers ate from metal platters with wooden implements, ordinary workers now dined on Wedgwood porcelain. Consumers came to demand an array of new household goods and furnishings: metal knives and forks...rugs, carpets, mirrors, cooking ranges, pots, pans, watches, clocks, and a dizzying array of furniture. The age of mass consumption had arrived.\nNew businesses appeared in towns and cities throughout Britain. Confectionery was one such industry that saw rapid expansion. According to food historian Polly Russell: \"chocolate and biscuits became products for the masses...By the mid-19th century, sweet biscuits were an affordable indulgence and business was booming. Manufacturers...transformed from small family-run businesses into state-of-the-art operations\". In 1847 Fry's of Bristol produced the first chocolate bar. Their competitor Cadbury, of Birmingham, was the first to commercialize the association between confectionery and romance when they produced a heart-shaped box of chocolates for Valentine's Day in 1868. The department store became a common feature in major High Streets; one of the first was opened in 1796 by Harding, Howell &amp; Co. on Pall Mall, London. The oldest toy store, Hamleys, opened in London in 1760. In the 1860s, fish and chip shops first appeared to satisfy the needs of the growing industrial population. Street sellers also became common in an increasingly urbanized country. Matthew White: \"Crowds swarmed in every thoroughfare. Scores of street sellers 'cried' merchandise from place to place, advertising the wealth of goods and services on offer. Milkmaids, orange sellers, fishwives and piemen...walked the streets offering their various wares for sale, while knife grinders and the menders of broken chairs and furniture could be found on street corners\". A soft drinks company, R. White's Lemonade, began in 1845 by selling drinks in London in a wheelbarrow.\nIncreased literacy, industrialisation, and the railway created a market for cheap literature for the masses and the ability for it to be circulated on a large scale. Penny dreadfuls were created in the 1830s to meet this demand, \"Britain's first taste of mass-produced popular culture for the young\", and \"the Victorian equivalent of video games\". By the 1860s and 70s more than one million boys' periodicals were sold per week. Labelled an \"authorpreneur\" by \"The Paris Review\", Charles Dickens used the innovations of the era to sell books: new printing presses, enhanced advertising revenues, and the railways. His first novel, \"The Pickwick Papers\" (1836), became a phenomenon, its unprecedented success sparking spin-offs and merchandise ranging from \"Pickwick\" cigars, playing cards, china figurines, Sam Weller puzzles, Weller boot polish and jokebooks. Nicholas Dames in \"The Atlantic\" writes, \"Literature\" is not a big enough category for \"Pickwick\". It defined its own, a new one that we have learned to call \"entertainment\". Urbanisation led to development of the music hall in the 1850s, with the newly created urban communities, cut off from their cultural roots, requiring new and accessible forms of entertainment.\nIn 1861, Welsh entrepreneur Pryce Pryce-Jones formed the first mail order business, an idea which changed retail. Selling Welsh flannel, he created catalogues, with customers able to order by mail for the first time\u2014this following the Uniform Penny Post in 1840 and invention of the postage stamp (Penny Black) with a charge of one penny for carriage between any two places in the UK irrespective of distance\u2014and the goods were delivered via the new railway system. As the railways expanded overseas, so did his business.\nPopulation increase.\nThe Industrial Revolution was the first time there was a simultaneous increase in population and per person income. The population of England and Wales, which had remained steady at six million in 1700\u201340, rose dramatically afterwards. England's population doubled from 8.3\u00a0million in 1801 to 17\u00a0million in 1850 and, by 1901, had doubled again to 31\u00a0million. Improved conditions led to the population of Britain increasing from 10\u00a0million to 30\u00a0million in the 19th century. Europe's population increased from 100\u00a0million in 1700 to 400\u00a0million by 1900.\nBetween 1815 and 1939, 20% of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labour, ready availability of land, and cheap transportation. Many did not find a satisfactory life, leading 7\u00a0million to return to Europe. This mass migration had large demographic effects: in 1800, less than 1% of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11%. The Americas felt the brunt of this huge emigration, largely concentrated in the US.\nUrbanization.\nThe growth of the industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe, then elsewhere, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of humans lived in cities, compared to 50% by 2000. Manchester had a population of 10,000 in 1717, by 1911 it had burgeoned to 2.3\u00a0million.\nEffect on women and family life.\nWomen's historians have debated the effect of the Industrial Revolution and capitalism on the status of women. Taking a pessimistic view, Alice Clark argues that when capitalism arrived in 17th-century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production, and women played a vital role in running farms and some trades and landed estates. Their economic role gave them a sort of equality. However, Clark argues, as capitalism expanded, there was more division of labour with husbands taking paid labour jobs outside the home, and wives reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.\nIn a more positive interpretation, Ivy Pinchbeck argues capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use, and women produced much of the needs of the households. The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife, and older children. The third, or modern, stage is the \"family consumer economy\", in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising consumption.\nIdeas of thrift and hard work characterised middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book \"Self-Help\", in which he states that the misery of the poorer classes was \"voluntary and self-imposed\u2014the results of idleness, thriftlessness, intemperance, and misconduct.\"\nLabour conditions.\nSocial structure and working conditions.\nHarsh working conditions were prevalent long before the Industrial Revolution. Pre-industrial society was very static and often cruel\u2014child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.\nThe Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Working people found increased opportunities for employment in mills and factories, but these were under strict working conditions with long hours dominated by a pace set by machines. As late as 1900, most US industrial workers worked 10-hour days, yet earned 20\u201340% less than that necessary for a decent life. Most workers in textiles, which was the leading industry in terms of employment, were women and children. For workers, industrial life \"was a stony desert, which they had to make habitable by their own efforts.\"\nFactories and urbanisation.\nIndustrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas as workers migrated into the cities in search of work in the factories. This was clearly illustrated in the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city. Manchester experienced a six-times increase in population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851, and by 1851 only 50% of its population were born there.\nFor much of the 19th century, production was done in small mills which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler. Some industrialists tried to improve factory and living conditions for their workers. One early reformer was Robert Owen, known for his pioneering efforts in improving conditions for at the New Lanark mills and often regarded as a key thinker of the early socialist movement.\nBy 1746 an integrated brass mill was working at Warmley near Bristol. Raw material was smelted into brass and turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton were other prominent early industrialists who employed the factory system.\nChild labour.\nThe chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly. There was still limited opportunity for education, and children were expected to work. Child labour had existed before, but with the increase in population and education it became more visible. Many children were forced to work in bad conditions for much lower pay than their elders, 10\u201320% of an adult male's wage, even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution, between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were children.\nReports detailing some of the abuses, particularly in the mines and textile factories, helped to popularise the children's plight. The outcry, especially among the upper and middle classes, helped stir change for the young workers' welfare. Politicians and the government tried to limit child labour by law, but factory owners resisted; some felt they were aiding the poor by giving their children money to buy food, others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: children younger than nine were not allowed to work, children were not permitted to work at night, and the working day for those under 18 was limited to 12 hours. Factory inspectors enforced the law; however, their scarcity made this difficult. A decade later, the employment of children and women in mining was forbidden. Although laws decreased child labourers, it remained significantly present in Europe and the US until the 20th century.\nOrganisation of labour.\nThe Industrial Revolution concentrated labour into mills, factories, and mines, thus facilitating the organisation of \"combinations\" or trade unions advance the interests of working people. A union could demand better terms by withdrawing and halting production. Employers had to decide between giving in at a cost, or suffering the cost of the lost production. Skilled workers were difficult to replace, and these were the first to successfully advance their conditions through this kind of bargaining.\nThe main method unions used, and still use, to effect change was strike action. Many strikes were painful events for both unions and management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were severely restricted. A British newspaper in 1834 described unions as \"the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...\"\nThe Reform Act 1832 extended the vote in Britain, but did not grant universal suffrage. Six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the lowering of wages in the 1830s. They refused to work for less than ten shillings per week, by this time wages had been reduced to seven shillings and were to be reduced to six. In 1834 James Frampton, a local landowner, wrote to Prime Minister Lord Melbourne to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Society had done. Six men were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 40s, the chartist movement was the first large-scale organised working-class political movement that campaigned for political equality and social justice. Its \"Charter\" of reforms received three million signatures, but was rejected by Parliament without consideration.\nWorking people formed friendly societies and cooperative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen supported these organisations to improve conditions. Unions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the chartist movement which stopped production across Britain. Eventually, effective political organisation for working people was achieved through trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist parties that merged to become the British Labour Party.\nLuddites.\nThe rapid industrialisation of the English economy cost many craft workers their jobs. The Luddite movement started first with lace and hosiery workers near Nottingham, and spread to other areas of the textile industry. Many weavers found themselves suddenly unemployed as they could no longer compete with machines which required less skilled labour to produce more cloth than one weaver. Many unemployed workers and others turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the movement began in 1811. The Luddites rapidly gained popularity, and the Government took drastic measures using the militia or army to protect industry. Rioters who were caught were tried and hanged, or transported for life.\nUnrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. The riots led to the first formation of trade unions and further pressure for reform.\nShift in production's centre of gravity.\nThe traditional centres of hand textile production such as India, the Middle East, and China could not withstand competition from machine-made textiles, which destroyed the hand-made textile industries and left millions without work, many of whom starved. The Industrial Revolution generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.\nCotton and the expansion of slavery.\nCheap cotton textiles increased demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. British production grew from 2 million pounds in 1700 to 5 million in 1781 to 56 million in 1800. The invention of the cotton gin by American Eli Whitney in 1792 was the decisive event. It allowed green-seeded cotton to become profitable, leading to the widespread growth of slave plantations in the US, Brazil, and the West Indies. In 1791, American cotton production was 2 million pounds, soaring to 35 million by 1800, half of which was exported. America's cotton plantations were highly efficient, profitable and able to keep up with demand. The U.S. Civil War created a \"cotton famine\" that led to increased production in other areas of the world, including European colonies in Africa.\nEffect on environment.\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution during the Industrial Revolution. The emergence of great factories and the linked immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centres; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Act 1863, to regulate the air pollution given off by the Leblanc process used to produce soda ash. Alkali inspectors were appointed to curb this pollution.\nThe manufactured gas industry began in British cities in 1812\u201320. This produced highly toxic effluent dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London indicted gas companies in the 1820s for polluting the Thames, poisoning its fish. Parliament wrote company charters to regulate toxicity. The industry reached the U.S. around 1850 causing pollution and lawsuits.\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898. It was founded by artist William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their smoke. It provided for sanctions against factories that emitted large amounts of black smoke.\nBeyond Great Britain.\nEurope.\nThe Industrial Revolution in continental Europe started in Belgium and France, then spread to German states by the middle of the 19th century. In many industries, this involved the application of technology developed in Britain. Typically, the technology was purchased from Britain, or British engineers and entrepreneurs moved abroad in search of opportunities. By 1809, part of the Ruhr in Westphalia was called 'Miniature England' because of its similarities. Most European governments provided state funding to the new industries. In some cases, such as iron, the different availability of resources locally meant only some aspects of the British technology were adopted.\nBelgium.\nBelgium was the second country in which the Industrial Revolution took place. Thanks to coal, Wallonia in south Belgium, took the lead. Starting in the 1820s, and especially after Belgium became independent in 1830, factories comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Li\u00e8ge and Charleroi. The leader was John Cockerill, a transplanted Englishman. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.\nWallonia exemplified the radical evolution of industrial expansion, it was also the birthplace of a strong socialist party and trade unions. With its \"Sillon industriel\", \"Especially in the Haine, Sambre and Meuse valleys...there was a huge industrial development based on coal-mining and iron-making...\". Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power...after Britain.\" \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth-making town of Ghent.\" Many 19th-century coal mines in Wallonia are now protected as World Heritage Sites. Though Belgium was the second industrial country after Britain, the effect of the Industrial Revolution was different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:\nThe Industrial Revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the early modern period, Flanders was characterised by the presence of large urban centres...at the beginning of the nineteenth century...Flanders...with an urbanisation degree of more than 30 percent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 percent in Wallonia, barely 10 percent in most West European countries, 16 percent in France, and 25 percent in Britain. 19th-century industrialisation did not affect the traditional urban infrastructure, except in Ghent... Also, in Wallonia, the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 percent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys...where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast...Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology...\nFrance.\nThe Industrial Revolution in France did not correspond to the main model followed by other countries. Most French historians argue France did not go through a clear \"take-off\". Instead, economic growth and industrialisation was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice L\u00e9vy-Leboyer:\nGermany.\nGermany's political disunity\u2014with three dozen states\u2014and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railway construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1871 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike in France, the goal was the support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France.\nBased on its leadership in chemical research in universities and industrial laboratories, Germany became dominant in the world's chemical industry in the late 19th century.\nSweden.\nBetween 1790-1815, Sweden experienced parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops, and farming tools and commercialisation of farming, and a \"proto industrialisation\", with small industries established in the countryside and workers switching between agriculture in summer and industrial production in winter. This led to economic growth benefiting the population and leading to a consumption revolution in the 1820s. Between 1815-50, the protoindustries developed into specialised and larger industries. This period witnessed regional specialisation with mining in Bergslagen, textile mills in Sjuh\u00e4radsbygden, and forestry in Norrland. Important institutional changes took place, such as free and mandatory schooling introduced in 1842 (first time in the world), abolition of the monopoly on trade in handicrafts in 1846, and a stock company law in 1848.\nFrom 1850 to 1890, there was a rapid expansion in exports, dominated by crops, wood, and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873. Large infrastructural investments were made, mainly in the expanding railroad network, which was financed by the government and private enterprises. From 1890 to 1930, new industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.\nAustria-Hungary.\nThe Habsburg realms, which became Austria-Hungary in 1867, had a population of 23 million in 1800, growing to 36 million by 1870. Between 1818-70, industrial growth averaged 3% annually, though development varied across regions. A boost to industrialisation came with the construction of the rail network between 1850-73, which transformed transport by making it faster, more reliable and affordable. Proto-industrialisation had begun by 1750 in Alpine and Bohemian regions\u2014now the Czech Republic\u2014which emerged as the industrial hub of the empire. The textile industry led this transformation, adopting mechanisation, steam engines, and the factory system. The first mechanical loom in the Czech lands was introduced in Varnsdorf in 1801 followed shortly by the arrival of steam engines in Bohemia and Moravia. Textile production flourished in industrial centers such as Prague and Brno\u2014the latter earning the nickname \"Moravian Manchester.\" The Czech lands became an industrial heartland due to rich natural resources, skilled workforce, and early adoption of technology. The iron industry also expanded in the Alpine regions after 1750. Hungary, by contrast, remained predominantly rural and under-industrialised until after 1870. However, reformers like Count Istv\u00e1n Sz\u00e9chenyi played a crucial role in laying the groundwork for future development. Often called \"the greatest Hungarian,\" Sz\u00e9chenyi advocated for economic modernisation, infrastructure development, and industrial education. His initiatives included the promotion of river regulation, bridge construction, and the founding of the Hungarian Academy of Sciences\u2014all aimed at fostering a market-oriented economy. In 1791, Prague hosted the first World's Fair, in Clementinum showcasing the region\u2019s growing industrial sophistication. An earlier industrial exhibition was held in conjunction with the coronation of Leopold II as King of Bohemia, celebrating advanced manufacturing techniques in the Czech lands. \nFrom 1870 to 1913, technological innovation drove industrialisation and urbanisation across the empire. Gross national product (GNP) per capita grew at an average annual rate of 1.8%\u2014surpassing Britain (1%), France (1.1%), and Germany (1.5%). Nevertheless, Austria-Hungary as a whole continued to lag behind more industrialised powers like Britain and Germany, largely due to its later start in the modernisation process.\nJapan.\nThe Industrial Revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railways, improved roads, and inaugurated a land reform program to prepare the country for further development. It inaugurated a new Western-based education system for young people, sent thousands of students to the US and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages.\nIn 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the US to learn Western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.\nUnited States.\nDuring the late 18th and early 19th centuries when Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country.\nImportant American technological contributions were the cotton gin and the development of a system for making interchangeable parts, which was aided by the development of the milling machine in the US. The development of machine tools and system of interchangeable parts was the basis for the rise of the US as the world's leading industrial nation in the late 19th century.\nOliver Evans invented an automated flour mill in the mid-1780s, that used control mechanisms and conveyors so no labour was needed from when grain was loaded into the elevator buckets, until the flour was discharged into a wagon. This is considered to be the first modern materials handling system, an important advance in the progress toward mass production.\nThe US originally used horse-powered machinery for small-scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.\nThomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills. This mill was designed to use horsepower, but the operators quickly learned that the horse-drawn platform was economically unstable, and had losses for years. Despite this, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.\nIn 1793, Samuel Slater (1768\u20131835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US. The Blackstone Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', Blackstone River, which, with its tributaries, cover more than . At its peak over 1,100 mills operated in this valley, including Slater's Mill.\nMerchant Francis Cabot Lowell from Newburyport, Massachusetts, memorised the design of textile machines on his tour of British factories in 1810. The War of 1812 ruined his import business but realising demand for domestic-finished cloth was emerging in America, on his return he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the US. Lowell, Massachusetts, using of canals and delivered by the Merrimack River. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Great Famine of Ireland, the system had been replaced by poor immigrant labour.\nA major U.S. contribution to industrialisation was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for firearms. Techniques included using fixtures to hold the parts in the proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardised and interchangeable parts became known as the American system of manufacturing.\nPrecision manufacturing techniques made it possible to build machines that mechanised the shoe and watch industries. The industrialisation of the watch industry started in 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.\nSecond Industrial Revolution.\nSteel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality.\nThis Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain, to the US and Germany. The increasing availability of economical petroleum products also reduced the importance of coal and widened the potential for industrialisation.\nA new revolution began with electricity and electrification in the electrical industries. By the 1890s, industrialisation had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.\nCauses.\nThe causes of the Industrial Revolution were complicated and remain debated. Geographic factors include Britain's vast mineral resources. In addition to metal ores, Britain had the highest quality coal reserves known at the time, as well as abundant water power, highly productive agriculture, numerous seaports and navigable waterways.\nSome historians believe the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century, although feudalism began to break down after the Black Death of the mid 14th century. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing farmers no longer self-sufficient into cottage industry, for example weaving, and in the longer term into the cities and newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are cited as factors, as is the scientific revolution of the 17th century. A change to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.\nUntil the 1980s, it was believed technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention of the steam engine. Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, earlier than most estimates. He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\". He cites the monastic emphasis on order and time-keeping, and the fact medieval cities had at their centre a church with bell ringing at regular intervals, as necessary precursors to a synchronisation necessary for later manifestations such as the steam engine. Anthropologist Joseph Henrich also argues for an origin in the Early Middle Ages but specifically identifies the primary cause as the dissolution of European kinship networks under pressure from the Catholic Church.\nThe presence of a large domestic market is considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.\nGovernments' grant of limited monopolies to inventors under a developing patent system is considered an influential factor. The effects of patents, on the development of industrialisation are clearly illustrated in the history of the steam engine. In return for publicly revealing the workings of an invention, patents rewarded inventors such as James Watt by allowing them to monopolise production, and increasing the pace of technological development. However, monopolies bring inefficiencies which counterbalance, or even overbalance, the benefits of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors from introducing improved steam engines, thereby slowing the spread of steam power.\nCauses in Europe.\nA question of active interest is why the Industrial Revolution occurred in Europe and not elsewhere, particularly China, India, and the Middle East (which pioneered in shipbuilding, textiles and water mills between 750-1100), or at other times like in Classical Antiquity or the Middle Ages. One account argues Europeans have been characterized for millennia by a freedom-loving culture originating from the aristocratic societies of Indo-European invaders. Many historians, however, have challenged this as being not only Eurocentric, but ignoring historical context. In fact, before the Industrial Revolution, \"there existed something of a global economic parity between the most advanced regions in the world economy.\" These historians have suggested other factors, including education, technological changes, \"modern\" government, \"modern\" work attitudes, ecology, and culture.\nChina was the most technologically advanced country for centuries; however, it stagnated and was surpassed by Europe before the Age of Discovery, by which time China banned imports and denied entry to foreigners. It taxed transported goods heavily. Modern estimates of per capita income in Western Europe in the late 18th century are roughly 1,500 dollars in purchasing power parity whereas China had only 450 dollars. India was feudal, politically fragmented and not as advanced as Western Europe.\nHistorians such as David Landes and sociologists Max Weber and Rodney Stark credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were products of Judeo-Christian and Greek thought. Chinese society was founded on men like Confucius, Mencius, Han Feizi, and Buddha, resulting in different worldviews. Other factors include the considerable distance of China's coal deposits from its cities, as well as the then unnavigable Yellow River that connects deposits to the sea.\nHistorian Joel Mokyr argued political fragmentation, the presence of many European states, made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily move to a neighboring state if one state suppressed their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China, by providing \"an insurance against economic and technological stagnation\". China had a printing press and movable type, and India had similar scientific and technological achievement as Europe in 1700, yet the Industrial Revolution occurred in Europe first. In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters. Political institutions could contribute to the relation between democratization and economic growth during the Great Divergence.\nEurope's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region \"at the hub of the largest and most varied network of exchange in history\", Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading Peter Stearns to conclude that \"Europe\u2019s industrial revolution, which was to have such dramatic effects on the wider world, stemmed in great part from Europe\u2019s changing position in the wider world, and a particular desire to catch up or surpass Asian manufacturing competitors.\"\nModern capitalism originated in the Italian city-states around the end of the first millennium. The city-states were prosperous cities that were independent from feudal lords. They were republics whose governments were composed of merchants, manufacturers, members of guilds, bankers and financiers. The city-states built a network of branch banks in western European cities and introduced double entry bookkeeping. Italian commerce was supported by schools that taught numeracy in financial calculations through abacus schools.\nCauses in Britain.\nBritain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors were:\nThere were two main values that drove the Industrial Revolution in Britain: self-interest and entrepreneurial spirit. Because of these, many advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements benefitted British society as a whole. Countries recognised the advancements and used them to begin their own revolutions.\nA debate sparked by Eric Williams in his work \"Capitalism and Slavery\" (1944), concerned the role of slavery in financing the Industrial Revolution. Williams argued European capital amassed from slavery was vital in the early years of the revolution. This led to historiographical debate, with Seymour Drescher critiquing Williams' arguments in \"Econocide\" (1977).\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"An unprecedented explosion of new ideas, and new technological inventions, transformed our use of energy, creating an increasingly industrial and urbanised country. Roads, railways and canals were built. Great cities appeared. Scores of factories and mills sprang up. Our landscape would never be the same again. It was a revolution that transformed not only the country, but the world itself.\"\n \u2013 British historian Jeremy Black on the BBC's \"Why the Industrial Revolution Happened Here\".\nThe greater liberalisation of trade, from a large merchant base, may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder, and with the only significant merchant fleet. Britain's extensive exporting cottage industries ensured markets were already available for early forms of manufactured goods. Most British warfare was conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position\u2014an island separated from the rest of Europe.\nBritain was able to succeed due to the key resources it possessed, and population density. Enclosure of common land and the related agricultural revolution made supply of labour readily available. There was a local coincidence of natural resources in the North of England, the Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.\nThe stable political situation in Britain from 1688, following the Glorious Revolution, and society's greater receptiveness to change than other European countries, can be said to be factors favouring the Industrial Revolution. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in 1688, and the emergence of a stable financial market based on the management of the national debt by the Bank of England, contributed to the capacity for private financial investment in industrial ventures. Peasant resistance to industrialisation was eliminated by the Enclosure movement, and the landed classes developed commercial interests that made them pioneers in removing obstacles to capitalism. Taking refuge in England in 1726, Voltaire wrote about commerce and religious diversity in \"Letters on the English\" (1733), noting why England was more prosperous compared to less religiously tolerant European neighbours: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.\"\nBritain's population grew 280% from 1550 to 1820, while the rest of Western Europe grew 50\u201380%. 70% of European urbanisation happened in Britain from 1750 to 1800. By 1800, only the Netherlands was more urbanised. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.\nIn 1700, five-sixths of the coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, lowest taxes, and most urbanised, well-paid, and literate population, it did not industrialise. Without coal, Britain would have run out of suitable river sites for mills by the 1830s. Based on science and experimentation from the continent, the steam engine was developed for pumping water out of mines, many of which in Britain had been mined to below the water table. Although inefficient they were economical because they used unsaleable coal. Iron rails were developed to transport coal, which was a major economic sector in Britain.\nBob Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution. These factors made it vastly more profitable to invest in research and development, and put technology to use than other societies. However, 2018 studies in \"The Economic History Review\" showed wages were not particularly high in spinning or construction sectors, casting doubt on Allen's explanation. A 2022 study found industrialization happened in areas with low wages and high mechanical skills, whereas literacy, banks and proximity to coal had little explanatory power.\nProtestant work ethic.\nAnother theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic and particular status of Baptists and dissenting Protestant sects, such as the Quakers and Presbyterians. English Dissenters were barred or discouraged from almost all public offices, as well as education at England's only two universities. When the restoration of the monarchy took place in 1688 and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences, vital to the development of manufacturing technologies.\nHistorians sometimes consider this social factor to be important. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants by many in the middle class, such as financiers or other businessmen. Given this tolerance and the supply of capital, the natural outlet for more enterprising members of these sects was in new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.\nTransfer of knowledge.\nKnowledge of innovation was spread by several means. Workers trained in a technique might move to another employer. A common method was the study tour, in which individuals gathered information abroad. Throughout the Industrial Revolution and preceding century, European countries and America engaged in such tours; Sweden and France trained civil servants and technicians to undertake them as policy, while in Britain and America manufacturers pursued tours independently. Tour diaries are invaluable records of methods.\nInnovation spread via informal networks such as the Lunar Society of Birmingham, whose members met from 1765 to 1809 to discuss natural philosophy and its industrial applications. They have been described as \u201cthe revolutionary committee of that most far-reaching of all the eighteenth-century revolutions, the Industrial Revolution.\u201d Similar societies published papers and proceedings; the Royal Society of Arts issued annual \"Transactions\" and illustrated volumes of inventions.\nTechnical encyclopaedias disseminated methods. John Harris\u2019s \"Lexicon Technicum\" (1704) offered extensive scientific and engineering entries. Abraham Rees\u2019s \"The Cyclopaedia; or, Universal Dictionary of Arts, Sciences, and Literature\" (1802\u201319) contained detailed articles and engraved plates on machines and processes. French works such as the \"Descriptions des Arts et M\u00e9tiers\" and Diderot\u2019s \"Encyclop\u00e9die\" similarly documented foreign techniques with engraved illustrations. Periodicals on manufacturing and patents emerged in the 1790s; for instance, French journals like the \"Annales des Mines\" printed engineers\u2019 travel reports on British factories, helping diffuse innovations.\nCriticisms.\nThe industrial revolution has been criticised for causing ecosystem collapse, mental illness, pollution and detrimental social systems. It has also been criticised for valuing profits and corporate growth over life and wellbeing. Multiple movements have arisen which reject aspects of the industrial revolution, such as the Amish or primitivists.\nHumanism and harsh conditions.\nSome humanists and individualists criticise the Industrial Revolution for mistreating women and children and turning men into work machines that lacked autonomy. Critics of the Industrial Revolution promoted a more interventionist state and formed new organisations to promote human rights.\nPrimitivism.\nPrimitivism argues that the Industrial Revolution has created an unnatural frame of society and the world in which humans need to adapt to an unnatural urban landscape in which humans are perpetual cogs without personal autonomy.\nCertain primitivists argue for a return to pre-industrial society, while others argue that technology such as modern medicine, and agriculture are all positive for humanity assuming they are controlled by and serve humanity and have no effect on the natural environment.\nPollution and ecological collapse.\nThe Industrial Revolution has been criticised for leading to immense ecological and habitat destruction. It has led to immense decrease in the biodiversity of life on Earth. The Industrial revolution has been said to be inherently unsustainable and will lead to eventual collapse of society, mass hunger, starvation, and resource scarcity.\nOpposition from Romanticism.\nDuring the Industrial Revolution, an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialisation, urbanisation and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley.\nThe movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\". Mary Shelley's \"Frankenstein\" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14918", "revid": "42563337", "url": "https://en.wikipedia.org/wiki?curid=14918", "title": "International Court of Justice", "text": "Primary judicial organ of the United Nations\nThe International Court of Justice (ICJ; , CIJ), or colloquially the World Court, is the principal judicial organ of the United Nations (UN). It settles legal disputes submitted to it by states and provides advisory opinions on legal questions referred to it by other UN organs and specialized agencies. The ICJ is the only international court that adjudicates general disputes between countries, with its rulings and opinions serving as primary sources of international law. It is one of the six principal organs of the United Nations.\nEstablished in June 1945 by the Charter of the United Nations, the Court began work in April 1946. It is the successor to the Permanent Court of International Justice (PCIJ), which was established by the League of Nations in 1920. Its founding statute is an integral part of the UN Charter and draws heavily from that of its predecessor. All UN member states are automatically parties to the ICJ Statute. However, the Court's jurisdiction in contentious cases is founded upon the consent of the states party to a dispute, which may be given through special agreements or declarations accepting the Court's compulsory jurisdiction.\nThe Court is composed of a panel of 15 judges elected by the UN General Assembly and Security Council for nine-year terms. The composition of the bench is required to represent the \"main forms of civilization and the principal legal systems of the world,\" and no two judges may be nationals of the same country. The ICJ is seated in the Peace Palace in The Hague, Netherlands, making it the only principal UN organ not located in New York City. Its official working languages are English and French.\nSince its first case was submitted in 1947, the Court has entertained 201 cases as of September 2025. While its judgments are binding on the parties and final, the ICJ possesses no formal enforcement mechanism. Enforcement of its rulings is ultimately a political matter for the UN Security Council, where it is subject to the veto power of the five permanent members.\nHistory.\nThe first permanent institution established for the purpose of settling international disputes was the Permanent Court of Arbitration (PCA), which was created by the Hague Peace Conference of 1899. Initiated by the Russian Tsar Nicholas II, the conference involved all the world's major powers, as well as several smaller states, and resulted in the first multilateral treaties concerned with the conduct of warfare. Among these was the \"Convention for the Pacific Settlement of International Disputes\", which set forth the institutional and procedural framework for arbitral proceedings, which would take place in The Hague, Netherlands. Although the proceedings would be supported by a permanent bureau\u2014whose functions would be equivalent to that of a secretariat or court registry\u2014the arbitrators would be appointed by the disputing states from a larger pool provided by each member of the convention. The PCA was established in 1900 and began proceedings in 1902.\nA second Hague Peace Conference in 1907, which involved most of the world's sovereign states, revised the convention and enhanced the rules governing arbitral proceedings before the PCA. During this conference, the United States, Great Britain and Germany submitted a joint proposal for a permanent court whose judges would serve full-time. As the delegates could not agree how the judges would be selected, the matter was shelved pending an agreement to be adopted at a later convention.\nThe Hague Peace Conferences, and the ideas that emerged therefrom, influenced the creation of the Central American Court of Justice, which was established in 1908 as one of the earliest regional judicial bodies. Various plans and proposals were made between 1911 and 1919 for the establishment of an international judicial tribunal, which would not be realized in the formation of a new international system following the First World War.\nThe Permanent Court of International Justice.\nThe unprecedented bloodshed of the First World War led to the creation of the League of Nations, established by the Paris Peace Conference of 1919 as the first worldwide intergovernmental organization aimed at maintaining peace and collective security. Article 14 League's Covenant called for the establishment of a Permanent Court of International Justice (PCIJ), which would be responsible for adjudicating any international dispute submitted to it by the contesting parties, as well as to provide an advisory opinion upon any dispute or question referred to it by the League of Nations.\nIn December 1920, following several drafts and debates, the Assembly of the league unanimously adopted the statute of the PCIJ, which was signed and ratified the following year by a majority of members. Among other things, the new Statute resolved the contentious issues of selecting judges by providing that the judges be elected by both the council and the Assembly of the league concurrently but independently. The makeup of the PCIJ would reflect the \"main forms of civilization and the principal legal systems of the world\". The PCIJ would be permanently placed at the Peace Palace in The Hague, alongside Permanent Court of Arbitration.\nThe PCIJ represented a major innovation in international jurisprudence in several ways:\nUnlike the ICJ, the PCIJ was not part of the league, nor were members of the league automatically a party to its Statute. The United States, which played a key role in both the second Hague Peace Conference and the Paris Peace Conference, was notably not a member of the league. However, several of its nationals served as judges of the court.\nFrom its first session in 1922 until 1940, the PCIJ dealt with 29 interstate disputes and issued 27 advisory opinions. The court's widespread acceptance was reflected by the fact that several hundred international treaties and agreements conferred jurisdiction upon it over specified categories of disputes. In addition to helping resolve several serious international disputes, the PCIJ helped clarify several ambiguities in international law that contributed to its development.\nThe United States played a major role in setting up the PCIJ but never joined. Presidents Wilson, Harding, Coolidge, Hoover, and Roosevelt all supported membership, but did not get the two-thirds majority in the Senate required for a treaty.\nEstablishment of the International Court of Justice.\nFollowing a peak of activity in 1933, the PCIJ began to decline in its activities due to the growing international tension and isolationism that characterized the era. The Second World War effectively put an end to the court, which held its last public session in December 1939 and issued its last orders in February 1940. In 1942 the United States and United Kingdom jointly declared support for establishing or re-establishing an international court after the war, and in 1943, the U.K. chaired a panel of jurists from around the world, the \"Inter-Allied Committee\", to discuss the matter. Its 1944 report recommended that:\nSeveral months later at the Moscow conference in 1943, the major Allied Powers\u2014China, the USSR, the U.K., and the U.S.\u2014issued a joint declaration recognizing the necessity \"of establishing at the earliest practicable date a general international organization, based on the principle of the sovereign equality of all peace-loving States, and open to membership by all such States, large and small, for the maintenance of international peace and security\".\nThe following Allied conference at Dumbarton Oaks, in the United States, published a proposal in October 1944 that called for the establishment of an intergovernmental organization that would include an international court. A meeting was subsequently convened in Washington, D.C., in April 1945, involving 44 jurists from around the world to draft a statute for the proposed court. The draft statute was substantially similar to that of the PCIJ, and it was questioned whether a new court should even be created. During the San Francisco Conference, which took place from 25 April to 26 June 1945 and involved 50 countries, it was decided that an entirely new court should be established as a principal organ of the new United Nations. The statute of this court would form an integral part of the United Nations Charter, which, to maintain continuity, expressly held that the Statute of the International Court of Justice (ICJ) was based upon that of the PCIJ.\nConsequently, the PCIJ convened for the last time in October 1945 and resolved to transfer its archives to its successor, which would take its place at the Peace Palace. The judges of the PCIJ all resigned on 31 January 1946, with the election of the first members of the ICJ taking place the following February at the First Session of the United Nations General Assembly and Security Council. In April 1946, the PCIJ was formally dissolved, and the ICJ, in its first meeting, was elected President Jos\u00e9 Gustavo Guerrero of El Salvador, who had served as the last president of the PCIJ. The court also appointed members of its Registry, mainly drawn from that of the PCIJ, and held an inaugural public sitting later that month.\nThe first case was submitted in May 1947 by the United Kingdom against Albania concerning incidents in the Corfu Channel.\nActivities.\nEstablished in 1945 by the UN Charter, the court began work in 1946 as the successor to the Permanent Court of International Justice. The Statute of the International Court of Justice, similar to that of its predecessor, is the main constitutional document constituting and regulating the court.\nThe court's workload covers a wide range of judicial activity. After the court ruled that the United States's covert war against Nicaragua was in violation of international law (\"Nicaragua v. United States\"), the United States withdrew from compulsory jurisdiction in 1986 to accept the court's jurisdiction only on a discretionary basis. Chapter XIV of the United Nations Charter authorizes the UN Security Council to enforce Court rulings. However, such enforcement is subject to the veto power of the five permanent members of the council, which the United States used in the \"Nicaragua\" case.\nComposition.\nThe ICJ is composed of fifteen judges elected to nine-year terms by the UN General Assembly and the UN Security Council from a list of people nominated by the national groups in the Permanent Court of Arbitration. The election process is set out in Articles 4\u201319 of the ICJ Statute. Elections are staggered, with five judges elected every three years to ensure continuity within the court. Should a judge die in office, the practice has generally been to elect a judge in a special election to complete the term. Historically, deceased judges have been replaced by judges from the same region, though not \u2014as often wrongly asserted\u2014 necessarily from the same nationality.\nArticle 3 states that no two judges may be nationals of the same country. According to Article 9, the membership of the court is supposed to represent the \"main forms of civilization and of the principal legal systems of the world\". This has been interpreted to include common law, civil law, socialist law, and Islamic law, while the precise meaning of \"main forms of civilization\" is contested.\nThere is an informal understanding that the seats will be distributed by geographic regions so that there are five seats for Western countries, three for African states (including one judge of Francophone civil law, one of Anglophone common law and one Arab), two for Eastern European states, three for Asian states and two for Latin American and Caribbean states. For most of the court's history, the five permanent members of the United Nations Security Council (France, USSR, China, the United Kingdom, and the United States) have always had a judge serving, thereby occupying three of the Western seats, one of the Asian seats and one of the Eastern European seats. Exceptions have been China not having a judge on the court from 1967 to 1985, during which time it did not put forward a candidate, and British judge Sir Christopher Greenwood being withdrawn as a candidate for election for a second nine-year term on the bench in 2017, leaving no judges from the United Kingdom on the court. Greenwood had been supported by the UN Security Council but failed to get a majority in the UN General Assembly. Indian judge Dalveer Bhandari took the seat instead.\nArticle 6 of the Statute provides that all judges should be \"elected regardless of their nationality among persons of high moral character\" who are either qualified for the highest judicial office in their home states or known as lawyers with sufficient competence in international law. Judicial independence is dealt with specifically in Articles 16\u201318.\nTo insure impartiality, Article 16 of the Charter requires independence from their national governments or other interested parties, stating, \"No member of the Court may exercise any political or administrative function, or engage in any other occupation of a professional nature.\" In addition, Article 17 requires that judges do not show any prior biases on cases before them, specifically, \"No member may participate in the decision of any case in which he has previously taken part as agent, counsel, or advocate for one of the parties, or as a member of a national or international court, or of a commission of enquiry, or in any other capacity.\"\nJudges of the International Court of Justice are entitled to the style of His/Her Excellency. Judges are not able to hold any other post or act as counsel. In practice, members of the court have their own interpretation of these rules and many have chosen to remain involved in outside arbitration and hold professional posts as long as there is no conflict of interest. Former judge Bruno Simma and current judge Georg Nolte have acknowledged that moonlighting should be restricted.\nA judge can be dismissed only by a unanimous vote of the other members of the court. Despite these provisions, the independence of ICJ judges has been questioned. For example, during the \"Nicaragua\" case, the United States issued a communiqu\u00e9 suggesting that it could not present sensitive material to the court because of the presence of judges from the Soviet bloc.\nJudges may deliver joint judgments or give their own separate opinions. Decisions and advisory opinions are by majority, and, in the event of an equal division, the president's vote becomes decisive, which occurred in the \"Legality of the Use by a State of Nuclear Weapons in Armed Conflict\" (Opinion requested by WHO), [1996] ICJ Reports 66. Judges may also deliver separate dissenting opinions.\nIn its 77 years of history, only five women have been elected to the Court, with former UN Special Rapporteur Philip Alston calling for states to take seriously questions of representation in the bench.\nIn 2023, judges elected to take office from 2024 did not include a Russian member, so for the first time, from 2024 there will be no member from the Commonwealth of Independent States. This is also the first time that Russia would not have a judge on the ICJ, even going back to its predecessor, the Soviet Union.\n\"Ad hoc\" judges.\nArticle 31 of the statute sets out a procedure whereby \"ad hoc\" judges sit on contentious cases before the court. The system allows any party to a contentious case (if it otherwise does not have one of that party's nationals sitting on the court) to select one additional person to sit as a judge on that case only. It is thus possible that as many as seventeen judges may sit on one case.\nThe system may seem strange when compared with domestic court processes, but its purpose is to encourage states to submit cases. For example, if a state knows that it will have a judicial officer who can participate in deliberation and offer other judges local knowledge and an understanding of the state's perspective, it may be more willing to submit to the jurisdiction of the court. Although this system does not sit well with the judicial nature of the body, it is usually of little practical consequence. \"Ad hoc\" judges usually (but not always) vote in favour of the state that appointed them and thus cancel each other out.\nChambers.\nGenerally, the court sits as full bench, but in the last fifteen years, it has on occasion sat as a chamber. Articles 26\u201329 of the statute allow the court to form smaller chambers, usually 3 or 5 judges, to hear cases. Two types of chambers are contemplated by Article 26: firstly, chambers for special categories of cases, and second, the formation of \"ad hoc\" chambers to hear particular disputes. In 1993, a special chamber was established, under Article 26(1) of the ICJ statute, to deal specifically with environmental matters (although it has never been used).\n\"Ad hoc\" chambers are more frequently convened. For example, chambers were used to hear the \"Gulf of Maine Case\" (Canada/US). In that case, the parties made clear they would withdraw the case unless the court appointed judges to the chamber acceptable to the parties. Judgments of chambers may have either less authority than full Court judgments or diminish the proper interpretation of universal international law informed by a variety of cultural and legal perspectives. On the other hand, the use of chambers might encourage greater recourse to the court and thus enhance international dispute resolution.\nCurrent composition.\nAs of 2025[ [update]], the composition of the court is as follows:&lt;ref name=\"GA/12285\"&gt;&lt;/ref&gt;\nJurisdiction.\nAs stated in Article 93 of the UN Charter, all 193 UN members are automatically parties to the court's statute. Non-UN members may also become parties to the court's statute under the Article 93(2) procedure, which was used by Switzerland in 1948 and Nauru in 1988, prior to either joining the UN. Once a state is a party to the court's statute, it is entitled to participate in cases before the court. However, being a party to the statute does not automatically give the court jurisdiction over disputes involving those parties. The issue of jurisdiction is considered in the three types of ICJ cases: contentious issues, incidental jurisdiction, and advisory opinions.\nContentious issues.\nIn contentious cases (adversarial proceedings seeking to settle a dispute), the ICJ produces a binding ruling between states that agree to submit to the ruling of the court. Only states may be parties in contentious cases; individuals, corporations, component parts of a federal state, NGOs, UN organs, and self-determination groups are excluded from direct participation, although the court may receive information from public international organizations. However, this does not preclude non-state interests from being the subject of proceedings; for example, a state may bring a case on behalf of one of its nationals or corporations, such as in matters concerning diplomatic protection.\nJurisdiction is often a crucial question for the court in contentious cases. The key principle is that the ICJ has jurisdiction only on the basis of consent. Under Article 36, there are four foundations for the court's jurisdiction:\nAdditionally, the court may have jurisdiction on the basis of tacit consent (\"forum prorogatum\"). In the absence of clear jurisdiction under Article 36, jurisdiction is established if the respondent accepts ICJ jurisdiction explicitly or simply pleads on the merits. This arose in the 1949 Corfu Channel Case (U.K. v. Albania), in which the court held that a letter from Albania stating that it submitted to the jurisdiction of the ICJ was sufficient to grant the court jurisdiction.\nIncidental jurisdiction.\nUntil rendering a final judgment, the court has competence to order interim measures for the protection of the rights of a party to a dispute. One or both parties to a dispute may apply the ICJ for issuing interim measures. In the \"Frontier Dispute\" Case, both parties to the dispute, Burkina Faso and Mali, submitted an application to the court to indicate interim measures. Incidental jurisdiction of the court derives from the Article 41 of its Statute. Similar to the final judgment, the order for interim measures of the court are binding on state parties to the dispute. The ICJ has competence to indicate interim measures only if the \"prima facie\" jurisdiction is satisfied.\nAdvisory opinions.\n An advisory opinion is a function of the court open only to specified United Nations bodies and agencies. The UN Charter grants the General Assembly or the Security Council the power to request the court to issue an advisory opinion on any legal question. Organs of the UN other than the General Assembly or the Security Council require the General Assembly's authorization to request an advisory opinion of the ICJ. These organs of the UN only request an advisory opinion regarding the matters that fall within the scope of their activities. On receiving a request, the court decides which states and organizations might provide useful information and gives them an opportunity to present written or oral statements. Advisory opinions were intended as a means by which UN agencies could seek the court's help in deciding complex legal issues that might fall under their respective mandates.\nIn principle, the court's advisory opinions are only consultative in character but they are influential and widely respected. Certain instruments or regulations can provide in advance that the advisory opinion shall be specifically binding on particular agencies or states, but inherently they are non-binding under the Statute of the court. This non-binding character does not mean that advisory opinions are without legal effect, because the legal reasoning embodied in them reflects the court's authoritative views on important issues of international law. In arriving at them, the court follows essentially the same rules and procedures that govern its binding judgments delivered in contentious cases submitted to it by sovereign states.\nAn advisory opinion derives its status and authority from the fact that it is the official pronouncement of the principal judicial organ of the United Nations.\nAdvisory opinions have often been controversial because the questions asked are controversial or the case was pursued as an indirect way of bringing what is really a contentious case before the court. Examples of advisory opinions can be found in the section advisory opinions in the List of International Court of Justice cases article. One such well-known advisory opinion is the \"Nuclear Weapons Case\".\nOn 23 July 2025, the court issued an advisory opinion regarding the State obligations in respect of climate change,\nupon request by the General Assembly to address two questions: the obligations of States under international law to protect the climate system from anthropogenic emissions for States and for present and future generations, and the legal consequences arising where states, by their acts and omissions, have caused significant harm to the climate system and other parts of the environment.\nThis was the biggest case in the history of the court, with 99 countries and more than 12 intergovernmental organizations heard over two weeks in December 2024.\nRelationship with UN Security Council.\nArticle 94 establishes the duty of all UN members to comply with decisions of the court involving them. If parties do not comply, the issue may be taken before the Security Council for enforcement action. There are obvious problems with such a method of enforcement. If the judgment is against one of the five permanent members of the Security Council or its allies, any resolution on enforcement could then be vetoed by that member. That occurred, for example, after the \"Nicaragua\" case, when Nicaragua brought the issue of the United States' noncompliance with the court's decision before the Security Council. Furthermore, if the Security Council refuses to enforce a judgment against any other state, there is no method of forcing the state to comply. Furthermore, the most effective form to take action for the Security Council, coercive action under Chapter VII of the United Nations Charter, can be justified only if international peace and security are at stake. The Security Council has never done that so far.\nThe relationship between the ICJ and the Security Council, and the separation of their powers, was considered by the court in 1992 in the \"Pan Am\" case. The court had to consider an application from Libya for the order of provisional measures of protection to safeguard its rights, which, it alleged, were being infringed by the threat of economic sanctions by the United Kingdom and United States. The problem was that these sanctions had been authorized by the Security Council, which resulted in a potential conflict between the Chapter VII functions of the Security Council and the judicial function of the court. The court decided, by eleven votes to five, that it could not order the requested provisional measures because the rights claimed by Libya, even if legitimate under the 1971 Montreal Convention, could not be \"prima facie\" regarded as appropriate since the action was ordered by the Security Council. In accordance with Article 103 of the UN Charter, obligations under the Charter took precedence over other treaty obligations. Nevertheless, the court declared the application admissible in 1998. A decision on the merits has not been given since the parties (United Kingdom, United States, and Libya) settled the case out of court in 2003.\nThere was a marked reluctance on the part of a majority of the court to become involved in a dispute in such a way as to bring it potentially into conflict with the council. The court stated in the \"Nicaragua\" case that there is no necessary inconsistency between action by the Security Council and adjudication by the ICJ. However, when there is room for conflict, the balance appears to be in favour of the Security Council.\nShould either party fail \"to perform the obligations incumbent upon it under a judgment rendered by the Court\", the Security Council may be called upon to \"make recommendations or decide upon measures\" if the Security Council deems such actions necessary. In practice, the court's powers have been limited by the unwillingness of the losing party to abide by the court's ruling and by the Security Council's unwillingness to impose consequences. However, in theory, \"so far as the parties to the case are concerned, a judgment of the Court is binding, final and without appeal\", and \"by signing the Charter, a State Member of the United Nations undertakes to comply with any decision of the International Court of Justice in a case to which it is a party.\"\nFor example, the United States had previously accepted the court's compulsory jurisdiction upon its creation in 1946 but in 1984, after \"Nicaragua v. United States\", withdrew its acceptance following the court's judgment that called on the US to \"cease and to refrain\" from the \"unlawful use of force\" against the government of Nicaragua. The court ruled (with only the American judge dissenting) that the United States was \"in breach of its obligation under the Treaty of Friendship with Nicaragua not to use force against Nicaragua\" and ordered the United States to pay war reparations.\nLaw applied.\nWhen deciding cases, the court applies international law as summarized in of the ICJ Statute, which provides that in arriving at its decisions the court shall apply international conventions, international custom and the \"general principles of law recognized by civilized nations.\" It may also refer to academic writing (\"the teachings of the most highly qualified publicists of the various nations\") and previous judicial decisions to help interpret the law although the court is not formally bound by its previous decisions under the doctrine of \"stare decisis\". makes clear that the common law notion of precedent or \"stare decisis\" does not apply to the decisions of the ICJ. The court's decision binds only the parties to that particular controversy. Under 38(1)(d), however, the court may consider its own previous decisions and frequently cites them.\nIf the parties agree, they may also grant the court the liberty to decide \"ex aequo et bono\" (\"out of equality, and for the good\"), granting the ICJ the freedom to make an equitable decision based on what is fair under the circumstances. As of January\u00a02025[ [update]], that provision has not been used in the court's history. As of September\u00a02025[ [update]], the International Court of Justice has dealt with 201 cases.\nProcedure.\nThe ICJ is vested with the power to make its own rules. Court procedure is set out in the \"Rules of Court of the International Court of Justice 1978\" (as amended on 29 September 2005).\nCases before the ICJ will follow a standard pattern. The case is lodged by the applicant, which files a written memorial setting out the basis of the court's jurisdiction and the merits of its claim. The respondent may accept the court's jurisdiction and file its own memorial on the merits of the case.\nPreliminary objections.\nA respondent that does not wish to submit to the jurisdiction of the court may raise preliminary objections. Any such objections must be ruled upon before the court can address the merits of the applicant's claim. Often, a separate public hearing is held on the preliminary objections and the court will render a judgment. Respondents normally file preliminary objections to the jurisdiction of the court and/or the admissibility of the case. Inadmissibility refers to a range of arguments about factors the court should take into account in deciding jurisdiction, such as the fact that the issue is not justiciable or that it is not a \"legal dispute\".\nIn addition, objections may be made because all necessary parties are not before the court. If the case necessarily requires the court to rule on the rights and obligations of a state that has not consented to the court's jurisdiction, the court does not proceed to issue a judgment on the merits.\nIf the court decides it has jurisdiction and the case is admissible, the respondent then is required to file a Memorial addressing the merits of the applicant's claim. Once all written arguments are filed, the court holds a public hearing on the merits.\nOnce a case has been filed, any party (usually the applicant) may seek an order from the court to protect the \"status quo\" pending the hearing of the case. Such orders are known as Provisional (or Interim) Measures and are analogous to interlocutory injunctions in United States law. Article 41 of the statute allows the court to make such orders. The court must be satisfied to have \"prima facie\" jurisdiction to hear the merits of the case before it grants provisional measures.\nApplications to intervene.\nIn cases in which a third state's interests are affected, that state may be permitted to intervene in the case and participate as a full party. Under Article 62, a state \"with an interest of a legal nature\" may apply; however, it is within the court's discretion whether or not to allow the intervention. Intervention applications are rare, and the first successful application occurred only in 1991.\nJudgment and remedies.\nOnce deliberation has taken place, the court issues a majority opinion. Individual judges may issue concurring opinions (if they agree with the outcome reached in the judgment of the court but differ in their reasoning) or dissenting opinions (if they disagree with the majority). No appeal is possible, but any party may ask for the court to clarify if there is a dispute as to the meaning or scope of the court's judgment.\nCriticisms.\nThe International Court has been criticized with respect to its rulings, its procedures, and its authority. As with criticisms of the United Nations, many critics and opponents of the court refer to the general authority assigned to the body by member states through its Charter, rather than to specific problems with the composition of judges or their rulings. Major criticisms include the following:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14919", "revid": "4842600", "url": "https://en.wikipedia.org/wiki?curid=14919", "title": "ISBN", "text": "Unique numeric book identifier since 1970\nThe International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.\nA different ISBN is assigned to each separate edition and variation of a publication, but not to a simple reprinting of an existing item. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN, but an unchanged reprint of the hardcover edition keeps the same ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\nThe first version of the ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (any 9-digit SBN can be converted to a 10-digit ISBN by prefixing it with a zero).\nPrivately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns ISBNs to such books on its own initiative.\nA separate identifier code of a similar kind, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\nHistory.\nThe Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. In 1965, British bookseller and stationers WHSmith announced plans to implement a standard numbering system for its books. They hired consultants to work on their behalf, and the system was devised by Gordon Foster, emeritus professor of statistics at Trinity College Dublin. The International Organization for Standardization (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the \"Father of the ISBN\") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).\nThe 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.\nAn SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of \"Mr. J. G. Reeder Returns\", published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, \"Woodstock Handmade Houses\" had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), and it cost US$.\nSince 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with \"Bookland\" European Article Numbers, which have 13 digits. Since 2016, ISBNs have also been used to identify mobile games by China's Administration of Press and Publication.\nThe United States, with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the Republic of Korea (329,582), Germany (284,000), China (263,066), the UK (188,553) and Indonesia (144,793). Lifetime ISBNs registered in the United States are over 39 million as of 2020.\nOverview.\nA separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book must each have a different ISBN assigned to it. The ISBN of an electronic publication is often colloquially called \"eISBN\", however no such notation is present in the documentation. Whether a publication is electronic or print cannot be inferred from the ISBN only. The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).\nSection 5 of the International ISBN Agency's official user manual describes the structure of the 13-digit ISBN, as follows:\nA 13-digit ISBN can be separated into its parts (\"prefix element\", \"registration group\", \"registrant\", \"publication\" and \"check digit\"), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (\"registration group\", \"registrant\", \"publication\" and \"check digit\") of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits. Online tools such the ISBN Converter provided by the Library of Congress and ISBNBarcode.org can assist with hyphenation and generate standard barcodes.\nIssuing process.\nISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.\nA full directory of ISBN agencies is available on the International ISBN Agency website. A list for a few countries is given below:\nRegistration group element.\nThe ISBN registration group element is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979), and can be separated between hyphens, such as \"978-1-...\". Registration groups have primarily been allocated within the 978 prefix element - many inherited from the 10 digit ISBN. The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0\u20135, 600\u2013631, 65, 7, 80\u201394, 950\u2013989, 9910\u20139989, and 99901\u201399993. Books published in rare languages typically have longer group elements.\nWithin the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.\nThe original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN.\nRegistrant element.\nThe national ISBN agency assigns the registrant element (cf. ) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.\nThe International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the https://. This database is freely searchable over the internet.\nPublishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.\nBy using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.\nEnglish-language pattern.\nEnglish-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:\nCheck digits.\nA check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain the letter 'X'.\nISBN-10 check digits.\nAccording to the 2001 edition of the International ISBN Agency's official user manual, the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if x is the ith digit (numbering from left to right), then x10 must be chosen such that:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nFor example, for an ISBN-10 of 0-306-40615-2:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nFormally, using modular arithmetic, this is rendered\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;formula_1\nIt is also true for ISBN 10s that the sum of all ten digits, each multiplied by its weight in \"ascending\" order from 1 to 10, is a multiple of 11. For this example:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nFormally, this is rendered\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;formula_2\nThe two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN 10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN 10s with eight identical digits and two transposed digits (these proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number). The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN\u2014the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.\nIn contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).\nISBN-10 check digit calculation.\nEach of the first nine digits of the 10-digit ISBN\u2014excluding the check digit itself\u2014is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.\nFor example, the check digit for an ISBN-10 of 0-306-40615-\"?\" is calculated as follows:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nAdding 2 to 130 gives a multiple of 11 (because 132 = 12\u00d711)\u2014this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of formula_3 required to satisfy this condition is 10, then an 'X' should be used.\nAlternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11 \u2212 0 \n 11, which is invalid. (Strictly speaking, the \"first\" \"modulo 11\" is not needed, but it may be considered to simplify the calculation.)\nFor example, the check digit for the ISBN of 0-306-40615-\"?\" is calculated as follows:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nThus the check digit is 2.\nIt is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding codice_1 into codice_2 computes the necessary multiples:\n// Returns ISBN error syndrome, zero for a valid ISBN, non-zero for an invalid one.\n// digits[i] must be between 0 and 10.\nint CheckISBN(int const digits[10]) {\n int i, s = 0, t = 0;\n for (i = 0; i &lt; 10; ++i) {\n t += digits[i];\n s += t;\n return s % 11;\nThe modular reduction can be done once at the end, as shown above (in which case codice_2 could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or codice_2 and codice_1 could be reduced by a conditional subtract after each addition.\nISBN-13 check digit calculation.\nAppendix 1 of the International ISBN Agency's official user manual describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both.\nFormally, using modular arithmetic, this is rendered:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;formula_4\nThe calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results.\nFor example, the ISBN-13 check digit of 978-0-306-40615-\"?\" is calculated as follows:\n s = 9\u00d71 + 7\u00d73 + 8\u00d71 + 0\u00d73 + 3\u00d71 + 0\u00d73 + 6\u00d71 + 4\u00d73 + 0\u00d71 + 6\u00d73 + 1\u00d71 + 5\u00d73\n = 9 + 21 + 8 + 0 + 3 + 0 + 6 + 12 + 0 + 18 + 1 + 15\n = 93\n 93 / 10 = 9 remainder 3\n 10 \u2013 3 = 7\nThus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.\nIn general, the ISBN check digit is calculated as follows.\nLet\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nThen\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nThis check system\u2014similar to the UPC check digit formula\u2014does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3 \u00d7 6 + 1 \u00d7 1 \n 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3 \u00d7 1 + 1 \u00d7 6 \n 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0\u20139 to express the check digit.\nAdditionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).\nISBN-10 to ISBN-13 conversion.\nA 10-digit ISBN is converted to a 13-digit ISBN by prepending \"978\" to the ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.\nErrors in usage.\nPublishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, is shared by two books\u2014\"Ninja gaiden: a novel based on the best-selling game by Tecmo\" (1990) and \"Wacky laws\" (1997), both published by Scholastic.\nMost libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\". The International Union Library Catalog (WorldCat OCLC\u2014Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library.\nEAN format used in barcodes, and upgrading.\nThe barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number \"978\", the Bookland \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1\u00d7 and 3\u00d7 weighting on alternating digits).\nPartly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. As of 2011[ [update]], all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0, while 979-1 to 979-9 are used for ISBN, 979-8- used for USA.\nPublisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.\nBarcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes &amp; Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN in North America.\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14921", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=14921", "title": "IP address", "text": "Numerical label used to identify a network interface in an IP network\nAn Internet Protocol address (IP address) is a numerical label such as that is assigned to a device connected to a computer network that uses the Internet Protocol for communication. IP addresses serve two main functions: network interface identification, and location addressing.\nInternet Protocol version 4 (IPv4) was the first standalone specification for the IP address, and has been in use since 1983. IPv4 addresses are defined as a 32-bit number, which became too small to provide enough addresses as the internet grew, leading to IPv4 address exhaustion over the 2010s. Its designated successor, IPv6, uses 128 bits for the IP address, giving it a larger address space. Although IPv6 deployment has been ongoing since the mid-2000s, both IPv4 and IPv6 are still used side-by-side as of 2025[ [update]].\nIP addresses are usually displayed in a human-readable notation, but systems may use them in various different computer number formats. CIDR notation can also be used to designate how much of the address should be treated as a routing prefix. For example, indicates that 24 significant bits of the address are the prefix, with the remaining 8 bits used for host addressing. This is equivalent to the historically used subnet mask (in this case, ).\nThe IP address space is managed globally by the Internet Assigned Numbers Authority (IANA) and the five regional Internet registries (RIRs). IANA assigns blocks of IP addresses to the RIRs, which are responsible for distributing them to local Internet registries in their region such as internet service providers (ISPs) and large institutions. Some addresses are reserved for private networks and are not globally unique.\nWithin a network, the network administrator assigns an IP address to each device. Such assignments may be on a \"static\" (fixed or permanent) or \"dynamic\" basis, depending on network practices and software features. Some jurisdictions consider IP addresses to be personal data.\nFunction.\nAn IP address serves two principal functions: it identifies the host, or more specifically, its network interface, and it provides the location of the host in the network, and thus, the capability of establishing a path to that host. Its role has been characterized as follows: \"A name indicates what we seek. An address indicates where it is. A route indicates how to get there.\" The header of each IP packet contains the IP address of the sending host and that of the destination host.\nIP versions.\nTwo versions of the Internet Protocol are in common use on the Internet today. The original version of the Internet Protocol that was first deployed in 1983 in the ARPANET, the predecessor of the Internet, is Internet Protocol version 4 (IPv4).\nBy the early 1990s, the rapid exhaustion of IPv4 address space available for assignment to Internet service providers and end-user organizations prompted the Internet Engineering Task Force (IETF) to explore new technologies to expand addressing capability on the Internet. The result was a redesign of the Internet Protocol which became eventually known as Internet Protocol Version 6 (IPv6) in 1995.\nIPv6 technology was in various testing stages until the mid-2000s when commercial production deployment commenced.\nToday, these two versions of the Internet Protocol are in simultaneous use. Among other technical changes, each version defines the format of addresses differently. Because of the historical prevalence of IPv4, the generic term \"IP address\" typically still refers to the addresses defined by IPv4. The gap in version sequence between IPv4 and IPv6 resulted from the assignment of version 5 to the experimental Internet Stream Protocol in 1979 which, however, was never referred to as \"IPv5\".\nVersions 1 to 9 were defined, but only 4 and 6 ever gained widespread use. Transmission Control Protocol (the protocol suite later referred to as \"TCP/IP\") versions 1 and 2 were published in 1974 and 1977. Version 3 was defined in 1978, and version 3.1 is the first version in which TCP is separated from IP. Version 6 is a synthesis of several suggested versions, version 6 \"Simple Internet Protocol\", version 7 \"TP/IX: The Next Internet\", version 8 \"P. Internet Protocol\", and version 9 \"TCP and UDP with Bigger Addresses (TUBA)\".\nSubnetworks.\nIP networks may be divided into subnetworks in both IPv4 and IPv6. For this purpose, an IP address is recognized as consisting of two parts: the \"network prefix\" in the high-order bits and the remaining bits called the \"rest field\", \"host identifier\", or \"interface identifier\" (IPv6), used for host numbering within a network. The subnet mask or CIDR notation determines how the IP address is divided into network and host parts.\nThe term \"subnet mask\" is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the \"routing prefix\". For example, an IPv4 address and its subnet mask may be and , respectively. The CIDR notation for the same IP address and subnet is , because the first 24 bits of the IP address indicate the network and subnet.\nIPv4 addresses.\nAn IPv4 address has a size of 32 bits, which limits the address space to (232) addresses. Of this number, some addresses are reserved for special purposes such as private networks (\u224818\u00a0million addresses) and multicast addressing (\u2248270\u00a0million addresses).\nIPv4 addresses are usually represented in dot-decimal notation, consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., . Each part represents a group of 8 bits (an octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.\nSubnetting history.\nIn the early stages of development of the Internet Protocol, the network number was always the highest order octet (most significant eight bits). Because this method allowed for only 256 networks, it soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the addressing specification was revised with the introduction of classful network architecture.\nClassful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the \"class\" of the address. Three classes (\"A\", \"B\", and \"C\") were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes (\"B\" and \"C\"). The following table gives an overview of this now-obsolete system.\nClassful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of networking in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes. Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.\nPrivate addresses.\nEarly network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be globally unique. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.\nComputers not connected to the Internet, such as factory machines that communicate only with each other via TCP/IP, need not have globally unique IP addresses. Today, such private networks are widely used and typically connect to the Internet with network address translation (NAT), when needed.\nThree non-overlapping ranges of IPv4 addresses for private networks are reserved. These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry. Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets; for example, many home routers automatically use a default address range of through ().\nIPv6 addresses.\nIn IPv6, the address size was increased from 32 bits in IPv4 to 128 bits, thus providing up to 2128 (approximately ) addresses. This is deemed sufficient for the foreseeable future.\nThe intent of the new design was not to provide just a sufficient quantity of addresses, but also redesign routing in the Internet by allowing more efficient aggregation of subnetwork routing prefixes. This resulted in slower growth of routing tables in routers. The smallest possible individual allocation is a subnet for 264 hosts, which is the square of the size of the entire IPv4 Internet. At these levels, actual address utilization ratios will be small on any IPv6 network segment. The new design also provides the opportunity to separate the addressing infrastructure of a network segment, i.e. the local administration of the segment's available space, from the addressing prefix used to route traffic to and from external networks. IPv6 has facilities that automatically change the routing prefix of entire networks, should the global connectivity or the routing policy change, without requiring internal redesign or manual renumbering.\nThe large number of IPv6 addresses allows large blocks to be assigned for specific purposes and, where appropriate, to be aggregated for efficient routing. With a large address space, there is no need to have complex address conservation methods as used in CIDR.\nAll modern desktop and enterprise server operating systems include native support for IPv6, but it is not yet widely deployed in other devices, such as residential networking routers, voice over IP (VoIP) and multimedia equipment, and some networking hardware.\nPrivate addresses.\nJust as IPv4 reserves addresses for private networks, blocks of addresses are set aside in IPv6. In IPv6, these are referred to as unique local addresses (ULAs). The routing prefix is reserved for this block, which is divided into two blocks with different implied policies. The addresses include a 40-bit pseudorandom number that minimizes the risk of address collisions if sites merge or packets are misrouted.\nEarly practices used a different block for this purpose (), dubbed site-local addresses. However, the definition of what constituted a \"site\" remained unclear and the poorly defined addressing policy created ambiguities for routing. This address type was abandoned and must not be used in new systems.\nAddresses starting with , called link-local addresses, are assigned to interfaces for communication on the attached link. The addresses are automatically generated by the operating system for each network interface. This provides instant and automatic communication between all IPv6 hosts on a link. This feature is used in the lower layers of IPv6 network administration, such as for the Neighbor Discovery Protocol.\nPrivate and link-local address prefixes may not be routed on the public Internet.\nIP address assignment.\nIP addresses are assigned to a host either dynamically as they join the network, or persistently by configuration of the host hardware or software. Persistent configuration is also known as using a static IP address. In contrast, when a computer's IP address is assigned each time it restarts, this is known as using a dynamic IP address.\nDynamic IP addresses are assigned by network using Dynamic Host Configuration Protocol (DHCP). DHCP is the most frequently used technology for assigning addresses. It avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows devices to share the limited address space on a network if only some of them are online at a particular time. Typically, dynamic IP configuration is enabled by default in modern desktop operating systems.\nThe address assigned with DHCP is associated with a \"lease\" and usually has an expiration period. If the lease is not renewed by the host before expiry, the address may be assigned to another device. Some DHCP implementations attempt to reassign the same IP address to a host, based on its MAC address, each time it joins the network. A network administrator may configure DHCP by allocating specific IP addresses based on MAC address.\nDHCP is not the only technology used to assign IP addresses dynamically. Bootstrap Protocol is a similar protocol and predecessor to DHCP. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol.\nComputers and equipment used for the network infrastructure, such as routers and mail servers, are typically configured with static addressing.\nIn the absence or failure of static or dynamic address configurations, an operating system may assign a link-local address to a host using stateless address autoconfiguration.\nSticky dynamic IP address.\n\"Sticky\" is an informal term used to describe a dynamically assigned IP address that seldom changes. IPv4 addresses, for example, are usually assigned with DHCP, and a DHCP service \"can\" use rules that maximize the chance of assigning the same address each time a client asks for an assignment. In IPv6, a prefix delegation can be handled similarly, to make changes as rare as feasible. In a typical home or small-office setup, a single router is the only device visible to an Internet service provider (ISP), and the ISP may try to provide a configuration that is as stable as feasible, i.e. \"sticky\". On the local network of the home or business, a local DHCP server may be designed to provide sticky IPv4 configurations, and the ISP may provide a sticky IPv6 prefix delegation, giving clients the option to use sticky IPv6 addresses. \"Sticky\" should not be confused with \"static\"; sticky configurations have no guarantee of stability, while static configurations are used indefinitely and only changed deliberately.\nAddress autoconfiguration.\nAddress block is defined for the special use of link-local addressing for IPv4 networks. In IPv6, every interface, whether using static or dynamic addresses, also receives a link-local address automatically in the block . These addresses are only valid on the link, such as a local network segment or point-to-point connection, to which a host is connected. These addresses are not routable and, like private addresses, cannot be the source or destination of packets traversing the Internet.\nWhen the link-local IPv4 address block was reserved, no standards existed for mechanisms of address autoconfiguration. Filling the void, Microsoft developed a protocol called Automatic Private IP Addressing (APIPA), whose first public implementation appeared in Windows 98. APIPA has been deployed on millions of machines and became a \"de facto\" standard in the industry. In May 2005, the IETF defined a formal standard for it.\nAddressing conflicts.\nAn IP address conflict occurs when two devices on the same local physical or wireless network claim to have the same IP address. A second assignment of an address generally stops the IP functionality of one or both of the devices. Many modern operating systems notify the administrator of IP address conflicts. When IP addresses are assigned by multiple people and systems with differing methods, any of them may be at fault. If one of the devices involved in the conflict is the default gateway access beyond the LAN for all devices on the LAN, all devices may be impaired.\nRouting.\nIP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.\nUnicast addressing.\nThe most common concept of an IP address is in unicast addressing, available in both IPv4 and IPv6. It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but a device or host may have more than one unicast address. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.\nBroadcast addressing.\nBroadcasting is an addressing technique available in IPv4 to address data to all possible destinations on a network in one transmission operation as an \"all-hosts broadcast\". All receivers capture the network packet. The address is used for network broadcast. In addition, a more limited directed broadcast uses the all-ones host address with the network prefix. For example, the destination address used for directed broadcast to devices on the network is .\nIPv6 does not implement broadcast addressing and replaces it with multicast to the specially defined all-nodes multicast address.\nMulticast addressing.\nA multicast address is associated with a group of interested receivers. In IPv4, addresses through (the former Class D addresses) are designated as multicast addresses. IPv6 uses the address block with the prefix for multicast. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all interested receivers (those that have joined the corresponding multicast group).\nAnycast addressing.\nLike broadcast and multicast, anycast is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is closest in the network. Anycast addressing is a built-in feature of IPv6. In IPv4, anycast addressing is implemented with Border Gateway Protocol using the shortest-path metric to choose destinations. Anycast methods are useful for global load balancing and are commonly used in distributed DNS systems.\nGeolocation.\nA host may use geolocation to deduce the geographic position of its communicating peer. This is typically done by retrieving geolocation info about the IP address of the other node from a database.\nPublic address.\nA public IP address is a globally routable unicast IP address, meaning that the address is not an address reserved for use in private networks, such as those reserved by , or the various IPv6 address formats of local scope or site-local scope, for example for link-local addressing. Public IP addresses may be used for communication between hosts on the global Internet.\nIn a home situation, a public IP address is the IP address assigned to the home's network by the ISP. In this case, it is also locally visible by logging into the router configuration.\nMost public IP addresses change relatively often. Any type of IP address that changes is called a dynamic IP address. In home networks, the ISP usually assigns a dynamic IP. If an ISP gave a home network an unchanging address, it is more likely to be abused by customers who host websites from home, or by hackers who can try the same IP address over and over until they breach a network.\nAddress translation.\nMultiple client devices can appear to share an IP address, either because they are part of a shared web hosting service environment or because an IPv4 network address translator (NAT) or proxy server acts as an intermediary agent on behalf of the client, in which case the real originating IP address is masked from the server receiving a request. A common practice is to have a NAT mask many devices in a private network. Only the public interface(s) of the NAT needs to have an Internet-routable address.\nThe NAT device maps different IP addresses on the private network to different TCP or UDP port numbers on the public network. In residential networks, NAT functions are usually implemented in a residential gateway. In this scenario, the computers connected to the router have private IP addresses and the router has a public address on its external interface to communicate on the Internet. The internal computers appear to share one public IP address.\nLaw.\nIn March 2024, the Supreme Court of Canada decided that IP addresses were protected private information under the \"Canadian Charter of Rights and Freedoms\", with police searches requiring a warrant in order to obtain them. IP addresses are considered personal data by the European Commission and are protected by the \"General Data Protection Regulation\".\nDiagnostic tools.\nComputer operating systems provide various diagnostic tools to examine network interfaces and address configuration. Microsoft Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems may use ifconfig, netstat, route, lanstat, fstat, and iproute2 utilities to accomplish the task.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14922", "revid": "50435832", "url": "https://en.wikipedia.org/wiki?curid=14922", "title": "If and only if", "text": "Logical connective\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nLogical symbols representing \"iff\"\u00a0\u00a0\nIn logic and related fields such as mathematics and philosophy, \"if and only if\" (often shortened as \"iff\") is paraphrased by the biconditional, a logical connective between statements. The biconditional is true in two cases, where either both statements are true or both are false. The connective is biconditional (a statement of material equivalence), and can be likened to the standard material conditional (\"only if\", equal to \"if ... then\") combined with its reverse (\"if\"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other (i.e. either both statements are true, or both are false), though it is controversial whether the connective thus defined is properly rendered by the English \"if and only if\"\u2014with its pre-existing meaning. For example, \"P if and only if Q\" means that \"P\" is true whenever \"Q\" is true, and the only case in which \"P\" is true is if \"Q\" is also true, whereas in the case of \"P if Q\", there could be other scenarios where \"P\" is true and \"Q\" is false.\nIn writing, phrases commonly used as alternatives to P \"if and only if\" Q include: \"Q is necessary and sufficient for P\", \"for P it is necessary and sufficient that Q\", \"P is equivalent (or materially equivalent) to Q\" (compare with material implication), \"P precisely if Q\", \"P precisely (or exactly) when Q\", \"P exactly in case Q\", and \"P just in case Q\". Some authors regard \"iff\" as unsuitable in formal writing; others consider it a \"borderline case\" and tolerate its use. In logical formulae, logical symbols, such as formula_1 and formula_2, are used instead of these phrases; see below.\nDefinition.\nThe truth table of \"P\" formula_1 \"Q\" is as follows:\nIt is equivalent to that produced by the XNOR gate, and opposite to that produced by the XOR gate.\nUsage.\nNotation.\nThe corresponding logical symbols are \"formula_1\", \"formula_2\", and formula_6, and sometimes \"iff\". These are usually treated as equivalent. However, some texts of mathematical logic (particularly those on first-order logic, rather than propositional logic) make a distinction between these, in which the first, formula_1, is used as a symbol in logic formulas, while formula_2 or formula_6 is used in reasoning about those logic formulas (e.g., in metalogic). In \u0141ukasiewicz's Polish notation, it is the prefix symbol formula_10.\nAnother term for the logical connective, i.e., the symbol in logic formulas, is exclusive nor.\nIn TeX, \"if and only if\" is shown as a long double arrow: formula_11 via command \\iff or \\Longleftrightarrow.\nProofs.\nIn most logical systems, one proves a statement of the form \"P iff Q\" by proving either \"if P, then Q\" and \"if Q, then P\", or \"if P, then Q\" and \"if not-P, then not-Q\". Proving these pairs of statements sometimes leads to a more natural proof, since there are no obvious conditions under which one would infer a biconditional directly. An alternative is to prove the disjunction \"(P and Q) or (not-P and not-Q)\", which itself can be inferred directly from either of its disjuncts\u2014that is, because \"iff\" is truth-functional, \"P iff Q\" follows if P and Q have been shown to be both true, or both false.\nOrigin of iff and pronunciation.\nUsage of the abbreviation \"iff\" first appeared in print in John L. Kelley's 1955 book \"General Topology\". Its invention is often credited to Paul Halmos, who wrote \"I invented 'iff,' for 'if and only if'\u2014but I could never believe I was really its first inventor.\"\nIt is somewhat unclear how \"iff\" was meant to be pronounced. In current practice, the single 'word' \"iff\" is almost always read as the four words \"if and only if\". However, in the preface of \"General Topology\", Kelley suggests that it should be read differently: \"In some cases where mathematical content requires 'if and only if' and euphony demands something less I use Halmos' 'iff'\". The authors of one discrete mathematics textbook suggest: \"Should you need to pronounce iff, really hang on to the 'ff' so that people hear the difference from 'if'\", implying that \"iff\" could be pronounced as .\nUsage in definitions.\nConventionally, definitions are \"if and only if\" statements; some texts \u2014 such as Kelley's \"General Topology\" \u2014 follow this convention, and use \"if and only if\" or \"iff\" in definitions of new terms. However, this usage of \"if and only if\" is relatively uncommon and overlooks the linguistic fact that the \"if\" of a definition is interpreted as meaning \"if and only if\". The majority of textbooks, research papers and articles (including English Wikipedia articles) follow the linguistic convention of interpreting \"if\" as \"if and only if\" whenever a mathematical definition is involved (as in \"a topological space is compact if every open cover has a finite subcover\"). Moreover, in the case of a recursive definition, the \"only if\" half of the definition is interpreted as a sentence in the metalanguage stating that the sentences in the definition of a predicate are the \"only sentences\" determining the extension of the predicate.\nIn terms of Euler diagrams.\nEuler diagrams show logical relationships among events, properties, and so forth. \"P only if Q\", \"if P then Q\", and \"P\u2192Q\" all mean that P is a subset, either proper or improper, of Q. \"P if Q\", \"if Q then P\", and Q\u2192P all mean that Q is a proper or improper subset of P. \"P if and only if Q\" and \"Q if and only if P\" both mean that the sets P and Q are identical to each other.\nMore general usage.\n\"Iff\" is used outside the field of logic as well. Wherever logic is applied, especially in mathematical discussions, it has the same meaning as above: it is an abbreviation for \"if and only if\", indicating that one statement is both necessary and sufficient for the other. This is an example of mathematical jargon (although, as noted above, \"if\" is more often used than \"iff\" in statements of definition).\nThe elements of \"X\" are \"all and only\" the elements of \"Y\" means: \"For any \"z\" in the domain of discourse, \"z\" is in \"X\" if and only if \"z\" is in \"Y\".\"\nWhen \"if\" means \"if and only if\".\nIn their \"\", Russell and Norvig note (page 282), in effect, that it is often more natural to express \"if and only if\" as \"if\" together with a \"database (or logic programming) semantics\". They give the example of the English sentence \"Richard has two brothers, Geoffrey and John\".\nIn a database or logic program, this could be represented simply by two sentences:\nBrother(Richard, Geoffrey).\nBrother(Richard, John).\nThe database semantics interprets the database (or program) as containing \"all\" and \"only\" the knowledge relevant for problem solving in a given domain. It interprets \"only if\" as expressing in the metalanguage that the sentences in the database represent the \"only\" knowledge that should be considered when drawing conclusions from the database. \nIn first-order logic (FOL) with the standard semantics, the same English sentence would need to be represented, using \"if and only if\", with \"only if\" interpreted in the object language, in some such form as:\nformula_12 X(Brother(Richard, X) iff X = Geoffrey or X = John).\nGeoffrey \u2260 John.\nCompared with the standard semantics for FOL, the database semantics has a more efficient implementation. Instead of reasoning with sentences of the form:\n\"conclusion iff conditions\"\nit uses sentences of the form:\n\"conclusion if conditions\"\nto reason forwards from \"conditions\" to \"conclusions\" or backwards from \"conclusions\" to \"conditions\".\nThe database semantics is analogous to the legal principle expressio unius est exclusio alterius (the express mention of one thing excludes all others). Moreover, it underpins the application of logic programming to the representation of legal texts and legal reasoning.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14923", "revid": "50687902", "url": "https://en.wikipedia.org/wiki?curid=14923", "title": "IP", "text": "IP most often refers to:\nIP, Ip, or ip may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14926", "revid": "4770293", "url": "https://en.wikipedia.org/wiki?curid=14926", "title": "List of Italian foods and drinks", "text": "This is a list of Italian foods and drinks. Italian cuisine has developed through centuries of social and political changes, with roots as far back as the 4th century BC. Italian cuisine has its origins in Etruscan, ancient Greek and ancient Roman cuisines. Significant changes occurred with the discovery of the New World and the introduction of potatoes, tomatoes, bell peppers and maize, now central to the cuisine, but not introduced in quantity until the 18th century.\nItalian cuisine includes deeply rooted traditions common to the whole country, as well as all the regional gastronomies, different from each other, especially between the north, the centre and the south of Italy, which are in continuous exchange. Many dishes that were once regional have proliferated with variations throughout the country. Italian cuisine offers an abundance of taste, and is one of the most popular and copied around the world. The most popular dishes and recipes, over the centuries, have often been created by ordinary people more so than by chefs, which is why many Italian recipes are suitable for home and daily cooking, respecting regional specificities.\nItaly is home to 395 Michelin star-rated restaurants. The Mediterranean diet forms the basis of Italian cuisine, rich in pasta, fish, fruits and vegetables. Cheese, cold cuts and wine are central to Italian cuisine, and along with pizza and coffee (especially espresso) form part of Italian gastronomic culture. Desserts have a long tradition of merging local flavours such as citrus fruits, pistachio and almonds with sweet cheeses such as mascarpone and ricotta or exotic tastes such as cocoa, vanilla and cinnamon. Gelato, tiramisu and cassata are among the most famous examples of Italian desserts, cakes and patisserie. Italian cuisine relies heavily on traditional products; the country has a large number of traditional specialities protected under EU law. Italy is the world's largest producer of wine, as well as the country with the widest variety of indigenous grapevine varieties in the world.\nFoods and drinks.\nNote: the \"Other foods and drinks\" section is necessary to list foods without a specific placement.\nSoups, sauces and condiments.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nBread.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCommon pizzas.\nA great number of pizza varieties exist, defined by the choice of toppings and sometimes also crust. There are also several styles of pizza, defined by their preparation method. The following lists feature only the notable ones.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPizza preparation styles.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPasta varieties.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPasta dishes.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nRice dishes.\nRice dishes are very common in northern Italy, especially in the Lombardy and Veneto regions, although rice dishes are found throughout the country.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFish dishes.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nMeat dishes and.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nVegetable dishes.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nWines.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nLiqueurs.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCheeses.\nConsidering the large number of Italian cheeses, only the most famous ones are listed below.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nDesserts and pastries.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOther foods and drinks.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCoffee.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nVinegar.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFruits, vegetables and legumes.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIngredients.\nMost important ingredients (see also: Italian herbs and spices):\nOther common ingredients.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nHerbs and spices.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14928", "revid": "32722751", "url": "https://en.wikipedia.org/wiki?curid=14928", "title": "Isaac Ambrose", "text": "English Puritan clergy\nIsaac Ambrose (1604 \u2013 20 January 1664) was an English Puritan divine. He graduated with a BA from Brasenose College, Oxford, on 1624. He obtained the curacy of St Edmund\u2019s Church, Castleton, Derbyshire, in 1627. He was one of king's four preachers in Lancashire in 1631. He was twice imprisoned by commissioners of array. He worked for the establishment of Presbyterianism; successively at Leeds, Preston, and Garstang, whence he was ejected for nonconformity in 1662. He also published religious works.\nBiography.\nAmbrose was born in 1604. He was the son of Richard Ambrose, vicar of Ormskirk, and was probably descended from the Ambroses of Lowick in Furness, a well-known Roman Catholic family. He entered Brasenose College, Oxford, in 1621, in his seventeenth year.\nHaving graduated B.A. in 1624 and been ordained, Ambroses received in 1627 the little cure of Castleton in Derbyshire. By the influence of William Russell, earl of Bedford, he was appointed one of the king's itinerant preachers in Lancashire, and after living for a time in Garstang, he was selected by the Lady Margaret Hoghton as vicar of Preston. He associated himself with Presbyterianism, and was on the celebrated committee for the ejection of \"scandalous and ignorant ministers and schoolmasters\" during the Commonwealth.\nSo long as Ambrose continued at Preston he was favoured with the warm friendship of the Hoghton family, their ancestral woods and the tower near Blackburn affording him sequestered places for those devout meditations and \"experiences\" that give such a charm to his diary, portions of which are quoted in his \"Prima Media &amp; Ultima\" (1650, 1659). The immense auditory of his sermon (\"Redeeming the Time\") at the funeral of Lady Hoghton was long a living tradition all over the county. On account of the feeling engendered by the civil war Ambrose left his great church of Preston in 1654, and became minister of Garstang, whence, however, in 1662 he was ejected along two thousand ministers who refused to conform (see Great Ejection). His after years were passed among old friends and in quiet meditation at Preston. He died of apoplexy about 20 January 1664.\nCharacter assessment.\nAs a religious writer Ambrose has a vividness and freshness of imagination possessed by scarcely any of the Puritan Nonconformists. Many who have no love for Puritan doctrine, nor sympathy with Puritan experience, have appreciated the pathos and beauty of his writings, and his \"Looking unto Jesus\" long held its own in popular appreciation with the writings of John Bunyan.\nDr Edmund Calamy the Elder (1600\u20131666) wrote about him:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Ambrose was a man of that substantial worth, that eminent piety, and that exemplary life, both as a minister and a Christian, that it is to be lamented the world should not have the benefit of particular memoirs of him.\nIn the opinion of John Eglington Bailey (his biographer in the DNB), his character has been misrepresented by Wood. He was of a peaceful disposition; and though he put his name to the fierce \"Harmonious Consent\", he was not naturally a partisan. He evaded the political controversies of the time. His gentleness of character and earnest presentation of the gospel attached him to his people. He was much given to secluding himself, retiring every May into the woods of Hoghton Tower and remaining there a month.\nBailey continues that Dr. Halley justly characterises him as the most meditative puritan of Lancashire. This quality pervades his writings, which abound, besides, in deep feeling and earnest piety. Mr. Hunter has called attention to his recommendation of diaries as a means of advancing personal piety, and has remarked, in reference to the fragments from Ambrose's diary quoted in the \"Media\", that \"with such passages before us we cannot but lament that the carelessness of later times should have suffered such a curious and valuable document to perish; for perished it is to be feared it has\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14929", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=14929", "title": "Internet/History", "text": ""}
{"id": "14931", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=14931", "title": "IteratedPrisonersDilemma", "text": ""}
{"id": "14932", "revid": "3113", "url": "https://en.wikipedia.org/wiki?curid=14932", "title": "Iterated Prisoners Dilemma", "text": ""}
{"id": "14933", "revid": "157970", "url": "https://en.wikipedia.org/wiki?curid=14933", "title": "International Convention for the Regulation of Whaling", "text": "International environmental agreement\nThe International Convention for the Regulation of Whaling is an international environmental agreement aimed at the \"proper conservation of whale stocks and thus make possible the orderly development of the whaling industry\". It governs the commercial, scientific, and aboriginal subsistence whaling practices of 88 member states.\nThe convention is a successor to the 1931 Geneva Convention for Regulation of Whaling and the 1937 International Agreement for the Regulation of Whaling, established in response to the overexploitation of whales in the post-World War I period. Neither instrument was effective, but each provided the framework for the International Convention for the Regulation of Whaling, which was spearheaded by the United States and signed by 15 countries in Washington, D.C., on 3 December 1946; the convention took effect on 10 November 1948. A protocol broadening the scope of the convention's enforcement was signed on 19 November 1956.\nThe objectives of the International Convention for the Regulation of Whaling are to protect all whale species from overhunting; establish a system of international regulation for whale fisheries to ensure proper conservation and development of whale stocks; and safeguard for future generations the important natural resources represented by whale stocks. The primary instrument implementing these aims is the International Whaling Commission, established by the convention as its main decision-making body. The IWC meets annually and adopts a binding \"schedule\" that regulates catch limits, whaling methods, protected areas, and the right to carry out scientific research involving the killing of whales.\nMembers.\nAs of January 2021, there are 88 parties to the convention. The initial signatories were Argentina, Australia, Brazil, Canada, Chile, Denmark, France, the Netherlands, New Zealand, Norway, Peru, South Africa, the Soviet Union, the United Kingdom and the United States.\nAlthough Norway is party to the convention, it maintains an objection to the 1986 IWC global moratorium and it does not apply to it.\nWithdrawals.\nTen states have withdrawn from the convention since its ratification: Canada, Egypt, Greece, Guatemala, Jamaica, Japan, Mauritius, the Philippines, Seychelles and Venezuela.\nBelize, Brazil, Dominica, Ecuador, Iceland, New Zealand, Norway, Panama, Solomon Islands, Sweden and Uruguay withdrew from the convention temporarily but joined again; the Netherlands withdrew twice, only to join a third time.\nJapan is the most recent member to withdraw, effective June 2019, so as to resume commercial whaling.\nEffectiveness.\nThere have been consistent disagreement over the scope of the convention. The 1946 Convention does not define a 'whale'. Some members of IWC claim that it has the legal competence to regulate catches of only great whales (the baleen whales and the sperm whale). Others believe that all cetaceans, including the smaller dolphins and porpoises, fall within IWC jurisdiction.\nAn analysis by the Carnegie Council determined that while the International Convention for the Regulation of Whaling has had \"ambiguous success\" owing to its internal divisions, it has nonetheless \"successfully managed the historical transition from open whale hunting to highly restricted hunting. It has stopped all but the most highly motivated whale-hunting countries. This success has made its life more difficult, since it has left the hardest part of the problem for last.\""}
{"id": "14934", "revid": "43681902", "url": "https://en.wikipedia.org/wiki?curid=14934", "title": "International Organization for Standardization", "text": "International standards development organization\nThe International Organization for Standardization (ISO ; ; ) is an independent, non-governmental, international standard development organization composed of representatives from the national standards organizations of member countries. \nMembership requirements are given in Article 3 of the ISO Statutes.\nISO was founded on 23 February 1947, and (as of \u00a02024[ [update]]) it has published over 25,000 international standards covering almost all aspects of technology and manufacturing. It has over 800 technical committees (TCs) and subcommittees (SCs) to take care of standards development.\nThe organization develops and publishes international standards in technical and nontechnical fields, including everything from manufactured products and technology to food safety, transport, IT, agriculture, and healthcare. More specialized topics like electrical and electronic engineering are instead handled by the International Electrotechnical Commission. It is headquartered in Geneva, Switzerland. The three official languages of ISO are English, French, and Russian.\nName and abbreviations.\nThe International Organization for Standardization in French is ' and in Russian, (').\nA common misconception is that \"ISO\" is an abbreviation for \"International Standardization Organization\" or a similar title in another language. In fact, the letters do not officially represent an acronym or initialism. The organization provides this explanation of the name:Because 'International Organization for Standardization' would have different acronyms in different languages (IOS in English, OIN in French), our founders decided to give it the short form \"ISO\". \"ISO\" is derived from the Greek word \"\" (, meaning \"equal\"). Whatever the country, whatever the language, the short form of our name is always \"ISO\".During the founding meetings of the new organization, however, the Greek word explanation was not invoked, so this meaning may be a false etymology.\nBoth the name \"ISO\" and the ISO logo are registered trademarks and their use is restricted.\nHistory.\nThe organization that is known today as ISO began in 1926 as the International Federation of the National Standardizing Associations (ISA), which primarily focused on mechanical engineering. The ISA was suspended in 1942 during World War\u00a0II but, after the war, the ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body.\nIn October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the International Organization for Standardization. The organization officially began operations on 23 February 1947.\nISO Standards were originally known as ISO Recommendations (ISO/R), e.g., \"ISO 1\" was issued in 1951 as \"ISO/R 1\".\nStructure and organization.\nISO is a voluntary organization whose members are recognized authorities on standards, each one representing one country. Members meet annually at a General Assembly to discuss the strategic objectives of ISO. The organization is coordinated by a central secretariat based in Geneva.\nA council with a rotating membership of 20 member bodies provides guidance and governance, including setting the annual budget of the central secretariat.\nThe technical management board is responsible for more than 250 technical committees, who develop the ISO standards.\nJoint technical committee with IEC.\nISO has a joint technical committee (JTC) with the International Electrotechnical Commission (IEC) to develop standards relating to information technology (IT). Known as JTC\u00a01 and entitled \"Information technology\", it was created in 1987 and its mission is \"to develop worldwide Information and Communication Technology (ICT) standards for business and consumer applications.\"\nThere was previously also a JTC 2 that was created in 2009 for a joint project to establish common terminology for \"standardization in the field of energy efficiency and renewable energy sources\". It was later disbanded.\nMembership.\nAs of 2025[ [update]], there are 173 national members representing ISO in\u00a0their country, with each country having only one member.\nISO has three membership categories,\nParticipating members are called \"P\" members, as opposed to observing members, who are called \"O\" members.\nFinancing.\nISO is funded by a combination of:\nInternational standards and other publications.\nInternational standards are the main products of ISO. It also publishes technical reports, technical specifications, publicly available specifications, technical corrigenda (corrections), and guides.\nInternational standards\n These are designated using the format \"ISO[/IEC] [/ASTM] [IS] nnnnn[-p]:[yyyy] Title\", where \"nnnnn\" is the number of the standard, \"p\" is an optional part number, \"yyyy\" is the year published, and \"Title\" describes the subject. \"IEC\" for \"International Electrotechnical Commission\" is included if the standard results from the work of ISO/IEC JTC 1 (the ISO/IEC Joint Technical Committee). \"ASTM\" (American Society for Testing and Materials) is used for standards developed in cooperation with ASTM International. \"yyyy\" and \"IS\" are not used for an incomplete or unpublished standard and, under some circumstances, may be left off the title of a published work.\nTechnical reports\n These are issued when a technical committee or subcommittee has collected data of a different kind from that normally published as an International Standard, such as references and explanations. The naming conventions for these are the same as for standards, except \"TR\" prepended instead of \"IS\" in the report's name.\nFor example:\nTechnical and publicly available specifications\n Technical specifications may be produced when \"the subject in question is still under development or where for any other reason there is the future but not immediate possibility of an agreement to publish an International Standard\". A publicly available specification is usually \"an intermediate specification, published prior to the development of a full International Standard, or, in IEC may be a 'dual logo' publication published in collaboration with an external organization\". By convention, both types of specification are named in a manner similar to the organization's technical reports.\nFor example:\nTechnical corrigenda\nWhen partnering with IEC in their joint technical committee, ISO also sometimes issues \"technical corrigenda\" (where \"corrigenda\" is the plural of corrigendum). These are amendments made to existing standards to correct minor technical flaws or ambiguities.\nISO guides\nThese are meta-standards covering \"matters related to international standardization\". They are named using the format \"ISO[/IEC] Guide N:yyyy: Title\".\nFor example:\nDocument copyright.\nISO documents have strict copyright restrictions and ISO charges for most copies. As of 2020[ [update]], the typical cost of a copy of an ISO standard is about US$ or more (and electronic copies typically have a single-user license, so they cannot be shared among groups of people). Some standards by ISO and its official U.S. representative (and, via the U.S. National Committee, the International Electrotechnical Commission) are made freely available.\nStandardization process.\nA standard published by ISO/IEC is the last stage of a long process that commonly starts with the proposal of new work within a committee. Some abbreviations used for marking a standard with its status are:\nAbbreviations used for amendments are:\nOther abbreviations are:\nInternational Standards are developed by ISO technical committees\u00a0(TC) and subcommittees\u00a0(SC) by a process with six steps:\nThe TC/SC may set up working groups\u00a0(WG) of experts for the preparation of a working drafts. Subcommittees may have several working groups, which may have several Sub Groups (SG).\nIt is possible to omit certain stages, if there is a document with a certain degree of maturity at the start of a standardization project, for example, a standard developed by another organization. ISO/IEC directives also allow the so-called \"Fast-track procedure\". In this procedure, a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS), if the document was developed by an international standardizing body recognized by the ISO Council.\nThe first step, a proposal of work (New Proposal), is approved at the relevant subcommittee or technical committee (e.g.,\u00a0SC\u00a029 and JTC\u00a01 respectively in the case of MPEG, the Moving Picture Experts Group). A working group (WG) of experts is typically set up by the subcommittee for the preparation of a working draft (e.g., MPEG is a collection of seven working groups as of 2023). When the scope of a new work is sufficiently clarified, some of the working groups may make an open request for proposals\u2014known as a \"call for proposals\". The first document that is produced, for example, for audio and video coding standards is called a verification model\u00a0(VM) (previously also called a \"simulation and test model\"). When a sufficient confidence in the stability of the standard under development is reached, a working draft\u00a0(WD) is produced. This is in the form of a standard, but is kept internal to working group for revision. When a working draft is sufficiently mature and the subcommittee is satisfied that it has developed an appropriate technical document for the problem being addressed, it becomes a committee draft\u00a0(CD) and is sent to the P-member national bodies of the SC for the collection of formal comments. Revisions may be made in response to the comments, and successive committee drafts may be produced and circulated until consensus is reached to proceed to the next stage, called the \"enquiry stage\".\nAfter a consensus to proceed is established, the subcommittee will produce a draft international standard (DIS), and the text is submitted to national bodies for voting and comment within a period of five months. A document in the DIS stage is available to the public for purchase and may be referred to with its ISO DIS reference number.\nFollowing consideration of any comments and revision of the document, the draft is then approved for submission as a Final Draft International Standard (FDIS) if a two-thirds majority of the P-members of the TC/SC are in favour and if not more than one-quarter of the total number of votes cast are negative. ISO will then hold a ballot among the national bodies where no technical changes are allowed (a yes/no final approval ballot), within a period of two months. It is approved as an International Standard\u00a0(IS) if a two-thirds majority of the P-members of the TC/SC is in favour and not more than one-quarter of the total number of votes cast are negative. After approval, the document is published by the ISO central secretariat, with only minor editorial changes introduced in the publication process before the publication as an International Standard.\nExcept for a relatively small number of standards, ISO standards are not available free of charge, but rather for a purchase fee, which has been seen by some as unaffordable for small open-source projects.\nThe process of developing standards within ISO was criticized around 2007 as being too difficult for timely completion of large and complex standards, and some members were failing to respond to ballots, causing problems in completing the necessary steps within the prescribed time limits. In some cases, alternative processes have been used to develop standards outside of ISO and then submit them for its approval. A more rapid \"fast-track\" approval procedure was used in ISO/IEC JTC 1 for the standardization of Office Open XML (OOXML, ISO/IEC\u00a029500, approved in April 2008), and another rapid alternative \"publicly available specification\" (PAS) process had been used by OASIS to obtain approval of OpenDocument as an ISO/IEC standard (ISO/IEC 26300, approved in May 2006).\nAs was suggested at the time by Martin Bryan, the outgoing convenor (chairman) of working group 1 (WG1) of ISO/IEC JTC 1/SC 34, the rules of ISO were eventually tightened so that participating members that fail to respond to votes are demoted to observer status.\nThe computer security entrepreneur and Ubuntu founder, Mark Shuttleworth, was quoted in a ZDNet blog article in 2008 about the process of standardization of OOXML as saying: \"I think it de-values the confidence people have in the standards setting process\", and alleged that ISO did not carry out its responsibility. He also said that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers, and resellers sympathetic to Office Open XML:\nWhen you have a process built on trust and when that trust is abused, ISO should halt the process... ISO is an engineering old boys club and these things are boring so you have to have a lot of passion\u00a0... then suddenly you have an investment of a lot of money and lobbying and you get artificial results. The process is not set up to deal with intensive corporate lobbying and so you end up with something being a standard that is not clear.\nInternational Workshop Agreements.\nInternational Workshop Agreements (IWAs) are documents that establish a collaboration agreement that allow \"key industry players to negotiate in an open workshop environment\" outside of ISO in a way that may eventually lead to development of an ISO standard.\nProducts named after ISO.\nOn occasion, the fact that many of the ISO-created standards are ubiquitous has led to common use of \"ISO\" to describe the product that conforms to a standard. Some examples of this are:\nISO awards.\nISO presents several awards to acknowledge the valuable contributions made in the realm of international standardization:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nISO divisions.\nSome of the 834 Technical Committees of the International Organization for Standardization (ISO) include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14936", "revid": "48729834", "url": "https://en.wikipedia.org/wiki?curid=14936", "title": "Individualist anarchism", "text": "Branch of anarchism that emphasizes the individual and their will\nIndividualist anarchism or anarcho-individualism is a collection of anarchist currents that generally emphasize the individual and their will over external determinants such as groups, society, traditions, and ideological systems. \nIndividualist anarchism can be divided into two main distinct movements, each with its own ideological orientations and choices. On one hand, there is American individualist anarchism, which began with Warren in the 1860s. It focuses primarily on economic freedom, drawing upon Stirner's egoist anarchism and Proudhon's mutualism, and develops perspectives that are notably financial in nature. Most American individualist anarchists of the 19th century advocated mutualism, a libertarian socialist form of market socialism, or a free-market socialist form of classical economics.\nOn the other hand, European individualist anarchism emerged between 1885 and 1895 in the labour movement. Much less studied and not directly connected to American individualist anarchism, with virtually no influence by Proudhon or Stirner for example, it generally consisted of militants with very different outlooks\u2014particularly marked by strong radicalism, general adherence to anarchist communism, and often highly radical positions, including significant support for revolutionary violence and propaganda of the deed. The European movement was also distinguished by its strong opposition to the emerging anarcho-syndicalism of the same period, its rejection of the distinction between bourgeoisie and proletariat\u2014seen as social constructs of capitalism to be abolished\u2014and its close affinity with the social outlook of the women, sex workers or criminals. This helps explain its rapid association with the rise of anarcha-feminism or illegalism in Europe, for example.\nAlthough usually contrasted with social anarchism, both individualist and social anarchism have influenced each other. Among the early influences on American individualist anarchism Josiah Warren (sovereignty of the individual), Max Stirner (egoism), Lysander Spooner (natural law), Pierre-Joseph Proudhon (mutualism), Henry David Thoreau (transcendentalism), Herbert Spencer (law of equal liberty) and Anselme Bellegarrigue (civil disobedience). For European individualist anarchism, one can find Pierre Martinet, Vittorio Pini, Cl\u00e9ment Duval, Errico Malatesta, \u00c9mile Henry, Zo d'Axa, or groups such as the Intransigeants of London and Paris or the Pieds plats. \nWithin anarchism, American individualist anarchism is primarily a literary phenomenon while social anarchism has been the dominant form of anarchism, emerging in the late 19th century as a distinction from \"individualist anarchism\" after anarcho-communism replaced collectivist anarchism as the dominant tendency. American individualist anarchism has been described by some as the anarchist branch most influenced by and tied to liberalism (specifically classical liberalism), or as a part of the liberal or liberal-socialist wing of anarchism \u2014 in contrast to the collectivist or communist wing of anarchism and libertarian socialism. However, others suggest a softer divide, seeing individualist anarchists as sharing with social anarchists an opposition to state, capitalism and authority, while diverging (a) due to their evolutionary approach to change, preferring the creation of alternative institutions, such as mutual banks or communes, and (b) in their preference for a market-based system of distribution over the need-based system advocated by social anarchists. The very idea of an individualist\u2013socialist divide is also contested by those who argue that individualist anarchism is largely socialistic and can be considered a form of individualist socialism, with non-Lockean individualism encompassing socialism. Lastly, some anarcho-capitalists claim anarcho-capitalism is part of the individualist anarchist tradition, while others disagree and reject the notion that anarcho-capitalism is a genuinely anarchist belief system or movement. \n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nGeneral and ideological overview.\nDistinction.\nAmerican and European individualist anarchism are distinct movements that can be considered independently from one another. This is largely due to the fact that the ideological perspectives within these currents are highly fragmented and that both movements don't influence each other in their first decades. For example, while Stirner's egoist anarchism and Proudhon's mutualism are major influences on the American individualist anarchist movement, they have little to no impact on the European movement.\nStirner, for instance, was only belatedly received in France by individualist anarchists, who found his ideas unoriginal and articulating concepts they believed they had already developed more effectively themselves. American individualist anarchism is usually way more studied and known than it's European counterpart.\nAmerican individualist anarchism.\nThe term \"individualist anarchism\" is often used as a classificatory term but in very different ways. Some such as the authors of \"An Anarchist FAQ\" use the classification individualist anarchism/social anarchism, while Bob Black contrasts an individualist anarchism and anarcho-capitalism (\"Type 2 anarchism\") less representative of the anarchist tradition with the more representative collectivist anarchism or anarcho-leftism (\"Type 1 anarchism\"). Others such as Geoffrey Ostergaard, who see individualist anarchism as distinctly non-socialist, recognizing anarcho-capitalist as part of the individualist anarchist tradition, use the classification individualist anarchism/socialist anarchism accordingly. However, others do not consider anarcho-capitalism as part of the anarchist movement, arguing that anarchism has historically been an anti-capitalist movement and anarchists reject that it is compatible with capitalism. In addition, an analysis of several individualist anarchists who advocated free-market anarchism shows that it is different from anarcho-capitalism and other capitalist theories due to these individualist anarchists retaining the labor theory of value and socialist doctrines. Other classifications include communal/mutualist anarchism. \nMichael Freeden identifies four broad types of individualist anarchism. Freeden says the first is the type associated with William Godwin that advocates self-government with a \"progressive rationalism that included benevolence to others\". The second type is the amoral self-serving rationality of egoism as most associated with Max Stirner. The third type is \"found in Herbert Spencer's early predictions, and in that of some of his disciples such as Wordsworth Donisthorpe, foreseeing the redundancy of the state in the source of social evolution\". The fourth type retains a moderated form of egoism and accounts for social cooperation through the advocacy of market relationships. Individualist anarchism of different kinds have the following things in common:\nTucker's two socialisms were the state socialism which he associated to the Marxist school and the libertarian socialism that he advocated. What those two schools of socialism had in common was the labor theory of value and the ends, by which anarchism pursued different means.\nThe egoist form of individualist anarchism, derived from the philosophy of Max Stirner, supports the individual doing exactly what he pleases\u2014taking no notice of God, state, or moral rules. To Stirner, rights were \"spooks\" in the mind, and he held that society does not exist but \"the individuals are its reality\"\u2014he supported property by force of might rather than moral right. Stirner advocated self-assertion and foresaw \"associations of egoists\" drawn together by respect for each other's ruthlessness. Another important tendency within individualist anarchist currents emphasizes individual subjective exploration and defiance of social conventions. Individualist anarchist philosophy attracted \"amongst artists, intellectuals and the well-read, urban middle classes in general\".\nAnarchist historian George Woodcock wrote of individualist anarchism's tendency to distrust co-operation beyond the bare necessities for an ascetic life.\nEuropean individualist anarchism.\nIn Europe, the tendency emerged later, during the 1880s and 1890s. Although it emerged later chronologically, this tendency generally did not exhibit causal links with previous American individualist anarchism, at least during its initial period. It emerged in the European labour movement of the time.\nBefore being theorized, the first European individualists, particularly in France, were often militants involved in illegalism or insurrectionarism. Within these circles, European individualist anarchism gradually took shape. In general, anarchists of this tendency shared several intellectual positions identified by Carl Frayne in their thesis regarding the French individualist tendency:\nThis last point is illustrated for example by the soup conferences held in Paris by Pierre Martinet or S\u00e9verine in the winters of the early 1890s. The distinction between the European and American individualist anarchist movements was noted by Errico Malatesta in 1897, when he said that there would be 'the individualist anarchists of Tucker\u2019s school' and 'the individualist anarchists of the communist school'.\nThe European individualist anarchism later on met the American one, such as in the thought, for example, of \u00c9mile Armand, who started including Stirner in his thought and his tendency of the European branch in the interwar period.\nIndividualist European anarchists are described as such by historian C\u00e9line Beaudet in her monograph on free communities in France at the beginning of the 20th century\u2014anarchist experiments often linked to individualist circles, such as the Naturians\u2014which prefigure anarcho-primitivism and engage in calls for a return to 'nature', she writes that:Dissenters among dissenters, 'anarchy within anarchism', [they] are rarely considered for their own sake, either relegated to the closets of History or described through the lens of their detractors: bourgeois or police informants for 'orthodox' anarchists, bandits or criminals for the bourgeois, 'dispersion of tendencies' or 'irresponsible' revolt among historians.\nAmerican individualist anarchism.\nEarly precursors.\nWilliam Godwin.\nWilliam Godwin can be considered an individualist anarchist. Godwin advocated extreme individualism, proposing that all cooperation in labor be eliminated. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us \"of more worth and importance\" than others depending on our utility in bringing about social good. Therefore, he does not believe in equal rights, but the person's life that should be favored that is most conducive to the general good. Godwin opposed government because it infringes on the individual's right to \"private judgement\" to determine which actions most maximize utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, minus the utilitarianism, was developed into a more extreme form later by Stirner.\nGodwin took individualism to the radical extent of opposing individuals performing together in orchestras, writing in \"Political Justice\" that \"everything understood by the term co-operation is in some sense an evil\". He believed democracy to be preferable to other forms of government.\nGodwin's political views were diverse and do not perfectly agree with any of the ideologies that claim his influence as writers of the \"Socialist Standard\", organ of the Socialist Party of Great Britain, consider Godwin both an individualist and a communist; Murray Rothbard did not regard Godwin as being in the individualist camp at all, referring to him as the \"founder of communist anarchism\";&lt;ref name=\"rothbard/burke\"&gt;Rothbard, Murray. \"http:// .\"&lt;/ref&gt; and historian Albert Weisbord considers him an individualist anarchist without reservation.\nPierre-Joseph Proudhon.\nPierre-Joseph Proudhon was the first philosopher to label himself an \"anarchist\". Some consider Proudhon to be an individualist anarchist while others regard him to be a social anarchist. Some commentators do not identify Proudhon as an individualist anarchist due to his preference for association in large industries rather than individual control.\nMax Stirner.\nJohann Kaspar Schmidt, better known as Max Stirner (the \"nom de plume\" he adopted from a schoolyard nickname he had acquired as a child because of his high brow, in German \"Stirn\"), was a German philosopher who ranks as one of the literary fathers of nihilism, existentialism, post-modernism and anarchism, especially of individualist anarchism. Stirner's main work is \"The Ego and Its Own\", also known as \"The Ego and His Own\" (\"Der Einzige und sein Eigentum\" in German which translates literally as \"The Only One [individual] and his Property\" or \"The Unique Individual and His Property\"). This work was first published in 1844 in Leipzig and has since appeared in numerous editions and translations.\nEgoism.\nMax Stirner's philosophy, sometimes called egoism, is a form of individualist anarchism. Stirner was a Hegelian philosopher whose \"name appears with familiar regularity in historically oriented surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism\". In 1844, Stirner's work \"The Ego and Its Own\" was published and is considered to be \"a founding text in the tradition of individualist anarchism\". Stirner does not recommend that the individual try to eliminate the state, but simply that they disregard the state when it conflicts with one's autonomous choices and go along with it when doing so is conducive to one's interests. Stirner says that the egoist rejects pursuit of devotion to \"a great idea, a good cause, a doctrine, a system, a lofty calling\", arguing that the egoist has no political calling, but rather \"lives themselves out\" without regard to \"how well or ill humanity may fare thereby\". Stirner held that the only limitation on the rights of the individual is that individual's power to obtain what he desires. Stirner proposes that most commonly accepted social institutions, including the notion of state, property as a right, natural rights in general and the very notion of \"society\" as a legal and ideal abstractness, were mere spooks in the mind. Stirner wants to \"abolish not only the state but also society as an institution responsible for its members\". Stirner advocated self-assertion and foresaw Union of egoists, non-systematic associations which he proposed in as a form of organization in place of the state. A Union is understood as a relation between egoists which is continually renewed by all parties' support through an act of will. Even murder is permissible \"if it is right for me\", although it is claimed by egoist anarchists that egoism will foster genuine and spontaneous unions between individuals.\nStirner's concept of \"egoistic property\" not only a lack of moral restraint on how one obtains and uses \"things\", but includes other people as well. His embrace of egotism is in stark contrast to Godwin's altruism. Although Stirner was opposed to communism, for the same reasons he opposed capitalism, humanism, liberalism, property rights and nationalism, seeing them as forms of authority over the individual and as spooks in the mind, he has influenced many anarcho-communists and post-left anarchists. The writers of \"An Anarchist FAQ\" report that \"many in the anarchist movement in Glasgow, Scotland, took Stirner's 'Union of egoists' literally as the basis for their anarcho-syndicalist organising in the 1940s and beyond\". Similarly, the noted anarchist historian Max Nettlau states that \"[o]n reading Stirner, I maintain that he cannot be interpreted except in a socialist sense\".\nThis position on property is quite different from the native American, natural law, form of individualist anarchism which defends the inviolability of the private property that has been earned through labor. Other egoists include James L. Walker, Sidney Parker, Dora Marsden and John Beverly Robinson. In Russia, individualist anarchism inspired by Stirner combined with an appreciation for Friedrich Nietzsche attracted a small following of bohemian artists and intellectuals such as Lev Chernyi as well as a few lone wolves who found self-expression in crime and violence. They rejected organizing, believing that only unorganized individuals were safe from coercion and domination, believing this kept them true to the ideals of anarchism. This type of individualist anarchism inspired anarcha-feminist Emma Goldman.\nEarly individualist anarchism in the United States.\nThe intentional communal experiments pioneered by Josiah Warren influenced European individualist anarchists of the late 19th and early 20th centuries such as \u00c9mile Armand and the intentional communities they founded.\nThe American version of individualist anarchism has a strong emphasis on the non-aggression principle and individual sovereignty. Some individualist anarchists such as Thoreau do not speak of economics, but simply of the right of \"disunion\" from the state and foresee the gradual elimination of the state through social evolution.\nIndividualist anarchism in the United States.\nMutualism and utopianism.\nFor American anarchist historian Eunice Minette Schuster, \"[i]t is apparent [...] that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews [...] William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form\". William Batchelder Greene is best known for the works \"Mutual Banking\" (1850) which proposed an interest-free banking system and \"Transcendentalism\", a critique of the New England philosophical school. He saw mutualism as the synthesis of \"liberty and order\". His \"associationism [...] is checked by individualism. [...] 'Mind your own business,' 'Judge not that ye be not judged.' Over matters which are purely personal, as for example, moral conduct, the individual is sovereign, as well as over that which he himself produces. For this reason he demands 'mutuality' in marriage\u2014the equal right of a woman to her own personal freedom and property. Within some individualist anarchist circles, \"mutualism\" came to mean non-communist anarchism.\nBoston anarchists.\nAnother form of individualist anarchism was found in the United States as advocated by the so-called Boston anarchists. By default, American individualists had no difficulty accepting the concepts that \"one man employ another\" or that \"he direct him\", in his labor but rather demanded that \"all natural opportunities requisite to the production of wealth be accessible to all on equal terms and that monopolies arising from special privileges created by law be abolished\".\nThey believed state monopoly capitalism (defined as a state-sponsored monopoly) prevented labor from being fully rewarded.\nLysander Spooner besides his individualist anarchist activism was also an important anti-slavery activist and became a member of the First International.\nSome Boston anarchists, including Benjamin Tucker, identified themselves as socialists, which in the 19th century was often used in the sense of a commitment to improving conditions of the working class (i.e. \"the labor problem\"). The Boston anarchists such as Tucker and his followers continue to be considered socialists due to their opposition to usury. They do so because as the modern economist Jim Stanford points out there are many different kinds of competitive markets such as market socialism and capitalism is only one type of a market economy. By around the start of the 20th century, the heyday of individualist anarchism had passed.\nIndividualist anarchism in Europe.\nEarly precursors.\nFrance.\nFrom the legacy of Proudhon and Stirner there emerged a strong tradition of French individualist anarchism. An early important individualist anarchist was Anselme Bellegarrigue. He participated in the French Revolution of 1848, was author and editor of \"Anarchie, Journal de l'Ordre and Au fait ! Au fait ! Interpr\u00e9tation de l'id\u00e9e d\u00e9mocratique\" and wrote the important early Anarchist Manifesto in 1850. Catalan historian of individualist anarchism Xavier Diez reports that during his travels in the United States \"he at least contacted (Henry David) Thoreau and, probably (Josiah) Warren\".\nLater, this tradition continued with such intellectuals as Albert Libertad, Andr\u00e9 Lorulot, \u00c9mile Armand, Victor Serge, Zo d'Axa and Rirette Ma\u00eetrejean, who in 1905 developed theory in the main individualist anarchist journal in France, \"L'Anarchie\".\nIn this sense, \"the theoretical positions and the vital experiences of [F]rench individualism are deeply iconoclastic and scandalous, even within libertarian circles. The call of nudist naturism, the strong defence of birth control methods, the idea of \"unions of egoists\" with the sole justification of sexual practices, that will try to put in practice, not without difficulties, will establish a way of thought and action, and will result in sympathy within some, and a strong rejection within others\".\nIllegalists usually did not seek moral basis for their actions, recognizing only the reality of \"might\" rather than \"right\"; and for the most part, illegal acts were done simply to satisfy personal desires, not for some greater ideal. Influenced by theorist Max Stirner's egoism as well as Pierre-Joseph Proudhon (his view that \"property is theft!\"), Cl\u00e9ment Duval and Marius Jacob proposed the theory of la \"reprise individuelle\" (individual reclamation) which justified robbery on the rich and personal direct action against exploiters and the system.\nGermany.\nIn Germany, the Scottish-German John Henry Mackay became the most important propagandist for individualist anarchist ideas. He fused Stirnerist egoism with the positions of Benjamin Tucker and actually translated Tucker into German. Two semi-fictional writings of his own, \"Die Anarchisten\" and \"Der Freiheitsucher\", contributed to individualist theory through an updating of egoist themes within a consideration of the anarchist movement. English translations of these works arrived in the United Kingdom and in individualist American circles led by Tucker. Stirnerist individualism was popular among literary and intellectual anarchists in pre-WWI Germany, for instance influencing the expressionist movement and the early thought of Otto Gross.\nItaly.\nIn Italy, individualist anarchism had a strong tendency towards illegalism and violent propaganda by the deed similar to French individualist anarchism, but perhaps more extreme and which emphazised criticism of organization be it anarchist or of other type.\nRussia.\nIndividualist anarchism was one of the three categories of anarchism in Russia, along with the more prominent anarcho-communism and anarcho-syndicalism. The ranks of the Russian individualist anarchists were predominantly drawn from the intelligentsia and the working class. For anarchist historian Paul Avrich, \"[t]he two leading exponents of individualist anarchism, both based in Moscow, were Aleksei Alekseevich Borovoi and Lev Chernyi (born Pavel Dmitrievich Turchaninov). From Nietzsche, they inherited the desire for a complete overturn of all values accepted by bourgeois society political, moral, and cultural. Furthermore, strongly influenced by Max Stirner and Benjamin Tucker, the German and American theorists of individualist anarchism, they demanded the total liberation of the human personality from the fetters of organized society\".\nSome Russian individualists anarchists \"found the ultimate expression of their social alienation in violence and crime, others attached themselves to avant-garde literary and artistic circles, but the majority remained \"philosophical\" anarchists who conducted animated parlor discussions and elaborated their individualist theories in ponderous journals and books\".\nLev Chernyi advocated a Nietzschean overthrow of the values of bourgeois Russian society, and rejected the voluntary communes of anarcho-communist Peter Kropotkin as a threat to the freedom of the individual. Scholars including Avrich and Allan Antliff have interpreted this vision of society to have been greatly influenced by the individualist anarchists Max Stirner and Benjamin Tucker.\nSpain.\nWhile Spain was influenced by American individualist anarchism, it was more closely related to the French currents. Around the start of the 20th century, individualism in Spain gathered force through the efforts of people such as Dorado Montero, Ricardo Mella, Federico Urales, Miguel Gim\u00e9nez Igualada, Mariano Gallardo and J. Elizalde who translated French and American individualists. Important in this respect were also magazines such as \"La Idea Libre\", \"La Revista Blanca\", \"Etica\", \"Iniciales\", \"Al margen\", \"Estudios\" and \"Nosotros\". The most influential thinkers there were Max Stirner, \u00c9mile Armand and Han Ryner. Just as in France, the spread of Esperanto and anationalism had importance just as naturism and free love currents. Later, Armand and Ryner themselves started writing in the Spanish individualist press. Armand's concept of amorous camaraderie had an important role in motivating polyamory as realization of the individual.\nCatalan historian Xavier Diez reports that the Spanish individualist anarchist press was widely read by members of anarcho-communist groups and by members of the anarcho-syndicalist trade union CNT. There were also the cases of prominent individualist anarchists such as Federico Urales and Miguel Gim\u00e9nez Igualada who were members of the CNT and J. Elizalde who was a founding member and first secretary of the Iberian Anarchist Federation (IAF).\nBetween October 1937 and February 1938, Spanish individualist anarchist Miguel Gim\u00e9nez Igualada was editor of the individualist anarchist magazine \"Nosotros\" in which many works of Armand and Ryner appeared. He also participated in the publishing of another individualist anarchist maganize \"Al Margen: Publicaci\u00f3n quincenal individualista\". In his youth, he engaged in illegalist activities. His thought was deeply influenced by Max Stirner, of which he was the main popularizer in Spain through his own writings. He published and wrote the preface to the fourth edition in Spanish of \"The Ego and Its Own\" from 1900. He proposed the creation of a \"Union of egoists\" to be a federation of individualist anarchists in Spain, but it did not succeed.\nFederico Urales was an important individualist anarchist who edited \"La Revista Blanca\". The individualist anarchism of Urales was influenced by Auguste Comte and Charles Darwin. He saw science and reason as a defense against blind servitude to authority. He was critical of influential individualist thinkers such as Nietzsche and Stirner for promoting an asocial egoist individualism and instead promoted an individualism with solidarity seen as a way to guarantee social equality and harmony. He was highly critical of anarcho-syndicalism, which he viewed as plagued by excessive bureaucracy; and he thought that it tended towards reformism. Instead, he favored small groups based on ideological alignment. He supported and participated in the establishment of the IAF in 1927.\nIn 2000, Ateneo Libertario Ricardo Mella, Ateneo Libertario Al Margen, Ateneu Enciclop\u00e8dic Popular, Ateneo Libertario de Sant Boi and Ateneu Llibertari Poble Sec y Fundaci\u00f3 D'Estudis Llibertaris i Anarcosindicalistes republished \u00c9mile Armand's writings on free love and individualist anarchism in a compilation titled \"Individualist anarchism and Amorous camaraderie\".\nIndividualist anarchism in Latin America.\nArgentine anarchist historian \u00c1ngel Cappelletti reports that in Argentina \"[a]mong the workers that came from Europe in the 2 first decades of the century, there was curiously some stirnerian individualists influenced by the philosophy of Nietzsche, that saw syndicalism as a potential enemy of anarchist ideology. They established [...] affinity groups that in 1912 came to, according to Max Nettlau, to the number of 20. In 1911 there appeared, in Col\u00f3n, the periodical \"El \u00danico\", that defined itself as 'Publicaci\u00f3n individualista'\".\nDevelopments and expansion.\nAnarcha-feminism, free love, freethought and LGBT issues.\nIn Europe, interest in free love developed in French and Spanish individualist anarchist circles: \"Anticlericalism, just as in the rest of the libertarian movement, is another of the frequent elements which will gain relevance related to the measure in which the (French) Republic begins to have conflicts with the church [...] Anti-clerical discourse, frequently called for by the french individualist Andr\u00e9 Lorulot, will have its impacts in \"Estudios\" (a Spanish individualist anarchist publication). There will be an attack on institutionalized religion for the responsibility that it had in the past on negative developments, for its irrationality which makes it a counterpoint of philosophical and scientific progress. There will be a criticism of proselitism and ideological manipulation which happens on both believers and agnostics\". This tendencies will continue in French individualist anarchism in the work and activism of Charles-Auguste Bontemps and others. In the Spanish individualist anarchist magazine \"\u00c9tica\" and \"Iniciales\", \"there is a strong interest in publishing scientific news, usually linked to a certain atheist and anti-theist obsession, philosophy which will also work for pointing out the incompatibility between science and religion, faith and reason. In this way there will be a lot of talk on Darwin's theories or on the negation of the existence of the soul\".\nAnarcho-naturism.\nIndividualist anarchist groups also were affiliated with naturism and individualist anarchist Henri Zisly promoted naturism in France.\nCriticism.\nMurray Bookchin criticized individualist anarchism for its opposition to democracy and its embrace of \"lifestylism\" at the expense of anti-capitalism and class struggle.\nGeorge Bernard Shaw initially had flirtations with individualist anarchism before coming to the conclusion that it was \"the negation of socialism, and is, in fact, unsocialism carried as near to its logical conclusion as any sane man dare carry it\". Shaw's argument was that even if wealth was initially distributed equally, the degree of \"laissez-faire\" advocated by Tucker would result in the distribution of wealth becoming unequal because it would permit private appropriation and accumulation. According to Carlotta Anderson, American individualist anarchists accept that free competition results in unequal wealth distribution, but they \"do not see that as an injustice\". Nonetheless, Peter Marshall states that \"the egalitarian implications of traditional individualist anarchists\" such as Tucker and Lysander Spooner have been overlooked.\nCollectivist and social anarchists dispute the individualist anarchist claim that free competition and markets would yield the libertarian-egalitarian anarchist society that individualist anarchists share with them. In their views, \"state intervention merely props up a system of class exploitation and gives capitalism a human face\".\nThe authors of \"An Anarchist FAQ\" argue that individualist anarchists did not advocate free competition and markets as normative claims and merely thought those were better means than the ones proposed by anarcho-communists for the development of an anarchist society. Individualist anarchists such as Tucker thought interests, profits, rents and usury would disappear, something that both anarcho-capitalists and social anarchists did not think was true or believe would not happen. In a free market, people would be paid in proportion to how much labor they exerted and exploitation or usury was taking place if they were not. The theory was that unregulated banking would cause more money to be available and that this would allow proliferation of new businesses which would in turn raise demand for labor. This led Tucker to believe that the labor theory of value would be vindicated and equal amounts of labor would receive equal pay. Later in his life, Tucker grew skeptical that free competition could remove concentrated capital.\nIndividualist anarchism and anarcho-capitalism.\nWhile anarcho-capitalism is sometimes described as a form of individualist anarchism, some scholars have criticized those, including some Marxists and right-libertarians, for taking it at face value. Other scholars such as Benjamin Franks, who considers anarcho-capitalism part of individualist anarchism and hence excludes those forms of individualist anarchism that defend or reinforce hierarchical forms from the anarchist camp, have been criticized by those who include individualist anarchism as part of the anarchist and socialist traditions whilst excluding anarcho-capitalism, including the authors of \"An Anarchist FAQ\". Some anarchist scholars criticized those, especially in Anglo-American philosophy, who define anarchism only in terms of opposition to the state, when anarchism, including both individualist and social traditions, is much more than that. Anarchists, including both individualist and social anarchists, also criticized some Marxists and other socialists for excluding anarchism from the socialist camp. In \"European Socialism: A History of Ideas and Movements\", Carl Landauer summarized the difference between communist and individualist anarchists by stating that \"the communist anarchists also do not acknowledge any right to society to force the individual. They differ from the anarchistic individualists in their belief that men, if freed from coercion, will enter into voluntary associations of a communistic type, while the other wing believes that the free person will prefer a high degree of isolation\".\nWithout the labor theory of value, some argue that 19th-century individualist anarchists approximate the modern movement of anarcho-capitalism, although this has been contested or rejected. As economic theory changed, the popularity of the labor theory of classical economics was superseded by the subjective theory of value of neoclassical economics and Murray Rothbard, a student of Ludwig von Mises, combined Mises' Austrian School of economics with the absolutist views of human rights and rejection of the state he had absorbed from studying the individualist American anarchists of the 19th century such as Tucker and Spooner. In the mid-1950s, Rothbard was concerned with differentiating himself from communist and socialistic economic views of other anarchists, including the individualist anarchists of the 19th century, arguing that \"we are not anarchists [...] but not archists either [...]. Perhaps, then, we could call ourselves by a new name: nonarchist\".\nThere is a strong current within anarchism including anarchist activists and scholars which rejects that anarcho-capitalism can be considered a part of the anarchist movement because anarchism has historically been an anti-capitalist movement and anarchists see it as incompatible with capitalist forms. Although some regard anarcho-capitalism as a form of individualist anarchism, others contend individualist anarchism is largely socialistic and contest the concept of a socialist\u2013individualist divide. Some writers deny that anarcho-capitalism is a form of anarchism and that capitalism is compatible with anarchism.\n\"The Palgrave Handbook of Anarchism\" writes that \"[a]s Benjamin Franks rightly points out, individualisms that defend or reinforce hierarchical forms such as the economic-power relations of anarcho-capitalism are incompatible with practices of social anarchism based on developing immanent goods which contest such as inequalities\". Laurence Davis cautiously asks \"[I]s anarcho-capitalism really a form of anarchism or instead a wholly different ideological paradigm whose adherents have attempted to expropriate the language of anarchism for their own anti-anarchist ends?\" Davis cites Iain McKay, \"whom Franks cites as an authority to support his contention that 'academic analysis has followed activist currents in rejecting the view that anarcho-capitalism has anything to do with social anarchism'\", as arguing \"quite emphatically on the very pages cited by Franks that anarcho-capitalism is by no means a type of anarchism\". McKay writes that \"[i]t is important to stress that anarchist opposition to the so-called capitalist 'anarchists' does \"not\" reflect some kind of debate within anarchism, as many of these types like to pretend, but a debate between anarchism and its old enemy capitalism. [...] Equally, given that anarchists and 'anarcho'-capitalists have fundamentally \"different\" analyses and goals it is hardly 'sectarian' to point this out\".\nDavis writes that \"Franks asserts without supporting evidence that most major forms of individualist anarchism have been largely anarcho-capitalist in content, and concludes from this premise that most forms of individualism are incompatible with anarchism\". Davis argues that \"the conclusion is unsustainable because the premise is false, depending as it does for any validity it might have on the further assumption that anarcho-capitalism is indeed a form of anarchism. If we reject this view, then we must also reject the individual anarchist versus the communal anarchist 'chasm' style of argument that follows from it\". Davis maintains that \"the ideological core of anarchism is the belief that society can and should be organised without hierarchy and domination. Historically, anarchists have struggles against a wide range of regimes of domination, from capitalism, the state system, patriarchy, heterosexism, and the domination of nature to colonialism, the war system, slavery, fascism, white supremacy, and certain forms of organised religion\". According to Davis, \"[w]hile these visions range from the predominantly individualistic to the predominantly communitarian, features common to virtually all include an emphasis on self-management and self-regulatory methods of organisation, voluntary association, decentralised society, based on the principle of free association, in which people will manage and govern themselves\". Finally, Davis includes a footnote stating that \"[i]ndividualist anarchism may plausibly be re regarded as a form of both socialism and anarchism. Whether the individualist anarchists were \"consistent\" anarchists (and socialists) is another question entirely. [...] McKay comments as follows: 'any individualist anarchism which support wage labour is \"inconsistent\" anarchism. It \"can\" easily be made \"consistent\" anarchism by applying its own principles consistently. In contrast 'anarcho'-capitalism rejects so many of the basic, underlying, principles of anarchism [...] that it cannot be made consistent with the ideals of anarchism'\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14937", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=14937", "title": "Italo Calvino", "text": "Italian author (1923\u20131985)\nItalo Calvino (, ; ; 15 October 1923\u00a0\u2013 19 September 1985) was an Italian novelist and short story writer. His best-known works include the \"Our Ancestors\" trilogy (1952\u20131959), the \"Cosmicomics\" collection of short stories (1965), and the novels \"Invisible Cities\" (1972) and \"If on a winter's night a traveler\" (1979).\nAdmired in Britain, Australia and the United States, Calvino was the most translated contemporary Italian writer at the time of his death. He is buried in the garden cemetery of Castiglione della Pescaia in Tuscany.\nBiography.\nParents.\nItalo Calvino was born in Santiago de las Vegas, a suburb of Havana, Cuba, in 1923. His father, Mario, was a tropical agronomist and botanist who also taught agriculture and floriculture. Born 47 years earlier in Sanremo, Italy, Mario Calvino had emigrated to Mexico in 1909 where he took up an important position with the Ministry of Agriculture. In an autobiographical essay, Italo Calvino explained that his father \"had been in his youth an anarchist, a follower of Kropotkin and then a Socialist Reformist\". In 1917, Mario left for Cuba to conduct scientific experiments, after living through the Mexican Revolution.\nCalvino's mother, Giuliana Luigia Evelina \"Eva\" Mameli, was a botanist and university professor. A native of Sassari in Sardinia and 11 years younger than her husband, she married while still a junior lecturer at Pavia University. Born into a secular family, Eva was a pacifist educated in the \"religion of civic duty and science\". Eva gave Italo his unusual first name to remind him of his Italian heritage, although he would eventually grow up in Italy. Calvino thought his name sounded \"belligerently nationalist\". Calvino described his parents as being \"very different in personality from one another\", suggesting perhaps deeper tensions behind a comfortable, albeit strict, middle-class upbringing devoid of conflict. As an adolescent, he found it hard to relate to poverty and the working-class, and was \"ill at ease\" with his parents' openness to the labourers who filed into his father's study on Saturdays to receive their weekly paycheck.\nEarly life and education.\nIn 1925, less than two years after Calvino's birth, the family returned to Italy and settled permanently in Sanremo on the Ligurian coast. Calvino's brother Floriano, who became a distinguished geologist, was born in 1927. The family divided their time between the Villa Meridiana, an experimental floriculture station which also served as their home, and Mario's ancestral land at San Giovanni Battista. On this small working farm set in the hills behind Sanremo, Mario pioneered the cultivation of the then exotic fruits such as avocado and grapefruit, eventually obtaining an entry in the for his achievements. The vast forests and luxuriant fauna omnipresent in Calvino's early fiction such as \"The Baron in the Trees\" derive from this \"legacy\". In an interview, Calvino stated that \"San Remo continues to pop out in my books, in the most diverse pieces of writing.\"\nCalvino and Floriano would climb the tree-rich estate and perch for hours on the branches reading their favourite adventure stories. Less salubrious aspects of this \"paternal legacy\" are described in \"The Road to San Giovanni\", Calvino's memoir of his father in which he exposes their inability to communicate: \"Talking to each other was difficult. Both verbose by nature, possessed of an ocean of words, in each other's presence we became mute, would walk in silence side by side along the road to San Giovanni.\" A fan of Rudyard Kipling's \"The Jungle Book\" as a child, Calvino felt that his early interest in stories made him the \"black sheep\" of a family that held literature in less esteem than the sciences. Fascinated by American movies and cartoons, he was equally attracted to drawing, poetry, and theatre. On a darker note, Calvino recalled that his earliest memory was of a Marxist professor who had been brutally assaulted by Benito Mussolini's Blackshirts. He said: \"I remember clearly that we were at dinner when the old professor came in with his face beaten up and bleeding, his bowtie all torn up over it, asking for help.\"\nOther legacies include the parents' beliefs in Freemasonry, republicanism with elements of anarchism and Marxism. Austere freethinkers with an intense hatred of the ruling National Fascist Party, Eva and Mario also refused to give their sons any education in the Catholic Faith or any other religion. Italo attended the English nursery school St George's College, followed by a Protestant elementary private school run by Waldensians. His secondary schooling, with a classical lyceum curriculum, was completed at the state-run Liceo Gian Domenico Cassini where, at his parents' request, he was exempted from religion classes but frequently asked to justify his anti-conformism to teachers, janitors, and fellow pupils. In his mature years, Calvino described the experience as having made him \"tolerant of others' opinions, particularly in the field of religion, remembering how irksome it was to hear myself mocked because I did not follow the majority's beliefs\". In 1938, Eugenio Scalfari, who went on to found the weekly magazine and , a major Italian newspaper, came from Civitavecchia to join the same class though a year younger, and they shared the same desk. The two teenagers formed a lasting friendship, Calvino attributing his political awakening to their university discussions. Seated together \"on a huge flat stone in the middle of a stream near our land\", he and Scalfari founded a university movement called the MUL. Eva managed to delay her son's enrolment in the Party's armed scouts, the , and then arranged that he be excused, as a non-Catholic, from performing devotional acts in Church. But later on, as a compulsory member, he could not avoid the assemblies and parades of the , and was forced to participate in the Italian invasion of the French Riviera in June 1940.\nWorld War II.\nIn 1941, Calvino enrolled at the University of Turin, choosing the Agriculture Faculty where his father had previously taught courses in agronomy. Concealing his literary ambitions to please his family, he passed four exams in his first year while reading anti-Fascist works by Elio Vittorini, Eugenio Montale, Cesare Pavese, Johan Huizinga, and Pisacane, and works by Max Planck, Werner Heisenberg, and Albert Einstein on physics. Calvino's real aspiration was to be a playwright. His letters to Eugenio Scalfari overflow with references to Italian and foreign plays, and with plots and characters of future theatrical projects. Luigi Pirandello and Gabriele D'Annunzio, Cesare Vico Lodovici and Ugo Betti, Eugene O'Neill and Thornton Wilder are among the main authors Calvino cites as his sources of inspiration. Disdainful of Turin students, Calvino saw himself as enclosed in a \"provincial shell\" that offered the illusion of immunity from the Fascist nightmare: \"We were \u2018hard guys\u2019 from the provinces, hunters, snooker-players, show-offs, proud of our lack of intellectual sophistication, contemptuous of any patriotic or military rhetoric, coarse in our speech, regulars in the brothels, dismissive of any romantic sentiment and desperately devoid of women.\"\nCalvino transferred to the University of Florence in 1943 and reluctantly passed three more exams in agriculture. By the end of the year, the Germans had succeeded in occupying Liguria and setting up Benito Mussolini's puppet Republic of Sal\u00f2 in northern Italy. Now twenty years old, Calvino refused military service and went into hiding. Reading intensely in a wide array of subjects, he also reasoned politically that, of all the partisan groupings, the communists were the best organized with \"the most convincing political line\".\nIn spring 1944, Eva encouraged her sons to enter the Italian Resistance in the name of \"natural justice and family virtues\". Using the nom de guerre \"Santiago\", Calvino joined the \"Garibaldi Brigades\", a clandestine Communist group and, for twenty months, endured the fighting in the Maritime Alps until 1945 and the Liberation. As a result of his refusal to be a conscript, his parents were held hostage by the Nazis for an extended period at the Villa Meridiana. Calvino wrote of his mother's ordeal that \"she was an example of tenacity and courage\u2026 behaving with dignity and firmness before the SS and the Fascist militia, and in her long detention as a hostage, not least when the blackshirts three times pretended to shoot my father in front of her eyes. The historical events which mothers take part in acquire the greatness and invincibility of natural phenomena\".\nTurin and communism.\nCalvino settled in Turin in 1945, after a long hesitation over living there or in Milan. He often humorously belittled this choice, describing Turin as a \"city that is serious but sad\". Returning to university, he abandoned Agriculture for the Arts Faculty. A year later, he was initiated into the literary world by Elio Vittorini, who published his short story (1945; \"Gone to Headquarters\") in , a Turin-based weekly magazine associated with the university. The horror of the war had not only provided the raw material for his literary ambitions but deepened his commitment to the Communist cause. Viewing civilian life as a continuation of the partisan struggle, he confirmed his membership in the Italian Communist Party. On reading Vladimir Lenin's \"State and Revolution\", he plunged into post-war political life, associating himself chiefly with the worker's movement in Turin.\nIn 1947, he graduated with a Master's thesis on Joseph Conrad, wrote short stories in his spare time, and landed a job in the publicity department at the Einaudi publishing house run by Giulio Einaudi. Although brief, his stint put him in regular contact with Cesare Pavese, Natalia Ginzburg, Norberto Bobbio, and many other left-wing intellectuals and writers. He then left Einaudi to work as a journalist for the official Communist daily, , and the newborn Communist political magazine, . During this period, Pavese and poet Alfonso Gatto were Calvino's closest friends and mentors.\nHis first novel, (\"The Path to the Nest of Spiders\") written with valuable editorial advice from Pavese, won the Premio Riccione on publication in 1947. With sales topping 5000 copies, a surprise success in postwar Italy, the novel inaugurated Calvino's neorealist period. In a clairvoyant essay, Pavese praised the young writer as a \"squirrel of the pen\" who \"climbed into the trees, more for fun than fear, to observe partisan life as a fable of the forest\". In 1948, he interviewed one of his literary idols, Ernest Hemingway, travelling with Natalia Ginzburg to his home in Stresa.\n (\"The Crow Comes Last\"), a collection of stories based on his wartime experiences, was published to acclaim in 1949. Despite the triumph, Calvino grew increasingly worried by his inability to compose a worthy second novel. He returned to Einaudi in 1950, responsible this time for the literary volumes. He eventually became a consulting editor, a position that allowed him to hone his writing talent, discover new writers, and develop into \"a reader of texts\". In late 1951, presumably to advance in the Communist Party, he spent two months in the Soviet Union as a correspondent for . While in Moscow, he learned of his father's death on 25 October. The articles and correspondence he produced from this visit were published in 1952, winning the Saint-Vincent Prize for journalism.\nOver a seven-year period, Calvino wrote three realist novels, \"The White Schooner\" (1947\u20131949), \"Youth in Turin\" (1950\u20131951), and \"The Queen's Necklace\" (1952\u201354), but all were deemed defective. Calvino's first efforts as a fictionist were marked with his experience in the Italian resistance during the Second World War, however, his acclamation as a writer of fantastic stories came in the 1950s. During the eighteen months it took to complete (\"Youth in Turin\"), he made an important self-discovery: \"I began doing what came most naturally to me \u2013 that is, following the memory of the things I had loved best since boyhood. Instead of making myself write the book I \"ought\" to write, the novel that was expected of me, I conjured up the book I myself would have liked to read, the sort by an unknown writer, from another age and another country, discovered in an attic.\" The result was (1952; \"The Cloven Viscount\") composed in 30 days between July and September 1951. The protagonist, a seventeenth-century viscount sundered in two by a cannonball, incarnated Calvino's growing political doubts and the divisive turbulence of the Cold War. Skilfully interweaving elements of the fable and the fantasy genres, the allegorical novel launched him as a modern \"fabulist\". In 1954, Giulio Einaudi commissioned his (1956; \"Italian Folktales\") on the basis of the question, \"Is there an Italian equivalent of the Brothers Grimm?\" For two years, Calvino collated tales found in 19th century collections across Italy then translated 200 of the finest from various dialects into Italian. Key works he read at this time were Vladimir Propp's \"Morphology of the Folktale\" and \"Historical Roots of Russian Fairy Tales\", stimulating his own ideas on the origin, shape and function of the story.\nIn 1952 Calvino wrote with Giorgio Bassani for , a magazine named after the popular name of the party's head offices in Rome. He also worked for , a Marxist weekly.\nFrom 1955 to 1958 Calvino had an affair with Italian actress Elsa De Giorgi, a married, older woman. Excerpts of the hundreds of love letters Calvino wrote to her were published in the in 2004, causing some controversy.\nAfter communism.\nIn 1957, disillusioned by the 1956 Soviet invasion of Hungary, Calvino left the Italian Communist Party. In his letter of resignation published in on 7 August, he explained the reason for his dissent (the violent suppression of the Hungarian uprising and the revelation of Joseph Stalin's crimes) while confirming his \"confidence in the democratic perspectives\" of world Communism. He withdrew from taking an active role in politics and never joined another party. Ostracized by the PCI party leader Palmiro Togliatti and his supporters on publication of \"Becalmed in the Antilles\" (), a satirical allegory of the party's immobilism, Calvino began writing \"The Baron in the Trees\". Completed in three months and published in 1957, the fantasy is based on the \"problem of the intellectual's political commitment at a time of shattered illusions\". He found new outlets for his periodic writings in the journals and , the magazine , and the weekly . With Vittorini in 1959, he became co-editor of , a cultural journal devoted to literature in the modern industrial age, a position he held until 1966.\nDespite severe restrictions in the US against foreigners holding communist views, Calvino was allowed to visit the United States, where he stayed six months from 1959 to 1960 (four of which he spent in New York), after an invitation by the Ford Foundation. Calvino was particularly impressed by the \"New World\": \"Naturally I visited the South and also California, but I always felt a New Yorker. My city is New York.\" The letters he wrote to Einaudi describing this visit to the United States were first published as \"American Diary 1959\u20131960\" in \"Hermit in Paris\" in 2003.\nIn 1962 Calvino met Argentinian translator Esther Judith Singer (\"Chichita\") and married her in 1964 in Havana, during a trip in which he visited his birthplace and was introduced to Ernesto \"Che\" Guevara. On 15 October 1967, a few days after Guevara's death, Calvino wrote a tribute to him that was published in Cuba in 1968, and in Italy thirty years later. He and his wife settled in Rome in via Monte Brianzo where their daughter, Giovanna, was born in 1965. Once again working for Einaudi, Calvino began publishing some of his \"Cosmicomics\" in , a literary magazine.\nLater life and work.\nVittorini's death in 1966 greatly affected Calvino. He went through what he called an \"intellectual depression\", which the writer himself described as an important passage in his life: \"I ceased to be young. Perhaps it's a metabolic process, something that comes with age, I'd been young for a long time, perhaps too long, suddenly I felt that I had to begin my old age, yes, old age, perhaps with the hope of prolonging it by beginning it early.\"\nAmid the atmosphere that would evolve into 1968's cultural revolution (the French May), he and his family moved to Paris in 1967, taking up residence in a villa in the Square de Ch\u00e2tillon. Nicknamed , Calvino was invited by Raymond Queneau in 1968 to join the Oulipo () group of experimental writers where he met Roland Barthes and Georges Perec, who would influence his later work. That same year, he turned down the Viareggio Prize for (\"Time and the Hunter\") on the grounds that it was an award given by \"institutions emptied of meaning\". He accepted, however, both the Asti Prize and the Feltrinelli Prize for his writing in 1970 and 1972, respectively. In two autobiographical essays published in 1962 and 1970, Calvino described himself as \"atheist\" and his outlook as \"non-religious\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nThe catalogue of forms is endless: until every shape has found its city, new cities will continue to be born. When the forms exhaust their variety and come apart, the end of cities begins.&lt;/poem&gt;\nFrom \"Invisible Cities\" (1974) \nCalvino had more significant contact with the academic world, notably at the Sorbonne (with Barthes) and the University of Urbino. His literary interests spanned multiple periods, genres, and languages, including Honor\u00e9 de Balzac, Ludovico Ariosto, Dante, Ignatius of Loyola, Cervantes, Shakespeare, Cyrano de Bergerac, and Giacomo Leopardi. \nBetween 1972 and 1973, Calvino published two short stories, \"The Name, the Nose\" and the Oulipo-inspired \"The Burning of the Abominable House\", in the Italian edition of \"Playboy\". He also became a regular contributor to the Italian newspaper . During this period, Calvino spent his summer vacations in a house constructed in the pinewood of Roccamare, in Castiglione della Pescaia, Tuscany.\nIn 1975, Calvino was made an Honorary Member of the American Academy. Awarded the Austrian State Prize for European Literature in 1976, he visited Mexico, Japan, and the United States, where he gave a series of lectures in several American cities. After his mother died in 1978 at the age of 92, Calvino sold Villa Meridiana, the family home in San Remo. Two years later, he moved to Rome in Piazza Campo Marzio near the Pantheon and began editing the work of Tommaso Landolfi for Rizzoli. Awarded the French in 1981, he also accepted the role of jury president for the 38th Venice Film Festival.\nDuring the summer of 1985, Calvino prepared a series of texts on literature for the Charles Eliot Norton Lectures to be delivered at Harvard University in the fall. On 6 September 1985, Calvino suffered a stroke in his villa in Roccamare, where he was preparing for a lecture tour of the United States. Initially hospitalized at the Misericordia hospital in Grosseto, he was transferred to the hospital of Santa Maria della Scala in Siena (now a museum). After partially regaining consciousness, his condition worsened and he died during the night of 18/19 September of a cerebral haemorrhage, aged sixty-one. He is buried in the cemetery-garden of Castiglione della Pescaia. His lecture notes were published posthumously in Italian in 1988 and in English as \"Six Memos for the Next Millennium\" in 1993.\nSelected publications.\nA selected bibliography of Calvino's writings follows, listing the works that have been published in English translation, along with a few major untranslated works. More exhaustive bibliographies can be found in Martin McLaughlin's \"Italo Calvino\" and Beno Weiss's \"Understanding Italo Calvino\".\nLegacy.\nThe , an Italian curriculum school in Moscow, Russia, is named after him. A crater on the planet Mercury, Calvino, and a main-belt asteroid, \"22370 Italocalvino\", are also named after him. \"Salt Hill Journal\" and University of Louisville award annually the Italo Calvino Prize \"for a work of fiction written in the fabulist experimental style of Italo Calvino\". \nKai Nieminen (b. 1953) wrote his flute concerto (2001) based on the story of Mr. Palomar. The text was written to the dedicatee, Patrick Gallois.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\nGeneral\nExternal links.\nExcerpts, essays, artwork"}
{"id": "14938", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=14938", "title": "ICBM", "text": ""}
{"id": "14939", "revid": "751903", "url": "https://en.wikipedia.org/wiki?curid=14939", "title": "Intercontinental ballistic missile", "text": "Ballistic missile with a range of more than 5,500 kilometres\nAn intercontinental ballistic missile (ICBM) is a ballistic missile with a range greater than , primarily designed for nuclear weapons delivery (delivering one or more thermonuclear warheads). Conventional, chemical, and biological weapons can also be delivered with varying effectiveness but have never been deployed on ICBMs. Most modern designs support multiple independently targetable reentry vehicles (MIRVs), allowing a single missile to carry several warheads, each of which can strike a different target. The United States, Russia, China, France, India, the United Kingdom, Israel, and North Korea are the only countries known to have operational ICBMs. Pakistan is the only nuclear-armed state that does not possess ICBMs.\nEarly ICBMs had limited precision, which made them suitable for use only against the largest targets, such as cities. They were seen as a \"safe\" basing option, one that would keep the deterrent force close to home where it would be difficult to attack. Attacks against military targets (especially hardened ones) demanded the use of a more precise, crewed bomber. Second- and third-generation designs (such as the LGM-118 Peacekeeper) dramatically improved accuracy to the point where even the smallest point targets can be successfully attacked.\nICBMs are differentiated by having greater range and speed than other ballistic missiles: intermediate-range ballistic missiles (IRBMs), medium-range ballistic missiles (MRBMs), short-range ballistic missiles (SRBMs) and tactical ballistic missiles.\nHistory.\nWorld War II.\nThe first practical design for an ICBM grew out of Nazi Germany's V-2 rocket program. The liquid-fueled V-2, designed by Wernher von Braun and his team, was then widely used by Nazi Germany from mid-1944 until March 1945 to bomb British and Belgian cities, particularly Antwerp and London.\nUnder \"Projekt Amerika,\" von Braun's team developed the A9/10 ICBM, intended for use in bombing New York and other American cities. Initially intended to be guided by radio, it was changed to be a piloted craft after the failure of Operation Elster. The second stage of the A9/A10 rocket was tested a few times in January and February 1945.\nAfter the war, the US executed Operation Paperclip, which took von Braun and hundreds of other leading Nazi scientists to the United States to develop IRBMs, ICBMs, and launchers for the US Army.\nThis technology was predicted by US General of the Army Hap Arnold, who wrote in 1943:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Someday, not too distant, there can come streaking out of somewhere \u2013 we won't be able to hear it, it will come so fast \u2013 some kind of gadget with an explosive so powerful that one projectile will be able to wipe out completely this city of Washington.\nCold War.\nAfter World War II, the Americans and the Soviets started rocket research programs based on the V-2 and other German wartime designs. Each branch of the US military started its own programs, leading to considerable duplication of effort. In the Soviet Union, rocket research was centrally organized although several teams worked on different designs.\nThe US initiated ICBM research in 1946 with the RTV-A-2 Hiroc project. This was a three-stage effort with the ICBM development not starting until the third stage. However, funding was cut in 1948 after only three partially successful launches of the second stage design, that was used to test variations of the V-2 design. With overwhelming air superiority and truly intercontinental bombers, the newly formed US Air Force did not take the problem of ICBM development seriously. Things changed in 1953 with the Soviet testing of their first thermonuclear weapon, but it was not until 1954 that the Atlas missile program was given the highest national priority. The Atlas A first flew on 11 June 1957; the flight lasted only about 24 seconds before the rocket exploded. The first successful flight of an Atlas missile to full range occurred 28 November 1958. The first armed version of the Atlas, the Atlas D, was declared operational in January 1959 at Vandenberg, although it had not yet flown. The first test flight was carried out on 9 July 1959, and the missile was accepted for service on 1 September. The Titan I was another US multistage ICBM, with a successful launch February 5, 1959, with Titan I A3. Unlike the Atlas, the Titan I was a two-stage missile, rather than three. The Titan was larger, yet lighter, than the Atlas. Due to the improvements in engine technology and guidance systems the Titan I overtook the Atlas.\nIn the Soviet Union, early development was focused on missiles able to attack European targets. That changed in 1953, when Sergei Korolev was directed to start development of a true ICBM able to deliver newly developed hydrogen bombs. Given steady funding throughout, the R-7 developed with some speed. The first launch took place on 15 May 1957 and led to an unintended crash from the site. The first successful test followed on 21 August 1957; the R-7 flew over and became the world's first ICBM. The first strategic-missile unit became operational on 9 February 1959 at Plesetsk in north-west Russia.\nIt was the same R-7 launch vehicle that placed the first artificial satellite in space, Sputnik, on 4 October 1957. The first human spaceflight in history was accomplished on a derivative of R-7, Vostok, on 12 April 1961, by Soviet cosmonaut Yuri Gagarin. A heavily modernized version of the R-7 is still used as the launch vehicle for the Soviet/Russian Soyuz spacecraft, marking more than 60 years of operational history of Sergei Korolyov's original rocket design.\nThe R-7 and Atlas each required a large launch facility, making them vulnerable to attack, and could not be kept in a ready state. Failure rates were very high throughout the early years of ICBM technology. Human spaceflight programs (Vostok, Mercury, Voskhod, Gemini, etc.) served as a highly visible means of demonstrating confidence in reliability, with successes translating directly to national defense implications. The US was well behind the Soviets in the Space Race and so US President John F. Kennedy increased the stakes with the Apollo program, which used Saturn rocket technology that had been funded by President Dwight D. Eisenhower.\nThese early ICBMs also formed the basis of many space launch systems. Examples include R-7, Atlas, Redstone, Titan, and Proton, which was derived from the earlier ICBMs but never deployed as an ICBM. The Eisenhower administration supported the development of solid-fueled missiles such as the LGM-30 Minuteman, Polaris and Skybolt. Modern ICBMs tend to be smaller than their ancestors, due to increased accuracy and smaller and lighter warheads, and use solid fuels, making them less useful as orbital launch vehicles.\nThe Western view of the deployment of these systems was governed by the strategic theory of mutual assured destruction. In the 1950s and 1960s, development began on anti-ballistic missile systems by both the Americans and Soviets. Such systems were restricted by the 1972 Anti-Ballistic Missile Treaty. The first successful ABM test was conducted by the Soviets in 1961, which later deployed a fully operational system defending Moscow in the 1970s (see Moscow ABM system).\nThe 1972 SALT treaty froze the number of ICBM launchers of both the Americans and the Soviets at existing levels and allowed new submarine-based SLBM launchers only if an equal number of land-based ICBM launchers were dismantled. Subsequent talks, called SALT II, were held from 1972 to 1979 and actually reduced the number of nuclear warheads held by the US and Soviets. SALT II was never ratified by the US Senate, but its terms were honored by both sides until 1986, when the Reagan administration \"withdrew\" after it had accused the Soviets of violating the pact.\nIn the 1980s, President Ronald Reagan launched the Strategic Defense Initiative as well as the MX and Midgetman ICBM programs.\nChina developed a minimal independent nuclear deterrent entering its own cold war after an ideological split with the Soviet Union beginning in the early 1960s. After first testing a domestic built nuclear weapon in 1964, it went on to develop various warheads and missiles. Beginning in the early 1970s, the liquid fueled DF-5 ICBM was developed and used as a satellite launch vehicle in 1975. The DF-5, with a range of \u2014long enough to strike the Western United States and the Soviet Union\u2014was silo deployed, with the first pair in service by 1981 and possibly twenty missiles in service by the late 1990s. China also deployed the JL-1 Medium-range ballistic missile with a reach of aboard the ultimately unsuccessful Type 092 submarine.\nPost\u2013Cold War.\nIn 1991, the United States and the Soviet Union agreed in the START I treaty to reduce their deployed ICBMs and attributed warheads.\nAs of 2016[ [update]], all five of the nations with permanent seats on the United Nations Security Council have fully operational long-range ballistic missile systems; Russia, the United States, and China also have land-based ICBMs (the US missiles are silo-based, while China and Russia have both silo and road-mobile (DF-31, RT-2PM2 Topol-M missiles).\nIsrael is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008; an upgraded version is in development.\nIndia successfully test fired Agni V, with a strike range of more than on 19 April 2012, claiming entry into the ICBM club. The missile's actual range is speculated by foreign researchers to be up to with India having downplayed its capabilities to avoid causing concern to other countries. On 15 December 2022, first night trial of Agni-V was successfully carried out by SFC from Abdul Kalam Island, Odisha. The missile is now 20 percent lighter because the use of composite materials rather than steel material. The range has been increased to 7,000\u00a0km.\nBy 2012, there was speculation by some intelligence agencies that North Korea is developing an ICBM. North Korea successfully put a satellite into space on 12 December 2012 using the Unha-3 rocket. The United States claimed that the launch was in fact a way to test an ICBM. (See Timeline of first orbital launches by country.) In early July 2017, North Korea claimed for the first time to have tested successfully an ICBM capable of carrying a large thermonuclear warhead.\nIn July 2014, China announced the development of its newest generation of ICBM, the Dongfeng-41 (DF-41), which has a range of , capable of reaching the United States, and which analysts believe is capable of being outfitted with MIRV technology.\nMost countries in the early stages of developing ICBMs have used liquid propellants, with the known exceptions being the Indian Agni-V, the planned but cancelled South African RSA-4 ICBM, and the now in service Israeli Jericho III.\nThe RS-28 Sarmat (Russian: \u0420\u0421-28 \u0421\u0430\u0440\u043c\u0430\u0442; NATO reporting name: SATAN 2), is a Russian liquid-fueled, MIRV-equipped, super-heavy thermonuclear armed intercontinental ballistic missile in development by the Makeyev Rocket Design Bureau from 2009, intended to replace the previous R-36 missile. Its large payload would allow for up to 10 heavy warheads or 15 lighter ones or up to 24 hypersonic glide vehicles Yu-74, or a combination of warheads and massive amounts of countermeasures designed to defeat anti-missile systems; it was announced by the Russian military as a response to the US Prompt Global Strike.\nIn July 2023, North Korea fired a suspected intercontinental ballistic missile that landed short of Japanese waters. The launch follows North Korea's threat to retaliate against the US for alleged spy plane incursions.\nFlight phases.\nThe following flight phases can be distinguished:\nICBMs usually use the trajectory which optimizes range for a given amount of payload (the \"minimum-energy trajectory\"); an alternative is a depressed trajectory, which allows less payload, shorter flight time, and has a much lower apogee.\nModern ICBMs.\nModern ICBMs typically carry multiple independently targetable reentry vehicles (\"MIRVs\"), each of which carries a separate nuclear warhead, allowing a single missile to hit multiple targets. MIRV was an outgrowth of the rapidly shrinking size and weight of modern warheads and the Strategic Arms Limitation Treaties (SALT I and SALT II), which imposed limitations on the number of launch vehicles. It has also proved to be an \"easy answer\" to proposed deployments of anti-ballistic missile (ABM) systems: It is far less expensive to add more warheads to an existing missile system than to build an ABM system capable of shooting down the additional warheads; hence, most ABM system proposals have been judged to be impractical. The first operational ABM systems were deployed in the United States during the 1970s. The Safeguard ABM facility, located in North Dakota, was operational from 1975 to 1976. The Soviets deployed their ABM-1 Galosh system around Moscow in the 1970s, which remains in service. Israel deployed a national ABM system based on the Arrow missile in 1998, but it is mainly designed to intercept shorter-ranged theater ballistic missiles, not ICBMs. The Alaska-based United States national missile defense system attained initial operational capability in 2004.\nICBMs can be deployed from multiple platforms:\nThe last three kinds are mobile and therefore hard to detect prior to a missile launch. During storage, one of the most important features of the missile is its serviceability. One of the key features of the first computer-controlled ICBM, the Minuteman missile, was that it could quickly and easily use its computer to test itself.\nAfter launch, a booster pushes the missile and then falls away. Most modern boosters are solid-propellant rocket motors, which can be stored easily for long periods of time. Early missiles used liquid-fueled rocket motors. Many liquid-fueled ICBMs could not be kept fueled at all times as the cryogenic fuel liquid oxygen boiled off and caused ice formation, and therefore fueling the rocket was necessary before launch. This procedure was a source of significant operational delay and might allow the missiles to be destroyed by enemy counterparts before they could be used. To resolve this problem Nazi Germany invented the missile silo that protected the missile from strategic bombing and also hid fueling operations underground.\nAlthough the USSR/Russia preferred ICBM designs that use hypergolic liquid fuels, which can be stored at room temperature for more than a few years.\nOnce the booster falls away, the remaining \"bus\" releases several warheads, each of which continues on its own unpowered ballistic trajectory, much like an artillery shell or cannonball. The warhead is encased in a cone-shaped reentry vehicle and is difficult to detect in this phase of flight as there is no rocket exhaust or other emissions to mark its position to defenders. The high speeds of the warheads make them difficult to intercept and allow for little warning, striking targets many thousands of kilometers away from the launch site (and due to the possible locations of the submarines: anywhere in the world) within approximately 30 minutes.\nMany authorities say that missiles also release aluminized balloons, electronic noisemakers, and other decoys intended to confuse interception devices and radars.\nAs the nuclear warhead reenters the Earth's atmosphere, its high speed causes compression of the air, leading to a dramatic rise in temperature which would destroy it, if it were not shielded in some way. In one design, warhead components are contained within an aluminium honeycomb substructure, sheathed in a pyrolytic carbon-epoxy synthetic resin composite material heat shield. Warheads are also often radiation-hardened (to protect against nuclear armed ABMs or the nearby detonation of friendly warheads), one neutron-resistant material developed for this purpose in the UK is three-dimensional quartz phenolic.\nCircular error probable is crucial, because halving the circular error probable decreases the needed warhead energy by a factor of four. Accuracy is limited by the accuracy of the navigation system and the available geodetic information.\nStrategic missile systems are thought to use custom integrated circuits designed to calculate navigational differential equations thousands to millions of FLOPS in order to reduce navigational errors caused by calculation alone. These circuits are usually a network of binary addition circuits that continually recalculate the missile's position. The inputs to the navigation circuit are set by a general-purpose computer according to a navigational input schedule loaded into the missile before launch.\nOne particular weapon developed by the Soviet Union\u00a0\u2013 the Fractional Orbital Bombardment System\u00a0\u2013 had a partial orbital trajectory, and unlike most ICBMs its target could not be deduced from its orbital flight path. It was decommissioned in compliance with arms control agreements, which address the maximum range of ICBMs and prohibit orbital or fractional-orbital weapons. However, according to President Putin, Russia is working on the new Sarmat ICBM which leverages Fractional Orbital Bombardment concepts to use a southern polar approach instead of flying over the northern polar regions. It is theorized that, by using that approach, it could potentially avoid the American missile defense batteries in California and Alaska.\nNew development of ICBM technology are ICBMs able to carry hypersonic glide vehicles as a payload such as RS-28 Sarmat.\nOn 12 March 2024 India announced that it had joined a very limited group of countries, which are capable of firing multiple warheads on a single ICBM. The announcement came after successfully testing multiple independently targetable reentry vehicle (MIRV) technology.\nSpecific ICBMs.\nLand-based ICBMs.\n \n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Operational\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Under development\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Decommissioned or cancelled\nRussia, the United States, China, North Korea, India and Israel are the only countries currently known to possess land-based ICBMs.\nThe United States currently operates 405 ICBMs in three USAF bases. The only model deployed is LGM-30G Minuteman-III. All previous USAF Minuteman II missiles were destroyed in accordance with START II, and their launch silos have been sealed or sold to the public. The powerful MIRV-capable Peacekeeper missiles were phased out in 2005.\nThe Russian Strategic Rocket Forces have 286 ICBMs able to deliver 958 nuclear warheads: 46 silo-based R-36M2 (SS-18), 30 silo-based UR-100N (SS-19), 36 mobile RT-2PM \"Topol\" (SS-25), 60 silo-based RT-2UTTH \"Topol M\" (SS-27), 18 mobile RT-2UTTH \"Topol M\" (SS-27), 84 mobile RS-24 \"Yars\" (SS-29), and 12 silo-based RS-24 \"Yars\" (SS-29).\nChina has developed several long-range ICBMs, like the DF-31. The Dongfeng 5 or DF-5 is a 3-stage liquid fuel ICBM and has an estimated range of 13,000 kilometers. The DF-5 had its first flight in 1971 and was in operational service 10 years later. One of the downsides of the missile was that it took between 30 and 60 minutes to fuel. The Dong Feng 31 (a.k.a. CSS-10) is a medium-range, three-stage, solid-propellant intercontinental ballistic missile, and is a land-based variant of the submarine-launched JL-2.\nThe DF-41 or CSS-X-10 can carry up to 10 nuclear warheads, which are MIRVs and has a range of approximately . The DF-41 deployed underground in Xinjiang, Qinghai, Gansu and Inner Mongolia. The mysterious underground subway ICBM carrier systems are called the \"Underground Great Wall Project\".\nIsrael is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008. It is possible for the missile to be equipped with a single nuclear warhead or up to three MIRV warheads. It is believed to be based on the Shavit space launch vehicle and is estimated to have a range of . In November 2011 Israel tested an ICBM believed to be an upgraded version of the Jericho III.\nIndia has a series of ballistic missiles called Agni. On 19 April 2012, India successfully test fired its first Agni-V, a three-stage solid fueled missile, with a strike range of more than . Missile was test-fired for the second time on 15 September 2013. On 31 January 2015, India conducted a third successful test flight of the Agni-V from the Abdul Kalam Island facility. The test used a canisterised version of the missile, mounted over a Tata truck. On 15 December 2022, first night trial of Agni-V was successfully carried out by SFC from Abdul Kalam Island, Odisha. The missile is now 20 percent lighter because the use of composite materials rather than steel material. The range has been increased to 7,000 km.\nSubmarine-launched ICBMs.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Operational\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Under development\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Decommissioned or Cancelled\nMissile defense.\nAn anti-ballistic missile is a missile which can be deployed to counter an incoming nuclear or non-nuclear ICBM. ICBMs can be intercepted in three regions of their trajectory: boost phase, mid-course phase or terminal phase. The United States, Russia, India, France, Israel, and China have now developed anti-ballistic missile systems, of which the Russian A-135 anti-ballistic missile system, the American Ground-Based Midcourse Defense, the Indian Prithvi Defence Vehicle Mark-II and the Israeli Arrow 3 are the only systems having the capability to intercept and shoot down ICBMs carrying nuclear, chemical, biological, or conventional warheads.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14940", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=14940", "title": "Irish Sessions", "text": ""}
{"id": "14943", "revid": "43468913", "url": "https://en.wikipedia.org/wiki?curid=14943", "title": "Irish traditional music session", "text": "Mostly informal gathering at which people play Irish traditional music\nIrish traditional music sessions are mostly informal gatherings at which people play Irish traditional music. The Irish language word for \"session\" is \"seisi\u00fan\". This article discusses tune-playing, although \"session\" can also refer to a singing session or a mixed session (tunes and songs).\nBarry Foy's \"Field Guide to the Irish Music Session\" defines a session as:\n...a gathering of Irish traditional musicians for the purpose of celebrating their common interest in the music by playing it together in a relaxed, informal setting, while in the process generally beefing up the mystical cultural mantra that hums along uninterruptedly beneath all manifestations of Irishness worldwide.\nHistory.\nBefore the 1940s, Irish traditional music (both in Ireland and the diaspora) was typically played in private homes and farmyards and occasionally at dance halls. In Ireland, the UK, and Canada most public houses (\"pubs\") and taverns were not legally allowed to host music in the early 20th Century. This division between folk music at home and popular commercial music in bars held on to some in Eastern Canada, where the name \"kitchen party\" denotes a gathering of folk musicians.\nIn the post-war era, social dancing developed in new trends based on jazz and later rock and roll, which displaced traditional music from dance halls (a similar trend happened to Central European \"polka music\" in North America). The session as understood today was purportedly invented in 1946 at the Devonshire Arms pub in Kentish Town, London, UK by a group of Irish emigrants from the Irish west coast, particularly near the town of Tubbercurry.\nSocial and cultural aspects.\nThe general session scheme is that someone starts a tune, and those who know it join in. Good session etiquette requires not playing if one does not know the tune (or at least quietly playing an accompaniment part) and waiting until a tune one knows comes along. In an \"open\" session, anyone who is able to play Irish music is welcome. Most often there are more-or-less recognized session leaders; sometimes there are no leaders. At times a song will be sung or a slow air played by a single musician between sets.\nLocations and times.\nSessions are usually held in public houses or taverns. A pub owner might have one or two musicians paid to come regularly in order for the session to have a base. These musicians can perform during any gaps during the day or evening when no other performers are there and wish to play. Sunday afternoons and weekday nights (especially Tuesday and Wednesday) are common times for sessions to be scheduled, on the theory that these are the least likely times for dances and concerts to be held, and therefore the times that professional musicians will be most able to show up.\nSessions can be held in homes or at various public places in addition to pubs; often at a festival sessions will be got together in the beer tent or in the vendor's booth of a music-loving craftsperson or dealer. When a particularly large musical event \"takes over\" an entire village, spontaneous sessions may erupt on the street corners. Sessions may also take place occasionally at wakes. House sessions are not as common now as they were in the past. In her book \"Peig\", Peig Sayers notes that when she was young they often attended sessions at people's houses, in a practice called 'both\u00e1ntiocht'.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14946", "revid": "461300", "url": "https://en.wikipedia.org/wiki?curid=14946", "title": "Ice", "text": "Frozen water: the solid state of water\nIce is water that is frozen into a solid state, typically forming at or below temperatures of 0 \u00b0C, 32 \u00b0F, or 273.15 K. It occurs naturally on Earth, on other planets, in Oort cloud objects, and as interstellar ice. As a naturally occurring crystalline inorganic solid with an ordered structure, ice is considered to be a mineral. Depending on the presence of impurities such as particles of soil or bubbles of air, it can appear transparent or a more or less opaque bluish-white color. \nVirtually all of the ice on Earth is of a hexagonal crystalline structure denoted as \"ice Ih\" (spoken as \"ice one h\"). Depending on temperature and pressure, at least nineteen phases (packing geometries) can exist. The most common phase transition to ice Ih occurs when liquid water is cooled below (, ) at standard atmospheric pressure. When water is cooled rapidly (quenching), up to three types of amorphous ice can form. Interstellar ice is overwhelmingly low-density amorphous ice (LDA), which likely makes LDA ice the most abundant type in the universe. When cooled slowly, correlated proton tunneling occurs below (, ) giving rise to macroscopic quantum phenomena. \nIce is abundant on the Earth's surface, particularly in the polar regions and above the snow line, where it can aggregate from snow to form glaciers and ice sheets. As snowflakes and hail, ice is a common form of precipitation, and it may also be deposited directly by water vapor as frost. The transition from ice to water is melting and from ice directly to water vapor is sublimation. These processes play a key role in Earth's water cycle and climate. In the recent decades, ice volume on Earth has been decreasing due to climate change. The largest declines have occurred in the Arctic and in the mountains located outside of the polar regions. The loss of grounded ice (as opposed to floating sea ice) is the primary contributor to sea level rise.\nHumans have been using ice for various purposes for thousands of years. Some historic structures designed to hold ice to provide cooling are over 2,000 years old. Before the invention of refrigeration technology, the only way to safely store food without modifying it through preservatives was to use ice. Sufficiently solid surface ice makes waterways accessible to land transport during winter, and dedicated ice roads may be maintained. Ice also plays a major role in winter sports.\nPhysical properties.\nIce possesses a regular crystalline structure based on the molecule of water, which consists of a single oxygen atom covalently bonded to two hydrogen atoms, or H\u2013O\u2013H. However, many of the physical properties of water and ice are controlled by the formation of hydrogen bonds between adjacent oxygen and hydrogen atoms; while it is a weak bond, it is nonetheless critical in controlling the structure of both water and ice.\nAn unusual property of water is that its solid form\u2014ice frozen at atmospheric pressure\u2014is approximately 8.3% less dense than its liquid form; this is equivalent to a volumetric expansion of 9%. The density of ice is 0.9167\u20130.9168\u00a0g/cm3 at 0\u00a0\u00b0C and standard atmospheric pressure (101,325\u00a0Pa), whereas water has a density of 0.9998\u20130.999863\u00a0g/cm3 at the same temperature and pressure. Liquid water is densest, essentially 1.00\u00a0g/cm3, at 4\u00a0\u00b0C and begins to lose its density as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid. The density of ice increases slightly with decreasing temperature and has a value of 0.9340\u00a0g/cm3 at \u2212180\u00a0\u00b0C (93\u00a0K).\nWhen water freezes, it increases in volume (about 9% for fresh water). The effect of expansion during freezing can be dramatic, and ice expansion is a basic cause of freeze-thaw weathering of rock in nature and damage to building foundations and roadways from frost heaving. It is also a common cause of the flooding of houses when water pipes burst due to the pressure of expanding water when it freezes.\nBecause ice is less dense than liquid water, it floats, and this prevents bottom-up freezing of the bodies of water. Instead, a sheltered environment for animal and plant life is formed beneath the floating ice, which protects the underside from short-term weather extremes such as wind chill. Sufficiently thin floating ice allows light to pass through, supporting the photosynthesis of bacterial and algal colonies. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids. In turn, they provide food for animals such as krill and specialized fish like the bald notothen, fed upon in turn by larger animals such as emperor penguins and minke whales.\nWhen ice melts, it absorbs as much energy as it would take to heat an equivalent mass of water by . During the melting process, the temperature remains constant at . While melting, any energy added breaks the hydrogen bonds between ice (water) molecules. Energy becomes available to increase the thermal energy (temperature) only after enough hydrogen bonds are broken that the ice can be considered liquid water. The amount of energy consumed in breaking hydrogen bonds in the transition from ice to water is known as the \"heat of fusion\".\nAs with water, ice absorbs light at the red end of the spectrum preferentially as the result of an overtone of an oxygen\u2013hydrogen (O\u2013H) bond stretch. Compared with water, this absorption is shifted toward slightly lower energies. Thus, ice appears blue, with a slightly greener tint than liquid water. Since absorption is cumulative, the color effect intensifies with increasing thickness or if internal reflections cause the light to take a longer path through the ice. Other colors can appear in the presence of light absorbing impurities, where the impurity is dictating the color rather than the ice itself. For instance, icebergs containing impurities (e.g., sediments, algae, air bubbles) can appear brown, grey or green.\nBecause ice in natural environments is usually close to its melting temperature, its hardness shows pronounced temperature variations. At its melting point, ice has a Mohs hardness of 2 or less, but the hardness increases to about 4 at a temperature of and to 6 at a temperature of , the vaporization point of solid carbon dioxide (dry ice).\nPhases.\nMost liquids under increased pressure freeze at \"higher\" temperatures because the pressure helps to hold the molecules together. However, the strong hydrogen bonds in water make it different: for some pressures higher than , water freezes at a temperature \"below\" . Ice, water, and water vapour can coexist at the triple point, which is exactly at a pressure of 611.657\u00a0Pa. The kelvin was defined as of the difference between this triple point and absolute zero, though this definition changed in May 2019. Unlike most other solids, ice is difficult to superheat. In an experiment, ice at \u22123\u00a0\u00b0C was superheated to about 17\u00a0\u00b0C for about 250 picoseconds.\nSubjected to higher pressures and varying temperatures, ice can form in nineteen separate known crystalline phases at various densities, along with hypothetical proposed phases of ice that have not been observed. With care, at least fifteen of these phases (one of the known exceptions being ice X) can be recovered at ambient pressure and low temperature in metastable form. The types are differentiated by their crystalline structure, proton ordering, and density. There are also two metastable phases of ice under pressure, both fully hydrogen-disordered; these are Ice IV and Ice XII. Ice XII was discovered in 1996. In 2006, Ice XIII and Ice XIV were discovered. Ices XI, XIII, and XIV are hydrogen-ordered forms of ices Ih, V, and XII respectively. In 2009, ice XV was found at extremely high pressures and \u2212143\u00a0\u00b0C. At even higher pressures, ice is predicted to become a metal; this has been variously estimated to occur at 1.55\u00a0TPa or 5.62\u00a0TPa.\nAs well as crystalline forms, solid water can exist in amorphous states as amorphous solid water (ASW) of varying densities. In outer space, hexagonal crystalline ice is present in the ice volcanoes, but is extremely rare otherwise. Even icy moons like Ganymede are expected to mainly consist of other crystalline forms of ice. Water in the interstellar medium is dominated by amorphous ice, making it likely the most common form of water in the universe. Low-density ASW (LDA), also known as hyperquenched glassy water, may be responsible for noctilucent clouds on Earth and is usually formed by deposition of water vapor in cold or vacuum conditions. High-density ASW (HDA) is formed by compression of ordinary ice Ih or LDA at GPa pressures. Very-high-density ASW (VHDA) is HDA slightly warmed to 160\u00a0K under 1\u20132\u00a0GPa pressures.\nIce from a theorized superionic water may possess two crystalline structures. At pressures in excess of such \"superionic ice\" would take on a body-centered cubic structure. However, at pressures in excess of the structure may shift to a more stable face-centered cubic lattice. It is speculated that superionic ice could compose the interior of ice giants such as Uranus and Neptune.\nFriction properties.\nIce is \"slippery\" because it has a low coefficient of friction. This subject was first scientifically investigated in the 19th century. The preferred explanation at the time was \"pressure melting\" \u2013 i.e. the blade of an ice skate, upon exerting pressure on the ice, would melt a thin layer, providing sufficient lubrication for the blade to glide across the ice. Yet, research in 1939 by Frank P. Bowden and T. P. Hughes found that skaters would experience a lot more friction than they actually do if it were the only explanation. Further, the optimum temperature for figure skating is and for ice hockey; yet, according to pressure melting theory, skating below would be outright impossible. Instead, Bowden and Hughes argued that heating and melting of the ice layer is caused by friction. However, this theory does not sufficiently explain why ice is slippery when standing still even at below-zero temperatures.\nSubsequent research suggested that ice molecules at the interface cannot properly bond with the molecules of the mass of ice beneath (and thus are free to move like molecules of liquid water). These molecules remain in a semi-liquid state, providing lubrication regardless of pressure against the ice exerted by any object. However, the significance of this hypothesis is disputed by experiments showing a high coefficient of friction for ice using atomic force microscopy. Thus, the mechanism controlling the frictional properties of ice is still an active area of scientific study. A comprehensive theory of ice friction must take into account all of the aforementioned mechanisms to estimate friction coefficient of ice against various materials as a function of temperature and sliding speed. Research in 2014 suggests that frictional heating is the most important process under most typical conditions.\nNatural formation.\nThe term that collectively describes all of the parts of the Earth's surface where water is in frozen form is the \"cryosphere.\" Ice is an important component of the global climate, particularly in regard to the water cycle. Glaciers and snowpacks are an important storage mechanism for fresh water; over time, they may sublimate or melt. Snowmelt is an important source of seasonal fresh water. The World Meteorological Organization defines several kinds of ice depending on origin, size, shape, influence and so on. Clathrate hydrates are forms of ice that contain gas molecules trapped within their crystal lattice.\nIn the oceans.\nIce that is found at sea may be in the form of drift ice floating in the water, fast ice fixed to a shoreline or anchor ice if attached to the seafloor. Ice which calves (breaks off) from an ice shelf or a coastal glacier may become an iceberg. The aftermath of calving events produces a loose mixture of snow and ice known as ice m\u00e9lange. \nSea ice forms in several stages. At first, small, millimeter-scale crystals accumulate on the water surface in what is known as frazil ice. As they become somewhat larger and more consistent in shape and cover, the water surface begins to look \"oily\" from above, so this stage is called grease ice. Then, ice continues to clump together, and solidify into flat cohesive pieces known as ice floes. Ice floes are the basic building blocks of sea ice cover, and their horizontal size (defined as half of their diameter) varies dramatically, with the smallest measured in centimeters and the largest in hundreds of kilometers. An area which is over 70% ice on its surface is said to be covered by pack ice. \nFully formed sea ice can be forced together by currents and winds to form pressure ridges up to tall. On the other hand, active wave activity can reduce sea ice to small, regularly shaped pieces, known as pancake ice. Sometimes, wind and wave activity \"polishes\" sea ice to perfectly spherical pieces known as ice eggs.\nOn land.\nThe largest ice formations on Earth are the two ice sheets which almost completely cover the world's largest island, Greenland, and the continent of Antarctica. These ice sheets have an average thickness of over and have existed for millions of years. \nOther major ice formations on land include ice caps, ice fields, ice streams and glaciers. In particular, the Hindu Kush region is known as the Earth's \"Third Pole\" due to the large number of glaciers it contains. They cover an area of around , and have a combined volume of between 3,000 and 4,700\u00a0km3. These glaciers are nicknamed \"Asian water towers\", because their meltwater run-off feeds into rivers which provide water for an estimated two billion people.\nPermafrost is soil or underwater sediment which continuously remains below for two years or more. The ice within permafrost is divided into four categories: pore ice, vein ice (also known as ice wedges), buried surface ice and intrasedimental ice (from the freezing of underground waters). One example of ice formation in permafrost areas is aufeis \u2013 layered ice that forms in Arctic and subarctic stream valleys. Ice, frozen in the stream bed, blocks normal groundwater discharge, and causes the local water table to rise, resulting in water discharge on top of the frozen layer. This water then freezes, causing the water table to rise further and repeat the cycle. The result is a stratified ice deposit, often several meters thick. Snow line and snow fields are two related concepts, in that snow fields accumulate on top of and ablate away to the equilibrium point (the snow line) in an ice deposit.\nOn rivers and streams.\nIce which forms on moving water tends to be less uniform and stable than ice which forms on calm water. Ice jams (sometimes called \"ice dams\"), when broken chunks of ice pile up, are the greatest ice hazard on rivers. Ice jams can cause flooding, damage structures in or near the river, and damage vessels on the river. Ice jams can cause some hydropower industrial facilities to completely shut down. An ice dam is a blockage from the movement of a glacier which may produce a proglacial lake. Heavy ice flows in rivers can also damage vessels and require the use of an icebreaker vessel to keep navigation possible.\nIce discs are circular formations of ice floating on river water. They form within eddy currents, and their position results in asymmetric melting, which makes them continuously rotate at a low speed.\nOn lakes.\nIce forms on calm water from the shores, a thin layer spreading across the surface, and then downward. Ice on lakes is generally four types: primary, secondary, superimposed and agglomerate. Primary ice forms first. Secondary ice forms below the primary ice in a direction parallel to the direction of the heat flow. Superimposed ice forms on top of the ice surface from rain or water which seeps up through cracks in the ice which often settles when loaded with snow. An ice shove occurs when ice movement, caused by ice expansion and/or wind action, occurs to the extent that ice pushes onto the shores of lakes, often displacing sediment that makes up the shoreline.\nShelf ice is formed when floating pieces of ice are driven by the wind piling up on the windward shore. This kind of ice may contain large air pockets under a thin surface layer, which makes it particularly hazardous to walk across it. Another dangerous form of rotten ice to traverse on foot is candle ice, which develops in columns perpendicular to the surface of a lake. Because it lacks a firm horizontal structure, a person who has fallen through has nothing to hold onto to pull themselves out.\nAs precipitation.\nSnow and freezing rain.\nSnow crystals form when tiny supercooled cloud droplets (about 10\u00a0\u03bcm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice; then the droplet freezes around this \"nucleus\". Experiments show that this \"homogeneous\" nucleation of cloud droplets only occurs at temperatures lower than . In warmer clouds an aerosol particle or \"ice nucleus\" must be present in (or in contact with) the droplet to act as a nucleus. Our understanding of what particles make efficient ice nuclei is poor\u00a0\u2013 what we do know is they are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei are used in cloud seeding. The droplet then grows by condensation of water vapor onto the ice surfaces.\nAn ice storm is a type of winter storm characterized by freezing rain, which produces a glaze of ice on surfaces, including roads and power lines. In the United States, a quarter of winter weather events produce glaze ice, and utilities need to be prepared to minimize damages.\nHard forms.\nHail forms in storm clouds when supercooled water droplets freeze on contact with condensation nuclei, such as dust or dirt. The storm's updraft blows the hailstones to the upper part of the cloud. The updraft dissipates and the hailstones fall down, back into the updraft, and are lifted up again. Hail has a diameter of or more. Within METAR code, GR is used to indicate larger hail, of a diameter of at least and GS for smaller. Stones of , and are the most frequently reported hail sizes in North America. Hailstones can grow to and weigh more than . In large hailstones, latent heat released by further freezing may melt the outer shell of the hailstone. The hailstone then may undergo 'wet growth', where the liquid outer shell collects other smaller hailstones. The hailstone gains an ice layer and grows increasingly larger with each ascent. Once a hailstone becomes too heavy to be supported by the storm's updraft, it falls from the cloud.\nHail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing . Hail-producing clouds are often identifiable by their green coloration. The growth rate is maximized at about , and becomes vanishingly small much below as supercooled water droplets become rare. For this reason, hail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of . Entrainment of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporative cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is actually less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater depth. Hail in the tropics occurs mainly at higher elevations.\nIce pellets (METAR code \"PL\") are a form of precipitation consisting of small, translucent balls of ice, which are usually smaller than hailstones. This form of precipitation is also referred to as \"sleet\" by the United States National Weather Service. (In British English \"sleet\" refers to a mixture of rain and snow.) Ice pellets typically form alongside freezing rain, when a wet warm front ends up between colder and drier atmospheric layers. There, raindrops would both freeze and shrink in size due to evaporative cooling. So-called snow pellets, or graupel, form when multiple water droplets freeze onto snowflakes until a soft ball-like shape is formed. So-called \"diamond dust\" (METAR code \"IC\"), also known as ice needles or ice crystals, forms at temperatures approaching due to air with slightly higher moisture from aloft mixing with colder, surface-based air.\nOn surfaces.\nAs water drips and re-freezes, it can form hanging icicles, or stalagmite-like structures on the ground. On sloped roofs, buildup of ice can produce an ice dam, which stops melt water from draining properly and potentially leads to damaging leaks. More generally, water vapor depositing onto surfaces due to high relative humidity and then freezing results in various forms of atmospheric icing, or frost. Inside buildings, this can be seen as ice on the surface of un-insulated windows. Hoar frost is common in the environment, particularly in the low-lying areas such as valleys. In Antarctica, temperatures can be so low that electrostatic attraction is increased to the point hoarfrost on snow sticks together when blown by wind into tumbleweed-like balls known as yukimarimo. \nSometimes, drops of water crystallize on cold objects as rime instead of glaze. Soft rime has a density between a quarter and two thirds that of pure ice, due to a high proportion of trapped air, which also makes soft rime appear white. Hard rime is denser, more transparent, and more likely to appear on ships and aircraft. Cold wind specifically causes what is known as \"advection frost\" when it collides with objects. When it occurs on plants, it often causes damage to them. Various methods exist to protect agricultural crops from frost - from simply covering them to using wind machines. In recent decades, irrigation sprinklers have been calibrated to spray just enough water to preemptively create a layer of ice that would form slowly and so avoid a sudden temperature shock to the plant, and not be so thick as to cause damage with its weight.\nAblation.\nAblation of ice refers to both its melting and its dissolution.\nThe melting of ice entails the breaking of hydrogen bonds between the water molecules. The ordering of the molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. This is achieved by increasing the internal energy of the ice beyond the melting point. When ice melts it absorbs as much energy as would be required to heat an equivalent amount of water by 80\u00a0\u00b0C. While melting, the temperature of the ice surface remains constant at 0\u00a0\u00b0C. The rate of the melting process depends on the efficiency of the energy exchange process. An ice surface in fresh water melts solely by free convection with a rate that depends linearly on the water temperature, \"T\"\u221e, when \"T\"\u221e is less than 3.98\u00a0\u00b0C, and superlinearly when \"T\"\u221e is equal to or greater than 3.98\u00a0\u00b0C, with the rate being proportional to (T\u221e\u00a0\u2212\u00a03.98\u00a0\u00b0C)\"\u03b1\", with \"\u03b1\"\u00a0=\u00a0 for \"T\"\u221e much greater than 8\u00a0\u00b0C, and \u03b1\u00a0=\u00a0 for in between temperatures \"T\"\u221e.\nIn salty ambient conditions, dissolution rather than melting often causes the ablation of ice. For example, the temperature of the Arctic Ocean is generally below the melting point of ablating sea ice. The phase transition from solid to liquid is achieved by mixing salt and water molecules, similar to the dissolution of sugar in water, even though the water temperature is far below the melting point of the sugar. However, the dissolution rate is limited by salt concentration and is therefore slower than melting.\nRole in human activities.\nCooling.\nIce has long been valued as a means of cooling. In 400 BC Iran, Persian engineers had already developed techniques for ice storage in the desert through the summer months. During the winter, ice was transported from harvesting pools and nearby mountains in large quantities to be stored in specially designed, naturally cooled \"refrigerators\", called yakhchal (meaning \"ice storage\"). Yakhchals were large underground spaces (up to 5000\u00a0m3) that had thick walls (at least two meters at the base) made of a specific type of mortar called \"sarooj\" made from sand, clay, egg whites, lime, goat hair, and ash. The mortar was resistant to heat transfer, helping to keep the ice cool enough not to melt; it was also impenetrable by water. Yakhchals often included a qanat and a system of windcatchers that could lower internal temperatures to frigid levels, even during the heat of the summer. One use for the ice was to create chilled treats for royalty.\nHarvesting.\nThere were thriving industries in 16th\u201317th century England whereby low-lying areas along the Thames Estuary were flooded during the winter, and ice harvested in carts and stored inter-seasonally in insulated wooden houses as a provision to an icehouse often located in large country houses, and widely used to keep fish fresh when caught in distant waters. This was allegedly copied by an Englishman who had seen the same activity in China. Ice was imported into England from Norway on a considerable scale as early as 1823.\nIn the United States, the first cargo of ice was sent from New York City to Charleston, South Carolina, in 1799, and by the first half of the 19th century, ice harvesting had become a big business. Frederic Tudor, who became known as the \"Ice King\", worked on developing better insulation products for long distance shipments of ice, especially to the tropics; this became known as the ice trade.\nTrieste sent ice to Egypt, Corfu, and Zante; Switzerland, to France; and Germany sometimes was supplied from Bavarian lakes. From 1930s and up until 1994, the Hungarian Parliament building used ice harvested in the winter from Lake Balaton for air conditioning.\nIce houses were used to store ice formed in the winter, to make ice available all year long, and an early type of refrigerator known as an icebox was cooled using a block of ice placed inside it. Many cities had a regular ice delivery service during the summer. The advent of artificial refrigeration technology made the delivery of ice obsolete.\nIce is still harvested for ice and snow sculpture events. For example, a swing saw is used to get ice for the Harbin International Ice and Snow Sculpture Festival each year from the frozen surface of the Songhua River.\nArtificial production.\nThe earliest known written process to artificially make ice is by the 13th-century writings of Arab historian Ibn Abu Usaybia in his book \"Kitab Uyun al-anba fi tabaqat-al-atibba\" concerning medicine in which Ibn Abu Usaybia attributes the process to an even older author, Ibn Bakhtawayhi, of whom nothing is known.\nIce is now produced on an industrial scale, for uses including food storage and processing, chemical manufacturing, concrete mixing and curing, and consumer or packaged ice. Most commercial icemakers produce three basic types of fragmentary ice: flake, tubular and plate, using a variety of techniques. Large batch ice makers can produce up to 75 tons of ice per day. In 2002, there were 426 commercial ice-making companies in the United States, with a combined value of shipments of $595,487,000. Home refrigerators can also make ice with a built in icemaker, which will typically make ice cubes or crushed ice. The first such device was presented in 1965 by Frigidaire.\nLand travel.\nIce forming on roads is a common winter hazard, and black ice particularly dangerous because it is very difficult to see. It is both very transparent, and often forms specifically in shaded (and therefore cooler and darker) areas, i.e. beneath overpasses.\nWhenever there is freezing rain or snow which occurs at a temperature near the melting point, it is common for ice to build up on the windows of vehicles. Often, snow melts, re-freezes, and forms a fragmented layer of ice which effectively \"glues\" snow to the window. In this case, the frozen mass is commonly removed with ice scrapers. A thin layer of ice crystals can also form on the inside surface of car windows during sufficiently cold weather. In the 1970s and 1980s, some vehicles such as the Ford Thunderbird could be upgraded with heated windshields as the result. This technology fell out of style as it was too expensive and prone to damage, but rear-window defrosters are cheaper to maintain and so are more widespread.\nIn sufficiently cold places, the layers of ice on water surfaces can get thick enough for ice roads to be built. Some regulations specify that the minimum safe thickness is for a person, for a snowmobile and for an automobile lighter than 5 tonnes. For trucks, effective thickness varies with load \u2013 e.g. a vehicle with 9-ton total weight requires a thickness of . Notably, the speed limit for a vehicle moving at a road which meets its minimum safe thickness is , going up to if the road's thickness is 2 or more times larger than the minimum safe value. There is a known instance where a railroad has been built on ice.\nThe Road of Life across Lake Ladoga operated in the winters of 1941\u20131942 and 1942\u20131943, when it was the only land route available to the Soviet Union to relieve the Siege of Leningrad by the German Army Group North. The trucks moved hundreds of thousands tonnes of supplies into the city, and hundreds of thousands of civilians were evacuated. It is now a World Heritage Site.\nWater-borne travel.\nFor ships, ice presents two distinct hazards. Firstly, spray and freezing rain can produce an ice build-up on the superstructure of a vessel sufficient to make it unstable, potentially to the point of capsizing. Earlier, crewmembers were regularly forced to manually hack off ice build-up. After 1980s, spraying de-icing chemicals or melting the ice through hot water/steam hoses became more common. Secondly, icebergs\u00a0\u2013 large masses of ice floating in water (typically created when glaciers reach the sea)\u00a0\u2013 can be dangerous if struck by a ship when underway. Icebergs have been responsible for the sinking of many ships, notably the RMS \"Titanic\".\nFor harbors near the poles, being ice-free, ideally all year long, is an important advantage. Examples are Murmansk (Russia), Petsamo (Russia, formerly Finland), and Vard\u00f8 (Norway). Harbors which are not ice-free are opened up using specialized vessels, called icebreakers. Icebreakers are also used to open routes through the sea ice for other vessels, as the only alternative is to find the openings called \"polynyas\" or \"leads\". A widespread production of icebreakers began during the 19th century. Earlier designs simply had reinforced bows in a spoon-like or diagonal shape to effectively crush the ice. Later designs attached a forward propeller underneath the protruding bow, as the typical rear propellers were incapable of effectively steering the ship through the ice.\nAir travel.\nFor aircraft, ice can cause a number of dangers. As an aircraft climbs, it passes through air layers of different temperature and humidity, some of which may be conducive to ice formation. If ice forms on the wings or control surfaces, this may adversely affect the flying qualities of the aircraft. In 1919, during the first non-stop flight across the Atlantic, the British aviators Captain John Alcock and Lieutenant Arthur Whitten Brown encountered such icing conditions\u00a0\u2013 Brown left the cockpit and climbed onto the wing several times to remove ice which was covering the engine air intakes of the Vickers Vimy aircraft they were flying.\nOne vulnerability effected by icing that is associated with reciprocating internal combustion engines is the carburetor. As air is sucked through the carburetor into the engine, the local air pressure is lowered, which causes adiabatic cooling. Thus, in humid near-freezing conditions, the carburetor will be colder, and tend to ice up. This will block the supply of air to the engine, and cause it to fail. Between 1969 and 1975, 468 such instances were recorded, causing 75 aircraft losses, 44 fatalities and 202 serious injuries. Thus, carburetor air intake heaters were developed. Further, reciprocating engines with fuel injection do not require carburetors in the first place.\nJet engines do not experience carburetor icing, but they can be affected by the moisture inherently present in jet fuel freezing and forming ice crystals, which can potentially clog up fuel intake to the engine. Fuel heaters and/or de-icing additives are used to address the issue.\nRecreation and sports.\nIce plays a central role in winter recreation and in many sports such as ice skating, tour skating, ice hockey, bandy, ice fishing, ice climbing, curling, broomball and sled racing on bobsled, luge and skeleton. Many of the different sports played on ice get international attention every four years during the Winter Olympic Games.\nSmall boat-like craft can be mounted on blades and be driven across the ice by sails. This sport is known as ice yachting, and it had been practiced for centuries. Another vehicular sport is ice racing, where drivers must speed on lake ice, while also controlling the skid of their vehicle (similar in some ways to dirt track racing). The sport has even been modified for ice rinks.\nImpacts of climate change.\nHistorical.\nGreenhouse gas emissions from human activities unbalance the Earth's energy budget and so cause an accumulation of heat. About 90% of that heat is added to ocean heat content, 1% is retained in the atmosphere and 3\u20134% goes to melt major parts of the cryosphere. Between 1994 and 2017, 28 trillion tonnes of ice were lost around the globe as the result. Arctic sea ice decline accounted for the single largest loss (7.6 trillion tonnes), followed by the melting of Antarctica's ice shelves (6.5 trillion tonnes), the retreat of mountain glaciers (6.1 trillion tonnes), the melting of the Greenland ice sheet (3.8 trillion tonnes) and finally the melting of the Antarctic ice sheet (2.5 trillion tonnes) and the limited losses of the sea ice in the Southern Ocean (0.9 trillion tonnes).\nOther than the sea ice (which already displaces water due to Archimedes' principle), these losses are a major cause of sea level rise (SLR) and they are expected to intensify in the future. In particular, the melting of the West Antarctic ice sheet may accelerate substantially as the floating ice shelves are lost and can no longer buttress the glaciers. This would trigger poorly understood marine ice sheet instability processes, which could then increase the SLR expected for the end of the century (between and , depending on future warming), by tens of centimeters more. \nIce loss in Greenland and Antarctica also produces large quantities of fresh meltwater, which disrupts the Atlantic meridional overturning circulation (AMOC) and the Southern Ocean overturning circulation, respectively. These two halves of the thermohaline circulation are very important for the global climate. A continuation of high meltwater flows may cause a severe disruption (up to a point of a \"collapse\") of either circulation, or even both of them. Either event would be considered an example of tipping points in the climate system, because it would be extremely difficult to reverse. AMOC is generally not expected to collapse during the 21st century, while there is only limited knowledge about the Southern Ocean circulation.\nAnother example of ice-related tipping point is permafrost thaw. While the organic content in the permafrost causes CO2 and methane emissions once it thaws and begins to decompose, ice melting liqufies the ground, causing anything built above the former permafrost to collapse. By 2050, the economic damages from such infrastructure loss are expected to cost tens of billions of dollars.\nPredictions.\nIn the future, the Arctic Ocean is likely to lose effectively all of its sea ice during at least some Septembers (the end of the ice melting season), although some of the ice would refreeze during the winter. That is, an ice-free September is likely to occur once in every 40 years if global warming is at , but would occur once in every 8 years at and once in every 1.5 years at . This would affect the regional and global climate due to the ice-albedo feedback. Because ice is highly reflective of solar energy, persistent sea ice cover lowers local temperatures. Once that ice cover melts, the darker ocean waters begin to absorb more heat, which also helps to melt the remaining ice.\nGlobal losses of sea ice between 1992 and 2018, almost all of them in the Arctic, have already had the same impact as 10% of greenhouse gas emissions over the same period. If all the Arctic sea ice was gone every year between June and September (polar day, when the Sun is constantly shining), temperatures in the Arctic would increase by over , while the global temperatures would increase by around .\nBy 2100, at least a quarter of mountain glaciers outside of Greenland and Antarctica would melt, and effectively all ice caps on non-polar mountains are likely to be lost around 200 years after global warming reaches . The West Antarctic ice sheet is highly vulnerable and will likely disappear even if the warming does not progress further, although it could take around 2,000 years before its loss is complete. The Greenland ice sheet will most likely be lost with the sustained warming between and , although its total loss requires around 10,000 years. Finally, the East Antarctic ice sheet will take at least 10,000 years to melt entirely, which requires a warming of between and .\nIf all the ice on Earth melted, it would result in about of sea level rise, with some coming from East Antarctica. Due to isostatic rebound, the ice-free land would eventually become higher in Greenland and\u2009 in Antarctica, on average. Areas in the center of each landmass would become up to and\u2009 higher, respectively. The impact on global temperatures from losing West Antartica, mountain glaciers and the Greenland ice sheet is estimated at , and , respectively, while the lack of the East Antarctic ice sheet would increase the temperatures by .\nNon-water.\nThe solid phases of several other volatile substances are also referred to as \"ices\"; generally a volatile is classed as an ice if its melting or sublimation point lies above or around (assuming standard atmospheric pressure). The best known example is dry ice, the solid form of carbon dioxide. Its sublimation/deposition point occurs at .\nA \"magnetic analogue\" of ice is also realized in some insulating magnetic materials in which the magnetic moments mimic the position of protons in water ice and obey energetic constraints similar to the Bernal-Fowler ice rules arising from the geometrical frustration of the proton configuration in water ice. These materials are called spin ice.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14947", "revid": "41", "url": "https://en.wikipedia.org/wiki?curid=14947", "title": "Instantaneous dipole attraction", "text": ""}
{"id": "14948", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=14948", "title": "Induced-dipole attraction", "text": ""}
{"id": "14950", "revid": "34991604", "url": "https://en.wikipedia.org/wiki?curid=14950", "title": "Instantaneous-dipole induced-dipole attraction", "text": ""}
{"id": "14951", "revid": "6941696", "url": "https://en.wikipedia.org/wiki?curid=14951", "title": "Ionic bonding", "text": "Chemical bonding involving attraction between ions\nIonic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions, or between two atoms with sharply different electronegativities, and is the primary interaction occurring in ionic compounds. It is one of the main types of bonding, along with covalent bonding and metallic bonding. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions). Atoms that lose electrons make positively charged ions (called cations). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be more complex, e.g. polyatomic ions like NH4+ or SO42\u2212. In simpler words, an ionic bond results from the transfer of electrons from a metal to a non-metal to obtain a full valence shell for both atoms.\n\"Clean\" ionic bonding \u2013 in which one atom or molecule completely transfers an electron to another \u2013 cannot exist: all ionic compounds have some degree of covalent bonding or electron sharing. Thus, the term \"ionic bonding\" is given when the ionic character is greater than the covalent character \u2013 that is, a bond in which there is a large difference in electronegativity between the cation and anion, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent characters are called polar covalent bonds.\nIonic compounds conduct electricity when molten or in solution, typically not when solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility.\nOverview.\nAtoms that have an almost full or almost empty valence shell tend to be very reactive. Strongly electronegative atoms (such as halogens) often have only one or two empty electron states in their valence shell, and frequently bond with other atoms or gain electrons to form anions. Weakly electronegative atoms (such as alkali metals) have relatively few valence electrons, which can easily be lost to strongly electronegative atoms. As a result, weakly electronegative atoms tend to distort their electron cloud and form cations.\nFormation.\nIonic bonding can result from a redox reaction when atoms of an element (usually metal), whose ionization energy is low, give some of their electrons to achieve a stable electron configuration. In doing so, cations are formed. An atom of another element (usually nonmetal) with greater electron affinity accepts one or more electrons to attain a stable electron configuration, and after accepting electrons an atom becomes an anion. Typically, the stable electron configuration is one of the noble gases for elements in the s-block and the p-block, and particular stable electron configurations for d-block and f-block elements. The electrostatic attraction between the anions and cations leads to the formation of a solid with a crystallographic lattice in which the ions are stacked in an alternating fashion. In such a lattice, it is usually not possible to distinguish discrete molecular units, so that the compounds formed are not molecular. However, the ions themselves can be complex and form molecular ions like the acetate anion or the ammonium cation.\nFor example, common table salt is sodium chloride. When sodium (Na) and chlorine (Cl) are combined, the sodium atoms each lose an electron, forming cations (Na+), and the chlorine atoms each gain an electron to form anions (Cl\u2212). These ions are then attracted to each other in a 1:1 ratio to form sodium chloride (NaCl).\n Na + Cl \u2192 Na+ + Cl\u2212 \u2192 NaCl \nHowever, to maintain charge neutrality, strict ratios between anions and cations are observed so that ionic compounds, in general, obey the rules of stoichiometry despite not being molecular compounds. For compounds that are transitional to the alloys and possess mixed ionic and metallic bonding, this may not be the case anymore. Many sulfides, e.g., do form non-stoichiometric compounds.\nMany ionic compounds are referred to as salts as they can also be formed by the neutralization reaction of an Arrhenius base like NaOH with an Arrhenius acid like HCl\n NaOH + HCl \u2192 NaCl + H2O\nThe salt NaCl is then said to consist of the acid rest Cl\u2212 and the base rest Na+.\nThe removal of electrons to form the cation is endothermic, raising the system's overall energy. There may also be energy changes associated with breaking of existing bonds or the addition of more than one electron to form anions. However, the action of the anion's accepting the cation's valence electrons and the subsequent attraction of the ions to each other releases (lattice) energy and, thus, lowers the overall energy of the system.\nIonic bonding will occur only if the overall energy change for the reaction is favorable. In general, the reaction is exothermic, but, e.g., the formation of mercuric oxide (HgO) is endothermic. The charge of the resulting ions is a major factor in the strength of ionic bonding, e.g. a salt C+A\u2212 is held together by electrostatic forces roughly four times weaker than C2+A2\u2212 according to Coulomb's law, where C and A represent a generic cation and anion respectively. The sizes of the ions and the particular packing of the lattice are ignored in this rather simplistic argument.\nStructures.\nIonic compounds in the solid state form lattice structures. The two principal factors in determining the form of the lattice are the relative charges of the ions and their relative sizes. Some structures are adopted by a number of compounds; for example, the structure of the rock salt sodium chloride is also adopted by many alkali halides, and binary oxides such as magnesium oxide. Pauling's rules provide guidelines for predicting and rationalizing the crystal structures of ionic crystals\nStrength of the bonding.\nFor a solid crystalline ionic compound the enthalpy change in forming the solid from gaseous ions is termed the lattice energy. The experimental value for the lattice energy can be determined using the Born\u2013Haber cycle. It can also be calculated (predicted) using the Born\u2013Land\u00e9 equation as the sum of the electrostatic potential energy, calculated by summing interactions between cations and anions, and a short-range repulsive potential energy term. The electrostatic potential can be expressed in terms of the interionic separation and a constant (Madelung constant) that takes account of the geometry of the crystal. The further away from the nucleus the weaker the shield. The Born\u2013Land\u00e9 equation gives a reasonable fit to the lattice energy of, e.g., sodium chloride, where the calculated (predicted) value is \u2212756\u00a0kJ/mol, which compares to \u2212787\u00a0kJ/mol using the Born\u2013Haber cycle. In aqueous solution the binding strength can be described by the Bjerrum or Fuoss equation as function of the ion charges, rather independent of the nature of the ions such as polarizability or size. The strength of salt bridges is most often evaluated by measurements of equilibria between molecules containing cationic and anionic sites, most often in solution. Equilibrium constants in water indicate additive free energy contributions for each salt bridge. Another method for the identification of hydrogen bonds in complicated molecules is crystallography, sometimes also NMR-spectroscopy.\nThe attractive forces defining the strength of ionic bonding can be modeled by Coulomb's law. Ionic bond strengths are typically (cited ranges vary) between 170 and 1500 kJ/mol.\nPolarization power effects.\nIons in crystal lattices of purely ionic compounds are spherical; however, if the positive ion is small and/or highly charged, it will distort the electron cloud of the negative ion, an effect summarized in Fajans' rules. This polarization of the negative ion leads to a build-up of extra charge density between the two nuclei, that is, to partial covalency. Larger negative ions are more easily polarized, but the effect is usually important only when positive ions with charges of 3+ (e.g., Al3+) are involved. However, 2+ ions (Be2+) or even 1+ (Li+) show some polarizing power because their sizes are so small (e.g., LiI is ionic but has some covalent bonding present). Note that this is not the ionic polarization effect that refers to the displacement of ions in the lattice due to the application of an electric field.\nComparison with covalent bonding.\nIn ionic bonding, the atoms are bound by the attraction of oppositely charged ions, whereas, in covalent bonding, atoms are bound by sharing electrons to attain stable electron configurations. In covalent bonding, the molecular geometry around each atom is determined by valence shell electron pair repulsion VSEPR rules, whereas, in ionic materials, the geometry follows maximum packing rules. One could say that covalent bonding is more \"directional\" in the sense that the energy penalty for not adhering to the optimum bond angles is large, whereas ionic bonding has no such penalty. There are no shared electron pairs to repel each other, the ions should simply be packed as efficiently as possible. This often leads to much higher coordination numbers. In NaCl, each ion has 6 bonds and all bond angles are 90\u00b0. In CsCl the coordination number is 8. By comparison, carbon typically has a maximum of four bonds.\nPurely ionic bonding cannot exist, as the proximity of the entities involved in the bonding allows some degree of sharing electron density between them. Therefore, all ionic bonding has some covalent character. Thus, bonding is considered ionic where the ionic character is greater than the covalent character. The larger the difference in electronegativity between the two types of atoms involved in the bonding, the more ionic (polar) it is. Bonds with partially ionic and partially covalent character are called polar covalent bonds. For example, Na\u2013Cl and Mg\u2013O interactions have a few percent covalency, while Si\u2013O bonds are usually ~50% ionic and ~50% covalent. Pauling estimated that an electronegativity difference of 1.7 (on the Pauling scale) corresponds to 50% ionic character, so that a difference greater than 1.7 corresponds to a bond which is predominantly ionic.\nIonic character in covalent bonds can be directly measured for atoms having quadrupolar nuclei (2H, 14N, 81,79Br, 35,37Cl or 127I). These nuclei are generally objects of NQR nuclear quadrupole resonance and NMR nuclear magnetic resonance studies. Interactions between the nuclear quadrupole moments \"Q\" and the electric field gradients (EFG) are characterized via the nuclear quadrupole coupling constants\nQCC = \nwhere the \"eq\"zz term corresponds to the principal component of the EFG tensor and \"e\" is the elementary charge. In turn, the electric field gradient opens the way to description of bonding modes in molecules when the QCC values are accurately determined by NMR or NQR methods.\nIn general, when ionic bonding occurs in the solid (or liquid) state, it is not possible to talk about a single \"ionic bond\" between two individual atoms, because the cohesive forces that keep the lattice together are of a more collective nature. This is quite different in the case of covalent bonding, where we can often speak of a distinct bond localized between two particular atoms. However, even if ionic bonding is combined with some covalency, the result is \"not\" necessarily discrete bonds of a localized character. In such cases, the resulting bonding often requires description in terms of a band structure consisting of gigantic molecular orbitals spanning the entire crystal. Thus, the bonding in the solid often retains its collective rather than localized nature. When the difference in electronegativity is decreased, the bonding may then lead to a semiconductor, a semimetal or eventually a metallic conductor with metallic bonding.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14952", "revid": "41625025", "url": "https://en.wikipedia.org/wiki?curid=14952", "title": "IBF (disambiguation)", "text": "The International Boxing Federation is one of the four major boxing organisations.\nIBF may also refer to:\nSports.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14953", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=14953", "title": "IOC/Presidents", "text": ""}
{"id": "14958", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=14958", "title": "Immune system", "text": "Biological system protecting an organism against disease\nThe immune system is a network of biological systems that protects an organism from diseases. It detects and responds to a wide variety of pathogens, such as viruses, bacteria, and parasites, as well as cancer cells and objects, such as wood splinters\u2014distinguishing them from the organism's own healthy tissue. Many species have two major subsystems of the immune system. The innate immune system provides a preconfigured response to broad groups of situations and stimuli. The adaptive immune system provides a tailored response to each stimulus by learning to recognize molecules it has previously encountered. Both use molecules and cells to perform their functions.\nNearly all organisms have some kind of immune system. Bacteria have a rudimentary immune system in the form of enzymes that protect against viral infections. Other basic immune mechanisms evolved in ancient plants and animals and remain in their modern descendants. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt to recognize pathogens more efficiently. Adaptive (or acquired) immunity creates an immunological memory leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.\nDysfunction of the immune system can cause autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. Autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.\nLayered defense.\nThe immune system protects its host from infection with layered defenses of increasing specificity. Physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all animals. If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.\nBoth innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, \"self\" molecules are components of an organism's body that can be distinguished from foreign substances by the immune system. Conversely, \"non-self\" molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (originally named for being \"anti\"body \"gen\"erators) and are defined as substances that bind to specific immune receptors and elicit an immune response.\nSurface barriers.\nSeveral barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of most leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection. Organisms cannot be completely sealed from their environments, so systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.\nChemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the \u03b2-defensins. Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials. Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens. In the stomach, gastric acid serves as a chemical defense against ingested pathogens.\nWithin the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, changing the conditions in their environment, such as pH or available iron. As a result, the probability that pathogens will reach sufficient numbers to cause illness is reduced.\nInnate immune system.\nMicroorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms, or when damaged, injured or stressed cells send out alarm signals, many of which are recognized by the same receptors as those that recognize pathogens. Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way. This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms, and the only one in plants.\nImmune sensing.\nCells in the innate immune system use pattern recognition receptors to recognize molecular structures that are produced by pathogens. They are proteins expressed, mainly, by cells of the innate immune system, such as dendritic cells, macrophages, monocytes, neutrophils, and epithelial cells, to identify two classes of molecules: pathogen-associated molecular patterns (PAMPs), which are associated with microbial pathogens, and damage-associated molecular patterns (DAMPs), which are associated with components of hosts' cells that are released during cell damage or cell death. Cells in the innate immune system have pattern recognition receptors that detect internal infection or cell damage. Three major classes of these \"cytosolic\" receptors are NOD\u2013like receptors, RIG (retinoic acid-inducible gene)-like receptors, and cytosolic DNA sensors.\nRecognition of extracellular or endosomal PAMPs is mediated by transmembrane proteins known as toll-like receptors (TLRs). TLRs share a typical structural motif, the leucine rich repeats (LRRs), which give them a curved shape. Toll-like receptors were first discovered in \"Drosophila\" and trigger the synthesis and secretion of cytokines and activation of other host defense programs that are necessary for both innate or adaptive immune responses. Ten toll-like receptors have been described in humans.\nInnate immune cells.\nSome leukocytes (white blood cells) act like independent, single-celled organisms and are the second arm of the innate immune system. The innate leukocytes include the \"professional\" phagocytes (macrophages, neutrophils, and dendritic cells). These cells identify and eliminate pathogens, either by attacking larger pathogens through contact or by engulfing and then killing microorganisms. The other cells involved in the innate response include innate lymphoid cells, mast cells, eosinophils, basophils, and natural killer cells.\nPhagocytosis is an important feature of cellular innate immunity performed by cells called phagocytes that engulf pathogens or particles. Phagocytes generally patrol the body searching for pathogens, but can be called to specific locations by cytokines. Once a pathogen has been engulfed by a phagocyte, it becomes trapped in an intracellular vesicle called a phagosome, which subsequently fuses with another vesicle called a lysosome to form a phagolysosome. The pathogen is killed by the activity of digestive enzymes or following a respiratory burst that releases free radicals into the phagolysosome. Phagocytosis evolved as a means of acquiring nutrients, but this role was extended in phagocytes to include engulfment of pathogens as a defense mechanism. Phagocytosis probably represents the oldest form of host defense, as phagocytes have been identified in both vertebrate and invertebrate animals.\nNeutrophils and macrophages are phagocytes that travel throughout the body in pursuit of invading pathogens. Neutrophils are normally found in the bloodstream and are the most abundant type of phagocyte, representing 50% to 60% of total circulating leukocytes. During the acute phase of inflammation, neutrophils migrate toward the site of inflammation in a process called chemotaxis and are usually the first cells to arrive at the scene of infection. Macrophages are versatile cells that reside within tissues and produce an array of chemicals including enzymes, complement proteins, and cytokines. They can also act as scavengers that rid the body of worn-out cells and other debris and as antigen-presenting cells (APCs) that activate the adaptive immune system.\nDendritic cells are phagocytes in tissues that are in contact with the external environment; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines. They are named for their resemblance to neuronal dendrites, as both have many spine-like projections. Dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens to T cells, one of the key cell types of the adaptive immune system.\nGranulocytes are leukocytes that have granules in their cytoplasm. In this category are neutrophils, mast cells, basophils, and eosinophils. Mast cells reside in connective tissues and mucous membranes and regulate the inflammatory response. They are most often associated with allergy and anaphylaxis. Basophils and eosinophils are related to neutrophils. They secrete chemical mediators that are involved in defending against parasites and play a role in allergic reactions, such as asthma.\nInnate lymphoid cells (ILCs) are a group of innate immune cells that are derived from common lymphoid progenitor and belong to the lymphoid lineage. These cells are defined by the absence of antigen-specific B- or T-cell receptor (TCR) because of the lack of recombination activating gene. ILCs do not express myeloid or dendritic cell markers.\nNatural killer cells (NK cells) are lymphocytes and a component of the innate immune system that does not directly attack invading microbes. Rather, NK cells destroy compromised host cells, such as tumor cells or virus-infected cells, recognizing such cells by a condition known as \"missing self\". This term describes cells with low levels of a cell-surface marker called MHC I (major histocompatibility complex)\u2014a situation that can arise in viral infections of host cells. Normal body cells are not recognized and attacked by NK cells because they express intact self MHC antigens. Those MHC antigens are recognized by killer cell immunoglobulin receptors, which essentially put the brakes on NK cells.\nInflammation.\nInflammation is one of the first responses of the immune system to infection. The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue. Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have antiviral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote the healing of any damaged tissue following the removal of pathogens. The pattern-recognition receptors called inflammasomes are multiprotein complexes (consisting of an NLR, the adaptor protein ASC, and the effector molecule pro-caspase-1) that form in response to cytosolic PAMPs and DAMPs, whose function is to generate active forms of the inflammatory cytokines IL-1\u03b2 and IL-18.\nHumoral defenses.\nThe complement system is a biochemical cascade that attacks the surfaces of foreign cells. It contains over 20 different proteins and is named for its ability to \"complement\" the killing of pathogens by antibodies. Complement is the major humoral component of the innate immune response. Many species have complement systems, including non-mammals like plants, fish, and some invertebrates. In humans, this response is activated by complement binding to antibodies that have attached to these microbes or the binding of complement proteins to carbohydrates on the surfaces of microbes. This recognition signal triggers a rapid killing response. The speed of the response is a result of signal amplification that occurs after sequential proteolytic activation of complement molecules, which are also proteases. After complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. This produces a catalytic cascade that amplifies the initial signal by controlled positive feedback. The cascade results in the production of peptides that attract immune cells, increase vascular permeability, and opsonize (coat) the surface of a pathogen, marking it for destruction. This deposition of complement can also kill cells directly by disrupting their plasma membrane via the formation of a membrane attack complex.\nAdaptive immune system.\nThe adaptive immune system evolved in early vertebrates and allows for a stronger immune response as well as immunological memory, where each pathogen is \"remembered\" by a signature antigen. The adaptive immune response is antigen-specific and requires the recognition of specific \"non-self\" antigens during a process called antigen presentation. Antigen specificity allows for the generation of responses that are tailored to specific pathogens or pathogen-infected cells. The ability to mount these tailored responses is maintained in the body by \"memory cells\". Should a pathogen infect the body more than once, these specific memory cells are used to quickly eliminate it.\nRecognition of antigen.\nThe cells of the adaptive immune system are special types of leukocytes, called lymphocytes. B cells and T cells are the major types of lymphocytes and are derived from hematopoietic stem cells in the bone marrow. B cells are involved in the humoral immune response, whereas T cells are involved in cell-mediated immune response. Killer T cells only recognize antigens coupled to Class I MHC molecules, while helper T cells and regulatory T cells only recognize antigens coupled to Class II MHC molecules. These two mechanisms of antigen presentation reflect the different roles of the two types of T cell. A third, minor subtype are the \u03b3\u03b4 T cells that recognize intact antigens that are not bound to MHC receptors. The double-positive T cells are exposed to a wide variety of self-antigens in the thymus, in which iodine is necessary for its thymus development and activity. In contrast, the B cell antigen-specific receptor is an antibody molecule on the B cell surface and recognizes native (unprocessed) antigen without any need for antigen processing. Such antigens may be large molecules found on the surfaces of pathogens, but can also be small haptens (such as penicillin) attached to carrier molecule. Each lineage of B cell expresses a different antibody, so the complete set of B cell antigen receptors represent all the antibodies that the body can manufacture. When B or T cells encounter their related antigens they multiply and many \"clones\" of the cells are produced that target the same antigen. This is called clonal selection.\nAntigen presentation to T lymphocytes.\nBoth B cells and T cells carry receptor molecules that recognize specific targets. T cells recognize a \"non-self\" target, such as a pathogen, only after antigens (small fragments of the pathogen) have been processed and presented in combination with a \"self\" receptor called a major histocompatibility complex (MHC) molecule.\nCell mediated immunity.\nThere are two major subtypes of T cells: the killer T cell and the helper T cell. In addition there are regulatory T cells which have a role in modulating immune response.\nKiller T cells.\nKiller T cells are a sub-group of T cells that kill cells that are infected with viruses (and other pathogens), or are otherwise damaged or dysfunctional. As with B cells, each type of T cell recognizes a different antigen. Killer T cells are activated when their T-cell receptor binds to this specific antigen in a complex with the MHC Class I receptor of another cell. Recognition of this MHC:antigen complex is aided by a co-receptor on the T cell, called CD8. The T cell then travels throughout the body in search of cells where the MHC I receptors bear this antigen. When an activated T cell contacts such cells, it releases cytotoxins, such as perforin, which form pores in the target cell's plasma membrane, allowing ions, water and toxins to enter. The entry of another toxin called granulysin (a protease) induces the target cell to undergo apoptosis. T cell killing of host cells is particularly important in preventing the replication of viruses. T cell activation is tightly controlled and generally requires a very strong MHC/antigen activation signal, or additional activation signals provided by \"helper\" T cells (see below).\nHelper T cells.\nHelper T cells regulate both the innate and adaptive immune responses and help determine which immune responses the body makes to a particular pathogen. These cells have no cytotoxic activity and do not kill infected cells or clear pathogens directly. They instead control the immune response by directing other cells to perform these tasks.\nHelper T cells express T cell receptors that recognize antigen bound to Class II MHC molecules. The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (such as Lck) that are responsible for the T cell's activation. Helper T cells have a weaker association with the MHC:antigen complex than observed for killer T cells, meaning many receptors (around 200\u2013300) on the helper T cell must be bound by an MHC:antigen to activate the helper cell, while killer T cells can be activated by engagement of a single MHC:antigen molecule. Helper T cell activation also requires longer duration of engagement with an antigen-presenting cell. The activation of a resting helper T cell causes it to release cytokines that influence the activity of many cell types. Cytokine signals produced by helper T cells enhance the microbicidal function of macrophages and the activity of killer T cells. In addition, helper T cell activation causes an upregulation of molecules expressed on the T cell's surface, such as CD40 ligand (also called CD154), which provide extra stimulatory signals typically required to activate antibody-producing B cells.\nGamma delta T cells.\nGamma delta T cells (\u03b3\u03b4 T cells) possess an alternative T-cell receptor (TCR) as opposed to CD4+ and CD8+ (\u03b1\u03b2) T cells and share the characteristics of helper T cells, cytotoxic T cells and NK cells. The conditions that produce responses from \u03b3\u03b4 T cells are not fully understood. Like other 'unconventional' T cell subsets bearing invariant TCRs, such as CD1d-restricted natural killer T cells, \u03b3\u03b4 T cells straddle the border between innate and adaptive immunity. On one hand, \u03b3\u03b4 T cells are a component of adaptive immunity as they rearrange TCR genes to produce receptor diversity and can also develop a memory phenotype. On the other hand, the various subsets are also part of the innate immune system, as restricted TCR or NK receptors may be used as pattern recognition receptors. For example, large numbers of human V\u03b39/V\u03b42 T cells respond within hours to common molecules produced by microbes, and highly restricted V\u03b41+ T cells in epithelia respond to stressed epithelial cells.\nHumoral immune response.\nA B cell identifies pathogens when antibodies on its surface bind to a specific foreign antigen. This antigen/antibody complex is taken up by the B cell and processed by proteolysis into peptides. The B cell then displays these antigenic peptides on its surface MHC class II molecules. This combination of MHC and antigen attracts a matching helper T cell, which releases lymphokines and activates the B cell. As the activated B cell then begins to divide, its offspring (plasma cells) secrete millions of copies of the antibody that recognizes this antigen. These antibodies circulate in blood plasma and lymph, bind to pathogens expressing the antigen and mark them for destruction by complement activation or for uptake and destruction by phagocytes. Antibodies can also neutralize challenges directly, by binding to bacterial toxins or by interfering with the receptors that viruses and bacteria use to infect cells.\nNewborn infants have no prior exposure to microbes and are particularly vulnerable to infection. Several layers of passive protection are provided by the mother. During pregnancy, a particular type of antibody, called IgG, is transported from mother to baby directly through the placenta, so human babies have high levels of antibodies even at birth, with the same range of antigen specificities as their mother. Breast milk or colostrum also contains antibodies that are transferred to the gut of the infant and protect against bacterial infections until the newborn can synthesize its own antibodies. This is passive immunity because the fetus does not actually make any memory cells or antibodies\u2014it only borrows them. This passive immunity is usually short-term, lasting from a few days up to several months. In medicine, protective passive immunity can also be transferred artificially from one individual to another.\nImmunological memory.\nWhen B cells and T cells are activated and begin to replicate, some of their offspring become long-lived memory cells. Throughout the lifetime of an animal, these memory cells remember each specific pathogen encountered and can mount a strong response if the pathogen is detected again. T-cells recognize pathogens by small protein-based infection signals, called antigens, that bind directly to T-cell surface receptors. B-cells use the protein, immunoglobulin, to recognize pathogens by their antigens. This is \"adaptive\" because it occurs during the lifetime of an individual as an adaptation to infection with that pathogen and prepares the immune system for future challenges. Immunological memory can be in the form of either passive short-term memory or active long-term memory.\nPhysiological regulation.\nThe immune system is involved in many aspects of physiological regulation in the body. The immune system interacts intimately with other systems, such as the endocrine and the nervous systems. The immune system also plays a crucial role in embryogenesis (development of the embryo), as well as in tissue repair and regeneration.\nHormones.\nHormones can act as immunomodulators, altering the sensitivity of the immune system. For example, female sex hormones are known immunostimulators of both adaptive and innate immune responses. Some autoimmune diseases such as lupus erythematosus strike women preferentially, and their onset often coincides with puberty. By contrast, male sex hormones such as testosterone seem to be immunosuppressive. Other hormones appear to regulate the immune system as well, most notably prolactin, growth hormone and vitamin D.\nVitamin D.\nAlthough early cellular studies suggested vitamin D might influence immune responses, more recent large-scale clinical trials and meta-analyses (2022\u20132024) have found that vitamin D supplementation can reduce the risk and severity of autoimmune diseases such as rheumatoid arthritis and multiple sclerosis, and may modestly reduce the incidence of acute respiratory tract infections and improve tuberculosis outcomes. A 2011 United States Institute of Medicine report stated that \"outcomes related to ... immune functioning and autoimmune disorders, and infections ... could not be linked reliably with calcium or vitamin D intake and were often conflicting.\"\nSleep and rest.\nThe immune system is affected by sleep and rest, and sleep deprivation is detrimental to immune function. Complex feedback loops involving cytokines, such as interleukin-1 and tumor necrosis factor-\u03b1 produced in response to infection, appear to also play a role in the regulation of non-rapid eye movement (NREM) sleep. Thus the immune response to infection may result in changes to the sleep cycle, including an increase in slow-wave sleep relative to rapid eye movement (REM) sleep.\nIn people with sleep deprivation, active immunizations may have a diminished effect and may result in lower antibody production, and a lower immune response, than would be noted in a well-rested individual. Additionally, proteins such as NFIL3, which have been shown to be closely intertwined with both T-cell differentiation and circadian rhythms, can be affected through the disturbance of natural light and dark cycles through instances of sleep deprivation. These disruptions can lead to an increase in chronic conditions such as heart disease, chronic pain, and asthma.\nIn addition to the negative consequences of sleep deprivation, sleep and the intertwined circadian system have been shown to have strong regulatory effects on immunological functions affecting both innate and adaptive immunity. First, during the early slow-wave-sleep stage, a sudden drop in blood levels of cortisol, epinephrine, and norepinephrine causes increased blood levels of the hormones leptin, pituitary growth hormone, and prolactin. These signals induce a pro-inflammatory state through the production of the pro-inflammatory cytokines interleukin-1, interleukin-12, TNF-alpha and IFN-gamma. These cytokines then stimulate immune functions such as immune cell activation, proliferation, and differentiation. During this time of a slowly evolving adaptive immune response, there is a peak in undifferentiated or less differentiated cells, like na\u00efve and central memory T cells. In addition to these effects, the milieu of hormones produced at this time (leptin, pituitary growth hormone, and prolactin) supports the interactions between APCs and T-cells, a shift of the Th1/Th2 cytokine balance towards one that supports Th1, an increase in overall Th cell proliferation, and na\u00efve T cell migration to lymph nodes. This is also thought to support the formation of long-lasting immune memory through the initiation of Th1 immune responses.\nDuring wake periods, differentiated effector cells, such as cytotoxic natural killer cells and CD45RA+ cytotoxic T lymphocytes, peak in numbers. Anti-inflammatory molecules, such as cortisol and catecholamines, also peak during awake active times. Inflammation can cause oxidative stress and the presence of melatonin during sleep times could counteract free radical production during this time.\nPhysical exercise.\nPhysical exercise has a positive effect on the immune system and depending on the frequency and intensity, the pathogenic effects of diseases caused by bacteria and viruses are moderated. Immediately after intense exercise there is a transient immunodepression, where the number of circulating lymphocytes decreases and antibody production declines. This may give rise to a window of opportunity for infection and reactivation of latent virus infections, but the evidence is inconclusive.\nChanges at the cellular level.\nDuring exercise there is an increase in circulating white blood cells of all types. This is caused by the frictional force of blood flowing on the endothelial cell surface and catecholamines affecting \u03b2-adrenergic receptors (\u03b2ARs). The number of neutrophils in the blood increases and remains raised for up to six hours and immature forms are present. Although the increase in neutrophils (\"neutrophilia\") is similar to that seen during bacterial infections, after exercise the cell population returns to normal by around 24 hours.\nThe number of circulating lymphocytes (mainly natural killer cells) decreases during intense exercise but returns to normal after 4 to 6 hours. Although up to 2% of the cells die most migrate from the blood to the tissues, mainly the intestines and lungs, where pathogens are most likely to be encountered.\nSome monocytes leave the blood circulation and migrate to the muscles where they differentiate and become macrophages. These cells differentiate into two types: proliferative macrophages, which are responsible for increasing the number of stem cells and restorative macrophages, which are involved their maturing to muscle cells.\nRepair and regeneration.\nThe immune system, particularly the innate component, plays a decisive role in tissue repair after an insult. Key actors include macrophages and neutrophils, but other cellular actors, including \u03b3\u03b4 T cells, innate lymphoid cells (ILCs), and regulatory T cells (Tregs), are also important. The plasticity of immune cells and the balance between pro-inflammatory and anti-inflammatory signals are crucial aspects of efficient tissue repair. Immune components and pathways are involved in regeneration as well, for example in amphibians such as in axolotl limb regeneration. According to one hypothesis, organisms that can regenerate (\"e.g.\", axolotls) could be less immunocompetent than organisms that cannot regenerate.\nDisorders of human immunity.\nFailures of host defense occur and fall into three broad categories: immunodeficiencies, autoimmunity, and hypersensitivities.\nImmunodeficiencies.\nImmunodeficiencies occur when one or more of the components of the immune system are inactive. The ability of the immune system to respond to pathogens is diminished in both the young and the elderly, with immune responses beginning to decline at around 50 years of age due to immunosenescence. In developed countries, obesity, alcoholism, and drug use are common causes of poor immune function, while malnutrition is the most common cause of immunodeficiency in developing countries. Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production. Additionally, the loss of the thymus at an early age through genetic mutation or surgical removal results in severe immunodeficiency and a high susceptibility to infection. Immunodeficiencies can also be inherited or 'acquired'. Severe combined immunodeficiency is a rare genetic disorder characterized by the disturbed development of functional T cells and B cells caused by numerous genetic mutations. Chronic granulomatous disease, where phagocytes have a reduced ability to destroy pathogens, is an example of an inherited, or congenital, immunodeficiency. AIDS and some types of cancer cause acquired immunodeficiency.\nAutoimmunity.\nOveractive immune responses form the other end of immune dysfunction, particularly the autoimmune diseases. Here, the immune system fails to properly distinguish between self and non-self, and attacks part of the body. Under normal circumstances, many T cells and antibodies react with \"self\" peptides. One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus.\nHypersensitivity.\nHypersensitivity is an immune response that damages the body's own tissues. It is divided into four classes (Type I\u00a0\u2013 IV) based on the mechanisms involved and the time course of the hypersensitive reaction. Type I hypersensitivity is an immediate or anaphylactic reaction, often associated with allergy. Symptoms can range from mild discomfort to death. Type I hypersensitivity is mediated by IgE, which triggers degranulation of mast cells and basophils when cross-linked by antigen.\nType II hypersensitivity occurs when antibodies bind to antigens on the individual's own cells, marking them for destruction. This is also called antibody-dependent (or cytotoxic) hypersensitivity, and is mediated by IgG and IgM antibodies. Immune complexes (aggregations of antigens, complement proteins, and IgG and IgM antibodies) deposited in various tissues trigger Type III hypersensitivity reactions. Type IV hypersensitivity (also known as cell-mediated or \"delayed type hypersensitivity\") usually takes between two and three days to develop. Type IV reactions are involved in many autoimmune and infectious diseases, but may also involve contact dermatitis. These reactions are mediated by T cells, monocytes, and macrophages.\nIdiopathic inflammation.\nInflammation is one of the first responses of the immune system to infection, but it can appear without known cause.\nInflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.\nManipulation in medicine.\nThe immune response can be manipulated to suppress unwanted responses resulting from autoimmunity, allergy, and transplant rejection, and to stimulate protective responses against pathogens that largely elude the immune system (see immunization) or cancer.\nImmunosuppression.\nImmunosuppressive drugs are used to control autoimmune disorders or inflammation when excessive tissue damage occurs, and to prevent rejection after an organ transplant.\nAnti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs and can have many undesirable side effects, such as central obesity, hyperglycemia, and osteoporosis. Their use is tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine.\nCytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. This killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.\nImmunostimulation.\nClaims made by marketers of various products and alternative health providers, such as chiropractors, homeopaths, and acupuncturists to be able to stimulate or \"boost\" the immune system generally lack meaningful explanation and evidence of effectiveness.\nVaccination.\nLong-term \"active\" memory is acquired following infection by activation of B and T cells. Active immunity can also be generated artificially, through vaccination. The principle behind vaccination (also called immunization) is to introduce an antigen from a pathogen to stimulate the immune system and develop specific immunity against that particular pathogen without causing disease associated with that organism. This deliberate induction of an immune response is successful because it exploits the natural specificity of the immune system, as well as its inducibility. With infectious disease remaining one of the leading causes of death in the human population, vaccination represents the most effective manipulation of the immune system mankind has developed.\nMany vaccines are based on acellular components of micro-organisms, including harmless toxin components. Since many antigens derived from acellular vaccines do not strongly induce the adaptive response, most bacterial vaccines are provided with additional adjuvants that activate the antigen-presenting cells of the innate immune system and maximize immunogenicity.\nTumor immunology.\nAnother important role of the immune system is to identify and eliminate tumors. This is called immune surveillance. The \"transformed cells\" of tumors express antigens that are not found on normal cells. To the immune system, these antigens appear foreign, and their presence causes immune cells to attack the transformed tumor cells. The antigens expressed by tumors have several sources; some are derived from oncogenic viruses like human papillomavirus, which causes cancer of the cervix, vulva, vagina, penis, anus, mouth, and throat, while others are the organism's own proteins that occur at low levels in normal cells but reach high levels in tumor cells. One example is an enzyme called tyrosinase that, when expressed at high levels, transforms certain skin cells (for example, melanocytes) into tumors called melanomas. A third possible source of tumor antigens are proteins normally important for regulating cell growth and survival, that commonly mutate into cancer inducing molecules called oncogenes.\nThe main response of the immune system to tumors is to destroy the abnormal cells using killer T cells, sometimes with the assistance of helper T cells. Tumor antigens are presented on MHC class I molecules in a similar way to viral antigens. This allows killer T cells to recognize the tumor cell as abnormal. NK cells also kill tumorous cells in a similar way, especially if the tumor cells have fewer MHC class I molecules on their surface than normal; this is a common phenomenon with tumors. Sometimes antibodies are generated against tumor cells allowing for their destruction by the complement system.\nSome tumors evade the immune system and go on to become cancers. Tumor cells often have a reduced number of MHC class I molecules on their surface, thus avoiding detection by killer T cells. Some tumor cells also release products that inhibit the immune response; for example by secreting the cytokine TGF-\u03b2, which suppresses the activity of macrophages and lymphocytes. In addition, immunological tolerance may develop against tumor antigens, so the immune system no longer attacks the tumor cells.\nParadoxically, macrophages can promote tumor growth when tumor cells send out cytokines that attract macrophages, which then generate cytokines and growth factors such as tumor-necrosis factor alpha that nurture tumor development or promote stem-cell-like plasticity. In addition, a combination of hypoxia in the tumor and a cytokine produced by macrophages induces tumor cells to decrease production of a protein that blocks metastasis and thereby assists spread of cancer cells. Anti-tumor M1 macrophages are recruited in early phases to tumor development but are progressively differentiated to M2 with pro-tumor effect, an immunosuppressor switch. The hypoxia reduces the cytokine production for the anti-tumor response and progressively macrophages acquire pro-tumor M2 functions driven by the tumor microenvironment, including IL-4 and IL-10. Cancer immunotherapy covers the medical ways to stimulate the immune system to attack cancer tumors.\nPredicting immunogenicity.\nSome drugs can cause a neutralizing immune response, meaning that the immune system produces neutralizing antibodies that counteract the action of the drugs, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids; however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set. A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells. The emerging field of bioinformatics-based studies of immunogenicity is referred to as \"immunoinformatics\". Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.\nEvolution and other mechanisms.\nEvolution of the immune system.\nIt is likely that a multicomponent, adaptive immune system arose with the first vertebrates, as invertebrates do not generate lymphocytes or an antibody-based humoral response. Immune systems evolved in deuterostomes as shown in the cladogram.\nMany species, however, use mechanisms that appear to be precursors of these aspects of vertebrate immunity. Immune systems appear even in the structurally simplest forms of life, with bacteria using a unique defense mechanism, called the restriction modification system to protect themselves from viral pathogens, called bacteriophages. Prokaryotes (bacteria and archaea) also possess acquired immunity, through a system that uses CRISPR sequences to retain fragments of the genomes of phage that they have come into contact with in the past, which allows them to block virus replication through a form of RNA interference. Prokaryotes also possess other defense mechanisms. Offensive elements of the immune systems are also present in unicellular eukaryotes, but studies of their roles in defense are few.\nPattern recognition receptors are proteins used by nearly all organisms to identify molecules associated with pathogens. Antimicrobial peptides called defensins are an evolutionarily conserved component of the innate immune response found in all animals and plants, and represent the main form of invertebrate systemic immunity. The complement system and phagocytic cells are also used by most forms of invertebrate life. Ribonucleases and the RNA interference pathway are conserved across all eukaryotes, and are thought to play a role in the immune response to viruses.\nUnlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.\nAlternative adaptive immune system.\nEvolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (for example, immunoglobulins and T-cell receptors) exist only in jawed vertebrates. A distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\nManipulation by pathogens.\nThe success of any pathogen depends on its ability to elude host immune responses. Therefore, pathogens evolved several methods that allow them to successfully infect a host, while evading detection or destruction by the immune system. Bacteria often overcome physical barriers by secreting enzymes that digest the barrier, for example, by using a type II secretion system. Alternatively, using a type III secretion system, they may insert a hollow tube into the host cell, providing a direct route for proteins to move from the pathogen to the host. These proteins are often used to shut down host defenses.\nAn evasion strategy used by several pathogens to avoid the innate immune system is to hide within the cells of their host (also called intracellular pathogenesis). Here, a pathogen spends most of its life-cycle inside host cells, where it is shielded from direct contact with immune cells, antibodies and complement. Some examples of intracellular pathogens include viruses, the food poisoning bacterium \"Salmonella\" and the eukaryotic parasites that cause malaria (\"Plasmodium spp.\") and leishmaniasis (\"Leishmania spp.\"). Other bacteria, such as \"Mycobacterium tuberculosis\", live inside a protective capsule that prevents lysis by complement. Many pathogens secrete compounds that diminish or misdirect the host's immune response. Some bacteria form biofilms to protect themselves from the cells and proteins of the immune system. Such biofilms are present in many successful infections, such as the chronic \"Pseudomonas aeruginosa\" and \"Burkholderia cenocepacia\" infections characteristic of cystic fibrosis. Other bacteria generate surface proteins that bind to antibodies, rendering them ineffective; examples include \"Streptococcus\" (protein G), \"Staphylococcus aureus\" (protein A), and \"Peptostreptococcus magnus\" (protein L).\nThe mechanisms used to evade the adaptive immune system are more complicated. The simplest approach is to rapidly change non-essential epitopes (amino acids and/or sugars) on the surface of the pathogen, while keeping essential epitopes concealed. This is called antigenic variation. An example is HIV, which mutates rapidly, so the proteins on its viral envelope that are essential for entry into its host target cell are constantly changing. These frequent changes in antigens may explain the failures of vaccines directed at this virus. The parasite \"Trypanosoma brucei\" uses a similar strategy, constantly switching one type of surface protein for another, allowing it to stay one step ahead of the antibody response. Masking antigens with host molecules is another common strategy for avoiding detection by the immune system. In HIV, the envelope that covers the virion is formed from the outermost membrane of the host cell; such \"self-cloaked\" viruses make it difficult for the immune system to identify them as \"non-self\" structures.\nHistory of immunology.\nImmunology is a science that examines the structure and function of the immune system. It originates from medicine and early studies on the causes of immunity to disease. The earliest known reference to immunity was during the plague of Athens in 430 BC. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. In the 18th century, Pierre-Louis Moreau de Maupertuis experimented with scorpion venom and observed that certain dogs and mice were immune to this venom. In the 10th century, Persian physician al-Razi (also known as Rhazes) wrote the first recorded theory of acquired immunity, noting that a smallpox bout protected its survivors from future infections. Although he explained the immunity in terms of \"excess moisture\" being expelled from the blood\u2014therefore preventing a second occurrence of the disease\u2014this theory explained many observations about smallpox known during this time.\nThese and other observations of acquired immunity were later exploited by Louis Pasteur in his development of vaccination and his proposed germ theory of disease. Pasteur's theory was in direct opposition to contemporary theories of disease, such as the miasma theory. It was not until Robert Koch's 1891 proofs, for which he was awarded a Nobel Prize in 1905, that microorganisms were confirmed as the cause of infectious disease. Viruses were confirmed as human pathogens in 1901, with the discovery of the yellow fever virus by Walter Reed.\nImmunology made a great advance towards the end of the 19th century, through rapid developments in the study of humoral immunity and cellular immunity. Particularly important was the work of Paul Ehrlich, who proposed the side-chain theory to explain the specificity of the antigen-antibody reaction; his contributions to the understanding of humoral immunity were recognized by the award of a joint Nobel Prize in 1908, along with the founder of cellular immunology, Elie Metchnikoff. In 1974, Niels Kaj Jerne developed the immune network theory; he shared a Nobel Prize in 1984 with Georges J. F. K\u00f6hler and C\u00e9sar Milstein for theories related to the immune system.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral bibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14959", "revid": "8660314", "url": "https://en.wikipedia.org/wiki?curid=14959", "title": "Immunology", "text": "Branch of medicine studying the immune system\nImmunology is a branch of biology and medicine that covers the study of immune systems in all organisms.\nImmunology charts, measures, and contextualizes the physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases, hypersensitivities, immune deficiency, and transplant rejection); and the physical, chemical, and physiological characteristics of the components of the immune system \"in vitro\", \"in situ\", and \"in vivo\". Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, rheumatology, virology, bacteriology, parasitology, psychiatry, and dermatology.\nThe term was coined by Russian biologist Ilya Ilyich Mechnikov, who advanced studies on immunology and received the Nobel Prize for his work in 1908 with Paul Ehrlich \"in recognition of their work on immunity\". He pinned small thorns into starfish larvae and noticed unusual cells surrounding the thorns. This was the active response of the body trying to maintain its integrity. It was Mechnikov who first observed the phenomenon of phagocytosis, in which the body defends itself against a foreign body. Ehrlich accustomed mice to the poisonous ricin and abrin. After feeding them with small but increasing dosages of ricin he ascertained that they had become \"ricin-proof\". Ehrlich interpreted this as immunization and observed that it was abruptly initiated after a few days and was still in existence after several months.\nPrior to the designation of immunity, from the etymological root , which is Latin for 'exempt', early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus, bone marrow, and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. However, many components of the immune system are cellular in nature, and not associated with specific organs, but rather embedded or circulating in various tissues located throughout the body.\nClassical immunology.\nClassical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.\nThe study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.\nThe immune system has the capability of self and non-self-recognition. An antigen is a substance that ignites the immune response. The cells involved in recognizing the antigen are Lymphocytes. Once they recognize, they secrete antibodies. Antibodies are proteins that neutralize the disease-causing microorganisms. Antibodies do not directly kill pathogens, but instead, identify antigens as targets for destruction by other immune cells such as phagocytes or NK cells.\nThe (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B\u00a0lymphocytes, while antigens are defined as anything that elicits the generation of antibodies (antibody generators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.\nIt is now getting clear that the immune responses contribute to the development of many common disorders not traditionally viewed as immunologic, including metabolic, cardiovascular, cancer, and neurodegenerative conditions like Alzheimer's disease. Besides, there are direct implications of the immune system in the infectious diseases (tuberculosis, malaria, hepatitis, pneumonia, dysentery, and helminth infestations) as well. Hence, research in the field of immunology is of prime importance for the advancements in the fields of modern medicine, biomedical research, and biotechnology.\nDiagnostic immunology.\nThe specificity of the bond between antibody and antigen has made the antibody an excellent tool for the detection of substances by a variety of diagnostic techniques. Antibodies specific for a desired antigen can be conjugated with an isotopic (radio) or fluorescent label or with a color-forming enzyme in order to detect it. However, the similarity between some antigens can lead to false positives and other errors in such tests by antibodies cross-reacting with antigens that are not exact matches.\nImmunotherapy.\nThe use of immune system components or antigens to treat a disease or disorder is known as immunotherapy. Immunotherapy is most commonly used to treat allergies, autoimmune disorders such as Crohn's disease, Hashimoto's thyroiditis and rheumatoid arthritis, and certain cancers. Immunotherapy is also often used for patients who are immunosuppressed (such as those with HIV) and people with other immune deficiencies.\nThis includes regulating factors such as IL-2, IL-10, GM-CSF B, IFN-\u03b1.\nClinical immunology.\nClinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.\nThe diseases caused by disorders of the immune system fall into two broad categories:\nOther immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.\nThe most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ (\"helper\") T cells, dendritic cells and macrophages by the human immunodeficiency virus (HIV).\nClinical immunologists also study ways to prevent the immune system's attempts to destroy allografts (transplant rejection).\nClinical immunology and allergy is usually a subspecialty of internal medicine or pediatrics. Fellows in Clinical Immunology are typically exposed to many of the different aspects of the specialty and treat allergic conditions, primary immunodeficiencies and systemic autoimmune and autoinflammatory conditions. As part of their training fellows may do additional rotations in rheumatology, pulmonology, otorhinolaryngology, dermatology and the immunologic lab.\nClinical and pathology immunology.\nWhen health conditions worsen to emergency status, portions of immune system organs, including the thymus, spleen, bone marrow, lymph nodes, and other lymphatic tissues, can be surgically excised for examination while patients are still alive.\nTheoretical immunology.\nImmunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between \"cellular\" and \"humoral\" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells\u00a0\u2013 more precisely, phagocytes\u00a0\u2013 that were responsible for immune responses. In contrast, the humoral theory of immunity, held by Robert Koch and Emil von Behring, among others, stated that the active immune agents were soluble components (molecules) found in the organism's \"humors\" rather than its cells.\nIn the mid-1950s, Macfarlane Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: \"self\" constituents (constituents of the body) do not trigger destructive immune responses, while \"nonself\" entities (e.g., pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex \"two-signal\" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.\nMore recently, several theoretical frameworks have been suggested in immunology, including \"autopoietic\" views, \"cognitive immune\" views, the \"danger model\" (or \"danger theory\"), and the \"discontinuity\" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.\nDevelopmental immunology.\nThe body's capability to react to antigens depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child's immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like \"Staphylococcus\" and \"Pseudomonas\". In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T\u00a0cells. Also, T\u00a0cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B\u00a0cells develop early during gestation but are not fully active.\nMaternal factors also play a role in the body's immune response. At birth, most of the immunoglobulin present is maternal IgG. These antibodies are transferred from the placenta to the fetus using the FcRn (neonatal Fc receptor). Because IgM, IgD, IgE and IgA do not cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly, the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six and nine months after birth, a child's immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.\nDuring adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-\u03b2-estradiol (an estrogen) and, in males, is testosterone. Estradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids not only act directly on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.\nThe female sex hormone 17-\u03b2-estradiol has been shown to regulate the level of immunological response, while some male androgens such as testosterone seem to suppress the stress response to infection. Other androgens, however, such as DHEA, increase immune response. As in females, the male sex hormones seem to have more control of the immune system during puberty and post-puberty than during the rest of a male's adult life.\nPhysical changes during puberty such as thymic involution also affect immunological response.\nEcoimmunology and behavioural immunity.\nEcoimmunology, or ecological immunology, explores the relationship between the immune system of an organism and its social, biotic and abiotic environment.\nMore recent ecoimmunological research has focused on host pathogen defences traditionally considered \"non-immunological\", such as pathogen avoidance, self-medication, symbiont-mediated defenses, and fecundity trade-offs. Behavioural immunity, a phrase coined by Mark Schaller, specifically refers to psychological pathogen avoidance drivers, such as disgust aroused by stimuli encountered around pathogen-infected individuals, such as the smell of vomit. More broadly, \"behavioural\" ecological immunity has been demonstrated in multiple species. For example, the Monarch butterfly often lays its eggs on certain toxic milkweed species when infected with parasites. These toxins reduce parasite growth in the offspring of the infected Monarch. However, when uninfected Monarch butterflies are forced to feed only on these toxic plants, they suffer a fitness cost as reduced lifespan relative to other uninfected Monarch butterflies. This indicates that laying eggs on toxic plants is a costly behaviour in Monarchs which has probably evolved to reduce the severity of parasite infection.\nSymbiont-mediated defenses are also heritable across host generations, despite a non-genetic direct basis for the transmission. Aphids, for example, rely on several different symbionts for defense from key parasites, and can vertically transmit their symbionts from parent to offspring. Therefore, a symbiont that successfully confers protection from a parasite is more likely to be passed to the host offspring, allowing coevolution with parasites attacking the host in a way similar to traditional immunity.\nThe preserved immune tissues of extinct species, such as the thylacine (\"Thylacine cynocephalus\"), can also provide insights into their biology.\nCancer immunology.\nThe study of the interaction of the immune system with cancer cells can lead to diagnostic tests and therapies with which to find and fight cancer. The immunology concerned with physiological reaction characteristic of the immune state. Inflammation is an immune response that has been observed in many types of cancers.\nReproductive immunology.\nThis area of the immunology is devoted to the study of immunological aspects of the reproductive process including fetus acceptance. The term has also been used by fertility clinics to address fertility problems, recurrent miscarriages, premature deliveries and dangerous complications such as pre-eclampsia.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14960", "revid": "23646674", "url": "https://en.wikipedia.org/wiki?curid=14960", "title": "IPA", "text": "IPA commonly refers to:\nIPA may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14961", "revid": "1302391931", "url": "https://en.wikipedia.org/wiki?curid=14961", "title": "Ice beer", "text": "Beer style\nIce beer is a beer that has undergone some degree of freezing during production. These beers generally have a higher alcohol content, and lower price relative to it.\nThe process of \"icing\" beer involves lowering the temperature until ice crystals form. Since ethanol has a much lower freezing point (-114\u00a0\u00b0C; -173.2\u00a0\u00b0F) than water (0\u00a0\u00b0C; 32\u00a0\u00b0F), when the ice is removed the alcohol concentration of the beer increases. The process is known as fractional freezing or freeze distillation.\nHistory.\nIce beer was developed by brewing a strong, dark lager, then freezing the beer and removing some of the ice. This concentrates the aroma and taste of the beer, and also raises the alcoholic strength of the finished beer. This produces a beer with 12 to 15 per cent alcohol. In North America, water would be added to lower the alcohol level.\nEisbock was introduced to Canada in 1989 by the microbrewery Niagara Falls Brewing Company. The brewers started with a strong dark lager (15.3 degrees Plato/1.061 original gravity, 6% alcohol by volume), then used the traditional method of freezing and removing ice to concentrate aroma and flavours while increasing the alcoholic strength to 8% ABV. Niagara Falls Eisbock was released annually as a seasonal winter beer; each year the label would feature a different historic view of nearby Niagara Falls in winter. This continued each year until the company was sold in 1994.\nDespite this precedent, the large Canadian brewer Molson (now part of Molson Coors) claimed to have made the first ice beer in North America when it introduced \"Canadian Ice\" in April 1993. However, Molson's main competitor in Canada, Labatt (now part of Anheuser-Busch InBev), claimed to have patented the ice beer process earlier. When Labatt introduced an ice beer in August 1993, capturing a 10% market share in Canada, this instigated the so-called \"Ice Beer Wars\" of the 1990s.\nLabatt had patented a specific method for making ice beer in 1997, 1998 and 2000, \"A process for chill-treating, which is exemplified by a process for preparing a fermented malt beverage wherein brewing materials are mashed with water and the resulting mash is heated and wort separated therefrom. The wort is boiled cooled and fermented, and the beer is subjected to a finishing stage, which includes aging, to produce the final beverage. The improvement comprises subjecting the beer to a cold stage comprising rapidly cooling the beer to a temperature of about its freezing point in such a manner that ice crystals are formed therein in only minimal amounts. The resulting cooled beer is then mixed for a short period of time with a beer slurry containing ice crystals, without any appreciable collateral increase in the amount of ice crystals in the resulting mixture. Finally, the so-treated beer is extracted from the mixture.\"\nMiller acquired the U.S. marketing and distribution rights to Molson's products, and first introduced the Molson product in the United States in August 1993 as \"Molson Ice\". Miller also introduced the \"Icehouse\" brand under the \"Plank Road Brewery\" brand name shortly thereafter, and it is still sold nationwide.\nAnheuser-Busch introduced \"Bud Ice\" (5.5% ABV) in 1994, and it remains one of the country's top selling ice beers. \"Bud Ice\" has a somewhat lower alcohol content than most other ice beer brands. In 1995, Anheuser-Busch also introduced two other major brands: \"Busch Ice\" (5.9% ABV, introduced 1995) and \"Natural Ice\" (also 5.9% ABV, also introduced in 1995). \"Natural Ice\" is the No. 1 selling ice beer brand in the United States; its low price makes it very popular on college campuses all over the country. \nCommon ice beer brands in Canada in 2017, with approximately 5.5 to 6 per cent alcohol content, include Carling Ice, Molson's Black Ice, Busch Ice, Old Milwaukee Ice, Brick's Laker Ice and Labatt Ice. There is a Labatt Maximum Ice too, with 7.1 per cent alcohol.\nCharacteristics and regulation.\nIce beers are typically known for their high alcohol-to-price ratio. In some areas, a substantial number of ice beer products are considered to often be bought by \"street drunks,\" and are prohibited for sale. For example, most of the products that are explicitly listed as prohibited in the beer and malt liquor category in the Seattle area are ice beers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14962", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=14962", "title": "Identity element", "text": "Specific element of an algebraic structure\nIn mathematics, an identity element or neutral element of a binary operation is an element that leaves unchanged every element when the operation is applied. For example, 0 is an identity element of the addition of real numbers. This concept is used in algebraic structures such as groups and rings. The term \"identity element\" is often shortened to \"identity\" (as in the case of additive identity and multiplicative identity) when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.\nDefinitions.\nLet (\"S\",\u2009\u2217) be a set\u00a0S equipped with a binary operation\u00a0\u2217. Then an element\u00a0e of\u00a0S is called a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;left identity if \"e\" \u2217 \"s\" = \"s\" for all\u00a0s in\u00a0S, and a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;right identity if \"s\" \u2217 \"e\" = \"s\" for all\u00a0s in\u00a0S. If e is both a left identity and a right identity, then it is called a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;two-sided identity, or simply an &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;identity.\nAn identity with respect to addition is called an &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;additive identity (often denoted as\u00a00) and an identity with respect to multiplication is called a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;multiplicative identity (often denoted as\u00a01). These need not be ordinary addition and multiplication\u2014as the underlying operation could be rather arbitrary. In the case of a group for example, the identity element is sometimes simply denoted by the symbol formula_1. The distinction between additive and multiplicative identity is used most often for sets that support both binary operations, such as rings, integral domains, and fields. The multiplicative identity is often called &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;unity in the latter context (a ring with unity). This should not be confused with a unit in ring theory, which is any element having a multiplicative inverse. By its own definition, unity itself is necessarily a unit.\nProperties.\nIn the example \"S\" = {\"e,f\"} with the equalities given, \"S\" is a semigroup. It demonstrates the possibility for (\"S\",\u2009\u2217) to have several left identities. In fact, every element can be a left identity. In a similar manner, there can be several right identities. But if there is both a right identity and a left identity, then they must be equal, resulting in a single two-sided identity. \nTo see this, note that if l is a left identity and r is a right identity, then \"l\" = \"l\" \u2217 \"r\" = \"r\". In particular, there can never be more than one two-sided identity: if there were two, say e and f, then \"e\" \u2217 \"f\" would have to be equal to both e and f.\nIt is also quite possible for (\"S\",\u2009\u2217) to have \"no\" identity element, such as the case of even integers under the multiplication operation. Another common example is the cross product of vectors, where the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied. That is, it is not possible to obtain a non-zero vector in the same direction as the original. Yet another example of structure without identity element involves the additive semigroup of positive natural numbers.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14963", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=14963", "title": "International Tropical Timber Agreement, 1983", "text": ""}
{"id": "14964", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=14964", "title": "International Tropical Timber Agreement, 1994", "text": ""}
{"id": "14966", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=14966", "title": "Ica", "text": ""}
{"id": "14967", "revid": "50716912", "url": "https://en.wikipedia.org/wiki?curid=14967", "title": "Instrumental", "text": "Music without vocals\nAn instrumental, instrumental music, or sometimes \"instrumental song\" is music without any vocals, although it might include some inarticulate vocals, such as shouted backup vocals in a big band setting. Through semantic widening, a broader sense of the word \"song\" may refer to instrumentals. The music is primarily or exclusively produced using musical instruments. An instrumental can exist in music notation, after it is written by a composer; in the mind of the composer (especially in cases where the composer themselves will perform the piece, as in the case of a blues solo guitarist or a folk music fiddle player); as a piece that is performed live by a single instrumentalist or a musical ensemble, which could range in components from a duo or trio to a large big band, concert band or orchestra.\nIn a song that is otherwise sung, a section that is not sung but which is played by instruments can be called an instrumental interlude, or, if it occurs at the beginning of the song, before the singer starts to sing, an instrumental introduction. If the instrumental section highlights the skill, musicality, and often the virtuosity of a particular performer (or group of performers), the section may be called a \"solo\" (e.g., the guitar solo that is a key section of heavy metal music and hard rock songs). If the instruments are percussion instruments, the interlude can be called a percussion interlude or \"percussion break\". These interludes are a form of break in the song.\nIn popular music.\nIn commercial popular music, instrumental tracks are sometimes renderings, remixes of a corresponding release that features vocals, but they may also be compositions originally conceived without vocals. One example of a genre in which both vocal/instrumental and solely instrumental songs are produced is blues. A blues band often uses mostly songs that have lyrics that are sung, but during the band's show, they may also perform instrumental songs which only include electric guitar, harmonica, upright bass/electric bass and drum kit. Instrumental versions of songs can also be used to create remixes and mashups or used in DJ sets. If an instrumental version of a track is not released, it can be created through stem separation/vocal removal.\nBorderline cases.\nSome recordings which include brief or non-musical use of the human voice are typically considered instrumentals. Examples include songs with the following:\nSongs including actual musical\u2014rhythmic, melodic, and lyrical\u2014vocals might still be categorized as instrumentals if the vocals appear only as a short part of an extended piece (e.g., \"Unchained Melody\" (Les Baxter), \"Batman Theme\", \"TSOP (The Sound of Philadelphia)\", \"Pick Up the Pieces\", \"The Hustle\", \"Fly, Robin, Fly\", \"Get Up and Boogie\", \"Do It Any Way You Wanna\", and \"Gonna Fly Now\"), though this definition is loose and subjective.\nFalling just outside of that definition is \"Theme from \"Shaft\"\" by Isaac Hayes.\n\"Better Off Alone\", which began as an instrumental by DJ Jurgen, had vocals by Judith Pronk, who would become a seminal part of Alice Deejay, added in later releases of the track."}
{"id": "14968", "revid": "35839912", "url": "https://en.wikipedia.org/wiki?curid=14968", "title": "Regular icosahedron", "text": "Solid with twenty equal triangular faces\nThe regular icosahedron (or simply icosahedron) is a convex polyhedron that can be constructed from pentagonal antiprism by attaching two pentagonal pyramids with regular faces to each of its pentagonal faces, or by putting points onto the cube. The resulting polyhedron has 20 equilateral triangles as its faces, 30 edges, and 12 vertices. It is an example of a Platonic solid and of a deltahedron. The icosahedral graph represents the skeleton of a regular icosahedron.\nMany polyhedra and other related figures are constructed from the regular icosahedron, including its 59 stellations. The great dodecahedron, one of the Kepler\u2013Poinsot polyhedra, is constructed by either stellation of the regular dodecahedron or faceting of the icosahedron. Some of the Johnson solids can be constructed by removing the pentagonal pyramids. The regular icosahedron's dual polyhedron is the regular dodecahedron, and their relation has a historical background in the comparison mensuration. It is analogous to a four-dimensional polytope, the 600-cell.\nRegular icosahedra can be found in nature; a well-known example is the capsid in biology. Other applications of the regular icosahedron are the usage of its net in cartography, and the twenty-sided dice that may have been used in ancient times but are now commonplace in modern tabletop role-playing games.\nConstruction.\nThe regular icosahedron is a twenty-sided polyhedron wherein the faces are equilateral triangles. It is one of the eight convex deltahedra, a polyhedron wherein all of its faces are equilateral triangles. Variously, it can be constructed as follows:\nThe regular icosahedron can be unfolded into 43,380 different nets. The earliest net appeared in Albrecht Durer's \"Painter's Manual\" in 1525.\nProperties.\nSurface area and volume.\nThe surface area of a polyhedron is the sum of the areas of its faces. In the case of a regular icosahedron, its surface area formula_3 is twenty times that of each of its equilateral triangle faces. Its volume formula_4 can be obtained as twenty times that of a pyramid whose base is one of its faces and whose apex is the regular icosahedron's center; or as the sum of the volume of two uniform pentagonal pyramids and a pentagonal antiprism. Given that the edge length formula_5 of a regular icosahedron, both expressions are:\nformula_6\nRelation to the spheres.\nThe insphere of a convex polyhedron is a sphere touching every polyhedron's face within. The circumsphere of a convex polyhedron is a sphere that contains the polyhedron and touches every vertex. The midsphere of a convex polyhedron is a sphere tangent to every edge. Given that the edge length formula_5 of a regular icosahedron, the radius of insphere (inradius) formula_8, the radius of circumsphere (circumradius) formula_9, and the radius of midsphere (midradius) formula_10 are, respectively:\nformula_11\nA problem dating back to the ancient Greeks is determining which of two shapes has a larger volume: a regular icosahedron inscribed in a sphere, or a regular dodecahedron inscribed in the same sphere. The problem was solved by Hero, Pappus, and Fibonacci, among others. Apollonius of Perga discovered the curious result that the ratio of volumes of these two shapes is the same as the ratio of their surface areas. Both volumes have formulas involving the golden ratio, but taken to different powers. As it turns out, the regular icosahedron occupies less of the sphere's volume (60.54%) than the regular dodecahedron (66.49%).\nOther measurements.\nThe dihedral angle of a regular icosahedron is formula_12, where formula_13 is the golden ratio. This angle is about 138.19\u00b0, and can be calculated by adding the angle of pentagonal pyramids with regular faces and a pentagonal antiprism. The dihedral angle of a pentagonal antiprism and pentagonal pyramid between two adjacent triangular faces is approximately 38.2\u00b0. The dihedral angle of a pentagonal antiprism between pentagon-to-triangle is 100.8\u00b0, and the dihedral angle of a pentagonal pyramid between the same faces is 37.4\u00b0. Therefore, for the regular icosahedron, the dihedral angle between two adjacent triangles, on the edge where the pentagonal pyramid and pentagonal antiprism are attached, is 37.4\u00b0 + 100.8\u00b0 = 138.2\u00b0.\nThe regular icosahedron has three types of closed geodesics. These are paths on its surface that are locally straight: they avoid the polyhedron's vertices, follow line segments across the faces that they cross, and form complementary angles on the two incident faces of each edge that they cross. The first geodesic forms a regular decagon perpendicular to the longest diagonal and has the length formula_14. The other two geodesics are non-planar, with lengths formula_15 and formula_16.\nSymmetry.\nThe regular icosahedron has the thirty-one axes of rotational symmetry (that is, rotating around an axis that results in an identical appearance). There are six axes passing through two opposite vertices, ten axes rotating a triangular face, and fifteen axes passing through any of its edges. Respectively, these axes are five-fold rotational symmetry (0\u00b0, 72\u00b0, 144\u00b0, 216\u00b0, and 288\u00b0), three-fold rotational symmetry (0\u00b0, 120\u00b0, and 240\u00b0), and two-fold rotational symmetry (0\u00b0 and 180\u00b0). The regular icosahedron also has fifteen mirror planes that can be represented as great circles on a sphere. It divides the surface of a sphere into 120 triangles fundamental domains; these triangles are called Mobius triangles. Both reflections and rotational symmetries are the isometries\u2014transformations in order to maintain the appearance\u2014which forms the full icosahedral symmetry formula_17 of order 120. This symmetry group is isomorphic to the product of the rotational symmetry group and the cyclic group of size two, generated by the reflection through the center of the regular icosahedron.\nThe rotational symmetry group of the regular icosahedron is isomorphic to the alternating group on five letters. This non-abelian simple group is the only non-trivial normal subgroup of the symmetric group on five letters. Since the Galois group of the general quintic equation is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the Abel\u2013Ruffini theorem uses this simple fact, and Felix Klein wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation.\nThe regular icosahedron is isogonal, isohedral, and isotoxal: any two vertices, two faces, and two edges of a regular icosahedron can be transformed by rotations and reflections under its symmetry orbit, which preserves the appearance. Each regular polyhedron has a convex hull on its edge midpoints; icosidodecahedron is the convex hull of a regular icosahedron. Each vertex is surrounded by five equilateral triangles, so the regular icosahedron denotes formula_18 in vertex configuration or formula_19 in Schl\u00e4fli symbol.\nAppearances.\nToys.\nDice are among the most common objects that utilize different polyhedra, one of which is the regular icosahedron. The twenty-sided die was found in ancient times. One example is the die from Ptolemaic Egypt, which was later used with Greek letters inscribed on the faces in the period of Greece and Rome.\nAnother example was found in the treasure of Tipu Sultan, which was made out of gold and with numbers written on each face.\nIn several roleplaying games, such as \"Dungeons &amp; Dragons\", the twenty-sided die (labeled as d20) is commonly used in determining success or failure of an action. It may be numbered from \"0\" to \"9\" twice, in which form it usually serves as a ten-sided die (d10); most modern versions are labeled from \"1\" to \"20\". \"Scattergories\" is another board game in which the player names the category entries on a card within a given set time. The naming of such categories is initially with the letters contained in every twenty-sided dice.\nNatural forms and sciences.\nIn virology, herpes virus have icosahedral shells, especially well-known in adenovirus. The outer protein shell of HIV is enclosed in a regular icosahedron, as is the head of a typical myovirus. Several species of radiolarians discovered by Ernst Haeckel, described their shells as like-shaped various regular polyhedra; one of which is \"Circogonia icosahedra\", whose skeleton is shaped like a regular icosahedron.\nIn chemistry, the closo-carboranes are compounds with a shape resembling the regular icosahedron. The crystal twinning with icosahedral shapes also occurs in crystals, especially nanoparticles. Many borides and allotropes of boron such as \u03b1- and \u03b2-rhombohedral contain boron B12 icosahedron as a basic structure unit.\nIn cartography, R. Buckminster Fuller used the net of a regular icosahedron to create a map known as Dymaxion map, by subdividing the net into triangles, followed by calculating the grid on the Earth's surface, and transferring the results from the sphere to the polyhedron. This projection was created during the time that Fuller realized that Greenland is smaller than South America.\nIn the Thomson problem, concerning the minimum-energy configuration of formula_20 charged particles on a sphere, and for the Tammes problem of constructing a spherical code maximizing the smallest distance among the points, the minimum solution known for formula_21 places the points at the vertices of a regular icosahedron, inscribed in a sphere. This configuration is proven optimal for the Tammes problem, and also for the Thomas problem.\nIn tensegrity, the regular icosahedron is composed of six struts and twenty-four cables that connect twelve nodes. One self-stress state is present within the combination achieved through the use of cellular morphogenesis.\nAncient texts.\nThe regular icosahedron is one of the five Platonic solids. The regular polyhedra have been known since antiquity, but are named after Plato who, in his \"Timaeus\" dialogue, identified these with the five elements, whose elementary units were attributed these shapes: fire (tetrahedron), air (octahedron), water (icosahedron), earth (cube) and the shape of the universe as a whole (dodecahedron). Euclid's \"Elements\" defined the Platonic solids and solved the problem of finding the ratio of the circumscribed sphere's diameter to the edge length.\nFollowing their identification with the elements by Plato, Johannes Kepler in his \"Harmonices Mundi\" sketched each of them, in particular, the regular icosahedron. In his \"Mysterium Cosmographicum\", he also proposed a model of the Solar System based on the placement of Platonic solids in a concentric sequence of increasing radius of the inscribed and circumscribed spheres whose radii gave the distance of the six known planets from the common center. The ordering of the solids, from innermost to outermost, consisted of: regular octahedron, regular icosahedron, regular dodecahedron, regular tetrahedron, and cube.\nLeonardo da Vinci's \"Divina proportione\" drew both regular icosahedron and dodecahedron.\nRepresentation.\nAs a graph.\nEvery Platonic graph, including the icosahedral graph, is a polyhedral graph: they can be drawn in the plane without crossing its edges, and the removal of any two of its vertices leaves a connected subgraph. According to Steinitz's theorem, the icosahedral graph endowed with these heretofore properties represents the skeleton of a regular icosahedron.\nThe icosahedral graph has twelve vertices, the same number of vertices as a regular icosahedron. These vertices are connected by five edges from each vertex, making the icosahedral graph 5-regular. The icosahedral graph is Hamiltonian, because it has a cycle that can visit each vertex exactly once. Any subset of four vertices has three connected edges, with one being the central of all of those three, and the icosahedral graph has no induced subgraph, a claw-free graph.\nThe icosahedral graph is a graceful graph, meaning that each vertex can be labeled with an integer between 0 and 30 inclusive, in such a way that the absolute difference between the labels of an edge's two vertices is different for every edge.\nAs a configuration matrix.\nThe regular icosahedron can be represented as a configuration matrix, a matrix in which the rows and columns correspond to the elements of a polyhedron as the vertices, edges, and faces. The diagonal of a matrix denotes the number of each element that appears in a polyhedron, whereas the non-diagonal of a matrix denotes the number of the column's elements that occur in or at the row's element. The following matrix is:\nformula_22\nRelated figures.\nInscribed in other Platonic solids.\nThe regular icosahedron is the dual polyhedron of the regular dodecahedron. A regular icosahedron can be inscribed in a regular dodecahedron by placing its vertices at the face centers of the regular dodecahedron, and vice versa. The regular dodecahedron has icosahedral symmetry.\nApart from the construction above, the regular icosahedron can be inscribed in a regular octahedron by placing its twelve vertices on the twelve edges of the octahedron such that they divide each edge in the golden section. Because the resulting segments are unequal, there are five different ways to do this consistently, so five disjoint icosahedra can be inscribed in each octahedron. Another relation between the two is that they are part of the progressive transformation from the cuboctahedron's rigid struts and flexible vertices, known as jitterbug transformation.\nA regular icosahedron of edge length formula_23 can be inscribed in a unit-edge-length cube by placing six of its edges\u2014three orthogonal opposite pairs\u2014on the square faces of the cube, centered on the face centers and parallel or perpendicular to the square's edges. Because there are five times as many icosahedron edges as cube faces, there are five ways to do this consistently, so five disjoint icosahedra can be inscribed in each cube. The edge lengths of the cube and the inscribed icosahedron are in the golden ratio.\nStellations.\nThe regular icosahedron has a large number of stellations, constructed by extending the faces of a regular icosahedron. in their work, \"The Fifty-Nine Icosahedra\", identified fifty-nine stellations for the regular icosahedron. The regular icosahedron itself is the zeroth stellation of an icosahedron, and the first stellation has each original face augmented by a low pyramid. The final stellation includes all of the cells in the icosahedron's stellation diagram, meaning every three intersecting face planes of the icosahedral core intersect either on a vertex of this polyhedron or inside it.\nFaceting.\nThe great dodecahedron has other ways to construct from the regular icosahedron. Aside from the stellation, the great dodecahedron can be constructed by faceting the regular icosahedron, that is, removing the pentagonal faces of the regular icosahedron without removing the vertices or creating a new one; or forming a regular pentagon by each of the five vertices inside of a regular icosahedron, and twelve regular pentagons intersecting each other, making a pentagram as its vertex figure.\nOther polyhedra construction.\nThe triakis icosahedron is a Catalan solid constructed by attaching the base of triangular pyramids onto each face of a regular icosahedron, the Kleetope of an icosahedron. The truncated icosahedron is an Archimedean solid constructed by truncating the vertices of a regular icosahedron; the resulting polyhedron may be considered as a football because of having a pattern of numerous hexagonal and pentagonal faces, or a structural form of carbon known as buckminsterfullerene that has 60 carbon atoms bonded together.\nA Johnson solid is a polyhedron whose faces are all regular but which is not uniform. In other words, they do not include the Archimedean solids, the Catalan solids, the prisms, or the antiprisms. Some Johnson solids can be derived by removing part of a regular icosahedron, a process known as \"diminishment\". They are gyroelongated pentagonal pyramid, metabidiminished icosahedron, and tridiminished icosahedron, which remove one, two, and three pentagonal pyramids from the icosahedron, respectively.\nThe edge-contracted icosahedron has a surface like a regular icosahedron but with some faces lie in the same plane.\nSpherical icosahedron.\nThe spherical icosahedron represents a regular icosahedron projected to a sphere, a part of spherical polyhedron. It can be modeled by the arc of great circles, creating bounds as the edges of spherical triangle. Identified by R. Buckminster Fuller, there are 31 great circles in a spherical icosahedron. Its dual is the spherical dodecahedron. The appearance of this shape may be found in \"Impossiball\", a similar toy to Rubik's Cube.\nMiscellaneous.\nAnother related shape can be derived by keeping the vertices of a regular icosahedron in their original positions and replacing certain pairs of equilateral triangles with pairs of isosceles triangles. The resulting polyhedron has the non-convex version of the regular icosahedron. Nonetheless, it is occasionally incorrectly known as Jessen's icosahedron because of the similar visual, of having the same combinatorial structure and symmetry as Jessen's icosahedron; the difference is that the non-convex one does not form a tensegrity structure and does not have right-angled dihedrals.\nThe regular icosahedron is analogous to the 600-cell, a regular 4-dimensional polytope. This polytope has six hundred regular tetrahedra as its cells. The regular icosahedron is a cell for the honeycomb in a hyperbolic three-dimensional space.\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliographies.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14971", "revid": "6008650", "url": "https://en.wikipedia.org/wiki?curid=14971", "title": "Industrial archaeology of Dartmoor", "text": "The industrial archaeology of Dartmoor covers a number of the industries which have, over the ages, taken place on Dartmoor, and the remaining evidence surrounding them. Currently only three industries are economically significant, yet all three will inevitably leave their own traces on the moor: china clay mining, farming and tourism.\nA good general guide to the commercial activities on Dartmoor at the end of the 19th century is William Crossing's \"The Dartmoor Worker\".\nMining.\nIn former times, lead, silver, tin and copper were mined extensively on Dartmoor. The most obvious evidence of mining to the casual visitor to Dartmoor are the remains of the old engine-house at Wheal Betsy which is alongside the A386 road between Tavistock and Okehampton. The word \"Wheal\" has a particular meaning in Devon and Cornwall being either a tin or a copper mine, however in the case of Wheal Betsy it was principally lead and silver which were mined.\nOnce widely practised by many miners across the moor, by the early 1900s only a few tinners remained, and mining had almost completely ceased twenty years later. Some of the more significant mines were Eylesbarrow, Knock Mine, Vitifer Mine and Hexworthy Mine. The last active mine in the Dartmoor area was Great Rock Mine, which shut down in 1969.\nQuarrying.\nDartmoor granite has been used in many Devon and Cornish buildings. The prison at Princetown was built from granite taken from Walkhampton Common. When the horse tramroad from Plymouth to Princetown was completed in 1823, large quantities of granite were more easily transported.\nThere were three major granite quarries on the moor: Haytor, Foggintor and Merrivale. The granite quarries around Haytor were the source of the stone used in several famous structures, including the New London Bridge, completed in 1831. This granite was transported from the moor via the Haytor Granite Tramway, stretches of which are still visible.\nThe extensive quarries at Foggintor provided granite for the construction of London's Nelson's Column in the early 1840s, and New Scotland Yard was faced with granite from the quarry at Merrivale. Merrivale Quarry continued excavating and working its own granite until the 1970s, producing gravestones and agricultural rollers. Work at Merrivale continued until the 1990s, for the last 20 years imported stone such as gabbro from Norway and Italian marble was dressed and polished. The unusual pink granite at Great Trowlesworthy Tor was also quarried, and there were many other small granite quarries dotted around the moor. Various metamorphic rocks were also quarried in the metamorphic aureole around the edge of the moor, most notably at Meldon.\nGunpowder factory.\nIn 1844 a factory for making gunpowder was built on the open moor, not far from Postbridge. Gunpowder was needed for the tin mines and granite quarries then in operation on the moor. The buildings were widely spaced from one another for safety and the mechanical power for grinding (\"incorporating\") the powder was derived from waterwheels driven by a leat.\nNow known as \"Powdermills\" or \"Powder Mills\", there are extensive remains of this factory still visible. Two chimneys still stand and the walls of the two sturdily-built incorporating mills with central waterwheels survive well: they were built with substantial walls but flimsy roofs so that in the event of an explosion, the force of the blast would be directed safely upwards. The ruins of a number of ancillary buildings also survive. A proving mortar\u2014a type of small cannon used to gauge the strength of the gunpowder\u2014used by the factory still lies by the side of the road to the nearby pottery.\nPeat-cutting.\nPeat-cutting for fuel occurred at some locations on Dartmoor until certainly the 1970s, usually for personal use. The right of Dartmoor commoners to cut peat for fuel is known as \"turbary\". These rights were conferred a long time ago, pre-dating most written records. The area once known as the \"Turbary of Alberysheved\" between the River Teign and the headwaters of the River Bovey is mentioned in the Perambulation of the Forest of Dartmoor of 1240 (by 1609 the name of the area had changed to Turf Hill).\nAn attempt was made to commercialise the cutting of peat in 1901 at Rattle Brook Head, however this quickly failed.\nWarrens.\nFrom at least the 13th century until early in the 20th, rabbits were kept on a commercial scale, both for their flesh and their fur. Documentary evidence for this exists in place names such as Trowlesworthy Warren (mentioned in a document dated 1272) and Warren House Inn. The physical evidence, in the form of pillow mounds is also plentiful, for example there are 50 pillow mounds at Legis Tor Warren. The sophistication of the warreners is shown by the existence of vermin traps that were placed near the warrens to capture weasels and stoats attempting to get at the rabbits.\nThe significance of the term \"warren\" nowadays is not what it once was. In the Middle Ages it was a privileged place, and the creatures of the warren were protected by the king 'for his princely delight and pleasure'.\nThe subject of warrening on Dartmoor was addressed in Eden Phillpotts' story \"The River\".\nFarming.\nFarming has been practised on Dartmoor since time immemorial. The dry-stone walls which separate fields and mark boundaries give an idea of the extent to which the landscape has been shaped by farming. There is little or no arable farming within the moor, mostly being given over to livestock farming on account of the thin and rocky soil. Some Dartmoor farms are remote in the extreme.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14972", "revid": "50854881", "url": "https://en.wikipedia.org/wiki?curid=14972", "title": "Idempotence", "text": "Property of operations\nIdempotence (, ) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract algebra (in particular, in the theory of projectors and closure operators) and functional programming (in which it is connected to the property of referential transparency).\nThe term was introduced by American mathematician Benjamin Peirce in 1870 in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means \"(the quality of having) the same power\", from + \"potence\" (same + power).\nDefinition.\nAn element formula_1 of a set formula_2 equipped with a binary operator formula_3 is said to be \"idempotent\" under formula_3 if\n formula_5.\nThe \"binary operation\" formula_3 is said to be \"idempotent\" if\n formula_5 for all formula_8.\nExamples.\nIdempotent functions.\nIn the monoid formula_52 of the functions from a set formula_32 to itself (see set exponentiation) with function composition formula_54, idempotent elements are the functions formula_55 such that formula_56, that is such that formula_57 for all formula_58 (in other words, the image formula_59 of each element formula_58 is a fixed point of formula_61). For example:\nIf the set formula_32 has formula_67 elements, we can partition it into formula_68 chosen fixed points and formula_69 non-fixed points under formula_61, and then formula_71 is the number of different idempotent functions. Hence, taking into account all possible partitions,\n formula_72\nis the total number of possible idempotent functions on the set. The integer sequence of the number of idempotent functions as given by the sum above for \"n\" = 0, 1, 2, 3, 4, 5, 6, 7, 8, ... starts with 1, 1, 3, 10, 41, 196, 1057, 6322, 41393, ... (sequence in the OEIS).\nNeither the property of being idempotent nor that of being not is preserved under function composition. As an example for the former, formula_73 mod 3 and formula_74 are both idempotent, but formula_75 is not, although formula_76 happens to be. As an example for the latter, the negation function formula_77 on the Boolean domain is not idempotent, but formula_78 is. Similarly, unary negation formula_79 of real numbers is not idempotent, but formula_80 is. In both cases, the composition is simply the identity function, which is idempotent.\nComputer science meaning.\nIn computer science, the term \"idempotence\" may have a different meaning depending on the context in which it is applied:\nThis is a very useful property in many situations, as it means that an operation can be repeated or retried as often as necessary without causing unintended effects. With non-idempotent operations, the algorithm may have to keep track of whether the operation was already performed or not.\nComputer science examples.\nA function looking up a customer's name and address in a database is typically idempotent, since this will not cause the database to change. Similarly, a request for changing a customer's address to XYZ is typically idempotent, because the final address will be the same no matter how many times the request is submitted. However, a customer request for placing an order is typically not idempotent since multiple requests will lead to multiple orders being placed. A request for canceling a particular order is idempotent because no matter how many requests are made the order remains canceled.\nA sequence of idempotent subroutines where at least one subroutine is different from the others, however, is not necessarily idempotent if a later subroutine in the sequence changes a value that an earlier subroutine depends on\u2014\"idempotence is not closed under sequential composition\". For example, suppose the initial value of a variable is 3 and there is a subroutine sequence that reads the variable, then changes it to 5, and then reads it again. Each step in the sequence is idempotent: both steps reading the variable have no side effects and the step changing the variable to 5 will always have the same effect no matter how many times it is executed. Nonetheless, executing the entire sequence once produces the output (3, 5), but executing it a second time produces the output (5, 5), so the sequence is not idempotent.\nint x = 3;\nint main() {\n sequence(); // prints \"3\\n5\\n\"\n sequence(); // prints \"5\\n5\\n\"\n return 0;\nIn the Hypertext Transfer Protocol (HTTP), idempotence and safety are the major attributes that separate HTTP methods. Of the major HTTP methods, GET, PUT, and DELETE should be implemented in an idempotent manner according to the standard, but POST doesn't need to be. GET retrieves the state of a resource; PUT updates the state of a resource; and DELETE deletes a resource. As in the example above, reading data usually has no side effects, so it is idempotent (in fact \"nullipotent\"). Updating and deleting a given data are each usually idempotent as long as the request uniquely identifies the resource and only that resource again in the future. PUT and DELETE with unique identifiers reduce to the simple case of assignment to a variable of either a value or the null-value, respectively, and are idempotent for the same reason; the end result is always the same as the result of the initial execution, even if the response differs.\nViolation of the unique identification requirement in storage or deletion typically causes violation of idempotence. For example, storing or deleting a given set of content without specifying a unique identifier: POST requests, which do not need to be idempotent, often do not contain unique identifiers, so the creation of the identifier is delegated to the receiving system which then creates a corresponding new record. Similarly, PUT and DELETE requests with nonspecific criteria may result in different outcomes depending on the state of the system \u2013 for example, a request to delete the most recent record. In each case, subsequent executions will further modify the state of the system, so they are not idempotent.\nIn event stream processing, idempotence refers to the ability of a system to produce the same outcome, even if the same file, event or message is received more than once.\nIn a load\u2013store architecture, instructions that might possibly cause a page fault are idempotent. So if a page fault occurs, the operating system can load the page from disk and then simply re-execute the faulted instruction. In a processor where such instructions are not idempotent, dealing with page faults is much more complex.\nWhen reformatting output, pretty-printing is expected to be idempotent. In other words, if the output is already \"pretty\", there should be nothing to do for the pretty-printer.\nIn service-oriented architecture (SOA), a multiple-step orchestration process composed entirely of idempotent steps can be replayed without side-effects if any part of that process fails.\nMany operations that are idempotent often have ways to \"resume\" a process if it is interrupted\u00a0\u2013 ways that finish much faster than starting all over from the beginning. For example, resuming a file transfer, \nsynchronizing files, creating a software build, installing an application and all of its dependencies with a package manager, etc.\nApplied examples.\nApplied examples that many people could encounter in their day-to-day lives include elevator call buttons and crosswalk buttons. The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial activation and the request being satisfied have no effect, unless the system is designed to adjust the time for satisfying the request based on the number of activations.\nSimilarly, the elevator \"close\" button may be pressed many times to the same effect as once, since the doors close on a fixed schedule \u2013 unless the \"open\" button is pressed. The \"open\" button is not idempotent, because each press adds further delay.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14973", "revid": "38056903", "url": "https://en.wikipedia.org/wiki?curid=14973", "title": "Ithaca, New York", "text": "Ithaca () is a city in the U.S. state of New York. It is situated on the southern shore of Cayuga Lake in the Finger Lakes region of Upstate New York. With a population of 32,108 as of the 2020 census, Ithaca is the largest community in the Ithaca metropolitan statistical area, which includes all 103,558 residents of Tompkins County, of which it is the county seat. The city is named after the Greek island of Ithaca, home of the protagonist Odysseus in Homer's \"Odyssey\".\nIthaca is a college town best known for hosting Cornell University, an Ivy League university founded in 1865, as well as Ithaca College. Until the late 18th century, present-day Ithaca was inhabited by the Cayuga people of the Haudenosaunee (Iroquois) Confederacy. In 1789, the federal and state governments began granting land in the area, known as the Central New York Military Tract, to compensate veterans of the American Revolutionary War. Located in the township of Ulysses, or Tract 22, Ithaca was populated by white settlers in 1794 and formally established in 1821.\nHistory.\n17th century.\nNative Americans lived in this area for thousands of years. When reached by Europeans, this area was controlled by the Cayuga (), one of the five tribes comprising the Iroquois Confederacy (Haudenosaunee). Jesuit missionaries from New France in present-day Quebec had a mission to convert the Cayuga as early as 1657.\n18th century.\nSaponi and Tutelo peoples, Siouan-speaking tribes, later occupied lands at the south end of Cayuga Lake. Dependent tributaries of the Cayuga, they had been permitted to settle on the tribe's hunting lands at the south end of Cayuga Lake, and in Pony (originally Sapony) Hollow of present-day Newfield, New York and Cayuta, New York. Remnants of these tribes had been forced from Virginia and North Carolina by tribal conflicts and European colonial settlement. Similarly, the Tuscarora people, an Iroquoian-speaking tribe from the Carolinas, migrated after defeat in the Yamasee War; they settled with the Oneida people and became the sixth nation of the Haudenosaunee, with chiefs stating the migration was complete in 1722.\nDuring the American Revolutionary War, four of the then six Iroquois nations helped the British attempt to crush the revolution, although bands made decisions on fighting in a highly decentralized way. Conflict with the rebel colonists was fierce throughout the Mohawk Valley and Western New York. In retaliation for conflicts to the east and resentment at the way in which the Iroquois made war, the 1779 Sullivan Expedition was conducted against the Iroquois in the west of the state, destroying more than 40 villages and stored winter crops and forcing their retreat from the area. It destroyed the Tutelo village of Coreorgonel, located near what is now the junction of state routes 13 and 13A just south of Ithaca. Most Iroquois were forced from the state after the Revolutionary War, but some remnants remained. The state sold off the former Iroquois lands to stimulate development and settlement by non-indigenous Americans; lands were also granted as payment to veterans of the war.\nWithin the current boundaries of Ithaca, Native Americans maintained a temporary hunting camp at the base of Cascadilla Gorge. In 1788, eleven men from Kingston, New York, came to the area with two Lenape guides, to explore what they considered wilderness. The following year Jacob Yaple, Isaac Dumond, and Peter Hinepaw returned with their families and constructed log cabins. That same year Abraham Bloodgood of Albany obtained a patent from the state for 1,400 acres, which included all of the present downtown west of Tioga Street.\nIn 1790, the federal government and state began an official program to grant land in the area, known as the Central New York Military Tract, as payment for service to Continental Army soldiers of the Revolutionary War, when the newly established federal government was cash poor. Most local land titles trace back to these Revolutionary war grants. However, the Bloodgood tract was not part of the state bounties to veterans. It was originally granted to a member of the state militia, Martinus Zielie, as a bounty under a different law for recruiting men to enlist in the Continental Army.\nAs part of this process, the Central New York Military Tract, which included northern Tompkins County, was surveyed under the direction of Simeon De Witt, Bloodgood's son-in-law and the Surveyor General of New York. Simeon commissioned his first cousin, Moses De Witt, after whom DeWitt, New York, is named, to survey the area around the south end of Cayuga Lake. Both Simeon and Moses were first cousins of DeWitt Clinton through his mother, Mary De Witt, who married James Clinton, brother of Governor George Clinton. The Commissioners of Lands of New York State (chairman Gov. George Clinton) met in 1790. The Military Tract township in which Ithaca is located was named the Town of Ulysses. A few years later De Witt moved to Ithaca, then called variously \"The Flats,\" \"The City,\" or \"Sodom\"; he (or his clerk, Robert Harpur) renamed it for the Greek island home of Ulysses in the spirit of the multitude of settlement names in the region derived from classical literature, such as Aurelius, Ovid, and especially of Ulysses, New York, the town that contained Ithaca at the time.\nAround 1791, De Witt surveyed what is now the downtown area into lots and sold them at modest prices. That same year John Yaple built a grist mill on Cascadilla Creek.\nOn November 11, 1794, the Treaty of Canandaigua was ratified between approximately 50 Sachems and leaders of the Iroquois and Timothy Pickering on behalf of President George Washington and the United States of America. Among the treaty's numerous provisions, the Cayuga agreed to officially cede their right to all land in present-day Tompkins County in exchange for an approximately 64,000 acre reservation at the north end of Cayuga Lake. Today, the Cayuga Nation of New York, the Cayuga signatories' ancestors, still point to the Treaty of Canandaigua as evidence of their legal sovereignty.\nIthaca's first frame house was erected in 1800 by Abram Markle. In 1804, the village had a postmaster and, in 1805, a tavern.https://\n19th century.\nIthaca became a transshipping point for salt from curing beds near Salina, New York, to buyers south and east. This prompted construction in 1810 of the Owego Turnpike.https:// When the War of 1812 cut off access to gypsum in Nova Scotia, which was used for fertilizer, Ithaca became the center of trade in Cayuga gypsum. The Cayuga Steamboat Company was organized in 1819 and, in 1820, launched the first steamboat on Cayuga Lake, the \"Enterprise.\" In 1821, the village was incorporated at the same time the Town of Ithaca was organized and separated from the parent Town of Ulysses. In 1834, the Ithaca and Owego Railroad's first horse drawn train began service, connecting traffic on the east\u2013west Erie Canal, which was completed in 1825, with the Susquehanna River to the south to expand the trade network.\nWith the Long Depression of 1837, the Ithaca and Owego Railroad was re-organized as the Cayuga and Susquehanna Railroad. It was re-engineered with switchbacks downhill into Ithaca in the late 1840s. In the late 20th century, a short section of its abandoned right-of-way in the City and Town of Ithaca was used for the South Hill Recreation Way.\nHowever, easier early railroad routes were constructed that bypassed Ithaca, such as that of the Syracuse, Binghamton &amp; New York, built in 1854. In the decade following the American Civil War, railroads were built from Ithaca to Auburn, Geneva, Cayuga, Cortland, Elmira, and Athens, Pennsylvania, mainly with financing from Ezra Cornell. These were all branch-lines, since the city, located on a steep hill by the lake, prevented it from being directly connected to a major transportation artery.\nIn 1892, when the Bethlehem, Pennsylvania-based Lehigh Valley Railroad built its main, double-track freight line from Van Etten Junction to Geneva and on to Buffalo, New York, it bypassed Ithaca and Auburn to the west, running via Burdett and eastern Schuyler County on easier grades, just as the Delaware, Lackawanna and Western Railroad had done a decade earlier, in 1882, with its own, new Binghamton-Buffalo mainline extension to the south and west, via Owego, Waverly, Bath, and Dansville. Two of three daily New York City-Buffalo roundtrip passenger trains served Ithaca on the older, original Lehigh Valley Ithaca Branch between Van Etten Junction and Geneva, until discontinuance of the \"Black Diamond\" daylight train, on May 11, 1959. On May 25, 1959, the overnight \"Maple Leaf\" train was shifted back to the Ithaca Branch from the main line via Burdett, and operated on this route until the Lehigh Valley Railroad discontinued this last passenger service on February 4, 1961.\nIn the late 19th century, more industry developed in Ithaca. In 1883, William Henry Baker and his partners founded the Ithaca Gun Company, which manufactured shotguns. The original factory was located in the Fall Creek neighborhood of the city, on a slope later known as Gun Hill, where the nearby waterfall supplied the main source of energy for the plant. The company became an icon in the hunting and shooting world, and its shotguns were known for their fine decorative work. Wooden gunstocks with knots or other imperfections were donated to the high school woodworking shop to be made into lamps. John Philip Sousa and trick-shooter Annie Oakley favored Ithaca Gun Company guns. In 1937, the company began producing the Ithaca 37, based on a 1915 patent by noted firearms designer John Browning. Its 12-gauge shotguns were the standard used for decades by the New York City Police Department and Los Angeles Police Department.\nIn 1885, Ithaca Children's Home was established on West Seneca Street. The orphanage had two programs at the time: a residential home for both orphaned and destitute children, and a day nursery. The village established its first trolley in 1887. Ithaca developed as a small manufacturing and retail center and was incorporated as a city in 1888. The largest industrial company in the area was Morse Chain, elements of which were absorbed into Emerson Power Transmission on South Hill and Borg Warner Automotive in Lansing, New York.\nIthaca claims to be the birthplace of the ice cream sundae, created in 1892 when fountain shop owner Chester Platt \"served his local priest vanilla ice cream covered in cherry syrup with a dark candied cherry on top. The priest suggested the dessert be named after the day, Sunday, although the spelling was later changed out of fear some would find it offensive.\" The local Unitarian church, where the priest, Rev. John Scott, preached, has an annual \"Sundae Sunday\" every September in commemoration. Ithaca's claim has long been disputed by Two Rivers, Wisconsin. Also in 1892, the Ithaca Kitty became one of the first mass-produced stuffed animal toys in the United States.\n20th century.\nIn 1900, Cornell University anatomy professor G. S. Moler made an early movie using frame-by-frame technology. For \"The Skeleton Dance,\" he took single-frame photos of a human skeleton in varying positions, giving the illusion of a dancing skeleton. During the early 20th century, Ithaca was an important center in the silent film industry. These films often featured the local natural scenery. Many of these films were the work of Leopold Wharton and his brother Theodore; The Wharton Studio was on the site of what is now Stewart Park.\nIn 1903, a typhoid epidemic resulting from poor sanitation infrastructure devastated the city. Not having access to unpolluted water was one suspicion to the cause of the outbreak because \"[r]efuse and the contents of the early sewer system dumped directly into the inlet\". One out of ten citizens fell ill or died. Local residents lost fifty-one people to the illness that year, but there was \"an average of thirty-nine cases each year\" for the consecutive ten years following.\nThe Star Theatre on East Seneca Street was built in 1911 and became the most popular vaudeville venue in the region. Wharton movies were also filmed and shown there. After the film industry centralized in Hollywood, production in Ithaca effectively ceased. Few of the silent films made in Ithaca have been preserved.\nIthaca had film studio activity during the silent film era including Wharton Studio.\nAfter World War II, the Langmuir Research Labs of General Electric developed as a major employer; the defense industry continued to expand. GE's headquarters were in Schenectady, New York, to the northeast in the Mohawk Valley.\nAlthough Ithaca has a history of Ku Klux Klan activity, including a cross-burning in 1923 and 1924, \"the peak years of Klan activity in Ithaca were 1923-1925\" and it represented only a fraction of the population. Ithaca is known for its political activism regarding civil rights and environmental issues. \u201cMartin Luther King Jr. came to speak twice in Ithaca, in 1960 and 1961\u201d. The annual Ithaca Festival, which often takes place on the Ithaca Commons or Stewart Park, frequently centers around themes promoting \"a political statement into a cultural and festive event\u201d\n21st century.\nThe Ithaca Gun Company tested their shotguns behind the plant on Lake Street; the shot fell into the Fall Creek gorge at the base of Ithaca Falls. Lead accumulated in the soil in and around the factory and gorge. A major lead clean-up effort sponsored by the United States Superfund took place from 2002 to 2004, managed through the Environmental Protection Agency. The old Ithaca Gun building has been dismantled, though its iconic smokestack remains standing. It was scheduled to be replaced by the development of an apartment complex on the cleaned land.\nThe former Morse Chain company factory on South Hill, now owned by Emerson Power Transmission, was the site of extensive groundwater and soil contamination from its industrial operations. Emerson Power Transmission has been working with the state and South Hill residents to determine the extent and danger of the contamination and aid in cleanup.\nIn 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council: the first black man to be elected to the council and the youngest African American to be elected to office in the United States. He served his full term and has mentored other student politicians. In 2011, Svante Myrick, a 2009 Cornell University graduate, was elected as the youngest mayor of the city of Ithaca.\nIn 2023, President of the Cornell Student Assembly Patrick Kuehl launched a secret write-in campaign and succeeded in unseating Alderperson Jorge Defendini, another Cornell alumnus, from the Ithaca Common Council. Kuehl had collaborated with fellow students Alderperson Tiffany Kumar and Alderperson-elect Clyde Lederman and canvassed for several months but did not publicly express his intent to run for the seat until the evening of election day. Kuehl received an inordinate number of provisional ballots and absentee ballots from residents of the Sigma Phi and Delta Kappa Epsilon fraternity houses. Various Ithaca and Cornell groups condemned the secret campaign as undemocratic.\nGeography.\nThe valley in which Cayuga Lake is located is long and narrow with a north\u2013south orientation. Ithaca is located at the southern end (the \"head\") of the lake, but the valley continues to the southwest behind the city. Originally a river valley, it was deepened and widened by the action of Pleistocene ice sheets over the last several hundred thousand years.\nThese ice sheets gouged the land crosswise to preexisting streams, producing hanging valleys. Once the last ice sheets receded \u2014 around twenty or thirty thousand years ago \u2014 these streams cut deep into the steep hillsides, forming the many distinctive gorges, rapids, and waterfalls seen in the region; examples include Fall and Cascadilla Creeks in Ithaca, and nearby Buttermilk Falls, Enfield Gorge, and Taughannock Falls. Cayuga Lake is the most recent lake in a long series of lakes which developed as the ice retreated northward. The lake drains to the north, and was formed behind a dam of glacial debris called a moraine.\nRock in the region is predominantly Devonian shale and sandstone. North of Ithaca, it is relatively fossil rich. The world-renowned fossils found in this area can be examined at the Museum of the Earth. Glacial erratics can also be found in the area.\nIthaca was founded on flat land just south of the lake \u2014 land that formed in fairly recent geological times when silt filled the southern end of the lake. The city ultimately spread to the adjacent hillsides, which rise several hundred feet above the central flats: East Hill, West Hill, and South Hill. The Cornell campus is loosely bounded to the north and south by Fall and Cascadilla Creeks, respectively.\nThe natural vegetation of the Ithaca area is northern temperate broadleaf forest. It is dominated by deciduous trees, including maple, sycamore, black walnut, birch, and oak; coniferous trees include white pine, Norway spruce, and eastern hemlock. The city of Ithaca has a rich diversity of tree plantings, with over 190 species, including cherry, southern magnolia, and ginkgo. In addition to visual beauty, this species diversification helps reduce the impact of arboreal epidemics, such as that caused by the emerald ash borer.\nClimate.\nAccording to the K\u00f6ppen climate classification method, Ithaca experiences a warm-summer humid continental climate, also known as a hemiboreal climate (\"Dfb\"). Summers are warm but brief, and it is cool-to-cold the rest of the year, with long, snowy winters; an average of of snow falls per year. In addition, frost may occur any time of year except mid-summer.\nWinter is typically characterized by freezing temperatures, cloudy skies and light-to-moderate snows, with some heavier falls; the largest snowfall in one day was on February 14, 1914. But the season is also variable; there can be short mild periods with some rain, but also outbreaks of frigid air with night temperatures down to or lower. Summers usually bring sunshine, along with moderate heat and humidity, but also frequent afternoon thunderstorms. Nights are pleasant and sometimes cool. Occasionally, there can be heatwaves, with temperatures rising into the to range, but they tend to be brief.\nThe average date of the first freeze is October 5, and the average date of the last freeze is May 15, giving Ithaca a growing season of 141 days. The average date of the first and last snowfalls are November 12 and April 7, respectively. The hardiness zone is between 5b and 6a. Extreme temperatures range from as recently as February 2, 1961, up to on July 9, 1936.\nThe valley flatland has slightly cooler weather in winter, and occasionally Ithaca residents experience simultaneous snow on the hills and rain in the valley. The phenomenon of mixed precipitation (rain, wind, and snow), common in the late fall and early spring, is known tongue-in-cheek as \"ithacation\" to many of the local residents.\nDue to the microclimates created by the impact of the lakes, the region surrounding Ithaca (Finger Lakes American Viticultural Area) experiences a short but adequate growing season for winemaking similar to the Rhine Valley wine district of Germany. As such, the region is home to many wineries.\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\nThe most reported ancestries in 2020 were:\nIthaca is the principal city of the Ithaca-Cortland Combined Statistical Area, which includes the Ithaca Metropolitan Statistical Area (Tompkins County) and the Cortland Micropolitan Statistical Area (Cortland County), which had a combined population of 145,100 at the 2000 census.\nAs of the census of 2000, there were 29,287 people, 10,287 households, and 2,962 families residing in the city. The population density was . There were 10,736 housing units at an average density of . The racial makeup of the city was 73.97% White, 13.65% Asian, 6.71% Black or African American, 0.39% Native American, 0.05% Pacific Islander, 1.86% from other races, and 3.36% from two or more races. Hispanic or Latino of any race were 5.31% of the population.\nThere were 10,287 households, out of which 14.2% had children under the age of 18 living with them, 19.0% were married couples living together, 7.8% had a female householder with no husband present, and 71.2% were non-families. 43.3% of all households were made up of individuals, and 7.4% had someone living alone who was 65 years of age or older. The average household size was 2.13 and the average family size was 2.81.\nIn the city, the population was spread out, with 9.2% under the age of 18, 53.8% from 18 to 24, 20.1% from 25 to 44, 10.6% from 45 to 64, and 6.3% who were 65 years of age or older. The median age was 22 years. For every 100 females, there were 102.6 males. For every 100 females age 18 and over, there were 102.2 males.\nThe median income for a household in the city was $21,441, and the median income for a family was $42,304. Males had a median income of $29,562 versus $27,828 for females. The per capita income for the city was $13,408. About 13.2% of individuals and 4.2% of families were below the poverty line.\nGreater Ithaca.\nThe term \"Greater Ithaca\" encompasses both the City and Town of Ithaca, as well as several smaller settled places within or adjacent to the Town:\nMunicipalities\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCensus-designated places\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nEconomy.\nThe economy of Ithaca is based on education and further supported by agriculture, technology and tourism. As of 2006, Ithaca has continued to have one of the few expanding economies in New York State outside New York City. It draws commuters for work from the neighboring rural counties of Cortland, Tioga, and Schuyler, as well as from the more urbanized Chemung County.\nIthaca has tried to maintain its traditional downtown shopping area with its pedestrian orientation; this includes the Ithaca Commons pedestrian mall, Press Bay Alley, and Press Bay Court. Another commercial center, Collegetown, is located next to the Cornell campus. It features a number of restaurants, shops and bars, and an increasing number of high-rise apartments. It is primarily frequented by Cornell University students.\nIthaca has many of the businesses characteristic of small American university towns: bookstores, art-house cinemas, craft stores and vegetarian-friendly restaurants. Moosewood Restaurant, founded as a collective in 1973, published a number of vegetarian cookbooks. \"Bon Appetit\" magazine ranked it among the thirteen most influential restaurants of the 20th century. Ithaca has many local restaurants and chains, both in the city and town, with a range of ethnic foods.\nThe Ithaca Farmers Market, a cooperative, first opened for business on Saturdays in 1973. It is located at Steamboat Landing, where steamboats from Cayuga Lake used to dock.\nThe South Hills Business Campus originally opened in 1957 as the regional headquarters of the National Cash Register Company. Running three full factory shifts, NCR was a major employer. Although it was sold in 1991 to American Telephone and Telegraph and later acquired by Cognitive TPG, it remains a major tenant of the South Hill Business Campus, which is now owned by a group of private investors.\nAgriculture.\nIthaca, home to the Cornell University College of Agriculture and Life Sciences, has a deep connection to Central New York's farming and dairy industries. About 60 small farms are located in the greater Ithaca/Trumansburg area, including a number of research farms managed by the Cornell University Agricultural Experiment Station. Cornell's Dairy Research Facility is a center of research and support for New York's large and growing milk and yogurt industries.\nArts and culture.\nFounded in 1983, the Sciencenter is a non-profit hands-on science museum, accredited by the American Alliance of Museums (AAM). It is a member of the Association of Science-Technology Centers (ASTC) and Association of Children's Museums (ACM).\nThe Museum of the Earth is a natural history museum created in 2003 by the Paleontological Research Institution (PRI). The PRI was founded in Ithaca in 1932 and is the publisher of the oldest journal of paleontology in the Western Hemisphere. Exhibits cover the 4.5-billion-year history of the Earth in an accessible manner, including interactive displays. As of 2004, the PRI is now formally affiliated with Cornell.\nThe Cayuga Nature Center occupies the site of the 1914 Cayuga Preventorium, a facility for children with tuberculosis; treatment of what was then considered an incurable disease was based on rest and good nutrition. In 1981, the Cayuga Nature Center was incorporated as an independent, private, non-profit educational organization, offering environmental education to local school districts. In 2011, the PRI merged with the Cayuga Nature Center, making it a sister organization to the Museum of the Earth.\nThe Cornell Lab of Ornithology is located in the Imogene Powers Johnson Center for Birds and Biodiversity. The Lab's Visitors' Center and observation areas are open to the public. Displays include a surround-sound theater, object-theater presentation, sound studio and informational kiosks featuring bird sounds and information.\nThe Herbert F. Johnson Museum of Art at Cornell houses one of the finest collections of art in upstate New York. Special exhibitions are mounted each year, plus selections from a global permanent collection, which is displayed on six public floors. The collection includes art from throughout Asia, Africa, Europe, the Americas, graphic arts, medallic art and Tiffany glass, ranging from the ancient to the contemporary.\nIthaca has a number of active professional theaters. The Center for the Arts at Ithaca, Inc., operates the \"Hangar Theatre\". Opened in 1975 in a renovated municipal airport hangar, the Hangar hosts a summer season and brings a range of theatre to regional audiences including students, producing a school tour and Artists-in-the-Schools programs.\nIthaca is also the home to Kitchen Theatre Company, a non-profit professional company founded in 1991 with a theatre on West State Street. \nThe Cherry Arts was founded in 2017, operating a flexible theatre, the Cherry Artspace. They present productions of international and unusual works by the Cherry Artists' Collective, as well as hosting performances by other artists and organizations. They also operate the Cherry Gallery, which presents solo and group exhibitions, often in dialogue with their performance works, and the Camilla Schade Studio, which houses rehearsals, salon performances, and the Cherry Art Hive. \nBeyond these three theaters, Civic Ensemble is a creative collaborative ensemble (without its own performance space) staging emerging playwrights' work and community-based original productions.\nIthaca is noted for its annual community celebration, The Ithaca Festival. Ithaca also hosts one of the largest used-book sales in the United States.\nThe Constance Saltonstall Foundation for the Arts supports New York State artists and writers through two key programs: free stipend-supported, juried residencies and a self-directed, non-juried, low-cost retreats. Saltonstall was founded in 1995 and is located eight miles outside Ithaca. Between 1996 and 2008, the Saltonstall Foundation provided grants to individual artists and writers from central and western New York counties. Saltonstall hosts Open House events for the public to meet the current artists and writers at the residency and to learn about the foundation.\nFounded in 1992, the Namgyal Monastery in Ithaca is the North American seat of the Dalai Lama's Namgyal Monastery.\nThe city and town also sponsor The Apple Festival in the fall, the Chili Fest in February, the Finger Lakes International Dragon Boat Festival in July, Porchfest in late September and the Ithaca Brew Fest in Stewart Park in September.\nIthaca has also pioneered the Ithaca Health Fund, a popular cooperative health insurance. Ithaca is home to Ithaca Hours, one of the first local currency systems in the United States. It was developed by Paul Glover.\nMusic.\nIthaca is the home of the Cayuga Chamber Orchestra.\nThe Cornell Concert Series has been hosting musicians and ensembles of international stature since 1903. For its initial 84 years, the series featured Western classical artists exclusively. In 1987, however, the series broke with tradition to present Ravi Shankar and has since grown to encompass a broader spectrum of the world's great music. Now, it balances a mix of Western classical music, traditions from around the world, jazz, and new music in these genres. In a single season, Cornell Concert Series presents performers ranging from the Leipzig Tomanerchor and Danish Quartet to Simon Shaheen, Vida Guitar Quartet, and Eighth Blackbird.\nThe School of Music at Ithaca College was founded in 1892 by William Egbert as a music conservatory on Buffalo Street. Among the degree programs offered are those in Performance, Theory, Music Education and Composition. Since 1941, the School of Music has been accredited by the National Association of Schools of Music.\nIthaca's Suzuki school, Ithaca Talent Education, provides musical training for children of all ages and also teacher training for undergraduate and graduate-level students. The Community School of Music and Art uses an extensive scholarship system to offer classes and lessons to any student, regardless of age, background, economic status or artistic ability.\nA number of musicians call Ithaca home, most notably Samite of Uganda, The Burns Sisters, The Horse Flies, Johnny Dowd, Mary Lorson, cellist Hank Roberts, Anna Coogan, John Brown's Body, Kurt Riley, X Ambassadors, and Alex Kresovich. Old-time music is a staple, and folk music is featured weekly on WVBR-FM's \"Bound for Glory\", North America's longest-running live folk concert broadcast. The Finger Lakes GrassRoots Festival of Music and Dance, hosted by local band Donna the Buffalo, is held annually during the third week in July in the nearby village of Trumansburg, with more than 60 local, national and international acts.\nIthaca is the center of a thriving live music scene, featuring more than 200 groups playing most genres of American popular and world music, the predominant genres being folk, rock, blues, jazz, country, lo-fi and reggae. There are more than 80 live music venues within a 40-mile radius of the city, including cafes, pubs, clubs and concert halls.\nGovernment.\nThere are two governmental entities in the area: the Town of Ithaca and the City of Ithaca. The Town of Ithaca is one of the nine towns comprising Tompkins County. The City of Ithaca is surrounded by, but legally independent of the Town of Ithaca.\nThe City of Ithaca has a council\u2013manager government. The charter of the City of Ithaca provides for a full-time mayor and the Uniform City Court Act provides for two, full-time city court judges. Since 1995, the mayor has been elected to a four-year term, and since 1989, the city court judges have been elected to a ten-year term. At a referendum in 2022, city voters approved a city charter amendment adopting from a council\u2013manager government in place of a mayor\u2013council government. The city's first city manager, Deb Mohlenhoff, took office on 1 January 2024.\nSince 1983, the city has been divided into five wards. Each elects two representatives to the city council, known as the Common Council, for staggered four-year terms. In March 2015, the Common Council unanimously adopted a resolution recognizing freedom from domestic violence as a fundamental human right. In September 2023, the Common Council unanimously passed an extensive local ordinance to \"protect, defend and shield transgender individuals\".\nIn December, 2005, the City and Town governments began discussing opportunities for increased government consolidation, including the possibility of joining the two into a single entity. This topic had been previously discussed in 1963 and 1969. Cayuga Heights, a village adjacent to the city on its northeast, voted against annexation into the city of Ithaca in 1954.\nPolitics.\nPolitically, the majority of the city's voters (many of them students) have supported liberalism and the Democratic Party. A November 2004 study by ePodunk lists it as New York's most liberal city. This contrasts with the more conservative leanings of the generally rural Upstate New York region. The city's voters are also more liberal than those in the rest of Tompkins County, although the county was last carried by a Republican presidential candidate in 1980. In 2008, Barack Obama, running against New York State's US Senator Hillary Clinton, won Tompkins County in the Democratic Presidential Primary, the only county that he won in New York State. Obama won Tompkins County (including Ithaca) by a wide margin of 41% over his opponent John McCain in the November 2008 election. Tompkins County is currently part of New York's 19th congressional district.\nIthaca Police Department.\nThe Ithaca Police Department (IPD) is responsible for all public safety and law enforcement services within the municipal boundaries of Ithaca. The IPD was established on June 1, 1888, upon Ithaca's formal incorporation as a city. As of 2024, it consisted of 54 employees including department chief Thomas Kelly.\nThe City of Ithaca Common Council oversees IPD and makes budgeting determinations on behalf of the department.\nThe Ithaca Police Benevolent Association (IPBA) is a police union established by members of IPD.\nSister city.\nIthaca is a sister city of:\nEducation.\nColleges.\nIthaca is a major educational center in Central New York. The two major post-secondary educational institutions located in Ithaca were each founded in the late 19th century. In 1865, Ezra Cornell founded Cornell University, which overlooks the town from East Hill. It was opened as a coeducational institution. Women first enrolled in 1870. Ezra Cornell also established a public library for the city. Ithaca College was founded as the Ithaca Conservatory of Music in 1892. Ithaca College was originally located in the downtown area but relocated to South Hill in the 1960s. In 2018, there were 23,600 students enrolled at Cornell and 6,700 at Ithaca College. Tompkins Cortland Community College is located in the neighboring town of Dryden, and has an extension center in downtown Ithaca. Empire State College offers non-traditional college courses to adults in downtown Ithaca.\nPublic schools.\nThe Ithaca City School District, based in Ithaca, encompasses the city and its surrounding area and enrolls about 5,500 K-12 students in eight elementary schools (roughly one for every neighborhood), two middle schools (Boynton and Dewitt), Ithaca High School and the Lehman Alternative Community School, a combined middle and high school. Several private elementary and secondary schools are located in the Ithaca area, including the Roman Catholic Immaculate Conception School, the Cascadilla School, the New Roots Charter School, the Elizabeth Ann Clune Montessori School, the Namaste Montessori School (in the Trumansburg area) and the Ithaca Waldorf School. Ithaca has two networks for supporting its home-schooling families: Loving Education At Home (LEAH) and the Northern Light Learning Center (NLLC). TST BOCES is located in Tompkins County.\nLibrary.\nThe Tompkins County Public Library, located at 101 East Green Street, serves as the public library for Tompkins County and is the Central Library for the Finger Lakes Library System. The library serves over 38,000 registered borrowers and contains nearly 260,000 items in its circulating collection, and circulates about 800,000 items annually.\nMedia.\nThe \"Ithaca Journal\", founded in 1815, is a morning daily newspaper that has been owned by Gannett since 1912. The \"Ithaca Voice\" is a nonprofit digital news site with a mission to improve civic and political understanding in Ithaca and Tompkins County. The \"Ithaca Times\" is a free alternative weekly newspaper that's published every Wednesday. \"The Cornell Daily Sun\" is also published in Ithaca, operating since 1880. Other media outlets include the online magazine \"14850.com\".\nIthaca is home to several radio stations:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPublic radio:\nOther FM stations include: Saga's \"98.7 The Vine\", a low-powered translator station; WFIZ \"Z95.5\", airing a top-40 (CHR) format; contemporary Christian music station WCII 88.9; and classic rock \"The Wall\" WLLW 99.3 and 96.3, based in Seneca Falls with a transmitter in Ithaca.\nTransportation.\nIn 2009, the Ithaca metropolitan statistical area (MSA) ranked as the highest in the United States for the percentage of commuters who walked to work (15.1 percent). In 2013, the Ithaca MSA ranked as the second-lowest in the United States for percentage of commuters who traveled by private vehicle (68.7 percent). During the same year, 17.5 percent of commuters in the Ithaca MSA walked to work.\nRoads.\nIthaca is in the rural Finger Lakes region about northwest of New York City; the nearest larger cities, Binghamton and Syracuse, are an hour's drive away by car, Rochester and Scranton are two hours, Buffalo and Albany are three hours. New York City, Philadelphia, Toronto, and Ottawa are approximately four hours away.\nIthaca lies at over a half-hour's drive from any interstate highway, and all car trips to Ithaca involve some driving on two-lane state rural highways. The city is at the convergence of many regional two-lane state highways: Routes 13, 13A, 34, 79, 89, 96, 96B and 366. These are usually not congested except in Ithaca proper. However, Route 79 between the I-81 access at Whitney Point and Ithaca receives a significant amount of Ithaca-bound congestion right before Ithaca's colleges reopen after breaks.\nIn July 2008, a non-profit called Ithaca Carshare began a carsharing service in Ithaca. Ithaca Carshare has a fleet of vehicles shared by over 1500 members as of July, 2015 and has become a popular service among both city residents and the college communities. Vehicles are located throughout Ithaca downtown and at the two major institutions. With Ithaca Carshare as the first locally-run carsharing organization in New York State, others have since launched in Buffalo, Albany, and Syracuse.\nRideshare services to promote carpooling and vanpooling are operated by ZIMRIDE and VRIDE. A community mobility education program, Way2Go, is operated by Cornell Cooperative Extension of Tompkins County. Way2Go's website provides consumer information and videos. Way2Go works collaboratively to help people save money, stress less, go green and improve mobility options. The 2-1-1 Tompkins/Cortland Help line connects people with services, including transportation, in the community, by telephone and web on a 24/7 basis. The information and referral service is operated by the Human Services Coalition of Tompkins County, Inc. Together, 2-1-1 Information and Referral and Way2Go are a one-call, one-click resource designed to mobility services information for Ithaca and throughout Tompkins County.\nBus.\nThere is frequent intercity bus service by Greyhound Lines, New York Trailways, OurBus, FlixBus, and Shortline (Coach USA), particularly to Binghamton and New York City, with limited service to Rochester, Buffalo and Syracuse, and via connections in Binghamton to Utica and Albany. OurBus provides limited holiday services to Allentown, Pennsylvania, Philadelphia, and Washington, D.C.. Cornell University runs a premium campus-to-campus bus between its Ithaca campus and its medical school in Manhattan, New York City, which is open to the public. Starting in September 2019, intercity buses began serving Ithaca from the downtown bus stop at 131 East Green Street after the former Greyhound bus station on West State Street closed due to staff retirement and building maintenance issues. OurBus now picks up and drops off on Seneca Street, near the downtown Hilton Garden Inn.\nIthaca is the center of an extensive bus public transportation network. Tompkins Consolidated Area Transit, Inc. (TCAT, Inc.) is a not-for-profit corporation that provides public transportation for Tompkins County, New York. TCAT was reorganized as a non-profit corporation in 2004 and is primarily supported locally by Cornell University, the City of Ithaca and Tompkins County. TCAT's ridership increased from 2.7 million in 2004 to 4.4 million in 2013. TCAT operates 34 routes, many running seven days a week. It has frequent service to downtown, Cornell University, Ithaca College, and The Shops at Ithaca Mall in the Lansing, New York, but less frequent service to residential and rural areas, including Trumansburg and Newfield. Chemung County Transit (C-TRAN) runs weekday commuter service from Chemung County to Ithaca. Cortland Transit runs commuter service to Cornell University. Tioga County Public Transit operated three routes to Ithaca and Cornell, but ceased operations on November 30, 2014.\nGadabout Transportation Services, Inc. provides demand-response paratransit service for Tompkins County residents 55 and older, and people with disabilities. Ithaca Dispatch provides local and regional taxi service. In addition, Ithaca Airline Limousine and IthaCar Service connect to the local airports.\nAirports.\nIthaca is served by Ithaca Tompkins International Airport, located about three miles to the northeast of the city center. In late 2019, the airport completed a major $34.8 million renovation which included a larger terminal with additional passenger gates and jet bridges, expanded passenger amenities and a customs facility that enables it to receive international charter and private flights.\nAmerican Airlines pulled out of Ithaca on September 7, 2022, citing pilot shortages. Delta Connection provides service to and from John F. Kennedy International Airport, operated by its commuter partner Endeavor Air, using the Bombardier CRJ900. United Express offers daily flights to its hub at Newark Liberty International Airport, operated by its commuter partners GoJet Airlines and Republic Airways, using the Bombardier CRJ700 and the Embraer E175, respectively.\nRailways.\nInto the mid-20th century, it was possible to reach Ithaca by passenger rail. At least two trains per day serviced Ithaca along the Delaware, Lackawanna and Western Railroad until 1942, or the Lehigh Valley Railroad. The trip took about seven hours from New York City, about eight hours from Philadelphia, and about three hours from Buffalo, New York. There has been no passenger rail service since February 4, 1961.\nBeginning in the 1870s, there were trains to Buffalo via Geneva, New York, to New York City via Wilkes-Barre, Pennsylvania on the Lehigh Valley Railroad, to Hoboken, New Jersey with a train change in Owego via Binghampton and Scranton, Pennsylvania on the Delaware, Lackawanna and Western Railroad, and to the U.S. Northeast via Cortland, New York on the Lehigh Valley Railroad. The Lehigh Valley's top New York City-Ithaca-Buffalo passenger train, the daylight \"Black Diamond,\" was publicized as \"The Handsomest Train in the World,\" but it took a roundabout route to New York City, traveling south to Waverly, New York, to Wilkes-Barre and Easton, and then across New Jersey. It was named after the railroad's largest commodity, anthracite coal, and made its last run on May 11, 1959.\nUntil March 1942, the Lackawanna Railroad operated two shuttle trains a day between Ithaca and Owego, where passengers could transfer to westbound trains to Buffalo and Chicago or eastbound to Binghamton, Scranton, Pennsylvania, and Hoboken, New Jersey, across the Hudson River from New York City. Until September 15, 1958, the Lackawanna maintained Syracuse-Binghamton service through nearby Cortland, to the east. Until May 11, 1959, two Lehigh Valley trains a day made both westbound and eastbound stops in Ithaca. The last passenger train making stops in Ithaca was the Lehigh Valley's overnight \"Maple Leaf\", which discontinued in February 1961.\nWithin Ithaca, electric railways ran along Stewart Avenue and Eddy Street. Ithaca was the fourth community in New York state with a street railway; streetcars ran from 1887 until the summer of 1935.\nIn December 2018, the Ithaca Central Railroad, a Watco subsidiary, took over operation via lease of the Norfolk Southern Ithaca Secondary line from Sayre, Pennsylvania to the Cargill Salt mine site on the eastern shore of Cayuga Lake, near Myers Point. Unit coal trains carrying bituminous coal were delivered to the Ithaca Central at Sayre by Norfolk Southern for less than eight months afterward, traveling to the Ridge site of the Cayuga Operating Company: a coal-burning power plant known as Milliken Station during NYSEG ownership. The power plant closed on August 29, 2019. As of 2022, there are proposed plans to convert its brownfield site into a major data center. The main rail freight traffic is now salt from the Cargill salt mine farther north. Norfolk Southern's tracks, headed north on the former Lehigh Valley Auburn and Ithaca Branch, include a distinctive section in Ithaca that runs along the side of Fulton St. (NY13 southbound), although not in the street itself.\nPoints of interest.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReputation.\nIn addition to its liberal politics, Ithaca is commonly listed among the most culturally liberal of American small cities. In 1997, \"Utne Reader\" named Ithaca \"America's most enlightened town\". As of 2007, according to ePodunk's Gay Index, Ithaca has a score of 231, versus a national average score of 100. Like many small college towns, Ithaca has also received accolades for having a high overall quality of life.\nIn its earliest years, during the frontier days, what is now Ithaca was briefly known by the names \"The Flats\" and \"Sodom,\" the name of the Biblical city of sin, due to its reputation as a town of \"notorious immorality\", a place of horse racing, gambling, profanity, Sabbath-breaking and readily-available liquor. Simeon De Witt renamed the town Ithaca for Odysseus's home island in the early 19th century, though nearby Robert H. Treman State Park still contains Lucifer Falls. Ithaca is primarily known for its growing wineries and microbreweries, live music, colleges and small dairy farms.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14975", "revid": "51047428", "url": "https://en.wikipedia.org/wiki?curid=14975", "title": "Ivy League", "text": "Athletic conference of American universities\nThe Ivy League is an American collegiate athletic conference of eight private research universities in the Northeastern United States. It participates in the National Collegiate Athletic Association (NCAA) Division I, and in football, in the Football Championship Subdivision (FCS). The term \"Ivy League\" is used more broadly to refer to the eight schools that belong to the league, which are globally renowned as elite colleges associated with academic excellence, highly selective admissions, and social elitism. The term was used as early as 1933, and it became official in 1954 following the formation of the Ivy League athletic conference. At times, they have also been referred to as the \"Ancient Eight\".\nThe eight members of the Ivy League are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, University of Pennsylvania, Princeton University, and Yale University. The conference headquarters is in Princeton, New Jersey. All of the \"Ivies\" except Cornell were founded during the colonial period and therefore make up seven of the nine colonial colleges. The other two colonial colleges, Queen's College (now Rutgers University) and the College of William &amp; Mary, became public institutions.\nOverview.\nIvy League schools are some of the most prestigious universities in the world. All eight universities place in the top 15 of the 2025 \"U.S. News &amp; World Report\" National Universities ranking. \"U.S. News\" has named a member of the Ivy League as the best national university every year since 2001: as of 2020[ [update]], Princeton eleven times, Harvard twice, and the two schools tied for first five times. In the 2024\u20132025 \"U.S. News &amp; World Report\" Best Global University Ranking, six Ivies rank in the top 20: Harvard (#1), Columbia (#9), Yale (#10), Penn (#14), Princeton (#18), and Cornell (#19) \u2014ranks that \"U.S. News\" says are based on \"indicators that measure their academic research performance and their global and regional reputations.\" All eight Ivy League schools are members of the Association of American Universities, the most prestigious alliance of American research universities.\nUndergraduate enrollments range from about 4,500 to about 15,000, larger than most liberal arts colleges and smaller than most state university systems. Total enrollment, which includes graduate students, ranges from approximately 6,600 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $6.9\u00a0billion to Harvard's $53.2\u00a0billion, the largest financial endowment of any academic institution in the world.\nThe Ivy League is similar to other groups of universities in other countries, such as Oxbridge in England, the C9 League in China, the \u00c9coles Normales Sup\u00e9rieures in France, the SKY Universities in South Korea, and the Imperial Universities in Japan.\nMembers.\nIvy League universities have some of the largest university financial endowments in the world, allowing the universities to provide abundant resources for their academic programs, financial aid, and research endeavors. As of 2021, Harvard University had an endowment of $53.2\u00a0billion, the largest of any educational institution. Each university attracts millions of dollars in annual research funding from both the federal government and private sources.\nFormer affiliate members.\nBefore the 2000s, many of the Ivy League championships for men's and women's cross country, indoor and outdoor track &amp; field, and swimming &amp; diving were formatted as invitationals that many schools across the eastern United States would attend. In other sports, such as fencing, wrestling, men's and women's ice hockey, and men's and women's rowing, all of the Ivy League schools were members of other single-sport conferences and the top-performing Ivy League team would be crowned the champion.\nThe United States Military Academy and the United States Naval Academy were members of the Ivy League in many sports and were crowned as Ivy League champions while competing with Ivy League teams. Both schools left the conference in the early 2000s to join with their current conference, the Patriot League, except for football, for which they are affiliate members of the American Athletic Conference.\nHistory.\nInstitutional history.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\n\nOrigin of the name.\n\"Planting the ivy\" was a customary class day ceremony at many colleges in the 1800s. In 1893, an alumnus told \"The Harvard Crimson\", \"In 1850, class day was placed upon the University Calendar\u00a0... the custom of planting the ivy, while the ivy oration was delivered, arose about this time.\" At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as \"Ivy Day\" in 1874. Ivy planting ceremonies are recorded at Yale University, Simmons College, and Bryn Mawr College among other schools. Princeton's \"Ivy Club\" was founded in 1879.\nThe first usage of \"Ivy\" in reference to a group of colleges is from sportswriter Stanley Woodward (1895\u20131965).\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A proportion of our eastern ivy colleges are meeting little fellows another Saturday before plunging into the strife and the turmoil.\u2014\u200a\nThe first known instance of the term \"Ivy League\" appeared in \"The Christian Science Monitor\" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. At this time, however, none of these institutions made efforts to form an athletic league.\nA common folk etymology attributes the name to the Roman numeral for four (IV), asserting that there was such a sports league originally with four members. The \"Morris Dictionary of Word and Phrase Origins\" helped to perpetuate this belief. The supposed \"IV League\" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a fourth school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Columbia, and Yale met on November 23, 1876, at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.\nPre-Ivy League.\nSeven out of the eight Ivy League schools are Colonial Colleges: institutions of higher education founded prior to the American Revolution. Cornell, the exception to this commonality, was founded immediately after the American Civil War. These seven colleges served as the primary institutions of higher learning in British America's Northern and Middle Colonies. During the colonial era, the schools' faculties and founding boards were largely drawn from other Ivy League institutions. Also represented were British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, and the University of Edinburgh.\nThe influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities of their respective states. In 1801, a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California came from Yale, hence Berkeley's colors are Yale Blue and California Gold. Stanford University has, since its earliest days, been nicknamed the \"Cornell of the West\": more than half of Stanford's initial faculty, as well as its first two presidents, had connections to Cornell as alumni or faculty. Samuel Jones, the Baptist minister from Philadelphia who rewrote Brown's original charter (itself written by future Yale College president Ezra Stiles) was a graduate of the College of Philadelphia.\nA plurality of the Ivy League schools have identifiable Protestant roots. Harvard, Yale, and Dartmouth all held early associations with the Congregationalists. Princeton was financed by New Light Presbyterians, though originally led by a Congregationalist. Brown was founded by Baptists, though the university's charter stipulated that students should enjoy \"full liberty of conscience.\" Columbia was founded by Anglicans, who composed 10 of the college's first 15 presidents. Penn and Cornell were officially nonsectarian, though Protestants were well represented in their respective founding. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and religious traditions including compulsory chapel often lasted well into the twentieth century.\n\"Ivy League\" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.\nHistory of the athletic league.\n19th century.\nIn 1870, the nation's first formal athletic league was created in 1870 with the formation of the Rowing Association of American Colleges (RAAC), composed exclusively of Ivy League universities. RAAC hosted a national championship in rowing from 1870 to 1894.\nThe first Harvard vs Yale rugby football contest was held in 1875, two years after the inaugural Princeton\u2013Yale rugby football contest. Harvard athlete Nathaniel Curtis challenged Yale's captain, William Arnold to a rugby-style game.\nProgram for the \"Foot Ball Match\", Harvard v Yale, the first intercollegiate game. It is considered the first rugby game between Ivy League teams. The game was played at Hamilton Park, a venue in New Haven, Connecticut (located at the intersection of Whalley Avenue and West Park Avenue). The two teams played with 15 players (rugby) on a side instead of 11 (soccer) as Yale would have preferred.\nIn 1881, Penn, Harvard College, Haverford College, Princeton University (then known as College of New Jersey), and Columbia University (then known as Columbia College) formed The Intercollegiate Cricket Association, which Cornell University later joined. Penn won The Intercollegiate Cricket Association championship 23 times, including 18 solo victories and three shared with Haverford and Harvard, one shared with Haverford and Cornell, and one shared with just Haverford, during the 44 years that the Intercollegiate Cricket Association existed from 1881 through 1924.\nIn 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete.\nA basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale, and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.\n20th century.\nIn 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies. In February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie.\nTwo years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.\nThough schools now in Ivy League (such as Yale and Columbia) played against each other in the 1880s, it was not until 1930 that Columbia, Cornell, Dartmouth, Penn, Princeton and Yale formed the Eastern Intercollegiate Baseball League; they were later joined by Harvard, Brown, Army and Navy. Before the formal establishment of the Ivy League, there was an \"unwritten and unspoken agreement among certain Eastern colleges on athletic relations\". The earliest reference to the \"Ivy colleges\" came in 1933, when Stanley Woodward of the \"New York Herald Tribune\" used it to refer to the eight current members plus Army. In 1935, the Associated Press reported on an example of collaboration between the schools:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The athletic authorities of the so-called \"Ivy League\" are considering drastic measures to curb the increasing tendency toward riotous attacks on goal posts and other encroachments by spectators on playing fields.\u2014\u200a\nDespite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I can say with certainty that in the last five years\u2014and markedly in the last three months\u2014there has been a strong drift among the eight or ten universities of the East which see a good deal of one another in sport toward a closer bond of confidence and cooperation and toward the formation of a common front against the threat of a breakdown in the ideals of amateur sport in the interests of supposed expediency.\nPlease do not regard that statement as implying the organization of an Eastern conference or even a poetic \"Ivy League\". That sort of thing does not seem to be in the cards at the moment.\nWithin a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of \"the formation of an Ivy League\" gained enough traction among the undergraduate bodies of the universities that the \"Columbia Daily Spectator\", \"The Cornell Daily Sun\", \"The Dartmouth\", \"The Harvard Crimson\", \"The Daily Pennsylvanian\", \"The Daily Princetonian\" and the \"Yale Daily News\" would simultaneously run an editorial entitled \"Now Is the Time\", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Ivy League exists already in the minds of a good many of those connected with football, and we fail to see why the seven schools concerned should be satisfied to let it exist as a purely nebulous entity where there are so many practical benefits which would be possible under definite organized association. The seven colleges involved fall naturally together by reason of their common interests and similar general standards and by dint of their established national reputation they are in a particularly advantageous position to assume leadership for the preservation of the ideals of intercollegiate athletics.\nThe Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, \"The Oneida\", won the race and was presented with trophy black walnut oars from then-presidential nominee General Franklin Pierce. The proposal to create an athletic league did not succeed. On January 11, 1937, the athletic authorities at the schools rejected the \"possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track.\" However, they noted that the league \"has such promising possibilities that it may not be dismissed and must be the subject of further consideration.\"\nIntegration of athletic competition in the \"Ivy League\".\nThe integration of athletics followed a similar pattern to the overall integration of the Ivy League's in the 19th and early 20th century. There was no active policy that would discriminate against incorporating Black student athletes into the athletic coalition. Harvard has the earliest record of breaking the color barrier in athletics after recruiting William Henry Lewis to their football team in 1892. Dartmouth followed suit, with Black athletes integrating onto their football teams in 1904. Brown integrated their football team shortly after, in 1916. Cornell would follow suit in 1937.\nPenn had black students on their track and field team as early as 1903 (John Baxter Taylor, Jr., the first black athlete in the U.S. to win a gold medal in the Olympics) and a black student was named captain of the track team in 1918. Columbia's track and field team would be integrated in 1934. Basketball would become integrated at Yale in 1926, at Princeton in 1947.\nPost-World War II.\nIn 1945 the presidents of the eight schools signed the first \"Ivy Group Agreement\", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The members of the Group reaffirm their prohibition of athletic scholarships. Athletes shall be admitted as students and awarded financial aid only on the basis of the same academic standards and economic need as are applied to all other students.\nIn 1954, the presidents extended the Ivy Group Agreement to all intercollegiate sports, effective with the 1955\u201356 basketball season. This is generally reckoned as the formal formation of the Ivy League. As part of the transition, Brown, the only Ivy that had not joined the EIBL, did so for the 1954\u201355 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.\nAs late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie \"Animal House\" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, \"The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men's colleges.\"\nIn 1982 the Ivy League considered adding two members, with Army, Navy, and Northwestern as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.When Army and Navy departed the Eastern Intercollegiate Baseball League in 1992, nearly all intercollegiate competition involving the eight schools became united under the Ivy League banner. The major exception is hockey, with the Ivies that sponsor hockey\u2014all except Penn and Columbia\u2014members of ECAC Hockey. Wrestling was a second exception through the 2023-24 academic calendar; up until that point the Ivies that sponsor wrestling\u2014all except Dartmouth and Yale\u2014 were members of the Eastern Intercollegiate Wrestling Association.\nThe Ivy League was the first athletic conference to respond to the COVID-19 pandemic by shutting down all athletic competition in March 2020, leaving many Spring schedules unfinished. The Fall 2020 schedule was canceled in July, and winter sports were canceled before Thanksgiving. Of the 357 men's basketball teams in Division I, only ten did not play; the Ivy League made up eight of those ten. By giving up its automatic qualifying bid to March Madness, the Ivy League forfeited at least $280,000 in NCAA basketball funds. As a consequence of the pandemic, an unprecedented number of student athletes in the Ivy League either transferred to other schools, or temporarily unenrolled in hopes of maintaining their eligibility to play post-pandemic. Some Ivy alumni expressed displeasure with the League's position. In February 2021 it was reported that Yale declined a multi-million dollar offer from alum Joseph Tsai to create a sequestered \"bubble\" for the lacrosse team. The league announced in a May 2021 joint statement that \"regular athletic competition\" would resume \"across all sports\" in fall 2021.\nFollowing the Black Lives Matter protests in 2020, the Ivy League Conference committed itself to uphold \"diversity, equity, and inclusion,\" to combat racism and homophobia. At Brown, Columbia, Cornell, Dartmouth, Harvard, and Princeton there are Black Student Athlete groups and other affinity groups that are dedicated to ensuring their organizations are committed to anti-racism and anti-homophobia. In 2023, two former Brown University basketball players sued the Ivy League alleging that by denying athletic scholarships, the 1954 \"Ivy League Agreement\" is anticompetititive and violates antitrust laws. The lawsuit claims that the agreement constitutes price-fixing in violation of the Sherman Antitrust Act of 1890, and in effect raises the cost of Ivy League education for student athletes.\nAcademics.\nUndergraduate admissions.\nThe Ivy League schools are highly selective, with seven out of the eight universities reporting undergraduate acceptance rates below 6%. Admitted students come from around the world, although those from the Northeastern United States make up a significant proportion of students.\nIn 2021, all eight Ivy League schools recorded record high numbers of applications and record low acceptance rates. Year-over-year increases in the number of applicants ranged from 14.5% at Princeton to 51% at Columbia.\nThere have been arguments that Ivy League schools discriminate against Asian-American candidates. For example, in August 2020, the U.S. Justice Department argued that Yale University discriminated against Asian-American candidates on the basis of their race, a charge the university denied. Harvard faced a similar challenge from Students for Fair Admissions, which ultimately won its case in the U.S. Supreme Court in 2023, leading to the end of affirmative action in college admissions.\nPrestige.\nMembers of the League have been highly ranked by various university rankings. All of the Ivy League schools are consistently ranked within the top 20 national universities by the \"U.S. News &amp; World Report\" Best Colleges Ranking.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nCollaboration.\nCollaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group presidents, composed of each university president. During meetings, the presidents discuss common procedures and initiatives for their universities.\nThe universities collaborate academically through the IvyPlus Exchange Scholar Program, which allows students to cross-register at one of the Ivies or another eligible school such as Berkeley, Chicago, MIT, and Stanford.\nHistory of diversity.\nRacial segregation and integration.\nIvy League institutions have a complex history of racial segregation, and, eventually, integration. All of the universities in the Ivy League besides Cornell University were chartered during the American era of slavery. In 2003, Brown University was the first of the Ivies to take accountability for their historic ties to slavery and the . Following Brown, other Ivy League universities formed committees to examine their ties to slavery, and found various institutional relationships to slavery. Yale University, for example, used profits from slave traders and owners to fund its first scholarships, libraries, and faculty positions. To date, some of Yale's residential colleges are named after slave traders and supporters. The investigations at Harvard, Princeton, Columbia, and the University of Pennsylvania all found that, in the century following their charters, enslaved Black people lived on campus to care for students, professors, or the universities' presidents. Notably, Princeton's first nine presidents were slave owners, and in 1766, a slave auction reportedly took place on Princeton's campus.\nA small number of Black people did attend Ivy League institutions as students during their early years. These early students, however, were not always granted degrees. For example, some Black students were recorded studying privately with the Princeton University president as early as 1774, but no Black students received Princeton degrees until the middle of the twentieth century. Jonathan and Philip Gayienquitioga, two brothers of the Mohawk People, were the first people of color to enroll at Penn in 1755 after being recruited by Benjamin Franklin to attend the Academy of Philadelphia (then part of Penn). But there is no evidence that either earned a degree as the first Native American to graduate Penn did not occur until 1847 when Robert Daniel Ross, a member of the Cherokee Nation, graduated with a degree from Penn's medical school.\n19th and early 20th centuries.\nIn 1900, W. E. B. Du Bois oversaw and edited \"The College-bred Negro\". a study on Black integration in colleges and universities that found a combined total of 52 Black students had graduated from Ivy League schools in their collective histories. Since no official policies prohibited schools in the Ivy League from admitting students of color each university in the League had different policies regarding the admission of Black students. Dartmouth's first Black student graduated in 1828, while Princeton would only admit their first Black student under the V-12 Navy College Training Program in the 1940s.\nEarly Black student admits to Ivy League universities were controversial and often faced backlash. Dartmouth initially denied its first Black graduate, Edward Mitchell, supposedly to avoid \"offend[ing] students\". Dartmouth students protested this decision, leading to Mitchell's admission in 1824. Richard Henry Green was awarded an MD degree by Dartmouth College in 1864.\nHarvard admitted its first Black student, Beverly Garnett Williams, in 1847. News of his admission incited protests by Harvard students and faculty. Williams died before the academic year began, however, and never matriculated. Richard Theodore Greener was the first African American to receive a Harvard degree in 1870. Between 1890 and 1940, an average of three Black men enrolled at Harvard per year. In 1923, Harvard's Board of Overseers overruled University President Abbot Lawrence's ban on Black students living in dorms, announcing that all freshmen would be permitted to live in dorms regardless of race, but upheld that \u201cmen of the white and colored races shall not be compelled to live and eat together.\" Brown seems to have refused admission to Black students outright prior to the Civil War. Abolitionist Elizabeth Buffum Chase wrote in her book \"Anti Slavery Reminiscences\" about \"a lad of rare excellence and attainments [who] was refused an examination for admission by the authorities of Brown University on account of the color of his skin.\" Inman Page was the first Black student to graduate from Brown in 1877, and was class speaker.\nWilliam Adger, James Brister, and Nathan Francis Mossell were the first Black students enrolled at Penn in 1879. Brister graduated from the School of Dental Medicine (Penn Dental) in 1881 as the first African American to earn a degree from Penn, while Adger was the first African American to graduate from the college in 1883.\nColumbia University has claimed that four Black students earned University degrees between 1875 and 1900, though their names are apparently unknown.\nYale's Edward Bouchet, was the first Black person (a) elected to Phi Beta Kappa in the US in 1874 and (b) to earn a Ph.D. from any American university, completing his dissertation in physics in 1876. Bouchet was thought to have been the first African-American graduate of Yale, but research publicized in 2014 reported that Yale awarded a Black man, Richard Henry Green, a bachelor of arts degree in 1857.\nCornell seemed the most inclusive of the Ivy Leagues at its inception, with admission open to any race and gender. University co-founder Andrew Dickson White wrote in 1874 that the school had \"\"\"no colored students...at present but shall be very glad to receive any who are prepared to enter...if even one offered himself and passed the examinations, we should receive him even if all our five hundred white students were to ask for dismissal on that account.\" In 1890, Charles Chauveau Cook and Jane Eleanor Datcher were the first Black students awarded four-year undergraduate Cornell degrees. Despite this, Black students faced legal and social segregation in the town of Ithaca, New York. In 1905, Black students reported being denied housing while attending Cornell.\nPrinceton University, sometimes referred to as the \"Southern-most Ivy\", was the last to integrate. In Du Bois' \"The College-bred Negro\" (1900)\",\" a Princeton representative is quoted: \"We have never had any colored students here, though there is nothing in the University statutes to prevent their admission. It is possible, however, in view of our proximity to the South and the large number of southern students here, that Negro students would find Princeton less comfortable than some other institutions.\" Notably, in 1939, Princeton revoked admittance to Black student Bruce Wright upon his arrival on campus, when Director of Admission Radcliffe Heermance noticed Wright's race. When a disappointed Wright wrote Heermance requesting an explanation, Heermance responded:\"I cannot conscientiously advise a colored student to apply for admission to Princeton simply because I do not think that he would be happy in this environment. There are no colored students in the University and a member of your race might feel very much alone...My personal experience would enforce my advice to any colored student that he would be happier in an environment of others of his race, and that he would adjust himself far more easily to the life of a New England college or university, or one of the large state universities than he would to a residential college of this particular type.\"The few early Black students admitted to Ivy League universities were often from wealthy Caribbean families. Barriers preventing African American students from attending Ivy League universities included the universities' policies, poor recruitment, tuition costs, and the lack of secondary education opportunities in a racially segregated country. More Black students attended Ivy League graduate and professional schools than their undergraduate programs. By the middle of the 20th century, only 54 Black men and women had graduated with a Bachelor degree from Ivy League universities.\nLate 20th century.\nBy the middle of the 20th century, some Ivy League students and alumni were advocating for increased racial integration efforts. These efforts were met with mixed reactions from the schools themselves. Without a goal for integration shared by the institutions as a collective, each school increased racial diversity at different rates, with Dartmouth having 120 Black undergraduates in the class of 1945 and Princeton having a cumulative total of fewer than 100 Black undergraduates by 1967.\nThe V-12 Navy College Training Program in 1942 effectively forced all eight Ivy institutions to increase Black student enrollment. At Princeton University, the Black students in this program were the first ever granted bachelor's degrees by the University.\nThe 1954 Supreme Court decision in \"Brown v. Board of Education\" did not require private universities like those in the Ivy League to abide by the ruling. It wasn't until the Court's 1976 decision in \"Runyon v. McCrary\" that private institutions became legally prohibited from discriminating on the basis of race. By the early 1960s, however, some admissions offices in the Ivy League began to make concerted efforts to increase their number of Black applicants, rolling out initiatives that actively sought Black talent from high schools. Efforts for racial integration at Ivy League institutions relied on the support of student organizations, faculty-led initiatives, and third-party organizations like the National Scholarship Service and Fund for Negro Students to seek prospective Black applicants. These efforts also prompted internal University action, such as the creation of Cornell's Committee on Special Educational Projects (COSEP), an organization aimed to recruit and support Black students. By 1965, however, Black students still were only 2% of admitted students across all the Ivies.\nPrior to the 1960s, the majority of Ivy League universities explicitly prohibited the admission of women, instead forming partnerships with nearby women's colleges. As such, Black women were not able to attend Ivy League universities until they changed their policies. Lillian Lincoln Lambert was the first Black woman to receive a degree from Harvard University after graduating with a master's degree from Harvard Business School in 1969. Lincoln Lambert was also a founding member of Harvard's African American Student Union, which according to her, actively recruited Black students and created \"a space where Black students could find not only support but resources for everything from barber shops that cut Black hair to churches.\"\nAs Black student populations grew at Ivy League schools, on-campus activism saw an increase during the civil rights movement. In 1969, students in Cornell's Afro-American Society led an armed occupation of Willard Straight Hall to protest the university's racist policies and \u201cits slow progress in establishing a Black studies program.\u201d In the same year, students associated with Yale's New Left organization, Students for a Democratic Society, worked closely with the New Haven Black Panthers to lead sit-ins and protests that advocated for the admission of more students of color and the establishment of an African American studies department. At Brown University, identity-based student organizations such as the United African People and the African American Society called for an increase to the number of Black faculty and increased attention to the needs of Black students. Demonstrations at Harvard and Columbia took the form of occupations and non-violent sit-ins that were often subject to forceful removal by local police called by University administrators. Activism at Dartmouth took a different shape during this time period, as students would use demonstrations that were happening at other Ivies and colleges around the country, to effectively position their demands for progress within the prospect of taking actions similar to those happening elsewhere.\n21st century.\nContinuing the trajectory of the late 20th century, the number of Black students on Ivy League campuses has continued to increase in the 21st century. From 2006 to 2018, there was an approximated 50% increase in the admission of Black students into entering classes, growing from 1,110 to 1,663. As of 2018, the Ivy League universities unanimously supported Harvard University's \u201crace-conscious admissions\u201d model. Harvard University representatives credited this form of affirmative action as one of the factors increasing campus diversity.\nIn 2014 case \"Schuette v. Coalition to Defend Affirmative Action\", 572 U.S. https:// (2014) \u2014 the Supreme Court upheld Michigan's ban on affirmative action for public institutions and in 2016 in\"Fisher v. University of Texas II\", No. https://, 579 U.S. ___ (2016) the court upheld the university's limited use of race in admissions decisions because the university showed it had a clear goal of limited scope without other workable race-neutral means to achieve it.\nHowever, in 2023 \u2014 \"Students for Fair Admissions v. President and Fellows of Harvard College\", No. https://, 600 U.S. ___ (2023) the United States Supreme Court overruled the decades old decisions\"Regents of University of California v. Bakke\" and \"Grutter v. Bollinger\" and other cases mentioned above in this paragraph but disallowing non-individualized racial preferences in admissions for civilian universities. \nIn essence, the court interpreted the Fourteenth Amendment as not permitting Harvard's \u201crace-conscious admissions\u201d as the court decision now forbids the consideration of race in higher education admissions.\nInstitutions in favor of Harvard's model argue that in addition to academic excellence they also aim to form a diverse student body, while individuals that argue against the model state that it is discriminatory against certain applicants.\nThe growing Black student population in Ivy League universities in the early 2000s was accompanied by an increase in the number of Black faculty at these institutions, though rates of change among faculty have been slower and inconsistent. In 2005, 588\u2013 or about 3.9%\u2013 of the Ivies' 14,831 full-time faculty members were Black. This proportion decreased to 3.4% in 2015. Notably, in 2001, Ruth J. Simmons became the president of Brown University, making her the first and only Black president of an Ivy League institution.\nThe 21st century saw the continuation of demonstrations by Ivy League students revolving around race. Many of these demonstrations have sought to continue the work of their 20th century predecessors by advocating for increased admission and support of Black students. In light of the \"Students for Fair Admissions v. President and Fellows of Harvard College\" Supreme Court case, students from Yale and Harvard joined other universities in protesting in defense of race-conscious admissions policies.\nLikewise, Black students from Ivy League institutions continue to protest for the betterment of Black students' lives on campus and beyond. Following Michael Brown's death in 2014, students across the Ivies formed the Black Ivy Coalition, which included members from all eight institutions and aimed to combat anti-Black racism. Individual Ivy League universities also formed their own advocacy organizations and movements as a direct response to instances of anti-Black violence. After the murder of Michael Brown, Princeton University students formed the Black Justice League, which in 2015, occupied Nassau Hall and presented a list of demands to university administrators. Similarly, in 2017, Cornell students made demands to their administration protesting the assault of a Black student. Led by Black Students United, the demands included banning the Psi Upsilon fraternity for hate crimes, implementing implicit bias training, and introducing policies to increase the number of Black students at the university.\nStudent demonstrations have also focused on sparking change beyond Ivy League campuses. Following the Black Lives Matter protests in 2020, Harvard's Black Law Students Association, beyond calling for more Black faculty, critical race theory curriculum, and protection for student protestors, also called on the university to divest from prisons and denounce state-sanctioned violence.\nIn response to racially charged incidents across the country and prompting from student activists, Ivy League universities have removed and renamed campus landmarks. In response to the 2016 Black Lives Matter protests, Cornell renamed their botanical gardens, previously called the \"Cornell Plantations,\" to the \"Cornell Botanical Gardens.\" In 2018, Brown renamed one of its largest academic and administrative buildings after its first black graduates, Inman E. Page and Ethel Tremaine Robinson. In response to the murder of George Floyd in 2020, Princeton University removed Woodrow Wilson's name from a residential college and the School of Public and International Affairs because of his \u201cracist thinking and policies.\u201d\nFashion and lifestyle.\nDifferent fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and preppy are styles often associated with the Ivy League and its culture.\nIvy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.\nPreppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Yale, and Princeton.\nSome typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colors, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colors combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney &amp; Bourke became associated with preppy style.\nThough the Ivy League style is most commonly associated with the white, male elites that historically made up Ivy League campuses, the style was quickly popularized among Black communities during the civil rights era. Reinterpretations of this style by African-American men in the 1950s and 1960s combined the preppy Ivy League style with other popular Black styles of dress. This led to the emergence of a new style of dress, the Black Ivy style.\nToday, Ivy League styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as \"Classic American style\" or \"Traditional American style\".\nSocial elitism.\nThe Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper-middle and upper-class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.\nPhrases such as \"Ivy League snobbery\" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads \"the aridity of snobbery which he knew infected the Ivy League colleges\". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe phrase \"Ivy League\" historically has been perceived as connected not only with academic excellence but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Columbia, Princeton, Cornell, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider \"Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt\" as candidates for membership, he exhorted:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It would be well for the proponents of the Ivy League to make it clear (to themselves especially) that the proposed group would be inclusive but not \"exclusive\" as this term is used with a slight up-tilting of the tip of the nose.\nAspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having \"foreign-policy views born in Harvard Yard's boutique.\" \"New York Times\" columnist Maureen Dowd asked \"Wasn't this a case of the pot calling the kettle elite?\" Bush explained, however, that, unlike Harvard, Yale's reputation was \"so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it. ... Harvard boutique to me has the connotation of liberalism and elitism\" and said \"Harvard\" in his remark was intended to represent \"a philosophical enclave\" and not a statement about class. Columnist Russell Baker opined that \"Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets.\" Still, the next five consecutive presidents all attended Ivy League schools for at least part of their education\u2014George H. W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), Barack Obama (Columbia undergrad, Harvard Law School), and Donald Trump (Penn undergrad). Indeed, since 1989, Joe Biden has been the only president to \"not\" be Ivy League-educated.\nU.S. presidents in the Ivy League.\nOf the 45 persons who have served as President of the United States, 16 have graduated from an Ivy League university with either a Bachelor's or advanced degree. Of them, eight have degrees from Harvard, five from Yale, three from Columbia, two from Princeton and one from Penn. Twelve presidents have earned Ivy undergraduate degrees. Four of these were transfer students: Woodrow Wilson transferred from Davidson College, Barack Obama transferred from Occidental College, Donald Trump transferred from Fordham University, and John F. Kennedy transferred from Princeton to Harvard. John Adams was the first president to graduate from college, graduating from Harvard in 1755.\nStudent demographics.\nGeographic distribution.\nStudents of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.\nSocioeconomics and social class.\nStudents of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and lower middle class American families.\nIn 2013, a Harvard Crimson writer estimated that 46% of Harvard undergraduate students came from families in the top 3.8% of all American households (i.e., over $200,000 annual income). In 2012, the bottom 25% of the American income distribution accounted for only 3\u20134% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper-middle and upper classes. (The median household income in the U.S. in 2013 was $52,700.)\nIn the 2011\u20132012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) constituted 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.\nCompetition and athletics.\nIvy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. In one sport, rowing, the Ivies recognize team champions for each sex in both heavyweight and lightweight divisions. While the Intercollegiate Rowing Association governs all four sex- and bodyweight-based divisions of rowing, the only one that is sanctioned by the NCAA is women's heavyweight. The Ivy League was the last Division I basketball conference to institute a conference postseason tournament; the first tournaments for men and women were held at the end of the 2016\u201317 season. The tournaments only award the Ivy League automatic bids for the NCAA Division\u00a0I Men's and Women's Basketball Tournaments; the official conference championships continue to be awarded based solely on regular-season results. Before the 2016\u201317 season, the automatic bids were based solely on regular-season record, with a one-game playoff (or series of one-game playoffs if more than two teams were tied) held to determine the automatic bid. The Ivy League is one of only two Division I conferences which award their official basketball championships solely on regular-season results; the other is the Southeastern Conference. Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA basketball tournament.\nOn average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools. Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). In addition, the Ivies have a rigid policy against redshirting, even for medical reasons; an athlete loses a year of eligibility for every year enrolled at an Ivy institution. Additionally, the Ivies prohibit graduate students from participating in intercollegiate athletics, even if they have remaining athletic eligibility. The only exception to the ban on graduate students was that seniors graduating in 2021 were allowed to play at their current institutions as graduate students in 2021\u201322. This was a one-time-only response to the Ivies shutting down most intercollegiate athletics in 2020\u201321 due to COVID-19. Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies (although unlike the Ivies, the Patriot League allows both redshirting and play by eligible graduate students). To promote diversity and inclusion, student-athletes are required to have their gender pronouns listed on their roster pages on the athletic websites for most Ivy League schools.\nIn the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 15, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the \"Father of American Football,\" held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 18, followed closely by Harvard and Penn, each with 17 titles. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and Justin Watson (wide receiver), (three-time Super Bowl champion, winning Super Bowl LV with the Tampa Bay Buccaneers and Super Bowl LVII and LVIII with the Kansas City Chiefs), (Penn).\nFCS Championship.\nBeginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, from its inception in 1956 until 2024, the Ivy League had not played any postseason games due to concerns about the extended December schedule's effects on academics. (The last postseason game for a member was , the 1934 Rose Bowl, won by For this reason, any Ivy League team invited to the FCS playoffs turned down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football had been the only sport in which the Ivy League declined to compete for a national title. However, beginning in 2025, the Ivy League will participate in the FCS playoffs, with its conference champion automatically qualifying for the tournament.\nSprint football.\nIn addition to varsity football, Penn and Cornell also field teams in the 9-team Collegiate Sprint Football League, in which all players must weigh 178 pounds or less. With Princeton canceling its program in 2016, Penn is the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.\nMen's sponsored sports by school.\nMen's varsity sports not sponsored by the Ivy League.\nNotes:\n1: Though the Ivy League lists ice hockey as a sponsored sport, all six ice hockey playing Ivy League schools participate as members of ECAC Hockey.\nWomen's sponsored sports by school.\nWomen's varsity sports not sponsored by the Ivy League.\nNotes:\n1: Though the Ivy League lists ice hockey as a sponsored sport, all six ice hockey playing Ivy League schools participate as members of ECAC Hockey.\n2. The Ivy League is home to some of the oldest college rugby teams in the United States. Although none of the men's teams and half of the women's teams are not \"varsity\" sports, they all compete against each other as part of the Ivy Rugby Conference in addition to their own local conferences. Four of the women's teams (Brown, Dartmouth, Harvard, and Princeton) play as part of the NCAA emerging sport category.\nHistorical results.\nThe table above includes the number of team championships won from the beginning of official Ivy League competition (1956\u201357 academic year) through 2016\u201317. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished 10 times by Harvard and 24 times by Princeton, including a conference-record 15 championships in 2010\u201311. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005\u201306). In the 38 academic years beginning 1979\u201380, Princeton has averaged 10 championships per year, one-third of the conference total of 33 sponsored sports.\nIn the 12 academic years beginning 2005\u201306 Princeton has won championships in 31 different sports, all except wrestling and men's tennis.\nRivalries.\nRivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; \"Puck Frinceton\" T-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion).\nHarvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014. Harvard also won the 2013 Great Alaska Shootout, defeating TCU to become the only Ivy League school to win the now-defunct tournament.\nRivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have won 28 Ivy League Football Championships since 1982, Penn-16; Harvard-12). During that time Penn has had 8 undefeated Ivy League Football Championships and Harvard has had 6 undefeated Ivy League Football Championships. In men's lacrosse, Cornell and Princeton are perennial rivals, and they are two of three Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002\u201303, with one exception (Columbia women won the indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 15 football games and all but one of the last 13 crew races.\nIntra-conference football rivalries.\nThe Yale\u2013Princeton series is the nation's second-longest by games played, surpassed only by \"The Rivalry\" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional team sports were fledgling baseball leagues, these high-profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.\nChampionships.\nNCAA team championships.\nThis list, which is current through January 8, 2018, includes NCAA championships and women's AIAW championships (one each for Yale and Dartmouth and five for Cornell). Excluded from this list are all other national championships earned outside the scope of NCAA competition, including football titles and retroactive Helms Foundation titles.\nOther Ivies.\nThe term \"Ivy\" is sometimes used to connote a positive comparison to or an association with the Ivy League, often along academic lines. The term has been used to describe the Little Ivies, a grouping of small liberal arts colleges in the Northeastern United States. Other common uses include the Public Ivies, the Hidden Ivies, the Southern Ivies, and the Black Ivies.\nIvyPlus.\nThe informal term \"IvyPlus\" refers to the original eight Ivy league institutions along with four other institutions including the Massachusetts Institute of Technology, Stanford University, Duke University, and University of Chicago. Johns Hopkins University is sometimes included as well. Beyond rankings and prestige, the four schools are included in the grouping given their formal participation in academic exchange programs, university consortia, shared academic resources, collaborative alumni associations, or endowment comparisons.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14976", "revid": "14614106", "url": "https://en.wikipedia.org/wiki?curid=14976", "title": "Ithaca Hours", "text": "Local currency\nThe Ithaca HOUR was a local currency used in Ithaca, New York; now no longer in circulation. It was one of the longest-running local currency systems, and inspired other similar systems in Madison, Wisconsin; Santa Barbara, California;\nCorvallis, Oregon; and a proposed system in the Lehigh Valley, Pennsylvania. One Ithaca HOUR was valued at US$10 and was generally recommended to be used as payment for one hour's work, although the rate is negotiable.\nThe currency.\nIthaca HOURS were not backed by national currency and cannot be freely converted to national currency, although some businesses did agree to buy them.\nHOURS were printed on high-quality paper and used faint graphics that would be difficult to reproduce. Each bill was stamped with a serial number, to discourage counterfeiting.\nIn 2002, a one-tenth hour bill was introduced, partly due to the encouragement and funding from Alternatives Federal Credit Union and feedback from retailers who complained about the awkwardness of only having larger denominations with which to work; the bills bear the signatures of both HOURS president Steve Burke and the president of AFCU.\nIthaca HOUR notes began to fall into disuse for several reasons. First, the founder of the system, Paul Glover, moved out of the area. While in Ithaca, Glover had acted as an evangelist and networker for HOURS, helping spread their use and helping businesses find ways to spend HOURS they had received. Secondly, the use of HOURS declined as a result of the general shift away from cash transactions towards electronic transfers with debit or credit cards. Glover emphasized that every local currency needs at least one full-time networker to \"promote, facilitate and troubleshoot\" currency circulation.\nOrigin.\nIthaca HOURS were started by Paul Glover in November 1991. The system has historical roots in scrip and alternative and local currencies that proliferated in America during the Great Depression.\nWhile doing research into local economics during 1989, Glover had seen an \"Hour\" note issued by 19th century British industrialist Robert Owen to his workers for spending at his company store. After Ithaca HOURS began, Glover discovered that Owen's Hours were based on Josiah Warren's \"Time Store\" notes of 1827.\nIn May 1991, local student Patrice Jennings interviewed Glover about the Ithaca LETS enterprise. This conversation strongly reinforced his interest in trade systems. Jennings's research on the Ithaca LETS and its failure was integral to the development of the HOUR currency; conversations between Jennings and Glover helped ensure that HOURS used knowledge of what had not worked with the LETS system.\nWithin a few days, Glover had designs for the HOUR and Half HOUR notes. He established that each HOUR would be worth the equivalent of $10, which was about the average hourly amount that workers earned in surrounding Tompkins County, although the exact rate of exchange for any given transaction was to be decided by the parties themselves. At GreenStar Cooperative Market, a local food co-op, Glover approached Gary Fine, a local massage therapist, with photocopied samples. Fine became the first person to sign a list formally agreeing to accept HOURS in exchange for services. Soon after, Jim Rohrrsen, the proprietor of a local toy store, became the first retailer to sign-up to accept Ithaca HOURS in exchange for merchandise.\nWhen the system was first started, 90 people agreed to accept HOURS as pay for their services. They all agreed to accept HOURS despite the lack of a business plan or guarantee. Glover then began to ask for small donations to help pay for printing HOURS.\nFine Line Printing completed the first run of 1,500 HOURS and 1,500 Half HOURS in October 1991. These notes, the first modern local currency, were nearly twice as large as later printings of Ithaca HOURS. Because they didn't fit well in people's wallets, almost all of the original notes have been removed from circulation.\nThe first issue of Ithaca Money was printed at Our Press, a printing shop in Chenango Bridge, New York, on October 16, 1991. The next day Glover issued 10 HOURS to Ithaca Hours, the organization he founded to run the system, as the first of four reimbursements for the cost of printing HOURS. The day after that, October 18, 1991, 382 HOURS were disbursed and prepared for mailing to the first 93 pioneers.\nOn October 19, 1991, Glover bought a samosa from Catherine Martinez at the Farmers' Market with Half HOUR #751\u2014the first use of an HOUR. Several other Market vendors enrolled that day. During the next years more than a thousand individuals enrolled to accept HOURS, plus 500 businesses.\nStacks of the Ithaca Money newspaper were distributed all over town with an invitation to \"join the fun.\"\nA Barter Potluck was held at GIAC on November 12, 1991, the first of many monthly gatherings where food and skills were exchanged, acquaintances made, and friendships renewed.\nManagement and philosophy.\nIn 1996, Glover was running the Ithaca Hours system from his home, and the system had an advisory board and a governing board called the \"Barter Potluck\". The board and Glover put forth the idea that economic interactions should be based on harmony rather than on more Hobbesian forms of competition. In one interview, Glover stated that \"There's a growing movement called \"ecological economics\" and Ithaca HOURS is part of that cosmos. Last year I wrote an article which discusses moving us toward the provision of food, fuel, clothing, housing, transportation, [and other] necessities in ways which are healing of nature, or which are less depleting at least and which bring people together on the basis of their shared pride, not arrogance.\" Thus one underlying principle of the local currency movement is to create \"fair trade\" with a minimum of conflict or exploitation of either people or natural resources.\nThe advisory board incorporated the Ithaca HOUR system as Ithaca Hours, Inc. in October 1998, and hosted the first elections for Board of Directors in March 1999. In May 1999 Glover turned the administration of Ithaca HOURS over to the newly elected Board of Directors. Glover has continued to support Ithaca Hours through community outreach to present, most notably through the Ithaca Health Fund (now incorporated as part\nof the Ithaca Health Alliance) and Ithaca Community News.\nEconomic development.\nSeveral million dollars value of HOURS have been traded since 1991 among thousands of residents and over 500 area businesses, including the Cayuga Medical Center, Alternatives Federal Credit Union, the public library, many local farmers, movie theatres, restaurants, healers, plumbers, carpenters, electricians, and landlords.\nOne of the primary functions of the Ithaca Hours system is to promote local economic development. Businesses who receive Hours must spend them on local goods and services, thus building a network of inter-supporting local businesses. While non-local businesses are welcome to accept Hours, those businesses need to spend them on local goods and services to be economically sustainable.\nIn their mission to promote local economic development, the Board of Directors also makes interest-free loans of Ithaca HOURS to local businesses and grants to local non-profit organizations.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14978", "revid": "664855", "url": "https://en.wikipedia.org/wiki?curid=14978", "title": "Immortal Seven", "text": ""}
{"id": "14979", "revid": "29482141", "url": "https://en.wikipedia.org/wiki?curid=14979", "title": "Interstellar cloud", "text": "Accumulation of gas, plasma, and dust in space\nAn interstellar cloud is an accumulation of gas, plasma, and cosmic dust in galaxies. Put differently, an interstellar cloud is a denser-than-average region of the interstellar medium, the matter and radiation that exists in the space between the star systems in a galaxy. Depending on the density, size, and temperature of a given cloud, its hydrogen can be neutral, making an H I region; ionized, or plasma making it an H II region; or molecular, which are referred to simply as molecular clouds, or sometime dense clouds. Neutral and ionized clouds are sometimes also called \"diffuse clouds\". An interstellar cloud is formed by the gas and dust particles from a red giant in its later life.\nChemical compositions.\nThe chemical composition of interstellar clouds is determined by studying electromagnetic radiation that they emanate, and we receive \u2013 from radio waves through visible light, across the electromagnetic spectrum to gamma rays. Large radio telescopes scan the intensity in the sky of particular frequencies of electromagnetic radiation, which are characteristic of certain molecules' spectra. Some interstellar clouds are cold and tend to give out electromagnetic radiation of large wavelengths. A map of the abundance of these molecules can be made, enabling an understanding of the varying composition of the clouds. In hot clouds, there are often ions of many elements, whose spectra can be seen in visible and ultraviolet light.\nRadio telescopes can also scan over the frequencies from one point in the map, recording the intensities of each type of molecule. Peaks of frequencies mean that an abundance of that molecule or atom is present in the cloud. The height of the peak is proportional to the relative percentage that it makes up.\nUnexpected chemicals detected in interstellar clouds.\nUntil recently, the rates of reactions in interstellar clouds were expected to be very slow, with minimal products being produced due to the low temperature and density of the clouds. However, organic molecules were observed in the spectra that scientists would not have expected to find under these conditions, such as formaldehyde, methanol, and vinyl alcohol. The reactions needed to create such substances are familiar to scientists only at the much higher temperatures and pressures of earth and earth-based laboratories. The fact that they were found indicates that these chemical reactions in interstellar clouds take place faster than suspected, likely in gas-phase reactions unfamiliar to organic chemistry as observed on earth. These reactions are studied in the CRESU experiment.\nInterstellar clouds also provide a medium to study the presence and proportions of metals in space. The presence and ratios of these elements may help develop theories on the means of their production, especially when their proportions are inconsistent with those expected to arise from stars as a result of fusion and thereby suggest alternate means, such as cosmic ray spallation.\nHigh-velocity cloud.\nThese interstellar clouds possess a velocity higher than can be explained by the rotation of the Milky Way. By definition, these clouds must have a vlsr greater than 90\u00a0km s\u22121, where vlsr is the local standard rest velocity. They are detected primarily in the 21 cm line of neutral hydrogen, and typically have a lower portion of heavy elements than is normal for interstellar clouds in the Milky Way.\nTheories intended to explain these unusual clouds include materials left over from the formation of the galaxy, or tidally-displaced matter drawn away from other galaxies or members of the Local Group. An example of the latter is the Magellanic Stream. To narrow down the origin of these clouds, a better understanding of their distances and metallicity is needed.\nHigh-velocity clouds are identified with an HVC prefix, as with HVC 127-41-330.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14980", "revid": "38226348", "url": "https://en.wikipedia.org/wiki?curid=14980", "title": "Imhotep", "text": "Egyptian polymath, later deified\nImhotep (; \"(the one who) comes in peace\"; fl.\u2009late 27th century BC) was an Egyptian chancellor to the King Djoser, possible architect of Djoser's step pyramid, and high priest of the sun god Ra at Heliopolis. Very little is known of Imhotep as a historical figure, but in the 3,000\u00a0years following his death, he was gradually glorified and deified.\nTraditions from long after Imhotep's death treated him as a great author of wisdom texts and especially as a physician. No extant text from his lifetime mentions these capacities, and none mention his name in the first 1,200\u00a0years following his death. The Westcar Papyrus of the Hyksos period, written in classical Middle Egyptian (likely around the 13th Dynasty), contains a story about an official performing a miracle for Djoser, possibly Imhotep. However, this section is badly damaged and no mention of this character's name survived.\nApart from the three short contemporary inscriptions that establish him as chancellor to the Pharaoh, the first surviving text to name Imhotep dates to the time of Amenhotep III (c.\u20091391\u20131353\u00a0BC). It is addressed to the owner of a tomb and reads:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The wab-priest may give offerings to your ka. The wab-priests may stretch to you their arms with libations on the soil, as it is done for Imhotep with the remains of the water bowl. \u2014\u200a\nIt appears that this libation to Imhotep was done regularly, as they are attested on papyri associated with statues of Imhotep until the Late Period (c.\u2009664\u2013332\u00a0BC). Wildung (1977) explains the origin of this cult as a slow evolution of intellectuals' memory of Imhotep, from his death onward. Gardiner finds the cult of Imhotep during the New Kingdom (c.\u20091550\u20131077\u00a0BC) sufficiently distinct from the usual offerings made to other commoners that the epithet \"demigod\" is likely justified to describe his veneration.\nThe first references to the healing abilities of Imhotep occur from the Thirtieth Dynasty (c.\u2009380\u2013343\u00a0BC) onward, some 2,200\u00a0years after his death.12744\nImhotep is among fewer than a dozen non-royal Egyptians who were deified after their deaths. The center of his cult was in Memphis. The location of his tomb remains unknown, despite efforts to find it.\nHistoricity.\nImhotep's historicity is confirmed by two contemporary inscriptions made during his lifetime on the base or pedestal of one of Djoser's statues (Cairo JE 49889) and also by a graffito on the enclosure wall surrounding Sekhemkhet's unfinished step pyramid. The latter inscription suggests that Imhotep outlived Djoser by a few years and went on to serve in the construction of King Sekhemkhet's pyramid, which was abandoned due to this ruler's brief reign.\nImhotep held the ambiguous title \"bity sensen\" or \"bity senwy\", unique in ancient Egyptian history. This literally translates as \"the King of Lower Egypt, the two brothers\", and could be interpreted to mean that Imhotep might be twin brother of Pharaoh, which would explain his high position; with no known individuals with similar titles, however, interpretation remains highly speculative. If not a blood relative, he might have been the King's confidant or childhood friend.\nArchitecture and engineering.\nImhotep was one of the chief officials of the Pharaoh Djoser. Concurring with much later legends, Egyptologists credit him with the design and construction of the Pyramid of Djoser, a step pyramid at Saqqara built during the 3rd Dynasty. He may also have been responsible for the first known use of stone columns to support a building. Despite these later attestations, the pharaonic Egyptians themselves never credited Imhotep as the designer of the stepped pyramid, nor with the invention of stone architecture.\nDeification.\nGod of medicine.\nTwo thousand years after his death, Imhotep's status had risen to that of a god of medicine and healing. Eventually, Imhotep was equated with Thoth, the god of architecture, mathematics, and medicine, and patron of scribes: Imhotep's cult was merged with that of his own former tutelary god.\nHe was revered in the region of Thebes as the \"brother\" of Amenhotep, son of Hapu \u2013 another deified architect \u2013 in the temples dedicated to Thoth.v3, p104 Because of his association with health, the Greeks equated Imhotep with Asklepios, their own god of health who also was a deified mortal.\nAccording to myth, Imhotep's mother was a mortal named Khereduankh, she too being eventually revered as a demi-goddess as the daughter of Banebdjedet. Alternatively, since Imhotep was known as the \"Son of Ptah\",v?, p106 his mother was sometimes claimed to be Sekhmet, the patron of Upper Egypt whose consort was Ptah.\nPost-Alexander period.\nThe Upper Egyptian Famine Stela, which dates from the Ptolemaic period (305\u201330\u00a0BC), bears an inscription containing a legend about a famine lasting seven years during the reign of Djoser. Imhotep is credited with having been instrumental in ending it. One of his priests explained the connection between the god Khnum and the rise of the Nile to the Pharaoh, who then had a dream in which the Nile god spoke to him, promising to end the drought.\nA demotic papyrus from the temple of Tebtunis, dating to the 2nd\u00a0century\u00a0AD, preserves a long story about Imhotep. The Pharaoh Djoser plays a prominent role in the story, which also mentions Imhotep's family; his father the god Ptah, his mother Khereduankh, and his younger sister Renpetneferet. At one point Djoser desires Renpetneferet, and Imhotep disguises himself and tries to rescue her. The text also refers to the royal tomb of Djoser. Part of the legend includes an anachronistic battle between the Old Kingdom and the Assyrian armies where Imhotep fights an Assyrian sorceress in a duel of magic.\nAs an instigator of Egyptian culture, Imhotep's idealized image lasted well into the Roman period. In the Ptolemaic period, the Egyptian priest and historian Manetho credited him with inventing the method of a stone-dressed building during Djoser's reign, although he was not the first to actually build with stone. Stonewalling, flooring, lintels, and jambs had appeared sporadically during the Archaic Period, even though it is true that a building the size of the step pyramid made entirely out of stone had never before been constructed. Before Djoser, Kings were buried in mastaba tombs.\nMedicine.\nEgyptologist James Peter Allen states that \"The Greeks equated him with their own god of medicine, Asklepios, although ironically, there is no evidence that Imhotep himself was a physician.\"\nIn his Pulitzer-prize winning \u201cbiography\u201d of cancer \u2013 \"The Emperor of All Maladies\" \u2013 Siddhartha Mukherjee cites the oldest identified written diagnosis of cancer to Imhotep. Unfortunately, the therapy Imhotep laconically prescribed for it would be equally recognizable for millennia: \u201cThere is none\u201d.\nIn popular culture.\nImhotep's name is shared by the antagonist of the 1932 film \"The Mummy\", along with its 1999 remake.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14981", "revid": "19569748", "url": "https://en.wikipedia.org/wiki?curid=14981", "title": "Ictinus", "text": "Mid-5th-century BC Athenian architect\nIctinus (; , \"Iktinos\") was an architect active in the mid 5th century BC. Ancient sources identify Ictinus and Callicrates as co-architects of the Parthenon. He co-wrote a book on the project \u2013 which is now lost \u2013 in collaboration with Carpion.\nPausanias identifies Ictinus as architect of the Temple of Apollo at Bassae. That temple was Doric on the exterior, Ionic on the interior, and incorporated a Corinthian column, the earliest known, at the center rear of the cella. Sources also identify Ictinus as architect of the Telesterion at Eleusis, a gigantic hall used in the Eleusinian Mysteries.\nPericles also commissioned Ictinus to design the Telesterion (\"Hall of Final Things\") at Eleusis, but his involvement was terminated when Pericles fell from power. Three other architects took over instead. It seems likely that Ictinus's reputation was harmed by his links with the fallen ruler, as he is singled out for condemnation by Aristophanes in his play \"The Birds\", dated to around 414 BC. It depicts the royal kite or \"ictinus\" \u2013\u00a0a play on the architect's name \u2013\u00a0not as a noble bird of prey but as a scavenger stealing sacrifices from the gods and money from men. As no other classical author describes the bird in this fashion, Aristophanes likely intended it to be a dig at the architect.\nThe artist Jean Auguste Dominique Ingres painted a scene showing Ictinus together with the lyric poet Pindar. The painting is known as https:// and is exhibited at the National Gallery, London.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14982", "revid": "20957809", "url": "https://en.wikipedia.org/wiki?curid=14982", "title": "Isidore of Miletus", "text": "5th-century Byzantine Greek architect and mathematician\nIsidore of Miletus (; ; ) was one of the two main Byzantine Greek mathematician, physicist and architects (Anthemius of Tralles was the other) that Emperor Justinian I commissioned to design the cathedral Hagia Sophia in Constantinople from 532 to 537. He was born c.\u2009475 AD. The creation of an important compilation of Archimedes' works has been attributed to him. The spurious Book XV from Euclid's Elements has been partly attributed to Isidore of Miletus.\nBiography.\nIsidore of Miletus was a renowned scientist and mathematician before Emperor Justinian I hired him. Isidorus taught stereometry and physics at the universities of Alexandria and then of Constantinople, and wrote a commentary on an older treatise on vaulting. Eutocius together with Isidore studied Archimedes' work. Isidore is also renowned for producing the first comprehensive compilation of Archimedes' work, the Archimedes palimpsest survived to the present.\nTeachings and writings.\nA majority of Isidore's preserved work are his edits and commentaries on older Greek mathematical texts. For example, Isidore is known to have revised and checked some of Archimedes' works and also Book XV of Euclid's elements.\nThat being said, claims from Alan Cameron have been made about a hypothetical \"School of Isidore\". Between his work on architectural exploits, Isidore taught about math and geometry of the time. The School of Isidore is supported more by the presence of his teaching's in much of his students (such as Eutocious) works rather than his own writings. In an edit of the fifteenth book of Euclid's \"Elements,\" for instance, the editor quotes Isidore, but then proceeds to explain that Isidore did not publish much of his work himself. Instead, he taught, and once he himself could understand the material, did not see a need to write it down. It is because of this that Cameron claims that Isidore helped to revitalize interest in ancient mathematicians in Constantinople and Alexandria circa 510.\nIn addition to editing the works of others, Isidore is known to have written his own commentary on Hero of Alexandria's \"On Vaulting\", which discussed aspects of vault construction and design in relation to geometry. While this commentary is lost Eutocius makes mention of it in his own writings. It is when referring to this work that Eutocius credits Isidore with designing a special compass for the purpose of drawing parabolas. Isidore's invention allowed for the drawing of parabolas with a greater level accuracy than that of which many previous methods were capable. From Eutocius (or his copyist) it is believed that one notable use for Isidores invention was to visually solve the problem of doubling the volume of a cube. This was said to be done by drawing two parabolas and finding the point where they intersect. In addition to their mathematical applications, Isidore is believed to have highlighted the uses of applying the use of parabolas to the construction of vaults.\nHagia Sophia.\nEmperor Justinian I appointed his architects to rebuild the Hagia Sophia following his victory over protesters within the capital city of the Roman Empire, Constantinople. The first basilica was completed in 360 and remodelled from 404 to 415, but had been damaged in 532 in the course of the Nika Riot, \u201cThe temple of Sophia, the baths of Zeuxippus, and the imperial courtyard from the Propylaia all the way to the so-called House of Ares were burned up and destroyed, as were both of the great porticoes that lead to the forum that is named after Constantine, houses of prosperous people, and a great deal of other properties.\u201d\nThe rival factions of Constantinople's populace, the Blues and Greens, opposed each other in the chariot races at the Hippodrome and often resorted to violence. During the Nika Riot, more than thirty thousand people were killed. Emperor Justinian I ensured that his new structure would not be burned down, like its predecessors, by commissioning architects that would build the church mainly out of stone, rather than wood, \u201cHe compacted it of baked brick and mortar, and in many places bound it together with iron, but made no use of wood, so that the church should no longer prove combustible.\u201d The construction of the Hagia Sophia began so fast after the riots were quelled that many think that Justinian had his architects begin planning it before the riots even stopped.\nIsidore of Miletus and Anthemius of Tralles originally planned on a main hall of the Hagia Sophia that measured 70 by 75 metres (230 x 250\u00a0ft), making it the largest church in Constantinople, but the original dome was nearly 6 metres (20\u00a0ft) lower than it was constructed, \u201cJustinian suppressed these riots and took the opportunity of marking his victory by erecting in 532-7 the new Hagia Sophia, one of the largest, most lavish, and most expensive buildings of all time.\u201d\nAlthough Isidore of Miletus and Anthemius of Tralles were not formally educated in architecture, they were scientists who could organize the logistics of drawing thousands of labourers and unprecedented loads of rare raw materials from around the Roman Empire to construct the Hagia Sophia for Emperor Justinian I. Isidore and Anthemius obtained stone from as far away as Egypt, Syria, and Libya, and columns from several temples in Rome. The finished product was built in admirable form for the Roman Emperor, \u201cAll of these elements marvellously fitted together in mid-air, suspended from one another and reposing only on the parts adjacent to them, produce a unified and most remarkable harmony in the work, and yet do not allow the spectators to rest their gaze upon any one of them for a length of time.\u201d It is believed that Isidore did much of the work on the domes of the Hagia Sophia due to his extensive work on vaults, and his commentary, \"On Vaulting\".\nThe Hagia Sophia architects innovatively combined the longitudinal structure of a Roman basilica and the central plan of a drum-supported dome, in order to withstand the high magnitude earthquakes of the Marmara region, \u201cHowever, in May 558, little more than 20 years after the Church\u2019s dedication, following the earthquakes of August 553 and December 557, parts of the central dome and its supporting structure system collapsed.\u201d The Hagia Sophia was repeatedly cracked by earthquakes and was quickly repaired. Isidore of Miletus\u2019 nephew, Isidore the Younger, introduced the new dome design that can be viewed in the Hagia Sophia in present-day Istanbul, Turkey. Originally the dome was constructed without ribs, but achieved its present-day construction with ribs when Isidore the Younger repaired the church.\nAfter a great earthquake in 989 ruined the dome of Hagia Sophia, the Byzantine officials summoned Trdat the Architect to Byzantium to organize repairs. The restored dome was completed by 994.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14984", "revid": "3798137", "url": "https://en.wikipedia.org/wiki?curid=14984", "title": "International Atomic Energy Agency", "text": "International organization\nThe International Atomic Energy Agency (IAEA) is an intergovernmental organization that seeks to promote the peaceful use of nuclear energy and to inhibit its use for any military purpose, including nuclear weapons. It was established in 1957 as an autonomous organization within the United Nations system; though governed by its own founding treaty, the organization reports to both the General Assembly and the Security Council of the United Nations, and is headquartered at the UN Office at Vienna, Austria.\nThe IAEA was created in response to growing international concern toward nuclear weapons, especially amid rising tensions between the foremost nuclear powers, the United States and the Soviet Union. U.S. president Dwight D. Eisenhower's \"Atoms for Peace\" speech, which called for the creation of an international organization to monitor the global proliferation of nuclear resources and technology, is credited with catalyzing the formation of the IAEA, whose treaty came into force on 29 July 1957 upon U.S. ratification.\nThe IAEA serves as an intergovernmental forum for scientific and technical cooperation on the peaceful use of nuclear technology and nuclear power worldwide. It maintains several programs that encourage the development of peaceful applications of nuclear energy, science, and technology; provide international safeguards against misuse of nuclear technology and nuclear materials; and promote and implement nuclear safety (including radiation protection) and nuclear security standards. The organization also conducts research in nuclear science and provides technical support and training in nuclear technology to countries worldwide, particularly in the developing world.\nFollowing the ratification of the Treaty on the Non-Proliferation of Nuclear Weapons in 1968, all non-nuclear powers are required to negotiate a safeguards agreement with the IAEA, which is given the authority to monitor nuclear programs and to inspect nuclear facilities. In 2005, the IAEA and its administrative head, Director General Mohamed ElBaradei, were awarded the Nobel Peace Prize \"for their efforts to prevent nuclear energy from being used for military purposes and to ensure that nuclear energy for peaceful purposes is used in the safest possible way\".\nMissions.\nThe IAEA is generally described as having three main missions:\nPeaceful uses.\nAccording to Article II of the IAEA Statute, the objectives of the IAEA are \"to accelerate and enlarge the contribution of atomic energy to peace, health and prosperity throughout the world\" and to \"ensure ... that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose.\" Its primary functions in this area, according to Article III, are to encourage research and development, to secure or provide materials, services, equipment, and facilities for Member States, and to foster the exchange of scientific and technical information and training.\nThree of the IAEA's six departments are principally charged with promoting the peaceful uses of nuclear energy. The Department of Nuclear Energy focuses on providing advice and services to Member States on nuclear power and the nuclear fuel cycle. The Department of Nuclear Sciences and Applications focuses on the use of non-power nuclear and isotope techniques to help IAEA Member States in the areas of water, energy, health, biodiversity, and agriculture. The Department of Technical Cooperation provides direct assistance to IAEA Member States, through national, regional, and inter-regional projects through training, expert missions, scientific exchanges, and provision of equipment.\nSafeguards.\nArticle II of the IAEA Statute defines the Agency's twin objectives as promoting peaceful uses of atomic energy and \"ensur[ing], so far as it is able, that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose.\" To do this, the IAEA is authorized in Article III.A.5 of the Statute \"to establish and administer safeguards designed to ensure that special fissionable and other materials, services, equipment, facilities, and information made available by the Agency or at its request or under its supervision or control are not used in such a way as to further any military purpose; and to apply safeguards, at the request of the parties, to any bilateral or multilateral arrangement, or at the request of a State, to any of that State's activities in the field of atomic energy.\"\nThe Department of Safeguards is responsible for carrying out this mission, through technical measures designed to verify the correctness and completeness of states' nuclear declarations.\nNuclear safety.\nThe IAEA classifies safety as one of its top three priorities. It spends 8.9 percent of its 352 million-euro ($469\u00a0million) regular budget in 2011 on making plants secure from accidents. Its resources are used on the other two priorities: technical co-operation and preventing nuclear weapons proliferation.\nThe IAEA itself says that, beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The IAEA says that the same happened after the Fukushima disaster in Fukushima, Japan.\nIn June 2011, IAEA Director General Yukiya Amano said he had \"broad support for his plan to strengthen international safety checks on nuclear power plants to help avoid any repeat of Japan's Fukushima crisis\" and proposed peer-reviewed safety checks on reactors worldwide, organized by the IAEA.\nHistory.\nIn 1946 United Nations Atomic Energy Commission was founded, but stopped working in 1949 and was disbanded in 1952. In 1953, U.S. President Dwight D. Eisenhower proposed the creation of an international body to both regulate and promote the peaceful use of atomic power (nuclear power), in his Atoms for Peace address to the UN General Assembly. In September 1954, the United States proposed to the General Assembly the creation of an international agency to take control of fissile material, which could be used either for nuclear power or for nuclear weapons. This agency would establish a kind of \"nuclear bank\".\nThe United States also called for an international scientific conference on all of the peaceful aspects of nuclear power. By November 1954, it had become clear that the Soviet Union would reject any international custody of fissile material if the United States did not agree to disarmament first, but that a \"clearinghouse\" for nuclear transactions might be possible. From 8 to 20 August 1955, the United Nations held the International Conference on the Peaceful Uses of Atomic Energy in Geneva, Switzerland. In October 1957, a Conference on the IAEA Statute was held at the Headquarters of the United Nations to approve the founding document for the IAEA, which was negotiated in 1955\u20131957 by a group of twelve countries. The Statute of the IAEA was approved on 23 October 1956 and came into force on 29 July 1957.\nFormer US Congressman W. Sterling Cole served as the IAEA's first Director-General from 1957 to 1961. Cole served only one term, after which the IAEA was headed by two Swedes for nearly four decades: the scientist Sigvard Eklund held the job from 1961 to 1981, followed by former Swedish Foreign Minister Hans Blix, who served from 1981 to 1997. Blix was succeeded as Director General by Mohamed ElBaradei of Egypt, who served until November 2009.\nBeginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA increased its efforts in the field of nuclear safety. The same happened after the 2011 Fukushima disaster in Fukushima, Japan.\nBoth the IAEA and its then Director General, ElBaradei, were awarded the Nobel Peace Prize in 2005. In his acceptance speech in Oslo, ElBaradei stated that only one percent of the money spent on developing new weapons would be enough to feed the entire world, and that, if we hope to escape self-destruction, then nuclear weapons should have no place in our collective conscience, and no role in our security.\nOn 2 July 2009, Yukiya Amano of Japan was elected as the Director General for the IAEA, defeating Abdul Samad Minty of South Africa and Luis E. Ech\u00e1varri of Spain. On 3 July 2009, the Board of Governors voted to appoint Yukiya Amano \"by acclamation\", and IAEA General Conference in September 2009 approved. He took office on 1 December 2009. After Amano's death, his Chief of Coordination Cornel Feruta of Romania was named Acting Director General.\nOn 2 August 2019, Rafael Grossi was presented as the Argentine candidate to become the Director General of IAEA. On 28 October 2019, the IAEA Board of Governors held its first vote to elect the new Director General, but none of the candidates secured the two-thirds majority (23 votes) in the 35-member IAEA Board of Governors that was needed to be elected. The next day, 29 October, the second voting round was held, and Grossi won 24 votes. He assumed office on 3 December 2019. Following a special meeting of the IAEA General Conference to approve his appointment, on 3 December Grossi became the first Latin American to head the Agency.\nDuring the Russian invasion of Ukraine, Grossi visited Ukraine multiple times as part of the ongoing efforts to help prevent a nuclear accident during the war. He warned against any complacency towards the dangers that the Zaporizhzhia Nuclear Power Plant, Europe's largest nuclear power plant, was facing. The plant has come under fire multiple times during the war.\nFunction and structure.\nGeneral.\nThe IAEA's mission is guided by the interests and needs of Member States, strategic plans, and the vision embodied in the IAEA Statute (see below). Three main pillars \u2013 or areas of work \u2013 underpin the IAEA's mission: Safety and Security; Science and Technology; and Safeguards and Verification.\nThe IAEA as an autonomous organization is not under the direct control of the UN, but the IAEA does report to both the UN General Assembly and Security Council. Unlike most other specialized international agencies, the IAEA does much of its work with the Security Council, and not with the United Nations Economic and Social Council. The structure and functions of the IAEA are defined by its founding document, the IAEA Statute (see below). The IAEA has three main bodies: the Board of Governors, the General Conference, and the Secretariat.\nThe IAEA exists to pursue the \"safe, secure and peaceful uses of nuclear sciences and technology\" (Pillars 2005). The IAEA executes this mission with three main functions: the inspection of existing nuclear facilities to ensure their peaceful use, providing information and developing standards to ensure the safety and security of nuclear facilities, and as a hub for the various fields of science involved in the peaceful applications of nuclear technology.\nThe IAEA recognizes knowledge as the nuclear energy industry's most valuable asset and resource, without which the industry cannot operate safely and economically. Following the IAEA General Conference since 2002 resolutions the Nuclear Knowledge Management, a formal program was established to address Member States' priorities in the 21st century.\nIn 2004, the IAEA developed a Programme of Action for Cancer Therapy (PACT). PACT responds to the needs of developing countries to establish, to improve, or to expand radiotherapy treatment programs. The IAEA is raising money to help efforts by its Member States to save lives and reduce the suffering of cancer victims.\nThe IAEA has established programs to help developing countries in planning to build systematically the capability to manage a nuclear power program, including the Integrated Nuclear Infrastructure Group, which has carried out Integrated Nuclear Infrastructure Review missions in Indonesia, Jordan, Thailand and Vietnam. The IAEA reports that roughly 60 countries are considering how to include nuclear power in their energy plans.\nTo enhance the sharing of information and experience among IAEA Member States concerning the seismic safety of nuclear facilities, in 2008 the IAEA established the International Seismic Safety Centre. This centre is establishing safety standards and providing for their application in relation to site selection, site evaluation and seismic design.\nThe IAEA has its headquarters since its founding in Vienna, Austria. The IAEA has two \"Regional Safeguards Offices\" which are located in Toronto, Canada, and in Tokyo, Japan. The IAEA also has two liaison offices which are located in New York City, United States, and in Geneva, Switzerland. In addition, the IAEA has laboratories and research centers located in Seibersdorf, Austria, in Monaco and in Trieste, Italy.\nBoard of Governors.\nThe Board of Governors is one of two policy-making bodies of the IAEA. The Board consists of 22 member states elected by the General Conference, and at least 10 member states nominated by the outgoing Board. The outgoing Board designates the ten members who are the most advanced in atomic energy technology, plus the most advanced members from any of the following areas that are not represented by the first ten: North America, Latin America, Western Europe, Eastern Europe, Africa, Middle East, and South Asia, South East Asia, the Pacific, and the Far East. These members are designated for one year terms. The General Conference elects 22 members from the remaining nations to two-year terms. Eleven are elected each year. The 22 elected members must also represent a stipulated geographic diversity.\nThe Board generally meets five times per year: in March and June, twice in September (before and after the General Conference) and in November. In its meetings, it is responsible for making most of the policies of the IAEA. The Board makes recommendations to the General Conference on IAEA activities and budget, is responsible for publishing IAEA standards and appoints the Director-General subject to General Conference approval. Board members each receive one vote. Budget matters require a two-thirds majority. All other matters require only a simple majority. The simple majority also has the power to stipulate issues that will thereafter require a two-thirds majority. Two-thirds of all Board members must be present to call a vote. The Board elects its own chairman.\nGeneral Conference.\nThe General Conference is made up of all 180 member states. It meets once a year, typically in September, to approve the actions and budgets passed on from the Board of Governors. The General Conference also approves the nominee for Director General and requests reports from the Board on issues in question (Statute). Each member receives one vote. Issues of budget, Statute amendment and suspension of a member's privileges require a two-thirds majority and all other issues require a simple majority. Similar to the Board, the General Conference can, by simple majority, designate issues to require a two-thirds majority. The General Conference elects a President at each annual meeting to facilitate an effective meeting. The President only serves for the duration of the session (Statute).\nThe main function of the General Conference is to serve as a forum for debate on current issues and policies. Any of the other IAEA organs, the Director-General, the Board and member states can table issues to be discussed by the General Conference (IAEA Primer). This function of the General Conference is almost identical to the General Assembly of the United Nations.\nSecretariat.\nThe Secretariat is the professional and general service staff of the IAEA. The Secretariat is headed by the Director General. The Director General is responsible for enforcement of the actions passed by the Board of Governors and the General Conference. The Director General is selected by the Board and approved by the General Conference for renewable four-year terms. The Director General oversees six departments that do the actual work in carrying out the policies of the IAEA: Nuclear Energy, Nuclear Safety and Security, Nuclear Sciences and Applications, Safeguards, Technical Cooperation, and Management.\nThe IAEA budget is in two parts. The regular budget funds most activities of the IAEA and is assessed to each member nation (\u20ac344 million in 2014). The Technical Cooperation Fund is funded by voluntary contributions with a general target in the US$90\u00a0million range.\nMembership.\nThe process of joining the IAEA is fairly simple. Normally, a State would notify the Director General of its desire to join, and the Director would submit the application to the Board for consideration. If the Board recommends approval, and the General Conference approves the application for membership, the State must then submit its instrument of acceptance of the IAEA Statute to the United States, which functions as the depositary Government for the IAEA Statute. The State is considered a member when its acceptance letter is deposited. The United States then informs the IAEA, which notifies other IAEA Member States. Signature and ratification of the Nuclear Non-Proliferation Treaty (NPT) are not preconditions for membership in the IAEA.\nThe IAEA has 180 member states. Most UN members and the Holy See are Member States of the IAEA.\nFour states have withdrawn from the IAEA. North Korea was a Member State from 1974 to 1994, but withdrew after the Board of Governors found it in non-compliance with its safeguards agreement and suspended most technical co-operation. Nicaragua became a member in 1957, withdrew its membership in 1970, and rejoined in 1977, Honduras joined in 1957, withdrew in 1967, and rejoined in 2003, while Cambodia joined in 1958, withdrew in 2003, and rejoined in 2009.\nRegional Cooperative Agreements.\nThere are four regional cooperative areas within IAEA, that share information, and organize conferences within their regions:\nAFRA.\nThe African Regional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology (AFRA):\nARASIA.\nCooperative Agreement for Arab States in Asia for Research, Development and Training related to Nuclear Science and Technology (ARASIA):\nRCA.\nRegional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology for Asia and the Pacific (RCA):\nARCAL.\nCooperation Agreement for the Promotion of Nuclear Science and Technology in Latin America and the Caribbean (ARCAL):\nCriticism.\nIn 2011, Russian nuclear accident specialist Yuliy Andreev was critical of the response to Fukushima, and says that the IAEA did not learn from the 1986 Chernobyl disaster. He has accused the IAEA and corporations of \"wilfully ignoring lessons from the world's worst nuclear accident 25 years ago to protect the industry's expansion\". The IAEA's role \"as an advocate for nuclear power has made it a target for protests\".\nThe journal \"Nature\" has reported that the IAEA response to the 2011 Fukushima Daiichi nuclear disaster in Japan was \"sluggish and sometimes confusing\", drawing calls for the agency to \"take a more proactive role in nuclear safety\". But nuclear experts say that the agency's complicated mandate and the constraints imposed by its member states mean that reforms will not happen quickly or easily, although its INES \"emergency scale is very likely to be revisited\" given the confusing way in which it was used in Japan.\nSome scientists say that the Fukushima nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide. There are several problems with the IAEA says Najmedin Meshkati of University of Southern California:\nIt recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organisation overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT).\nIn 2011, the journal \"Nature\" reported that the International Atomic Energy Agency should be strengthened to make independent assessments of nuclear safety and that \"the public would be better served by an IAEA more able to deliver frank and independent assessments of nuclear crises as they unfold\".\nPublications.\nTypically issued in July each year, the IAEA Annual Report summarizes and highlights developments over the past year in major areas of the Agency's work. It includes a summary of major issues, activities, and achievements, and status tables and graphs related to safeguards, safety, and science and technology. Alongside the Annual Report, the IAEA also issues Topical Reviews which detail specific sectors of its work, comprising the Nuclear Safety Review, Nuclear Security Review, Safeguards Implementation Report, Nuclear Technology Review, and Technical Cooperation Report.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14985", "revid": "51014787", "url": "https://en.wikipedia.org/wiki?curid=14985", "title": "International Civil Aviation Organization", "text": "Specialised agency of the United Nations\nThe International Civil Aviation Organization (ICAO) is a specialized agency of the United Nations that coordinates the principles and techniques of international air navigation, and fosters the planning and development of international air transport to ensure safe and orderly growth. The ICAO headquarters are located in the Quartier international de Montr\u00e9al of Montreal, Quebec, Canada.\nThe ICAO Council adopts standards and recommended practices concerning air navigation, its infrastructure, flight inspection, prevention of unlawful interference, and facilitation of border-crossing procedures for international civil aviation. ICAO defines the protocols for air accident investigation that are followed by in countries signatory to the Convention on International Civil Aviation.\nThe Air Navigation Commission (ANC) is the technical body within ICAO. The commission is composed of 19 commissioners, nominated by the ICAO's contracting states and appointed by the ICAO Council. Commissioners serve as independent experts, who although nominated by their states, do not serve as state or political representatives. International Standards and Recommended Practices are developed under the direction of the ANC through the formal process of ICAO Panels. Once approved by the commission, standards are sent to the council, the political body of ICAO, for consultation and coordination with the member states before final adoption.\nICAO is distinct from other international air transport organizations, particularly because it alone is vested with international authority (among signatory states): other organizations include the International Air Transport Association (IATA), a trade association representing airlines; the Civil Air Navigation Services Organisation (CANSO), an organization for air navigation service providers (ANSPs); and the Airports Council International, a trade association of airport authorities. In addition there are several regional civil aviation commissions, such as the Latin America Civil Aviation Commission (LACAC) who focus on challenges and growth in specific regions.\nHistory.\n20th century.\nIn the early 20th Century, the International Telecommunication Union met to discuss and implement one of the first internationally agreed upon standards relating to aviation, country-specific prefixes for aircraft callsigns. The first convention was held in 1903 in Berlin, Germany, but no agreements were reached among the eight countries that attended. At the second convention in 1906, also held in Berlin, twenty-seven countries attended. The third convention, held in London in 1912, allocated the first radio callsigns for use by aircraft. Following this, at the Paris Convention of 1919, a forerunner to ICAO named ICAN was established, the International Commission for Air Navigation. ICAN continued to operate until 1945.\nThe Convention on International Civil Aviation, also known as the Chicago Convention, in Chicago, was signed by 52 countries on 7 December 1944. Under its terms, a Provisional International Civil Aviation Organization was to be established, to be replaced in turn by a permanent organization when twenty-six countries ratified the convention. PICAO began operating on 6 June 1945, replacing ICAN. The 26th country ratified the convention on 5 March 1947 and, consequently, PICAO held its last session from 29 April 1947 until 7 May 1947, with the Convention on International Civil Aviation coming into force on 4 April 1947.\nIn October 1947, ICAO became an agency of the United Nations under its Economic and Social Council (ECOSOC).\n21st century.\nIn April 2013, Qatar offered to serve as the new permanent seat of the Organization. Qatar promised to construct a massive new headquarters for ICAO and to cover all moving expenses, stating that Montreal \"was too far from Europe and Asia\", \"had cold winters\", was hard to attend due to the Canadian government's slow issuance of visas, and that the taxes imposed on ICAO by Canada were too high. According to \"The Globe and Mail\", Qatar's invitation was at least partly motivated by the pro-Israel foreign policy of Canadian Prime Minister Stephen Harper. Approximately a month later, Qatar withdrew its bid after a separate proposal to the ICAO's governing council to move the ICAO triennial conference to Doha was defeated by a vote of 22\u201314.\nIn June 2014, the Montreal Metro station closest to the ICAO headquarters was renamed Square-Victoria\u2013OACI, celebrating the 70th anniversary of ICAO's presence in Montreal.\nTaiwan controversy.\nIn January 2020, ICAO blocked several Twitter users, including think-tank analysts, U.S. Congressional staff, and journalists, who mentioned Taiwan in tweets related to ICAO. Many of the tweets were related to the COVID-19 pandemic and Taiwan's exclusion from ICAO safety and health bulletins due to pressure from China. In response, ICAO issued a tweet stating that publishers of \"irrelevant, compromising and offensive material\" would be \"precluded\".\nSince that action, the organization has followed a policy of blocking anyone asking about it.\nThe United States House Committee on Foreign Affairs harshly criticized ICAO's perceived failure to uphold principles of fairness, inclusion, and transparency by silencing non-disruptive opposing voices. Senator Marco Rubio also criticized the move. The Taiwanese Ministry of Foreign Affairs (MOFA) and legislators criticized the move, with MOFA head Jaushieh Joseph Wu tweeting in support of those blocked.\nIn January 2020, Anthony Philbin, Chief of Communications for the ICAO Secretary General, defended ICAO's actions, stating, 'We felt completely justified in taking steps to protect the integrity of the information and discussions that our followers reasonably expect from our feeds.' In exchanges with the International Flight Network, Philbin refused to acknowledge the existence of Taiwan.\nOn 1 February 2020, the United States Department of State issued a press release heavily criticizing ICAO's actions, characterizing them as \"outrageous, unacceptable, and not befitting of a UN organization.\"\nNorth Korea controversy.\nOn 2 May 2025, the ICAO Council expressed grave concern over ongoing Global Navigation Satellite System (GNSS) radio frequency interference in the Incheon Flight Information Region (FIR), incidents that have persisted since 2 October 2024, and are attributed to North Korea, officially known as the Democratic People's Republic of Korea (DPRK). The Council emphasized that such interference endangers international air navigation safety and violates the principles of the Chicago Convention. It strongly urged the DPRK to adhere to its international obligations and prevent future occurrences. Given the severity of the situation, the Council is considering reporting the matter to the 42nd Session of the ICAO Assembly in September 2025, as per Article 54(k) of the Convention, and will continue to monitor developments closely.\nStatute.\nThe 9th edition of the Convention on International Civil Aviation includes modifications from years 1948 up to 2006. ICAO refers to its current edition of the convention as the \"Statute\" and designates it as ICAO Document 7300/9. The convention has 19 Annexes that are listed by title in the article Convention on International Civil Aviation.\nMembership.\nAs of \u00a02019[ [update]], there are 193 ICAO members, consisting of 192 of the 193 UN members (all but Liechtenstein, which lacks an international airport), plus the Cook Islands.\nDespite Liechtenstein not being a direct party to ICAO, its government delegated Switzerland to enter into the treaty on its behalf in 1947, and the treaty is applicable in the territory of Liechtenstein.\nExclusion of Taiwan.\nThe Republic of China was a founding member of ICAO. Following its retreat to Taiwan, it was eventually replaced by the People's Republic of China as the legal representative of China in 1971.\nIn 2013, Taiwan was for the first time invited to attend the ICAO Assembly, at its 38th session, as a guest under the name of \"Chinese Taipei\". As of \u00a02019[ [update]], it has not been invited to participate again, due to renewed PRC pressure.\nThe host government, Canada, supports Taiwan's inclusion in ICAO. Support also comes from Canada's commercial sector with the president of the Air Transport Association of Canada saying in 2019 that \"It's about safety in aviation so from a strictly operational and non-political point of view, I believe Taiwan should be there.\"\nCouncil.\nThe ICAO Council is elected by the Assembly every three years and consists of 36 members elected in three groups. The present council was elected in October 2022.\nThe structure of the present Council is as follows:\nOn September 27, 2025, the ICAO Assembly held fresh elections to Group I and Group II of the Council and elected the following states for a three-year term. Elections to Group III were held on September 30:\nAir Navigation Commission.\nThe Air Navigation Commission (ANC) is the ICAO Council technical executive body in charge of 17 of the 19 Annexes to the Chicago Convention. ANC develops and recommend ICAO minimal standards that are related to these Annexes. To review and/or finalize the ongoing developments the commission meets for three sessions per year. Each session normally considers a number of documents being developments of ANC expert Panels. The ANC is composed of nineteen commissioners nominated by ICAO States in various aviation domains. However, legally these commissioners do not represent the interest of their State or any particular State or region. They have to conduct independently in the interest of the entire international civil aviation community. Additionally, several other representatives from ICAO States and up to eight members from the civil aviation industry may be invited to take part in ANC meetings as observers.\nStandards.\nICAO also standardizes certain functions for use in the airline industry, such as the Aeronautical Message Handling System (AMHS). This makes it a standards organization.\nEach country should have an accessible Aeronautical Information Publication (AIP), based on standards defined by ICAO, containing information essential to air navigation. Countries are required to update their AIP manuals every 28 days and so provide definitive regulations, procedures and information for each country about airspace and airports. ICAO's standards also dictate that temporary hazards to aircraft must be regularly published using NOTAMs.\nICAO defines an International Standard Atmosphere (also known as ICAO Standard Atmosphere), a model of the standard variation of pressure, temperature, density, and viscosity with altitude in the Earth's atmosphere. This is useful in calibrating instruments and designing aircraft. The standardized pressure is also used in calibrating instruments in-flight, particularly above the transition altitude.\nICAO is active in infrastructure management, including communication, navigation and surveillance / air traffic management (CNS/ATM) systems, which employ digital technologies (like satellite systems with various levels of automation) in order to maintain a seamless global air traffic management system.\nPassport standards.\nICAO has published standards for machine-readable passports. Machine-readable passports have an area where some of the information otherwise written in textual form is also written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition, which enables border controllers and other law enforcement agents to process such passports more quickly without having to enter the information manually into a computer.\nICAO's technical standard for machine-readable passports is contained in Document 9303 \"Machine Readable Travel Documents\".\nA more recent standard covers biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smart cards. Like some smart cards, the passport book design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.\nRegistered codes.\nBoth ICAO and IATA have their own airport and airline code systems.\nAirport codes.\nICAO uses 4-letter airport codes (vs. IATA's 3-letter codes). The ICAO code is based on the region and country of the airport\u2014for example, Charles de Gaulle Airport has an ICAO code of LFPG, where \"L\" indicates Southern Europe, \"F\", France, \"PG\", Paris de Gaulle, while Orly Airport has the code LFPO (the 3rd letter sometimes refers to the particular flight information region (FIR) or the last two may be arbitrary). In most parts of the world, ICAO and IATA codes are unrelated; for example, Charles de Gaulle Airport has an IATA code of CDG. However, the location prefix for the continental United States is \"K\", and ICAO codes are usually the IATA code with this prefix. For example, the ICAO code for Los Angeles International Airport is KLAX. Canada follows a similar pattern, where a prefix of \"C\" is usually added to an IATA code to create the ICAO code. For example, Calgary International Airport is YYC or CYYC. (In contrast, airports in Hawaii are in the Pacific region and so have ICAO codes that start with \"PH\"; Kona International Airport's code is PHKO. Similarly, airports in Alaska have ICAO codes that start with \"PA\". Merrill Field, for instance is PAMR.) Not all airports are assigned codes in both systems; for example, airports that do not have airline service do not need an IATA code.\nAirline codes.\nICAO also assigns three-letter airline codes versus the more-familiar two-letter IATA codes\u2014for example, \"UAL\" vs. \"UA\" for United Airlines. ICAO also provides telephony designators to aircraft operators worldwide, a one- or two-word designator used on the radio, usually, but not always, similar to the aircraft operator name. For example, the identifier for Japan Airlines International is \"JAL\" and the designator is \"Japan Air\", but Aer Lingus is \"EIN\" and \"Shamrock\". Thus, a Japan Airlines flight numbered 111 would be written as \"JAL111\" and pronounced \"Japan Air One One One\" on the radio, while a similarly numbered Aer Lingus would be written as \"EIN111\" and pronounced \"Shamrock One One One\". In the US, FAA practices require the digits of the flight number to be spoken in group format (\"Japan Air One Eleven\" in the above example) while individual digits are used for the aircraft tail number used for unscheduled civil flights.\nAircraft registrations.\nICAO maintains the standards for aircraft registration, including the alphanumeric codes that identify the country of registration.\nAircraft type designators.\nICAO is also responsible for issuing two to four character alphanumeric aircraft type designators for those aircraft types which are most commonly provided with air traffic service. These codes provide an abbreviated aircraft type identification, typically used in flight plans. For example, the Boeing 747-100, -200 and -300 are given the type designators \"B741\", \"B742\" and \"B743\" respectively.\nInternational System of Units.\nSince 2010, ICAO recommends a unification of units of measurement within aviation based on the International System of Units (SI), using:\nNon-SI units have been permitted for temporary use since 1979, but a termination date has not yet been established, which would complete metrication of worldwide aviation, and the following units are still in widespread use within commercial aviation:\ninches of mercury are used in Japan and North America to measure pressure, although sometimes METAR at Japanese airports show only hPa.\nAviation in Russia and China currently uses km/h for reporting airspeed, and many present-day European glider planes also indicate airspeed in kilometres per hour. China and North Korea use metres for reporting altitude when communicating with pilots. Russia also formerly used metres exclusively for reporting altitude, but in 2011 changed to feet for high altitude flight. From February 2017, Russian airspace started transitioning to reporting altitude in feet only. Runway lengths are now commonly given in metres worldwide, except in North America where feet are commonly used.\nThe following table summarizes units commonly used in flight and ground operations and their recommended replacement. A full list of recommended units can be found in annex 5 to the Convention on International Civil Aviation.\n\u2020 Altitude, elevation, height.\nRegions and regional offices.\nICAO has a headquarters, seven regional offices, and one regional sub-office:\nEnvironment.\nEmissions from international aviation are specifically excluded from the targets agreed under the Kyoto Protocol. Instead, the Protocol invites developed countries to pursue the limitation or reduction of emissions through the International Civil Aviation Organization. ICAO's environmental committee continues to consider the potential for using market-based measures such as trading and charging, but this work is unlikely to lead to global action. It is currently developing guidance for states who wish to include aviation in an emissions trading scheme (ETS) to meet their Kyoto commitments, and for airlines who wish to participate voluntarily in a trading scheme.\nEmissions from domestic aviation are included within the Kyoto targets agreed by countries. This has led to some national policies such as fuel and emission taxes for domestic air travel in the Netherlands and Norway, respectively. Although some countries tax the fuel used by domestic aviation, there is no duty on kerosene used on international flights.\nICAO is currently opposed to the inclusion of aviation in the European Union Emission Trading Scheme (EU ETS). The EU, however, is pressing ahead with its plans to include aviation.\nICAO has been called \"flawed and biased in favour of the industry\" by Jo Dardenne, the manager for aviation at Transport &amp; Environment.\nCarbon Offsetting and Reduction Scheme for International Aviation.\nOn 6 October 2016, the ICAO finalized an agreement among its 191 member nations to address the more than of carbon dioxide emitted annually by international passenger and cargo flights. The agreement will use an offsetting scheme called CORSIA (the Carbon Offsetting and Reduction Scheme for International Aviation) under which forestry and other carbon-reducing activities are directly funded, amounting to about 2% of annual revenues for the sector. Rules against 'double counting' should ensure that existing forest protection efforts are not recycled. The scheme did not take effect until 2021 and will be voluntary until 2027, but many countries, including the US and China, have promised to begin at its 2020 inception date. Under the agreement, the global aviation emissions target is a 50% reduction by 2050 relative to 2005. NGO reaction to the deal was mixed.\nThe agreement has critics. It is not aligned with the 2015 Paris climate agreement, which set the objective of restricting global warming to 1.5 to 2\u00a0\u00b0C. A late draft of the agreement would have required the air transport industry to assess its share of global carbon budgeting to meet that objective, but the text was removed in the agreed version. CORSIA will regulate only about 25 percent of aviation's international emissions, since it grandfathers all emissions below the 2020 level, allowing unregulated growth until then. Only 65 nations will participate in the initial voluntary period, not including significant emitters Russia, India and perhaps Brazil. The agreement does not cover domestic emissions, which are 40% of the global industry's overall emissions. One observer of the ICAO convention made this summary: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Airline claims that flying will now be green are a myth. Taking a plane is the fastest and cheapest way to fry the planet and this deal won't reduce demand for jet fuel one drop. Instead offsetting aims to cut emissions in other industries, although another critic called it \"a timid step in the right direction.\"\nICAO has expressed strong opposition to 2025 proposals from various UN bodies and international organizations, including the International Monetary Fund, suggesting new taxes or levies on international aviation to fund global climate initiatives. ICAO contends that such measures could undermine CORSIA, emphasizing that CORSIA is the sole global framework for addressing international aviation emissions and warning that additional levies could disrupt its implementation and the broader goal of sustainable aviation development. While acknowledging the need for increased climate finance, ICAO urged stakeholders to support existing mechanisms like CORSIA rather than introducing potentially conflicting financial measures.\nAir quality.\nEmissions limits for aircraft engines are defined by the Annex 16, Volume 2 of the ICAO Technology Standards, they include standards for hydrocarbons, carbon monoxide, NOx, smoke and particulate matter for local air quality near airports, below . The first ICAO emissions regulation was adopted in 1981, and more stringent NOx standards were subsequently adopted: CAEP/2 in 1993, CAEP/4 in 1999, CAEP/6 in 2005 and CAEP/8 in 2011. Higher bypass ratios, lean burn and Rich Quick Quench Lean combustor design can reduce NOx emissions.\nInvestigations of air disasters.\nMost air accident investigations are carried out by an agency of a country that is associated in some way with the accident. For example, the Air Accidents Investigation Branch conducts accident investigations on behalf of the British Government. ICAO has conducted four investigations involving air disasters, of which two were passenger airliners shot down while in international flight over hostile territory.\nDrone regulations and registration.\nICAO is looking at having a singular ledger for drone registration to help law enforcement globally. Currently, ICAO is responsible for creating drone regulations across the globe, and it is expected that it will only maintain the registry. This activity is seen as a forerunner to global regulations on flying drones under the auspices of the ICAO.\nICAO currently maintains the 'UAS Regulation Portal' for various countries to list their country's UAS regulations and also review the best practices from across the globe.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14986", "revid": "3159626", "url": "https://en.wikipedia.org/wiki?curid=14986", "title": "International Maritime Organization", "text": "Specialized agency of the United Nations\nThe International Maritime Organization (IMO; ; ) is a specialized agency of the United Nations regulating maritime transport. It was established following agreement at a UN conference held in Geneva in 1948, but this did not come into force for ten years, and the new body, then called the Inter-governmental Maritime Consultative Organization, first assembled on 6 January 1959. Headquartered in London, United Kingdom, the IMO has 176 Member States and three Associate Members as of 2025.\nThe IMO's purpose is to develop and maintain a comprehensive regulatory framework for shipping and its remit includes maritime safety, environmental concerns, and legal matters. IMO is governed by an assembly of members which meets every two years. Its finance and organization is administered by a council of 40 members elected from the assembly. The work of IMO is conducted through five committees supported by technical subcommittees. Other UN organizations may observe the proceedings of the IMO. Observer status is granted to qualified NGOs.\nIMO is supported by a permanent secretariat of employees who are representative of the organization's members. The secretariat is composed of a Secretary-General elected by the assembly, and various divisions such as those for marine safety, environmental protection and a conference section.\nHistory.\nIn February\u2013March 1948 the United Nations Maritime Conference in Geneva institutionalized the regulation of the safety of shipping into an international framework. Hitherto such international conventions had been initiated piecemeal, notably the Safety of Life at Sea Convention (SOLAS), adopted in 1914 following the \"Titanic\" disaster. The conference resolved \"that an international organization to be known as the Intergovernmental Maritime Consultative Organization shall be established\"; however many countries did not trust the convention, so ratification was slow and it took until March 1958 until it came into force.\nThe \"Inter-Governmental Maritime Consultative Organization\" (IMCO; now using the hyphen) held its first Assembly in London in January 1959. Its initial task was to update the SOLAS; the resulting 1960 International Convention for the Safety of Life at Sea was recast in 1974 and subsequently modified and updated to adapt to changes in safety requirements and technology. Since 1978, every last Thursday of September has been celebrated as World Maritime Day.\nWhen IMCO began its operations in 1959 certain other pre-existing conventions were brought under its aegis, most notably the International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) from 1954. Under the guidance of IMCO, the convention was amended in 1962, 1969, and 1971.\nAs oil trade and industry developed, many people in the industry saw a need for further improvements in regards to pollution prevention. This became increasingly apparent in 1967, when the tanker \"Torrey Canyon\" ran aground entering the English Channel and spilled record-breaking 120,000 tons of crude oil. This incident prompted a series of new conventions.\nIMO held an emergency session of its council to deal with the need to readdress regulations pertaining to maritime pollution. In 1969, the IMO Assembly decided to host an international gathering in 1973 dedicated to this issue. The goal was to develop an international agreement for controlling general environmental contamination by ships when out at sea. During the next few years IMO brought to the forefront a series of measures designed to prevent large ship accidents and to minimize their effects. It also detailed how to deal with the environmental threat caused by routine ship duties such as the cleaning of oil cargo tanks or the disposal of engine room wastes. By tonnage, the aforementioned was a bigger problem than accidental pollution. The most significant development to come out of this conference was the International Convention for the Prevention of Pollution from Ships, 1973 (MARPOL). It covers not only accidental and operational oil pollution but also different types of pollution by chemicals, goods in packaged form, sewage, garbage and air pollution. The original MARPOL was signed on 17 February 1973, but did not come into force due to lack of ratifications. The current convention is a combination of 1973 Convention and the 1978 Protocol. It entered into force on 2 October 1983. As of January 2018, 156 states, representing 99.42 per cent of the world's shipping tonnage, are signatories to the MARPOL convention.\nAs well as updates to MARPOL and SOLAS, the IMO facilitated several updated international maritime conventions, including the International Convention on Load Lines in 1966 (replacing an earlier 1930 Convention), the International Regulations for Preventing Collisions at Sea in 1972 (also replacing an earlier set of rules) and the STCW Convention in 1978. In 1975, the assembly of the IMO decided that future conventions of the International Convention for the Safety of Life at Sea (SOLAS) and other IMO instruments should use SI units. Sea transportation is one of few industrial areas that still commonly uses non-metric units such as the nautical mile (nmi) for distance and knots (kn) for speed or velocity.\nIn November 1975, the IMCO Assembly, as a part of comprehensive review of the Convention, decided to rename it as the \"International Maritime Organization\" (IMO); after ratifications, this happened in May 1982. Throughout its existence, the IMO has continued to produce new and updated conventions across a wide range of maritime issues covering not only safety of life and marine pollution but also encompassing safe navigation, search and rescue, wreck removal, tonnage measurement, liability and compensation, ship recycling, the training and certification of seafarers, and piracy. More recently SOLAS has been amended to bring an increased focus on maritime security through the International Ship and Port Facility Security (ISPS) Code. The IMO has also increased its focus on smoke emissions from ships. In 1983, the IMO established the World Maritime University in Malm\u00f6, Sweden and also facilitated the adoption of the IGC Code. In 1991, the IMO facilitated the adoption of the International Grain Code.\nIn December 2002, new amendments to the 1974 SOLAS Convention were enacted by the IMO. These gave rise to the International Ship and Port Facility Security (ISPS) Code, which went into effect on 1 July 2004. The concept of the code is to provide layered and redundant defences against smuggling, terrorism, piracy, stowaways, etc. The ISPS Code required most ships and port facilities engaged in international trade to establish and maintain strict security procedures as specified in ship and port specific Ship Security Plans and Port Facility Security Plans.\nOn 1 January 2017, the IMO introduced a new mandatory International Code for Ships Operating in Polar Waters (Polar Code), covering shipping operations in the polar regions of Earth.\nHeadquarters.\nThe IMO headquarters is a large purpose-built building facing the River Thames on the Albert Embankment, in Lambeth, London. The organization moved into its new headquarters in late 1982, with the building being officially opened by Queen Elizabeth II on 17 May 1983. The architects of the building were Douglass Marriott, Worby &amp; Robinson. The front of the building is dominated by a seven-metre high, ten-tonne bronze sculpture of the bow of a ship, with a lone seafarer maintaining a look-out. The previous headquarters of IMO were at 101 Piccadilly (now the home of the Embassy of Japan), prior to that at 22 Berners Street in Fitzrovia and originally in Chancery Lane.\nStructure.\nThe IMO consists of an Assembly, a Council and five main Committees. The organization is led by a Secretary-General. A number of Sub-Committees support the work of the main technical committees.\nGovernance of IMO.\nThe governing body of the International Maritime Organization is the Assembly which meets every two years. In between Assembly sessions a Council, consisting of 40 Member States elected by the Assembly, acts as the governing body. The technical work of the International Maritime Organization is carried out by a series of Committees. The Secretariat consists of some 300 international civil servants headed by a Secretary-General.\nThe Secretary-General Arsenio Dominguez took office for a four year term on 1 January 2024, having been elected in July 2023. The previous Secretary-General was Kitack Lim from South Korea elected for a four-year term at the 114th session of the IMO Council in June 2015 and at the 29th session of the IMO's Assembly in November 2015. His mandate started on 1 January 2016. At the 31st session of the Assembly in 2019 he was re-appointed for a second term, ending on 31 December 2023.\nTechnical committees.\nThe technical work of the International Maritime Organization is carried out by five principal Committees:\nMaritime Safety Committee.\nIt is regulated in the Article 28(a) of the Convention on the IMO:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;ARTICLE 28\n(a) The Maritime Safety Committee shall consider any matter within the scope of the Organization concerned with aids to navigation, construction and equipment of vessels, manning from a safety standpoint, rules for the prevention of collisions, handling of dangerous cargoes, maritime safety procedures and requirements, hydrographic information, log-books and navigational records, marine casualty investigation, salvage and rescue, and any other matters directly affecting maritime safety.\n(b) The Maritime Safety Committee shall provide machinery for performing any duties assigned to it by this Convention, the Assembly or the Council, or any duty within the scope of this Article which may be assigned to it by or under any other international instrument and accepted by the Organization.\n(c) Having regard to the provisions of Article 25, the Maritime Safety Committee, upon request by the Assembly or the Council or, if it deems such action useful in the interests of its own work, shall maintain such close relationship with other bodies as may further the purposes of the Organization\nThe Maritime Safety Committee is the most senior of these and is the main Technical Committee; it oversees the work of its nine sub-committees and initiates new topics. One broad topic it deals with is the effect of the human element on casualties; this work has been put to all of the sub-committees, but meanwhile, the Maritime Safety Committee has developed a code for the management of ships which will ensure that agreed operational procedures are in place and followed by the ship and shore-side staff.\nSub-Committees.\nThe MSC and MEPC are assisted in their work by a number of sub-committees which are open to all Member States. The committees are:\nThe names of the IMO sub-committees were changed in 2013. Prior to 2013 there were nine Sub-Committees as follows:\nMembership.\nTo join the IMO, a state ratifies a multilateral Convention on the International Maritime Organization. As of 2025, there are 176 member states of the IMO, which includes 175 of the UN member states plus the Cook Islands. The first state to ratify the convention was Canada in October 1948, but it took until March 1958 when Egypt and Japan brought the number of parties to 21, required by the Convention.\nThese are the current members with the year they joined:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThe three associate members are the Faroe Islands (2002), Hong Kong (1967) and Macau (1990).\nIn 1961, the territories of Sabah and Sarawak, which had been included through the participation of United Kingdom, became joint associate members; in 1963 they became part of Malaysia.\nThe most recent new members are Armenia (landlocked, January 2018), Nauru (May 2018), Botswana (landlocked, October 2021) and, on 27 February 2024, landlocked Kyrgyzstan became the 176th Member State of the organization.\nMost UN member states that are not members of IMO are landlocked countries. These include Afghanistan, Andorra, Bhutan, Burkina Faso, Burundi, Central African Republic, Chad, Eswatini, Laos, Lesotho, Liechtenstein, Mali, Niger, Rwanda, South Sudan, Tajikistan and Uzbekistan. The Federated States of Micronesia, an island-nation in the Pacific Ocean, is also a non-member. Taiwan, as Republic of China, was a member of the IMCO from 1958 until UN changed its recognition to People's Republic of China in 1971; its later attempts to join IMO were blocked, although it has a major shipping industry.\nLegal instruments.\nIMO is the source of approximately 60 legal instruments that guide the regulatory development of its member states to improve safety at sea, facilitate trade among seafaring states and protect the maritime environment. The most well known is the International Convention for the Safety of Life at Sea (SOLAS), as well as International Convention for the Prevention of Pollution from Ships (MARPOL). Others include the International Oil Pollution Compensation Funds (IOPC). It also functions as a depository of yet to be ratified treaties, such as the International Convention on Liability and Compensation for Damage in Connection with the Carriage of Hazardous and Noxious Substances by Sea, 1996 (HNS Convention) and Nairobi International Convention of Removal of Wrecks (2007).\nIMO regularly enacts regulations, which are broadly enforced by national and local maritime authorities in member countries, such as the International Regulations for Preventing Collisions at Sea (COLREG). The IMO has also enacted a Port state control (PSC) authority, allowing domestic maritime authorities such as coast guards to inspect foreign-flag ships calling at ports of the many port states. Memoranda of Understanding (protocols) were signed by some countries unifying Port State Control procedures among the signatories.\nConventions, Codes and Regulations:\nCurrent priorities.\nRecent initiatives at the IMO have included amendments to SOLAS, which among other things, included upgraded fire protection standards on passenger ships, the International Convention on Standards of Training, Certification and Watchkeeping for Seamen (STCW) which establishes basic requirements on training, certification and watchkeeping for seafarers and to the Convention on the Prevention of Maritime Pollution (MARPOL 73/78), which required double hulls on all tankers.\nEnvironmental issues.\nGHG emissions.\nThe IMO has a role in tackling international climate change. The First Intersessional Meeting of IMO's Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway (June 2008), tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC). The IMO participated in the 2015 United Nations Climate Change Conference in Paris seeking to establish itself as the \"appropriate international body to address greenhouse gas emissions from ships engaged in international trade\". Nonetheless, there has been widespread criticism of the IMO's relative inaction since the conclusion of the Paris conference, with the initial data-gathering step of a three-stage process to reduce maritime greenhouse emissions expected to last until 2020. In 2018, the Initial IMO Strategy on the reduction of GHG emissions from ships was adopted. \nIn April 2025, the IMO's Marine Environment Protection Committee (MEPC) approved net-zero regulations for the global shipping industry to reach net-zero GHG emissions in the shipping industry by or around 2050. The changes would require that from 2028, shipowners would be required to use cleaner fuels or face a carbon pricing mechanism. However, in October 2025, at a 2nd extraordinary session, adoption of the amendments to MARPOL Annex VI to bring the regulations into force were delayed to a future session.\nBallast water management.\nThe IMO has also taken action to mitigate the global effects of ballast water and sediment discharge, through the 2004 Ballast Water Management Convention, which entered into force in September 2017.\nBiofouling.\nIn April 2025, at the IMO MEPC 83 meeting, the IMO agreed to develop a legally binding framework for controlling and managing ships\u2019 biofouling to reduce the accumulation of marine organisms on the hulls of ships and thereby reduce the transfer of invasive aquatic species. Controlling ship's biofouling also improves the environmental efficiency of ships by reducing drag resistance.\nMaritime safety.\nThe IMO's e-Navigation system has harmonized marine navigation systems with supporting shore services, as available to seamen and shore-side traffic services called. An e-Navigation strategy was ratified in 2005, and an implementation plan was developed through three IMO sub-committees. The plan was completed by 2014 and implemented in November of that year. \nOn 1 January 2011, ECDIS (Electronic Chart Display Information Systems) were made mandatory for new ships and for existing ships subject to a phased update process which was completed on 1 July 2018.\nIn December 2023, the IMO adopted a resolution targeting \"Shadow fleet\" (\"dark fleet\") tankers that form a risk by undertaking illegal and unsafe activities at sea. Primarily working for Iran and Russia to breach international sanctions, the tankers, many of which are elderly and unreliable, often undertake mid ocean transfers in an attempt to evade sanctions. The resolution calls upon flag states to \u201cadhere to measures which lawfully prohibit or regulate\u201d the transfer of cargoes at sea, known as ship-to-ship transfers.\nIn June 2025, the IMO adopted amendments to SOLAS Regulation V/23 on improving pilot ladder safety, including associated new Performance Standards for pilot transfer arrangements (to take effect 1 January 2028).\nFishing safety.\nThe IMO Cape Town Agreement is an international International Maritime Organization legal instrument established in 2012, that sets out minimum safety requirements for fishing vessels of 24 metres in length and over or equivalent in gross tons. As of 2022, the Agreement is not yet in force but the IMO is encouraging more member States to ratify the Agreement.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n "}
{"id": "14987", "revid": "50939383", "url": "https://en.wikipedia.org/wiki?curid=14987", "title": "International Labour Organization", "text": "Specialized agency of the United Nations\nThe International Labour Organization (ILO) is a United Nations agency whose mandate is to advance social and economic justice by setting international labour standards. Founded in October 1919 under the League of Nations, it is one of the first and oldest specialized agencies of the UN. The ILO has 187 member states: 186 out of 193 UN member states plus the Cook Islands. It is headquartered in Geneva, Switzerland, with around 40 field offices around the world, and employs some 3,381 staff across 107 nations, of whom 1,698 work in technical cooperation programmes and projects.\nThe ILO's standards are aimed at ensuring accessible, productive, and sustainable work worldwide in conditions of freedom, equity, security and dignity. They are set forth in 189 conventions and treaties, of which eight are classified as fundamental according to the 1998 Declaration on Fundamental Principles and Rights at Work; together they protect freedom of association and the effective recognition of the right to collective bargaining, the elimination of forced or compulsory labour, the abolition of child labour, and the elimination of discrimination in respect of employment and occupation. The ILO is a major contributor to international labour law.\nWithin the UN system the organization has a unique tripartite structure: all standards, policies, and programmes require discussion and approval from the representatives of governments, employers, and workers. This framework is maintained in the ILO's three main bodies: The International Labour Conference, which meets annually to formulate international labour standards; the Governing Body, which serves as the executive council and decides the agency's policy and budget; and the International Labour Office, the permanent secretariat that administers the organization and implements activities. The secretariat is led by the Director-General, Gilbert Houngbo of Togo, who was elected by the Governing Body in 2022.\nIn 2019, the organization convened the Global Commission on the Future of Work, whose report made ten recommendations for governments to meet the challenges of the 21st century labour environment; these include a universal labour guarantee, social protection from birth to old age and an entitlement to lifelong learning. With its focus on international development, it is a member of the United Nations Development Group, a coalition of UN organizations aimed at helping meet the Sustainable Development Goals.\nTwo milestones in the history of the ILO were the Treaty of Versailles in 1919, establishing the International Labour Organization, Article 427. And secondly, the Declaration of Philadelphia in 1944, reestablishing the ILO under the United Nations and reaffirming the first principle that \"labour is not a commodity\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nStructure.\nThe ILO is a specialized agency of the United Nations (UN). As with other UN specialized agencies (or programmes) working on international development, the ILO is also a member of the United Nations Development Group.\nUnlike other United Nations specialized agencies, the International Labour Organization (ILO) has a tripartite governing structure that brings together governments, employers, and workers of 187 member States, to set labour standards, develop policies and devise programmes promoting decent work for all women and men. The structure is intended to ensure the views of all three groups are reflected in ILO labour standards, policies, and programmes, though governments have twice as many representatives as the other two groups.\nGoverning body.\nThe Governing Body is the executive body of the International Labour Organization. It meets three times a year, in March, June and November. It takes decisions on ILO policy, decides the agenda of the International Labour Conference, adopts the draft Programme and Budget of the Organization for submission to the Conference, elects the Director-General, requests information from the member states concerning labour matters, appoints commissions of inquiry and supervises the work of the International Labour Office.\nThe Governing Body is composed of 56 titular members (28 governments, 14 employers and 14 workers) and 66 deputy members (28 governments, 19 employers and 19 workers).\nTen of the titular government seats are permanently held by States of chief industrial importance: Brazil, China, France, Germany, India, Italy, Japan, the Russian Federation, the United Kingdom and the United States. The other Government members are elected by the Conference every three years (the last elections were held in June 2021). The Employer and Worker members are elected in their individual capacity.\nDirector-General.\nOn 25 March 2022 Gilbert Fossoun Houngbo was elected Director-General of ILO. On 1 October 2022 he succeeded Guy Ryder, who was elected by the ILO Governing Body in October 2012, and re-elected for a second five-year-term in November 2016. He is the organization's first African Director-General. In 2024, a group of African countries that play a crucial role visited the International Labour Office, such as Morocco, where they held talks with the Minister of Economic Integration, Small Business, Employment, and Skills Development, Younes Sekouri.\nThe list of the Directors-General of ILO since its establishment in 1919 is as follows:\nMembership.\nThe ILO has 187 state members. 186 of the 193 member states of the United Nations plus the Cook Islands are members of the ILO. The UN member states which are not members of the ILO are Andorra, Bhutan, Liechtenstein, Micronesia, Monaco, Nauru, and North Korea.\nThe ILO constitution permits any member of the UN to become a member of the ILO. To gain membership, a nation must inform the director-general that it accepts all the obligations of the ILO constitution. Other, non-UN states can be admitted by a two-thirds vote of all delegates, including a two-thirds vote of government delegates, at any ILO General Conference. The Cook Islands, a non-UN state, joined in June 2015.\nCountries that had been members of the ILO under the League of Nations remained members when the organization's new constitution came into effect in 1946.\nObjectives.\nThe Declaration of Philadelphia (10 May 1944) restated the traditional objectives of the International Labour Organization and then branched out in two new directions: the centrality of human rights to social policy, and the need for international economic planning. With the end of the world war in sight, it sought to adapt the guiding principles of the ILO \"to the new realities and to the new aspirations aroused by the hopes for a better world.\" It was adopted at the 26th Conference of the ILO in Philadelphia, Pennsylvania. \nIn 1946, when the ILO's constitution was being revised by the General Conference convened in Montreal, the Declaration of Philadelphia was annexed to the constitution and forms an integral part of it by Article 1. Most of the demands of the declaration were a result of a partnership of American and Western European labor unions and the ILO secretariat.\n\"Labour is not a commodity\" is the principle expressed in the preamble to the International Labour Organization's founding documents. It expresses the view that people should not be treated like inanimate commodities, capital, another mere factor of production, or resources. Instead, people who work for a living should be treated as human beings and accorded dignity and respect. Paul O'Higgins attributes the phrase to John Kells Ingram, who used it in 1880 during a meeting in Dublin of the British Trades Union Congress.\nHistory.\nOrigins.\nIt is the first organization for the UN.While the ILO was established as an agency of the League of Nations following World War I, its founders had made great strides in social thought and action before 1919. The core members all knew one another from earlier private professional and ideological networks, in which they exchanged knowledge, experiences, and ideas on social policy. Pre-war \"epistemic communities\", such as the International Association for Labour Legislation (IALL), founded in 1900, and political networks, such as the socialist Second International, were a decisive factor in the institutionalization of international labour politics.\nIn the post-World War I euphoria, the idea of a \"makeable society\" was an important catalyst behind the social engineering of the ILO architects. As a new discipline, international labour law became a useful instrument for putting social reforms into practice. The utopian ideals of the founding members\u2014social justice and the right to decent work\u2014were changed by diplomatic and political compromises made at the Paris Peace Conference of 1919, showing the ILO's balance between idealism and pragmatism.\nOver the course of the First World War, the international labour movement proposed a comprehensive programme of protection for the working classes, conceived as compensation for labour's support during the war. Post-war reconstruction and the protection of labour unions occupied the attention of many nations during and immediately after World War I. In Great Britain, the Whitley Commission, a subcommittee of the Reconstruction Commission, recommended in its July 1918 Final Report that \"industrial councils\" be established throughout the world. The British Labour Party had issued its own reconstruction programme in the document titled \"Labour and the New Social Order\". In February 1918, the third Inter-Allied Labour and Socialist Conference (representing delegates from Great Britain, France, Belgium and Italy) issued its report, advocating an international labour rights body, an end to secret diplomacy, and other goals. And in December 1918, the American Federation of Labor (AFL) issued its own distinctively apolitical report, which called for the achievement of numerous incremental improvements via the collective bargaining process.\nIFTU Bern Conference.\nAs the war drew to a close, two competing visions for the post-war world emerged. The first was offered by the International Federation of Trade Unions (IFTU), which called for a meeting in Bern, Switzerland, in July 1919. The Bern meeting would consider both the future of the IFTU and the various proposals which had been made in the previous few years. The IFTU also proposed including delegates from the Central Powers as equals. Samuel Gompers, president of the AFL, boycotted the meeting, wanting the Central Powers delegates in a subservient role as an admission of guilt for their countries' role in bringing about war. Instead, Gompers favoured a meeting in Paris which would consider President Woodrow Wilson's Fourteen Points only as a platform. Despite the American boycott, the Bern meeting went ahead as scheduled. In its final report, the Bern Conference demanded an end to wage labour and the establishment of socialism. If these ends could not be immediately achieved, then an international body attached to the League of Nations should enact and enforce legislation to protect workers and trade unions.\nCommission on International Labour Legislation.\nMeanwhile, the Paris Peace Conference sought to dampen public support for communism. Subsequently, the Allied Powers agreed that clauses should be inserted into the emerging peace treaty protecting labour unions and workers' rights, and that an international labour body be established to help guide international labour relations in the future. The advisory Commission on International Labour Legislation was established by the Peace Conference to draft these proposals. The Commission met for the first time on 1 February 1919, and Gompers was elected as the chairman.\nTwo competing proposals for an international body emerged during the Commission's meetings. The British proposed establishing an international parliament to enact labour laws which each member of the League would be required to implement. Each nation would have two delegates to the parliament, one each from labour and management. An international labour office would collect statistics on labour issues and enforce the new international laws. Philosophically opposed to the concept of an international parliament and convinced that international standards would lower the few protections achieved in the United States, Gompers proposed that the international labour body be authorized only to make recommendations and that enforcement be left up to the League of Nations. Despite vigorous opposition from the British, the American proposal was adopted.\nGompers also set the agenda for the draft charter protecting workers' rights. The Americans made 10 proposals. Three were adopted without change: That labour should not be treated as a commodity; that all workers had the right to a wage sufficient to live on; and that women should receive equal pay for equal work. A proposal protecting the freedom of speech, press, assembly, and association was amended to include only freedom of association. A proposed ban on the international shipment of goods made by children under the age of 16 was amended to ban goods made by children under the age of 14. A proposal to require an eight-hour work day was amended to require the eight-hour work day \"or\" the 40-hour work week (an exception was made for countries where productivity was low). Four other American proposals were rejected. Meanwhile, international delegates proposed three additional clauses, which were adopted: One or more days for weekly rest; equality of laws for foreign workers; and regular and frequent inspection of factory conditions.\nThe Commission issued its final report on 4 March 1919, and the Peace Conference adopted it without amendment on 11 April. The report became Part XIII of the Treaty of Versailles.\nInterwar period.\nThe first annual International Labour Conference (ILC) began on 29 October 1919 at the Pan American Union Building in Washington, D.C. and adopted the first six International Labour Conventions, which dealt with hours of work in industry, unemployment, maternity protection, night work for women, minimum age, and night work for young persons in industry. The prominent French socialist Albert Thomas became its first director-general.\nDespite open disappointment and sharp critique, the revived International Federation of Trade Unions (IFTU) quickly adapted itself to this mechanism. The IFTU increasingly oriented its international activities around the lobby work of the ILO.\nAt the time of establishment, the U.S. government was not a member of ILO, as the US Senate rejected the covenant of the League of Nations, and the United States could not join any of its agencies. Following the election of Franklin Delano Roosevelt to the U.S. presidency, the new administration made renewed efforts to join the ILO without league membership. On 19 June 1934, the U.S. Congress passed a joint resolution authorizing the president to join ILO without joining the League of Nations as a whole. On 22 June 1934, the ILO adopted a resolution inviting the U.S. government to join the organization. On 20 August 1934, the U.S. government responded positively and took its seat at the ILO.\nWartime and the United Nations.\nDuring the Second World War, when Switzerland was surrounded by German troops, ILO director John G. Winant made the decision to leave Geneva. In August 1940, the government of Canada officially invited the ILO to be housed at McGill University in Montreal. Forty staff members were transferred to the temporary offices and continued to work from McGill until 1948.\nThe ILO became the first specialized agency of the United Nations system after the demise of the League in 1946. Its constitution, as amended, includes the Declaration of Philadelphia (1944) on the aims and purposes of the organization.\nCold War era.\nBeginning in the late 1950s the organization was under pressure to make provisions for the potential membership of ex-colonies which had become independent; in the Director General's report of 1963 the needs of the potential new members were first recognized. The tensions produced by these changes in the world environment negatively affected the established politics within the organization and they were the precursor to the eventual problems of the organization with the USA.\nIn July 1970, the United States withdrew 50% of its financial support to the ILO following the appointment of an assistant director-general from the Soviet Union. This appointment (by the ILO's British director-general, C. Wilfred Jenks) drew particular criticism from AFL\u2013CIO president George Meany and from New Jersey Assemblyman John E. Rooney. However, the funds were eventually paid.\nOn 12 June 1975, the ILO voted to grant the Palestine Liberation Organization observer status at its meetings. Representatives of the United States and Israel walked out of the meeting. The U.S. House of Representatives subsequently decided to withhold funds. The United States gave notice of full withdrawal on 6 November 1975, stating that the organization had become politicized. The United States also suggested that representation from communist countries was not truly \"tripartite\"\u2014including government, workers, and employers\u2014because of the structure of these economies. The withdrawal became effective on 1 November 1977.\nThe United States returned to the organization in 1980 after extracting some concession from the organization. It was partly responsible for the ILO's shift away from a human rights approach and towards support for the Washington Consensus. Economist Guy Standing wrote \"the ILO quietly ceased to be an international body attempting to redress structural inequality and became one promoting employment equity\".\nIn 1981, the government of Poland declared martial law. It interrupted the activities of Solidarno\u015b\u0107 detained many of its leaders and members. The ILO Committee on Freedom of Association filed a complaint against Poland at the 1982 International Labour Conference. A Commission of Inquiry established to investigate found Poland had violated ILO Conventions No. 87 on freedom of association and No. 98 on trade union rights, which the country had ratified in 1957. The ILO and many other countries and organizations put pressure on the Polish government, which finally gave legal status to Solidarno\u015b\u0107 in 1989. During that same year, there was a roundtable discussion between the government and Solidarno\u015b\u0107 which agreed on terms of relegalization of the organization under ILO principles. The government also agreed to hold the first free elections in Poland since the Second World War.\nOffices.\nILO headquarters.\nThe ILO is headquartered in Geneva, Switzerland. In its first months of existence in 1919, it offices were located in London, only to move to Geneva in the summer 1920. The first seat in Geneva was on the Pregny hill in the \"Ariana\" estate, in the building that used to host the Thudicum boarding school and currently the headquarters of the International Committee of the Red Cross. As the office grew, the Office relocated to a purpose-built headquarters by the shores of lake Leman, designed by Georges \u00c9pitaux and inaugurated in 1926 (currently the seat of the World Trade Organization). During the Second World War the Office was temporarily relocated to McGill University in Montreal, Canada.\nThe current seat of the ILO's headquarters is located on the Pregny hill, not far from its initial seat. The building, a biconcave rectangular block designed by Eug\u00e8ne Beaudoin, Pier Luigi Nervi and Alberto Camenzind, was purpose-built between 1969\u20131974 in a severe rationalist style and, at the time of construction, constituted the largest administrative building in Switzerland.\nSub-regional offices.\nCalled \"Decent Work Technical Support Teams (DWT)\", they provide technical support to the work of a number of countries under their area of competence.\nActivities.\nConventions.\nThrough July 2018, the ILO had adopted 189 conventions. If these conventions are ratified by enough governments, they come in force. However, ILO conventions are considered international labour standards regardless of ratification. When a convention comes into force, it creates a legal obligation for ratifying nations to apply its provisions.\nEvery year the International Labour Conference's Committee on the Application of Standards examines a number of alleged breaches of international labour standards. Governments are required to submit reports detailing their compliance with the obligations of the conventions they have ratified. Conventions that have not been ratified by member states have the same legal force as recommendations.\nIn 1998, the 86th International Labour Conference adopted the \"Declaration on Fundamental Principles and Rights at Work\". This declaration contains four fundamental policies:\nThe ILO asserts that its members have an obligation to work towards fully respecting these principles, embodied in relevant ILO conventions. The ILO conventions that embody the fundamental principles have now been ratified by most member states.\nProtocols are always linked to Conventions, even though they are international treaties they do not exist on their own. As with Conventions, Protocols can be ratified.\nRecommendations do not have the binding force of conventions and are not subject to ratification. Recommendations may be adopted at the same time as conventions to supplement the latter with additional or more detailed provisions. In other cases recommendations may be adopted separately and may address issues separate from particular conventions.\nInternational Labour Conference.\nOnce a year, the ILO organizes the International Labour Conference (ILC) in Geneva to set the broad policies of the ILO, including conventions and recommendations. Also known as the \"international parliament of labour\", the conference makes decisions about the ILO's general policy, work programme and budget and also elects the Governing Body.\nThe first conference took place in 1919: see Interwar period above.\nEach member state is represented by a delegation composed of two government delegates, an employer delegate, a worker delegate. All of them have individual voting rights and all votes are equal, regardless of the population of the delegate's member State. The employer and worker delegates are normally chosen in agreement with the most representative national organizations of employers and workers. Usually, the workers and employers' delegates coordinate their voting. All delegates have the same rights and are not required to vote in blocs.\nDelegates can attend with advisers and substitute delegates, and all have the same rights: they can express themselves freely and vote as they wish. This diversity of viewpoints does not prevent decisions from being adopted by very large majorities or unanimously.\nHeads of State and prime ministers also participate in the Conference. International organizations, both governmental and others, also attend but as observers.\nThe 109th session of the International Labour Conference was delayed from 2020 to May 2021 and was held online because of the COVID-19 pandemic. The first meeting was on 20 May 2021 in Geneva for the election of its officers. Further sittings were held in June, November and December. The 110th session took place from 27 May to 11 June 2022. The 111th session of the International Labour Conference took place in June 2023.\nGlobal forum meetings.\nThe ILO organizes regular international tripartite gatherings and global dialogue fora on issues of interest to specific sectors of business and employment, for example on supply chain safety in the packing of containers for global shipping (2011), and on employment conditions in early childhood education (2012).\nLabour statistics.\nThe ILO is a major provider of labour statistics. Labour statistics are an important tool for its member states to monitor their progress toward improving labour standards. As part of their statistical work, ILO maintains several databases. This database covers 11 major data series for over 200 countries. In addition, ILO publishes a number of compilations of labour statistics, such as the Key Indicators of Labour Markets (KILM). KILM covers 20 main indicators on labour participation rates, employment, unemployment, educational attainment, labour cost, and economic performance. Many of these indicators have been prepared by other organizations. For example, the Division of International Labour Comparisons of the U.S. Bureau of Labor Statistics prepares the hourly compensation in manufacturing indicator.\nThe U.S. Department of Labor also publishes a yearly report containing a \"List of Goods Produced by Child Labor or Forced Labor\" issued by the Bureau of International Labor Affairs. The December 2014 updated edition of the report listed a total of 74 countries and 136 goods.\nThe ILO is the custodian agency for nine of the 17 indicators of Sustainable Development Goal 8 (SDG 8). This goal is about \"decent work and economic growth\". For example, ILO is the agency for Indicator 8.b.1 of Target 8.b. The wording of this target is: \"By 2020, develop and operationalize a global strategy for youth employment and implement the Global Jobs Pact of the International Labour Organization\". As such, ILO is in charge of the data gathering for the progress of the Global Youth Empowerment Strategy.\nTraining and teaching units.\nThe International Training Centre of the International Labour Organization (ITCILO) is based in Turin, Italy. Together with the University of Turin Department of Law, the ITC offers training for ILO officers and secretariat members, as well as offering educational programmes. The ITC offers more than 450 training and educational programmes and projects every year for some 11,000 people around the world.\nFor instance, the ITCILO offers a Master of Laws programme in management of development, which aims specialize professionals in the field of cooperation and development.\nResponses to issues.\nChild labour.\nChild labour is often defined as work that deprives children of their childhood, potential, dignity, and is harmful to their physical and mental development. \"Child labour\" refers to work that is mentally, physically, socially or morally dangerous and harmful to children. Further, it can involve interfering with their schooling by depriving them of the opportunity to attend school, obliging them to leave school prematurely, or requiring them to attempt to combine school attendance with excessively long and heavy work.\nThe ILO's International Programme on the Elimination of Child Labour (IPEC) was created in 1992 with the overall goal of the progressive elimination of child labour, which was to be achieved through strengthening the capacity of countries to deal with the problem and promoting a worldwide movement to combat child labour. The IPEC currently has operations in 88 countries, with an annual expenditure on technical cooperation projects that reached over US$61 million in 2008. It is the largest programme of its kind globally and the biggest single operational programme of the ILO.\nThe number and range of the IPEC's partners have expanded over the years and now include employers' and workers' organizations, other international and government agencies, private businesses, community-based organizations, NGOs, the media, parliamentarians, the judiciary, universities, religious groups and children and their families.\nThe IPEC's work to eliminate child labour is an important facet of the ILO's Decent Work Agenda. Child labour prevents children from acquiring the skills and education they need for a better future.\nThe ILO also hosts a Global Conference on the Elimination of Child Labour every four years. The most recent conference was held in Durban, South Africa from 15 to 20 May 2022.\nThe ILO has established the World Day Against Child Labour on June 12 as an annual event starting in 2002 to raise awareness and prompt action to tackle child labour worldwide. Coinciding with the Sustainable Development Goals, the event particularly targets the eradication of its worst forms, like slavery and the use of child soldiers, by 2025. The ILO distinguishes between detrimental child labour, which hinders children's development and education, and acceptable work that supports their growth and learning.\nIn 2023, the World Day's theme, 'Social Justice for All. End Child Labour!', calls for reinvigorated global efforts towards achieving social justice and underscores the critical need for the universal ratification and enforcement of ILO Conventions No. 138 and No. 182 to protect all children from child labour.\nExceptions in indigenous communities.\nBecause of different cultural views involving labour, the ILO developed a series of culturally sensitive mandates, including convention Nos. 169, 107, 138, and 182, to protect indigenous culture, traditions, and identities. Convention Nos. 138 and 182 lead in the fight against child labour, while Nos. 107 and 169 promote the rights of indigenous and tribal peoples and protect their right to define their own developmental priorities.\nIn many indigenous communities, parents believe that children learn important life lessons through the act of work and through the participation in daily life. Working is seen as a learning process preparing children of the future tasks they will eventually have to do as an adult. It is a belief that the family's and child well-being and survival is a shared responsibility between members of the whole family. They also see work as an intrinsic part of their child's developmental process. While these attitudes toward child work remain, many children and parents from indigenous communities still highly value education.\nForced labour.\nThe ILO has considered the fight against forced labour to be one of its main priorities. During the interwar years, the issue was mainly considered a colonial phenomenon, and the ILO's concern was to establish minimum standards protecting the inhabitants of colonies from the worst abuses committed by economic interests. After 1945, the goal became to set a uniform and universal standard, determined by the higher awareness gained during World War II of politically and economically motivated systems of forced labour, but debates were hampered by the Cold War and by exemptions claimed by colonial powers. Since the 1960s, declarations of labour standards as a component of human rights have been weakened by government of postcolonial countries claiming a need to exercise extraordinary powers over labour in their role as emergency regimes promoting rapid economic development.\nIn June 1998, the International Labour Conference adopted a Declaration on Fundamental Principles and Rights at Work and its follow-up that obligates member states to respect, promote and realize freedom of association and the right to collective bargaining, the elimination of all forms of forced or compulsory labour, the effective abolition of child labour, and the elimination of discrimination in respect of employment and occupation.\nIn November 2001, following the publication of the InFocus Programme's first global report on forced labour, the ILO's governing body created a special action programme to combat forced labour (SAP-FL), as part of broader efforts to promote the 1998 Declaration on Fundamental Principles and Rights at Work and its follow-up. The SAP-FL was created in November 2001 \"to tackle the elimination of all forms of forced or compulsory labour, one of its foremost concerns, through both technical assistance and promotional means.\" SAP-FL has developed indicators of forced labour practices and published survey reports on forced labour.\nIn 2013, the SAP-FL was integrated into the ILO's \"Fundamental Principles and Rights at Work Branch\" (FUNDAMENTALS) bringing together the fight against forced and child labour and working in the context of Alliance 8.7.\nOne major tool to fight forced labour was the adoption of the \"ILO Forced Labour Protocol\" by the \"International Labour Conference in 2014\". It was ratified for the second time in 2015 and on 9 November 2016 it entered into force. The new protocol brings the existing ILO Convention 29 on Forced Labour, adopted in 1930, into the modern era to address practices such as human trafficking. The accompanying Recommendation 203 provides technical guidance on its implementation.\nIn 2015, the ILO launched a global campaign to end modern slavery, in partnership with the International Organization of Employers (IOE) and the International Trade Union Confederation (ITUC). The \"50 for Freedom Campaign\" aims to mobilize public support and encourage countries to ratify the ILO's \"Forced Labour Protocol\".\nRole of civil society organizations in the International Labour Conference.\nCivil society organizations (CSOs) play a supportive and increasingly recognized role in the work of the International Labour Organization (ILO), particularly in relation to the annual International Labour Conference (ILC)\u2014the ILO\u2019s highest decision-making body. Through participation in the ILC and other ILO-led initiatives, CSOs contribute to the advancement of labor rights and the promotion of social justice on a global scale.\nThe ILO encourages engagement with CSOs as part of its tripartite approach to labor governance, which includes representatives from governments, employers, and workers. Civil society actors bring additional perspectives, often representing marginalized groups, informal workers, or specific thematic concerns (e.g., gender equity, forced labor, or child labor).\nOpportunities for participation.\nCSOs can engage with the ILO and its annual Conference through several mechanisms:\nMore information on the ILO\u2019s engagement with civil society is available at: https://www.ilo.org/partnering-development/civil-society-ilo-partnership\nMinimum wage law.\nTo protect the right of labours for fixing minimum wage, ILO has created Minimum Wage-Fixing Machinery Convention, 1928, Minimum Wage Fixing Machinery (Agriculture) Convention, 1951 and Minimum Wage Fixing Convention, 1970 as minimum wage law.\nCommercialized sex.\nFrom the moment its creation in the 1919 Treaty of Versailles, the ILO has been concerned with the controversial issue of commercial sex. Prior to the creation of the ILO and League of Nations, the issue of sex work had been exclusively under the jurisdiction of the state, now, the ILO and League of Nations believed the issue transcended borders and within their jurisdiction. In the early twentieth, commercialized sex was considered both immoral and criminal activity. Initially, the ILO strongly believed prostitution was linked to vulnerable single working women emigrating to other nations without being under the paternal supervision of a man. After the widespread destruction caused by World War I, the ILO saw prostitution as spreading contagion requiring regulation. Under the leadership of the French socialist Albert Thomas, the ILO created a medical division whose primary focus was on male sailors whose lives were viewed as \"nomadic\" and \"promiscuous\", which made these men susceptible to infection of STD's. After the conclusion of the Genoa maritime conference in 1920, the ILO proclaimed itself as the critical leader of the prevention and treatment of STD's in sailors. In the interwar years, the ILO also sought to protect female workers in dangers trades, but delegates to ILO Conferences did not consider the sex trade to be \"work\", which was conceived of as industrial labor. The ILO believed that if women worked industrial jobs, this would be a deterrent from them living immoral lives. In order to make these industrial jobs more attractive, the ILO promoted better wages and safer working conditions, both intended to prevent women from falling victim to the temptation of the sex trades.\nFollowing the creation of the United Nations, the ILO took a back seat to the newly formed organization on the issue of commercialized sex. The UN Commission on the Status of Women called for abolishing both sex trafficking and prostitution. In the 1950s, the UN Economic and Social Council and the International Police Organization sought to end any activity that resembled slavery, classifying sex trafficking and prostitution as criminal rather than labor issues. \nBeginning in 1976, the ILO and other organizations began to examine the working and living conditions of rural women in developing countries. One example the ILO investigated was the go-go bars and the growing phenomenon of \"hired wives\" in Thailand, which both thrived because the development of U.S. military bases in the region. In the late 1970s, the ILO established the \"Programme on Rural Women\", which investigated the involvement of young masseuses in the sex trade in Bangkok. It was critical because it was the first time in the history of the ILO or any of its branches that prostitution was described as a form of labor. In the decades that followed, the increase in sex tourism and the exploding AIDS epidemic strengthened ILO interest in the commercial sex trade. \nHIV/AIDS.\nThe International Labour Organization (ILO) is the lead UN-agency on HIV workplace policies and programmes and private sector mobilization. ILOAIDS is the branch of the ILO dedicated to this issue.\nThe ILO has been involved with the HIV response since 1998, attempting to prevent potentially devastating impact on labour and productivity and that it says can be an enormous burden for working people, their families and communities. In June 2001, the ILO's governing body adopted a pioneering code of practice on HIV/AIDS and the world of work, which was launched during a special session of the UN General Assembly.\nThe same year, ILO became a cosponsor of the Joint United Nations Programme on HIV/AIDS (UNAIDS).\nIn 2010, the 99th International Labour Conference adopted the ILO's recommendation concerning HIV and AIDS and the world of work, 2010 (No. 200), the first international labour standard on HIV and AIDS. The recommendation lays out a comprehensive set of principles to protect the rights of HIV-positive workers and their families, while scaling up prevention in the workplace. Working under the theme of \"Preventing HIV, Protecting Human Rights at Work\", ILOAIDS undertakes a range of policy advisory, research and technical support functions in the area of HIV and AIDS and the world of work. The ILO also works on promoting social protection as a means of reducing vulnerability to HIV and mitigating its impact on those living with or affected by HIV.\nILOAIDS ran a \"Getting to Zero\" campaign to arrive at zero new infections, zero AIDS-related deaths and zero-discrimination by 2015. Building on this campaign, ILOAIDS is executing a programme of voluntary and confidential counselling and testing at work, known as VCT@WORK.\nMigrant workers.\nAs the word \"migrant\" suggests, migrant workers refer to those who moves from one country to another to do their job. For the rights of migrant workers, the first ILC adopted a recommendation on equality and coordination, and the ILO has adopted conventions, including Migrant Workers (Supplementary Provisions) Convention, 1975 and United Nations Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families in 1990.\nDomestic workers.\nDomestic workers are those who perform a variety of tasks for and in other peoples' homes. For example, they may cook, clean the house, and look after children. Yet they are often the ones with the least consideration, excluded from labour and social protection. This is mainly due to the fact that women have traditionally carried out the tasks without pay. For the rights and decent work of domestic workers including migrant domestic workers, ILO has adopted the Convention on Domestic Workers on 16 June 2011.\nEnvironmental sustainability.\nThe ILO has been on working on integrating environmental sustainability into its activities (or \"greening\" its activities) and the broader discourse since the 1970s. For example, some of ILO's reports in 1972 to 1975 have investigated linkages between occupational safety and health, economic development and environmental protection. In the 2000s ILO began to promote a \"socially just transition to green jobs\". The organisation defined green jobs as \"decent jobs that contribute to preserving and restoring the environment\". \nSince 2017, the concept of \"just transition\" has been firmly embedded within the ILO\u2019s position. For example its \"Centenary Declaration for the Future of Work\" in 2019 stated that: \"[...] the ILO must direct its efforts to: (i) ensuring a just transition to a future of work that contributes to sustainable development in its economic, social and environmental dimensions\". A \"just transition\" focuses on the connection between energy transition and equitable approaches to decarbonization that support broader development goals.\nThe ILO has also looked at the transition to a green economy, and the impact thereof on employment. It came to the conclusion a shift to a greener economy could create 24 million new jobs globally by 2030, if the right policies are put in place. Also, if a transition to a green economy were not to take place, 72 million full-time jobs may be lost by 2030 due to heat stress, and temperature increases will lead to shorter available work hours, particularly in agriculture.\nAwards.\nIn 1969, the ILO received the Nobel Peace Prize for improving fraternity and peace among nations, pursuing decent work and justice for workers, and providing technical assistance to other developing nations. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14988", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=14988", "title": "IAEA", "text": ""}
{"id": "14989", "revid": "17482101", "url": "https://en.wikipedia.org/wiki?curid=14989", "title": "ICAO", "text": ""}
{"id": "14990", "revid": "28564", "url": "https://en.wikipedia.org/wiki?curid=14990", "title": "IMO", "text": "IMO or Imo may refer to:\nOther.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14993", "revid": "24442760", "url": "https://en.wikipedia.org/wiki?curid=14993", "title": "International Federation of the Red Cross", "text": ""}
{"id": "14996", "revid": "51038683", "url": "https://en.wikipedia.org/wiki?curid=14996", "title": "International English", "text": "English language as a global means of communication in numerous dialects\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nInternational English is the concept of using the English language as a global means of communication similar to an international auxiliary language, and often refers to the movement towards an international standard for the language. Related and sometimes synonymous terms include: Global English, World English, Continental English, General English and Common English. These terms may describe the fact that English is spoken and used in numerous dialects around the world or refer to a desired standardisation (i.e. Standard English).\nThere have been many proposals for making International English more accessible to people from different nationalities but there is no consensus; Basic English is an example, but it failed to make progress. More recently, there have been proposals for English as a lingua franca (ELF) in which non-native speakers take a highly active role in the development of the language.\nHistorical context.\nOrigins.\nThe modern concept of \"International English\" does not exist in isolation, but is the product of centuries of development of the English language.\nThe English language evolved in England, from a set of West Germanic dialects spoken by the Angles and Saxons, who arrived from continental Europe in the 5th century. Those dialects became known as \"Englisc\" (literally \"Anglish\"), the language today referred to as Anglo-Saxon or Old English (the language of the poem \"Beowulf\"). However, less than a quarter of the vocabulary of Modern English is derived from the shared ancestry with other West Germanic languages because of extensive borrowings from Norse, Norman, Latin, and other languages. It was during the Viking invasions of the Anglo-Saxon period that Old English was influenced by contact with Norse, a group of North Germanic dialects spoken by the Vikings, who came to control a large region in the North and East of England known as the Danelaw. Vocabulary items entering English from Norse (including the pronouns \"they\" and \"them\") are thus attributable to the on-again-off-again Viking occupation of Northern and Eastern England during the centuries prior to the Norman Conquest (see, e.g., Canute the Great). Soon after the Norman Conquest of 1066, the \"Englisc\" language ceased being a literary language (see, e.g., Ormulum) and was replaced by Anglo-Norman as the written language of England. During the Norman Period, English absorbed a significant component of French vocabulary (approximately one-third of the vocabulary of Modern English). With this new vocabulary, additional vocabulary borrowed from Latin (with Greek, another approximately one-third of Modern English vocabulary, though some borrowings from Latin and Greek date from later periods), a simplified grammar, and use of the orthographic conventions of French instead of Old English orthography, the language became Middle English (the language of Chaucer). The \"difficulty\" of English as a written language thus began in the High Middle Ages, when French orthographic conventions were used to spell a language whose original, more suitable orthography had been forgotten after centuries of nonuse. During the late medieval period, King Henry V of England (lived 1387\u20131422) ordered the use of the English of his day in proceedings before him and before the government bureaucracies. That led to the development of Chancery English, a standardised form used in the government bureaucracy. (The use of so-called Law French in English courts continued through the Renaissance, however.)\nThe emergence of English as a language of Wales results from the incorporation of Wales into England and also dates from approximately this time period. Soon afterward, the development of printing by Caxton and others accelerated the development of a standardised form of English. Following a change in vowel pronunciation that marks the transition of English from the medieval to the Renaissance period, the language of the Chancery and Caxton became Early Modern English (the language of Shakespeare's day) and with relatively moderate changes eventually developed into the English language of today. Scots, as spoken in the lowlands and along the east coast of Scotland, developed largely independent of Modern English, and is based on the Northern dialects of Anglo-Saxon, particularly Northumbrian, which also serve as the basis of Northern English dialects such as those of Yorkshire and Newcastle upon Tyne. Northumbria was within the Danelaw and therefore experienced greater influence from Norse than did the Southern dialects. As the political influence of London grew, the Chancery version of the language developed into a written standard across Great Britain, further progressing in the modern period as Scotland became united with England as a result of the Acts of Union of 1707.\nEnglish was introduced to Ireland twice\u2014a medieval introduction that led to the development of the now-extinct Yola and Fingallian dialects, and a modern introduction in which Hiberno-English largely replaced Irish as the most widely spoken language during the 19th century, following the Act of Union of 1800. Received Pronunciation (RP) is generally viewed as a 19th-century development and is not reflected in North American English dialects (except for an affected Transatlantic accent of the early to mid-20th century), which are based on 18th-century English.\nThe establishment of the first permanent English-speaking colony in North America in 1607 was a major step towards the globalisation of the language. British English was only partially standardised when the American colonies were established. Isolated from each other by the Atlantic Ocean, the dialects in England and the colonies began evolving independently.\nThe British colonisation of Australia starting in 1788 brought the English language to Oceania. By the 19th century, the standardisation of British English was more settled than it had been in the previous century, and this relatively well-established English was brought to Africa, Asia and New Zealand. It developed both as the language of English-speaking settlers from Britain and Ireland, and as the administrative language imposed on speakers of other languages in the various parts of the British Empire. The first form can be seen in New Zealand English, and the latter in Indian English. In Europe, English received a more central role particularly since 1919, when the Treaty of Versailles was composed not only in French, the common language of diplomacy at the time, but, under special request from American president Woodrow Wilson, also in English \u2013 a major milestone in the globalisation of English.\nThe English-speaking regions of Canada and the Caribbean are caught between historical connections with the UK and the Commonwealth and geographical and economic connections with the U.S. In some things they tend to follow British standards, whereas in others, especially commercial, they follow the U.S. standard.\nEnglish as a global language.\nBraj Kachru divides the use of English into three concentric circles.\nThe \"inner circle\" is the traditional base of English and includes countries such as the United Kingdom and Ireland and the anglophone populations of the former British colonies of the United States, Australia, New Zealand, South Africa, Canada, and various islands of the Caribbean, Indian Ocean, and Pacific Ocean.\nIn the \"outer circle\" are those countries where English has official or historical importance (\"special significance\"). This includes most of the countries of the Commonwealth of Nations (the former British Empire), including populous countries such as India, Pakistan, and Nigeria; and others, such as the Philippines, under the sphere of influence of English-speaking countries. English in this circle is used for official purposes such as in business, news broadcasts, schools, and air traffic. Some countries in this circle have made English their national language. Here English may serve as a useful lingua franca between ethnic and language groups. Higher education, the legislature and judiciary, national commerce, and so on, may all be carried out predominantly in English.\nThe \"expanding circle\" refers to those countries where English has no official role, but is nonetheless important for certain functions, e.g., international business and tourism. By the twenty-first century, non-native English speakers have come to outnumber native speakers by a factor of three, according to the British Council. Darius Degher, a former instructor at Malm\u00f6 University in Sweden, coined the term \"decentered English\" to describe this shift, along with attendant changes in what is considered important to English users and learners. The Scandinavian language area as well as the Netherlands have a near complete bilingualism between their native languages and English as a foreign second language. Elsewhere in Europe, although not universally, English knowledge is still rather common among non-native speakers. In many cases this leads to accents derived from the native languages altering pronunciations of the spoken English in these countries.\nResearch on English as a lingua franca in the sense of \"English in the Expanding Circle\" is comparatively recent. Linguists who have been active in this field are Jennifer Jenkins, Barbara Seidlhofer, Christiane Meierkord and Joachim Grzega.\nEnglish as a lingua franca in foreign language teaching.\nEnglish as an additional language (EAL) is usually based on the standards of either American English or British English as well as incorporating foreign terms. English as an international language (EIL) is EAL with emphasis on learning English's different major dialect forms; in particular, it aims to equip students with the linguistic tools to communicate internationally. Roger Nunn considers different types of competence in relation to the teaching of English as an International Language, arguing that linguistic competence has yet to be adequately addressed in recent considerations of EIL.\nSeveral models of \"simplified English\" have been suggested for teaching English as a foreign language:\nFurthermore, Randolph Quirk and Gabriele Stein thought about a Nuclear English, which, however, has never been fully developed.\nWith reference to the term \"Globish\", Robert McCrum has used this to mean \"English as global language\". Jean-Paul Nerriere uses it for a constructed language.\nBasic Global English.\nBasic Global English, or BGE, is a concept of global English initiated by German linguist Joachim Grzega. It evolved from the idea of creating a type of English that can be learned more easily than regular British or American English and that serves as a tool for successful global communication. BGE is guided by creating \"empathy and tolerance\" between speakers in a global context. This applies to the context of global communication, where different speakers with different mother tongues come together. BGE aims to develop this competence as quickly as possible.\nEnglish language teaching is almost always related to a corresponding culture, e.g. learners either deal with American English and therefore with American culture, or British English and therefore with British culture. Basic Global English seeks to solve this problem by creating one collective version of English. Additionally, its advocates promote it as a system suited for self-teaching as well as classroom teaching.\nBGE is based on 20 elementary grammar rules that provide a certain degree of variation. For example, regular as well as irregular formed verbs are accepted. Pronunciation rules are not as strict as in British or American English, so there is a certain degree of variation for the learners. Exceptions that cannot be used are pronunciations that would be harmful to mutual understanding and therefore minimise the success of communication.\nBasic Global English is based on a 750-word vocabulary. Additionally, every learner has to acquire the knowledge of 250 additional words. These words can be chosen freely, according to the specific needs and interests of the learner.\nBGE provides not only basic language skills, but also so called \"Basic Politeness Strategies\". These include creating a positive atmosphere, accepting an offer with \"Yes, please\" or refusing with \"No, thank you\", and small talk topics to choose and to avoid.\nBasic Global English has been tested in two elementary schools in Germany. For the practical test of BGE, 12 lessons covered half of a school year. After the BGE teaching, students could answer questions about themselves, their family, their hobbies etc. Additionally they could form questions themselves about the same topics. Besides that, they also learned the numbers from 1 to 31 and vocabulary including things in their school bag and in their classroom. The students as well as the parents had a positive impression of the project.\nVarying concepts.\nUniversality and flexibility.\nInternational English sometimes refers to English as it is actually being used and developed in the world; as a language owned not just by native speakers, but by all those who come to use it.\nBasically, it covers the English language at large, often (but not always or necessarily) implicitly seen as standard. It is certainly also commonly used in connection with the acquisition, use, and study of English as the world's lingua franca ('TEIL: Teaching English as an International Language'), and especially when the language is considered as a whole in contrast with \"British English\", \"American English\", \"South African English\", and the like. \u2014\u00a0McArthur (2002, p.\u00a0444\u2013445)\nIt especially means English words and phrases generally understood throughout the English-speaking world as opposed to localisms. The importance of non-native English language skills can be recognised behind the long-standing joke that the international language of science and technology is broken English.\nNeutrality.\nInternational English reaches toward cultural neutrality. This has a practical use:\nWhat could be better than a type of English that saves you from having to re-edit publications for individual regional markets! Teachers and learners of English as a second language also find it an attractive idea\u2014both often concerned that their English should be neutral, without American or British or Canadian or Australian colouring. Any regional variety of English has a set of political, social and cultural connotations attached to it, even the so-called 'standard' forms.\nThe development of International English often centres on academic and scientific communities, where formal English usage is prevalent, and creative use of the language is at a minimum. This formal International English allows entry into Western culture as a whole and Western cultural values in general.\nOpposition.\nThe continued growth of the English language itself is seen by authors such as Alastair Pennycook as a kind of cultural imperialism, whether it is English in one form or English in two slightly different forms.\nRobert Phillipson argues against the possibility of such neutrality in his \"Linguistic Imperialism\" (1992). Learners who wish to use purportedly correct English are in fact faced with the dual standard of American English and British English, and other less known standard Englishes (including Australian, Scottish and Canadian).\nEdward Trimnell, author of \"Why You Need a Foreign Language &amp; How to Learn One\" (2005) argues that the international version of English is only adequate for communicating basic ideas. For complex discussions and business/technical situations, English is not an adequate communication tool for non-native speakers of the language. Trimnell also asserts that native English-speakers have become \"dependent on the language skills of others\" by placing their faith in international English.\nAppropriation theory.\nSome reject both what they call \"linguistic imperialism\" and David Crystal's theory of the neutrality of English. They argue that the phenomenon of the global spread of English is better understood in the framework of appropriation (e.g., Spichtinger 2000), that is, English used for local purposes around the world. Demonstrators in non-English speaking countries often use signs in English to convey their demands to TV-audiences around the globe, for example.\nIn English-language teaching, Bobda shows how Cameroon has moved away from a mono-cultural, Anglo-centered way of teaching English and has gradually appropriated teaching material to a Cameroonian context. This includes non-Western topics, such as the rule of Emirs, traditional medicine, and polygamy (1997:225). Kramsch and Sullivan (1996) describe how Western methodology and textbooks have been appropriated to suit local Vietnamese culture. The Pakistani textbook \"Primary Stage English\" includes lessons such as \"Pakistan My Country\", \"Our Flag\", and \"Our Great Leader\" (Malik 1993: 5,6,7), which might sound jingoistic to Western ears. Within the native culture, however, establishing a connection between English Language Teaching (ELT), patriotism, and Muslim faith is seen as one of the aims of ELT. The Punjab Textbook Board openly states: \"The board ... takes care, through these books to inoculate in the students a love of the Islamic values and awareness to guard the ideological frontiers of your [the students] home lands.\" (Punjab Text Book Board 1997).\nMany Englishes.\nMany difficult choices must be made if further standardisation of English is pursued. These include whether to adopt a current standard or move towards a more neutral, but artificial one. A true International English might supplant both current American and British English as a variety of English for international communication, leaving these as local dialects, or would rise from a merger of General American and standard British English with admixture of other varieties of English and would generally replace all these varieties of English.\nWe may, in due course, all need to be in control of two standard Englishes\u2014the one which gives us our national and local identity, and the other which puts us in touch with the rest of the human race. In effect, we may all need to become bilingual in our own language. \u2014\u00a0David Crystal (1988: p.\u00a0265)\nThis is the situation long faced by many users of English who possess a \"non-standard\" dialect of English as their birth tongue but have also learned to write (and perhaps also speak) a more standard dialect. (This phenomenon is known in linguistics as \"diglossia\".) Many academics often publish material in journals requiring different varieties of English and change style and spellings as necessary without great difficulty.\nAs far as spelling is concerned, the differences between American and British usage became noticeable due to the first influential lexicographers (dictionary writers) on each side of the Atlantic. Samuel Johnson's dictionary of 1755 greatly favoured Norman-influenced spellings such as \"centre\" and \"colour\"; on the other hand, Noah Webster's first guide to American spelling, published in 1783, preferred spellings like \"center\" and the Latinate \"color\". The difference in strategy and philosophy of Johnson and Webster are largely responsible for the main division in English spelling that exists today. However, these differences are extremely minor. Spelling is but a small part of the differences between dialects of English, and may not even reflect dialect differences at all (except in phonetically spelled dialogue). International English refers to much more than an agreed spelling pattern.\nDual standard.\nTwo approaches to International English are the individualistic and inclusive approach and the new dialect approach.\nThe individualistic approach gives control to individual authors to write and spell as they wish (within purported standard conventions) and to accept the validity of differences. The \"Longman Grammar of Spoken and Written English\", published in 1999, is a descriptive study of both American and British English in which each chapter follows individual spelling conventions according to the preference of the main editor of that chapter.\nThe new dialect approach appears in \"The Cambridge Guide to English Usage\" (Peters, 2004), which attempts to avoid any language bias and accordingly uses an idiosyncratic international spelling system of mixed American and British forms.\nQualifications.\nStandardised testing in International English for non-native English language speakers has existed for a while. Learners can use their local dialect of English so it does not matter if they use British or American spelling. The International English Language Testing System (IELTS) is recognised in countries such as the USA, the UK, Canada, Australia and New Zealand and is the world's most popular English language test for higher education and immigration. Other options are the International Certificate (PTE General) and Cambridge English Qualifications which are also recognised globally and can be used as evidence of a required standard of English.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14997", "revid": "305208", "url": "https://en.wikipedia.org/wiki?curid=14997", "title": "International African Institute", "text": "The International African Institute (IAI) was founded (as the International Institute of African Languages and Cultures - IIALC) in 1926 in London for the study of African languages. Frederick Lugard was the first chairman (1926 to his death in 1945); Diedrich Hermann Westermann (1926 to 1939) and Maurice Delafosse (1926) were the initial co-directors.\nSince 1928, the IAI has published a quarterly journal, \"Africa\". For some years in the 1950s to 1970s, the assistant editor was the novelist Barbara Pym.\nIn 1946 the Italian ethnologist Vinigi Grottanelli joined the Executive Board, and remained a participant until 1968.\nThe IAI's mission is \"to promote the education of the public in the study of Africa and its languages and cultures\". Its operations includes seminars, journals, monographs, edited volumes and stimulating scholarship within Africa.\nPublications.\nThe IAI has been involved in scholarly publishing since 1927. Scholars whose work has been published by the institute include Emmanuel K. Akyeampong, Samir Amin, Karin Barber, Alex de Waal, Patrick Chabal, Mary Douglas, E. E. Evans-Pritchard, Jack Goody, Jane Guyer, Monica Hunter, Bronislaw Malinowski, Z. K. Matthews, D. A. Masolo, Achille Mbembe, Thomas Mofolo, John Middleton, Simon Ottenberg, J. D. Y. Peel, Mamphela Ramphele, Isaac Schapera, Monica Wilson and V. Y. Mudimbe.\nIAI publications fall into a number of series, notably International African Library and International African Seminars. The International African Library is published from volume 41 (2011) by Cambridge University Press; Volumes 7\u201340 are available from Edinburgh University Press. As of November 2016[ [update]], there are 49 volumes.\nArchives.\nThe archives of the International African Institute are held at the http:// of the Library of the London School of Economics. An http:// of these papers is available.\nHistory.\nAfrica alphabet.\nIn 1928, the IAI (then IIALC) published an \"Africa Alphabet\" to facilitate standardization of Latin-based writing systems for African languages.\nPrize for African-language literature, 1929\u201350.\nFrom April 1929 to 1950, the IAI offered prizes for works of literature in African languages.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14998", "revid": "38040997", "url": "https://en.wikipedia.org/wiki?curid=14998", "title": "IAI", "text": "IAI is an acronym for:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15000", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=15000", "title": "Insulin-like growth factor", "text": "Proteins similar to insulin that stimulate cell proliferation\nThe insulin-like growth factors (IGFs) are proteins with high sequence similarity to insulin. IGFs are part of a complex system that cells use to communicate with their physiologic environment. This complex system (often referred to as the IGF \"axis\") consists of two cell-surface receptors (IGF1R and IGF2R), two ligands (IGF-1 and IGF-2), a family of seven high-affinity IGF-binding proteins (IGFBP1 to IGFBP7), as well as associated IGFBP degrading enzymes, referred to collectively as proteases.\nIGF1/GH axis.\nThe IGF \"axis\" is also commonly referred to as the Growth Hormone/IGF-1 Axis. Insulin-like growth factor 1 (commonly referred to as IGF-1 or at times using Roman numerals as IGF-I) is mainly secreted by the liver as a result of stimulation by growth hormone (GH). IGF-1 is important for both the regulation of normal physiology, as well as a number of pathological states, including cancer. The IGF axis has been shown to play roles in the promotion of cell proliferation and the inhibition of cell death (apoptosis). \nInsulin-like growth factor 2 (IGF-2, at times IGF-II) is thought to be a primary growth factor required for early development while IGF-1 expression is required for achieving maximal growth. Gene knockout studies in mice have confirmed this, though other animals are likely to regulate the expression of these genes in distinct ways. While IGF-2 may be primarily fetal in action it is also essential for development and function of organs such as the brain, liver, and kidney.\nFactors that are thought to cause variation in the levels of GH and IGF-1 in the circulation include an individual's genetic make-up, the time of day, age, sex, exercise status, stress levels, nutrition level, body mass index (BMI), disease state, race, estrogen status, and xenobiotic intake.\nIGF-1 has an involvement in regulating neural development including neurogenesis, myelination, synaptogenesis, and dendritic branching and neuroprotection after neuronal damage. Increased serum levels of IGF-I in children have been associated with higher IQ.\nIGF-1 shapes the development of the cochlea through controlling apoptosis. Its deficit can cause hearing loss. Serum level of it also underlies a correlation between short height and reduced hearing abilities particularly around 3\u20135 years of age, and at age 18 (late puberty).\nIGF receptors.\nThe IGFs are known to bind the IGF-1 receptor, the insulin receptor, the IGF-2 receptor, the insulin-related receptor and possibly other receptors. The IGF-1 receptor is the \"physiological\" receptor. IGF-1 binds to it at significantly higher affinity than it binds the insulin receptor. Like the insulin receptor, the IGF-1 receptor is a receptor tyrosine kinase\u2014meaning the receptor signals by causing the addition of a phosphate molecule on particular tyrosines. The IGF-2 receptor only binds IGF-2 and acts as a \"clearance receptor\"\u2014it activates no intracellular signaling pathways, functioning only as an IGF-2 sequestering agent and preventing IGF-2 signaling.\nOrgans and tissues affected by IGF-1.\nSince many distinct tissue types express the IGF-1 receptor, IGF-1's effects are diverse. It acts as a neurotrophic factor, inducing the survival of neurons. It may catalyse skeletal muscle hypertrophy, by inducing protein synthesis, and by blocking muscle atrophy. It is protective for cartilage cells, and is associated with activation of osteocytes, and thus may be an anabolic factor for bone. Since at high concentrations it is capable of activating the insulin receptor, it can also complement for the effects of insulin. Receptors for IGF-1 are found in vascular smooth muscle, while typical receptors for insulin are not found in vascular smooth muscle.\nIGF-binding proteins.\nIGF-1 and IGF-2 are regulated by a family of proteins known as the IGF-binding proteins. These proteins help to modulate IGF action in complex ways that involve both inhibiting IGF action by preventing binding to the IGF-1 receptor as well as promoting IGF action possibly through aiding in delivery to the receptor and increasing IGF half-life. Currently, there are seven characterized IGF Binding Proteins (IGFBP1 to IGFBP7). There is currently significant data suggesting that IGFBPs play important roles in addition to their ability to regulate IGFs.\nIGF-1 and IGFBP-3 are GH dependent, whereas IGFBP-1 is insulin regulated.\nIGFBP-1 production from the liver is significantly elevated during insulinopenia while serum levels of bioactive IGF-1 is increased by insulin.\nDiseases affected by IGF.\nStudies of recent interest show that the Insulin/IGF axis play an important role in aging. Nematodes, fruit-flies, and other organisms have an increased life span when the gene equivalent to the mammalian insulin is knocked out. It is somewhat difficult to relate this finding to mammals, however, because there are many genes (at least 37 in the nematode \"Caenorhabditis elegans\") in smaller organisms that are \"insulin-like\" or \"IGF-1-like\", whereas in mammals insulin-like proteins comprise only seven members (insulin, IGFs, relaxins, EPIL, and relaxin-like factor). The human insulin-like genes have apparently distinct roles with some but less crosstalk presumably because there are multiple insulin-receptor-like proteins in humans. Simpler organisms typically have fewer receptors; for example, only one insulin-like receptor exists in the nematode \"C. elegans\". Additionally, \"C. elegans\" do not have specialized organs such as the (Islets of Langerhans), which sense insulin in response to glucose homeostasis. Moreover, IGF1 affects lifespan in nematodes by causing dauer formation, a developmental stage of \"C. elegans\" larva. There is no mammalian correlate. Therefore, it is an open question as to whether either IGF-1 or insulin in the mammal may perturb aging, although there is the suggestion that dietary restriction phenomena may be related.\nOther studies are beginning to uncover the important role the IGFs play in diseases such as cancer and diabetes, showing for instance that IGF-1 stimulates growth of both prostate and breast cancer cells. Researchers are not in complete agreement about the degree of cancer risk that IGF-1 poses.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15001", "revid": "1066991127", "url": "https://en.wikipedia.org/wiki?curid=15001", "title": "IGF", "text": "IGF may stand for:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15002", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=15002", "title": "IGF-1", "text": ""}
{"id": "15003", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=15003", "title": "Mental deficiency", "text": ""}
{"id": "15004", "revid": "45623977", "url": "https://en.wikipedia.org/wiki?curid=15004", "title": "Idiot", "text": "Person of low intelligence\nAn idiot, in modern use, is a stupid or foolish person. \"Idiot\" was formerly a technical term in legal and psychiatric contexts for some kinds of profound intellectual disability where the mental age is two years or less, and the person cannot guard themself against common physical dangers. The term was gradually replaced by \"profound mental retardation\", which has since been replaced by other terms. Along with terms like moron, imbecile, retard and cretin, its use to describe people with mental disabilities is considered archaic and offensive. Moral idiocy refers to a moral disability.\nEtymology.\nThe word \"idiot\" ultimately comes from the Greek noun \"idi\u014dt\u0113s\" 'a private person, individual' (as opposed to the state), 'a private citizen' (as opposed to someone with a political office), 'a common man', 'a person lacking professional skill, layman', later 'unskilled', 'ignorant', derived from the adjective \"idios\" 'personal' (not public, not shared). In Latin, \"idiota\" was borrowed in the meaning 'uneducated', 'ignorant', 'common', and in Late Latin came to mean 'crude, illiterate, ignorant'. In French, it kept the meaning of 'illiterate', 'ignorant', and added the meaning 'stupid' in the 13th century. In English, it added the meaning 'mentally deficient' in the 14th century.\nMany political commentators, starting as early as 1856, have interpreted the word \"idiot\" as reflecting the Ancient Athenians' attitudes to civic participation and private life, combining the ancient meaning of 'private citizen' with the modern meaning 'fool' to conclude that the Greeks used the word to say that it is selfish and foolish not to participate in public life. But this is not how the Greeks used the word.\nIt is certainly true that the Greeks valued civic participation and criticized non-participation. Thucydides quotes Pericles' Funeral Oration as saying: \"[we] regard... him who takes no part in these [public] duties not as unambitious but as useless\" (). However, neither he nor any other ancient author uses the word \"idiot\" to describe non-participants, or in a derogatory sense; its most common use was simply a private citizen or amateur as opposed to a government official, professional, or expert. The derogatory sense came centuries later, and was unrelated to the political meaning.\nDisability and early classification and nomenclature.\nIn 19th- and early 20th-century medicine and psychology, an \"idiot\" was a person with a very profound intellectual disability, being diagnosed with \"idiocy\". In the early 1900s, Dr. Henry H. Goddard proposed a classification system for intellectual disability based on the Binet-Simon concept of mental age. Individuals with the lowest mental age level (less than three years) were identified as \"idiots\"; \"imbeciles\" had a mental age of three to seven years, and \"morons\" had a mental age of seven to ten years. The term \"idiot\" was used to refer to people having an IQ below 30. IQ, or intelligence quotient, was originally determined by dividing a person's mental age, as determined by standardized tests, by their actual age. The concept of mental age has fallen into disfavor, though, and IQ is now determined on the basis of statistical distributions.\nIn the obsolete medical classification (ICD-9, 1977), these people were said to have \"profound mental retardation\" or \"profound mental subnormality\" with IQ under 20.\nRegional law.\nUnited States.\nUntil 2007, the California Penal Code Section 26 stated that \"Idiots\" were one of six types of people who are not capable of committing crimes. In 2007 the code was amended to read \"persons who are mentally incapacitated.\" In 2008, Iowa voters passed a measure replacing \"idiot, or insane person\" in the State's constitution with \"person adjudged mentally incompetent.\"\nIn the constitution of several U.S. states, \"idiots\" do not have the right to vote:\nThe constitution of the state of Arkansas was amended in the general election of 2008 to, among other things, repeal a provision (Article 3, Section 5) which had until its repeal prohibited \"idiots or insane persons\" from voting.\nIn literature.\nA few authors have used \"idiot\" characters in novels, plays and poetry. Often these characters are used to highlight or indicate something else (allegory). Examples of such usage are William Faulkner's \"The Sound and the Fury\", Daphne du Maurier's \"Rebecca\" and William Wordsworth's \"The Idiot Boy\". Idiot characters in literature are often confused with or subsumed within mad or lunatic characters. The most common intersection between these two categories of mental impairment occurs in the polemic surrounding Edmund from William Shakespeare's \"King Lear\".\nIn Fyodor Dostoevsky's novel \"The Idiot\" the title refers to the central character Prince Myshkin, a man whose innocence, kindness and humility, combined with his occasional epileptic symptoms, cause many in the corrupt, egoistic culture around him to mistakenly assume that he lacks intelligence. In \"The Antichrist\", Nietzsche applies the word \"idiot\" to Jesus in a comparable fashion, almost certainly in an allusion to Dostoevsky's use of the word: \"One has to regret that no Dostoevsky lived in the neighbourhood of this most interesting \"d\u00e9cadent\"; I mean someone who could feel the thrilling fascination of such a combination of the sublime, the sick and the childish.\"\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15008", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=15008", "title": "Isma'ilis", "text": ""}
{"id": "15009", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=15009", "title": "Ismailis", "text": ""}
{"id": "15012", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=15012", "title": "Islamism", "text": "Politico-religious ideology\nIslamism is a range of religious and political ideological movements that believe that Islam should influence political systems. Its proponents believe Islam is innately political, and that Islam as a political system is superior to communism, liberal democracy, capitalism, and other alternatives in achieving a just, successful society. The advocates of Islamism, also known as \"al-Islamiyyun\", are usually affiliated with Islamic institutions or social mobilization movements, emphasizing the implementation of \"sharia\", pan-Islamic political unity, and the creation of Islamic states.\nIn its original formulation, Islamism described an ideology seeking to revive Islam to its past assertiveness and glory, purifying it of foreign elements, reasserting its role into \"social and political as well as personal life\"; and in particular \"reordering government and society in accordance with laws prescribed by Islam\" (i.e. Sharia). According to at least one observer (author Robin Wright), Islamist movements have \"arguably altered the Middle East more than any trend since the modern states gained independence\", redefining \"politics and even borders\". Another sole author (Graham E. Fuller) has argued for a broader notion of Islamism as a form of identity politics, involving \"support for [Muslim] identity, authenticity, broader regionalism, revivalism, [and] revitalization of the community.\"\nCentral and prominent figures in 20th-century Islamism include Rashid Rida, Hassan al-Banna (founder of the Muslim Brotherhood), Sayyid Qutb, Abul A'la Maududi, Ruhollah Khomeini (founder of the Islamic Republic of Iran), Hassan Al-Turabi. Syrian Sunni cleric Muhammad Rashid Ri\u1e0d\u0101, a fervent opponent of Westernization, Zionism and nationalism, advocated Sunni internationalism through revolutionary restoration of a pan-Islamic Caliphate to politically unite the Muslim world. Ri\u1e0d\u0101 was a strong exponent of Islamic vanguardism, the belief that Muslim community should be guided by clerical elites (\"ulema\") who steered the efforts for religious education and Islamic revival. Ri\u1e0d\u0101's Salafi-Arabist synthesis and Islamist ideals greatly influenced his disciples like Hasan al-Banna, an Egyptian schoolteacher who founded the Muslim Brotherhood movement, and Hajji Amin al-Husayni, the anti-Zionist Grand Mufti of Jerusalem. Al-Banna and Maududi called for a \"reformist\" strategy to re-Islamizing society through grassroots social and political activism. Other Islamists (Al-Turabi) are proponents of a \"revolutionary\" strategy of Islamizing society through exercise of state power, or (Sayyid Qutb) for combining grassroots Islamization with armed revolution. The term has been applied to non-state reform movements, political parties, militias and revolutionary groups.\nIslamists themselves prefer terms such as \"Islamic movement\", or \"Islamic activism\" to \"Islamism\", objecting to the insinuation that Islamism is anything other than Islam renewed and revived. In public and academic contexts, the term \"Islamism\" has been criticized as having been given connotations of violence, extremism, and violations of human rights, by the Western mass media, leading to Islamophobia and stereotyping.\nProminent Islamist groups and parties across the world include the Muslim Brotherhood, Turkey's Justice and Development Party, Hamas, the Algerian Movement of Society for Peace, the Malaysian National Trust Party, Jamaat-e-Islami in Bangladesh and Pakistan and Bosnia's Party of Democratic Action. Following the Arab Spring, many post-Islamist currents became heavily involved in democratic politics, while others spawned \"the most aggressive and ambitious Islamist militia\" to date, such as the Islamic State of Iraq and the Levant (ISIL). ISIL has been rejected as blasphemous by the majority of Islamists.\nTerminology.\nOriginally the term \"Islamism\" was simply used to mean the religion of Islam, not an ideology or movement. It first appeared in the English language as \"Islamismus\" in 1696, and as \"Islamism\" in 1712. The term appears in the U.S. Supreme Court decision in \"In Re Ross\" (1891). By the turn of the twentieth century the shorter and purely Arabic term \"Islam\" had begun to displace it, and by 1938, when Orientalist scholars completed \"The Encyclopaedia of Islam\", \"Islamism\" seems to have virtually disappeared from English usage. The term remained \"practically absent from the vocabulary\" of scholars, writers or journalists until the Iranian Islamic Revolution of 1978\u201379, which brought Ayatollah Khomeini's concept of \"Islamic government\" to Iran.\nThis new usage appeared without taking into consideration how the term \"Islamist\" (m. sing.: \"Islami\", pl. nom/acc: \"Islamiyyun\", gen. \"Islamiyyin;\" f. sing/pl: \"Islamiyyah\") was already being used in traditional Arabic scholarship in a theological sense as in relating to the religion of Islam, not a political ideology. In heresiographical, theological and historical works, such as al-Ash'ari's well-known encyclopaedia \"Maq\u0101l\u0101t al-Isl\u0101miyy\u012bn\" (\"The Opinions of The Islamists\"), an Islamist refers to any person who attributes himself to Islam without affirming nor negating that attribution. If used consistently, it is for impartiality, but if used in reference to a certain person or group in particular without others, it implies that the author is either unsure whether to affirm or negate their attribution to Islam, or trying to insinuate his disapproval of the attribution without controversy. In contrast, referring to a person as a Muslim or a Kafir implies an explicit affirmation or a negation of that person's attribution to Islam. To evade the problem resulting from the confusion between the Western and Arabic usage of the term Islamist, Arab journalists invented the term \"Islamawi\" (\"Islamian\") instead of \"Islami\" (\"Islamist\") in reference to the political movement, though this term is sometimes criticized as grammatically incorrect.\nDefinitions.\nIslamism has been defined as:\nRelationship between Islam and Islamism.\nIslamists simply believe that their movement is either a corrected version or a revival of Islam, but others believe that Islamism is a modern deviation from Islam which should either be denounced or dismissed.\nA writer for the International Crisis Group maintains that \"the conception of 'political Islam'\" is a creation of Americans to explain the Iranian Islamic Revolution, ignoring the fact that (according to the writer) Islam is by definition political. In fact it is quietist/non-political Islam, not Islamism, that requires explanation, which the author gives\u2014calling it an historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\".\nHayri Abaza argues that the failure to distinguish Islam from Islamism leads many in the West to equate the two; they think that by supporting illiberal Islamic (Islamist) regimes, they are being respectful of Islam, to the detriment of those who seek to separate religion from politics.\nAnother source distinguishes Islamist from Islam by emphasizing the fact that Islam \"refers to a religion and culture in existence over a millennium\", whereas Islamism \"is a political/religious phenomenon linked to the great events of the 20th century\". Islamists have, at least at times, defined themselves as \"Islamiyyoun/Islamists\" to differentiate themselves from \"Muslimun/Muslims\". Daniel Pipes describes Islamism as a modern ideology that owes more to European utopian political ideologies and \"isms\" than to the traditional Islamic religion.\nAccording to Salman Sayyid, \"Islamism is not a replacement of Islam akin to the way it could be argued that communism and fascism are secularized substitutes for Christianity.\" Rather, it is \"a constellation of political projects that seek to position Islam in the centre of any social order\".\nIdeology.\nIslamic revival.\nThe modern revival of Islamic devotion and the attraction to things Islamic can be traced to several events.\nBy the end of World War I, most Muslim states were seen to be dominated by the Christian-leaning Western states. Explanations offered were: that the claims of Islam were false and the Christian or post-Christian West had finally come up with another system that was superior; or Islam had failed through not being true to itself. The second explanation being preferred by Muslims, a redoubling of faith and devotion by the faithful was called for to reverse this tide.\nThe connection between the lack of an Islamic spirit and the lack of victory was underscored by the disastrous defeat of Arab nationalist-led armies fighting Israel under the slogan \"Land, Sea and Air\" in the 1967 Six-Day War, compared to the (perceived) near-victory of the Yom Kippur War six years later. In that war the military's slogan was \"God is Great\".\nAlong with the Yom Kippur War came the Arab oil embargo where the (Muslim) Persian Gulf oil-producing states' dramatic decision to cut back on production and quadruple the price of oil, made the terms oil, Arabs and Islam synonymous with power throughout the world, and especially in the Muslim world's public imagination. Many Muslims believe as Saudi Prince Saud al Faisal did that the hundreds of billions of dollars in wealth obtained from the Persian Gulf's huge oil deposits were nothing less than a gift from God to the Islamic faithful.\nAs the Islamic revival gained momentum, governments such as Egypt's, which had previously repressed (and was still continuing to repress) Islamists, joined the bandwagon. They banned alcohol and flooded the airwaves with religious programming, giving the movement even more exposure.\nRestoration of the Caliphate.\nThe abolition of the Ottoman Sultanate by the Grand National Assembly of Turkey on 1 November 1922 ended the Ottoman Empire, which had lasted since 1299. On 11 November 1922, at the Conference of Lausanne, the sovereignty of the Grand National Assembly exercised by the Government in Angora (now Ankara) over Turkey was recognized. The last sultan, Mehmed VI, departed the Ottoman capital, Constantinople (now Istanbul), on 17 November 1922. The legal position was solidified with the signing of the Treaty of Lausanne on 24 July 1923. In March 1924, the Caliphate was abolished legally by the Turkish National Assembly, marking the end of Ottoman influence. This shocked the Sunni clerical world, and many felt the need to present Islam not as a traditional religion but as an innovative socio-political ideology of a modern nation-state.\nThe reaction to new realities of the modern world gave birth to Islamist ideologues like Rashid Rida and Abul A'la Maududi and organizations such as the Muslim Brotherhood in Egypt and Majlis-e-Ahrar-ul-Islam in India. Rashid Rida, a prominent Syrian-born Salafi theologian based in Egypt, was known as a revivalist of Hadith studies in Sunni seminaries and a pioneering theoretician of Islamism in the modern age. During 1922\u20131923, Rida published a series of articles in seminal \"Al-Manar\" magazine titled \"The Caliphate or the Supreme Imamate\". In this highly influential treatise, Rida advocates for the restoration of Caliphate guided by Islamic jurists and proposes gradualist measures of education, reformation and purification through the efforts of \"Salafiyya\" reform movements across the globe.\nSayyid Rashid Rida had visited India in 1912 and was impressed by the Deoband and Nadwatul Ulama seminaries. These seminaries carried the legacy of Sayyid Ahmad Shahid and his pre-modern Islamic emirate. In British India, the Khilafat movement (1919\u201324) following World War I led by Shaukat Ali, Maulana Mohammad Ali Jauhar, Hakim Ajmal Khan and Maulana Azad came to exemplify South Asian Muslims' aspirations for Caliphate.\nAnti-Westernization.\nMuslim alienation from Western ways, including its political ways.\nIn the words of Bernard Lewis:\nFor almost a thousand years, from the first Moorish landing in Spain to the second Turkish siege of Vienna, Europe was under constant threat from Islam. In the early centuries it was a double threat\u2014not only of invasion and conquest, but also of conversion and assimilation. All but the easternmost provinces of the Islamic realm had been taken from Christian rulers, and the vast majority of the first Muslims west of Iran and Arabia were converts from Christianity ... Their loss was sorely felt and it heightened the fear that a similar fate was in store for Europe.\nFor Islamists, the primary threat of the West is cultural rather than political or economic. Cultural dependency robs one of faith and identity and thus destroys Islam and the Islamic community (\"ummah\") far more effectively than political rule.&lt;ref name=\"Haddad/Esposito1\"&gt;Haddad/Esposito p. xvi&lt;/ref&gt;\nStrength of identity politics.\nIslamism is described by Graham E. Fuller as part of identity politics, specifically the religiously oriented nationalism that emerged in the Third World in the 1970s: \"resurgent Hinduism in India, Religious Zionism in Israel, militant Buddhism in Sri Lanka, resurgent Sikh nationalism in the Punjab, 'Liberation Theology' of Catholicism in Latin America, and Islamism in the Muslim world.\"\nAnti-communist stances.\nBy the late 1960s, non-Soviet Muslim-majority countries had won their independence and they tended to fall into one of the two cold-war blocs \u2013 with \"Nasser's Egypt, Baathist Syria and Iraq, Muammar el-Qaddafi's Libya, Algeria under Ahmed Ben Bella and Houari Boumedienne, Southern Yemen, and Sukarno's Indonesia\" aligned with Moscow. Aware of the close attachment of the population with Islam, \"school books of the 1960s in these countries \"went out of their way to impress upon children that socialism was simply Islam properly understood.\"\nOlivier Roy writes that the \"failure of the 'Arab socialist' model ... left room for new protest ideologies to emerge in deconstructed societies ...\" Gilles Kepel notes that when a collapse in oil prices led to widespread violent and destructive rioting by the urban poor in Algeria in 1988, what might have appeared to be a natural opening for the left, was instead the beginning of major victories for the Islamist Islamic Salvation Front (FIS) party. The reason being the corruption and economic malfunction of the policies of the Third World socialist ruling party (FNL) had \"largely discredited\" the \"vocabulary of socialism\".\nIn the post-colonial era, many Muslim-majority states such as Indonesia, Egypt, Syria, and Iraq, were ruled by authoritarian regimes which were often continuously dominated by the same individuals or their cadres for decades. Simultaneously, the military played a significant part in the government decisions in many of these states (the outsized role played by the military could be seen also in democratic Turkey).\nThe authoritarian regimes, backed by military support, took extra measures to silence leftist opposition forces, often with the help of foreign powers. Silencing of leftist opposition deprived the masses a channel to express their economic grievances and frustration toward the lack of democratic processes. As a result, in the post-Cold War era, civil society-based Islamist movements such as the Muslim Brotherhood were the only organizations capable to provide avenues of protest.\nThe dynamic was repeated after the states had gone through a democratic transition. In Indonesia, some secular political parties have contributed to the enactment of religious bylaws to counter the popularity of Islamist oppositions. In Egypt, during the short period of the democratic experiment, Muslim Brotherhood seized the momentum by being the most cohesive political movement among the opposition.\nInfluence.\nFew observers contest the immense influence of Islamism within the Muslim world. Following the collapse of the Soviet Union, political movements based on the liberal ideology of free expression and democratic rule have led the opposition in other parts of the world such as Latin America, Eastern Europe and many parts of Asia; however \"the simple fact is that political Islam currently reigns [circa 2002-3] as the most powerful ideological force across the Muslim world today\".\nThe strength of Islamism also draws from the strength of religiosity in general in the Muslim world. Compared to other societies around the globe, \"[w]hat is striking about the Islamic world is that ... it seems to have been the least penetrated by irreligion\". Where other peoples may look to the physical or social sciences for answers in areas which their ancestors regarded as best left to scripture, in the Muslim world, religion has become more encompassing, not less, as \"in the last few decades, it has been the fundamentalists who have increasingly represented the cutting edge\" of Muslim culture.\nWriting in 2009, German journalist Sonja Zekri described Islamists in Egypt and other Muslim countries as \"extremely influential. ... They determine how one dresses, what one eats. In these areas, they are incredibly successful. ... Even if the Islamists never come to power, they have transformed their countries.\" Political Islamists were described as \"competing in the democratic public square in places like Turkey, Tunisia, Malaysia and Indonesia\".\nTypes.\nIslamism is not a united movement and takes different forms and spans a wide range of strategies and tactics towards the powers in place\u2014\"destruction, opposition, collaboration, indifference\"\u2014not because (or not just because) of differences of opinions, but because it varies as circumstances change.p.\u00a054\nModerate and reformist Islamists who accept and work within the democratic process include parties like the Tunisian Ennahda Movement. Some Islamists can be religious populists or far-right. \nJamaat-e-Islami of Pakistan is basically a socio-political and \"vanguard party\" working with in Pakistan's Democratic political process, but has also gained political influence through military coup d'\u00e9tats in the past. Other Islamist groups like Hezbollah in Lebanon and Hamas in Palestine claim to participate in the democratic and political process as well as armed attacks by their powerful paramilitary wings. Jihadist organizations like al-Qaeda and the Egyptian Islamic Jihad, and groups such as the Taliban, entirely reject democracy, seeing it as a form of \"kufr\" (disbelief) calling for offensive jihad on a religious basis.\nAnother major division within Islamism is between what Graham E. Fuller has described as the \"conservative\" \"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"revolutionary\" \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood. Olivier Roy argues that \"Sunni pan-Islamism underwent a remarkable shift in the second half of the 20th century\" when the Muslim Brotherhood movement and its focus on Islamisation of pan-Arabism was eclipsed by the Salafi movement with its emphasis on \"sharia rather than the building of Islamic institutions\". Following the Arab Spring (starting in 2011), Roy has described Islamism as \"increasingly interdependent\" with democracy in much of the Arab Muslim world, such that \"neither can now survive without the other.\" While Islamist political culture itself may not be democratic, Islamists need democratic elections to maintain their legitimacy. At the same time, their popularity is such that no government can call itself democratic that excludes mainstream Islamist groups.\nArguing distinctions between \"radical/moderate\" or \"violent/peaceful\" Islamism were \"simplistic\", circa 2017, scholar Morten Valbj\u00f8rn put forth these \"much more sophisticated typologies\" of Islamism:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nModerate and reformist Islamism.\nThroughout the 1980s and '90s, major moderate Islamist movements such as the Muslim Brotherhood and the Ennahda were excluded from democratic political participation. At least in part for that reason, Islamists attempted to overthrow the government in the Algerian Civil War (1991\u20132002) and waged a terror campaign in Egypt in the '90s. These attempts were crushed and in the 21st century, Islamists turned increasingly to non-violent methods, and \"moderate Islamists\" now make up the majority of the contemporary Islamist movements.\nAmong some Islamists, Democracy has been harmonized with Islam by means of \"Shura\" (consultation). The tradition of consultation by the ruler being considered Sunnah of the prophet Muhammad, (\"Majlis-ash-Shura\" being a common name for legislative bodies in Islamic countries).\nAmong the varying goals, strategies, and outcomes of \"moderate Islamist movements\" are a formal abandonment of their original vision of implementing \"sharia\" (also termed Post-Islamism) \u2013 done by the Ennahda Movement of Tunisia, and Prosperous Justice Party (PKS) of Indonesia. Others, such as the National Congress of Sudan, have implemented the sharia with support from wealthy, conservative states (primarily Saudi Arabia).\nAccording to one theory \u2013 \"inclusion-moderation\"\u2014the interdependence of political outcome with strategy means that the more moderate the Islamists become, the more likely they are to be politically included (or unsuppressed); and the more accommodating the government is, the less \"extreme\" Islamists become. A prototype of harmonizing Islamist principles within the modern state framework was the \"Turkish model\", based on the apparent success of the rule of the Turkish Justice and Development Party (AKP) led by Recep Tayyip Erdo\u011fan. Turkish model, however, came \"unstuck\" after a purge and violations of democratic principles by the Erdo\u011fan regime. Critics of the concept \u2013 which include both Islamists who reject democracy and anti-Islamists \u2013 hold that Islamist aspirations are fundamentally incompatible with the democratic principles.\nSalafi movement.\nThe contemporary Salafi movement is sometimes described as a variety of Islamism and sometimes as a different school of Islam, such as a \"phase between fundamentalism and Islamism\".\nOriginally a reformist movement of Jamal al-Din al-Afghani, Muhammad Abdul, and Rashid Rida, that rejected maraboutism (Sufism), the established schools of fiqh, and demanded individual interpretation (\"ijtihad\") of the Quran and Sunnah; it evolved into a movement embracing the conservative doctrines of the medieval Hanbali theologian Ibn Taymiyyah. While all salafi believe Islam covers every aspect of life, that sharia law must be implemented completely and that the Caliphate must be recreated to rule the Muslim world, they differ in strategies and priorities, which generally fall into three groups: \nMilitant Islamism/Jihadism.\nQutbism.\nQutbism refers to the Jihadist ideology formulated by Sayyid Qutb, (an influential figure of the Muslim Brotherhood in Egypt during the '50s and '60s). Qutbism argued that not only was sharia essential for Islam, but that since it was not in force, Islam did not really exist in the Muslim world, which was in \"Jahiliyya\" (the state of pre-Islamic ignorance). To remedy this situation he urged a two-pronged attack of 1) preaching to convert, and 2) jihad to forcibly eliminate the \"structures\" of \"Jahiliyya\". Defensive jihad against \"Jahiliyya\" Muslim governments would not be enough. \"Truth and falsehood cannot coexist on this earth\", so offensive Jihad was needed to eliminate \"Jahiliyya\" not only from the Islamic homeland but from the face of the Earth. In addition, vigilance against Western and Jewish conspiracies against Islam would-be needed.\nAlthough Qutb was executed before he could fully spell out his ideology, his ideas were disseminated and expanded on by the later generations, among them Abdullah Yusuf Azzam and Ayman Al-Zawahiri, who was a student of Qutb's brother Muhammad Qutb and later became a mentor of Osama bin Laden. Al-Zawahiri helped to pass on stories of \"the purity of Qutb's character\" and persecution he suffered, and played an extensive role in the normalization of offensive Jihad among followers of Qutb.\nSalafi Jihadism.\nSalafi Jihadism or revolutionary Salafism emerged prominent during the 1980s when Osama bin Laden and thousands of other militant Muslims came from around the Muslim world to unite against the Soviet Union after it invaded Afghanistan. Local Afghan Muslims (mujahideen) had declared jihad against the Soviets and were aided with financial, logistical and military support by Saudi Arabia and the United States, but after Soviet forces left Afghanistan, this funding and interest by America and Saudi ceased. The international volunteers, (originally organized by Abdullah Azzam), were triumphant in victory, away from the moderating influence of home and family, among the \nradicalized influence of other militants. Wanting to capitalize on financial, logistical and military network that had been developed they sought to continue waging jihad elsewhere. Their new targets, however, included the United States\u2014funder of the mujahideen but \"perceived as the greatest enemy of the faith\"; and governments of majority-Muslims countries\u2014perceived of as apostates from Islam.\nSalafist-jihadist ideology combined the literal and traditional interpretations of scripture of Salafists, with the promotion and fighting of jihad against military and civilian targets in the pursuit of the establishment of an Islamic state and eventually a new Caliphate.\nOther characteristics of the movement include the formal process of taking \"bay'ah\" (oath of allegiance) to the leader (\"amir\"), which is inspired by Hadiths and early Muslim practice and included in Wahhabi teaching; and the concepts of \"near enemy\" (governments of majority-Muslims countries) and \"far enemy\" (United States and other Western countries). (The term \"near enemy\" was coined by Mohammed Abdul-Salam Farag who led the assassination of Anwar al-Sadat with Egyptian Islamic Jihad (EIJ) in 1981.) The \"far enemy\" was introduced and formally declared under attack by al-Qaeda in 1996.\nThe ideology saw its rise during the '90s when the Muslim world experienced numerous geopolitical crisis, notably the Algerian Civil War (1991\u20132002), Bosnian War (1992\u20131995), and the First Chechen War (1994\u20131996). Within these conflicts, political Islam often acted as a mobilizing factor for the local belligerents, who demanded financial, logistical and military support from al-Qaeda, in the exchange for active proliferation of the ideology. After the 1998 bombings of US embassies, September 11 attacks (2001), the US-led invasion of Afghanistan (2001) and Iraq (2003), Salafi Jihadism lost its momentum, being devastated by the US counterterrorism operations, culminating in bin Laden's death in 2011. After the Arab Spring (2011) and subsequent Syrian civil war (2011\u2013present), the remnants of al-Qaeda franchise in Iraq restored their capacity, rapidly developing into the Islamic State of Iraq and the Levant, spreading its influence throughout the conflict zones of MENA region and the globe. Salafi Jihadism makes up a minority of the contemporary Islamist movements.\nShi'i Islamism.\nAlthough most of the research and reporting about Islamism or political Islam has been focused on Sunni Islamist movements,\nIslamism exists in Twelver Shia Islam (the second largest branch of Islam that makes up approximately 10% of all Muslims.). Islamist Shi'ism, also known as Shi'i Islamism, is primarily but not exclusively\n associated with the thought of Ayatollah Ruhollah Khomeini, with the Islamist Revolution he led, Islamic Republic of Iran that he founded, and the religious-political activities and resources of the republic.\nCompared to the \"Types\" of Islamism mentioned above, Khomeinism differs from Wahhabism (which does not consider Shi'ism truly Islamic), Salafism (both orthodox or Jihadi\u2014Shi'a do not consider some of the most prominent salaf worthy of emulation), reformist Islamism (the Islamic Republic executed more than 3,400 political dissidents between June 1981 and March 1982 in the process of consolidating power).\nKhomeini and his followers helped translate the works of Maududi and Qutb into Persian and were influenced by them, but their views differed from them and other Sunni Islamists in being \"more leftist and more clerical\":\nKhomeini was a \"radical\" Islamist, like Qutb and unlike Maudidi. He believed that foreigners, Jews and their agents were conspiring \"to keep us backward, to keep us in our present miserable state\". Those who call themselves Muslims but were secular and Westernizing, were not just corrupt or misguided, but \"agents\" of the Western governments, helping to \"plunder\" Muslim lands as part of a long-term conspiracy against Islam. Only the rule of an Islamic jurist, administering Sharia law, stood between this abomination and justice, and could not wait for peaceful, gradual transition. It is the duty of Muslims to \"destroy\" \"all traces\" of any other sort of government other than true Islamic governance because these are \"systems of unbelief\". \"Troublesome\" groups that cause \"corruption in Muslim society,\" and damage \"Islam and the Islamic state\" are to be eliminated just as the Prophet Muhammad eliminated the Jews of Bani Qurayza. Islamic revolution to install \"the form of government willed by Islam\" will not end with one Islamic state in Iran. Once this government comes \"into being, none of the governments now existing in the world\" will \"be able to resist it;\" they will \"all capitulate\".\nRuling Islamic Jurist.\nKhomeini's form of Islamism was particularly unique in the world because it completely swept the old regime away, created a new regime with a new constitution, new institutions and a new concept of governance (the \"Velayat-e Faqih\"). A historical event, it changed militant Islam from a topic of limited impact and interest to a topic that few either inside or outside the Muslim world were unaware of. As he originally described it in lectures to his students, the system of \"Islamic Government\" was one where the leading Islamic jurist would enforce sharia law\u2014law which \"has absolute authority over all individuals and the Islamic government\". The jurist would not be elected, and no legislature would be needed since divine law called for rule by jurist and \"there is not a single topic in human life for which Islam has not provided instruction and established a norm\". Without this system, injustice, corruption, waste, exploitation and sin would reign, and Islam would decay. This plan was disclosed to his students and the religious community but not widely publicized. The constitution of the Islamic Republic written after the revolution did include a legislature and president, but supervising the entire government was a \"Supreme Leader\"/guardian jurist.\nIslamist Shi'ism has been crucial to the development of worldwide Islamism, because the Iranian regime attempted to export its revolution. Although, the Islamist ideology was originally imported from Muslim Brotherhood, Iranian relations between the Muslim Brotherhood and Islamic Republic of Iran deteriorated due to its involvement in the Syrian civil war. However, the majority Usuli Shi'ism rejects the idea of an Islamist State in the period of Occultation of the Hidden Imam.\nShi'ism and Iran.\nTwelver Shia Muslim live mainly in a half dozen or so countries scattered around the Middle East and South Asia.\nThe Islamic Republic of Iran has become \"the de facto leader\" of the Shi'i world by virtue of being the largest Shia-majority state, having a long history of national cohesion and Shia-rule, being the site of the first and \"only true\" Islamist revolution (see History section below), and having the financial resources of a major petroleum exporter. Iran's influence has spread into a cultural-geographic area of \"Irano-Arab Shiism\", establishing Iranian regional power, supporting \"Shia militias and parties beyond its borders\", intertwining assistance to fellow Shi'a with \"Iranization\" of them.\nShi'i Islamism in Iran has been influenced by the Sunni Islamists and their organizations, particularly Sayyid Rashid Rida, Hassan al-Banna (founder of the Muslim Brotherhood organization), Sayyid Qutb, Abul A'la Maududi, \nbut has also been described as \"distinct\" from Sunni Muslim Brotherhood Islamism, \"more leftist and more clerical\", with its own historical influencers:\nExplanations for the growth and popularity of Islamism.\nSociological, economic and political.\nSome Western political scientists see the unchanging socio-economic condition in the Muslim world as a major factor. Olivier Roy believes \"the socioeconomic realities that sustained the Islamist wave are still here and are not going to change: poverty, uprootedness, crises in values and identities, the decay of the educational systems, the North-South opposition, and the problem of immigrant integration into the host societies\".\nCharitable work.\nIslamist movements such as the Muslim Brotherhood, \"are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups.\" All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.\nEconomic stagnation.\nThe Arab world\u2014the original heart of the Muslim world\u2014has been afflicted with economic stagnation. For example, it has been estimated that in the mid-1990s the exports of Finland, a country of five million, exceeded those of the entire Arab world of 260 million, excluding oil revenue.\nSociology of rural migration.\nDemographic transition (caused by the gap in time between the lowering of death rates from medical advances and the lowering of fertility rates), leads to population growth beyond the ability of housing, employment, public transit, sewer and water to provide. Combined with economic stagnation, urban agglomerations have been created in Cairo, Istanbul, Tehran, Karachi, Dhaka, and Jakarta, each with well over 12 million citizens, millions of them young and unemployed or underemployed. Such a demographic, alienated from the westernized ways of the urban elite, but uprooted from the comforts and more passive traditions of the villages they came from, is understandably favourably disposed to an Islamic system promising a better world\u2014an ideology providing an \"emotionally familiar basis for group identity, solidarity, and exclusion; an acceptable basis for legitimacy and authority; an immediately intelligible formulation of principles for both a critique of the present and a program for the future.\" One American anthropologist in Iran in the early 1970s (before the revolution), when comparing a \"stable village with a new urban slum\", discovered that where \"the villagers took religion with a grain of salt and even ridiculed visiting preachers\", the slum dwellers\u2014all recently dispossessed peasants \u2013 \"used religion as a substitute for their lost communities, oriented social life around the mosque, and accepted with zeal the teachings of the local mullah\".\nGilles Kepel also notes that Islamist uprisings in Iran and Algeria, though a decade apart, coincided with the large numbers of youth who were \"the first generation taught en masse to read and write and had been separated from their own rural, illiterate progenitors by a cultural gulf that radical Islamist ideology could exploit\". Their \"rural, illiterate\" parents were too settled in tradition to be interested in Islamism and their children \"more likely to call into question the utopian dreams of the 1970s generation\", but they embraced revolutionary political Islam. Olivier Roy also asserts \"it is not by chance that the Iranian Revolution took place the very year the proportion of city-dweller in Iran passed the 50% mark\". and offers statistics in support for other countries (in 1990 Algeria, housing was so crowded that there was an average of eight inhabitants to a room, and 80% of youth aged 16 to 29 still lived with their parents). \"The old clan or ethnic solidarities, the clout of the elders, and family control are fading little by little in the face of changes in the social structure ...\"\nThis theory implies that a decline in illiteracy and rural emigration will mean a decline in Islamism.\nGeopolitics.\nState-sponsorship.\nSaudi Arabia.\nStarting in the mid-1970s the Islamic resurgence was funded by an abundance of money from Saudi Arabian oil exports. The tens of billions of dollars in \"petro-Islam\" largesse obtained from the recently heightened price of oil funded an estimated \"90% of the expenses of the entire faith.\"\nThroughout the Muslim world, religious institutions for people both young and old, from children's madrassas to high-level scholarships received Saudi funding,\n\"books, scholarships, fellowships, and mosques\" (for example, \"more than 1500 mosques were built and paid for with money obtained from public Saudi funds over the last 50 years\"), along with training in the Kingdom for the preachers and teachers who went on to teach and work at these universities, schools, mosques, etc.\nThe funding was also used to reward journalists and academics who followed the Saudis' strict interpretation of Islam; and satellite campuses were built around Egypt for Al-Azhar University, the world's oldest and most influential Islamic university.\nThe interpretation of Islam promoted by this funding was the strict, conservative Saudi-based Wahhabism or Salafism. In its harshest form it preached that Muslims should not only \"always oppose\" infidels \"in every way,\" but \"hate them for their religion ... for Allah's sake,\" that democracy \"is responsible for all the horrible wars of the 20th century,\" that Shia and other non-Wahhabi Muslims were infidels, etc. While this effort has by no means converted all, or even most Muslims to the Wahhabist interpretation of Islam, it has done much to overwhelm more moderate local interpretations, and has set the Saudi-interpretation of Islam as the \"gold standard\" of religion in minds of some or many Muslims.\nQatar.\nThough the much smaller Qatar could not provide the same level of funding as Saudi Arabia, it was also a petroleum exporter and also sponsored Islamist groups. Qatar backed the Muslim Brotherhood in Egypt even after the 2013 overthrow of the MB regime of Mohamed Morsi, with Qatar ruler Sheikh Tamim bin Hamad Al Thani denouncing the coup. In June 2016, Mohamed Morsi was sentenced to life for passing state secrets to Qatar.\nQatar has also backed Islamist factions in Libya, Syria and Yemen.\nIn Libya, Qatar supported Islamists with tens of millions of dollars in aid, military training and \"more than 20,000 tons of weapons\", both before and after the 2011 fall of Muammar Gaddafi.\nHamas, in Palestine, has received considerable financial support as well as diplomatic help.\nWestern support of Islamism during the Cold War.\nDuring the Cold War, particularly during the 1950s, during the 1960s, and during most of the 1970s, the U.S. and other countries in the Western Bloc occasionally attempted to take advantage of the rise of Islamic religiousity by directing it against secular leftist/communist/nationalist insurgents/adversaries, particularly against the Soviet Union and Eastern Bloc states, whose ideology was not just secular but anti-religious.\nIn 1957, U.S. President Eisenhower and senior U.S. foreign policy officials, agreed on a policy of using the communists' lack of religion against them: \"We should do everything possible to stress the 'holy war' aspect\" that has currency in the Middle East.\nDuring the 1970s and sometimes later, this aid sometimes went to fledgling Islamists and Islamist groups that later came to be seen as dangerous enemies. The US spent billions of dollars to aid the mujahideen Muslim Afghanistan enemies of the Soviet Union, and non-Afghan veterans of the war (such as Osama bin Laden) returned home with their prestige, \"experience, ideology, and weapons\", and had considerable impact.\nAlthough it is a strong opponent of Israel's existence, Hamas, officially founded in 1987, traces its origins back to institutions and clerics which were supported by Israel in the 1970s and 1980s. Israel tolerated and supported Islamist movements in Gaza, with figures like Ahmed Yassin, as Israel perceived them preferable to the secular and then more powerful al-Fatah with the PLO.\nEgyptian President Anwar Sadat\u00a0\u2013 whose policies included opening Egypt to Western investment (\"infitah\"); transferring Egypt's allegiance from the Soviet Union to the United States; and making peace with Israel\u2014released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists. His \"encouraging of the emergence of the Islamist movement\" was said to have been \"imitated by many other Muslim leaders in the years that followed.\" This \"gentlemen's agreement\" between Sadat and Islamists broke down in 1975 but not before Islamists came to completely dominate university student unions. Sadat was later assassinated and a formidable insurgency was formed in Egypt in the 1990s. The French government has also been reported to have promoted Islamist preachers \"in the hope of channeling Muslim energies into zones of piety and charity.\"\nHistory.\nOlivier Roy dates the beginning of the Islamism movement \"more or less in 1940\", and its development proceeding \"over half a century\".\nPreceding movements.\nSome Islamic revivalist movements and leaders which pre-date Islamism but share some characteristics with it include:\nEarly history.\nThe end of the 19th century saw the dismemberment of most of the Muslim Ottoman Empire by non-Muslim European colonial powers, despite the empire's spending massive sums on Western civilian and military technology to try to modernize and compete with the encroaching European powers. In the process the Ottomans went deep into debt to these powers.\nPreaching Islamic alternatives to this humiliating decline were Jamal ad-din al-Afghani (1837\u201397), Muhammad Abduh (1849\u20131905) and Rashid Rida (1865\u20131935). Abduh's student Rida is widely regarded as one of the \"ideological forefathers\" of contemporary Islamist movement, and along with early Salafiyya Hassan al-Banna, and Mustafa al-Siba'i, preached that a truly Islamic society would follow sharia law, reject taqlid, (the blind imitation of earlier authorities), restore the Caliphate.\nSayyid Rashid Rida.\nSyrian-Egyptian Islamic cleric Muhammad Rashid Rida was one of the earliest 20th-century Sunni scholars to articulate the modern concept of an Islamic state, influencing the Muslim Brotherhood and other Sunni Islamist movements. In his influential book \"al-Khilafa aw al-Imama al-'Uzma\" (\"The Caliphate or the Grand Imamate\"); Rida explained that the societies that properly obeyed \"Sharia\" would be successful alternatives to the disorder and injustice of both capitalism and socialism.\nThis society would be ruled by a Caliphate; the ruling Caliph (\"Khalifa\") governing through \"shura\" (consultation), and applying Sharia (Islamic laws) in partnership with Islamic juristic clergy, who would use \"Ijtihad\" to update \"fiqh\" by evaluating scripture. With the \"Khilafa\" providing true Islamic governance, Islamic civilization would be revitalised, the political and legal independence of the Muslim \"umma\" (community of Muslim believers) would be restored, and the heretical influences of Sufism would be cleansed from Islam. This doctrine would become the blueprint of future Islamist movements.\nMuhammad Iqbal.\nMuhammad Iqbal was a philosopher, poet and politician in British India, widely regarded as having inspired the Islamic Nationalism and Pakistan Movement in British India.\nIqbal expressed fears of secularism and secular nationalism weakening the spiritual foundations of Islam and Muslim society, and of India's Hindu-majority population crowding out Muslim heritage, culture and political influence. In \n1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India which inspired the Pakistan movement.\nHe also promoted pan-Islamic unity in his travels to Egypt, Afghanistan, Palestine and Syria.\nHis ideas later influenced many reformist Islamists, e.g., Muhammad Asad, Sayyid Abul Ala Maududi and Ali Shariati.\nSayyid Abul Ala Maududi.\nSayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Maududi was an Islamist ideologue and Hanafi Sunni scholar active in Hyderabad Deccan and later in Pakistan. Maududi was born to a clerical family and got his early education at home. At the age of eleven, he was admitted to a public school in Aurangabad. In 1919, he joined the Khilafat Movement and got closer to the scholars of Deoband. He commenced the \"Dars-i Nizami\" education under supervision of Deobandi seminary at the Fatihpuri mosque in Delhi. Trained as a lawyer he worked as a journalist, and gained a wide audience with his books (translated into many languages) which placed Islam in a modern context. His writings had a profound impact on Sayyid Qutb. Maududi also founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972.\nIn 1925, he wrote a book on Jihad, \"al-Jihad fil-Islam\" (), that can be regarded as his first contribution to Islamism. Maududi believed that Muslim society could not be Islamic without Sharia (influencing Qutb and Khomeini), and the establishment of an Islamic state to enforce it. The state would be based on the principles of: \"tawhid\" (unity of God), \"risala\" (prophethood) and \"khilafa\" (caliphate). Maududi was uninterested in violent revolution or populist policies such as those of the Iranian Revolution, but sought gradual change in the hearts and minds of individuals from the top of society downward through an educational process or \"da'wah\". Maududi believed that Islam was all-encompassing: \"Everything in the universe is 'Muslim' for it obeys God by submission to His laws.\" \"The man who denies God is called Kafir (concealer) because he conceals by his disbelief what is inherent in his nature and embalmed in his own soul.\"\nMuslim Brotherhood.\nRoughly contemporaneous with Maududi was the founding of the Muslim Brotherhood in Ismailiyah, Egypt in 1928 by Hassan al Banna. His was arguably the first, largest and most influential modern Islamic political/religious organization. Under the motto \"the Qur'an is our constitution\",\nit sought Islamic revival through preaching and also by providing basic community services including schools, mosques, and workshops. Like Maududi, Al Banna believed in the necessity of government rule based on Shariah law implemented gradually and by persuasion, and of eliminating all Western imperialist influence in the Muslim world.\nSome elements of the Brotherhood did engage in violence, assassinating Egypt's premier Mahmoud Fahmy El Nokrashy in 1948. MB founder Al-Banna was assassinated in retaliation three months later. The Brotherhood has suffered periodic repression in Egypt and has been banned several times, in 1948 and several years later following confrontations with Egyptian president Gamal Abdul Nasser, who jailed thousands of members for several years.\nThe Brotherhood expanded to many other countries, particularly in the Arab world. In Egypt, despite periodic repression\u2014for many years it was\ndescribed as \"semi-legal\"\u2014it was the only opposition group in Egypt able to field candidates during elections. In the 2011\u201312 Egyptian parliamentary election, the political parties identified as \"Islamist\" (the Brotherhood's Freedom and Justice Party, Salafi Al-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, the candidate of the Muslim Brotherhood's party, was the first democratically elected president of Egypt. However, he was deposed during the 2013 Egyptian coup d'\u00e9tat, after mass protests against what were perceived as undemocratic moves by him. Today, the Muslim Brotherhood is designated as a terrorist organization by Bahrain, Russia, Syria, Egypt, Saudi Arabia and the United Arab Emirates.\nSayyid Qutb (1906\u20131966).\nQutb, a leading member of the Muslim Brotherhood movement, is considered by some (Fawaz A. Gerges) to be \"the founding father and leading theoretician\" of modern jihadists, such as Osama bin Laden. He was executed for allegedly participating in a presidential assassination plot in 1966.\nMaududi's political ideas influenced Sayyid Qutb. Like Maududi, he believed Sharia was crucial to Islam, so the restoration of its full enforcement was vital to the world. Since Sharia had not been fully enforced for centuries, Islam had \"been extinct for a few centuries\". Qutb preached that Muslims must engage in a two-pronged attack of converting individuals through preaching Islam peacefully but also using \"physical power and jihad\". Force was necessary because \"those who have usurped the authority of God\" would not give up their power through friendly persuasion.\nLike Khomeini, whom he influenced he believed the West was engaged in a vicious centuries long war against Islam.\nSix-Day War (1967).\nThe defeat of the armies of several Arab states by Israel during the Six-Day War marked a significant moment in the Arab world. The loss, coupled with economic stagnation in these countries, was attributed by some to the secular Arab nationalism of the ruling regimes. This period saw a decline in the popularity and credibility of secular, socialist, and nationalist ideologies, such as Ba'athism, Arab socialism, and Arab nationalism. In contrast, various Islamist movements, both democratic and anti-democratic, inspired by figures like Maududi and Sayyid Qutb, began to gain influence.\nIranian Revolution (1978\u20131979).\nThe first modern \"Islamist state\" (with the possible exception of Zia's Pakistan) was established among the Shia of Iran. In a major shock to the rest of the world, Muslim and non-Muslim, a revolution led by Ayatollah Ruhollah Khomeini overthrew the secular, oil-rich, well-armed, pro-American monarchy of Shah Muhammad Reza Pahlavi. The revolution was an \"indisputable sea change\"; Islamism had been a topic of limited impact and interest before 1979, but after the revolution, \"nobody within the Muslim world or outside it\" remained unaware of militant Islam.\nEnthusiasm for the Iranian revolution in the Muslim world could be intense; \nand there were many reasons for optimism among Islamists outside Iran. Khomeini was implementing Islamic law. He was interested in Pan-Islamic (and pan-Islamist) unity and made efforts to \"bridge the gap\" between Shiites and Sunnis, declaring \"it permissible for Shiites to pray behind Sunni imams\", and forbidding Shiites from \"criticizing the Caliphs who preceded Ali\" (revered by Sunnis but not Shia). The Islamic Republic also downplayed Shia rituals (such as the Day of Ashura), and shrines Before the Revolution, Khomeini acolytes (such as today's Supreme Leader of Iran, Ali Khamenei), translated and championed the works of the Muslim Brotherhood jihadist theorist, Sayyid Qutb, and other Sunni Islamists/revivalists.\nThis campaign did not survive his death however. As previously submissive Shia (usually minorities) became more assertive, Sunnis saw mostly \"Shia mischief\" and a challenge to Sunni dominance. \"What followed was a Sunni-versus-Shia contest for dominance, and it grew intense.\" Animosity between the two sects in Iran and its neighbors is systemic as of 2014, with thousands killed from sectarian fighting in Iraq and Pakistan. Also tarnishing the revolution's image have been \"purges, executions, and atrocities\", and periodic and increasingly widespread domestic unrest and protest by young Iranians.\nAmong the \"most important by-products of the Iranian revolution\" (according to Mehrzad Boroujerdi as of 2014) include \"the emergence of Hezbollah in Lebanon, the moral boost provided to Shia forces in Iraq, the regional cold war against Saudi Arabia and Israel, lending an Islamic flavour to the anti-imperialist, anti-American sentiment in the Middle East, and inadvertently widening the Sunni-Shia cleavage\". The Islamic Republic has also maintained its hold on power in Iran in spite of US economic sanctions, and has created or assisted like-minded Shia terrorist groups in Iraq (SCIRI) and Lebanon (Hezbollah) (two Muslim countries that also have a large percentage of Shiites).\nThe campaign to overthrow the shah led by Khomeini had had a strong class flavor (Khomeini preached that the shah was widening the gap between rich and poor; condemning the working class to a life of poverty, misery, and drudgery, etc.); and the \"pro-rural and pro-poor\" approach has led to almost universal access to electricity and clean water, but critics of the regime complain of promises made and not kept: the \"sons of the revolution's leaders and the business class that decides to work within the rules of the regime ... flaunt their wealth, driving luxury sportscars around Tehran, posting Instagram pictures of their ski trips and beach trips around the world, all while the poor and the middle class are struggling to survive or maintain the appearance of a dignified life\" (according to Shadi Mokhtari). One commitment made (to his followers if not the Iranian public) that has been kept is Guardianship by the Islamic jurist. But Rather than strengthening Islam and eliminating secular values and practices, the \"regime has ruined the Iranian people's belief in religion\" (\"anonymous expert\").\nGrand Mosque seizure (1979).\nThe strength of the Islamist movement was manifest in an event which might have seemed sure to turn Muslim public opinion against fundamentalism, but did just the opposite. In 1979 the Grand Mosque in Mecca Saudi Arabia was seized by an armed fundamentalist group and held for over a week. Scores were killed, including many pilgrim bystanders in a gross violation of one of the most holy sites in Islam (and one where arms and violence are strictly forbidden).\nInstead of prompting a backlash against the movement that inspired the attackers, however, Saudi Arabia, already very conservative, responded by shoring up its fundamentalist credentials with even more Islamic restrictions. Crackdowns followed on everything from shopkeepers who did not close for prayer and newspapers that published pictures of women, to the selling of dolls, teddy bears (images of animate objects are considered haraam), and dog food (dogs are considered unclean).\nIn other Muslim countries, blame for and wrath against the seizure was directed not against fundamentalists, but against Islamic fundamentalism's foremost geopolitical enemy\u2014the United States. Ayatollah Khomeini sparked attacks on American embassies when he announced: \"It is not beyond guessing that this is the work of criminal American imperialism and international Zionism\", despite the fact that the object of the fundamentalists' revolt was the Kingdom of Saudi Arabia, America's major ally in the region. Anti-American demonstrations followed in the Philippines, Turkey, Bangladesh, India, the UAE, Pakistan, and Kuwait. The US Embassy in Libya was burned by protesters chanting pro-Khomeini slogans and the embassy in Islamabad, Pakistan was burned to the ground.\nPakistan's Islamization (1979).\nIn 1979, after the coup by Zia al-Haq, the leader brought in Hudud Ordinances. Some of these laws continue to exist in Pakistan to this day.\nSoviet invasion of Afghanistan (1979\u20131989).\nIn 1979, the Soviet Union deployed its 40th Army into Afghanistan, attempting to suppress an Islamic rebellion against an allied Marxist regime in the Afghan Civil War. The conflict, pitting indigenous impoverished Muslims (mujahideen) against an anti-religious superpower, galvanized thousands of Muslims around the world to send aid and sometimes to go themselves to fight for their faith. Leading this pan-Islamic effort was Palestinian 'alim Abdullah Yusuf Azzam. While the military effectiveness of these \"Afghan Arabs\" was marginal, an estimated 16,000 to 35,000 Muslim volunteers came from around the world to fight in Afghanistan.\nWhen the Soviet Union abandoned the Marxist Najibullah regime and withdrew from Afghanistan in 1989 (the regime finally fell in 1992), the victory was seen by many Muslims as the triumph of Islamic faith over superior military power and technology that could be duplicated elsewhere.\nThe jihadists gained legitimacy and prestige from their triumph both within the militant community and among ordinary Muslims, as well as the confidence to carry their jihad to other countries where they believed Muslims required assistance.\nThe collapse of the Soviet Union itself, in 1991, was seen by many Islamists, including Bin Laden, as the defeat of a superpower at the hands of Islam. Concerning the $6\u00a0billion in aid given by the US and Pakistan's military training and intelligence support to the mujahideen, bin Laden wrote: \"[T]he US has no mentionable role\" in \"the collapse of the Soviet Union... rather the credit goes to God and the \"mujahidin\"\" of Afghanistan.\nPersian Gulf War (1990\u20131991).\nAnother factor in the early 1990s that worked to radicalize the Islamist movement was the Gulf War, which brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait. Prior to 1990 Saudi Arabia played an important role in restraining the many Islamist groups that received its aid. But when Saddam, secularist and Ba'athist dictator of neighboring Iraq, attacked Kuwait (his enemy in the war), western troops came to protect the Saudi monarchy. Islamists accused the Saudi regime of being a puppet of the west.\nThese attacks resonated with conservative Muslims and the problem did not go away with Saddam's defeat either, since American troops remained stationed in the kingdom, and a de facto cooperation with the Palestinian-Israeli peace process developed. Saudi Arabia attempted to compensate for its loss of prestige among these groups by repressing those domestic Islamists who attacked it (bin Laden being a prime example), and increasing aid to Islamic groups (Islamist madrassas around the world and even aiding some violent Islamist groups) that did not, but its pre-war influence on behalf of moderation was greatly reduced. One result of this was a campaign of attacks on government officials and tourists in Egypt, a bloody civil war in Algeria and Osama bin Laden's terror attacks climaxing in the 9/11 attack.\nSocial and cultural triumph in the 2000s.\nBy the beginning of the twenty first century, \"the word secular, a label proudly worn\" in the 1960s and '70s was \"shunned\" and \"used to besmirch\" political foes in Egypt and the rest of the Muslim world. Islamists surpassed the small secular opposition parties in terms of \"doggedness, courage,\" \"risk-taking\" or \"organizational skills\". As of 2002,\nIn the Middle East and Pakistan, religious discourse dominates societies, the airwaves, and thinking about the world. Radical mosques have proliferated throughout Egypt. Book stores are dominated by works with religious themes ... The demand for sharia, the belief that their governments are unfaithful to Islam and that Islam is the answer to all problems, and the certainty that the West has declared war on Islam; these are the themes that dominate public discussion. Islamists may not control parliaments or government palaces, but they have occupied the popular imagination.\nOpinion polls in a variety of Islamic countries showed that significant majorities opposed groups like ISIS, but also wanted religion to play a greater role in public life.\n\"Post-Islamism\".\nBy 2020, approximately 40 years after the Islamic overthrow of the Shah of Iran and the seizure of the Grand Mosque by extremists, a number of observers (Olivier Roy, Mustafa Akyol, Nader Hashemi) detected a decline in the vigor and popularity of Islamism. Islamism had been an idealized/utopian concept to compare with the grim reality of the status quo, but in more than four decades it had failed to establish a \"concrete and viable blueprint for society\" despite repeated efforts (Olivier Roy); and instead had left a less than inspiring track record of its impact on the world (Nader Hashemi). Consequently,\nin addition to the trend towards moderation by Islamist or formerly Islamist parties (such as PKS of Indonesia, AKP of Turkey, and PAS of Malaysia) mentioned above, there has been a social/religious and sometimes political backlash against Islamist rule in countries like Turkey, Iran, and Sudan (Mustafa Akyol).\nWriting in 2020, Mustafa Akyol argues there has been a strong reaction by many Muslims against political Islam, including a weakening of religious faith\u2014the very thing Islamism was intended to strengthen. He suggests this backlash against Islamism among Muslim youth has come from all the \"terrible things\" that have happened in the Arab world in the twenty first century \"in the name of Islam\"\u2014such as the \"sectarian civil wars in Syria, Iraq and Yemen\".\nPolls taken by Arab Barometer in six Arab countries \u2013 Algeria, Egypt, Tunisia, Jordan, Iraq and Libya \u2013 found \"Arabs are losing faith in religious parties and leaders.\" In 2018\u201319, in all six countries, fewer than 20% of those asked whether they trusted Islamist parties answered in the affirmative. That percentage had fallen (in all six countries) from when the same question was asked in 2012\u201314. Mosque attendance also declined more than 10 points on average, and the share of those Arabs describing themselves as \"not religious\" went from 8% in 2013 to 13% in 2018\u201319. In Syria, Sham al-Ali reports \"rising apostasy among Syrian youths\".\nWriting in 2021, Nader Hashemi notes that in Iraq, Sudan, Tunisia, Egypt, Gaza, Jordan and other places were Islamist parties have come to power or campaigned to, \"one general theme stands. The popular prestige of political Islam has been tarnished by its experience with state power.\"\nIn Iran, hardline Ayatollah Mohammad-Taqi Mesbah Yazdi has complained, \"Iranians are evading religious teachings and turning to secularism.\"\nEven Islamist terrorism was in decline and tended \"to be local\" rather than pan-Islamic. As of 2021, Al-Qaeda consisted of \"a bunch of militias\" with no effective central command (Fareed Zakaria).\nCriticism.\nIslamism, or elements of Islamism, have been criticized on numerous grounds, including repression of free expression and individual rights, rigidity, hypocrisy, anti-semitism, misinterpreting the Quran and Sunnah, lack of true understanding of and innovations to Islam (bid'ah) \u2013 notwithstanding proclaimed opposition to any such innovation by Islamists.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "15014", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=15014", "title": "Instructional theory", "text": "Theory that offers explicit guidance on how to better help people learn and develop\nAn instructional theory is \"a theory that offers explicit guidance on how to better help people learn and develop.\" It provides insights about what is likely to happen and why with respect to different kinds of teaching and learning activities while helping indicate approaches for their evaluation. Instructional designers focus on how to best structure material and instructional behavior to facilitate learning.\nDevelopment.\nOriginating in the United States in the late 1970s, \"instructional theory\" is influenced by three basic theories in educational thought: behaviorism, the theory that helps us understand how people conform to predetermined standards; cognitivism, the theory that learning occurs through mental associations; and constructivism, the theory explores the value of human activity as a critical function of gaining knowledge. Instructional theory is heavily influenced by the 1956 work of Benjamin Bloom, a University of Chicago professor, and the results of his Taxonomy of Education Objectives\u2014one of the first modern codifications of the learning process. One of the first instructional theorists was Robert M. Gagne, who in 1965 published \"Conditions of Learning\" for the Florida State University's Department of Educational Research.\nDefinition.\nInstructional theory is different than learning theory. A learning theory \"describes\" how learning takes place, and an instructional theory \"prescribes\" how to better help people learn. Learning theories often inform instructional theory, and three general theoretical stances take part in this influence: behaviorism (learning as response acquisition), cognitivism (learning as knowledge acquisition), and constructivism (learning as knowledge construction). Instructional theory helps us create conditions that increases the probability of learning. Its goal is understanding the instructional system and to improve the process of instruction.\nOverview.\nInstructional theories identify what instruction or teaching should be like. It outlines strategies that an educator may adopt to achieve the learning objectives. Instructional theories are adapted based on the educational content and more importantly the learning style of the students. They are used as teaching guidelines/tools by teachers/trainers to facilitate learning. Instructional theories encompass different instructional methods, models and strategies.\nDavid Merrill's First Principles of Instruction discusses universal methods of instruction, situational methods and core ideas of the post-industrial paradigm of instruction.\nUniversal Methods of Instruction:\nSituational Methods:\nbased on different approaches to instruction\nbased on different learning outcomes:\nCore ideas for the Post-industrial Paradigm of Instruction:\nFour tasks of Instructional theory:\nCritiques.\nPaulo Freire's work appears to critique instructional approaches that adhere to the knowledge acquisition stance, and his work \"Pedagogy of the Oppressed\" has had a broad influence over a generation of American educators with his critique of various \"banking\" models of education and analysis of the teacher-student relationship.\nFreire explains, \"Narration (with the teacher as narrator) leads the students to memorize mechanically the narrated content. Worse yet, it turns them into \"containers\", into \"receptacles\" to be \"filled\" by the teacher. The more completely she fills the receptacles, the better a teacher she is. The more meekly the receptacles permit themselves to be filled, the better students they are.\" In this way he explains educator creates an act of depositing knowledge in a student. The student thus becomes a repository of knowledge. Freire explains that this system that diminishes creativity and knowledge suffers. Knowledge, according to Freire, comes about only through the learner by inquiry and pursuing the subjects in the world and through interpersonal interaction.\nFreire further states, \"In the banking concept of education, knowledge is a gift bestowed by those who consider themselves knowledgeable upon those whom they consider to know nothing. Projecting an absolute ignorance onto others, a characteristic of the ideology of oppression, negates education and knowledge as processes of inquiry. The teacher presents himself to his students as their necessary opposite; by considering their ignorance absolute, he justifies his own existence. The students, alienated like the slave in the Hegelian dialectic, accept their ignorance as justifying the teacher's existence\u2014but, unlike the slave, they never discover that they educate the teacher.\"\nFreire then offered an alternative stance and wrote, \"The raison d'etre of libertarian education, on the other hand, lies in its drive towards reconciliation. Education must begin with the solution of the teacher-student contradiction, by reconciling the poles of the contradiction so that both are simultaneously teachers and students.\"\nIn the article, \"A process for the critical analysis of instructional theory\", the authors use an ontology-building process to review and analyze concepts across different instructional theories. Here are their findings:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nLinking Premise to Practice: An Instructional Theory-Strategy Model Approach By: Bowden, Randall. Journal of College Teaching &amp; Learning, v5 n3 p69-76 Mar 2008"}
{"id": "15018", "revid": "33407744", "url": "https://en.wikipedia.org/wiki?curid=15018", "title": "Infusoria", "text": "Microorganisms in freshwater ponds\nInfusoria is a word used to describe various freshwater microorganisms, including ciliates, copepods, euglenoids, planktonic crustaceans, protozoa, unicellular algae and small invertebrates. Some authors (e.g., B\u00fctschli) have used the term as a synonym for Ciliophora. In modern, formal classifications, the term is considered obsolete; the microorganisms previously and colloquially referred to as Infusoria are mostly assigned to the clades Diaphoretickes (e.g. ciliates, most algae), Amorphea (e.g. crustaceans and other small animals) and Discoba (euglenids).\nIn other contexts, the term is used to define various aquatic microorganisms found in decomposing matter.\nAquarium use.\nCertain microorganisms, including cyclops and daphnia (among others), are sold as a supplemental fish food. Some fish stores or pet shops may have these infusoria available for live purchase, but typically they are sold in frozen cubes\u2014for example, by the Japan-based fish food brand Hikari. Still, some advanced aquarists, with especially large collections of fish, will breed and cultivate their own supplies of the microorganisms.\nInfusoria are especially used by aquarists and fish breeders to feed fish fry; because of their small sizes, infusoria can be used to rear newly-hatched offspring of many common (and also less common) aquarium species. Many average home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own cultures, either through utilizing their own existing aquarium water or by using one of the many commercial cultures available. \nInfusoria can be cultured at-home by soaking any decomposing vegetative matter, such as papaya or cucumber peels, in a jar of aged (i.e., chlorine-free) water, preferably from an existing aquarium setup. The culture starts to proliferate in two to three days, depending on temperature and light received. The water first turns cloudy because of a rise in levels of bacteria, but clears up once the infusoria consume them. At this point, the infusoria are usually visible to the naked eye as small, white motile specks. They can be easily fed to fish with the use of a large turkey-baster or by gently scooping with a very fine net. Additionally, the water in which the infusoria are kept in can be changed periodically, even one to two times per week, by draining and replacing up to 50% of the volume of water (for hygienic and maintenance purposes).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15019", "revid": "1306030117", "url": "https://en.wikipedia.org/wiki?curid=15019", "title": "ISO/IEC 8859-1", "text": "Character encoding\n&lt;templatestyles src=\"Mono/styles.css\" /&gt;\nISO/IEC\u00a08859-1:1998, \"Information technology\u20148-bit single-byte coded graphic character sets\u2014Part 1: Latin alphabet No. 1\", is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1987. ISO/IEC\u00a08859-1 encodes what it refers to as \"Latin alphabet no.\u00a01\", consisting of 191 characters from the Latin script. This character-encoding scheme is used throughout the Americas, Western Europe, Oceania, and much of Africa. It is the basis for some popular 8-bit character sets and the first two blocks of characters in Unicode.\nAs of \u00a02025[ [update]], 1.0% of all web sites use ISO/IEC 8859-1. It is the most declared single-byte character encoding, but as Web browsers and the HTML5 standard interpret them as the superset Windows-1252, these documents may include characters from that set. Some countries or languages show a higher usage than the global average, in 2025 Brazil according to website use, use is at 2.3%, and in Germany at 2.3%. \nISO-8859-1 was (according to the standard, at least) the default encoding of documents delivered via HTTP with a MIME type beginning with , the default encoding of the values of certain descriptive HTTP headers, and defined the repertoire of characters allowed in HTML\u00a03.2 documents. It is specified by many other standards. In practice, the superset encoding Windows-1252 is the more likely effective default and it is increasingly common for UTF-8 to work whether or not a standard specifies it.\nISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. The following other aliases are registered: iso-ir-100, csISOLatin1, latin1, l1, IBM819, Code page\u00a028591 a.k.a. Windows-28591 is used for it in Windows. IBM calls it code page\u00a0819 or CP819 (CCSID\u00a0819). Oracle calls it WE8ISO8859P1.\nCoverage.\nEach character is encoded as a single eight-bit code value. These code values can be used in almost any data interchange system to communicate in the following languages (while it may exclude correct quotation marks such as for many languages including German and Icelandic):\nModern languages with complete coverage.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nLanguages with incomplete coverage.\nISO-8859-1 was commonly used for certain languages, even though it lacks characters used by these languages. In most cases, only a few letters are missing or they are rarely used, and they can be replaced with characters that are in ISO-8859-1 using some form of typographic approximation. The following table lists such languages.\nThe letter \"\u00ff\", which appears in French only very rarely, mainly in city names such as L'Ha\u00ff-les-Roses and never at the beginning of words, is included only in lowercase form. The slot corresponding to its uppercase form is occupied by the lowercase letter \"\u00df\" from the German language, which did not have an uppercase form at the time when the standard was created.\nQuotation marks.\nTypographical (6- or 9-shaped) quotation marks are missing, as are any baseline quotation marks used by some of the supported languages. Only , , and are included. Some fonts will display the spacing grave accent (0x60) and the apostrophe (0x27) as a matching pair of oriented single quotation marks (see ), but this is not considered part of the modern standard.\nSuperscript digits.\nOnly 3 superscript digits have been encoded: codice_1 at 0xB2, codice_2 at 0xB3, and codice_3 at 0xB9, lacking the superscript digit 0 and digits 4\u20139. Additionally, none of the subscript digits have been encoded. A workaround would be to use rich text formatting for the digits not covered by this standard.\nEuro sign.\nThe euro sign was first presented to the public on 12 December 1996. Due to this character set being introduced in 1987, it does not include the euro sign. Later character sets similar to ISO/IEC 8859-1 include a euro sign, such as Windows-1252 and ISO/IEC 8859-15.\nHistory.\nISO\u00a08859-1 was based on the Multinational Character Set (MCS) used by Digital Equipment Corporation (DEC) in the popular VT220 terminal in 1983. It was developed within the European Computer Manufacturers Association (ECMA), and published in March 1985 as ECMA-94, by which name it is still sometimes known. The second edition of ECMA-94 (June 1986) also included ISO\u00a08859-2, ISO\u00a08859-3, and ISO\u00a08859-4 as part of the specification.\nThe original draft of ISO\u00a08859-1 placed French \"\u0152\" and \"\u0153\" at code points 215 (0xD7) and 247 (0xF7), as in the MCS. However, the delegate from France, being neither a linguist nor a typographer, falsely stated that these are not independent French letters on their own, but mere ligatures (like \"\ufb01\" or \"\ufb02\"), supported by the delegate team from Bull Publishing Company, who regularly did not print French with \"\u0152/\u0153\" in their house style at the time. An anglophone delegate from Canada insisted on retaining \"\u0152/\u0153\" but was rebuffed by the French delegate and the team from Bull. These code points were soon filled with \u00d7 and \u00f7 under the suggestion of the German delegation. Support for French was further reduced when it was again falsely stated that the letter \"\u00ff\" is \"not French\", resulting in the absence of the capital \"\u0178\". In fact, the letter \"\u00ff\" is found in a number of French proper names, and the capital letter has been used in dictionaries and encyclopedias. These characters were added to ISO/IEC\u00a08859-15:1999. BraSCII matches the original draft.\nIn 1985, Commodore adopted ECMA-94 for its new AmigaOS operating system. The Seikosha MP-1300AI impact dot-matrix printer, used with the Amiga\u00a01000, included this encoding.\nIn 1990, the first version of Unicode used the code points of ISO-8859-1 as the first 256 Unicode code points.\nIn 1992, the IANA registered the character map ISO_8859-1:1987, more commonly known by its preferred MIME name of ISO-8859-1 (note the extra hyphen over ISO\u00a08859-1), a superset of ISO\u00a08859-1, for use on the Internet. This map assigns the C0 and C1 control codes to the unassigned code values thus provides for 256 characters via every possible 8-bit value.\nSimilar character sets.\nISO/IEC 8859-15.\nISO/IEC\u00a08859-15 was developed in 1999, as an update of ISO/IEC\u00a08859-1. It provides some characters for French and Finnish text and the euro sign, which are missing from ISO/IEC\u00a08859-1. This required the removal of some infrequently used characters from ISO/IEC\u00a08859-1, including fraction symbols and letter-free diacritics: , , , , , , , and . Ironically, three of the newly added characters (, , and ) had already been present in DEC's 1983 Multinational Character Set (MCS), the predecessor to ISO/IEC\u00a08859-1 (1987). Since their original code points were now reused for other purposes, the characters had to be reintroduced under different, less logical code points.\nISO-IR-204, a more minor modification (called code page\u00a061235 by FreeDOS), had been registered in 1998, altering ISO-8859-1 by replacing the universal currency sign (\u00a4) with the euro sign (the same substitution made by ISO-8859-15).\nWindows-1252.\nThe popular Windows-1252 character set adds all the missing characters provided by ISO/IEC\u00a08859-15, plus a number of typographic symbols, by replacing the rarely used C1 controls in the range 128 to 159 (hex 80 to 9F). It is very common for Windows-1252 text to be mislabelled as ISO-8859-1. A common result was that all the quotes and apostrophes (produced by \"smart quotes\" in word-processing software) were replaced with question marks or boxes on non-Windows operating systems, making text difficult to read. Many Web browsers and e-mail clients will interpret ISO-8859-1 control codes as Windows-1252 characters, and that behavior was later standardized in HTML5.\nMac Roman.\nThe Apple Macintosh computer introduced a character encoding called Mac Roman in 1984. It was meant to be suitable for Western European desktop publishing. It is a superset of ASCII, and has most of the characters that are in ISO-8859-1 and all the extra characters from Windows-1252, but in a totally different arrangement. The few printable characters that are in ISO/IEC\u00a08859-1, but not in this set, are often a source of trouble when editing text on Web sites using older Macintosh browsers, including the last version of Internet Explorer for Mac.\nOther.\nDOS has code page 850, which has all printable characters that ISO-8859-1 has, albeit in a totally different arrangement, plus the most widely used graphic characters from code page\u00a0437.\nBetween 1989 and 2015, Hewlett-Packard used another superset of ISO-8859-1 on many of their calculators. This proprietary character set was sometimes referred to simply as \"ECMA-94\" as well. HP also has code page 1053, which adds the medium shade (\u2592, U+2592) at 0x7F.\nSeveral EBCDIC code pages were purposely designed to have the same set of characters as ISO-8859-1, to allow easy conversion between them.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15020", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=15020", "title": "ISO/IEC 8859", "text": "Series of standards for 8-bit character encodings\n&lt;templatestyles src=\"Mono/styles.css\" /&gt;\nISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.\nISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.\nIntroduction.\nWhile the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7\u00a0bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.\nThe ISO/IEC 8859 standard parts only define printable characters, although they explicitly set apart the byte ranges 0x00\u20131F and 0x7F\u20139F as \"combinations that do not represent graphic characters\" (i.e. which are reserved for use as control characters) in accordance with ISO/IEC 4873; they were designed to be used in conjunction with a separate standard defining the control functions associated with these bytes, such as ISO 6429 or ISO 6630. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-\"n\" as their preferred MIME name or, in cases where a preferred MIME name is not specified, their canonical name. Many people use the terms ISO/IEC 8859-\"n\" and ISO-8859-\"n\" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.\nCharacters.\nThe ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.\nAn inexact rule based on practical experience states that if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it did not get in. Hence the directional double quotation marks \"\u00ab\" and \"\u00bb\" used for some European languages were included, but not the directional double quotation marks \"\u201c\" and \"\u201d\" used for English and some other languages.\nFrench did not get its \"\u0153\" and \"\u0152\" ligatures because they could be typed as 'oe'. Likewise, \"\u0178\", needed for all-caps text, was dropped as well. Albeit under different codepoints, these three characters were later reintroduced with ISO/IEC 8859-15 in 1999, which also introduced the new euro sign character \u20ac. Likewise Dutch did not get the \"\u0133\" and \"\u0132\" letters, because Dutch speakers had become used to typing these as two letters instead.\nRomanian did not initially get its \"\u0218\"/\"\u0219\" and \"\u021a\"/\"\u021b\" (with comma) letters, because these letters were initially unified with \"\u015e\"/\"\u015f\" and \"\u0162\"/\"\u0163\" (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.\nMost of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters, although the Thai, Hebrew, and Arabic ones do also contain combining characters.\nThe standard makes no provision for the scripts of East Asian languages (\"CJK\"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics such as in Windows-1258) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, as in JIS X 0201, but like several other alphabets of the world they are not encoded in the ISO/IEC 8859 system.\nThe parts of ISO/IEC 8859.\nISO/IEC 8859 is divided into the following parts:\nEach part of ISO/IEC 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1\u20134, 9, 10, 13\u201316), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1\u20134 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.\nTable.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0unassigned code points.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0new additions in and versions, previously unassigned.\nRelationship to Unicode and the UCS.\nSince 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the \"U+nnnn\" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).\nSingle-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.\nCurrent status.\nThe ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of Unicode's Universal Coded Character Set.\nThe WHATWG Encoding Standard, which specifies the character encodings permitted in HTML5 which compliant browsers must support, includes most parts of ISO/IEC 8859, except for parts 1, 9 and 11, which are instead interpreted as Windows-1252, Windows-1254 and Windows-874 respectively. Authors of new pages and the designers of new protocols are instructed to use UTF-8 instead.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15021", "revid": "119438", "url": "https://en.wikipedia.org/wiki?curid=15021", "title": "Imad Mughniyeh (version 2)", "text": ""}
{"id": "15022", "revid": "36761478", "url": "https://en.wikipedia.org/wiki?curid=15022", "title": "Infrared", "text": "Form of electromagnetic radiation\nInfrared (IR; sometimes called infrared light) is electromagnetic radiation (EMR) with wavelengths longer than that of visible light but shorter than microwaves. The infrared spectral band begins with the waves that are just longer than those of red light (the longest waves in the visible spectrum), so IR is invisible to the human eye. IR is generally (according to ISO, CIE) understood to include wavelengths from around to . IR is commonly divided between longer-wavelength thermal IR, emitted from terrestrial sources, and shorter-wavelength IR or near-IR, part of the solar spectrum. Longer IR wavelengths (30\u2013100\u00a0\u03bcm) are sometimes included as part of the terahertz radiation band. Almost all black-body radiation from objects near room temperature is in the IR band. As a form of EMR, IR carries energy and momentum, exerts radiation pressure, and has properties corresponding to both those of a wave and of a particle, the photon.\nIt was long known that fires emit invisible heat; in 1681 the pioneering experimenter Edme Mariotte showed that glass, though transparent to sunlight, obstructed radiant heat. In 1800 the astronomer Sir William Herschel discovered that infrared radiation is a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the energy from the Sun was eventually found, through Herschel's studies, to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has an important effect on Earth's climate.\nInfrared radiation is emitted or absorbed by molecules when changing rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range.\nInfrared radiation is used in industrial, scientific, military, commercial, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, to detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, to assist firefighting, and to detect the overheating of electrical components. Military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10\u00a0\u03bcm. Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.\nDefinition and relationship to the electromagnetic spectrum.\nThere is no universally accepted definition of the range of infrared radiation. Typically, it is taken to extend from the nominal red edge of the visible spectrum at 780 nm to 1\u00a0mm. This range of wavelengths corresponds to a frequency range of approximately 430\u00a0THz down to 300\u00a0GHz. Beyond infrared is the microwave portion of the electromagnetic spectrum. Increasingly, terahertz radiation is counted as part of the microwave band, not infrared, moving the band edge of infrared to 0.1\u00a0mm (3\u00a0THz).\nNature.\nSunlight, at an effective temperature of 5,780\u00a0K (5,510\u00a0\u00b0C, 9,940\u00a0\u00b0F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1\u00a0kW per square meter at sea level. Of this energy, 527 W is infrared radiation, 445 W is visible light, and 32 W is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 \u03bcm.\nOn the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. Black-body, or thermal, radiation is continuous: it radiates at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy.\nRegions.\nIn general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law. The infrared band is often subdivided into smaller sections, although how the IR spectrum is thereby divided varies between different areas in which IR is employed.\nVisible limit.\nInfrared radiation is generally considered to begin with wavelengths longer than visible by the human eye. There is no hard wavelength limit to what is visible, as the eye's sensitivity decreases rapidly but smoothly, for wavelengths exceeding about 700\u00a0nm. Therefore wavelengths just longer than that can be seen if they are sufficiently bright, though they may still be classified as infrared according to usual definitions. Light from a near-IR laser may thus appear dim red and can present a hazard since it may actually carry a large amount of energy. Even IR at wavelengths up to 1,050\u00a0nm from pulsed lasers can be seen by humans under certain conditions.\nCommonly used subdivision scheme.\nA commonly used subdivision scheme is:\nNIR and SWIR together is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\".\nCIE division scheme.\nThe International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands:\nISO 20473 scheme.\nISO 20473 specifies the following scheme:\nAstronomy division scheme.\nAstronomers typically divide the infrared spectrum as follows:\nThese divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space.\nThe most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers.\nSensor response division scheme.\nA third scheme divides up the band based on the response of various detectors:\nNear-infrared is the region closest in wavelength to the radiation detectable by the human eye. Mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050\u00a0nm, while InGaAs's sensitivity starts around 950\u00a0nm and ends between 1,700 and 2,600\u00a0nm, depending on the specific configuration). No international standards for these specifications are currently available.\nThe onset of infrared is defined (according to different standards) at various values typically between 700\u00a0nm and 800\u00a0nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700\u00a0nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. Particularly intense near-IR light (e.g., from lasers, LEDs or bright daylight with the visible light filtered out) can be detected up to approximately 780\u00a0nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1,050\u00a0nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect, which consists of IR-glowing foliage.\nTelecommunication bands.\nIn optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources, transmitting/absorbing materials (fibers), and detectors:\nThe C-band is the dominant band for long-distance telecommunications networks. The S and L bands are based on less well established technology, and are not as widely deployed.\nHeat.\nInfrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25\u00a0\u03bcm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law).\nHeat is energy in transit that flows due to a temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that are associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiation is associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (e.g. the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth.\nThe concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the ideal of a black body. To further explain, two objects at the same physical temperature may not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler (assuming, as is often the case, that the surrounding environment is cooler than the objects being viewed). When an object has less than perfect emissivity, it obtains properties of reflectivity and/or transparency, and so the temperature of the surrounding environment is partially reflected by and/or transmitted through the object. If the object were in a hotter environment, then a lower emissivity object at the same temperature would likely appear to be hotter than a more emissive one. For that reason, incorrect selection of emissivity and not accounting for environmental temperatures will give inaccurate results when using infrared cameras and pyrometers.\nApplications.\nNight vision.\n Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source.\nThe use of infrared light and night vision devices should not be confused with thermal imaging, which creates images based on differences in surface temperature by detecting infrared radiation (heat) that emanates from objects and their surrounding environment.\nThermography.\nInfrared radiation can be used to remotely determine the temperature of objects (if the emissivity is known). This is termed thermography, or in the case of very hot objects in the NIR or visible it is termed pyrometry. Thermography (thermal imaging) is mainly used in military and industrial applications but the technology is reaching the public market in the form of infrared cameras on cars due to greatly reduced production costs.\nThermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 9,000\u201314,000 nm or 9\u201314\u00a0\u03bcm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black-body radiation law, thermography makes it possible to \"see\" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name).\nHyperspectral imaging.\nA hyperspectral image is a \"picture\" containing continuous spectrum through a wide spectral range at each pixel. Hyperspectral imaging is gaining importance in the field of applied spectroscopy particularly with NIR, SWIR, MWIR, and LWIR spectral regions. Typical applications include biological, mineralogical, defence, and industrial measurements.\nThermal infrared hyperspectral imaging can be similarly performed using a thermographic camera, with the fundamental difference that each pixel contains a full LWIR spectrum. Consequently, chemical identification of the object can be performed without a need for an external light source such as the Sun or the Moon. Such cameras are typically applied for geological measurements, outdoor surveillance and UAV applications.\nOther imaging.\nIn infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can view intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy.\nTracking.\nInfrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as \"heat-seekers\" since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background.\nHeating.\nInfrared radiation can be used as a deliberate heating source. For example, it is used in infrared saunas to heat the occupants. It may also be used in other heating applications, such as to remove ice from the wings of aircraft (de-icing).\nInfrared heating is also becoming more popular in industrial manufacturing processes, e.g. curing of coatings, forming of plastics, annealing, plastic welding, and print drying. In these applications, infrared heaters replace convection ovens and contact heating.\nCooling.\nA variety of technologies or proposed technologies take advantage of infrared emissions to cool buildings or other systems. The LWIR (8\u201315\u00a0\u03bcm) region is especially useful since some radiation at these wavelengths can escape into space through the atmosphere's infrared window. This is how passive daytime radiative cooling (PDRC) surfaces are able to achieve sub-ambient cooling temperatures under direct solar intensity, enhancing terrestrial heat flow to outer space with zero energy consumption or pollution. PDRC surfaces maximize shortwave solar reflectance to lessen heat gain while maintaining strong longwave infrared (LWIR) thermal radiation heat transfer. When imagined on a worldwide scale, this cooling method has been proposed as a way to slow and even reverse global warming, with some estimates proposing a global surface area coverage of 1-2% to balance global heat fluxes.\nCommunications.\nIR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that may be concentrated by a lens into a beam that the user aims at the detector. The beam is modulated, i.e. switched on and off, according to a code which the receiver interprets. Usually very near-IR is used (below 800\u00a0nm) for practical reasons. This wavelength is efficiently detected by inexpensive silicon photodiodes, which the receiver uses to convert the detected radiation to an electric current. That electrical signal is passed through a high-pass filter which retains the rapid pulsations due to the IR transmitter but filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances.\nInfrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.\nFree-space optical communication using infrared lasers can be a relatively inexpensive way to install a communications link in an urban area operating at up to 4 gigabit/s, compared to the cost of burying fiber optic cable, except for the radiation damage. \"Since the eye cannot detect IR, blinking or closing the eyes to help prevent or reduce damage may not happen.\"\nInfrared lasers are used to provide the light for optical fiber communications systems. Wavelengths around 1,330\u00a0nm (least dispersion) or 1,550\u00a0nm (best transmission) are the best choices for standard silica fibers.\nIR data transmission of audio versions of printed signs is being researched as an aid for visually impaired people through the Remote infrared audible signage project.\nTransmitting IR data from one device to another is sometimes referred to as beaming.\nIR is sometimes used for assistive audio as an alternative to an audio induction loop.\nSpectroscopy.\nInfrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH2) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from the mid-infrared, 4,000\u2013400\u00a0cm\u22121. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200\u00a0cm\u22121). The unit for expressing radiation in this application, cm\u22121, is the spectroscopic wavenumber. It is the frequency divided by the speed of light in vacuum.\nThin film metrology.\nIn the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi\u2013Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures.\nMeteorology.\nWeather satellites equipped with scanning radiometers produce thermal or infrared images, which can then enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. The scanning is typically in the range 10.3\u201312.5\u00a0\u03bcm (IR4 and IR5 channels).\nClouds with high and cold tops, such as cyclones or cumulonimbus clouds, are often displayed as red or black, lower warmer clouds such as stratus or stratocumulus are displayed as blue or grey, with intermediate clouds shaded accordingly. Hot land surfaces are shown as dark-grey or black. One disadvantage of infrared imagery is that low clouds such as stratus or fog can have a temperature similar to the surrounding land or sea surface and do not show up. However, using the difference in brightness of the IR4 channel (10.3\u201311.5\u00a0\u03bcm) and the near-infrared channel (1.58\u20131.64\u00a0\u03bcm), low clouds can be distinguished, producing a \"fog\" satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied.\nThese infrared pictures can depict ocean eddies or vortices and map currents such as the Gulf Stream, which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Ni\u00f1o phenomena can be spotted. Using color-digitized techniques, the gray-shaded thermal images can be converted to color for easier identification of desired information.\nThe main water vapour channel at 6.40 to 7.08\u00a0\u03bcm can be imaged by some weather satellites and shows the amount of moisture in the atmosphere.\nClimatology.\nIn the field of climatology, atmospheric infrared radiation is monitored to detect trends in the energy exchange between the Earth and the atmosphere. These trends provide information on long-term changes in Earth's climate. It is one of the primary parameters studied in research into global warming, together with solar radiation.\nA pyrgeometer is utilized in this field of research to perform continuous outdoor measurements. This is a broadband infrared radiometer with sensitivity for infrared radiation between approximately 4.5\u00a0\u03bcm and 50\u00a0\u03bcm.\nAstronomy.\nAstronomers observe objects in the infrared portion of the electromagnetic spectrum using optical components, including mirrors, lenses and solid state digital detectors. For this reason it is classified as part of optical astronomy. To form an image, the components of an infrared telescope need to be carefully shielded from heat sources, and the detectors are chilled using liquid helium.\nThe sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy.\nThe infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.)\nInfrared light is also useful for observing the cores of active galaxies, which are often cloaked in gas and dust. Distant galaxies with a high redshift will have the peak portion of their spectrum shifted toward longer wavelengths, so they are more readily observed in the infrared.\nCleaning.\nInfrared cleaning is a technique used by some motion picture film scanners, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting.\nArt conservation and analysis.\nInfrared reflectography can be applied to paintings to reveal underlying layers in a non-destructive manner, in particular the artist's underdrawing or outline drawn as a guide. Art conservators use the technique to examine how the visible layers of paint differ from the underdrawing or layers in between (such alterations are called pentimenti when made by the original artist). This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti, the more likely a painting is to be the prime version. It also gives useful insights into working practices. Reflectography often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Infrared reflectography can be realized by modified commercial digital cameras in the NIR spectral region or by dedicated instruments in the SWIR spectral region. The recent extension of reflectography into the MWIR spectral region has proved capable of detecting subtle differences in surface materials. \nFinally, NIR reflectography can be performed with good results using smartphone cameras .\nRecent progress in the design of infrared-sensitive cameras makes it possible to discover and depict not only underpaintings and pentimenti, but entire paintings that were later overpainted by the artist. Notable examples are Picasso's \"Woman Ironing\" and \"Blue Room\", where in both cases a portrait of a man has been made visible under the painting as it is known today.\nSimilar uses of infrared are made by conservators and scientists on various types of objects, especially very old written documents such as the Dead Sea Scrolls, the Roman works in the Villa of the Papyri, and the Silk Road texts found in the Dunhuang Caves. Carbon black used in ink can show up extremely well.\nBiological systems.\nThe pit viper has a pair of infrared sensory pits on its head. There is uncertainty regarding the exact thermal sensitivity of this biological infrared detection system.\nOther organisms that have thermoreceptive organs are pythons (family Pythonidae), some boas (family Boidae), the Common Vampire Bat (\"Desmodus rotundus\"), a variety of jewel beetles (\"Melanophila acuminata\"), darkly pigmented butterflies (\"Pachliopta aristolochiae\" and \"Troides rhadamantus plateni\"), and possibly blood-sucking bugs (\"Triatoma infestans\"). By detecting the heat that their prey emits, crotaline and boid snakes identify and capture their prey using their IR-sensitive pit organs. Comparably, IR-sensitive pits on the Common Vampire Bat (\"Desmodus rotundus\") aid in the identification of blood-rich regions on its warm-blooded victim. The jewel beetle, \"Melanophila acuminata\", locates forest fires via infrared pit organs, where on recently burnt trees, they deposit their eggs. Thermoreceptors on the wings and antennae of butterflies with dark pigmentation, such \"Pachliopta aristolochiae\" and \"Troides rhadamantus plateni\", shield them from heat damage as they sunbathe in the sun. Additionally, it's hypothesised that thermoreceptors let bloodsucking bugs (\"Triatoma infestans\") locate their warm-blooded victims by sensing their body heat.\nSome fungi like \"Venturia inaequalis\" require near-infrared light for ejection.\nAlthough near-infrared vision (780\u20131,000\u00a0nm) has long been deemed impossible due to noise in visual pigments, sensation of near-infrared light was reported in the common carp and in three cichlid species. Fish use NIR to capture prey and for phototactic swimming orientation. NIR sensation in fish may be relevant under poor lighting conditions during twilight and in turbid surface waters.\nPhotobiomodulation.\nNear-infrared light, or photobiomodulation, is used for treatment of chemotherapy-induced oral ulceration as well as wound healing. There is some work relating to anti-herpes virus treatment. Research projects include work on central nervous system healing effects via cytochrome c oxidase upregulation and other possible mechanisms.\nHealth hazards.\nStrong infrared radiation in certain industry high-heat settings may be hazardous to the eyes, resulting in damage or blindness to the user. Since the radiation is invisible, special IR-proof goggles must be worn in such places.\nScientific history.\nThe discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them \"Calorific Rays\". The term \"infrared\" did not appear until late 19th century. The Latin prefix \"infra-\" means below, as it is light below red on the spectrum. An earlier experiment in 1790 by Marc-Auguste Pictet demonstrated the reflection and focusing of radiant heat via mirrors in the absence of visible light.\nOther important dates include:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15023", "revid": "609725", "url": "https://en.wikipedia.org/wiki?curid=15023", "title": "Icosidodecahedron", "text": "Archimedean solid with 32 faces\nIn geometry, an icosidodecahedron or pentagonal gyrobirotunda is a polyhedron with twenty (\"icosi-\") triangular faces and twelve (\"dodeca-\") pentagonal faces. An icosidodecahedron has 30 identical vertices, with two triangles and two pentagons meeting at each, and 60 identical edges, each separating a triangle from a pentagon. As such, it is one of the Archimedean solids and more particularly, a quasiregular polyhedron.\nConstruction.\nOne way to construct the icosidodecahedron is to start with two pentagonal rotunda by attaching them to their bases. These rotundas cover their decagonal base so that the resulting polyhedron has 32 faces, 30 vertices, and 60 edges. This construction is similar to one of the Johnson solids, the pentagonal orthobirotunda. The difference is that the icosidodecahedron is constructed by twisting its rotundas by 36\u00b0, a process known as gyration, resulting in the pentagonal face connecting to the triangular one. The icosidodecahedron has an alternative name, \"pentagonal gyrobirotunda\".\nThere is another way to construct it, and that is rectification of an icosahedron or a dodecahedron.\nConvenient Cartesian coordinates for the vertices of an icosidodecahedron with unit edges are given by the even permutations of:\nformula_1\nwhere formula_2 denotes the golden ratio.\nProperties.\nThe surface area of an icosidodecahedron A can be determined by calculating the area of all pentagonal faces. The volume of an icosidodecahedron V can be determined by slicing it off into two pentagonal rotunda, after which summing up their volumes. Therefore, its surface area and volume can be formulated as:\nformula_3\nThe dihedral angle of an icosidodecahedron between pentagon-to-triangle is\nformula_4\ndetermined by calculating the angle of a pentagonal rotunda.\nAn icosidodecahedron has icosahedral symmetry, and its first stellation is the compound of a dodecahedron and its dual icosahedron, with the vertices of the icosidodecahedron located at the midpoints of the edges of either.\nThe icosidodecahedron is an Archimedean solid, meaning it is a highly symmetric and semi-regular polyhedron, and two or more different regular polygonal faces meet in a vertex. The polygonal faces that meet for every vertex are two equilateral triangles and two regular pentagons, and the vertex figure of an icosidodecahedron is formula_5. Its dual polyhedron is rhombic triacontahedron, a Catalan solid.\nThe icosidodecahedron has 6 central decagons. Projected into a sphere, they define 6 great circles. used these 6 great circles, along with 15 and 10 others in two other polyhedra to define his 31 great circles of the spherical icosahedron.\nThe long radius (center to vertex) of the icosidodecahedron is in the golden ratio to its edge length; thus its radius is \u03c6 if its edge length is 1, and its edge length is if its radius is 1. Only a few uniform polytopes have this property, including the four-dimensional 600-cell, the three-dimensional icosidodecahedron, and the two-dimensional decagon. (The icosidodecahedron is the equatorial cross-section of the 600-cell, and the decagon is the equatorial cross-section of the icosidodecahedron.) These \"radially golden\" polytopes can be constructed, with their radii, from golden triangles which meet at the center, each contributing two radii and an edge.\nRelated polytopes.\nThe icosidodecahedron is a rectified dodecahedron and also a rectified icosahedron, existing as the full-edge truncation between these regular solids.\nThe icosidodecahedron contains 12 pentagons of the dodecahedron and 20 triangles of the icosahedron:\nThe icosidodecahedron exists in a sequence of symmetries of quasiregular polyhedra and tilings with vertex configurations (3.\"n\")2, progressing from tilings of the sphere to the Euclidean plane and into the hyperbolic plane. With orbifold notation symmetry of *\"n\"32 all of these tilings are wythoff construction within a fundamental domain of symmetry, with generator points at the right angle corner of the domain.\nRelated polyhedra.\nThe truncated cube can be turned into an icosidodecahedron by dividing the octagons into two pentagons and two triangles. It has pyritohedral symmetry.\nEight uniform star polyhedra share the same vertex arrangement. Of these, two also share the same edge arrangement: the small icosihemidodecahedron (having the triangular faces in common), and the small dodecahemidodecahedron (having the pentagonal faces in common). The vertex arrangement is also shared with the compounds of five octahedra and of five tetrahemihexahedra.\nRelated polychora.\nIn four-dimensional geometry, the icosidodecahedron appears in the regular 600-cell as the equatorial slice that belongs to the vertex-first passage of the 600-cell through 3D space. In other words: the 30 vertices of the 600-cell which lie at arc distances of 90 degrees on its circumscribed hypersphere from a pair of opposite vertices, are the vertices of an icosidodecahedron. The wireframe figure of the 600-cell consists of 72 flat regular decagons. Six of these are the equatorial decagons to a pair of opposite vertices, and these six form the wireframe figure of an icosidodecahedron.\nIf a 600-cell is stereographically projected to 3-space about any vertex and all points are normalised, the geodesics upon which edges fall comprise the icosidodecahedron's barycentric subdivision.\nGraph.\nThe skeleton of an icosidodecahedron can be represented as the symmetric graph with 30 vertices and 60 edges, one of the Archimedean graphs. It is a symmetric quartic graph, meaning that each vertex is connected to four other vertices.\nThe related \"hemi-icosidodecahederal graph\" exists in the real projective plane, with 15 vertices and 30 edges. It is also a symmetric quartic graph. It can be drawn inside of a regular decagon perimeter, with opposite vertices and edges glued together.\nApplications.\nThe icosidodecahedron may appear in structures, as in the geodesic dome or the Hoberman sphere.\nIcosidodecahedra can be found in all eukaryotic cells, including human cells, as Sec13/31 COPII coat-protein formations.\nThe icosidodecahedron may also found in popular culture. In Star Trek universe, the Vulcan game of logic Kal-Toh has the goal of creating a shape with two nested holographic icosidodecahedra joined at the midpoints of their segments.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15024", "revid": "44572431", "url": "https://en.wikipedia.org/wiki?curid=15024", "title": "ISO 8601", "text": "International standards for dates and times\nISO 8601 is an international standard covering the worldwide exchange and communication of date and time-related data. It is maintained by the International Organization for Standardization (ISO) and was first published in 1988, with updates in 1991, 2000, 2004, and 2019, and an amendment in 2022. The standard provides a well-defined, unambiguous method of representing calendar dates and times in worldwide communications, especially to avoid misinterpreting numeric dates and times when such data is transferred between countries with different conventions for writing numeric dates and times.\nISO\u00a08601 applies to these representations and formats: \"dates\", in the Gregorian calendar (including the proleptic Gregorian calendar); \"times\", based on the 24-hour timekeeping system, with optional UTC offset; \"time intervals\"; and combinations thereof. The standard does not assign specific meaning to any element of the dates/times represented: the meaning of any element depends on the context of its use. Dates and times represented cannot use words that do not have a specified numerical meaning within the standard (thus excluding names of years in the Chinese calendar), or that do not use computer characters (excludes images or sounds).\nIn representations that adhere to the ISO\u00a08601 \"interchange standard\", dates and times are arranged such that the greatest temporal term (typically a year) is placed at the left and each successively lesser term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and the specific computer characters (such as \"&amp;hyphen;\", \":\", \"T\", \"W\", \"Z\") that are assigned specific meanings within the standard; that is, such commonplace descriptors of dates (or parts of dates) as \"January\", \"Thursday\", or \"New Year's Day\" are not allowed in interchange representations within the standard.\nHistory.\nThe first edition of the ISO\u00a08601 standard was published as \"ISO\u00a08601:1988\" in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition \"ISO\u00a08601:2000\" in 2000, by a third edition \"ISO\u00a08601:2004\" published on 1 December 2004, and withdrawn and revised by \"ISO\u00a08601-1:2019\" and \"ISO\u00a08601-2:2019\" on 25 February 2019. ISO\u00a08601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.\nISO\u00a02014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order [YYYY]-[MM]-[DD]. The ISO week numbering system was introduced in ISO\u00a02015, and the identification of days by ordinal dates was originally defined in ISO\u00a02711.\nIssued in February 2019, the fourth revision of the standard ISO\u00a08601-1:2019 represents slightly updated contents of the previous ISO\u00a08601:2004 standard, whereas the new ISO\u00a08601-2:2019 defines various extensions such as uncertainties or parts of the Extended Date/Time Format (EDTF).\nAn amendment to ISO 8601-1 was published in October 2022 featuring minor technical clarifications and attempts to remove ambiguities in definitions. The most significant change, however, was the reintroduction of the \"24:00:00\" format to refer to the instant at the \"end\" of a calendar day.\nAn amendment to ISO 8601-2 was published in January 2025.\nDates.\nThe standard uses the Gregorian calendar, which \"serves as an international standard for civil use\".\nISO\u00a08601 allows Gregorian dates from the introduction of the calendar on 15 October 1582. For earlier (pre-Gregorian) dates, the calendar may be extended before its introduction (the proleptic Gregorian calendar) by explicit agreement of the parties involved. Such proleptic dates may not be adjusted to reconcile them with Julian dates.\nYears.\nISO\u00a08601 prescribes, as a minimum, a four-digit year [YYYY] to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD, similar to astronomical year numbering. However, years before 1583 (the first full year following the introduction of the Gregorian calendar) are not automatically allowed by the standard. Instead, the standard states that \"values in the range [0000] through [1582] shall only be used by mutual agreement of the partners in information interchange\".\nTo represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [\u00b1YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or - sign instead of the more common AD/BC (or CE/BCE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled -0001, and so on.\nCalendar dates.\nCalendar date representations are in the form shown in the adjacent box. [YYYY] indicates a four-digit year, 0000 through 9999. [MM] indicates a two-digit month of the year, 01 through 12. [DD] indicates a two-digit day of that month, 01 through 31. For example, \"5 April 1981\" may be represented as either \"1981-04-05\" in the \"extended format\" or \"19810405\" in the \"basic format\".\nThe standard also allows for calendar dates to be written with reduced precision. For example, one may write \"1981-04\" to mean \"1981 April\". One may simply write \"1981\" to refer to that year, \"198\" to refer to the decade from 1980 to 1989 inclusive, or \"19\" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the \"YYYY-MM-DD\" and YYYYMMDD formats for complete calendar date representations, if the day [DD] is omitted then only the YYYY-MM format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used). The 2000 version also allowed writing the truncation \"--04-05\" to mean \"April 5\" but the 2004 version does not allow omitting the year when a month is present.\nExamples:\nWeek dates.\nWeek date representations are in the formats as shown in the adjacent box. [YYYY] indicates the \"ISO week-numbering year\" which is slightly different from the traditional Gregorian calendar year (see below). [Www] is the \"week number\" prefixed by the letter \"W\", from W01 through W53. [D] is the \"weekday number\", from 1 through 7, beginning with Monday and ending with Sunday.\nThere are several mutually equivalent and compatible descriptions of week 01:\nAs a consequence, if 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.\nThe week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.\nThe \"ISO week-numbering year\" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The first ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is ending; if three, they are Monday, Tuesday and Wednesday. Similarly, the last ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is starting; if three, they are Friday, Saturday, and Sunday. The Thursday of each ISO week is always in the Gregorian calendar year denoted by the ISO week-numbering year.\nExamples:\nOrdinal dates.\nAn \"ordinal date\" is an ordinal format for the multiples of a day elapsed since the start of year.\nIt is represented as \"YYYY-DDD\" (or YYYYDDD), where [YYYY] indicates a year and [DDD] is the \"day of year\", from 001 through 365 (366 in leap years). For example, \"1981-04-05\" is the same as \"1981-095\".\nThis simple form is preferable for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. \nThis format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes referred to as \"Julian Date\", but this can cause confusion with the astronomical Julian day, a sequential count of the number of days since day 0 beginning 1 January 4713 BC Greenwich noon, Julian proleptic calendar (or noon on ISO date -4713-11-24 which uses the Gregorian proleptic calendar with a year 0000).\nTimes.\nISO\u00a08601 uses the 24-hour clock system. As of ISO\u00a08601-1:2019, the \"basic format\" is T[hh][mm][ss] and the \"extended format\" is T[hh]:[mm]:[ss]. Earlier versions omitted the T (representing time) in both formats.\nSo a time might appear as either \"T134730\" in the \"basic format\" or \"T13:47:30\" in the \"extended format\". ISO\u00a08601-1:2019 allows the T to be omitted in the extended format, as in \"13:47:30\", but only allows the T to be omitted in the basic format when there is no risk of confusion with date expressions.\nEither the seconds, or the minutes and seconds, may be omitted from the basic or extended time formats for greater brevity but decreased precision; the resulting reduced precision time formats are:\nAs of ISO\u00a08601-1:2019/Amd 1:2022, \"00:00:00\" may be used to refer to \"midnight\" corresponding to the instant at the beginning of a calendar day; and \"24:00:00\" to refer to midnight corresponding to the instant at the end of a calendar day. ISO\u00a08601-1:2019 as originally published removed \"24:00:00\" as a representation for the end of day although it had been permitted in earlier versions of the standard.\nA decimal fraction may be added to the lowest order time element present in any of these representations. A decimal mark, either a comma or a dot on the baseline, is used as a separator between the time element and its fraction. (Following ISO 80000-1 according to ISO\u00a08601:1-2019, it does not stipulate a preference except within International Standards, but with a preference for a comma according to ISO\u00a08601:2004.)\nFor example, to denote \"14 hours, 30 and one half minutes\", do not include a seconds figure; represent it as \"14:30,5\", \"T1430,5\", \"14:30.5\", or \"T1430.5\".\nThere is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties. For example, in Microsoft SQL Server, the precision of a decimal fraction is 3 for a DATETIME, i.e., \"yyyy-mm-ddThh:mm:ss[.mmm]\".\nTime zone designators.\nTime zones in ISO\u00a08601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.\nLocal time (unqualified).\nIf no UTC relation information is given with a time representation, the time is assumed to be in local time. While it \"may\" be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. Even within a single geographic time zone, some local times will be ambiguous if the region observes daylight saving time. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.\nCoordinated Universal Time (UTC).\nIf the time is in UTC, add a \"Z\" directly after the time without a space. \"Z\" is the zone designator for the zero UTC offset. \"09:30 UTC\" is therefore represented as \"09:30Z\" or \"T0930Z\". \"14:45:15 UTC\" would be \"14:45:15Z\" or \"T144515Z\".\nThe \"Z\" suffix in the ISO\u00a08601 time representation is sometimes referred to as \"Zulu time\" or \"Zulu meridian\" because the same letter is used to designate the Zulu time zone. However the ACP 121 standard that defines the list of military time zones makes no mention of UTC and derives the \"Zulu time\" from the Greenwich Mean Time which was formerly used as the international civil time standard. GMT is no longer precisely defined by the scientific community and can refer to either UTC or UT1 depending on context.\nTime offsets from UTC.\nThe UTC offset is appended directly to the time instead of \"Z\" suffix above; other nautical time zone letters are not used. The offset is applied to UTC to get the civil time in the designated time zone in the format '\u00b1[hh]:[mm]', '\u00b1[hh][mm]', or '\u00b1[hh]'.\nA negative UTC offset describes a time zone west of the prime meridian where the civil time is behind UTC. So the zone designation for New York (on standard time) would be \"\u221205:00\",\"\u22120500\", or \"\u221205\".\nConversely, a positive UTC offset describes a time zone east of the prime meridian where the civil time is ahead of UTC. So the zone designation for Cairo will be \"+02:00\",\"+0200\", or \"+02\".\nA time zone where the civil time coincides with UTC is always designated as positive, though the offset is zero (see related specifications below). So the zone designation for London (on standard time) would be \", \", or \".\nAdditional examples.\nSee List of UTC offsets for other UTC offsets.\nOther time offset specifications.\nIt is not permitted to state a zero value time offset with a negative sign, as \"\u221200:00\", \"\u22120000\", or \"\u221200\". The section dictating sign usage states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. A plus-minus-sign (&amp;pm;) may also be used if it is available. \nContrary to this rule, RFC 3339, which is otherwise a profile of ISO\u00a08601, permits the use of \"\u221200\" with the same denotation as \"+00\" but a differing connotation: an unknown UTC offset.\nTo represent a negative offset, ISO\u00a08601 specifies using a minus sign (\u2212). If the interchange character set is limited and does not have a minus sign character, then the hyphen-minus should be used (-). ASCII does not have a minus sign, so its hyphen-minus character (code 4510) would be used. If the character set has a minus sign, such as in Unicode, then that character should be used. The HTML character entity invocation for \u2212 is codice_1.\nISO 8601-2:2019 allows for general durations for time offsets. For example, more precision can be added to the time offset with the format '&lt;time&gt;\u00b1[hh]:[mm]:[ss].[sss]' or '&lt;time&gt;\u00b1[n]H[n]M[n]S' as below.\nCombined date and time representations.\nA single point in time can be represented by concatenating a complete date expression, the letter \"T\" as a delimiter, and a valid time expression. For example, \"2007-04-05T14:30\". In ISO\u00a08601:2004 it was permitted to omit the \"T\" character by mutual agreement as in \"200704051430\", but this provision was removed in ISO\u00a08601-1:2019.\nSeparating date and time parts with other characters such as space is not allowed in ISO\u00a08601, but allowed in its profile RFC 3339.\nIf a time zone designator is required, it follows the combined date and time. For example, \"2007-04-05T14:30Z\" or \"2007-04-05T12:30-02:00\".\nEither basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time may be represented using a specified reduced precision format.\nDurations.\nDurations define the amount of intervening time in a time interval and are represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W as shown on the aside. In these representations, the [n] is replaced by the value for each of the date and time elements that follow the [n]. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters \"P\", \"Y\", \"M\", \"W\", \"D\", \"T\", \"H\", \"M\", and \"S\" are designators for each of the date and time elements and are not replaced.\nFor example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds\".\nDate and time elements including their designator may be omitted if their value is zero, and lower-order elements may also be omitted for reduced precision. For example, \"P23DT23H\" and \"P4Y\" are both acceptable duration representations. However, at least one element must be present, thus \"P\" is not a valid representation for a duration of 0 seconds. \"PT0S\" or \"P0D\", however, are both valid and represent the same duration.\nTo resolve ambiguity, \"P1M\" is a one-month duration and \"PT1M\" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in \"P0.5Y\" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in \"P0,5Y\" or \"P0.5Y\". The standard does not prohibit date and time values in a duration representation from exceeding their \"carry over points\" except as noted below. Thus, \"PT36H\" could be used as well as \"P1DT12H\" for representing the same duration. But keep in mind that \"PT36H\" is not the same as \"P1DT12H\" when switching from or to Daylight saving time.\nAlternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]. For example, the first duration shown above would be \"P0003-06-04T12:30:05\". However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).\nThe standard describes a duration as part of time intervals, which are discussed in the next section. The duration format on its own is ambiguous regarding the total number of days in a calendar year and calendar month. The number of seconds in a calendar day is also ambiguous because of leap seconds. For example \"P1M\" on its own could be 28, 29, 30, or 31 days. There is no ambiguity when used in a time interval. Using example \"P2M\" duration of two calendar months:\nThe duration format (or a subset thereof) is widely used independent of time intervals, as with the Java 8 Duration class which supports a subset of the duration format.\nTime intervals.\nA time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.\nThere are four ways to express a time interval:\nOf these, the first three require two values separated by an \"interval designator\" which is usually a solidus (more commonly referred to as a forward slash \"/\"). Section 3.2.6 of ISO\u00a08601-1:2019 notes that \"A solidus may be replaced by a double hyphen [\"--\"] by mutual agreement of the communicating partners\", and previous versions used notations like \"2000--2002\". Use of a double hyphen instead of a solidus allows inclusion in computer filenames; in common operating systems, a solidus is a reserved character and is not allowed in a filename.\nFor &lt;start&gt;/&lt;end&gt; expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be shown as \"2007-12-14T13:30/15:30\", where \"/15:30\" implies \"/2007-12-14T15:30\" (the same date as the start), or the beginning and end dates of a monthly billing period as \"2008-02-15/03-14\", where \"/03-14\" implies \"/2008-03-14\" (the same year as the start).\nIf greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted \"2007-11-13/15\" can start at any time on 2007-11-13 and end at any time on 2007-11-15, whereas \"2007-11-13T09:00/15T17:00\" includes the start and end times.\nTo explicitly include all of the start and end dates, the interval would be represented as \"2007-11-13T00:00/16T00:00\".\nRepeating intervals.\nRepeating intervals are specified in clause \"4.5 Recurring time interval\". They are formed by adding \"R[n]/\" to the beginning of an interval expression, where \"R\" is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] or specifying a value of -1, means an unbounded number of repetitions. A value of 0 for [n] means the interval is not repeated.\nIf the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of \"P1Y2M10DT2H30M\" five times starting at \"2008-03-01T13:00:00Z\", use \"R5/2008-03-01T13:00:00Z/P1Y2M10DT2H30M\".\nTruncated representations (deprecated).\nISO\u00a08601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used as well as the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO\u00a08601:2004.\nThe first and seventh examples given above omit the leading codice_2 for century. Other formats have one leading codice_2 per omitted century, year, month, week, hour and minute as necessary to disambiguate the format.\nStandardised extensions.\nISO\u00a08601-2:2019 defines a set of standardised extensions to the ISO\u00a08601 date and time formats.\nExtended Date/Time Format (EDTF).\nThe EDTF is given as an example of a profile of ISO\u00a08601 in Part 2. It comes in three levels, where level 0 is a subset of Part 1 restricted to extended month-day notation, CCYY-MM-DD, similar to RFC 3339, and levels 1 and 2 build upon that. \nEDTF is mostly tailored to bibliographic and historiographic needs. Some of its features are:\nThe EDTF features are described in the \"Date and Time Extensions\" section of ISO\u00a08601-2:2019, and the profile itself is found as Annex A. It is virtually identical to the prior specification pioneered by the US Library of Congress.\nRepeat rules for recurring time intervals.\nISO\u00a08601-2:2019 also defines a format to constrain repeating intervals based on syntax from iCalendar, i.e. IETF RFC 5545.\nImplementations.\nGNU CoreUtils's date command has an codice_4 option among other formating patterns:\n$ date --iso-8601=ns\n2025-02-16T12:03:17,646296349+01:00\n$ date -u --rfc-3339=ns\n2025-02-16 10:58:44.966864492+00:00\n$ date -u +\"%Y-%m-%dT%H:%M:%S.%6NZ\"\n2025-02-16T10:58:44.965071Z\nUsage.\nOn the Internet, the World Wide Web Consortium (W3C) uses the IETF standard based on ISO\u00a08601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software. The very simple specification is based on a draft of the RFC 3339 mentioned below.\nISO\u00a08601 is referenced by several specifications, but the full range of options of ISO\u00a08601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO\u00a08601.\nThe X.690 encoding standard's GeneralizedTime makes use of another subset of ISO\u00a08601.\nCommerce.\nAs of 2006, the ISO week date appears in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced.\nRFCs.\nIETF RFC 3339 defines a profile of ISO\u00a08601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.\nRFC 3339 deviates from ISO\u00a08601 in allowing a zero time zone offset to be specified as \"-00:00\", which ISO\u00a08601 forbids. RFC 3339 intends \"-00:00\" to carry the connotation that it is not stating a preferred time zone, whereas the conforming \"+00:00\" or any non-zero offset connotes that the offset being used is preferred. This convention regarding \"-00:00\" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO\u00a08601, and so was free to use this convention without conflict.\nBuilding upon the foundations of RFC 3339, the IETF introduced the Internet Extended Date/Time Format (IXDTF) in RFC 9557. This format extends the timestamp representation to include additional information such as an associated time zone name. The inclusion of time zone names is particularly useful for applications that need to account for events like daylight saving time transitions. Furthermore, IXDTF maintains compatibility with pre-existing syntax for attaching time zone names to timestamps, providing a standardized and flexible approach to timestamp representation on the internet. Example:\ncodice_5\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15026", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=15026", "title": "Inertial mass", "text": ""}
{"id": "15027", "revid": "20967954", "url": "https://en.wikipedia.org/wiki?curid=15027", "title": "Isa", "text": "Isa or ISA may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15028", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=15028", "title": "International Seabed Authority", "text": "Intergovernmental body to regulate mineral-related activities on the seabed\nThe International Seabed Authority (ISA; ) is a Kingston, Jamaica-based intergovernmental body of 167 member states and the European Union. It was established under the 1982 UN Convention on the Law of the Sea (UNCLOS) and its 1994 Agreement on Implementation. The ISA's dual mission is to authorize and control the development of mineral related operations in the international seabed, which is considered the \"common heritage of all mankind\", and to protect the ecosystem of the seabed, ocean floor and subsoil in \"The Area\" beyond national jurisdiction. The ISA is responsible for safeguarding the international deep sea, defined as waters below 200 meters (656 feet), where photosynthesis is hampered by inadequate light. Governing approximately half of the total area of the world's oceans, the ISA oversees activities that might threaten biological diversity and harm the marine environment. \nSince its inception in 1994, the ISA has approved over two dozen ocean floor mining exploration contracts in the Atlantic, Pacific and Indian Oceans. The majority of these contracts are for exploration in the Clarion\u2013Clipperton zone between Hawaii and Mexico, where polymetallic nodules contain copper, cobalt and other minerals essential for powering electric batteries. To date, the Authority has not authorized any commercial mining contracts as it continues to deliberate over regulations amid global calls for a moratorium on deep sea mining. Scientists and environmentalists warn that such mining could wreak havoc on the ocean, a crucial carbon sink and home to rare and diverse species.\nFunded by UNCLOS members and mining contractors, the Authority operates as an autonomous international organization with its own Assembly, Council, and Secretariat. The current secretary-general of the agency is Leticia Carvalho, whose four-year term began on 1 January 2025.\nOrigin.\nThe Authority held its inaugural meeting in its host country, Jamaica, on 16 November 1994, the day the Convention came into force. The articles governing the Authority have been made \"noting the political and economic changes, including market-oriented approaches, affecting the implementation\" of the convention. The Authority obtained its observer status to the United Nations in October 1996. The Authority has 167 members and the European Union, composed of all parties to the United Nations Convention on the Law of the Sea. The Authority operates by contracting with private and public corporations and other entities authorizing them to explore, and potentially exploit, specified areas on the deep seabed for mineral resources, such as cobalt, nickel and manganese.\n\"Common Heritage of All Mankind\".\nUnder UNCLOS, Part XI, Section 2. \"The Area and its resources are the common heritage of mankind.\" As a result, ISA must ensure that activities in the Area are undertaken only for peaceful purposes and for the benefit of all humankind, with economic benefits shared equitably and special consideration given to the needs of developing nations.\nGovernance and operations.\nAlong with a Secretary-General, two principal organs establish the policies and govern the work of the Authority: the Assembly, in which all UNCLOS parties are represented, and a 36-member Council elected by the Assembly.\nSecretary-General.\nThe Assembly elects a Secretary-General to serve a four-year term as the ISA's chief administrative officer, oversee Authority staff and issue an annual report to the Assembly. The Secretary-General is prohibited from holding a financial interest in any mining operations authorized by the Authority.\nThere have been four Secretary-Generals since ISA's creation in 1996:\nAssembly.\nThe Assembly, which consists of all members of the Authority, elects the 36-member Council, as well as the Secretary-General from among candidates the Council recommends. The Assembly also has the power to approve or reject the council's recommendations for the following: rules and regulations governing seabed mining, distribution of financial benefits accrued from authorized mining and the Authority's annual budget.\nCouncil.\nThe 36-member Council, elected by the Assembly, authorizes contracts with governments and private corporations to explore and mine the international seabed and sets rules and procedures, subject to the Assembly's approval, for ISA governance. The council also nominates a Secretary-General, who then must be elected by the full Assembly to serve a four-year term. The ISA's annual plenary sessions, which usually last two weeks, are held in Kingston.\nAdvisory bodies.\nAlso established is a 30-member Legal and Technical Commission which advises the Council and a 15-member Finance Committee that deals with budgetary and related matters. All members are experts nominated by governments and elected to serve in their individual capacity.\nEnterprise.\nThe convention also established a body called the Enterprise which is to serve as the Authority's own mining operator, potentially generating \"hundreds of millions of dollars in royalties\" to be shared with developing nations.\" The environmental organization Greenpeace has expressed concerns over the ISA's alleged conflict of interest as both regulator and business operator, though the ISA denies the conflict of interest charge.\nStatus.\nThe Authority has a Secretariat of 37 authorized posts and a 2022 biennial budget of approximately $10,000,000.\nJurisdiction.\nUNCLOS defines the international seabed area\u2014the part under ISA jurisdiction\u2014as \"the seabed and ocean floor and the subsoil thereof, beyond the limits of national jurisdiction\" UNCLOS outlines the areas of national jurisdiction as a \"12 nautical-mile territorial sea; an exclusive economic zone of up to 200 nautical miles and a continental shelf\", unless a nation can demonstrate that its continental shelf is naturally prolonged beyond that limit, in which case it may claim up to . ISA has no role in determining this boundary. Rather, this task is left to another body established by UNCLOS, the Commission on the Limits of the Continental Shelf, which examines scientific data submitted by coastal states that claim a broader reach. \nExploration contracts and commercial mining.\nCommercial.\nAlthough the ISA has yet to approve commercial mining contracts, the Authority anticipates commercial mining could begin as early as 2023\u20132024 with the completion of much-debated ISA regulations. In 2021, the Pacific Island nation of Nauru triggered a deadline that requires the ISA to approve final commercial mining regulations by July 2023 or allow contractors to mine under existing draft regulations.\nExploratory.\nExploratory mining involves \"deep-sea mapping, manned submersibles or remotely-operated vehicles, photographic and video systems, and drilling devices.\"\nClarion\u2013Clipperton zone.\nMost areas of exploration are in the Clarion\u2013Clipperton zone (CCZ), in the Equatorial North Pacific Ocean, south and southeast of Hawaii, between Hawaii and Mexico. The quiet CCZ, as wide as the continental U.S., is home to polymetallic nodules or trillions of potato-size lumps of matter formed over millions of years that contain nickel, manganese, copper, zinc and cobalt, as well as deep water coral, sponges and unusual species (\"ghost octopus\", crustaceans, worms and sea cucumbers) that in a near light-less environment attach to the rock-like nodules for shelter. Contractors want to mine polymetallic nodules for battery storage for electric vehicles, smartphones, and solar and wind energy.\nOther areas of exploration.\nExploration contracts for polymetallic nodules have also been issued for contractors operating in the Central Indian Ocean Basin and Western Pacific Ocean. The ISA has issued exploration contracts for polymetallic sulphides in the South West Indian Ridge, Central Indian Ridge and the Mid-Atlantic Ridge, and contracts for exploration for cobalt-rich crusts in the Western Pacific Ocean.\nRequirements of contractors.\nEach contractor is required to develop a contingency plan should something go wrong during exploration, report annually on its activities in its assigned area and propose a training program for developing countries .\nList of exploratory contractors.\nThe ISA has signed 15-year contracts for exploration with 22 contractors seeking polymetallic nodules, polymetallic sulphides and cobalt-rich ferromanganese crusts in the deep seabed.\nIn 2001-2002 the ISA signed contracts with Yuzhmorgeologya (Russian Federation); Interoceanmetal Joint Organization (IOM) (Bulgaria, Cuba, Slovakia, Czech Republic, Poland and Russian Federation); the Government of the Republic of Korea; China Ocean Minerals Research and Development Association (COMRA) (China); Deep Ocean Resources Development Company (DORD) (Japan); Institut fran\u00e7ais de recherche pour l\u2019exploitation de la mer (IFREMER) (France); the Government of India. In 2006, the Authority signed a 15-year contract with the Federal Institute for Geosciences and Natural Resources of Germany.\nIn 2008, the Authority received two new applications for authorization to explore for polymetallic nodules, coming for the first time from private firms in developing island nations of the Pacific. Sponsored by their respective governments, they were submitted by Nauru Ocean Resources Inc. and Tonga Offshore Mining Limited. A 15-year exploration contract was granted by the Authority to Nauru Ocean Resources Inc. on 22 July 2011 and to Tonga Offshore Mining Limited on 12 January 2012.\nFifteen-year exploration contracts for polymetallic nodules were also granted to G-TECH Sea Mineral Resources NV (Belgium) on 14 January 2013; Marawa Research and Exploration Ltd (Kiribati) on 19 January 2015; Ocean Mineral Singapore Pte Ltd on 22 January 2015; UK Seabed Resources Ltd (two contracts on 8 February 2013 and 29 March 2016 respectively); Cook Islands Investment Corporation on 15 July 2016 and more recently China Minmetals Corporation on 12 May 2017.\nThe Authority has signed seven contracts for the exploration for polymetallic sulphides in the South West Indian Ridge, Central Indian Ridge and Mid-Atlantic Ridge with China Ocean Mineral Resources Research and Development Association (18 November 2011); the Government of Russia (29 October 2012); Government of the Republic of Korea (24 June 2014); Institut fran\u00e7ais de recherche pour l\u2019exploitation de la mer (Ifremer, France, 18 November 2014); the Federal Institute for Geosciences and Natural Resources of Germany (6 May 2015); and the Government of India (26 September 2016) and the Government of the Republic of Poland (12 February 2018).\nThe Authority holds five contracts for the exploration of cobalt-rich ferromanganese crusts in the Western Pacific Ocean with China Ocean Mineral Resources Research and Development Association (29 April 2014); Japan Oil Gas and Metals National Corporation (JOGMEC, 27 January 2014); Ministry of Natural Resources and Environment of the Russian Federation (10 March 2015), Companhia De Pesquisa de Recursos Minerais (9 November 2015) and the Government of the Republic of Korea (27 March 2018).\nControversy.\nEnvironmental concerns and climate crisis.\nEnvironmentalists, scientists from 44 countries, Google, BMW and Volvo, World Wildlife Fund and several Pacific nations, including Fiji and Papua New Guinea, have called for a moratorium on deep-sea mining until more scientific research is conducted on its impact on the marine environment.\nAdvocates for deep sea mining argue extraction of rare metals is critical for electric car batteries necessary to develop a fossil-free economy.\nOpponents argue seabed mining could wreak havoc on the world's oceans, which act as a carbon sink absorbing a quarter of the world's carbon emissions each year.\nThe environmental organization Greenpeace has raised objections about deep seabed mining disrupting the habitats of newly reported species, from crabs to whales to snails that survive without eating and congregate near bioluminescent thermal vents. Greenpeace has urged the ISA to further develop UNCLOS' foundational Article 136 principle \"of common heritage to all mankind\" to revise regulations and set conservation targets. In a 2018 Greenpeace Research Laboratories report the organization stressed the importance of protecting marine biodiversity from toxins released during seabed mining for natural gas and rare metals for photovoltaic cells. Greenpeace maintains the \"pro-exploitation\" ISA is not the appropriate authority to regulate deep sea mining (DSM). In 2019 Greenpeace activists protested outside the annual meeting of the International Seabed Authority in Jamaica, calling for a global ocean treaty to ban deep sea mining in ocean sanctuaries. Some of the activists had sailed to Jamaica aboard Greenpeace's ship, the Esperanza, which travelled from the \"Lost City in the mid-Atlantic\", an area Greenpeace says is threatened by exploratory mining the ISA authorized.\nISA Secretary-General Michael Lodge said Greenpeace's support for a global ocean treaty, not the ISA, to control deep sea mining did not make sense.\nConcern over transparency issues.\nIn 2022, The Guardian reported the ISA failed to renew the contract for Earth Negotiations Bulletin (ENB), a division of the International Institute for Sustainable Development (IISD), which covered past proceedings to maintain an independent record of the ISA. The decision came amid warnings from scientists that commercial ocean floor mining \"would be \u201cdangerous\u201d, \u201creckless\u201d and \u201cirreversible\u201d in its harm to the ecosystem. In its defense, the ISA said ENB's non-renewal was triggered by budget cuts. The Guardian also reported that Germany and environmentalists had raised questions about the lack of transparency by the ISA's Legal and Technical Commission (LTC), which conducts closed meetings to set standards and issue guidelines for seabed mining.\nIn response to criticism, ISA Secretary-General Michael Lodge defended ISA as a \"transparent public forum of consensus-building.\"\nCharges of conflict of interest.\nIn 2022, the \"Los Angeles Times\" reported that the International Seabed Authority faced criticisms over conflicts of interest. The \"LA Times\" reported that the ISA was scheduled to approve seabed mining, despite concerns by scientists and environmentalists about the environmental impact. ISA head Michael Lodge had criticized these groups, saying there was \"a growing environmental absolutism and dogmatism bordering on fanaticism\" and arguing that seabed mining was \"predictable and manageable\". Scientists and members of Lodge's staff objected to Lodge's appearance in a mining company video seeking investments in robotic exploration for minerals to manufacture electric vehicles. In the video, Lodge said his agency supported a 15-year exploration contract because \"land-based resources are becoming increasingly difficult to access.\"\nUnited States' non-ratification of UNCLOS.\nThe exact nature of the ISA's mission and authority has been questioned by opponents of the Law of the Sea Treaty who are generally skeptical of multilateral engagement by the United States. In 2007, although the US Senate Foreign Relations Committee voted in favor of treaty ratification, the full Senate failed to ratify the treaty, with some Republicans arguing UNCLOS might threaten national security by interfering with ocean military operations and hinder seabed mining corporations by imposing environmental regulations.\nOne of the main anti-ratification arguments being a charge that the ISA is flawed or unnecessary. In its original form, the Convention included certain provisions that some found objectionable, such as:\nBecause of these concerns, the United States pushed for modification of the Convention, obtaining a 1994 Agreement on Implementation that somewhat mitigates them and thus modifies the ISA's authority. Despite this change the United States has not ratified the Convention and so is not a member of ISA, although it sends sizable delegations to participate in meetings as an observer.\nAs an observer, not an UNCLOS signatory, the U.S. will not be allowed to vote on approval of final commercial mining regulations and will be unable to sponsor companies to apply for contracts in international waters. This is because the ISA requires contractors be sponsored by a state that is a signatory to UNCLOS. U.S.-based military contractor Lockheed Martin, however, is participating in two British deep sea mining projects.\nPalau's advocacy against deep-sea mining.\nPalau was the first country to call for a moratorium, or precautionary pause, on deep-sea mining until the impact of such a practice is better understood. By July 10, 2023, 17 countries had called for a deep-sea mining moratorium or pause, including Germany, New Zealand, Spain, France, Sweden, Fiji, and the Federated States of Micronesia.\nOn July 29, 2024, President Surangel S. Whipps Jr. of Palau delivered an address titled \"Upholding the Common Heritage of Humankind\" to the 29th General Assembly of the International Seabed Authority (ISA) in Kingston, Jamaica. In his speech, President Whipps emphasized the importance of safeguarding the deep ocean from exploitation and modern-day colonialism. He highlighted Palau\u2019s deep cultural and economic ties to the ocean and reiterated the call for an immediate moratorium on deep-sea mining, citing the associated environmental risks and uncertainties. In his speech he referred to the ocean as \"Our greatest ally in our fight against climate change,\" highlighting its role as the largest carbon sink on the planet. He underscored the critical role deep ocean ecosystems play in global environmental health and advocated for prioritizing long-term sustainability over short-term economic gains. He urged the assembly to act responsibly on behalf of future generations, reinforcing the deep seabed\u2019s status as the \"common heritage of (hu)mankind.\"\nThe number of countries against the imminent start of mining for metallic nodules on the seafloor increased to 32 during the 29th ISA annual assembly, with Austria, Guatemala, Honduras, Malta, and Tuvalu joining the list.\nActivities.\nLegislative.\nThe Authority's main legislative accomplishment has been the adoption, in the year 2000, of regulations governing exploration for polymetallic nodules. These resources, also called manganese nodules, contain varying amounts of manganese, cobalt, copper and nickel. They occur as potato-sized lumps scattered about on the surface of the ocean floor, mainly in the central Pacific Ocean in the Clarion\u2013Clipperton zone but with some deposits in the Indian Ocean.\nIn 2013, the ISA approved amendments to its mining code on deep sea exploration, stating a prospector should take a precautionary approach to avoid polluting the ocean and should immediately inform the Secretary-General of any prospect-related incidents that threaten the marine environment. The amended regulations also said a contractor can recover \"a reasonable amount of material\" for testing but not for sale.\nIn July 2019, the ISA's Legal and Trade Commission prepared \"Draft regulations on exploitation of mineral resources in the Area.\"\nIn 2010, the ISA adopted Regulations on Prospecting and Exploration for Polymetallic Sulphides.\nIn 2012, the Authority adopted Regulations on Prospecting and Exploration for Cobalt-Rich Ferromanganese Crusts.\nThe Council of the Authority began work in August 2002 on another set of regulations, covering polymetallic sulfides and cobalt-rich ferromanganese crusts, which are rich sources of such minerals as copper, iron, zinc, silver and gold, as well as cobalt. The sulphides are found around volcanic hot springs, especially in the western Pacific Ocean, while the crusts occur on oceanic ridges and elsewhere at several locations around the world. The Council decided in 2006 to prepare separate sets of regulations for sulphides and for crusts, with priority given to sulphides. It devoted most of its sessions in 2007 and 2008 to this task, but several issues remained unresolved. Chief among these were the definition and configuration of the area to be allocated to contractors for exploration, the fees to be paid to the Authority and the question of how to deal with any overlapping claims that might arise. Meanwhile, the Legal and Technical Commission reported progress on ferromanganese crusts.\nWorkshops and research.\nIn addition to its legislative work, the Authority organizes annual workshops on various aspects of seabed exploration, with emphasis on measures to protect the marine environment from any harmful consequences. It disseminates the results of these meetings through publications. Studies over several years covering the key mineral area of the Central Pacific resulted in a technical study on biodiversity, species ranges and gene flow in the abyssal Pacific nodule province, with emphasis on predicting and managing the impacts of deep seabed mining A workshop at Manoa, Hawaii, in October 2007 produced a rationale and recommendations for the establishment of \"preservation reference areas\" in the Clarion\u2013Clipperton zone, where nodule mining would be prohibited in order to leave the natural environment intact. In recent years, the ISA hosted workshops on enhancing the role of women in conducting deep-sea scientific studies, sustainable management of deep seabed resources, a series for Africa on resources and technologies for DSM and a session on sharing the economic benefits of DSM.\nNational trends in seabed mining.\nIn recent years, however, interest in deep sea mining, especially with regard to ferromanganese crusts and polymetallic sulphides, has picked up among several firms now operating in waters within the national zones of Papua New Guinea, Fiji and Tonga. Papua New Guinea was the first country in the world to grant commercial exploration licenses for seafloor massive sulphide deposits when it granted the initial license to Nautilus Minerals in 1997. Japan's new ocean policy emphasizes the need to develop methane hydrate and hydrothermal deposits within Japan's exclusive economic zone and calls for the commercialization of these resources within the next 10 years. Reporting on these developments in his annual report to the Authority in April 2008, Secretary-General Nandan referred also to the upward trend in demand and prices for cobalt, copper, nickel and manganese, the main metals that would be derived from seabed mining, and he noted that technologies being developed for offshore extraction could be adapted for deep sea mining.\nRecently, there has been much interest in the possibility of exploiting seabed resources in the Arctic Ocean, bordered by Canada, Denmark, Iceland, Norway, Russia and the United States (see Territorial claims in the Arctic). In 2020, an international coalition of environmental groups urged the government of Norway to not only abandon plans for deep sea mining under national jurisdiction, but to also speak out against DSM Arctic mining before the International Seaboard Authority.\nEndowment fund.\nIn 2006 the Authority established an Endowment Fund to Support Collaborative Marine Scientific Research on the International Seabed Area. The Fund will aid experienced scientists and technicians from developing countries to participate in deep-sea research organized by international and national institutions. A campaign was launched in February 2008 to identify participants, establish a network of cooperating bodies and seek outside funds to augment the initial $3 million endowment from the Authority.\nVoluntary commitments.\nIn 2017, the Authority registered seven voluntary commitments with the UN Oceans Conference for Sustainable Development Goal 14. These were:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15029", "revid": "1126265", "url": "https://en.wikipedia.org/wiki?curid=15029", "title": "Industry Standard Architecture", "text": "Internal expansion bus in early PC compatibles\nIndustry Standard Architecture (ISA) is the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles.\nOriginally referred to as the PC bus (8-bit) or AT bus (16-bit), it was also termed \"I/O Channel\" by IBM. The ISA term was coined as a retronym by IBM PC clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT bus with its new and incompatible Micro Channel architecture.\nThe 16-bit ISA bus was also used with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as VESA Local Bus and PCI were used instead, often along with ISA slots on the same mainboard. Derivatives of the AT bus structure were and still are used in ATA/IDE, the PCMCIA standard, CompactFlash, the PC/104 bus, and internally within Super I/O chips.\nEven though ISA disappeared from consumer desktops many years ago, it is still used in industrial PCs, where certain specialized expansion cards that never transitioned to PCI and PCI Express are used.\nHistory.\nThe original PC bus was developed by a team led by Mark Dean at IBM as part of the IBM PC project in 1981. It was an 8-bit bus based on the I/O bus of the IBM System/23 Datamaster system - it used the same physical connector, and a similar signal protocol and pinout. A 16-bit version, the IBM AT bus, was introduced with the release of the IBM PC/AT in 1984. The AT bus was a mostly backward-compatible extension of the PC bus\u2014the AT bus connector was a superset of the PC bus connector. In 1988, the 32-bit EISA standard was proposed by the \"Gang of Nine\" group of PC-compatible manufacturers that included Compaq. Compaq created the term \"Industry Standard Architecture\" (ISA) to replace \"PC compatible\". In the process, they retroactively renamed the AT bus to ISA to avoid infringing IBM's trademark on its PC and PC/AT systems (and to avoid giving their major competitor, IBM, free advertisement). \nIBM designed the 8-bit version as a buffered interface to the motherboard buses of the Intel 8088 (16/8 bit) CPU in the IBM PC and PC/XT, augmented with prioritized interrupts and DMA channels. The 16-bit version was an upgrade for the motherboard buses of the Intel 80286 CPU (and expanded interrupt and DMA facilities) used in the IBM AT, with improved support for bus mastering. The ISA bus was therefore synchronous with the CPU clock until sophisticated buffering methods were implemented by chipsets to interface ISA to much faster CPUs.\nISA was designed to connect peripheral cards to the motherboard and allows for bus mastering. Only the first 16 MB of main memory is addressable. The original 8-bit bus ran from the 4.77\u00a0MHz clock of the 8088 CPU in the IBM PC and PC/XT. The original 16-bit bus ran from the CPU clock of the 80286 in IBM PC/AT computers, which was 6\u00a0MHz in the first models and 8\u00a0MHz in later models. The IBM RT PC also used the 16-bit bus. ISA was also used in some non-IBM compatible machines such as Motorola 68k-based Apollo (68020) and Amiga 3000 (68030) workstations, the short-lived AT&amp;T Hobbit and the later PowerPC-based BeBox.\nCompanies like Dell improved the AT bus's performance but in 1987, IBM replaced the AT bus with its proprietary Micro Channel Architecture (MCA). MCA overcame many of the limitations then apparent in ISA but was also an effort by IBM to regain control of the PC architecture and the PC market. MCA was far more advanced than ISA and had many features that would later appear in PCI. However, MCA was also a closed standard whereas IBM had released full specifications and circuit schematics for ISA. Computer manufacturers responded to MCA by developing the Extended Industry Standard Architecture (EISA) and the later VESA Local Bus (VLB). VLB used some electronic parts originally intended for MCA because component manufacturers were already equipped to manufacture them. Both EISA and VLB were backward-compatible expansions of the AT (ISA) bus.\nUsers of ISA-based machines had to know special information about the hardware they were adding to the system. While a handful of devices were essentially plug-n-play, this was rare. Users frequently had to configure parameters when adding a new device, such as the IRQ line, I/O address, or DMA channel. MCA had done away with this complication and PCI actually incorporated many of the ideas first explored with MCA, though it was more directly descended from EISA.\nThis trouble with configuration eventually led to the creation of ISA PnP, a plug-n-play system that used a combination of modifications to hardware, the system BIOS, and operating system software to automatically manage resource allocations. In reality, ISA PnP could be troublesome and did not become well-supported until the architecture was in its final days.\nA PnP ISA, EISA or VLB device may have a 5-byte \"EISA ID\" (3-byte manufacturer ID\u00a0+ 2-byte hex number) to identify the device. For example, CTL0044 corresponds to Creative Sound Blaster 16/32 PnP.\nPCI slots were the first physically incompatible expansion ports to directly squeeze ISA off the motherboard. At first, motherboards were largely ISA, including a few PCI slots. By the mid-1990s, the two slot types were roughly balanced, and ISA slots soon were in the minority of consumer systems. Microsoft's PC-99 specification recommended that ISA slots be removed entirely, though the system architecture still required ISA to be present in some vestigial way internally to handle the floppy drive, serial ports, etc., which was why the software compatible LPC bus was created. ISA slots remained for a few more years and towards the turn of the century it was common to see systems with an Accelerated Graphics Port (AGP) sitting near the central processing unit, an array of PCI slots, and one or two ISA slots near the end. In late 2008, even floppy disk drives and serial ports were disappearing, and the extinction of vestigial ISA (by then the LPC bus) from chipsets was on the horizon.\nPCI slots are rotated compared to their ISA counterparts\u2014PCI cards were essentially inserted upside-down, allowing ISA and PCI connectors to squeeze together on the motherboard. Only one of the two connectors can be used in each slot at a time, but this allowed for greater flexibility.\nThe AT Attachment (ATA) hard disk interface is directly descended from the 16-bit ISA of the PC/AT. ATA has its origins in the IBM Personal Computer Fixed Disk and Diskette Adapter, the standard dual-function floppy disk controller and hard disk controller card for the IBM PC AT; the fixed disk controller on this card implemented the register set and the basic command set which became the basis of the ATA interface (and which differed greatly from the interface of IBM's fixed disk controller card for the PC XT). Direct precursors to ATA were third-party ISA hardcards that integrated a hard disk drive (HDD) and a hard disk controller (HDC) onto one card. This was at best awkward and at worst damaging to the motherboard, as ISA slots were not designed to support such heavy devices as HDDs. The next generation of Integrated Drive Electronics drives moved both the drive and controller to a drive bay and used a ribbon cable and a very simple interface board to connect it to an ISA slot. ATA is basically a standardization of this arrangement plus a uniform command structure for software to interface with the HDC within the drive. ATA has since been separated from the ISA bus and connected directly to the local bus, usually by integration into the chipset, for much higher clock rates and data throughput than ISA could support. ATA has clear characteristics of 16-bit ISA, such as a 16-bit transfer size, signal timing in the PIO modes and the interrupt and DMA mechanisms.\nISA bus architecture.\nThe PC/XT-bus is an eight-bit ISA bus used by Intel 8086 and Intel 8088 systems in the IBM PC and IBM PC XT in the 1980s. Among its 62 pins were demultiplexed and electrically buffered versions of the 8 data and 20 address lines of the 8088 processor, along with power lines, clocks, read/write strobes, interrupt lines, etc. Power lines included \u22125\u00a0V and \u00b112\u00a0V in order to directly support pMOS and enhancement mode nMOS circuits such as dynamic RAMs among other things. The XT bus architecture uses a single Intel 8259 PIC, giving eight vectorized and prioritized interrupt lines. It has four DMA channels originally provided by the Intel 8237. Three of the DMA channels are brought out to the XT bus expansion slots; of these, 2 are normally already allocated to machine functions (diskette drive and hard disk controller):\nThe PC/AT-bus, a 16-bit (or 80286-) version of the PC/XT bus, was introduced with the IBM PC/AT. This bus was officially termed \"I/O Channel\" by IBM. It extends the XT-bus by adding a second shorter edge connector in-line with the eight-bit XT-bus connector, which is unchanged, retaining compatibility with most 8-bit cards. The second connector adds four additional address lines for a total of 24, and 8 additional data lines for a total of 16. It also adds new interrupt lines connected to a second 8259 PIC (connected to one of the lines of the first) and 4 \u00d7 16-bit DMA channels, as well as control lines to select 8- or 16-bit transfers.\nThe 16-bit AT bus slot originally used two standard edge connector sockets in early IBM PC/AT machines. However, with the popularity of the AT architecture and the 16-bit ISA bus, manufacturers introduced specialized 98-pin connectors that integrated the two sockets into one unit. These can be found in almost every AT-class PC manufactured after the mid-1980s. The ISA slot connector is typically black (distinguishing it from the brown EISA connectors and white PCI connectors).\nNumber of devices.\nMotherboard devices have dedicated IRQs (not present in the slots). 16-bit devices can use either PC-bus or PC/AT-bus IRQs. It is therefore possible to connect up to 6 devices that use one 8-bit IRQ each and up to 5 devices that use one 16-bit IRQ each. At the same time, up to 4 devices may use one 8-bit DMA channel each, while up to 3 devices can use one 16-bit DMA channel each.\nVarying bus speeds.\nOriginally, the bus clock was synchronous with the CPU clock, resulting in varying bus clock frequencies among the many different IBM clones on the market (sometimes as high as 16 or 20\u00a0MHz), leading to software or electrical timing problems for certain ISA cards at bus speeds they were not designed for. Later motherboards or integrated chipsets used a separate clock generator, or a clock divider which either fixed the ISA bus frequency at 4, 6, or 8\u00a0MHz or allowed the user to adjust the frequency via the BIOS setup. When used at a higher bus frequency, some ISA cards (certain Hercules-compatible video cards, for instance), could show significant performance improvements.\n8/16-bit incompatibilities.\nMemory address decoding for the selection of 8 or 16-bit transfer mode was limited to 128\u00a0KB sections, leading to problems when mixing 8- and 16-bit cards as they could not co-exist in the same 128\u00a0KB area. This is because the MEMCS16 line is required to be set based on the value of LA17-23 only.\nPast and current use.\nISA is still used today for specialized industrial purposes. In 2008, IEI Technologies released a modern motherboard for Intel Core 2 Duo processors which, in addition to other special I/O features, is equipped with two ISA slots. It was marketed to industrial and military users who had invested in expensive specialized ISA bus adaptors, which were not available in PCI bus versions.\nSimilarly, ADEK Industrial Computers released a modern motherboard in early 2013 for Intel Core i3/i5/i7 processors, which contains one (non-DMA) ISA slot. Also, MSI released a modern motherboard with one ISA slot in 2020, for use with Skylake and Kaby Lake Intel processors; per its official specifications, it can be accessed within the 32-bit Windows 7 installation. DFI also released a motherboard featuring two ISA slots, for use with Coffee Lake Intel processors; in this example, due to ISA's historic nature, they can only be accessed within KVM-based virtual machines.\nThe PC/104 bus, used in industrial and embedded applications, is a derivative of the ISA bus, utilizing the same signal lines with different connectors. The LPC bus has replaced the ISA bus as the connection to the legacy I/O devices on current motherboards; while physically quite different, LPC looks just like ISA to software, so the peculiarities of ISA such as the 16 MiB DMA limit (which corresponds to the full address space of the Intel 80286 CPU used in the original IBM AT) are likely to stick around for a while.\nATA.\nAs explained in the \"History\" section, ISA was the basis for development of the ATA interface, used for ATA (a.k.a. IDE) hard disks. Physically, ATA is essentially a simple subset of ISA, with 16 data bits, support for exactly one IRQ and one DMA channel, and 3 address bits. To this ISA subset, ATA adds two IDE address select (\"chip select\") lines (i.e. address decodes, effectively equivalent to address bits) and a few unique signal lines specific to ATA/IDE hard disks (such as the Cable Select/Spindle Sync. line.) In addition to the physical interface channel, ATA goes beyond and far outside the scope of ISA by also specifying a set of physical device registers to be implemented on every ATA (IDE) drive and a full set of protocols and device commands for controlling fixed disk drives using these registers. The ATA device registers are accessed using the address bits and address select signals in the ATA physical interface channel, and all operations of ATA hard disks are performed using the ATA-specified protocols through the ATA command set. The earliest versions of the ATA standard featured a few simple protocols and a basic command set comparable to the command sets of MFM and RLL controllers (which preceded ATA controllers), but the latest ATA standards have much more complex protocols and instruction sets that include optional commands and protocols providing such advanced optional-use features as sizable hidden system storage areas, password security locking, and programmable geometry translation.\nIn the mid-1990s, the ATA host controller (usually integrated into the chipset) was moved to PCI form. A further deviation between ISA and ATA is that while the ISA bus remained locked into a single standard clock rate (for backward hardware compatibility), the ATA interface offered many different speed modes, could select among them to match the maximum speed supported by the attached drives, and kept adding faster speeds with later versions of the ATA standard (up to 133 MB/s for ATA-6, the latest.) In most forms, ATA ran much faster than ISA, provided it was connected directly to a local bus (e.g. southbridge-integrated IDE interfaces) faster than the ISA bus.\nXT-IDE.\nBefore the 16-bit ATA/IDE interface, there was an 8-bit XT-IDE (also known as XTA) interface for hard disks. It was not nearly as popular as ATA has become, and XT-IDE hardware is now fairly hard to find. Some XT-IDE adapters were available as 8-bit ISA cards, and XTA sockets were also present on the motherboards of Amstrad's later XT clones as well as a short-lived line of Philips units. The XTA pinout was very similar to ATA, but only eight data lines and two address lines were used, and the physical device registers had completely different meanings. A few hard drives (such as the Seagate ST351A/X) could support either type of interface, selected with a jumper.\nMany later AT (and AT successor) motherboards had no integrated hard drive interface but relied on a separate hard drive interface plugged into an ISA/EISA/VLB slot. There were even a few 80486-based units shipped with MFM/RLL interfaces and drives instead of the increasingly common AT-IDE.\nCommodore built the XT-IDE-based peripheral hard drive and memory expansion unit A590 for their Amiga 500 and 500+ computers that also supported a SCSI drive. Later models \u2013 the A600, A1200, and the Amiga 4000 series \u2013 use AT-IDE drives.\nPCMCIA.\nThe PCMCIA specification can be seen as a superset of ATA. The standard for PCMCIA hard disk interfaces, which included PCMCIA flash drives, allows for the mutual configuration of the port and the drive in an ATA mode. As a de facto extension, most PCMCIA flash drives additionally allow for a simple ATA mode that is enabled by pulling a single pin low, so that PCMCIA hardware and firmware are unnecessary to use them as an ATA drive connected to an ATA port. PCMCIA flash drive to ATA adapters are thus simple and inexpensive but are not guaranteed to work with any and every standard PCMCIA flash drive. Further, such adapters cannot be used as generic PCMCIA ports, as the PCMCIA interface is much more complex than ATA.\nEmulation by embedded chips.\nAlthough most modern computers do not have physical ISA buses, almost all PCs \u2013 IA-32, and x86-64 \u2013 have ISA buses allocated in physical address space. Some southbridges and some CPUs themselves provide services such as temperature monitoring and voltage readings through ISA buses as ISA devices.\nStandardization.\nIEEE started a standardization of the ISA bus in 1985, called the P996 specification. However, despite books being published on the P996 specification, it never officially progressed past draft status.\nModern ISA cards.\nThere still is an existing user base with old computers, so some ISA cards are still manufactured, e.g. with USB ports or complete single-board computers based on modern processors, USB 3.0, and SATA.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15030", "revid": "50135124", "url": "https://en.wikipedia.org/wiki?curid=15030", "title": "Intergovernmental Panel on Climate Change", "text": "Scientific intergovernmental body\nThe Intergovernmental Panel on Climate Change (IPCC) is an intergovernmental body of the United Nations. Its job is to \"provide governments at all levels with scientific information that they can use to develop climate policies\". The World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP) set up the IPCC in 1988. The United Nations endorsed the creation of the IPCC later that year. It has a secretariat in Geneva, Switzerland, hosted by the WMO. It has 195 member states who govern the IPCC. The member states elect a bureau of scientists to serve through an assessment cycle. A cycle is usually six to seven years. The bureau selects experts in their fields to prepare IPCC reports. There is a formal nomination process by governments and observer organizations to find these experts. The IPCC has three working groups and a task force, which carry out its scientific work.\nThe IPCC informs governments about the state of knowledge of climate change. It does this by examining all the relevant scientific literature on the subject. This includes the natural, economic and social impacts and risks. It also covers possible response options. The IPCC does not conduct its own original research. It aims to be objective and comprehensive. Thousands of scientists and other experts volunteer to review the publications. They compile key findings into \"Assessment Reports\" for policymakers and the general public; Experts have described this work as the biggest peer review process in the scientific community. The IPCC was the first of three global science policy panels to be established, followed by and IPBES (Intergovernmental Platform on Biodiversity and Ecosystem Services), established in 2012 and the Intergovernmental Science-Policy Panel on Chemicals, Waste and Pollution (ISPCWP) established in 2025. \nLeading climate scientists and all member governments endorse the IPCC's findings. This underscores that the IPCC is a well-respected authority on climate change. Governments, civil society organizations, and the media regularly quote from the panel's reports. IPCC reports play a key role in the annual climate negotiations held by the United Nations Framework Convention on Climate Change (UNFCCC). The IPCC Fifth Assessment Report was an important influence on the landmark Paris Agreement in 2015. The IPCC shared the 2007 Nobel Peace Prize with Al Gore for contributions to the understanding of climate change.\nThe seventh assessment cycle of the IPCC began in 2023. In August 2021, the IPCC published its Working Group I contribution to the Sixth Assessment Report on the physical science basis of climate change. \"The Guardian\" described this report as the \"starkest warning yet\" of \"major inevitable and irreversible climate changes\". Many newspapers around the world echoed this theme. In February 2022, the IPCC released its Working Group II report on impacts and adaptation. It published Working Group III's \"mitigation of climate change\" contribution to the Sixth Assessment in April 2022. The Sixth Assessment Report concluded with a Synthesis Report in March 2023.\nDuring the period of the Sixth Assessment Report, the IPCC released three special reports. The first and most influential was the Special Report on Global Warming of 1.5\u00b0C in 2018. In 2019 the Special Report on Climate Change and Land, and the Special Report on the Ocean and Cryosphere in a Changing Climate came out. The IPCC also updated its methodologies in 2019. So the sixth assessment cycle was the most ambitious in the IPCC's history.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nOrigins.\nThe predecessor of the IPCC was the Advisory Group on Greenhouse Gases (AGGG). Three organizations set up the AGGG in 1986. These were the International Council of Scientific Unions, the United Nations Environment Programme (UNEP), and the World Meteorological Organization (WMO). The AGGG reviewed scientific research on greenhouse gases. It also studied increases in greenhouse gases. Climate science was becoming more complicated and covering more disciplines. This small group of scientists lacked the resources to cover climate science.\nThe United States Environmental Protection Agency sought an international convention to restrict greenhouse gas emissions. The Reagan Administration worried that independent scientists would have too much influence. The WMO and UNEP therefore created the IPCC as an intergovernmental body in 1988. Scientists take part in the IPCC as both experts and government representatives. The IPCC produces reports backed by all leading relevant scientists. Member governments must also endorse the reports by consensus agreement. So the IPCC is both a scientific body and an organization of governments. Its job is to tell governments what scientists know about climate change. It also examines the impacts of climate change and options for dealing with it. The IPCC does this by assessing peer-reviewed scientific literature.\nThe United Nations endorsed the creation of the IPCC in 1988. The General Assembly resolution noted that human activity could change the climate. This could lead to severe economic and social consequences. It said increasing concentrations of greenhouse gases could warm the planet. This would cause the sea level to rise. The effects on humanity would be disastrous if timely steps were not taken.\nOrganization.\nWay of working.\nThe IPCC does not conduct original research. It produces comprehensive assessments on the state of knowledge of climate change. It prepares reports on special topics relevant to climate change. It also produces methodologies. These methodologies help countries estimate their greenhouse gas emissions and removals through sinks. Its assessments build on previous reports and scientific publications. Throughout six assessments the reports reflect the growing evidence for a changing climate. And they show how this is due to human activity.\nRules and governing principles.\nThe IPCC has adopted its rules of procedure in the \"Principles Governing IPCC Work\". These state that the IPCC will assess:\nUnder IPCC rules its assessments are comprehensive, objective, open, and transparent. They cover all the information relevant to the scientific understanding of climate change. This draws on scientific, technical, and socioeconomic information. IPCC reports must be neutral regarding policy recommendations. However, they may address the objective factors relevant to enacting policies.\nStructure.\nThe IPCC has the following structure:\nChair.\nThe chair of the IPCC is British energy scientist Jim Skea, who is hosted by the International Institute for Environment and Development. Skea has served since 28 July 2023 with the election of the new IPCC Bureau. His predecessor was Korean economist Hoesung Lee, elected in 2015. The previous chairs were Rajendra K. Pachauri, elected in 2002, Robert Watson, elected in 1997, and Bert Bolin, elected in 1988.\nPanel.\nThe Panel consists of representatives appointed by governments. They take part in plenary sessions of the IPCC and its Working Groups. Non-governmental and intergovernmental organizations may attend as observers. Meetings of IPCC bodies are by invitation only. About 500 people from 130 countries attended the 48th Session of the Panel in Incheon, Republic of Korea. This took place in October 2018. They included 290 government officials and 60 representatives of observer organizations. The opening ceremonies of sessions of the Panel and of Lead Author Meetings are open to media. Otherwise, IPCC meetings are closed.\nFunding.\nThe IPCC receives funding through a dedicated trust fund. UNEP and the WMO established the fund in 1989. The trust fund receives annual financial contributions from member governments. The WMO, UNEP, and other organizations also contribute. Payments are voluntary and there is no set amount required. The WMO covers the operating costs of the secretariat. It also sets the IPCC's financial regulations and rules. The Panel sets the annual budget.\nIn 2021, the IPCC's annual budget amounts to approximately six million euros, financed by the 195 UN Member states, who contribute \"independently and voluntarily\". In 2021, the countries giving the most money include the United States, Japan, France, Germany and Norway. Other countries, often developing ones, give an \"in-kind contribution, by hosting IPCC meetings\". In 2022, this budget was a little less than eight million euros.\nActivities other than report preparation.\nThe IPCC bases its work on the decisions of the WMO and UNEP, which established the IPCC. It also supports the work of the UNFCCC. The main work of the IPCC is to prepare assessments and other reports. It also supports other activities such as the Data Distribution Centre. This helps manage data related to IPCC reports.\nThe IPCC has a \"Gender Policy and Implementation Plan\" to pay attention to gender in its work. It aims to carry out its work inclusively and respectfully. The IPCC aims for balance in participation in IPCC work. This should offer all participants equal opportunity.\nCommunications and dissemination activities.\nThe IPCC enhanced its communications activities for the Fifth Assessment Report. For instance, it made the approved report and press release available to registered media under embargo before the release. And it expanded its outreach activities with an outreach calendar. The IPCC held an Expert Meeting on Communication in February 2016, at the start of the Sixth Assessment Report cycle. Members of the old and new Bureaus worked with communications experts and practitioners at this meeting. This meeting produced a series of recommendations. The IPCC adopted many of them. One was to bring people with communications expertise into the Working Group Technical Support Units. Another was to consider communication questions early on in the preparation of reports.\nFollowing these steps in communications, the IPCC saw a significant increase in media coverage of its reports. This was particularly the case with the \"Special Report on Global Warming of 1.5 \u00b0C\" in 2018 and \"Climate Change 2021: The Physical Science Basis\", the Working Group I contribution to the Sixth Assessment Report, in 2021. There was also much greater public interest, reflected in the youth and other movements that emerged in 2018.\nIPCC reports are important for public awareness of climate change and related policymaking. This has led to several academic studies of IPCC communications, for example in 2021.\nArchiving.\nThe IPCC archives its reports and electronic files on its website. They include the review comments on drafts of reports. The Environmental Science and Public Policy Archives in the Harvard Library also archives them.\nAssessment reports.\nBetween 1990 and 2023, the IPCC published six comprehensive assessment reports reviewing the latest climate science. The IPCC has also produced 14 special reports on particular topics. Each assessment report has four parts. These are a contribution from each of the three working groups, plus a synthesis report. The synthesis report integrates the working group contributions. It also integrates any special reports produced in that assessment cycle.\nReview process of scientific literature.\nThe IPCC does not carry out research. It does not monitor climate-related data. The reports by IPCC assess scientific papers and independent results from other scientific bodies. The IPCC sets a deadline for publication of scientific papers that a report will cover. That report will not include new information that emerges after this deadline. However, there is a steady evolution of key findings and levels of scientific confidence from one assessment report to the next. Each IPCC report notes areas where the science has improved since the previous report. It also notes areas that would benefit from further research.\nThe First Assessment Report was published in 1990 and received an update in 1992. In intervals of about six years, new editions of IPCC Assessment Report followed.\nSelection and role of authors.\nThe focal points of the Member states \u2014 the individual appointed by each state to liaise with the IPCC \u2014 and the observer organizations submit to the IPCC Bureau a list of personalities, which they have freely constituted. The Bureau (more precisely, the co-chairs of the relevant working group, with the help of its technical support unit) uses these lists as a basis for appointing authors while retaining the possibility of appointing people who are not on the list, primarily based on scientific excellence and diversity of viewpoints, and to a lesser extent by ensuring geographical diversity, experience within the IPCC and gender. Authors may include, in addition to researchers, personalities from the private sector and experts from NGOs.\nThe IPCC Bureau or Working Group Bureau selects the authors of the reports from government nominations. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (\"grey literature\"), if they are of sufficient quality. These could include reports from government agencies and non-governmental organizations. Industry journals and model results are other examples of non-peer-reviewed sources.\nAuthors prepare drafts of a full report divided into chapters. They also prepare a technical summary of the report, and a summary for policymakers.\nEach chapter has many authors to write and edit the material. A typical chapter has two coordinating lead authors, ten to fifteen lead authors, and a larger number of contributing authors. The coordinating lead authors assemble the contributions of the other authors. They ensure that contributions meet stylistic and formatting requirements. They report to the Working Group co-chairs. Lead authors write sections of chapters. They invite contributing authors to prepare text, graphs, or data for inclusion. Review editors must ensure that authors respond to comments received during the two stages of drafts review: the first is only open to external experts and researchers, while the second is also open to government representatives.\nThe Bureau aims for a range of views, expertise, and geographical representation in its choice of authors. This ensures the author team includes experts from both developing and developed countries. The Bureau also seeks a balance between male and female authors. It aims for a balance between those who have worked previously on IPCC reports and those new to the process.\nScientists who work as authors on IPCC reports do not receive any compensation for this work, and all work voluntarily. They depend on the salaries they receive from their home institutions or other work. The work is labour-intensive with a big time commitment. It can disrupt participating scientists' research. This has led to concern that the IPCC process may discourage qualified scientists from participating. More than 3,000 authors (coordinating lead authors, lead authors, review editors) have participated in the drafting of IPCC reports since its creation.\nReview process for assessment reports.\nExpert reviewers comment at different stages on the drafts. Reviewers come from member governments and IPCC observers. Also, anyone may become an IPCC reviewer by stating they have the relevant expertise.\nThere are generally three stages in the review process. First comes an expert review of the first draft of the chapters. The next stage is a review by governments and experts of the revised draft of the chapters and the first draft of the Summary for Policymakers. The third stage is a government review of the revised Summary for Policymakers. Review comments and author responses remain in an open archive for at least five years. Finally, government representatives together with the authors review the Summary for Policymakers. They go through the Summary for Policymakers line by line to ensure it is a good summary of the underlying report. This final review of the Summary of Policymakers takes place at sessions of the responsible working group or of the Panel.\nThere are several types of endorsement that documents receive:\nKey findings and impacts.\nSixth assessment report (2021/2022).\nThe IPCC's most recent report is the Sixth Assessment Report (AR6). The first three installments of AR6 appeared in 2021 and 2022. The final synthesis report was completed in March 2023.\nThe IPCC published the Working Group I report, \"Climate Change 2021: The Physical Science Basis\", in August 2021. It confirms that the climate is already changing in every region. Many of these changes have not been seen in thousands of years. Many of them such as sea-level rise are irreversible over hundreds of thousands of years. Strong reductions in greenhouse gas emissions would limit climate change. But it could take 20\u201330 years for the climate to stabilize. This report attracted enormous media and public attention. U.N. Secretary-General Ant\u00f3nio Guterres described it as \"code red for humanity\".\nThe IPCC published the Working Group II report, \"Climate Change 2022: Impacts, Adaptation and Vulnerability\", in February 2022. Climate change due to human activities is already affecting the lives of billions of people, it said. It is disrupting nature. The world faces unavoidable hazards over the next two decades even with global warming of 1.5\u00a0\u00b0C, it said.\nThe IPCC published the Working Group III report, \"Climate Change 2022: Mitigation of Climate Change\", in April 2022. It will be impossible to limit warming to 1.5\u00a0\u00b0C without immediate and deep cuts in greenhouse gas emissions. It is still possible to halve emissions by 2050, it said.\nOther reports.\nSpecial reports.\nThe IPCC also publishes other types of reports. It produces Special Reports on topics proposed by governments or observer organizations. Between 1994 and 2019 the IPCC published 14 special reports. Now usually more than one working group cooperates to produce a special report. The preparation and approval process is the same as for assessment reports.\nSpecial reports in 2011.\nDuring the fifth assessment cycle, the IPCC produced two special reports. It completed the Special Report on Renewable Energy Sources and Climate Change Mitigation in 2011. Working Group III prepared this report. The report examined options to use different types of renewable energy to replace fossil fuels. The report noted that the cost of most renewable technologies had fallen. It was likely to fall even more with further advances in technology. It said renewables could increase access to energy. The report reviewed 164 scenarios that examine how renewables could help stop climate change. In more than half of these scenarios, renewables would contribute more than 27% of the primary energy supply in the mid-century. This would be more than double the 13% share in 2008. In the scenarios with the highest shares for renewable energy, it will contribute 77% by 2050.\nLater in 2011, the IPCC released the Special Report on Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation. This was a collaboration between Working Groups I and II. It was the first time two IPCC working groups worked together on a special report. The report shows how climate change has contributed to changes in extreme weather. And it shows how policies to avoid and prepare for extreme weather events can reduce their impact. In the same way, policies to respond to events and recover from them can make societies more resilient.\nSpecial reports 2018\u20132019.\nDuring the sixth assessment cycle, the IPCC produced three special reports. This made it the most ambitious cycle in IPCC history. The UNFCCC set a goal of keeping global warming well below while trying to hold it at , when it reached the Paris Agreement at COP21 in 2015. But at the time there was little understanding of what warming of 1.5\u00a0\u00b0C meant. There was little scientific research explaining how the impacts of 1.5\u00a0\u00b0C would differ from 2\u00a0\u00b0C. And there was little understanding about how to keep warming to 1.5\u00a0\u00b0C. So the UNFCCC invited the IPCC to prepare a report on global warming of 1.5\u00a0\u00b0C. The IPCC subsequently released the Special Report on Global Warming of 1.5 \u00b0C (SR15) in 2018. The report showed that it was possible to keep warming below 1.5\u00a0\u00b0C during the 21st century. But this would mean deep cuts in emissions. It would also mean rapid, far-reaching changes in all aspects of society. The report showed warming of 2\u00a0\u00b0C would have much more severe impacts than 1.5\u00a0\u00b0C. In other words: every bit of warming matters. SR15 had an unprecedented impact on an IPCC report in the media and with the public. It put the 1.5\u00a0\u00b0C target at the centre of climate activism.\nIn 2019 the IPCC released two more special reports that examine different parts of the climate system. The Special Report on Climate Change and Land examined how the way we use land affects the climate. It looked at emissions from activities such as farming and forestry rather than from energy and transport. It also looked at how climate change is affecting land. All three IPCC working groups and its Task Force on National Greenhouse Gas Inventories collaborated on the report. The report found that climate change is adding to the pressures we are putting on the land we use to live on and grow our food. It will only be possible to keep warming well below 2\u00a0\u00b0C if we reduce emissions from all sectors including land and food, it said.\nThe Special Report on the Ocean and Cryosphere in a Changing Climate examined how the ocean and frozen parts of the planet interact with climate change. (The cryosphere includes frozen systems such as ice sheets, glaciers, and permafrost.) IPCC Working Groups I and II prepared the report. The report highlighted the need to tackle unprecedented changes in the ocean and cryosphere. It also showed how adaptation could help sustainable development.\nMethodology Reports.\nThe IPCC has a National Greenhouse Gas Inventories Programme. It develops methodologies and software for countries to report their greenhouse gas emissions. The IPCC's Task Force on National Greenhouse Gas Inventories (TFI) has managed the program since 1998. Japan's Institute for Global Environmental Strategies hosts the TFI's Technical Support Unit.\nRevised 1996 IPCC Guidelines.\nThe IPCC released its first Methodology Report, the IPCC \"Guidelines for National Greenhouse Gas Inventories\", in 1994. The \"Revised 1996 IPCC Guidelines for National Greenhouse Gas Inventories\" updated this report. Two \"good practice reports\" complete these guidelines. These are the \"Good Practice Guidance\" \"and Uncertainty Management in National Greenhouse Gas Inventories\" and \"Good Practice Guidance for Land Use, Land-Use Change and Forestry\". Parties to the UNFCCC and its Kyoto Protocol use the 1996 guidelines and two good practice reports for their annual submissions of inventories.\n2006 IPCC Guidelines.\nThe \"2006 IPCC Guidelines for National Greenhouse Gas Inventories\" further update these methodologies. They include a large number of \"default emission factors\". These are factors to estimate the amount of emissions for an activity. The IPCC prepared this new version of the guidelines at the request of the UNFCCC. The UNFCCC accepted them for use at its 2013 Climate Change Conference, COP19, in Warsaw. The IPCC added further material in its \"2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories\".\nThe TFI has started preparations for a methodology report on short-lived climate forcers (SLCFs). It will complete this report in the next assessment cycle, the seventh.\nChallenges and controversies.\nIPCC reports also attract criticism. Criticisms come from both people who say the reports exaggerate the risks and people who say they understate them. The IPCC consensus approach has faced internal and external challenges.\nConservative nature of IPCC reports.\nSome critics have argued that IPCC reports tend to be too conservative in their assessments of climate risk. In 2012, it was reported that the IPCC has been criticized by some scientists, who argue that the reports consistently underestimate the pace and impacts of global warming. As a result, they believe this leads to findings that are the \"lowest common denominator\". Similar claims have also been made by scientists who found that for the last several assessment reports, the focus of the IPCC reports skewed more and more towards lower temperatures, especially 1.5\u00b0C. Temperatures above 2\u00b0C however, have seen much less attention, even though they seem more likely given current emission trajectories. \nDavid Biello, writing in the Scientific American, argues that, because of the need to secure consensus among governmental representatives, the IPCC reports give conservative estimates of the likely extent and effects of global warming. \"Science\" editor Brooks Hanson states in a 2010 editorial: \"The IPCC reports have underestimated the pace of climate change while overestimating societies' abilities to curb greenhouse gas emissions.\"\nClimate scientist James E. Hansen argues that the IPCC's conservativeness seriously underestimates the risk of sea-level rise on the order of meters\u2014enough to inundate many low-lying areas, such as the southern third of Florida. In January 2024, he told the Guardian, \"We are now in the process of moving into the 1.5C world.\" He added that \"passing through the 1.5C world is a significant milestone because it shows that the story being told by the United Nations, with the acquiescence of its scientific advisory body, the IPCC, is a load of bullshit.\"\nRoger A. Pielke Sr. has also stated \"Humans are significantly altering the global climate, but in a variety of diverse ways beyond the radiative effect of carbon dioxide. The IPCC assessments have been too conservative in recognizing the importance of these human climate forcings as they alter regional and global climate.\"\nStefan Rahmstorf, a professor of physics and oceanography at University of Potsdam, argued in 2007 that the IPCC's tendency to make conservative risk assessments had benefits. Rahmstorf argued that \"In a way, it is one of the strengths of the IPCC to be very conservative and cautious and not overstate any climate change risk\". IPCC reports aim to inform policymakers about the state of knowledge on climate change. They do this by assessing the findings of the thousands of scientific papers available on the subject at a given time. Individual publications may have different conclusions from IPCC reports. This includes those appearing just after the release of an IPCC report. This can lead to criticism that the IPCC is either alarmist or conservative. New findings must wait for the next assessment for consideration.\nPotential industry and political influence.\nA memo by ExxonMobil to the Bush administration in the United States in 2002 was an example of possible political influence on the IPCC. The memo led to strong Bush administration lobbying to oust Robert Watson, a climate scientist, as IPCC chair. They sought to replace him with Rajendra Pachauri. Many considered Pachauri at the time as more mild-mannered and industry-friendly.\nGovernments form the membership of the IPCC. They are the prime audience for IPCC reports. IPCC rules give them a formal role in the scoping, preparation, and approval of reports. For instance governments take part in the review process and work with authors to approve the Summary for Policymakers of reports. But some activists have argued that governments abuse this role to influence the outcome of reports.\nIn 2023, it was reported that pressure from Brazil and Argentina, two countries with large beef industries, caused the IPCC to abandon text recommending the adoption of plant-based diets. An earlier draft of the report, which noted \"plant-based diets can reduce GHG emissions by up to 50% compared to the average emission-intensive Western diet\", was leaked online in March 2023.\nControversy and review after Fourth Assessment Report in 2007.\nThe IPCC came under unprecedented media scrutiny in 2009 in the run-up to the Copenhagen climate conference. This \"Climatic Research Unit email controversy\" involved the leak of emails from climate scientists. Many of these scientists were authors of the Fourth Assessment Report which came out in 2007. The discovery of an error in this report that the Himalayan glaciers would melt by 2035 put the IPCC under further pressure. Scientific bodies upheld the general findings of the Fourth Assessment Report and the IPCC's approach. But many people thought the IPCC should review the way it works.\nInterAcademy Council review in 2010.\nPublic debate after the publication of AR4 in 2009 put the IPCC under scrutiny, with controversies over alleged bias and inaccuracy in its reports. In 2010, this prompted U.N. Secretary-General Ban Ki-moon and IPCC chair Rajendra K. Pachauri to request that the InterAcademy Council (IAC) review the IPCC and recommend ways to strengthen its processes and procedures for the preparation of AR5. The IAC report made recommendations to fortify IPCC's management structure, to further develop its conflict-of-interest policy, to strengthen the review process, to clarify the guidelines on the use of so-called gray literature, to ensure consistency in the use of probabilities for the likelihood of outcomes, and to improve its communications strategy, especially regarding transparency and rapidity of response.\nThe United Nations Secretary-General and the Chair of the IPCC asked the InterAcademy Council (IAC) in March 2010 to review the IPCC's processes for preparing its reports. The IAC panel, chaired by Harold Tafler Shapiro, released its report on 1 September 2010. The IAC panel made seven formal recommendations for improving the IPCC's assessment process. The IPCC implemented most of the review's recommendations by 2012. One of these was the introduction of a protocol to handle errors in reports. Other recommendations included strengthening the science-review process and improving communications. However, the IPCC did not adopt the proposal to appoint a full-time executive secretary.\nIssues with consensual approach.\nMichael Oppenheimer, a long-time participant in the IPCC, has said the IPCC consensus approach has some limitations. Oppenheimer, a coordinating lead author of the Fifth Assessment Report, called for concurring, smaller assessments of special problems instead of the large-scale approach of previous IPCC assessments. Others see \"mixed blessings\" in the drive for consensus within the IPCC. They suggest including dissenting or minority positions. Others suggest improving statements about uncertainties.\nCriticism by experts involved with the IPCC process.\nSome of the criticism has originated from experts invited by the IPCC to submit reports or serve on its panels. For example, John Christy, a contributing author who works at the University of Alabama in Huntsville, explained in 2007 the difficulties of establishing scientific consensus on the precise extent of human action on climate change. \"Contributing authors essentially are asked to contribute a little text at the beginning and to review the first two drafts. We have no control over editing decisions. Even less influence is granted to the 2,000 or so reviewers. Thus, to say that 800 contributing authors or 2,000 reviewers reached consensus on anything describes a situation that is not reality\", he wrote. Christopher Landsea, a hurricane researcher, said of \"the part of the IPCC to which my expertise is relevant\" that \"I personally cannot in good faith continue to contribute to a process that I view as both being motivated by pre-conceived agendas and being scientifically unsound,\" because of comments made at a press conference by Kevin Trenberth of which Landsea disapproved. Trenberth said \"Landsea's comments were not correct\"; the IPCC replied \"individual scientists can do what they wish in their own rights, as long as they are not saying anything on behalf of the IPCC\".\nEndorsements and awards.\nEndorsements from scientific bodies.\nIPCC reports are the benchmark for climate science. There is widespread support for the IPCC in the scientific community. Publications by other scientific bodies and experts show this. Many scientific bodies have issued official statements that endorse the findings of the IPCC. For example:\nNobel Peace Prize in 2007.\nIn December 2007, the IPCC received the Nobel Peace Prize \"for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change\". It shared the award with former U.S. Vice-president Al Gore for his work on climate change and the documentary \"An Inconvenient Truth\".\nGulbenkian Prize for Humanity in 2022.\nIn October 2022, the IPCC and IPBES shared the Gulbenkian Prize for Humanity. The two intergovernmental bodies won the prize because they \"produce scientific knowledge, alert society, and inform decision-makers to make better choices for combatting climate change and the loss of biodiversity\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "15031", "revid": "784330", "url": "https://en.wikipedia.org/wiki?curid=15031", "title": "IPCC (disambiguation)", "text": "The IPCC, or Intergovernmental Panel on Climate Change, is a scientific body under the auspices of the United Nations.\nIPCC may also refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "15032", "revid": "91088", "url": "https://en.wikipedia.org/wiki?curid=15032", "title": "IBM Personal Computer", "text": "Personal computer model released in 1981\nThe IBM Personal Computer (model 5150, commonly known as the IBM PC) is the first microcomputer released in the IBM PC model line and the basis for the IBM PC compatible \"de facto\" standard. Released on August 12, 1981, it was created by a team of engineers and designers at International Business Machines (IBM), directed by William C. Lowe and Philip Don Estridge in Boca Raton, Florida.\nPowered by an x86-architecture Intel 8088 processor, the machine was based on open architecture and third-party peripherals. Over time, expansion cards and software technology increased to support it. The PC had a substantial influence on the personal computer market; the specifications of the IBM PC became one of the most popular computer design standards in the world. The only significant competition it faced from a non-compatible platform throughout the 1980s was from Apple's Macintosh product line, as well as consumer-grade platforms created by companies like Commodore and Atari. Most present-day personal computers share architectural features in common with the original IBM PC, including the Intel-based Mac computers manufactured from 2006 to 2022.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nPrior to the 1980s, IBM had largely been known as a provider of business computer systems. As the 1980s opened, their market share in the growing minicomputer market failed to keep up with competitors, while other manufacturers were beginning to see impressive profits in the microcomputer space. The market for personal computers was dominated at the time by Tandy, Commodore, and Apple, whose machines sold for several hundred dollars each and had become very popular. The microcomputer market was large enough for IBM's attention, with $15\u00a0billion in sales by 1979 and projected annual growth of more than 40% during the early 1980s. Other large technology companies had entered it, such as Hewlett-Packard, Texas Instruments, and Data General, and some large IBM customers were buying Apples.\nAs early as 1980 there were rumors of IBM developing a personal computer, possibly a miniaturized version of the IBM System/370, and Matsushita acknowledged publicly that it had discussed with IBM the possibility of manufacturing a personal computer in partnership, although this project was abandoned. The public responded to these rumors with skepticism, owing to IBM's tendency towards slow-moving, bureaucratic business practices tailored towards the production of large, sophisticated and expensive business systems. As with other large computer companies, its new products typically required about four to five years for development, and a well publicized quote from an industry analyst was, \"IBM bringing out a personal computer would be like teaching an elephant to tap dance.\"\nIBM had previously produced microcomputers, such as 1975's IBM 5100, but targeted them towards businesses; the 5100 had a price tag as high as $20,000. Their entry into the home computer market needed to be competitively priced.\nIn the summer of 1979, Ron Mion, IBM\u2019s Senior Business Trends Advisor for entry-level systems, proposed a plan for IBM to enter the emerging microcomputer market. At that time, the likes of Apple and Tandy were starting to encroach on the small-business marketplace that IBM intended to dominate. Mion believed that that market would grow significantly and that IBM should aggressively pursue it. However, he felt that they wouldn\u2019t be successful unless IBM departed from its long-standing business model.\nMion\u2019s plan called for three major departures from how IBM traditionally did business. Mion felt that, if IBM wanted to compete in the microcomputer market, it would need to:\na) Greatly reduce manufacturing costs by using standard, off-the-shelf components (e.g., disk drives, CRTs, power supplies, keyboards) in order to produce a competitively priced microcomputer\nb) Use a low-cost, third-party operating system. Mion felt that this was imperative in order to foster a cottage industry that could develop a broad array of applications that would help small businesses justify the purchase of a computer. Mion recommended Digital Research\u2019s CP/M and a new O/S called MS-DOS from a little-known company named Microsoft.\nc) Allow its microcomputers to be sold and serviced by a distribution channel consisting of independent resellers. (At that time, IBM had been experimenting with a chain of IBM Business Systems Center storefronts but their least-expensive computer cost $14,000.)\nThat plan made its way up the chain of command but was ultimately rejected in the fall. The top IBM executives reaffirmed that all \u201cIBM\u201d computers, and their major components, must be developed, manufactured, sold, and serviced by IBM.\nIn January 1980, Tandy released their Annual Report and, as was predicted in Mion's plan, it confirmed that their 1979 shipments had exceeded 100,000 TRS-80s (about $50 million worth). IBM quickly dusted off Mion\u2019s marketing plan.\nIn 1980, IBM president John Opel, recognizing the value of entering this growing market, assigned William C. Lowe and Philip Don Estridge as heads of the new Entry Level Systems unit in Boca Raton, Florida. Market research found that computer dealers were very interested in selling an IBM product, but they insisted the company use a design based on standard parts, not IBM-designed ones so that stores could perform their own repairs rather than requiring customers to send machines back to IBM for service. Another source cites time pressure as the reason for the decision to use third-party components.\nAtari proposed to IBM in 1980 that it act as original equipment manufacturer for an IBM microcomputer, a potential solution to IBM's known inability to move quickly to meet a rapidly changing market. The idea of acquiring Atari was considered but rejected in favor of a proposal by Lowe that by forming an independent internal working group and abandoning all traditional IBM methods, a design could be delivered within a year and a prototype within 30 days. The prototype worked poorly but was presented with a detailed business plan which proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM practice. It also estimated sales of 220,000 computers over three years, more than IBM's entire installed base.\nThis swayed the Corporate Management Committee, which converted the group into a business unit named \"Project Chess\", and provided the necessary funding and authority to do whatever was needed to develop the computer in the given timeframe. The team received permission to expand to 150 people by the end of 1980, and in one day more than 500 IBM employees called in asking to join.\nDesign process.\nThe design process was kept under a policy of strict secrecy, with all other IBM divisions kept in the dark about the project.\nSeveral CPUs were considered, including the Texas Instruments TMS9900, Motorola 68000 and Intel 8088. The 68000 had 32 bit registers with a flat 24 bit address space for up to 16MB of memory and was considered the best choice, but was not production-ready like the others. The IBM 801 RISC processor was also considered, since it was considerably more powerful than the other options, but rejected due to the design constraint to use off-the-shelf parts. The TMS9900 had only 16 bits of address space which was the same as other 8 bit chips and was rejected as it was inferior to the Intel 8088 which had 20 bits of address space which could use one megabyte of memory.\nThe Intel 8086 architecture had 16 bit registers and used a segment scheme to increase the address space to 20 bits or 1MB of memory which complicated programming but was a big step up from 64K limit of most 8 bit chips. The 8086 was designed as a source code compatible, though not binary compatible, extension of the older 8080 which made it easier to port existing software like BASIC. IBM chose the 8088 variant of the 16 bit 8086 because Intel offered a better price for the former and could provide more units, and the 8088's 8-bit bus reduced the cost of the rest of the computer. The 8088 had the advantage that IBM already had familiarity with the 8085 from designing the IBM System/23 Datamaster. The 62-pin expansion bus slots were also designed to be similar to the Datamaster slots, and its keyboard design and layout became the Model F keyboard shipped with the PC, but otherwise the PC design differed in many ways.\nThe 8088 motherboard was designed in 40 days, with a working prototype created in four months, demonstrated in January 1981. The design was essentially complete by April 1981, when it was handed off to the manufacturing team. PCs were assembled in an IBM plant in Boca Raton, with components made at various IBM and third party factories. The monitor was an existing design from IBM Japan; the printer was manufactured by Epson. Because none of the functional components were designed by IBM, they obtained only a handful of patents on the PC, covering such features as the bytecoding for color monitors, DMA access operation, and the keyboard interface. They were never enforced.\nMany of the designers were computer hobbyists who owned their own computers, including many Apple II owners, which influenced the decisions to design the computer with an open architecture and publish technical information so others could create compatible software and expansion slot peripherals.\nDuring the design process IBM avoided vertical integration as much as possible, for example choosing to license Microsoft BASIC rather than utilizing the in-house version of BASIC used for mainframes due to the better existing public familiarity with the Microsoft version.\nDebut.\nThe IBM PC debuted on August 12, 1981, after a twelve-month development. Pricing started at $1,565 for a configuration with 16\u00a0KB RAM, Color Graphics Adapter, keyboard, and no disk drives. The price was designed to compete with comparable machines in the market. For comparison, the Datamaster, announced two weeks earlier as IBM's least expensive computer, cost $10,000.\nIBM's marketing campaign licensed the likeness of Charlie Chaplin's character \"The Little Tramp\" for a series of advertisements based on Chaplin's movies, played by Billy Scudder.\nThe PC was IBM's first attempt to sell a computer through retail channels rather than directly to customers. Because it did not have retail experience other than the IBM Product Centers it began opening in 1980, the company ensured that software like VisiCalc and EasyWriter was quickly available. It partnered with the retail chains ComputerLand and Sears, whicih provided important knowledge of the marketplace and became the main outlets for the PC. More than 190 ComputerLand stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product.\nReception was overwhelmingly positive, with analysts estimating sales volume in the billions of dollars in the first few years after release. After release, IBM's PC immediately became the talk of the entire computing industry. Dealers were overwhelmed with orders, including customers offering pre-payment for machines with no guaranteed delivery date. By the time the machine began shipping, the term \"PC\" was becoming a household name.\nSuccess.\nSales exceeded IBM's expectations by as much as 800% (9x), with the company at one point shipping as many as 40,000 PCs per month. IBM estimated that home users made up 50 to 70% of purchases from retail stores. In 1983, IBM sold more than 750,000 machines; the company understood how the retail market differed from corporate customers better than traditional rivals. Hewlett-Packard, Xerox, and Digital Equipment Corporation (DEC)'s personal computers were unsuccessful; DEC, one of the companies whose success in competing against IBM had caused the latter to enter the PC market, sold only 69,000.\nSoftware support from the industry grew rapidly, with the IBM nearly instantly becoming the primary target for most microcomputer software development. One publication counted 753 software packages available a year after the PC's release, four times as many as were available for the Macintosh a year after its launch. Hardware support also grew rapidly, with 30\u201340 companies competing to sell memory expansion cards within a year.\nBy 1984, IBM's revenue from the PC market was $4 billion, more than twice that of Apple. A 1983 study of corporate customers found that two thirds of large customers standardizing on one computer chose the PC, while only 9% chose Apple. A 1985 \"Fortune\" survey found that 56% of American companies with personal computers used PCs while 16% used Apple.\nAlmost as soon as the PC reached the market, rumors of hardware and software compatible clones began, and the first legal PC-compatible clone\u2014the MPC 1600 by Columbia Data Products\u2014was released in June 1982, less than a year after the PC's debut.\nEventually, IBM sold its PC business to Lenovo in 2004.\nHardware.\nFor low cost and a quick design turnaround time, the hardware design of the IBM PC used entirely \"off-the-shelf\" parts from third party manufacturers, rather than unique hardware designed by IBM.\nThe PC is housed in a wide, short steel chassis intended to support the weight of a CRT monitor. The front panel is made of plastic, with an opening where one or two disk drives can be installed. The back panel houses a power inlet and switch, a keyboard connector, a cassette connector and a series of tall vertical slots with blank metal panels which can be removed in order to install expansion cards.\nInternally, the chassis is dominated by a motherboard which houses the CPU, built-in RAM, expansion RAM sockets, and slots for expansion cards.\nThe IBM PC was highly expandable and upgradeable, but the base factory configuration included:\nMotherboard.\nThe PC is built around a single large circuit board called a motherboard which carries the processor, built-in RAM, expansion slots, keyboard and cassette ports, and the various peripheral integrated circuits that connected and controlled the components of the machine.\nThe peripheral chips included an Intel 8259 PIC, an Intel 8237 DMA controller, and an Intel 8253 PIT. The PIT provides 18.2 Hz clock \"ticks\" and dynamic memory refresh timing.\nCPU and RAM.\nThe CPU is an Intel 8088, a cost-reduced form of the Intel 8086 which largely retains the 8086's internal 16-bit logic, but exposes only an 8-bit bus. The CPU is clocked at 4.77\u00a0MHz; clones and later PC models have higher CPU speeds that break compatibility with software developed for the original PC. The single base clock frequency for the system is 14.31818\u00a0MHz, which when divided by 3, yielded the 4.77\u00a0MHz for the CPU (which was considered close enough to the then 5\u00a0MHz limit of the 8088), and when divided by 4, yields the required 3.579545\u00a0MHz for the NTSC color carrier frequency.\nThe PC motherboard includes a second, empty socket, described by IBM simply as an \"auxiliary processor socket\", although the most obvious use was the addition of an Intel 8087 math coprocessor, which improves floating-point math performance.\nPC mainboards were manufactured with the first memory bank of initially Mostek 4116-compatible, or later 4164-compatible DIP DRAMs soldered to the board, for a minimum configuration of first just 16 KB, or later 64 KB of RAM. Memory upgrades from IBM and third parties provide socketed installation in three further onboard banks, and as ISA expansion cards.\nThe first 400,000 16\u00a0KB mainboards (\"16KB-64KB\" ID) sold until March 1983 can be upgraded to a maximum of 64\u00a0KB onboard without using slots, and the later 64\u00a0KB revision (\"64KB-256KB\" ID) to a maximum of 256\u00a0KB on the motherboard. RAM cards can upgrade either variant further, for a total of 640 KB conventional memory, and possibly several megabytes of expanded memory beyond that, though on PC/XT-class machines, the latter was a very expensive third-party hardware option only available later in the IBM 5150's lifecycle and only usable with dedicated software support (i.e. only accessible via a RAM window in the Upper Memory Area); this was relatively rarely equipped and utilized on the original IBM PC, much less fully so, thus the machine's maximum RAM configuration as commonly understood is 640\u00a0KB.\nROM BIOS.\nThe BIOS is the firmware of the IBM PC, occupying one 8\u00a0KB chip on the motherboard. It provides bootstrap code and a library of common functions that all software can use for many purposes, such as video output, keyboard input, disk access, interrupt handling, testing memory, and other functions.\nIBM shipped three versions of the BIOS throughout the PC's lifespan, with the dates 04/24/81, 10/19/81, and 10/27/82 (the first to boot from hard disk). The company offered an upgrade kit.\nDisplay.\nWhile most home computers had built-in video output hardware, IBM took the unusual approach of offering two different graphics options, the MDA and CGA cards. The former provided high-resolution monochrome text, but could not display anything except text, while the latter provided medium- and low-resolution color graphics and text.\nCGA used the same scan rate as NTSC television, allowing it to provide a composite video output which could be used with any compatible television or composite monitor, as well as a direct-drive TTL output suitable for use with any RGBI monitor using an NTSC scan rate. IBM also sold the 5153 color monitor for this purpose, but it was not available at release and was not released until March 1983.\nMDA scanned at a higher frequency and required a proprietary monitor, the IBM 5151. The card also included a built-in printer port.\nBoth cards could also be installed simultaneously for mixed graphics and text applications. For instance, AutoCAD, Lotus 1-2-3 and other software allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Third parties went on to provide an enormous variety of aftermarket graphics adapters, such as the Hercules Graphics Card.\nThe software and hardware of the PC, at release, was designed around a single 8-bit adaptation of the ASCII character set, now known as code page 437.\nStorage.\nThe two bays in the front of the machine could be populated with one or two 5.25\u2033 floppy disk drives, storing 160\u00a0KB per disk side for a total of 320\u00a0KB of storage on one disk. The floppy drives require a controller card inserted in an expansion slot, and connect with a single ribbon cable with two edge connectors. The IBM floppy controller card provides an external 37-pin D-sub connector for attachment of an external disk drive, although IBM did not offer one for purchase until 1986.\nAs was common for home computers of the era, the IBM PC offered a port for connecting a cassette data recorder. Unlike the typical home computer however, this was never a major avenue for software distribution, probably because very few PCs were sold without floppy drives. The port was removed on the very next PC model, the XT.\nAt release, IBM did not offer any hard disk drive option and adding one was difficult - the PC's stock power supply had inadequate power to run a hard drive, the motherboard did not support BIOS expansion ROMs which was needed to support a hard drive controller, and both PC DOS and the BIOS had no support for hard disks. After the XT was released, IBM altered the design of the 5150 to add most of these capabilities, except for the upgraded power supply. At this point adding a hard drive was possible, but required the purchase of the IBM 5161 Expansion Unit, which contained a dedicated power supply and included a hard drive.\nAlthough official hard drive support did not exist, the third party market did provide early hard drives that connected to the floppy disk controller, but required a patched version of PC DOS to support the larger disk sizes.\nHuman interface.\nThe only option for human interface provided in the base PC was the built-in keyboard port, meant to connect to the included Model F keyboard. The Model F was initially developed for the IBM Datamaster, and was substantially better than the keyboards provided with virtually all home computers on the market at that time in many regards - number of keys, reliability and ergonomics. While some home computers of the time utilized chiclet keyboards or inexpensive mechanical designs, the IBM keyboard provided good ergonomics, reliable and positive tactile key mechanisms and flip-up feet to adjust its angle. Public reception of the keyboard was extremely positive, with some sources describing it as a major selling point of the PC and even as \"the best keyboard available on any microcomputer.\"\nAt release, IBM provided a Game Control Adapter which offered a 15-pin port intended for the connection of up to two joysticks, each having two analog axes and two buttons. (The early PCs did not have support for a mouse.)\nCommunications.\nConnectivity to other computers and peripherals was initially provided through serial and parallel ports.\nIBM provided a serial card based on an 8250 UART. The BIOS supports up to two serial ports.\nIBM provided two different options for connecting Centronics-compatible parallel printers. One was the IBM Printer Adapter, and the other was integrated into the MDA as the IBM Monochrome Display and Printer Adapter.\nExpansion.\nThe expansion capability of the IBM PC was very significant to its success in the market. Some publications highlighted IBM's uncharacteristic decision to publish complete, thorough specifications of the system bus and memory map immediately on release, with the intention of fostering a market of compatible third-party hardware and software.\nThe motherboard includes five 62-pin card edge connectors which are connected to the CPU's I/O lines. IBM referred to these as \"I/O slots\", but after the expansion of the PC clone industry they became retroactively known as the ISA bus. At the back of the machine is a metal panel, integrated into the steel chassis of the system unit, with a series of vertical slots lined up with each card slot.\nMost expansion cards have a matching metal bracket which slots into one of these openings, serving two purposes. First, a screw inserted through a tab on the bracket into the chassis fastens the card securely in place, preventing the card from wiggling out of place. Second, any ports the card provides for external attachment are bolted to the bracket, keeping them secured in place as well.\nThe PC expansion slots can accept an enormous variety of expansion hardware, adding capabilities such as:\nThe market reacted as IBM had intended, and within a year or two of the PC's release the available options for expansion hardware were immense.\n5161 Expansion Unit.\nThe expandability of the PC was important, but had significant limitations.\nOne major limitation was the inability to install a hard drive, as described above. Another was that there were only five expansion slots, which tended to get filled up by essential hardware - a PC with a graphics card, memory expansion, parallel card and serial card was left with only one open slot, for instance.\nIBM rectified these problems in the later XT, which included more slots and support for an internal hard drive, but at the same time released the 5161 Expansion Unit, which could be used with either the XT or the original PC. The 5161 connected to the PC system unit using a cable and a card plugged into an expansion slot, and provided a second system chassis with more expansion slots and a hard drive.\nSoftware.\nIBM initially announced intent to support multiple operating systems: CP/M-86, UCSD p-System, and an in-house product called IBM PC DOS, based on 86-DOS from Seattle Computer Products and provided by Microsoft. In practice, IBM's expectation and intent was for the market to primarily use PC DOS. CP/M-86 was not available for six months after the PC's release and received extremely few orders once it was, and p-System was also not available at release. PC DOS rapidly established itself as the standard OS for the PC and remained the standard for over a decade, with a variant being sold by Microsoft themselves as MS-DOS.\nThe PC included BASIC in ROM (four 8\u00a0KB chips), a common feature of 1980s home computers. Its ROM BASIC supported the cassette tape interface, but PC DOS did not, limiting use of that interface to BASIC only.\nPC DOS version 1.00 supported only 160\u00a0KB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160\u00a0KB SSDD and 320\u00a0KB DSDD floppies. Support for the slightly larger nine sector per track 180\u00a0KB and 360\u00a0KB formats was added in March 1983.\nThird-party software support grew extremely quickly, and within a year the PC platform was supplied with a vast array of titles for any conceivable purpose.\nReception.\nThe PC improved IBM's reputation with investors, customers, and the general public. The computer's reception was extremely positive. Even before its release reviewers were impressed by the advertised specifications of the machine, and upon its release reviews praised virtually every aspect of its design both in comparison to contemporary machines and with regards to new and unexpected features.\nPraise was directed at the build quality of the PC, in particular its keyboard, IBM's decision to use open specifications to encourage third party software and hardware development, their speed at delivering documentation and the quality therein, the quality of the video display, and the use of commodity components from established suppliers in the electronics industry. The price was considered extremely competitive compared to the value per dollar of competing machines.\nTwo years after its release, \"Byte\" magazine retrospectively concluded that the PC had succeeded both because of its features \u2013 an 80-column screen, open architecture, and high-quality keyboard \u2013 and the failure of other computer manufacturers to achieve these features first:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\n\"Creative Computing\" that year named the PC the best desktop computer between $2,000 and $4,000, praising its vast hardware and software selection, manufacturer support, and resale value.\nMany IBM PCs remained in service long after their technology became largely obsolete. For instance, as of June 2006 (23\u201325 years after release) IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, processing data returned from radiosondes attached to weather balloons.\nDue to its status as the first entry in the extremely influential PC industry, the original IBM PC remains valuable as a collector's item. As of 2007[ [update]], the system had a market value of $50\u2013$500.\nModel line.\nIBM sold a number of computers under the \"Personal Computer\" or \"PC\" name throughout the 1980s. The name was not used for several years before being reused for the IBM PC Series in the 1990s and early 2000s. The PC line was replaced by the next generation IBM PS/2 in 1987 which introduced new hardware standards incompatible with those previously established by IBM and adopted in the IBM PC compatible industry. The IBM PC Series returned to the ISA standard hardware and was replaced by the IBM NetVista, which in turn was replaced by ThinkCentre models in 2003. IBM Personal Systems Group was sold to Lenovo in 2005.\nAs with all PC-derived systems, all IBM PC models are nominally software-compatible, although some timing-sensitive software will not run correctly on models with faster CPUs.\nClones.\nWhile IBM released documentation for most of the PC's architecture in order to allow third-party manufacturers to produce compatible hardware and software, the BIOS remained IBM's proprietary intellectual property. Because the IBM PC was based on commodity hardware rather than unique IBM components, and because its operation was extensively documented by IBM, creating machines that were fully compatible with the PC presented few challenges other than the creation of a compatible BIOS ROM.\nSimple duplication of the IBM PC BIOS was a direct violation of copyright law, but soon into the PC's life the BIOS was reverse-engineered with clean-room design by companies like Compaq, Phoenix Software Associates, American Megatrends and Award, who either built their own computers that could run the same software and use the same expansion hardware as the PC, or sold their BIOS code to other manufacturers who wished to build their own machines.\nThese machines became known as IBM compatibles or \"clones\", and software was widely marketed as compatible with \"IBM PC or 100% compatible\". Shortly thereafter, clone manufacturers began to make improvements and extensions to the hardware, such as by using faster processors like the NEC V20, which executed the same software as the 8088 at a higher speed up to 10\u00a0MHz.\nThe clone market eventually became so large that it lost its associations with the original PC and became a set of \"de facto\" standards established by various hardware manufacturers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
