{"id": "14097", "revid": "49047035", "url": "https://en.wikipedia.org/wiki?curid=14097", "title": "History of Asia", "text": "The history of Asia can be seen as the collective history of East Asia, South Asia, Southeast Asia and West Asia. The continent is home to two of the world's oldest, continuous civilizations, Chinese civilization and Indian civilization.\nAsia was also home to the Mesopotamian, Indus Valley and Yellow river civilizations. These civilizations were among the first in the world, and developed around fertile river valleys as they were conducive to agriculture. They shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Other inventions such as writing likely developed independently as did Cities, states, and empires.\nThe steppe region had long been inhabited by nomads, and from the central steppes, they could reach all parts of the Asian continent. The northern part of the continent, covering much of Siberia was inaccessible to the steppe nomads due to the dense forests and the tundra. These areas in Siberia were very sparsely populated. Mountains and deserts such as the Caucasus, Himalayas, Karakum and Gobi Desert formed natural barriers against the steppe nomads. The urban centers were technologically and culturally more advanced, but could do little militarily to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grassland to support a large horse mounted force. Thus the nomads who conquered states in West Asia were soon forced to adopt local customs.\nAsia has been the birthplace of many religions. They include Abrahamic religions such as Judaism, Christianity and Islam, as well as Indian religions such as Hinduism, Buddhism, Jainism and Sikhism. Other religions include Zoroastrianism and the now dead religion of Manichaeism. The spread of Islam ushered in the Islamic Golden Age and the Timurid Renaissance, which later went on to influence the Islamic gunpowder empires.\nThe history of Asia includes major developments such as the invention of gunpowder in medieval China, which was later developed by the Gunpowder empires, mainly the Mughals and Safavids, and led to significant advancements in warfare. The Silk Road, helped spread cultures, languages, religions, as well as diseases throughout Asia and Europe.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nPrehistory.\nA report by archaeologist Rakesh Tewari on Lahuradewa, India shows new C14 datings that range between 9000 and 8000 BC associated with rice, making Lahuradewa the earliest Neolithic site in entire South Asia. Settled life emerged on the subcontinent in the western margins of the Indus River alluvium approximately 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BC.\nG\u00f6bekli Tepe is a Neolithic site in the Southeastern Anatolia Region of Turkey. Dated to the Pre-Pottery Neolithic, between c. 9500 and 8000 BC, the site comprises a number of large circular structures supported by massive stone pillars \u2013 the world's oldest known megaliths.\nThe prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 8000\u20137000 BC, Neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than 1,200 square meters and the collection of Neolithic findings at the site consists of two phases.\nAround 5500 BC the Halafian culture appeared in Lebanon, Israel, Syria, Anatolia, and northern Mesopotamia, based upon dryland agriculture.\nIn southern Mesopotamia were the alluvial plains of Sumer and Elam. Since there was little rainfall, irrigation systems were necessary. The Ubaid culture flourished from 5500 BC.\nAncient.\nBronze Age.\nThe Chalcolithic period (or Copper Age) began about 4500 BC, then the Bronze Age began about 3500 BC, replacing the Neolithic cultures.\nThe Indus Valley civilization (IVC) was a Bronze Age civilization (3300\u20131300 BC; mature period 2600\u20131900 BC) which was centered mostly in the western part of the Indian Subcontinent; it is considered that an early form of Hinduism was performed during this civilization. Some of the great cities of this civilization include Harappa and Mohenjo-daro, which had a high level of town planning and arts. The cause of the destruction of these regions around 1700 BC is debatable, although evidence suggests it was caused by natural disasters (especially flooding). This era marks Vedic period in India, which lasted from roughly 1500 to 500 BC. During this period, the Sanskrit language developed and the Vedas were written, epic hymns that told tales of gods and wars. This was the basis for the Vedic religion, which would eventually sophisticate and develop into Hinduism.\nChina and Vietnam were also centres of metalworking. Dating back to the Neolithic Age, the first bronze drums, called the Dong Son drums have been uncovered in and around the Red River Delta regions of Vietnam and Southern China. These relate to the prehistoric Dong Son Culture of Vietnam.\nIn Ban Chiang, Thailand (Southeast Asia), bronze artifacts have been discovered dating to 2100 BC. In Nyaunggan, Burma bronze tools have been excavated along with ceramics and stone artifacts. Dating is still currently broad (3500\u2013500 BC).\nIron and Axial Age.\nThe Iron Age saw the widespread use of iron tools, weaponry, and armor throughout the major civilizations of Asia.\nMiddle East.\nThe Achaemenid dynasty of the Persian Empire, founded by Cyrus the Great, ruled an area from Greece and Turkey to the Indus River and Central Asia during the 6th to 4th centuries BC. Persian politics included a tolerance for other cultures, a highly centralized government, and significant infrastructure developments. Later, in Darius the Great's rule, the territories were integrated, a bureaucracy was developed, nobility were assigned military positions, tax collection was carefully organized, and spies were used to ensure the loyalty of regional officials. \nThe primary religion of Persia at this time was Zoroastrianism. Developed by the philosopher Zoroaster, the religion introduced an early form of monotheism to the area and tenets such as spiritual salvation through personal moral action, an end time, and both general and Particular judgment with a heaven or hell. It also banned animal sacrifice and the use of intoxicants in rituals. Rooted in ancient religious practices predating known history, these concepts would heavily influence later emperors and the masses. \nThe Persian Empire was successful in establishing peace and stability throughout the Middle East and was a major influence in art, politics (affecting Hellenistic leaders), and religion.\nAlexander the Great conquered this dynasty in the 4th century BC, creating the brief Hellenistic period. However, he was unable to establish stability; after his death, Persia broke into small, weak dynasties including the Seleucid Empire, followed by the Parthian Empire. By the end of the Classical age, Persia had been reconsolidated into the Sassanid Empire, also known as the second Persian Empire.\nThe Roman Empire would later control parts of Western Asia. The Seleucid, Parthian and Sassanid dynasties of Persia dominated Western Asia for centuries.\nIndia.\nThe Maurya and Gupta empires are called the Golden Age of India and were marked by extensive inventions and discoveries in science, technology, art, religion, and philosophy that crystallized the elements of what is generally known as Indian culture. The religions of Hinduism and Buddhism, which began in the Indian sub-continent, were important influences on South, East and Southeast Asia.\nBy 600 BC, the Indian subcontinent was politically fragmented into numerous states, including the sixteen major Mah\u0101janapadas (Sanskrit: \u092e\u0939\u093e\u091c\u0928\u092a\u0926) and many smaller kingdoms and republics that often competed and feuded with one another. \nIn 327 BC, Alexander the Great advanced through Bactria and crossed into northwestern India. He campaigned across the Punjab to the Beas River, but his army refused to march farther east into the Ganges basin, forcing him to retreat. The Macedonian withdrawal weakened Greek control in the region and opened the way for new powers. \nShortly afterward, Chandragupta Maurya, with the counsel of Chanakya, overthrew the Nanda dynasty around 321 BC and established the Maurya Empire (Sanskrit: \u092e\u094c\u0930\u094d\u092f \u0930\u093e\u091c\u0935\u0902\u0936, \"Maurya R\u0101java\u1e43\u015ba\"). The Mauryan state became one of the world\u2019s largest empires of its time, stretching north to the Himalayas, east into what is now Assam, west beyond modern Pakistan, and annexing Balochistan and much of present-day Afghanistan at its greatest extent. South of the empire lay Tamilakam, an independent region dominated by the Cholas, Pandyas, and Cheras. \nThe Mauryan Empire was notable for its sophisticated administration. Chandragupta and his successors ruled through an autocratic monarchy supported by a standing army, provincial governors, a complex bureaucracy, a regulated taxation system, state monopolies in key commodities, standardized coinage, and even a postal network. \nChandragupta\u2019s grandson, Ashoka (r. 268\u2013232 BC), expanded Mauryan control across almost the entire subcontinent (except for the southern tip). Following his conversion to Buddhism, Ashoka promoted dhamma (Sanskrit: \u0927\u0930\u094d\u092e, Pali: \u0927\u092e\u094d\u092e, \"dhamma\" = righteousness), non-violence, and public welfare through his famous edicts, leaving a lasting impact on Indian society and religion. After Ashoka\u2019s death, the empire gradually fragmented; by 185 BC the last Mauryan ruler was overthrown, and successor states arose in the north and northwest, including the \u015au\u1e45gas, Indo-Greeks, and \u015aakas. \nThe Kushan Empire (Sanskrit: \u0915\u0941\u0937\u093e\u0923 \u0930\u093e\u091c\u0935\u0902\u0936, \"Ku\u1e63\u0101\u1e47a R\u0101java\u1e43\u015ba\"; 1st\u20133rd centuries AD), founded by Central Asian invaders from the northwest, became a major power across northern India and Central Asia. Under Emperor Kanishka (c. 127\u2013150 AD), the Kushans were notable patrons of Buddhism, supporting its spread along the Silk Roads into Central Asia and China. While Buddhism flourished under their rule, in later centuries its close association with foreign dynasties contributed to perceptions of it as an \u201coutside\u201d religion, a factor in its eventual decline within India. \nThe Gupta Empire (Sanskrit: \u0917\u0941\u092a\u094d\u0924 \u0930\u093e\u091c\u0935\u0902\u0936, \"Gupta R\u0101java\u1e43\u015ba\"), founded by Chandragupta I around AD 320, later unified much of northern India through conquest and alliance. Gupta rule covered less territory than the Mauryas but brought long-lasting stability and is remembered as a classical age of Indian civilization, marked by advances in art, science, and literature. By the mid-6th century AD, sustained invasions by the H\u016b\u1e47as and internal divisions contributed to Gupta decline and fragmentation. \nClassical China.\nZhou dynasty.\nSince 1029 BC, the Zhou dynasty ( ), had existed in China and it would continue to until 258 BC. The Zhou dynasty had been using a feudal system by giving power to local nobility and relying on their loyalty in order to control its large territory. As a result, the Chinese government at this time tended to be very decentralized and weak, and there was often little the emperor could do to resolve national issues. Nonetheless, the government was able to retain its position with the creation of the Mandate of Heaven, which could establish an emperor as divinely chosen to rule. The Zhou additionally discouraged the human sacrifice of the preceding eras and unified the Chinese language. Finally, the Zhou government encouraged settlers to move into the Yangtze River valley, thus creating the Chinese Middle Kingdom.\nBut by 500 BC, its political stability began to decline due to repeated nomadic incursions and internal conflict derived from the fighting princes and families. This was lessened by the many philosophical movements, starting with the life of Confucius. His philosophical writings (called Confucianism) concerning the respect of elders and of the state would later be popularly used in the Han dynasty. Additionally, Laozi's concepts of Taoism, including yin and yang and the innate duality and balance of nature and the universe, became popular throughout this period. Nevertheless, the Zhou dynasty eventually disintegrated as the local nobles began to gain more power and their conflict devolved into the Warring States period, from 402 to 201 BC.\nQin dynasty.\nOne leader eventually came on top, Qin Shi Huang (, \"Sh\u01d0 Hu\u00e1ngd\u00ec\"), who overthrew the last Zhou emperor and established the Qin dynasty. The Qin dynasty (Chinese: \u79e6\u671d; pinyin: Q\u00edn Ch\u00e1o) was the first ruling dynasty of Imperial China, lasting from 221 to 207 BC. The new Emperor abolished the feudal system and directly appointed a bureaucracy that would rely on him for power. Huang's imperial forces crushed any regional resistance, and they furthered the Chinese empire by expanding down to the South China Sea and northern Vietnam. Greater organization brought a uniform tax system, a national census, regulated road building (and cart width), standard measurements, standard coinage, and an official written and spoken language. Further reforms included new irrigation projects, the encouragement of silk manufacturing, and (most famously) the beginning of the construction of the Great Wall of China\u2014designed to keep out the nomadic raiders who'd constantly badger the Chinese people. However, Shi Huang was infamous for his tyranny, forcing laborers to build the Wall, ordering heavy taxes, and severely punishing all who opposed him. He oppressed Confucians and promoted Legalism, the idea that people were inherently evil, and that a strong, forceful government was needed to control them. Legalism was infused with realistic, logical views and rejected the pleasures of educated conversation as frivolous. All of this made Shi Huang extremely unpopular with the people. As the Qin began to weaken, various factions began to fight for control of China.\nHan dynasty.\nThe Han dynasty (; 206 BC \u2013 220 AD) was the second imperial dynasty of China, preceded by the Qin dynasty and succeeded by the Three Kingdoms (220\u2013265 AD). Spanning over four centuries, the period of the Han dynasty is considered a golden age in Chinese history. One of the Han dynasty's greatest emperors, Emperor Wu of Han, established a peace throughout China comparable to the Pax Romana seen in the Mediterranean a hundred years later. To this day, China's majority ethnic group refers to itself as the \"Han people\". The Han dynasty was established when two peasants succeeded in rising up against Shi Huang's significantly weaker successor-son. The new Han government retained the centralization and bureaucracy of the Qin, but greatly reduced the repression seen before. They expanded their territory into Korea, Vietnam, and Central Asia, creating an even larger empire than the Qin.\nThe Han developed contacts with the Persian Empire in the Middle East and the Romans, through the Silk Road, with which they were able to trade many commodities\u2014primarily silk. Many ancient civilizations were influenced by the Silk Road, which connected China, India, the Middle East and Europe. Han emperors like Wu also promoted Confucianism as the national \"religion\" (although it is debated by theologians as to whether it is defined as such or as a philosophy). Shrines devoted to Confucius were built and Confucian philosophy was taught to all scholars who entered the Chinese bureaucracy. The bureaucracy was further improved with the introduction of an examination system that selected scholars of high merit. These bureaucrats were often upper-class people educated in special schools, but whose power was often checked by the lower-class brought into the bureaucracy through their skill. The Chinese imperial bureaucracy was very effective and highly respected by all in the realm and would last over 2,000 years. The Han government was highly organized and it commanded the military, judicial law (which used a system of courts and strict laws), agricultural production, the economy, and the general lives of its people. The government also promoted intellectual philosophy, scientific research, and detailed historical records.\nHowever, despite all of this impressive stability, central power began to lose control by the turn of the Common Era. As the Han dynasty declined, many factors continued to pummel it into submission until China was left in a state of chaos. By 100 AD, philosophical activity slowed, and corruption ran rampant in the bureaucracy. Local landlords began to take control as the scholars neglected their duties, and this resulted in heavy taxation of the peasantry. Taoists began to gain significant ground and protested the decline. They started to proclaim magical powers and promised to save China with them; the Taoist Yellow Turban Rebellion in 184 (led by rebels in yellow scarves) failed but was able to weaken the government. The aforementioned Huns combined with diseases killed up to half of the population and officially ended the Han dynasty by 220. The ensuing period of chaos was so terrible it lasted for three centuries, where many weak regional rulers and dynasties failed to establish order in China. This period of chaos and attempts at order is commonly known as that of the Six Dynasties. The first part of this included the Three Kingdoms which started in 220 and describes the brief and weak successor \"dynasties\" that followed the Han. In 265, the Jin dynasty of China was started and this soon split into two different empires in control of northwestern and southeastern China. In 420, the conquest and abdication of those two dynasties resulted in the first of the Southern and Northern dynasties. The Northern and Southern dynasties passed through until finally, by 557, the Northern Zhou dynasty ruled the north and the Chen dynasty ruled the south.\nMedieval.\nDuring this period, the Eastern world empires continued to expand through trade, migration and conquests of neighboring areas. Gunpowder was widely used as early as the 11th century and they were using moveable type printing five hundred years before Gutenberg created his press. Buddhism, Taoism, Confucianism were the dominant philosophies of the Far East during the Middle Ages. Marco Polo was not the first Westerner to travel to the Orient and return with amazing stories of this different culture, but his accounts published in the late 13th and early 14th centuries were the first to be widely read throughout Europe.\nWestern Asia (Middle East).\nThe Arabian peninsula and the surrounding Middle East and Near East regions saw dramatic change during the Medieval era caused primarily by the spread of Islam and the establishment of the Arabian Empires.\nIn the 5th century, the Middle East was separated into small, weak states; the two most prominent were the Sassanian Empire of the Persians in what is now Iran and Iraq, and the Byzantine Empire in Anatolia (modern-day Turkey). The Byzantines and Sassanians fought with each other continually, a reflection of the rivalry between the Roman Empire and the Persian Empire seen during the previous five hundred years. The fighting weakened both states, leaving the stage open to a new power. Meanwhile, the nomadic Bedouin tribes who dominated the Arabian desert saw a period of tribal stability, greater trade networking and a familiarity with Abrahamic religions or monotheism.\nWhile the Byzantine Roman and Sassanid Persian empires were both weakened by the Byzantine\u2013Sasanian War of 602\u2013628, a new power in the form of Islam grew in the Middle East under Muhammad in Medina. In a series of rapid Muslim conquests, the Rashidun army, led by the Caliphs and skilled military commanders such as Khalid ibn al-Walid, swept through most of the Middle East, taking more than half of Byzantine territory in the Arab\u2013Byzantine wars and completely engulfing Persia in the Muslim conquest of Persia. It would be the Arab Caliphates of the Middle Ages that would first unify the entire Middle East as a distinct region and create the dominant ethnic identity that persists today. These Caliphates included the Rashidun Caliphate, Umayyad Caliphate, Abbasid Caliphate, and later the Seljuq Empire.\nAfter Muhammad introduced Islam, it jump-started Middle Eastern culture into an Islamic Golden Age, inspiring achievements in architecture, the revival of old advances in science and technology, and the formation of a distinct way of life. Muslims saved and spread Greek advances in medicine, algebra, geometry, astronomy, anatomy, and ethics that would later find their way back to Western Europe.\nThe dominance of the Arabs came to a sudden end in the mid-11th century with the arrival of the Seljuq Turks, migrating south from the Turkic homelands in Central Asia. They conquered Persia, Iraq (capturing Baghdad in 1055), Syria, Palestine, and the Hejaz. This was followed by a series of Christian Western Europe invasions. The fragmentation of the Middle East allowed joined forces, mainly from England, France, and the emerging Holy Roman Empire, to enter the region. In 1099 the knights of the First Crusade captured Jerusalem and founded the Kingdom of Jerusalem, which survived until 1187, when Saladin retook the city. Smaller crusader fiefdoms survived until 1291. In the early 13th century, a new wave of invaders, the armies of the Mongol Empire, swept through the region, sacking Baghdad in the Siege of Baghdad (1258) and advancing as far south as the border of Egypt in what became known as the Mongol conquests. The Mongols eventually retreated in 1335, but the chaos that ensued throughout the empire deposed the Seljuq Turks. In 1401, the region was further plagued by the Turko-Mongol, Timur, and his ferocious raids. By then, another group of Turks had arisen as well, the Ottomans.\nCentral Asia.\nMongol Empire.\nThe Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Medieval Asia was the kingdom of the Khans. Never before had any person controlled as much land as Genghis Khan. He built his power unifying separate Mongol tribes before expanding his kingdom south and west. He and his grandson, Kublai Khan, controlled lands in China, Burma, Central Asia, Russia, Iran, the Middle East, and Eastern Europe. Genghis Khan was a Khagan who tolerated nearly every religion.\nSouth Asia/Indian Subcontinent.\nIndia.\nThe Indian early medieval age, 600 to 1200, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. The Cholas could under the rule of Raja Raja Chola defeat their rivals and rise to a regional power. Cholas expanded northward and defeated Eastern Chalukya, Kalinga and the Pala. Under Rajendra Chola the Cholas created the first notable navy of Indian subcontinent. The Chola navy extended the influence of Chola empire to southeast asia. During this time, pastoral peoples whose land had been cleared to make way for the growing agricultural economy were accommodated within caste society, as were new non-traditional ruling classes.\nThe Muslim conquest in the Indian subcontinent mainly took place from the 12th century onwards, though earlier Muslim conquests include the limited inroads into modern Afghanistan and Pakistan and the Umayyad campaigns in India, during the time of the Rajput kingdoms in the 8th century.\nMajor economic and military powers like the Delhi Sultanate and Bengal Sultanate, were seen to be established. The search of their wealth led the Voyages of Christopher Columbus.\nSher Shah Suri of the Sur Empire of India introduced the standardised r\u016bpya coin in the 16th century now used as the currency of Indonesia, India, Pakistan, Bangladesh, Sri Lanka and Nepal. \nThe Vijayanagara Empire based in the Deccan Plateau region of South India, was established in 1336 by the brothers Harihara I and Bukka Raya I of the Sangama dynasty, patronized by saint Vidyaranya, the 12th Shankaracharya of Sringeri in Karnataka. The empire rose to prominence as a result of attempts by the southern powers to resist and ward off Turkic Islamic invasions by the end of the 13th century. At its peak, it subjugated almost all of South India's rulers and pushed the sultans of the Deccan beyond the Tungabhadra-Krishna river region. After annexing modern day Odisha (ancient Kalinga) from the Gajapati Empire, became a notable power. The empire lasted until 1646 after a major military defeat in the Battle of Talikota in 1565 by the combined armies of the Deccan sultanates.\nEast Asia.\nChina.\nChina saw the rise and fall of the Sui, Tang, Song, and Yuan dynasties and therefore improvements in its bureaucracy, the spread of Buddhism, and the advent of Neo-Confucianism. It was an unsurpassed era for Chinese ceramics and painting. Medieval architectural masterpieces the Great South Gate in Todaiji, Japan, and the Tien-ning Temple in Peking, China are some of the surviving constructs from this era.\nSui dynasty.\nA new powerful dynasty began to rise in the 580s, amongst the divided factions of China. This was started when an aristocrat named Yang Jian married his daughter into the Northern Zhou dynasty. He proclaimed himself Emperor Wen of Sui and appeased the nomadic military by abandoning the Confucian scholar-gentry. Emperor Wen soon led the conquest of the southern Chen dynasty and united China once more under the Sui dynasty. The emperor lowered taxes and constructed granaries that he used to prevent famine and control the market. Later Wen's son would murder him for the throne and declare himself Emperor Yang of Sui. Emperor Yang revived the Confucian scholars and the bureaucracy, much to anger of the aristocrats and nomadic military leaders. Yang became an excessive leader who overused China's resources for personal luxury and perpetuated exhaustive attempts to conquer Goguryeo. His military failures and neglect of the empire forced his own ministers to assassinate him in 618, ending the Sui dynasty.\nTang dynasty.\nFortunately, one of Yang's most respectable advisors, Li Yuan, was able to claim the throne quickly, preventing a chaotic collapse. He proclaimed himself Emperor Gaozu, and established the Tang dynasty in 623. The Tang saw expansion of China through conquest to Tibet in the west, Vietnam in the south, and Manchuria in the north. Tang emperors also improved the education of scholars in the Chinese bureaucracy. A Ministry of Rites was established and the examination system was improved to better qualify scholars for their jobs. In addition, Buddhism became popular in China with two different strains between the peasantry and the elite, the Pure Land and Zen strains, respectively. Greatly supporting the spread of Buddhism was Empress Wu, who additionally claimed an unofficial \"Zhou dynasty\" and displayed China's tolerance of a woman ruler, which was rare at the time. However, Buddhism would also experience some backlash, especially from Confucianists and Taoists. This would usually involve criticism about how it was costing the state money, since the government was unable to tax Buddhist monasteries, and additionally sent many grants and gifts to them.\nThe Tang dynasty began to decline under the rule of Emperor Xuanzong, who began to neglect the economy and military and caused unrest amongst the court officials due to the excessive influence of his concubine, Yang Guifei, and her family. This eventually sparked a revolt in 755. Although the revolt failed, subduing it required involvement with the unruly nomadic tribes outside of China and distributing more power to local leaders\u2014leaving the government and economy in a degraded state. The Tang dynasty officially ended in 907 and various factions led by the aforementioned nomadic tribes and local leaders would fight for control of China in the Five Dynasties and Ten Kingdoms period.\nLiao, Song and Jin dynasties.\nBy 960, most of China proper had been reunited under the Song dynasty, although it lost territories in the north and could not defeat one of the nomadic tribes there\u2014the Liao dynasty of the highly sinicized Khitan people. From then on, the Song would have to pay tribute to avoid invasion and thus set the precedent for other nomadic kingdoms to oppress them. The Song also saw the revival of Confucianism in the form of Neo-Confucianism. This had the effect of putting the Confucian scholars at a higher status than aristocrats or Buddhists and also intensified the reduction of power in women. The infamous practice of foot binding developed in this period as a result. Eventually the Liao dynasty in the north was overthrown by the Jin dynasty of the Manchu-related Jurchen people. The new Jin kingdom invaded northern China, leaving the Song to flee farther south and creating the Southern Song dynasty in 1126. There, cultural life flourished.\nYuan dynasty.\n \nBy 1227, the Mongols had conquered the Western Xia kingdom northwest of China. Soon the Mongols incurred upon the Jin empire of the Jurchens. Chinese cities were soon besieged by the Mongol hordes that showed little mercy for those who resisted and the Southern Song Chinese were quickly losing territory. In 1271 the current great khan, Kublai Khan, claimed himself Emperor of China and officially established the Yuan dynasty. By 1290, all of China was under control of the Mongols, marking the first time they were ever completely conquered by a foreign invader; the new capital was established at Khanbaliq (modern-day Beijing). Kublai Khan segregated Mongol culture from Chinese culture by discouraging interactions between the two peoples, separating living spaces and places of worship, and reserving top administrative positions to Mongols, thus preventing Confucian scholars to continue the bureaucratic system. Nevertheless, Kublai remained fascinated with Chinese thinking, surrounding himself with Chinese Buddhist, Taoist, or Confucian advisors.\nMongol women displayed a contrasting independent nature compared to the Chinese women who continued to be suppressed. Mongol women often rode out on hunts or even to war. Kublai's wife, Chabi, was a perfect example of this; Chabi advised her husband on several political and diplomatic matters; she convinced him that the Chinese were to be respected and well-treated in order to make them easier to rule. However, this was not enough to affect Chinese women's position, and the increasingly Neo-Confucian successors of Kublai further repressed Chinese and even Mongol women.\nThe Black Death, which would later ravage Western Europe, had its beginnings in Asia, where it wiped out large populations in China in 1331.\nJapan.\nAsuka period.\nJapan's medieval history began with the Asuka period, from around 600 to 710. The time was characterized by the Taika Reform and imperial centralization, both of which were a direct result of growing Chinese contact and influences. In 603, Prince Sh\u014dtoku of the Yamato dynasty began significant political and cultural changes. He issued the Seventeen-article constitution in 604, centralizing power towards the emperor (under the title \"tenno\", or heavenly sovereign) and removing the power to levy taxes from provincial lords. Sh\u014dtoku was also a patron of Buddhism and he encouraged building temples competitively.\nNara period.\nSh\u014dtoku's reforms transitioned Japan to the Nara period (c. 710 to c. 794), with the moving of the Japanese capital to Nara in Honshu. This period saw the culmination of Chinese-style writing, etiquette, and architecture in Japan along with Confucian ideals to supplement the already present Buddhism. Peasants revered both Confucian scholars and Buddhist monks. However, in the wake of the 735\u2013737 Japanese smallpox epidemic, Buddhism gained the status of state religion and the government ordered the construction of numerous Buddhist temples, monasteries, and statues. The lavish spending combined with the fact that many aristocrats did not pay taxes, put a heavy burden on peasantry that caused poverty and famine. Eventually the Buddhist position got out of control, threatening to seize imperial power and causing Emperor Kanmu to move the capital to Heian-ky\u014d to avoid a Buddhist takeover. This marked the beginning of the Heian period and the end of Taika reform.\nHeian period.\nWith the Heian period (from 794 to 1185) came a decline of imperial power. Chinese influence also declined, as a result of its correlation with imperial centralization and the heavenly mandate, which came to be regarded as ineffective. By 838, the Japanese court discontinued its embassies in China; only traders and Buddhist monks continued to travel to China. Buddhism itself came to be considered more Japanese than Chinese, and persisted to be popular in Japan. Buddhists monks and monasteries continued their attempts to gather personal power in courts, along with aristocrats. One particular noble family that dominated influence in the imperial bureaucracy was the Fujiwara clan. During this time cultural life in the imperial court flourished. There was a focus on beauty and social interaction and writing and literature was considered refined. Noblewomen were cultured the same as noblemen, dabbling in creative works and politics. A prime example of both Japanese literature and women's role in high-class culture at this time was \"The Tale of Genji\", written by the lady-in-waiting Murasaki Shikibu. Popularization of wooden palaces and sh\u014dji sliding doors amongst the nobility also occurred.\nLoss of imperial power also led to the rise of provincial warrior elites. Small lords began to function independently. They administered laws, supervised public works projects, and collected revenue for themselves instead of the imperial court. Regional lords also began to build their own armies. These warriors were loyal only their local lords and not the emperor, although the imperial government increasingly called them in to protect the capital. The regional warrior class developed into the samurai, which created its own culture: including specialized weapons such as the katana and a form of chivalry, bushido. The imperial government's loss of control in the second half of the Heian period allowed banditry to grow, requiring both feudal lords and Buddhist monasteries to procure warriors for protection. As imperial control over Japan declined, feudal lords also became more independent and seceded from the empire. These feudal states squandered the peasants living in them, reducing the farmers to an almost serfdom status. Peasants were also rigidly restricted from rising to the samurai class, being physically set off by dress and weapon restrictions. As a result of their oppression, many peasants turned to Buddhism as a hope for reward in the afterlife for upright behavior.\nWith the increase of feudalism, families in the imperial court began to depend on alliances with regional lords. The Fujiwara clan declined from power, replaced by a rivalry between the Taira clan and the Minamoto clan. This rivalry grew into the Genpei War in the early 1180s. This war saw the use of both samurai and peasant soldiers. For the samurai, battle was ritual and they often easily cut down the poorly trained peasantry. The Minamoto clan proved successful due to their rural alliances. Once the Taira was destroyed, the Minamoto established a military government called the shogunate (or bakufu), centered in Kamakura.\nKamakura period.\nThe end of the Genpei War and the establishment of the Kamakura shogunate marked the end of the Heian period and the beginning of the Kamakura period in 1185, solidifying feudal Japan.\nKorea.\nThree Kingdoms of Korea.\nThe three Kingdoms of Korea involves Goguryeo in north, Baekje in southwest, and Silla in southeast Korean peninsula. These three kingdoms act as a bridge of cultures between China and Japan. Prince Sh\u014dtoku of Japan had been taught by two teachers. One was from Baekje, the other was from Goguryeo. Once Japan invaded Silla, Goguryeo helped Silla to defeat Japan. Baekje met the earliest heyday of them. Its heyday was the 5th century AD. Its capital was Seoul. During its heyday, the kingdom made colonies overseas. Liaodong, China and Kyushu, Japan were the colonies of Baekje during its short heyday. Goguryeo was the strongest kingdom of all. They sometimes called themselves as an Empire. Its heyday was 6th century. King Gwanggaeto widened its territory to north. So Goguryeo dominated from Korean peninsula to Manchuria. And his son, King Jangsu widened its territory to south. He occupied Seoul, and moved its capital to Pyeongyang. Goguryeo almost occupied three quarters of South Korean peninsula thanks to king Jangsu who widened the kingdom's territory to south. Silla met the latest heyday. King Jinheung went north and occupiedSeoul. But it was short. Baekje became stronger and attacked Silla. Baekje occupied more than 40 cities of Silla. So Silla could hardly survive.\nChina's Sui dynasty invaded Goguryeo and Goguryeo\u2013Sui War occurred between Korea and China. Goguryeo won against China and Sui dynasty fell. After then, Tang dynasty reinvaded Goguryeo and helped Silla to unify the peninsula. Goguryeo, Baekje, and Japan helped each other against Tang-Silla alliance, but Baekje and Goguryeo fell. Unfortunately, Tang dynasty betrayed Silla and invaded Korean peninsula in order to occupy the whole Korean peninsula (Silla-Tang war). Silla advocated 'Unification of Three Korea', so people of fallen Baekje and Goguryeo helped Silla against Chinese invasion. Eventually Silla could beat China and unified the peninsula. This war helped Korean people to unite mentally.\nNorth-South States Period.\nThe rest of Goguryeo people established Balhae and won the war against Tang in later 7th century AD. Balhae is the north state, and Later Silla was the south state. Balhae was a quite strong kingdom as their ancestor Goguryeo did. Finally, the Emperor of Tang dynasty admits Balhae as 'A strong country in the East'. They liked to trade with Japan, China, and Silla. Balhae and Later Silla sent a lot of international students to China. And Arabian merchants came into Korean peninsula, so Korea became known as 'Silla' in the western countries. Silla improved Korean writing system called Idu letters. Idu affected Katakana of Japan. Liao dynasty invaded Balhae in early 10th century, so Balhae fell.\nLater Three Kingdoms of Korea.\nThe unified Korean kingdom, Later Silla divided into three kingdoms again because of the corrupt central government. It involves Later Goguryeo (also as known as \"Taebong\"), Later Baekje, and Later Silla. The general of Later Goguryeo, Wang Geon took the throne and changed the name of kingdom into Goryeo, which was derived by the ancient strong kingdom, Goguryeo, and Goryeo reunified the peninsula.\nGoryeo.\nGoryeo reunited the Korean peninsula during the later three kingdoms period and named itself as 'Empire'. But nowadays, Goryeo is known as a kingdom. The name 'Goryeo' was derived from Goguryeo, and the name Korea was derived from Goryeo. Goryeo adopted people from fallen Balhae. They also widened their territory to north by defending Liao dynasty and attacking the Jurchen people. Goryeo developed a splendid culture. The first metal type printed book Jikji was also from Korea. The Goryeo ware is one of the most famous legacies of this kingdom. Goryeo imported Chinese government system and developed into their own ways.\nDuring this period, laws were codified and a civil service system was introduced. Buddhism flourished and spread throughout the peninsula. The Tripitaka Koreana is 81,258 books total. It was made to keep Korea safe against the Mongolian invasion. It is now a UNESCO world heritage. Goryeo won the battle against Liao dynasty. Then, the Mongolian Empire invaded Goryeo. Goryeo did not disappear but it had to obey Mongolians. After 80 years, in 14th century, the Mongolian dynasty Yuan lost power, King Gongmin tried to free themselves against Mongol although his wife was also Mongolian. At the 14th century, Ming dynasty wanted Goryeo to obey China. But Goryeo didn't. They decided to invade China. Going to China, the general of Goryeo, Lee Sung-Gae came back and destroyed Goryeo. Then, in 1392, he established new dynasty, Joseon. And he became Taejo of Joseon, which means the first king of Joseon.\nSoutheast Asia.\nKhmers.\nIn 802, Jayavarman II consolidated his rule over neighboring peoples and declared himself chakravartin, or \"universal ruler\". The Khmer Empire effectively dominated all Mainland Southeast Asia from the early 9th until the 15th century, during which time they developed a sophisticated monumental architecture of most exquisite expression and mastery of composition at Angkor.\nVietnam.\nThe history of Vietnam can be traced back to around 20,000 years ago, as the first modern humans arrived and settled on this land, known as the Hoabinhians, which can be traced back to the modern-day Negritos. Archaeological findings from 1965, which are still under research, show the remains of two hominins closely related to the Sinanthropus, dating as far back as the Middle Pleistocene era, roughly half a million years ago.\nPre-historic Vietnam was home to some of the world's earliest civilizations and societies\u2014making them one of the world's first people who had practiced agriculture. The Red River valley formed a natural geographic and economic unit, bounded to the north and west by mountains and jungles, to the east by the sea and to the south by the Red River Delta. The need to have a single authority to prevent floods of the Red River, to cooperate in constructing hydraulic systems, trade exchange, and to repel invaders, led to the creation of the first legendary Vietnamese states approximately 2879 BC. While in the later times, ongoing research from archaeologists have suggested that the Vietnamese \u0110\u00f4ng S\u01a1n culture were traceable back to Northern Vietnam, Guangxi and Laos around 700 BC.\nVietnam's long coastal and narrowed lands, rugged mountainous terrains, with two major deltas, were soon home to several different ancient cultures and civilizations. In the north, the \u0110\u00f4ng S\u01a1n culture and its indigenous chiefdoms of V\u0103n Lang and \u00c2u L\u1ea1c started to flourish by 500 BC. In Central, Sa Hu\u1ef3nh culture of Austronesian Chamic peoples also thrived. Both were swept by the Chinese Han dynasty expansion from the north - the Han conquest of Nanyue brought parts of Vietnam under the Chinese rule in 111 BC. Traditional Chinese became the official script as well as the later developed independent N\u00f4m script of Vietnamese.\nIn 40 AD, the Tr\u01b0ng Sisters led the first uprising of indigenous tribes and peoples against Chinese domination. The rebellion was however defeated, but as the Han dynasty began to weaken by late 2nd century and China (\u4e2d\u56fd) started to descend into state of turmoil, the indigenous peoples of Vietnam rose again and some became free. In 192 AD, the Chams of Central Vietnam revolted against the Chinese and subsequently became independent Kingdom of Champa, while the Red River Delta saw loosening Northern control. At that time, with the introduction of Buddhism and Hinduism by the second century AD, Vietnam was the first place in Southeast Asia which shared influences of both Indian and Sino cultures, and the rise of first Indianized kingdoms Champa and Funan.\nDuring these 1,000 years there were many uprisings against Chinese domination, and at certain periods Vietnam was independently governed under the Tr\u01b0ng Sisters, Early L\u00fd, Kh\u00fac and D\u01b0\u01a1ng \u0110\u00ecnh Ngh\u1ec7\u2014although their triumphs and reigns were temporary.\nWhen Ng\u00f4 Quy\u1ec1n (Emperor of Vietnam, 938\u2013944) restored sovereign power in the country with the victory at The battle of B\u1ea1ch \u0110\u1eb1ng River (938), the next millennium was advanced by the accomplishments of successive local dynasties: Ng\u00f4, \u0110inh, Early L\u00ea, L\u00fd, Tr\u1ea7n, H\u1ed3, Later Tr\u1ea7n, Later L\u00ea, M\u1ea1c, Revival L\u00ea, T\u00e2y S\u01a1n and Nguy\u1ec5n. N\u00f4m script (Ch\u1eef N\u00f4m) of the Vietnamese started to develop and become more sophisticated, with literature being published and written in N\u00f4m. At various points during the imperial dynasties, Vietnam was ravaged and divided by civil wars and witnessed interventions by the Song, Yuan, Cham, Ming, Siamese, Qing, French, and Empire of Japan.\nThe Ming Empire conquered the Red River valley for a while before native Vietnamese regained control and the French Empire reduced Vietnam to a French dependency for nearly a century, followed by brief but brutal occupation by the Japanese Empire. During the French period, widespread brutality, inequality and cultural remnants of H\u00e1n-N\u00f4m were being destroyed, with the French wishing to rid the Vietnamese of their Confucian legacy from the 1880s. French was the official language during this period. The Vietnamese Latin script, seen to be a Latin transliteration of H\u00e1n-N\u00f4m, superseded the H\u00e1n-N\u00f4m logographic scripts and became the main mode of written as well as spoken language since the 20th century.\nJapan invaded in 1940, creating deep resentment that fuelled resistance to post-World War II military-political efforts by the returning power of France, and the United States who had viewed themselves as fighters for liberty and democracy against the red waves of communism. In the Vietnam War, the United States or the Western Bloc supported South Vietnam and the Soviet Union or the Eastern Bloc supported North Vietnam. Political upheaval, a period of intense fighting and war, followed by Communist insurrection and victory further put an end to the monarchy after World War II, and the country was proclaimed a Socialist Republic. Vietnam suffered heavy sanctions as well as political and economic isolation following brutal wars with China and Cambodia in the successive years. Following that era, the \u0110\u1ed5i M\u1edbi (renovation/innovation) reformations were enacted. The forces of market liberalisation and globalisation has shaped Vietnam's economic and political circumstances since.\nEarly modern.\nThe Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing dynasty. In the 16th century, the Mughal Empire controlled much of India and initiated the second golden age for India. China was the largest economy in the world for much of the time, followed by India until the 18th century.\nMing China.\nBy 1368, Zhu Yuanzhang had claimed himself Hongwu Emperor and established the Ming dynasty of China. Immediately, the new emperor and his followers drove the Mongols and their culture out of China and beyond the Great Wall. The new emperor was somewhat suspicious of the scholars that dominated China's bureaucracy, for he had been born a peasant and was uneducated. Nevertheless, Confucian scholars were necessary to China's bureaucracy and were reestablished as well as reforms that would improve the exam systems and make them more important in entering the bureaucracy than ever before. The exams became more rigorous, cut down harshly on cheating, and those who excelled were more highly appraised. Finally, Hongwu also directed more power towards the role of emperor so as to end the corrupt influences of the bureaucrats.\nSociety and economy.\nThe Hongwu emperor, perhaps for his sympathy of the common-folk, had built many irrigation systems and other public projects that provided help for the peasant farmers. They were also allowed to cultivate and claim unoccupied land without having to pay any taxes and labor demands were lowered. However, none of this was able to stop the rising landlord class that gained many privileges from the government and slowly gained control of the peasantry. Moneylenders foreclosed on peasant debt in exchange for mortgages and bought up farmer land, forcing them to become the landlords' tenants or to wander elsewhere for work. Also during this time, Neo-Confucianism intensified even more than the previous two dynasties (the Song and Yuan). Focus on the superiority of elders over youth, men over women, and teachers over students resulted in minor discrimination of the \"inferior\" classes. The fine arts grew in the Ming era, with improved techniques in brush painting that depicted scenes of court, city or country life; people such as scholars or travelers; or the beauty of mountains, lakes, or marshes. The Chinese novel fully developed in this era, with such classics written such as \"Water Margin\", \"Journey to the West\", and \"Jin Ping Mei\".\nEconomics grew rapidly in the Ming dynasty as well. The introduction of American crops such as maize, sweet potatoes, and peanuts allowed for cultivation of crops in infertile land and helped prevent famine. The population boom that began in the Song dynasty accelerated until China's population went from 80 or 90 million to 150 million in three centuries, culminating in 1600. This paralleled the market economy that was growing both internally and externally. Silk, tea, ceramics, and lacquer-ware were produced by artisans that traded them in Asia and to Europeans. Westerners began to trade (with some Chinese-assigned limits), primarily in the port-towns of Macau and Canton. Although merchants benefited greatly from this, land remained the primary symbol of wealth in China and traders' riches were often put into acquiring more land. Therefore, little of these riches were used in private enterprises that could've allowed for China to develop the market economy that often accompanied the highly-successful Western countries.\nForeign interests.\nIn the interest of national glory, the Chinese began sending impressive junk ships across the South China Sea and the Indian Ocean. From 1403 to 1433, the Yongle Emperor commissioned expeditions led by the admiral Zheng He, a Muslim eunuch from China. Chinese junks carrying hundreds of soldiers, goods, and animals for zoos, traveled to Southeast Asia, Persia, southern Arabia, and east Africa to show off Chinese power. Their prowess exceeded that of current Europeans at the time, and had these expeditions not ended, the world economy may be different from today. In 1433, the Chinese government decided that the cost of a navy was an unnecessary expense. The Chinese navy was slowly dismantled and focus on interior reform and military defense began. It was China's longstanding priority that they protect themselves from nomads and they have accordingly returned to it. The growing limits on the Chinese navy would leave them vulnerable to foreign invasion by sea later on.\nAs was inevitable, Westerners arrived on the Chinese east coast, primarily Jesuit missionaries which reached the mainland in 1582. They attempted to convert the Chinese people to Christianity by first converting the top of the social hierarchy and allowing the lower classes to subsequently convert. To further gain support, many Jesuits adopted Chinese dress, customs, and language. Some Chinese scholars were interested in certain Western teachings and especially in Western technology. By the 1580s, Jesuit scholars like Matteo Ricci and Adam Schall amazed the Chinese elite with technological advances such as European clocks, improved calendars and cannons, and the accurate prediction of eclipses. Although some the scholar-gentry converted, many were suspicious of the Westerners whom they called \"barbarians\" and even resented them for the embarrassment they received at the hand of Western correction. Nevertheless, a small group of Jesuit scholars remained at the court to impress the emperor and his advisors.\nDecline.\nNear the end of the 1500s, the extremely centralized government that gave so much power to the emperor had begun to fail as more incompetent rulers took the mantle. Along with these weak rulers came increasingly corrupt officials who took advantage of the decline. Once more the public projects fell into disrepair due to neglect by the bureaucracy and resulted in floods, drought, and famine that rocked the peasantry. The famine soon became so terrible that some peasants resorted to selling their children to slavery to save them from starvation, or to eating bark, the feces of geese, or other people. Many landlords abused the situation by building large estates where desperate farmers would work and be exploited. In turn, many of these farmers resorted to flight, banditry, and open rebellion.\nAll of this corresponded with the usual dynastic decline of China seen before, as well as the growing foreign threats. In the mid-16th century, Japanese and ethnic Chinese pirates began to raid the southern coast, and neither the bureaucracy nor the military were able to stop them. The threat of the northern Manchu people also grew. The Manchu were an already large state north of China, when in the early 17th century a local leader named Nurhaci suddenly united them under the Eight Banners\u2014armies that the opposing families were organized into. The Manchus adopted many Chinese customs, specifically taking after their bureaucracy. Nevertheless, the Manchus still remained a Chinese vassal. In 1644 Chinese administration became so weak, the 16th and last emperor, the Chongzhen Emperor, did not respond to the severity of an ensuing rebellion by local dissenters until the enemy had invaded the Forbidden City (his personal estate). He soon hanged himself in the imperial gardens. For a brief amount of time, the Shun dynasty was claimed, until a loyalist Ming official called support from the Manchus to put down the new dynasty. The Shun dynasty ended within a year and the Manchu were now within the Great Wall. Taking advantage of the situation, the Manchus marched on the Chinese capital of Beijing. Within two decades all of China belonged to the Manchu and the Qing dynasty was established.\nKorea: Joseon dynasty (1392\u20131897).\nIn early-modern Korea, the 500-year-old kingdom, Goryeo fell and new dynasty Joseon rose in August 5, 1392. Taejo of Joseon changed the country's name from Goryeo to Joseon. Sejong the Great created Hangul, the modern Korean alphabet, in 1443; likewise the Joseon dynasty saw several improvements in science and technology, like Sun Clocks, Water Clocks, Rain-Measuring systems, Star Maps, and detailed records of Korean small villages. The ninth king, Seongjong accomplished the first complete Korean law code in 1485. So the culture and people's lives were improved again.\nIn 1592, Japan under Toyotomi Hideyoshi invaded Korea. That war is Imjin war. Before that war, Joseon was in a long peace like PAX ROMANA. So Joseon was not ready for the war. Joseon had lost again and again. Japanese army conquered Seoul. The whole Korean peninsula was in danger. But Yi Sun-sin, the most renowned general of Korea, defeated Japanese fleet in southern Korea coast even 13 ships VS 133 ships. This incredible battle is called \"Battle of Myeongnyang\". After that, Ming dynasty helped Joseon, and Japan lost the battle. So Toyotomi Hideyoshi's campaign in Korea failed, and the Tokugawa Shogunate has later began. Korea was hurt a lot at Imjin war. Not long after, Manchurian people invaded Joseon again. It is called Qing invasion of Joseon. The first invasion was for sake. Because Qing was at war between Ming, so Ming's alliance with Joseon was threatening. And the second invasion was for Joseon to obey Qing. After that, Qing defeated Ming and took the whole Chinese territories. Joseon also had to obey Qing because Joseon lose the second war against Qing.\nAfter the Qing invasion, the princes of the Joseon dynasty lived their childhood in China. The son of King Injo met Adam Schall in Beijing. So he wanted to introduce western technologies to Korean people when he becomes a king. He died before he could take the throne. After then, the alternative prince became the 17th king of the Joseon dynasty, Hyojong, trying to revenge for his kingdom and fallen Ming dynasty to Qing. Later kings such as Yeongjo and Jeongjo tried to improve their people's lives and stop the governors' unreasonable competition. From the 17th century to the 18th century, Joseon sent diplomats and artists to Japan more than 10 times. This group was called 'Tongshinsa'. They were sent to Japan to teach Japan about advanced Korean culture. Japanese people liked to receive poems from Korean nobles. At that time, Korea was more powerful than Japan. But that relationship between Joseon and Japan was reversed after the 19th century. Because Japan became more powerful than Korea and China, either. So Joseon sent diplomats called 'Sooshinsa' to learn Japanese advanced technologies. After king Jeongjo's death, some noble families controlled the whole kingdom in the early 19th century. At the end of that period, Western people invaded Joseon. In 1876, Joseon was set free from Qing so they did not have to obey Qing. But Japanese Empire was happy because Joseon became a perfect independent kingdom. So Japan could intervene in the kingdom more. After this, Joseon traded with the United States and sent 'Sooshinsa' to Japan, 'Youngshinsa' to Qing, and 'Bobingsa' to the US and Europe. These groups took many modern things to the Korean peninsula.\nJapan: Tokugawa or Edo period (1603\u20131867).\nIn early-modern Japan following the Sengoku period of \"warring states\", central government had been largely reestablished by Oda Nobunaga and Toyotomi Hideyoshi during the Azuchi\u2013Momoyama period. After the Battle of Sekigahara in 1600, central authority fell to Tokugawa Ieyasu who completed this process and received the title of \"sh\u014dgun\" in 1603.\nSociety in the Japanese \"Tokugawa period\" (see Edo society), unlike the shogunates before it, was based on the strict class hierarchy originally established by Toyotomi Hideyoshi. The \"daimy\u014ds\" (feudal lords) were at the top, followed by the warrior-caste of samurai, with the farmers, artisans, and merchants ranking below. The country was strictly closed to foreigners with few exceptions with the \"Sakoku\" policy. Literacy rose in the two centuries of isolation.\nIn some parts of the country, particularly smaller regions, \"daimy\u014ds\" and samurai were more or less identical, since \"daimy\u014ds\" might be trained as samurai, and samurai might act as local lords. Otherwise, the largely inflexible nature of this social stratification system unleashed disruptive forces over time. Taxes on the peasantry were set at fixed amounts which did not account for inflation or other changes in monetary value. As a result, the tax revenues collected by the samurai landowners were worth less and less over time. This often led to numerous confrontations between noble but impoverished samurai and well-to-do peasants. None, however, proved compelling enough to seriously challenge the established order until the arrival of foreign powers.\nIndia.\nIn the Indian subcontinent, the Mughal Empire ruled most of India in the early 18th century. During emperor Shah Jahan and his son Aurangzeb's Islamic sharia reigns, the empire reached its architectural and economic zenith, and became the world's largest economy, worth over 25% of world GDP. In the mid-18th century it was a major proto-industrializing region.\nFollowing major events such as the Nader Shah's invasion of the Mughal Empire, Battle of Plassey, Battle of Buxar and the long Anglo-Mysore Wars, most of South Asia was colonised and governed by the British Empire, thus establishing the British Raj. The \"classic period\" ended with the death of Mughal Emperor Aurangzeb, although the dynasty continued for another 150 years. During this period, the Empire was marked by a highly centralized administration connecting the different regions. All the significant monuments of the Mughals, their most visible legacy, date to this period which was characterised by the expansion of Persian cultural influence in the Indian subcontinent, with brilliant literary, artistic, and architectural results. The Maratha Empire was located in the south west of present-day India and expanded greatly under the rule of the Peshwas, the prime ministers of the Maratha empire. In 1761, the Maratha army lost the Third Battle of Panipat against Ahmad shah Durrani king of Afghanistan which halted imperial expansion and the empire was then divided into a confederacy of Maratha states.\nBritish and Dutch colonization.\nThe European economic and naval powers pushed into Asia, first to do trading, and then to take over major colonies. The Dutch led the way followed by the British. Portugal had arrived first, but was too weak to maintain its small holdings and was largely pushed out, retaining only Goa and Macau. The British set up a private organization, the East India Company, which handled both trade and Imperial control of much of India.\nThe commercial colonization of India commenced in 1757, after the Battle of Plassey, when the Nawab of Bengal surrendered his dominions to the British East India Company, in 1765, when the company was granted the \"diwani\", or the right to collect revenue, in Bengal and Bihar, or in 1772, when the company established a capital in Calcutta, appointed its first Governor-General, Warren Hastings, and became directly involved in governance.\nThe Maratha states, following the Anglo-Maratha wars, eventually lost to the British East India Company in 1818 with the Third Anglo-Maratha War. The rule lasted until 1858, when, after the Indian rebellion of 1857 and consequent of the Government of India Act 1858, the British government assumed the task of directly administering India in the new British Raj. In 1819 Stamford Raffles established Singapore as a key trading post for Britain in their rivalry with the Dutch. However, their rivalry cooled in 1824 when an Anglo-Dutch treaty demarcated their respective interests in Southeast Asia. From the 1850s onwards, the pace of colonization shifted to a significantly higher gear.\nThe Dutch East India Company (1800) and British East India Company (1858) were dissolved by their respective governments, who took over the direct administration of the colonies. Only Thailand was spared the experience of foreign rule, although, Thailand itself was also greatly affected by the power politics of the Western powers. Colonial rule had a profound effect on Southeast Asia. While the colonial powers profited much from the region's vast resources and large market, colonial rule did develop the region to a varying extent.\nLate modern.\nCentral Asia: The Great Game, Russia vs Great Britain.\nThe Great Game was a political and diplomatic confrontation between Great Britain and Russia over Afghanistan and neighbouring territories in Central and South Asia. It lasted from 1828 to 1907. There was no war, but there were many threats. Russia was fearful of British commercial and military inroads into Central Asia, and Britain was fearful of Russia threatening its largest and most important possession, India. This resulted in an atmosphere of distrust and the constant threat of war between the two empires. Britain made it a high priority to protect all the approaches to India, and the \"great game\" is primarily how the British did this in terms of a possible Russian threat. Historians with access to the archives have concluded that Russia had no plans involving India, as the Russians repeatedly stated.\nThe Great Game began in 1838 when Britain decided to gain control over the Emirate of Afghanistan and make it a protectorate, and to use the Ottoman Empire, the Persian Empire, the Khanate of Khiva, and the Emirate of Bukhara as buffer states between both empires. This would protect India and also key British sea trade routes by stopping Russia from gaining a port on the Persian Gulf or the Indian Ocean. Russia proposed Afghanistan as the neutral zone, and the final result was diving up Afghanistan with a neutral zone in the middle between Russian areas in the north and British in the South. Important episodes included the failed First Anglo-Afghan War of 1838, the First Anglo-Sikh War of 1845, the Second Anglo-Sikh War of 1848, the Second Anglo-Afghan War of 1878, and the annexation of Kokand by Russia. The 1901 novel \"Kim\" by Rudyard Kipling made the term popular and introduced the new implication of great power rivalry. It became even more popular after the 1979 advent of the Soviet\u2013Afghan War.\nQing China.\nBy 1644, the northern Manchu people had conquered Ming dynasty and established a foreign dynasty\u2014the Qing dynasty\u2014once more. The Manchu Qing emperors, especially Confucian scholar Kangxi, remained largely conservative\u2014retaining the bureaucracy and the scholars within it, as well as the Confucian ideals present in Chinese society. However, changes in the economy and new attempts at resolving certain issues occurred too. These included increased trade with Western countries that brought large amounts of silver into the Chinese economy in exchange for tea, porcelain, and silk textiles. This allowed for a new merchant-class, the compradors, to develop. In addition, repairs were done on existing dikes, canals, roadways, and irrigation works. This, combined with the lowering of taxes and government-assigned labor, was supposed to calm peasant unrest. However, the Qing failed to control the growing landlord class which had begun to exploit the peasantry and abuse their position.\nBy the late 18th century, both internal and external issues began to arise in Qing China's politics, society, and economy. The exam system with which scholars were assigned into the bureaucracy became increasingly corrupt; bribes and other forms of cheating allowed for inexperienced and inept scholars to enter the bureaucracy and this eventually caused rampant neglect of the peasantry, military, and the previously mentioned infrastructure projects. Poverty and banditry steadily rose, especially in rural areas, and mass migrations looking for work throughout China occurred. The perpetually conservative government refused to make reforms that could resolve these issues.\nOpium War.\nChina saw its status reduced by what it perceived as parasitic trade with Westerners. Originally, European traders were at a disadvantage because the Chinese cared little for their goods, while European demand for Chinese commodities such as tea and porcelain only grew. In order to tip the trade imbalance in their favor, British merchants began to sell Indian opium to the Chinese. Not only did this sap Chinese bullion reserves, it also led to widespread drug addiction amongst the bureaucracy and society in general. A ban was placed on opium as early as 1729 by the Yongzheng Emperor, but little was done to enforce it. By the early 19th century, under the new Daoguang Emperor, the government began serious efforts to eradicate opium from Chinese society. Leading this endeavour were respected scholar-officials including Imperial Commissioner Lin Zexu.\nAfter Lin destroyed more than 20,000 chests of opium in the summer of 1839, Europeans demanded compensation for what they saw as unwarranted Chinese interference in their affairs. When it was not paid, the British declared war later the same year, starting what became known as the First Opium War. The outdated Chinese junks were no match for the advanced British gunboats, and soon the Yangzi River region came under threat of British bombardment and invasion. The emperor had no choice but to sue for peace, resulting in the exile of Lin and the making of the Treaty of Nanking, which ceded the British control of Hong Kong and opened up trade and diplomacy with other European countries, including Germany, France, and the USA.\nManchuria.\nManchuria/Northeast China came under influence of Russia with the building of the Chinese Eastern Railway through Harbin to Vladivostok. The Empire of Japan replaced Russian influence in the region as a result of the Russo-Japanese War in 1904\u20131905, and Japan laid the South Manchurian Railway in 1906 to Port Arthur. During the Warlord Era in China, Zhang Zuolin established himself in Northeast China, but was murdered by the Japanese for being too independent. The former Chinese emperor, Puyi, was then placed on the throne to lead a Japanese puppet state of Manchukuo. In August 1945, the Soviet Union invaded the region. From 1945 to 1948, Northeast China was a base area for Mao Zedong's People's Liberation Army in the Chinese Civil War. With the encouragement of the Kremlin, the area was used as a staging ground during the Civil War for the Chinese Communists, who were victorious in 1949 and have controlled ever since.\nJoseon.\nWhen it became the 19th century, the king of Joseon was powerless. Because the noble family of the king's wife got the power and ruled the country by their way. The 26th king of Joseon dynasty, Gojong's father, Heungseon Daewongun wanted the king be powerful again. Even he wasn't the king. As the father of young king, he destroyed noble families and corrupt organizations. So the royal family got the power again. But he wanted to rebuild Gyeongbokgung palace in order to show the royal power to people. So he was criticized by people because he spent enormous money and inflation occurred because of that. So his son, the real king Gojong got power.\nKorean Empire.\nBy the Treaty of Shimonoseki article 1 of the first Sino-Japanese war, Korea was independented from China. The 26th king of Joseon, Gojong changed the nation's name to \"Daehan Jeguk\" (Korean Empire). And he also promoted himself as an emperor. The new empire accepted more western technology and strengthened military power. And Korean Empire was going to become a neutral nation. Unfortunately, in the Russo-Japanese war, Japan ignored this, and eventually Japan won against Russian Empire, and started to invade Korea. Japan first stole the right of diplomacy from Korean Empire illegally. But every western country ignored this invasion because they knew Japan became a strong country as they defeated Russian Empire. So emperor Gojong sent diplomats to a Dutch city known as The Hague to let everyone know that Japan stole the Empire's right illegally. But it was failed. Because the diplomats couldn't go into the conference room. Japan kicked Gojong off on the grounds that this reason. 3 years after, In 1910, Korean Empire became a part of Empire of Japan. It was the first time ever after invasion of Han dynasty in 108 BC.\nContemporary.\nThe European powers had control of other parts of Asia by the early 20th century, such as British India, French Indochina, Spanish East Indies, and Portuguese Macau and Goa. The Great Game between Russia and Britain was the struggle for power in the Central Asian region in the nineteenth century. The Trans-Siberian Railway, crossing Asia by train, was complete by 1916. Parts of Asia remained free from European control, although not influence, such as Persia, Thailand and most of China. In the 20th century, the Empire of Japan expanded into China and Southeast Asia during the Second Sino-Japanese War and World War II. After the war, many Asian countries became independent from European powers. During the Cold War, the northern parts of Asia were communist controlled with the Soviet Union and People's Republic of China, while western allies formed pacts such as CENTO and SEATO. Conflicts such as the Korean War, Vietnam War and Soviet invasion of Afghanistan were fought between communists and anti-communists. In the decades after the Second World War, a massive restructuring plan drove Japan to become the world's second-largest economy, a phenomenon known as the Japanese post-war economic miracle. The Arab\u2013Israeli conflict has dominated much of the recent history of the Middle East. After the Soviet Union's collapse in 1991, there were many new independent nations in Central Asia.\nChina.\nPrior to World War II, China faced a civil war between Mao Zedong's Communist party and Chiang Kai-shek's nationalist party; the nationalists appeared to be in the lead. However, once the Japanese invaded in 1937, the two parties were forced to form a temporary cease-fire in order to defend China. The nationalists faced many military failures that caused them to lose territory and subsequently, respect from the Chinese masses. In contrast, the communists' use of guerilla warfare (led by Lin Biao) proved effective against the Japanese's conventional methods and put the Communist Party on top by 1945. They also gained popularity for the reforms they were already applying in controlled areas, including land redistribution, education reforms, and widespread health care. For the next four years, the nationalists would be forced to retreat to the small island east of Fujian province, known as Taiwan (formerly known as Formosa), where they remain today. In mainland China, People's Republic of China was established by the Communist Party, with Mao Zedong as its state chairman.\nThe communist government in China was defined by the party cadres. These hard-line officers controlled the People's Liberation Army, which itself controlled large amounts of the bureaucracy. This system was further controlled by the Central Committee, which additionally supported the state chairman who was considered the head of the government. The People's Republic's foreign policies included the repressing of secession attempts in Mongolia and Tibet and supporting of North Korea and North Vietnam in the Korean War and Vietnam War, respectively. By 1960 China and the USSR became adversaries, battling worldwide for control of local communist movements.\nToday China plays important roles in world economics and politics. China today is the world's second largest economy and the second fastest growing economy.\nIndian Subcontinent.\nFrom the mid-18th century to the mid-19th century, large regions of India were gradually annexed by the East India Company, a chartered company acting as a sovereign power on behalf of the British government. Dissatisfaction with company rule in India led to the Indian Rebellion of 1857, which rocked parts of north and central India, and led to the dissolution of the company. India was afterwards ruled directly by the British Crown, in the British Raj. After World War I, a nationwide struggle for independence was launched by the Indian National Congress, led by Mahatma Gandhi, and noted for nonviolence. Later, the All-India Muslim League would advocate for a separate Muslim-majority nation state.\nIn August 1947, the British Indian Empire was partitioned into the Union of India and Dominion of Pakistan. In particular, the partition of Punjab and Bengal led to rioting between Hindus, Muslims, and Sikhs in these provinces and spread to other nearby regions, leaving some 500,000 dead. The police and army units were largely ineffective. The British officers were gone, and the units were beginning to tolerate if not actually indulge in violence against their religious enemies. Also, this period saw one of the largest mass migrations anywhere in modern history, with a total of 12\u00a0million Hindus, Sikhs and Muslims moving between the newly created nations of India and Pakistan (which gained independence on 15 and 14 August 1947 respectively). In 1971, Bangladesh, formerly East Pakistan and East Bengal, seceded from Pakistan through an armed conflict sparked by the rise of the Bengali nationalist and self-determination movement.\nKorea.\nDuring the period when the Korean War occurred, Korea divided into North and South. Syngman Rhee became the first president of South Korea, and Kim Il Sung became the supreme leader of North Korea. After the war, the president of South Korea, Syngman Rhee tries to become a dictator. So the April Revolution occurred, eventually Syngman Rhee was exiled from his country. \nIn 1963, Park Chung Hee was empowered with a military coup d'\u00e9tat. He dispatched Republic of Korea Army to Vietnam War. And during this age, the economy of South Korea outran that of North Korea.\nAlthough Park Chung Hee improved the nation's economy, he was a dictator, so people didn't like him. Eventually, he was murdered by Kim Jae-gyu. In 1979, Chun Doo-hwan was empowered by another coup d\u2019\u00e9tat by military. He oppressed the resistances in the city of Gwangju. That event is called 'Gwangju Uprising'. Despite the Gwangju Uprising, Chun Doo-hwan became the president. But the people resisted again in 1987. This movement is called 'June Struggle'. As a result of Gwangju Uprising and June Struggle, South Korea finally became a democratic republic in 1987.\nRoh Tae-woo (1988\u201393), Kim Young-sam (1993\u201398), Kim Dae-jung (1998\u20132003), Roh Moo-hyun (2003\u20132008), Lee Myung-bak (2008\u20132013), Park Geun-hye (2013\u20132017), Moon Jae-in (2017\u2013) were elected as a president in order after 1987. In 1960, North Korea was far wealthier than South Korea. But in 1970, South Korea begins to outrun the North Korean economy. In 2018, South Korea is ranked #10 in world GDP ranking.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14098", "revid": "41006825", "url": "https://en.wikipedia.org/wiki?curid=14098", "title": "History of the Americas", "text": "The human history of the Americas is thought to begin with people migrating to these areas from Asia during the height of an ice age. These groups are generally believed to have been isolated from the people of the \"Old World\" until the coming of Europeans in 1492 with the voyages of Christopher Columbus.\nThe ancestors of today's American Indigenous peoples were the Paleo-Indians; they were hunter-gatherers who migrated into North America. The most popular theory asserts that migrants came to the Americas via Beringia, the land mass now covered by the ocean waters of the Bering Strait. Small lithic stage peoples followed megafauna like bison, mammoth (now extinct), and caribou, thus gaining the modern nickname \"big-game hunters.\" Groups of people may also have traveled into North America on shelf or sheet ice along the northern Pacific coast.\nSedentary societies developed primarily in two regions: Mesoamerica and the Andean civilizations. Mesoamerican cultures include Zapotec, Toltec, Olmec, Maya, Aztec, Mixtec, Totonac, Teotihuacan, Huastec people, Pur\u00e9pecha, Izapa and Mazatec. Andean cultures include Inca, Caral-Supe, Wari, Tiwanaku, Chimor, Moche, Muisca, Chavin, Paracas, and Nazca.\nAfter the voyages of Christopher Columbus in 1492, Spanish and later Portuguese, English, French and Dutch colonial expeditions arrived in the New World, conquering and settling the discovered lands, which led to a transformation of the cultural and physical landscape in the Americas. Spain colonized most of the Americas from present-day Southwestern United States, Florida and the Caribbean to the southern tip of South America. Portugal settled in what is mostly present-day Brazil while England established colonies on the Eastern coast of the United States, as well as the North Pacific coast and in most of Canada. France settled in Quebec and other parts of Eastern Canada and claimed an area in what is today the central United States. The Netherlands settled New Netherland (administrative centre New Amsterdam \u2013 now New York), some Caribbean islands and parts of Northern South America.\nEuropean colonization of the Americas led to the rise of new cultures, civilizations and eventually states, which resulted from the fusion of Native American, European, and African traditions, peoples and institutions. The transformation of American cultures through colonization is evident in architecture, religion, gastronomy, the arts and particularly languages, the most widespread being Spanish (376 million speakers), English (348 million) and Portuguese (201 million). The colonial period lasted approximately three centuries, from the early 16th to the early 19th centuries, when Brazil and the larger Hispanic American nations declared independence. The United States obtained independence from Great Britain much earlier, in 1776, while Canada formed a federal dominion in 1867 and received legal independence in 1931. Others remained attached to their European parent state until the end of the 19th century, such as Cuba and Puerto Rico which were linked to Spain until 1899. Smaller territories such as Guyana obtained independence in the mid-20th century, while French Guiana, the Falkland Islands, Bermuda and several Caribbean islands remain part of a European power to this day.\nPre-colonization.\nLithic stage (before 8000 BCE).\nThe Lithic stage or \"Paleo-Indian period\", is the earliest classification term referring to the first stage of human habitation in the Americas, covering the Late Pleistocene epoch. The time period derives its name from the appearance of \"Lithic flaked\" stone tools. Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest well known human activity in the Americas. Lithic reduction stone tools are used by archaeologists and anthropologists to classify cultural periods.\nArchaic stage (8000\u20131000 BCE).\nSeveral thousand years after the first migrations, the first complex civilizations arose as hunter-gatherers settled into semi-agricultural communities. Identifiable sedentary settlements began to emerge in the so-called Middle Archaic period around 6000 BCE. Particular archaeological cultures can be identified and easily classified throughout the Archaic period.\nIn the late Archaic, on the north-central coastal region of Peru, a complex civilization arose which has been termed the Norte Chico civilization, also known as Caral-Supe. It is the oldest known civilization in the Americas and one of the six sites where civilization originated independently and indigenously in the ancient world, flourishing between the 30th and 18th centuries BC. It pre-dated the Mesoamerican Olmec civilization by nearly two millennia. It was contemporaneous with the ancient civilisation in Egypt following the unification of its kingdom under Narmer and the emergence of the first Egyptian hieroglyphics.\nMonumental architecture, including earthwork platform mounds and sunken plazas, have been identified as part of the civilization. Archaeological evidence points to the use of textile technology and the worship of common god symbols. Government, possibly in the form of theocracy, is assumed to have been required to manage the region. However, numerous questions remain about its organization. In archaeological nomenclature, the culture was pre-ceramic culture of the pre-Columbian Late Archaic period. It appears to have lacked ceramics and art.\nOngoing scholarly debate persists over the extent to which the flourishing of Norte Chico resulted from its abundant maritime food resources, and the relationship that these resources would suggest between coastal and inland sites. The role of seafood in the Norte Chico diet has been a subject of scholarly debate. In 1973, examining the Aspero region of Norte Chico, Michael E. Moseley contended that a maritime subsistence (seafood) economy had been the basis of society and its early flourishing. This theory, later termed \"maritime foundation of Andean Civilization\" was at odds with the general scholarly consensus that civilization arose as a result of intensive grain-based agriculture, as had been the case in the emergence of civilizations in northeast Africa (Egypt) and southwest Asia (Mesopotamia).\nWhile earlier research pointed to edible domestic plants such as squash, beans, l\u00facuma, guava, pacay, and camote at Caral, publications by Haas and colleagues have added avocado, achira, and maize (Zea Mays) to the list of foods consumed in the region. In 2013, Haas and colleagues reported that maize was a primary component of the diet throughout the period of 3000 to 1800 BC. Cotton was another widespread crop in Norte Chico, essential to the production of fishing nets and textiles. Jonathan Haas noted a mutual dependency, whereby \"The prehistoric residents of the Norte Chico needed the fish resources for their protein and the fishermen needed the cotton to make the nets to catch the fish.\"\nIn the 2005 book \"\", journalist Charles C. Mann surveyed the literature at the time, reporting a date \"sometime before 3200 BC, and possibly before 3500 BC\" as the beginning date for the formation of Norte Chico. He notes that the earliest date securely associated with a city is 3500 BC, at Huaricanga in the (inland) Fortaleza area. The Norte Chico civilization began to decline around 1800 BC as more powerful centers appeared to the south and north along its coast, and to the east within the Andes Mountains.\nMesoamerica, the Woodland Period, and Mississippian culture (2000 BCE \u2013 500 CE).\nAfter the decline of the Norte Chico civilization, numerous complex civilizations and centralized polities developed in the Western Hemisphere: The Chavin, Nazca, Moche, Huari, Quitus, Ca\u00f1aris, Chimu, Pachacamac, Tiahuanaco, Aymara and Inca in the Andes; the Muisca, Tairona, Miskito, Huetar, and Talamanca in the Intermediate Area; the Ta\u00ednos in the Caribbean; and the Olmecs, Maya, Toltecs, Mixtecs, Zapotecs, Aztecs, Purepecha and Nicoya in Mesoamerica.\nThe Olmec civilization was the first Mesoamerican civilization, beginning around 1600\u20131400 BC and ending around 400 BC. Mesoamerica is considered one of the six sites around the globe in which civilization developed independently and indigenously. This civilization is considered the mother culture of the Mesoamerican civilizations. The Mesoamerican calendar, numeral system, writing, and much of the Mesoamerican pantheon seem to have begun with the Olmec.\nSome elements of agriculture seem to have been practiced in Mesoamerica quite early. The domestication of maize is thought to have begun around 7,500 to 12,000 years ago. The earliest record of lowland maize cultivation dates to around 5100 BC. Agriculture continued to be mixed with a hunting-gathering-fishing lifestyle until quite late compared to other regions, but by 2700 BC, Mesoamericans were relying on maize, and living mostly in villages. Temple mounds and classes started to appear. By 1300/1200 BC, small centres coalesced into the Olmec civilization, which seems to have been a set of city-states, united in religious and commercial concerns. The Olmec cities had ceremonial complexes with earth/clay pyramids, palaces, stone monuments, aqueducts and walled plazas. The first of these centers was at San Lorenzo (until 900 BC). La Venta was the last great Olmec centre. Olmec artisans sculpted jade and clay figurines of Jaguars and humans. Their iconic giant heads \u2013 believed to be of Olmec rulers \u2013 stood in every major city.\nThe Olmec civilization ended in 400 BC, with the defacing and destruction of San Lorenzo and La Venta, two of the major cities. It nevertheless spawned many other states, most notably the Mayan civilization, whose first cities began appearing around 700\u2013600 BC. Olmec influences continued to appear in many later Mesoamerican civilizations.\nCities of the Aztecs, Mayas, and Incas were as large and organized as the largest in the Old World, with an estimated population of 200,000 to 350,000 in Tenochtitlan, the capital of the Aztec Empire. The market established in the city was said to have been the largest ever seen by the conquistadors when they arrived. The capital of the Cahokians, Cahokia, located near modern East St. Louis, Illinois, may have reached a population of over 20,000. At its peak, between the 12th and 13th centuries, Cahokia may have been the most populous city in North America. Monk's Mound, the major ceremonial center of Cahokia, remains the largest earthen construction of the prehistoric New World.\nThese civilizations developed agriculture as well, breeding maize (corn) from having ears 2\u20135\u00a0cm in length to perhaps 10\u201315\u00a0cm in length. Potatoes, tomatoes, beans (greens), pumpkins, avocados, and chocolate are now the most popular of the pre-Columbian agricultural products. The civilizations did not develop extensive livestock as there were few suitable species, although alpacas and llamas were domesticated for use as beasts of burden and sources of wool and meat in the Andes. By the 15th century, maize was being farmed in the Mississippi River Valley after introduction from Mexico. The course of further agricultural development was greatly altered by the arrival of Europeans.\nClassic stage (800 BCE \u2013 1533 CE).\nCahokia.\nCahokia was a major regional chiefdom, with trade and tributary chiefdoms located in a range of areas from bordering the Great Lakes to the Gulf of Mexico.\nHaudenosaune.\nThe Iroquois League of Nations or \"People of the Long House\", based in present-day upstate and western New York, had a confederacy model from the mid-15th\u00a0century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.\nLeadership was restricted to a group of 50 sachem chiefs, each representing one clan within a tribe; the Oneida and Mohawk people had nine seats each; the Onondagas held fourteen; the Cayuga had ten seats; and the Seneca had eight. Representation was not based on population numbers, as the Seneca tribe greatly outnumbered the others. When a sachem chief died, his successor was chosen by the senior woman of his tribe in consultation with other female members of the clan; property and hereditary leadership were passed matrilineally. Decisions were not made through voting but through consensus decision making, with each sachem chief holding theoretical veto power. The Onondaga were the \"firekeepers\", responsible for raising topics to be discussed. They occupied one side of a three-sided fire (the Mohawk and Seneca sat on one side of the fire, the Oneida and Cayuga sat on the third side.)\nLong-distance trading did not prevent warfare and displacement among the indigenous peoples, and their oral histories tell of numerous migrations to the historic territories where Europeans encountered them. The Iroquois invaded and attacked tribes in the Ohio River area of present-day Kentucky and claimed the hunting grounds. Historians have placed these events as occurring as early as the 13th\u00a0century, or in the 17th\u00a0century Beaver Wars.\nThrough warfare, the Iroquois drove several tribes to migrate west to what became known as their historically traditional lands west of the Mississippi River. Tribes originating in the Ohio Valley who moved west included the Osage, Kaw, Ponca and Omaha people. By the mid-17th century, they had resettled in their historical lands in present-day Kansas, Nebraska, Arkansas and Oklahoma. The Osage warred with Caddo-speaking Native Americans, displacing them in turn by the mid-18th century and dominating their new historical territories.\nOasisamerica.\nPueblo people.\nThe Pueblo people of what is now occupied by the Southwestern United States and northern Mexico, living conditions were that of large stone apartment like adobe structures. They live in Arizona, New Mexico, Utah, Colorado, and possibly surrounding areas.\nAridoamerica.\nChichimeca.\nChichimeca was the name that the Mexica (Aztecs) generically applied to a wide range of semi-nomadic peoples who inhabited the north of modern-day Mexico, and carried the same sense as the European term \"barbarian\". The name was adopted with a pejorative tone by the Spaniards when referring especially to the semi-nomadic hunter-gatherer peoples of northern Mexico.\nMesoamerica.\nOlmec.\nThe Olmec civilization emerged around 1200 BCE in Mesoamerica and ended around 400 BCE. Olmec art and concepts influenced surrounding cultures after their downfall. This civilization was thought to be the first in America to develop a writing system. After the Olmecs abandoned their cities for unknown reasons, the Maya, Zapotec and Teotihuacan arose.\nPurepecha.\nThe Purepecha civilization emerged around 1000 CE in Mesoamerica. They flourished from 1100 CE to 1530 CE. They continue to live on in the state of Michoac\u00e1n. Fierce warriors, they were never conquered and in their glory years, successfully sealed off huge areas from Aztec domination.\nMaya.\nMaya history spans 3,000 years. The Classic Maya may have collapsed due to changing climate in the end of the 10th century.\nToltec.\nThe Toltec were a nomadic people, dating from the 10th\u201312th century, whose language was also spoken by the Aztecs.\nTeotihuacan.\nTeotihuacan (4th century BCE \u2013 7/8th century CE) was both a city, and an empire of the same name, which, at its zenith between 150 and the 5th century, covered most of Mesoamerica.\nAztec.\nThe Aztec having started to build their empire around 14th century found their civilization abruptly ended by the Spanish conquistadors. They lived in Mesoamerica, and surrounding lands. Their capital city Tenochtitlan was one of the largest cities of all time.\nSouth America.\nValdivia culture.\nThe Valdivia culture is one of the oldest settled cultures recorded in the Americas. It emerged from the earlier Las Vegas culture and thrived along the coast of Santa Elena peninsula in Santa Elena Province of Ecuador between 3500 BCE and 1500 BCE.\nNorte Chico.\nOne of the oldest known civilization of the Americas was established in the Norte Chico region of modern Peru. Complex society emerged in the group of coastal valleys, between 3000 and 1800 BCE. The Quipu, a distinctive recording device among Andean civilizations, apparently dates from the era of Norte Chico's prominence.\nChav\u00edn.\nThe Chav\u00edn established a trade network and developed agriculture by as early as (or late compared to the Old World) 900 BCE according to some estimates and archaeological finds. Artifacts were found at a site called Chav\u00edn in modern Peru at an elevation of 3,177 meters. Chav\u00edn civilization spanned from 900 BCE to 300 BCE.\nUpano Valley.\nThe Upano Valley sites in present-day eastern Ecuador predate all known complex Amazonian societies, spanning from approximately 500 BCE to 300-600 CE.\nInca.\nHolding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533.\nKnown as \"Tawantinsuyu\", or \"the land of the four regions\", in Quechua, the Inca culture was highly distinct and developed. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful trepanation of the skull in Inca civilization.\nEuropean colonization.\nAround 1000, the Vikings established a short-lived settlement in Newfoundland, now known as L'Anse aux Meadows. Speculations exist about other Old World discoveries of the New World, but none of these are generally or completely accepted by most scholars.\nSpain sponsored a major exploration led by Italian explorer Christopher Columbus in 1492; it quickly led to extensive European colonization of the Americas. The Europeans brought Old World diseases which are thought to have caused catastrophic epidemics and a huge decrease of the native population. Columbus came at a time in which many technical developments in sailing techniques and communication made it possible to report his voyages easily and to spread word of them throughout Europe. It was also a time of growing religious, imperial and economic rivalries that led to a competition for the establishment of colonies.\nColonial period.\n15th to 19th century colonies in the New World:\nDecolonization.\nThe formation of sovereign states in the New World began with the United States Declaration of Independence of 1776. The American Revolutionary War lasted through the period of the Siege of Yorktown\u2014its last major campaign\u2014in the early autumn of 1781, with peace being achieved in 1783. In 1804, after the French of Napoleon Bonaparte were defeated during the Haitian Revolution under the black leadership of Jean-Jacques Dessalines declare the colony of Saint-Domingue independence of the Haitian Declaration of Independence as he renamed the country \"Ayiti\" meaning (Land of Mountains), Haiti became the world's first black-led republic in the New World, the first Caribbean state as well as the first Latin American country and the second oldest independent nation in the Western Hemisphere after the United States to win independence from Britain in 1783.\nThe Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Sim\u00f3n Bol\u00edvar and Jos\u00e9 de San Mart\u00edn, among others, led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of Latin America politically allied, they rapidly became independent of one another as well, and several further wars were fought, such as the Paraguayan War and the War of the Pacific. (See Latin American integration.) In the Portuguese colony Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese king Dom Jo\u00e3o VI, proclaimed the country's independence in 1822 and became Brazil's first Emperor. This was peacefully accepted by the crown in Portugal, upon compensation.\nEffects of slavery.\nSlavery has had a significant role in the economic development of the New World after the colonization of the Americas by the Europeans. The cotton, tobacco, and sugarcane harvested by slaves became important exports for the United States and the Caribbean countries.\n20th century.\nNorth America.\nAs a part of the British Empire, Canada immediately entered World War I when it broke out in 1914. Canada bore the brunt of several major battles during the early stages of the war, including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on 1 July 1916, the First day on the Somme.\nThe United States stayed out of the conflict until 1917, when it joined the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe. Mexico was not part of the war, as the country was embroiled in the Mexican Revolution at the time.\nThe 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada. From 1926 to 1929, there was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.\nOnce again, Canada found itself at war before its neighbors, with numerically modest but significant contributions overseas such as the Battle of Hong Kong and the Battle of Britain. The entry of the United States into the war helped to tip the balance in favour of the allies. Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to enter the conflict with a declaration of war on the Axis nations. The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs, especially the United States, which emerged as a \"superpower\".\nThe early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. In Canada, Quebec was transformed by the Quiet Revolution and the emergence of Quebec nationalism. Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as \"El Milagro Mexicano\" (the Mexican miracle). The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.\nThe civil rights movement in the U.S. ended Jim Crow and empowered black voters in the 1960s, which allowed black citizens to move into high government offices for the first time since Reconstruction. However, the dominant New Deal coalition collapsed in the mid-1960s in disputes over race and the Vietnam War, and the conservative movement began its rise to power, as the once dominant liberalism weakened and collapsed. Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. In 1982, at the end of his tenure, Canada enshrined a new constitution.\nCanada's Brian Mulroney not only ran on a similar platform but also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989. Mexican presidents Miguel de la Madrid, in the early 1980s and Carlos Salinas de Gortari in the late 1980s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and regulation to stimulate the economy.\nThe end of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On 1 January 1994, Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years. The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which also involved Canada. Canada did not support the United States' later move to invade Iraq, however.\nIn the U.S. the Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s (decade), which burst in 2008 and became the Late-2000s recession with prolonged unemployment.\nCentral America.\nDespite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856\u20131857 the region successfully established a military coalition to repel an invasion by United States adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).\nIn 1907, a Central American Court of Justice was created. On 13 December 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market (\"CACM\"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 \"Football War\" between El Salvador and Honduras. A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.\nSouth America.\nIn the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These dictatorships detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the United States Cold War doctrine of \"National Security\" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see T\u00fapac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is widespread now. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued.\nInternational indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century. In recent years, South American governments have drifted to the left, with socialist leaders being elected in Chile, Bolivia, Brazil, Venezuela, and a leftist president in Argentina and Uruguay. Despite the move to the left, South America is still largely capitalist. With the founding of the Union of South American Nations, South America has started down the road of economic integration, with plans for political integration in the European Union style.\nCaribbean.\nThroughout the 20th century, several island countries, such as Jamaica and Barbados gained independence from British rule. As a result, many of the English-speaking states and territories shifted their economies to tourism and offshore bank industries.\nDuring the Cold War, the Caribbean has faced a series of military interventions from the United States, such as the Banana Wars and the Cuban Missile Crisis.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "14099", "revid": "45310094", "url": "https://en.wikipedia.org/wiki?curid=14099", "title": "History of Africa", "text": "Archaic humans emerged out of Africa between 0.5 and 1.8\u00a0million years ago. This was followed by the emergence of modern humans (\"Homo sapiens\") in East Africa around 300,000\u2013250,000 years ago. In the 4th millennium BC written history arose in Ancient Egypt, and later in Nubia's Kush, the Horn of Africa's D\u02bfmt, and Ifrikiya's Carthage. Between around 3000 BCE and 500 CE, the Bantu expansion swept from north-western Central Africa (modern day Cameroon) across much of Central, Eastern, and Southern Africa, displacing or absorbing groups such as the Khoisan and Pygmies. The oral word is revered in most African cultures, and history has generally been passed down through oral tradition. This has led anthropologists to term them \"oral civilisations\".\nThere were many kingdoms and empires all over the continent that rose and fell. Most states were created through conquest or the borrowing and assimilation of ideas and institutions, while others developed largely in isolation. Some African empires and kingdoms include:\nSome societies were heterarchical and egalitarian, while others were organised into chiefdoms. The continent has between 1250 and 2100 languages, and at its peak it is estimated that Africa had around 10,000 polities, with most following traditional religions.\nFrom the 7th century CE, Islam spread west amid the Arab conquest of North Africa, and by proselytization to the Horn of Africa, bringing with it a new social system. It later spread southwards to the Swahili coast assisted by Muslim dominance of the Indian Ocean trade, and across the Sahara into the western Sahel and Sudan, catalysed by the Fula jihads of the 18th and 19th centuries. Systems of servitude and slavery were historically widespread and commonplace in parts of Africa, as they were in much of the ancient and medieval world. When the trans-Saharan, Red Sea, Indian Ocean and Atlantic slave trades began, local slave systems started supplying captives for slave markets outside Africa. This reorientated many African economies, and created various diasporas, especially in the Americas.\nFrom 1870 to 1914, driven by the Second Industrial Revolution European colonisation of Africa grew rapidly in the \"Scramble for Africa\", and saw the major European powers partition the continent at the 1884 Berlin Conference, resulting in territory under European imperial control increasing from one-tenth of the continent to over nine-tenths. European colonialism had significant impacts on Africa's societies, and colonies were maintained for the purpose of economic exploitation of human and natural resources. Colonial historians dismissed oral traditions, claiming that Africa had no history other than that of Europeans in Africa. Pre-colonial Christian states include Ethiopia, Makuria, and Kongo. Widespread conversions to Christianity occurred under European rule in southern West Africa, Central Africa, and Southern Africa due to successful missions, and the syncretization of Christianity with local beliefs.\nThe rise of nationalism gave birth to independence movements in many parts of the continent, and with a weakened Europe after the Second World War, a wave of decolonisation took place, culminating in the 1960 Year of Africa and the establishment of the Organisation of African Unity in 1963 (the predecessor to the African Union), with countries deciding to keep their colonial borders. Traditional power structures, which had been incorporated into the colonial administration, remained partly in place in many parts of Africa, and their roles, powers, and influence vary greatly. Political decolonisation was mirrored by a movement to decolonise African historiography by incorporating oral sources into a multidisciplinary approach, culminating in UNESCO publishing the \"General History of Africa\" from 1981. Many countries have experienced the rise and fall of nationalism, and continue to face challenges such as internal conflict, neocolonialism, and climate change. &lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory in Africa.\nIn accordance with African cosmology, African historical consciousness viewed historical change and continuity, order and purpose within the framework of human and their environment, the gods, and their ancestors, and they believed themself part of a holistic spiritual entity. In African societies, the historical process is largely a communal one, with eyewitness accounts, hearsay, reminiscences, and occasionally visions, dreams, and hallucinations crafted into narrative oral traditions which were performed and transmitted through generations.1248 In oral traditions time is sometimes mythical and social, and ancestors were considered historical actors. Mind and memory shapes traditions, as events are condensed over time and crystallise into clich\u00e9s.11 Oral tradition can be exoteric or esoteric. It speaks to people according to their understanding, unveiling itself in accordance with their aptitudes.168 In African epistemology, the epistemic subject \"experiences the epistemic object in a sensuous, emotive, intuitive, abstractive understanding, rather than through abstraction alone, as is the case in Western epistemology\" to arrive at a \"complete knowledge\", and as such oral traditions, music, proverbs, and the like were used in the preservation and transmission of knowledge.\nEarly prehistory.\nThe first known hominids evolved in Africa. According to paleontology, the early hominids' skull anatomy was similar to that of the gorilla and the chimpanzee, great apes that also evolved in Africa, but the hominids had adopted a bipedal locomotion which freed their hands. This gave them a crucial advantage, enabling them to live in both forested areas and on the open savanna at a time when Africa was drying up and the savanna was encroaching on forested areas, which occurred sometime between about 10 million and 5 million years ago.\nThe fossil record shows \"Homo sapiens\" (also known as \"modern humans\" or \"anatomically modern humans\") living in Africa by about 350,000\u2013260,000 years ago. The earliest known \"Homo sapiens\" fossils include the Jebel Irhoud remains from Morocco (c.\u2009315,000 years ago), the Florisbad Skull from South Africa (c.\u2009259,000 years ago), and the Omo remains from Ethiopia (c.\u2009233,000 years ago). Scientists have suggested that \"Homo sapiens\" may have arisen between 350,000 and 260,000 years ago through a merging of populations in East Africa and South Africa.\nEvidence of a variety of behaviors indicative of Behavioral modernity date to the African Middle Stone Age, associated with early \"Homo sapiens\" and their emergence. Abstract imagery, widened subsistence strategies, and other \"modern\" behaviors have been discovered from that period in Africa, especially South, North, and East Africa.\nThe Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was confirmed to be around 77,000 and 100\u201375,000 years old. Ostrich egg shell containers engraved with geometric designs dating to 60,000 years ago were found at Diepkloof, South Africa. Beads and other personal ornamentation have been found from Morocco which might be as much as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads dating from significantly prior to 50,000 years ago, and shell beads dating to about 75,000 years ago have been found at Blombos Cave, South Africa.\nAround 65\u201350,000 years ago, the species' expansion out of Africa launched the colonization of the planet by modern human beings. By 10,000 BC, \"Homo sapiens\" had spread to most corners of Afro-Eurasia. Their dispersals are traced by linguistic, cultural and genetic evidence. Eurasian back-migrations, specifically West-Eurasian backflow, started in the early Holocene or already earlier in the Paleolithic period, sometimes between 30 and 15,000 years ago, followed by pre-Neolithic and Neolithic migration waves from the Middle East, mostly affecting Northern Africa, the Horn of Africa, and wider regions of the Sahel zone and East Africa.\nAffad 23 is an archaeological site located in the Affad region of southern Dongola Reach in northern Sudan, which hosts \"the well-preserved remains of prehistoric camps (relics of the oldest open-air hut in the world) and diverse hunting and gathering loci some 50,000 years old\".\nAround 16,000 BC, from the Red Sea Hills to the northern Ethiopian Highlands, nuts, grasses and tubers were being collected for food. By 13,000 to 11,000 BC, people began collecting wild grains. This spread to Western Asia, which domesticated its wild grains, wheat and barley. Between 10,000 and 8,000 BC, Northeast Africa was cultivating wheat and barley and raising sheep and cattle from Southwest Asia.\nA wet climatic phase in Africa turned the Ethiopian Highlands into a mountain forest. Omotic speakers domesticated enset around 6,500\u20135,500 BC. Around 7,000 BC, the settlers of the Ethiopian highlands domesticated donkeys, and by 4,000 BC domesticated donkeys had spread to Southwest Asia. Cushitic speakers, partially turning away from cattle herding, domesticated teff and finger millet between 5,500 and 3,500 BC.\nDuring the 11th millennium BP, pottery was independently invented in Africa, with the earliest pottery there dating to about 9,400 BC from central Mali. It soon spread throughout the southern Sahara and Sahel. In the steppes and savannahs of the Sahara and Sahel in Northern West Africa, the Nilo-Saharan speakers and Mand\u00e9 peoples started to collect and domesticate wild millet, African rice and sorghum between 8,000 and 6,000 BC. Later, gourds, watermelons, castor beans, and cotton were also collected and domesticated. The people started capturing wild cattle and holding them in circular thorn hedges, resulting in domestication.\nThey also started making pottery and built stone settlements (e.g., Tichitt, Oualata). Fishing, using bone-tipped harpoons, became a major activity in the numerous streams and lakes formed from the increased rains. Mande peoples have been credited with the independent development of agriculture about 4,000\u20133,000 BC.\nEvidence of the early smelting of metals\u00a0\u2013 lead, copper, and bronze\u00a0\u2013 dates from the fourth millennium BC.\nEgyptians smelted copper during the predynastic period, and bronze came into use after 3,000 BC at the latest in Egypt and Nubia. Nubia became a major source of copper as well as of gold. The use of gold and silver in Egypt dates back to the predynastic period.\nIn the A\u00efr Mountains of present-day Niger people smelted copper independently of developments in the Nile valley between 3,000 and 2,500 BC. They used a process unique to the region, suggesting that the technology was not brought in from outside; it became more mature by about 1,500 BC.\nBy the 1st millennium BC iron working had reached Northwestern Africa, Egypt, and Nubia. Zangato and Holl document evidence of iron-smelting in the Central African Republic and Cameroon that may date back to 3,000 to 2,500 BC. Assyrians using iron weapons pushed Nubians out of Egypt in 670 BC, after which the use of iron became widespread in the Nile valley.\nThe theory that iron spread to Sub-Saharan Africa via the Nubian city of Meroe is no longer widely accepted, and some researchers believe that sub-Saharan Africans invented iron metallurgy independently. Metalworking in West Africa has been dated as early as 2,500 BC at Egaro west of the Termit in Niger, and iron working was practiced there by 1,500 BC. Iron smelting has been dated to 2,000 BC in southeast Nigeria. Central Africa provides possible evidence of iron working as early as the 3rd millennium BC. Iron smelting developed in the area between Lake Chad and the African Great Lakes between 1,000 and 600 BC, and in West Africa around 2,000 BC, long before the technology reached Egypt. Before 500 BC, the Nok culture in the Jos Plateau was already smelting iron. Archaeological sites containing iron-smelting furnaces and slag have been excavated at sites in the Nsukka region of southeast Nigeria in Igboland: dating to 2,000 BC at the site of Lejja (Eze-Uzomaka 2009) and to 750 BC and at the site of Opi (Holl 2009). The site of Gbabiri (in the Central African Republic) has also yielded evidence of iron metallurgy, from a reduction furnace and blacksmith workshop; with earliest dates of 896\u2013773 BC and 907\u2013796 BC respectively.\nAncient Africa (4000BC - 6th century).\nNorth-East Africa.\nFrom around 3,500 BCE, a coalition of Horus-worshipping nomes in the western Nile Delta conquered the Andjety-worshipping nomes of the east to form Lower Egypt, whilst Set-worshipping nomes in the south coalesced to form Upper Egypt. Egypt was first united when Narmer of Upper Egypt conquered Lower Egypt, giving rise to the 1st and 2nd dynasties of Egypt whose efforts presumably consisted of conquest and consolidation, with unification completed by the 3rd dynasty to form the Old Kingdom of Egypt in 2,686 BCE. The Kingdom of Kerma emerged around this time to become the dominant force in Nubia, controlling an area as large as Egypt between the 1st and 4th cataracts of the Nile, with Egyptian records speaking of its rich and populous agricultural regions.\nThe height of the Old Kingdom came under the 4th dynasty who constructed numerous great pyramids, however under the 6th dynasty of Egypt power began to decentralise to the nomarchs, culminating in anarchy exacerbated by drought and famine in 2,200 BCE, and the onset of the First Intermediate Period in which numerous nomarchs ruled simultaneously. Throughout this time, power bases were built and destroyed in Memphis, and in Heracleopolis, when Mentuhotep II of Thebes and the 11th dynasty conquered all of Egypt to form the Middle Kingdom in 2,055 BCE. The 12th dynasty oversaw advancements in irrigation and economic expansion in the Faiyum Oasis, as well as expansion into Lower Nubia at the expense of Kerma. In 1,700 BCE, Egypt fractured in two, ushering in the Second Intermediate Period.\nThe Hyksos, a militaristic people from Palestine, capitalised on this fragmentation and conquered Lower Egypt, establishing the 15th dynasty of Egypt, whilst Kerma coordinated invasions deep into Egypt to reach its greatest extent, looting royal statues and monuments. A rival power base developed in Thebes with Ahmose I of the 18th dynasty eventually expelling the Hyksos from Egypt, forming the New Kingdom in 1,550 BCE. Utilising the military technology the Hyksos had brought, they conducted numerous campaigns to conquer the Levant from the Canaanites, Amorites, Hittites, and Mitanni, and extinguish Kerma, incorporating Nubia into the empire, sending the Egyptian empire into its golden age. Internal struggles, drought and famine, and invasions by a confederation of seafaring peoples, contributed to the New Kingdom's collapse in 1,069 BCE, ushering in the Third Intermediate Period which saw Egypt fractured into many pieces amid widespread turmoil.\nEgypt's disintegration liberated the more Egyptianized Kingdom of Kush in Nubia, and later in the 8th century BCE the Kushite king Kashta would expand his power and influence by manoeuvring his daughter into a position of power in Upper Egypt, paving the way for his successor Piye to conquer Lower Egypt and form the Kushite Empire. The Kushites assimilated further into Egyptian society by reaffirming Ancient Egyptian religious traditions, and culture, while introducing some unique aspects of Kushite culture and overseeing a revival in pyramid-building. After a century of rule they were forcibly driven out of Egypt by the Assyrians as reprisal for the Kushites agitating peoples within the Assyrian Empire in an attempt to gain a foothold in the region. The Assyrians installed a puppet dynasty which later gained independence and once more unified Egypt, with Upper Egypt becoming a rich agricultural region whose produce Lower Egypt then sold and traded.\nIn 525 BCE Egypt was conquered by the expansionist Achaemenids, however later regained independence in 404 BCE until 343 BCE when it was re-annexed by the Achaemenid Empire. Persian rule in Egypt ended with the defeat of the Achaemenids by Alexander the Great in 332 BCE, marking the beginning of Hellenistic rule by the Macedonian Ptolemaic dynasty in Egypt. The Hellenistic rulers, seeking legitimacy from their Egyptian subjects, gradually Egyptianized and participated in Egyptian religious life. Following the Syrian Wars with the Seleucid Empire, the Ptolemaic Kingdom lost its holdings outside Africa, but expanded its territory by conquering Cyrenaica from its respective tribes, and subjugated Kush.\nBeginning in the mid second century BCE, dynastic strife and a series of foreign wars weakened the kingdom, and it became increasingly reliant on the Roman Republic. Under Cleopatra VII, who sought to restore Ptolemaic power, Egypt became entangled in a Roman civil war, which ultimately led to its conquest by Rome in 30 BCE. The Crisis of the Third Century in the Roman Empire freed the Levantine city state of Palmyra who conquered Egypt, however their rule lasted only a few years before Egypt was reintegrated into the Roman Empire. In the midst of this, Kush regained total independence from Egypt, and they would persist as a major regional power until, having been weakened from internal rebellion amid worsening climatic conditions, invasions by both the Aksumites and the Noba caused their disintegration into Makuria, Alodia, and Nobatia in the 5th century CE. The Romans managed to hold on to Egypt for the rest of the ancient period.\nHorn of Africa.\nIn the Horn of Africa there was the Land of Punt, a kingdom on the Red Sea, likely located in modern-day Eritrea or northern Somaliland. The Ancient Egyptians initially traded via middle-men with Punt until in 2,350 BCE when they established direct relations. They would become close trading partners for over a millennium, with Punt exchanging gold, aromatic resins, blackwood, ebony, ivory and wild animals. Towards the end of the ancient period, northern Ethiopia and Eritrea bore the Kingdom of D'mt beginning in 980 BCE, whose people developed irrigation schemes, used ploughs, grew millet, and made iron tools and weapons. In modern-day Somalia and Djibouti there was the Macrobian Kingdom, with archaeological discoveries indicating the possibility of other unknown sophisticated civilisations at this time.\nAfter D'mt's fall in the 5th century BCE the Ethiopian Plateau came to be ruled by numerous smaller unknown kingdoms who experienced strong south Arabian influence, until the growth and expansion of Aksum in the 1st century BCE. Along the Horn's coast there were many ancient Somali city-states which thrived off of the wider Red Sea trade and transported their cargo via beden, exporting myrrh, frankincense, spices, gum, incense, and ivory, with freedom from Roman interference causing Indians to give the cities a lucrative monopoly on cinnamon from ancient India.\nThe Kingdom of Aksum grew from a principality into a major power on the trade route between Rome and India through conquering its unfortunately unknown neighbours, gaining a monopoly on Indian Ocean trade in the region. Aksum's rise had them rule over much of the regions from the Lake Tana to the valley of the Nile, and they further conquered parts of the ailing Kingdom of Kush, led campaigns against the Noba and Beja peoples, and expanded into South Arabia. This led the Persian prophet Mani to consider Aksum as one of the four great powers of the 3rd century CE alongside Persia, Rome, and China. In the 4th century CE Aksum's king converted to Christianity and Aksum's population, who had followed syncretic mixes of local beliefs, slowly followed.\nIn the early 6th century CE, Cosmas Indicopleustes later described his visit to the city of Aksum, mentioning rows of throne monuments, some made out of \"excellent white marble\" and \"entirely...hewn out of a single block of stone\", with large inscriptions attributed to various kings, likely serving as victory monuments documenting the wars waged. The turn of the 6th century saw Aksum balanced against the Himyarite Kingdom in southwestern Arabia, as part of the wider Byzantine-Sassanian conflict. In 518, Aksum invaded Himyar against the persecution of the Christian community by Dhu Nuwas, the Jewish Himyarite king. Following the capture of Najran, the Aksumites implanted a puppet on the Himyarite throne, however a coup d'\u00e9tat in 522 brought Dhu Nuwas back to power who again began persecuting Christians. The Aksumites invaded again in 525, and with Byzantine aid conquered the kingdom, incorporating it as a vassal state after some minor internal conflict. In the late 6th century the Aksumites were driven out of Yemen by the Himyarite king with the aid of the Sassanids.\nNorth-West Africa.\nFurther north-west, the Maghreb and Ifriqiya were mostly cut off from the cradle of civilisation in Egypt by the Libyan desert, exacerbated by Egyptian boats being tailored to the Nile and not coping well in the open Mediterranean Sea. This caused its societies to develop contiguous to those of Southern Europe, until Phoenician settlements came to dominate the most lucrative trading locations in the Gulf of Tunis, initially searching for sources of metal. Phoenician settlements subsequently grew into Ancient Carthage after gaining independence from Phoenicia in the 6th century BCE, and they would build an extensive empire, countering Greek influence in the Mediterranean, as well as a strict mercantile network reaching as far as west Asia and northern Europe, distributing an array of commodities from all over the ancient world along with locally produced goods, all secured by one of the largest and most powerful navies in the ancient Mediterranean.\nCarthage's political institutions received rare praise from both Greeks and Romans, with its constitution and aristocratic council providing stability, with birth and wealth paramount for election. In 264 BCE the First Punic War began when Carthage came into conflict with the expansionary Roman Republic on the island of Sicily, leading to what has been described as the greatest naval war of antiquity, causing heavy casualties on both sides, but ending in Carthage's eventual defeat and loss of Sicily. The Second Punic War broke out when the Romans opportunistically took Sardinia and Corsica whilst the Carthaginians were putting down a ferocious Libyan revolt, with Carthage initially experiencing considerable success following Hannibal's infamous crossing of the alps into northern Italy. In a 14 year long campaign Hannibal's forces conquered much of mainland Italy, only being recalled after the Romans conducted a bold naval invasion of the Carthaginian homeland and then defeated him in climactic battle in 202 BCE.\nCarthage was forced to give up their fleet, and the subsequent collapse of their empire would produce two further polities in the Maghreb; Numidia, a polity made up of two Numidian tribal federations until the Massylii conquered the Masaesyli, and assisted the Romans in the Second Punic War; Mauretania, a Mauri tribal kingdom, home of the legendary King Atlas; and various tribes such as Garamantes, Musulamii, and Bavares. The Third Punic War would result in Carthage's total defeat in 146 BCE and the Romans established the province of Africa, with Numidia assuming control of many of Carthage's African ports. Towards the end of the 2nd century BCE Mauretania fought alongside Numidia's Jugurtha in the Jugurthine War against the Romans after he had usurped the Numidian throne from a Roman ally. Together they inflicted heavy casualties that quaked the Roman Senate, with the war only ending inconclusively when Mauretania's Bocchus I sold out the Jugurtha to the Romans.\nAt the turn of the millennium they both would face the same fate as Carthage and be conquered by the Romans who established Mauretania and Numidia as provinces of their empire, whilst Musulamii, led by Tacfarinas, and Garamantes were eventually defeated in war in the 1st century CE however weren't conquered. In the 5th century CE the Vandals conquered north Africa precipitating the fall of Rome. Swathes of indigenous peoples would regain self-governance in the Mauro-Roman Kingdom and its numerous successor polities in the Maghreb, namely the kingdoms of Ouarsenis, Aur\u00e8s, and Altava. The Vandals ruled Ifriqiya for a century until Byzantine's reconquest in the early 6th century CE. The Byzantines and the Berber kingdoms fought minor inconsequential conflicts, such as in the case of Garmul, however largely coexisted. Further inland to the Byzantine Exarchate of Africa were the Sanhaja in modern-day Algeria, a broad grouping of three groupings of tribal confederations, one of which is the Masmuda grouping in modern-day Morocco, along with the nomadic Zenata; their composite tribes would later go onto shape much of North African history.\nWest Africa.\nIn the western Sahel the rise of settled communities occurred largely as a result of the domestication of millet and of sorghum. Archaeology points to sizable urban populations in West Africa beginning in the 4th millennium BCE, which had crucially developed iron metallurgy by 1,200 BCE, in both smelting and forging for tools and weapons. Extensive east-west belts of deserts, grasslands, and forests from north to south were crucial in the moulding of their respective societies and meant that prior to the accession of trans-Saharan trade routes, symbiotic trade relations developed in response to the opportunities afforded by north\u2013south diversity in ecosystems, trading meats, copper, iron, salt, and gold.\nVarious civilisations prospered in this period. From 4,000 BCE, the Tichitt culture in modern-day Mauritania and Mali was the oldest known complexly organised society in West Africa, with a four tiered hierarchical social structure. Other civilisations include the Kintampo culture from 2,500 BCE in modern-day Ghana, the Nok culture from 1,500 BCE in modern-day Nigeria, the Daima culture around Lake Chad from 550 BCE, and Djenn\u00e9-Djenno from 250 BCE in modern-day Mali.\nTowards the end of the 3rd century CE, a wet period in the Sahel opened areas for human habitation and exploitation which had not been habitable for the better part of a millennium. Based on large tumuli scattered across West Africa dating to this period, it has been proposed that there were several contemporaneous kingdoms which have unfortunately been lost to history. Some important polities likely founded in the early-to-middle 1st millennium who did make it into the historical record include Mema, Takrur, Silla, and Wagadu (commonly called the Ghana Empire).\nSoninke traditions mention four previous foundings of Wagadu, and hold that the final founding of Wagadu occurred after their first king did a deal with \"Bida\u200a\", a serpent deity who was guarding a well, to sacrifice one maiden a year in exchange for assurance regarding plenty of rainfall and gold supply. Soninke tradition portrays early Ghana as warlike, with horse-mounted warriors key to increasing its territory and population, although details of their expansion are extremely scarce.\nCentral, Eastern, and Southern Africa.\nIn the 4th millennium BCE the Congo Basin was inhabited by the Bambenga, Bayaka, Bakoya, and Babongo in the west, the Bambuti in the east, and the Batwa who were widely scattered and also present in the Great Lakes region; together they are grouped as Pygmies. On the later-named Swahili coast there were Cushitic-speaking peoples, and the Khoisan (a neologism for the Khoekhoe and San) in the continent's south. Early San society left a rich legacy of cave paintings across Southern Africa.\nThe Bantu expansion constituted a major series of migrations of Bantu-speaking peoples from Central Africa to Eastern and Southern Africa and was substantial in the settling of the continent. Commencing in the 2nd millennium BCE, the Bantu began to migrate from Cameroon to the Congo Basin, and eastward to the Great Lakes region to form the Urewe culture from the 5th century BC. In the 7th century AD, Bantu spread to the Upemba Depression, forming the Upemba culture.\nDuring the 1st millennium BCE the Bantu spread further from the Great Lakes to Southern and East Africa. One early movement headed south to the upper Zambezi basin in the 2nd century BCE. The Bantu then split westward to the savannahs of present-day Angola and eastward into Malawi, Zambia, and Zimbabwe in the 1st century CE, forming the Gokomere culture in the 5th century CE. The second thrust from the Great Lakes was eastward, also in the 1st century AD, expanding to Kenya, Tanzania, and the Swahili coast.\nPrior to this migration, the northern part of the Swahili coast was home to the elusive Azania, most likely a Southern Cushitic polity, extending southwards to modern-day Tanzania. The Bantu populations crowded out Azania, with Rhapta being its last stronghold by the 1st century CE, and traded via the Indian Ocean trade Madagascar was possibly first settled by Austronesians from 350 BC-550 CE, termed the \"Vazimba\" in Malagasy oral traditions, although there is considerable academic debate.\nThe eastern Bantu group would eventually meet with the southern migrants from the Great Lakes in Malawi, Zambia, and Zimbabwe and both groups continued southward, with eastern groups continuing to Mozambique and reaching Maputo in the 2nd century CE. Further to the south, settlements of Bantu peoples who were iron-using agriculturists and herdsmen were well established south of the Limpopo River by the 4th century CE, displacing and assimilating the Khoisan.\nBy the Chari River south of Lake Chad the Sao civilisation flourished for over a millennium beginning in the 6th century BCE, in territory that later became part of present-day Cameroon and Chad. Sao artifacts show that they were skilled workers in bronze, copper, and iron,19 with finds including bronze sculptures, terracotta statues of human and animal figures, coins, funerary urns, household utensils, jewellery, highly decorated pottery, and spears.19 Nearby, around Lake Ejagham in south-west Cameroon, the Ekoi civilisation rose circa 2nd century CE, and are most notable for constructing the Ikom monoliths and developing the Nsibidi script.\nEarly Medieval Africa (600-1250).\nNorth Africa.\nNorthern Africa.\nThe turn of the 7th century saw much of North Africa controlled by the Byzantine Empire. Christianity was the state religion of the empire, and Semitic and Coptic subjects in Roman Egypt faced persecution due to their 'heretical' Miaphysite churches, paying a heavy tax. The Exarchate of Africa covered much of Ifriqiya and the eastern Maghreb, surrounded by numerous Berber kingdoms that followed Christianity heavily syncretised with traditional Berber religion. The interior was dominated by various groupings of tribal confederations, namely the nomadic Zenata, the Masmuda of Sanhaja in modern-day Morocco, and the other two Sanhaja in the Sahara in modern-day Algeria, who all mainly followed traditional Berber religion. In 618 the Sassanids conquered Egypt during the Byzantine-Sasanian War, however the province was reconquered three years later.\nThe early 7th century saw the inception of Islam and the beginning of the Arab conquests intent on converting peoples to Islam and monotheism. The nascent Rashidun Caliphate won a series of crucial victories and expanded rapidly, forcing the Byzantines to evacuate Syria. With Byzantine regional presence shattered, Egypt was quickly conquered by 642, with the Egyptian Copts odious of Byzantine rule generally putting up little resistance. The Muslims' attention then turned west to the Maghreb where the Exarchate of Africa had declared independence from Constantinople under Gregory the Patrician. The Muslims conquered Ifriqiya and in 647 defeated and killed Gregory and his army decisively in battle. The Berbers of the Maghreb proposed payment of annual tribute, which the Muslims, not wishing to annex the territory, accepted. After a brief civil war in the Muslim empire, the Rashidun were supplanted by the Umayyad dynasty in 661 and the capital moved from Medina to Damascus.\nWith intentions to expand further in all directions, the Muslims returned to the Maghreb to find the Byzantines had reinforced the Exarchate and allied with the Berber Kingdom of Altava under Kusaila, who was approached prior to battle and convinced to convert to Islam. Initially having become neutral, Kusaila objected to integration into the empire and in 683 destroyed the poorly supplied Arab army and conquered the newly-found Kairouan, causing an epiphany among the Berber that this conflict was not just against the Byzantines. The Arabs returned and defeated Kusaila and Altava in 690, and, after a set-back, expelled the Byzantines from North Africa. To the west, Kahina of the Kingdom of the Aur\u00e8s declared opposition to the Arab invasion and repelled their armies, securing her position as the uncontested ruler of the Maghreb for five years. The Arabs received reinforcements and in 701 Kahina was killed and the kingdom defeated. They completed their conquest of the rest of the Maghreb, with large swathes of Berbers embracing Islam, and the combined Arab and Berber armies would use this territory as a springboard into Iberia to expand the Muslim empire further.\nLarge numbers of Berber and Coptic people willingly converted to Islam, and followers of Abrahamic religions (\"People of the Book\") constituting the Dhimmi class were permitted to practice their religion and exempted from military service in exchange for a tax, which was improperly extended to include converts. Followers of traditional Berber religion, which were mostly those of tribal confederations in the interior, were violently oppressed and often given the ultimatum to convert to Islam or face captivity or enslavement. Converted natives were permitted to participate in the governing of the Muslim empire in order to quell the enormous administrative problems owing to the Arabs' lack of experience governing and rapid expansion. Unorthodox sects such as the Kharijite, Ibadi, Isma'ili, Nukkarite and Sufrite found fertile soil among many Berbers dissatisfied with the oppressive Umayyad regime, with religion being utilised as a political tool to foster organisation. In the 740s the Berber Revolt rocked the caliphate and the Berbers took control over the Maghreb, whilst revolts in Ifriqiya were suppressed.\nThe Abbasid dynasty came to power via revolution in 750 and attempted to reconfigure the caliphate to be multi-ethnic rather than Arab exclusive, however this wasn't enough to prevent gradual disintegration on its peripheries. Various short-lived native dynasties would form states such as the Barghawata of Masmuda, the Ifranid dynasty, and the Midrarid dynasty, both from the Zenata. The Idrisid dynasty would come to rule most of modern-day Morocco with the support of the Masmuda, whilst the growing Ibadi movement among the Zenata culminated in the Rustamid Imamate, centred on Tahert, modern-day Algeria. At the turn of the 9th century the Abbasids' sphere of influence would degrade further with the Aghlabids controlling Ifriqiya under only nominal Abbasid rule and in 868 when the Tulunids wrestled the independence of Egypt for four decades before again coming under Abbasid control. Late in the 9th century, a revolt by East African slaves in the Abbasid's homeland of Iraq diverted its resources away from its other territories, devastating important ports in the Persian Gulf, and was eventually put down after decades of violence, resulting in between 300,000 and 2,500,000 dead.\nThis gradual bubbling of disintegration of the caliphate boiled over when the Fatimid dynasty rose out of the Bavares tribal confederation and in 909 conquered the Aghlabids to gain control over all of Ifriqiya. Proclaiming Isma'ilism, they established a caliphate rivalling the Abbasids, who followed Sunni Islam. The nascent caliphate quickly conquered the ailing Rustamid Imamate and fought a proxy war against the remnants of the Umayyad dynasty centred in Cordoba, resulting the eastern Maghreb coming under the control of the vassalized Zirid dynasty, who hailed from the Sanhaja. In 969 the Fatimids finally conquered Egypt against a weakened Abbasid Caliphate after decades of attempts, moving their capital to Cairo and deferring Ifriqiya to the Zirids. From there they conquered up to modern-day Syria and Hejaz, securing the holy cities of Mecca and Medina. The Fatimids became absorbed by the eastern realms of their empire, and in 972, after encouragement from faqirs, the Zirids changed their allegiance to recognise the Abbasid Caliphate.\nIn retaliation the Fatimids commissioned an invasion by nomadic Arab tribes to punish them, leading to their disintegration with the Khurasanid dynasty and Arab tribes ruling Ifriqiya, to be later displaced by the Norman Kingdom of Africa. In the late 10th and early 11th centuries the Fatimids would lose the Maghreb to the Hammadids in modern-day Algeria and the Maghrawa in modern-day Morocco, both from Zenata. In 1053 the Saharan Sanhaja, spurred on by puritanical Sunni Islam, conquered Sijilmasa and captured Aoudaghost from the Ghana Empire to control the affluent trans-Saharan trade routes in the Western Sahara, forming the Almoravid empire before conquering Maghrawa and intervening in the reconquest of Iberia by the Christian powers on the side of the endangered Muslim taifas, which were produced from the fall of the remnant Umayyad Caliphate in Cordoba. The Almoravids incorporated the taifas into their empire, enjoying initial success, until a devastating ambush crippled their military leadership, and throughout the 12th century they gradually lost territory to the Christians.\nTo the east, the Fatimids saw their empire start to collapse in 1061, beginning with the loss of the holy cities to the Sharifate of Mecca and exacerbated by rebellion in Cairo. The Seljuk Turks, who saw themselves as the guardian of the Abbasid Caliphate, capitalised and conquered much of their territories in the east, however the Fatimids repelled them from encroaching on Egypt. Amid the Christians' First Crusade against the Seljuks, the Fatimids opportunistically took back Jerusalem, but then lost it again to the Christians in decisive defeat. The Fatimids' authority collapsed due to intense internal struggle in political rivalries and religious divisions, amid Christian invasions of Egypt, creating a power vacuum in North Africa. The Zengid dynasty, nominally under Seljuk suzerainty, invaded on the pretext of defending Egypt from the Christians, and usurped the position of vizier in the caliphate.\nFollowing the assassination of the previous holder, the position of vizier passed onto Salah ad-Din Yusuf ibn Ayyub (commonly referred to as Saladin). After a joint Zengid-Fatimid effort repelled the Christians and after he had put down a revolt from the Fatimid army, Saladin eventually deposed the Fatimid caliph in 1171 and established the Ayyubid dynasty in its place, choosing to recognise the Abbasid Caliphate. From there the Ayyubids captured Cyrenaica, and went on a prolific campaign to conquer Arabia from the Zengids and the Yemeni Hamdanids, Palestine from the Christian Kingdom of Jerusalem, and Syria and Upper Mesopotamia from other Seljuk successor states. To the west, there was a new domestic threat to Almoravid rule; a religious movement headed by Ibn Tumart from the Masmuda tribal grouping, who was considered by his followers to be the true Mahdi. Initially fighting a guerilla war from the Atlas Mountains, they descended from the mountains in 1130 but were crushed in battle, with Ibn Tumart dying shortly after.\nThe movement consolidated under the leadership of self-proclaimed caliph Abd al-Mu'min and, after gaining the support of the Zenata, swept through the Maghreb, conquering the Hammadids, the Hilalian Arab tribes, and the Norman Kingdom of Africa, before gradually conquering the Almoravid remnant in Al-Andalus, proclaiming the Almohad Caliphate and extending their rule from the western Sahara and Iberia to Ifriqiya by the turn of the 13th century. Later, the Christians capitalised on internal conflict within the Almohads in 1225 and conquered Iberia by 1228, with the Emirate of Granada assuming control in the south. Following this, the embattled Almohads faced invasions from an Almoravid remnant in the Balearics and gradually lost territory to the Marinids in modern-day Morocco, the Zayyanids in modern-day Algeria, both of Zenata, and the Hafsids of Masmuda in modern-day Tunisia, before finally being extinguished in 1269. Meanwhile, after defeating the Christians' Fifth Crusade in 1221, internal divisions involving Saladin's descendants appeared within the Ayyubid dynasty, crippling the empire's unity. In the face of Mongol expansion, the Ayyubids became increasingly reliant on Mamluk generals.\nEast Africa.\nHorn of Africa.\nAt the end of the 6th century, the Kingdom of Aksum ruled over much of modern-day Ethiopia and Eritrea, with the Harla Kingdom to its east, while ancient Somali city-states such as Mosylon, Opone, Sarapion, Avalites, and Aromata on the Somali Peninsula continued to thrive off of the lucrative Indian Ocean trade and their preferential relations with India.\nFollowing the birth of Islam in the early 7th century, the north-central Harar Plateau was settled by early Muslims fleeing persecution, intermingling with the Somali who became some of the first non-Arabs to convert to Islam. Muslim-Aksumite relations were initially positive with Aksum giving refuge to early Muslims in 613, however relations soured after Aksum made incursions along the Arab coast and Muslims settled the Dahlak archipelago. Despite having ancient roots, the Red Sea slave trade expanded and flourished following the Muslim conquests with Bejas, Nubians, and Ethiopians exported to Hejaz. Aksum gradually lost their control of the Red Sea, and the expulsion of the Byzantines from the region isolated them, causing their society to become introspective, drawing inspiration from biblical traditions of the Old Testament.\nMeanwhile, during the 7th, 8th, and 9th centuries Islam spread through the Somali Peninsula, largely via \"da'wah\". The Harla Kingdom of Hubat also converted to Islam circa 700. The Somalis were organised into various clans, and relations with Arabs led tradition to hold their lineages to Samaale, Daarood or Sheikh Ishaaq, traditionally descendants of Muhammad's cousins. To the west from the 7th to 15th century, Arab tribes migrated into the Sudan, during which time the Beja Islamised and adopted Arab customs. In the 8th century, Beja nomads invaded Aksum's northern territories and occupied the Eritrean Highlands, leading punitive raids into Aksum, with the Beja establishing various kingdoms. The Aksumite population migrated further inland into the Ethiopian Highlands, moving their capital from Aksum to Kubar, and later in the 9th century expanded southwards.\nThe history becomes murky, however tradition holds that Aksum's expansion brought it into conflict in 960 with the Jewish Kingdom of Beta Israel, led by queen Gudit and located in the Simien Mountains. Accordingly, Gudit defeated and killed Aksum's king, and burnt their churches. It's possible that Gudit was a pagan queen who led resistance to Aksum's southward expansion. To the east in the 9th and 10th centuries, the Somali clans such as the Dir and other groups formed states in the Harar Plateau, including Fatagar, Dawaro, Bale, Hadiya, Hargaya, Mora, Kwelgora, and Adal, with the latter centred on the port city of Zeila (previously Avalites). They neighboured the Sultanate of Shewa to their south, whose dynasty hailed from the Meccan Banu Makhzum. On the Horn's southeast coast the Tunni clan established the Tunni Sultanate, and the clans of Sarapion formed the Sultanate of Mogadishu.\nTraditionally, Gudit's dynasty reigned until 1137 when they were overthrown or conquered by Mara Takla Haymanot who established the Zagwe dynasty, with traditions differing on whether he was an Aksumite general or relative of Gudit. In Ethiopia tradition holds that prior to his accession to the throne, Gebre Meskel Lalibela was guided by Christ on a tour of Jerusalem, and instructed to build a second Jerusalem in Ethiopia.115 Accordingly, this led to the commissioning of eleven rock-hewn churches outside the capital in Roha, which was renamed Lalibela in his honour, and quickly became a holy city in Ethiopian Christianity. Possibly formed in the 10th century, the Kingdom of Damot, following a traditional religion, had become a powerful state by the 13th century (and is possibly synonymous with the Kingdom of Wolaita founded by Motolomi Sato).59\nThe history continues to be murky, however regional hegemony was contested between the Kingdom of Damot, the Zagwe, and the Sultanate of Shewa. Damot likely drew its economic power from gold production, which was exported to Zeila. The Zagwe and Shewa were forced into a conditional alliance to counter Damot, with Shewa at times forced to pay tribute to the pagans. In the 13th century the Ajuran clan established the Ajuran Sultanate on the eastern coast of the Horn and expanded, conquering the Tunni and vassalising Mogadishu, coming to dominate the Indian Ocean trade, while the Warsangali clan formed the Warsangali Sultanate on the Horn's north-eastern coast.\nSwahili coast, Madagascar, and the Comoro Islands.\nThe turn of the 7th century saw the Swahili coast continue to be inhabited by the Swahili civilisation, whose economies were primarily based on , however they traded via the Indian Ocean trade and later developed local industries, with their iconic stone architecture. Forested river estuaries created natural harbours whilst the yearly monsoon winds assisted trade, and the Swahili civilisation consisted of hundreds of settlements and linked the societies and kingdoms of the interior, such as those of the Zambezi basin and the Great Lakes, to the wider Indian Ocean trade. There is much debate around the chronology of the settlement of Madagascar, although most scholars agree that the island was further settled by Austronesian peoples from the 5th or 7th centuries AD who had proceeded through or around the Indian Ocean by outrigger boats, to also settle the Comoros. This second wave possibly found the island of Madagascar sparsely populated by descendants of the first wave a few centuries earlier, with the \"Vazimba\" of the interior's highlands being represented as primitive dwarves in Malagasy oral traditions and revered.\nThe wider region underwent a trade expansion from the 7th century, as the Swahili engaged in the flourishing Indian Ocean trade following the early Muslim conquests. Settlements further centralised and some major states included Gedi, Ungwana , Pate, Malindi, Mombasa, and Tanga in the north, Unguja Ukuu on Zanzibar, Kaole, Dar es Salaam, Kilwa, Kiswere, Monapo, Mozambique, and Angoche in the middle, and Quelimane, Sofala, Chibuene, and Inhambane in the south. Via mtumbwi, mtepe and later ngalawa they exported gold, iron, copper, ivory, slaves, , cotton cloth, wood, grain, and rice, and imported silk, glassware, jewellery, Islamic pottery, and Chinese porcelain. Relations between the states fluctuated and varied, with Mombasa, Pate, and Kilwa emerging as the strongest. This prosperity led some Arab and Persian merchants to settle and assimilate into the various societies, and from the 8th to the 14th century the region gradually Islamised due to the increased trading opportunities it brought, with some oral traditions having rulers of Arab or Persian descent.\nThe Kilwa Chronicle, supposedly based on oral tradition, holds that a Persian prince from Shiraz arrived and acquired the island of Kilwa from the local inhabitants, before quarrel with the Bantu king led to the severing Kilwa's land bridge to the mainland. Settlements in northern Madagascar such as Mahilaka, Irodo, and Iharana also engaged in the trade, attracting Arab immigration. Bantu migrated to Madagascar and the Comoros from the 9th century, when zebu were first brought. From the 10th century Kilwa expanded its influence, coming to challenge the dominance of Somalian Mogadishu located to its north, however details of Kilwa's rise remain scarce. In the late 12th century Kilwa wrestled control of Sofala in the south, a key trading city linking to Great Zimbabwe in the interior and famous for its Zimbabwean gold, which was substantial in the usurpation of Mogadishu's hegemony, while also conquering Pemba and Zanzibar. Kilwa's administration consisted of representatives who ranged from governing their assigned cities to fulfilling the role of ambassador in the more powerful ones. Meanwhile, the Pate Chronicle has Pate conquering Shanga, Faza, and prosperous Manda, and was at one time led by the popular Fumo Liyongo. The islands of Pemba, Zanzibar, Lamu, Mafia and the Comoros were further settled by Shirazi and grew in importance due to their geographical positions for trade.\nBy 1100, all regions of Madagascar were inhabited, although the total population remained small.48 Societies organised at the behest of \"hasina,\" which later evolved to embody kingship, and competed with one another over the island's estuaries, with oral histories describing bloody clashes and earlier settlers often pushed along the coast or inland. An Arab geographer wrote in 1224 that the island consisted of a great many towns and kingdoms, with kings making war on each other. Assisted by climate change, the peoples gradually transformed the island from dense forest to grassland for cultivation and zebu pastoralism. From the 13th century Muslim settlers arrived, integrating into the respective societies, and held high status owing to Islamic trading networks.\nWest Africa.\nThe western Sahel and Sudan.\nThe 7th to 13th centuries in West Africa were a period of relatively abundant rainfall that saw the explosive growth of trade, particularly across the Sahara desert, and the flourishing of numerous important states. The introduction of the camel to the western Sahel was a watershed moment, allowing more merchandise to move more easily. These desert-side states are the first to appear in the written record, with Arab and Berber merchants from North Africa leaving descriptions of their power and wealth. Nevertheless, there remain big gaps in the historical record, and many details are speculative and/or based on much later traditions.\nOne of the most powerful and well known of these states was Wagadu, commonly called the Ghana Empire, likely the dominant player in the western Sahel from the 6th century onwards. Wagadu was the most powerful of a constellation of states stretching from Takrur on the Senegal river valley to Mema in the Niger valley, all of whom were subservient to Ghana at least some of the time. Like Wagadu, the Gao Empire which rose in the 7th century had at least seven kingdoms accepting their suzerainty. Both Gao and Kumbi Saleh (capital of Wagadu) grew fabulously rich through the trans-Saharan trade routes linking these cities with Tadmekka, Kairouan, and Sijilmassa in North Africa along which flowed trade in salt, gold, slaves, and more.\nThe arrival of Islam in West Africa had seismic consequences for the history of the entire region. By the 10th century, the king of Gao had converted, possibly to Ibadi Islam. In 1035 king War Jabi of Takrur became the first ruler to adopt Sunni Islam. The rise of the Almoravid Sanhaja in the 1050s, perhaps inspired and supported by Muslims in Takrur, pushed the leaders of Sahelian states to institutionalize Islam in the subsequent decades. Historians debate whether the Almoravids conquered Wagadu or merely dominated them politically but not militarily. In any case the period saw significant upheaval and a shift in trade patterns as previously important cities like Awdaghost and Tadmekka fell victim to the Almoravids and their allies. In the confusion, some vassals achieved independence such as Mema, Sosso, and Diarra/Diafunu, with the last two being especially powerful. Despite Wagadu regaining full independence and power throughout the 12th century, this could not counteract the worsening climate and shifts in trade south and east. Around the turn of the 13th century, the Sosso Empire united the region and conquered a weakened Ghana from its south, spurring large-scale Soninke out-migration.\nSosso's Soumaoro Kante conquered Diarra, Gajaaga, and the Manding region. According to the oral Epic of Sundiata, Sundiata Keita, a Mandinka prince in exile, returned to Manden to save his people of the tyrannical Sosso king. Sundiata unified the Mandinka clans, allied with Mema, and defeated Soumaoro Kante at the Battle of Kirina in the early 13th century. He then proclaimed the \"Kouroukan Fouga\" of the nascent Mali Empire. Allied kingdoms, including Mema and Wagadu, retained leadership of their province, while conquered leaders were assigned a \"farin\" subordinate to the \"mansa\" (emperor), with provinces retaining a great deal of autonomy.\nIn addition to campaigns in the north to subdue Diafunu, Mali established suzerainty over the highlands of Fouta Djallon. After being insulted by the Wolof king of Kita, Sundiata sent Tiramakhan Traore west at the head of a large army, ultimately bringing most of Senegambia under the empire's control and, after defeating the Bainuk king, established dozens of Mandinka vassal kingdoms in the Gambia and Casamance basins, a region known as Kaabu.\nWithin the Niger bend and the forest region.\nWhile the precise timeline is unknown, archaeological evidence points to settlements in Ile-Ife being one of the earliest south of the Niger river, dating back as early as the 10th to 6th century BCE. The city gradually transitioned into a more urban center around the 4th to 7th centuries CE. By the 8th century, a powerful city-state had formed, laying the foundation for the eventual rise of the Ife Empire (circa 1200\u20131420).\nUnder figures like the now defied figures such as Oduduwa, revered as the first divine king of the Yoruba, the Ife Empire grew. Ile-Ife, its capital, rose to prominence, its influence extending across a vast swathe of what is now southwestern Nigeria.\nThe period between 1200 and 1400 is often referred to as the \"golden age\" of Ile-Ife, marked by exceptional artistic production, economic prosperity, and urban development. The city's artisans excelled in crafting exquisite sculptures from bronze, terracotta, and stone. These works, renowned for their naturalism and technical mastery, were not only objects of aesthetic appreciation but also likely held religious significance, potentially reflecting the cosmology and belief systems of the Ife people.\nThis artistic tradition coincided with Ile-Ife's role as a major commercial hub. The Ife Empire's strategic location facilitated its participation in extensive trade networks that spanned West Africa. Of note is the evidence of a thriving glass bead industry in Ile-Ife. Archaeological excavations have unearthed numerous glass beads, indicating local production and pointing to the existence of specialized knowledge and technology. These beads, particularly the dichroic beads known for their iridescent qualities, were highly sought-after trade items, found as far afield as the Sahel region, demonstrating the far-reaching commercial connections of the Ife Empire.\nCentral Africa.\nThe central Sahel and Cameroon.\nIn northern modern-day Nigeria, Hausa tradition holds that Bayajidda came to Daura in the 9th century, and his descendants founded the kingdoms of Daura, Kano, Rano, Katsina, Gobir, Kingdom of Zazzau, and Biram in the 10th, 11th, and 12th centuries, with his bastard descendants founding various others. While the historical validity of these legends is unknowable, the Arab geographer al-Yaqubi, writing in 872/873 CE (AH 259), describes a kingdom called \"HBShH\" with a city named \"ThBYR\" located between the Niger and the Kanem\u2013Bornu Empire which may refer to Hausa.\nThe Congo Basin.\nFollowing the Bantu migrations, a period of state and class formation began circa 700 with four centres; one in the west around Pool Malebo, one south around the highlands of Angola, a third north-central around Lake Mai-Ndombe, and a fourth in the far southeast in the Upemba Depression.\nIn the Upemba Depression social stratification and governance began to form after the 10th century based on villages.\nSouthern Africa.\nSouthern Great Lakes and the Zambezi and Limpopo basins.\nBy the 4th century, Bantu peoples had established farming villages south of the Zambezi River. The San, having inhabited the region for around 100,000 years, were driven off their ancestral lands or incorporated by Bantu speaking groups. The Zambezi Plateau came to be dotted with the agricultural chiefdoms of the Zhizo people and Leopard's Kopje people, in which cattle was the primary identifier of wealth. External trade began around the 7th century, primarily exporting gold and ivory.14 Around 900, motivated by the ivory trade, some Zhizo moved south to settle the Limpopo-Shashe Basin. Their capital and most populated settlement was Schroda, and via the coastal Swahili city-state Chibuene they engaged in the Indian Ocean trade.10\u201314\nThe 10th century saw increased global demand for gold as various Muslim, European, and Indian states began issuing gold coinage. Around 1000, some Leopard's Kopje people moved south to settle Bambandyanalo (known as K2), as the Zhizo moved west to settle Toutswe in modern-day Botswana. Some scholars believe their relations to have been hostile, however others insist they were more complex, both socially and politically. The San, who were believed to have closer connections to the old spirits of the land, were often turned to by other societies for rainmaking. The community at K2 chose the San rather than the Zhizo, their political rivals, because the San did not believe in ancestors, and by not acknowledging the Zhizo's ancestors they would not be held to ransom by them.\nNorthwest, the community at Mapela Hill had possibly developed sacral kingship by the 11th century. To the east, an early settlement was Gumanye. Great Zimbabwe was founded around 1000 AD, and construction on the city's iconic dry-stone walls began in the 11th century. From the 12th century Great Zimbabwe wrestled with other settlements, such as Chivowa, for economic and political dominance in the Southern Zambezi Escarpment. Further south by 1200, K2 had a population of 1500. The large wealth generated by the Indian Ocean trade created unprecedented inequalities, evolving over time from a society based on social ranking to one based on social classes. K2's spatial arrangement became unsuited to this development.30\nAmid a harsh drought which likely troubled the society, royal elites moved the capital to Mapungubwe Hill and settled its flat-topped summit around 1220, while most people settled below, surrounding the sacred leader in a protective circle. Mapungubwe Hill became the sole rainmaking hill, and its habitation by the leader emphasised a link between himself and rainmaking, which was substantial in the development of sacral kingship. The first king had their palace on the western part of the hill, and is called \"Shiriyadenga\" in Venda oral traditions. His entourage included soldiers and praise singers, along with musicians who played mbiras and xylophones. The state likely covered 30,000\u00a0km2 (12,000 square miles). They traded locally with Toutswe and Eiland among others. High global demand saw gold and ivory exported to the Indian Ocean trade via Sofala. It is unclear to what extent coercion and conflict played in Mapungubwe's growth and dominance due to this being challenging to recognise archaeologically, however the stone walls likely served a defensive purpose, indicating warfare was conventional.\nMedieval Africa (1250-1800).\nNorth Africa.\nNorthern Africa.\nc. 1250\u20131500.\nThe Ayyubids were in a precarious position. In 1248, the Christians began the Seventh Crusade with intent to conquer Egypt, but were decisively defeated by the embattled Ayyubids who had relied on Mamluk generals. The Ayyubid sultan attempted to alienate the victorious Mamluks, who revolted, killing him and seizing power in Egypt, with rule given to a military caste of Mamluks headed by the Bahri dynasty, whilst the remaining Ayyubid empire was destroyed in the Mongol invasions of the Levant. Following the Mongol Siege of Baghdad in 1258, the Mamluks re-established the Abbasid Caliphate in Cairo, and over the next few decades conquered the Crusader states and, assisted by civil war in the Mongol Empire, defeated the Mongols, before consolidating their rule over the Levant and Syria. To the west, the three dynasties vied for supremacy and control of the trans-Saharan trade.\nFollowing the collapse of the Abbasids, the Hafsids were briefly recognised as caliphs by the sharifs of Mecca and the Mamluks. Throughout the 14th century, the Marinids intermittently occupied the Zayyanids several times, and devastated the Hafsids in 1347 and 1357. The Marinids then succumbed to internal division, exacerbated by plague and financial crisis, culminating in the rise of the Wattasid dynasty from Zenata in 1472, with the Hafsids becoming the dominant power. Throughout the 15th century, the Spanish colonised the Canary Isles in the first example of modern settler colonialism, causing the genocide of the native Berber population in the process. To the east, the turn of the 15th century saw the Mamluks oppose the expansionist Ottomans and Timurids in the Middle East, with plague and famine eroding Mamlukian authority, until internal conflict was reconciled. The following decades saw the Mamluks reach their greatest extent with efficacious economic reforms, however the threat of the growing Ottomans and Portuguese trading practices in the Indian Ocean posed great challenges to the empire at the turn of the 16th century.\nEast Africa.\nHorn of Africa.\nc. 1250\u20131500.\nThe 13th century saw power balanced between the Zagwe dynasty, Sultanate of Shewa, and Kingdom of Damot, with the Ajuran Sultanate on the Horn's eastern coast.\nIn 1270, supported by the Kebra Nagast painting the Zagwe as illegitimate usurpers, Yekuno Amlak rebelled with assistance from Shewa and defeated the Zagwe king in battle, establishing the Solomonic dynasty of the nascent Ethiopian Empire.131 In accordance with the Kebra Negast, they claimed their descent from the last king of Aksum, and ultimately from Aksumite queen Makeda and the Israelite king Solomon. Fifteen years later, in the Sultanate of Shewa, which was exhausted following wars with Damot and suffering internal strife, was conquered by Umar Walasma of the Walashma dynasty, who established the Sultanate of Ifat.143 Over the following decades Ifat incorporated the polities of Adal, Gidaya, Bale, Mora, Hargaya, Hubat, and Fatagar among others. In the 13th century the Afar founded the Dankali Sultanate north of Ethiopia.\nIn Ethiopia Amda Seyon I came to the throne in 1314 and conquered Harla, Gojjam, Hadiya, and crucially Damot, with Ennarea splitting from the latter. He also campaigned in the north where Beta Israel had been gaining prominence, and reconquered the Tigrayan Enderta Province. In 1321, a religious dispute between Amda Seyon and the Mamluk sultan which involved threats to tamper with the Nile gave Ifat's Haqq ad-Din I pretext to invade and execute an Ethiopian envoy. Seven years later, Amda Seyon's forces overwhelmed Ifat's outposts, defeated Ifat's armies and killed Haqq ad-Din, with lack of unity among the Muslims proving fatal.\nThe Ethiopian emperor raided the Muslim states and made them tributaries. Following this, sultan Sabr ad-Din I led a rebellion and \"jihad\" in 1332 seeking to restore prestige and rule a Muslim Ethiopia, garnering widespread support in the early stages from the Muslim states and even from nomads.145 They were defeated by Amda Seyon, ushering in a golden age for the Ethiopian Empire. Ethiopia incorporated Ifat, Hadiya, Dawaro, Fatagar, and Shewa as one vassal headed by the Walashma dynasty. The Ethiopian emperor ruled the Muslim states by divide and rule, and had the final say on succession, with various sultans and sheikhs drawn to his court.148\nSuccessive sultans rebelled and struggled to shake off Ethiopian vassalage, moving Ifat's capital to Adal in an attempt to escape Ethiopia's sphere of control. To the south-west according to oral traditions, Amda Seyon expanded into the Gurage. According to oral traditions, the Kingdom of Kaffa was established in 1390 after \"ousting a dynasty of 32 kings\". In the late 14th century the sultans began to expand eastwards into the decentralised Somali interior. Sa'ad ad-Din II propagated insecurity on Ethiopia's eastern frontier, however was defeated by Dawit I. The sultan was repeatedly pursued by the Ethiopian emperor to Zeila on the coast and killed in 1415, leaving the former Sultanate of Ifat fully occupied.\nIn 1415 Sabr ad-Din III of the Walashma dynasty returned to the region from exile to establish the Adal Sultanate. The Ethiopian armies were defeated, and he and his successors expanded to regain the territory of the former sultanate. Jamal ad-Din II's reign saw a sharp rise in the slave trade, with India, Arabia, Hormuz, Hejaz, Egypt, Syria, Greece, Iraq, and Persia reportedly becoming \"full of Abyssinian slaves\".59 In 1445 Badlay attempted an invasion into the Ethiopian Highlands, supported by Mogadishu, however he was defeated by Zara Yaqob, with the successor sultan securing peace between the two states.\nIn the 1440s Ethiopia conquered much of the Tigray, placing the land under a vassal ruled by the Bahr Negus.71 Baeda Maryam I campaigned against the Dobe'a with the support of Dankalia, resulting in their defeat and incorporation into the empire. In 1471, a Harari emir leading a militant faction seized power in Adal with the sultan retaining a ceremonious role. His successor raided the Ethiopian frontier against the sultan's wishes, and was defeated by the emperors in 1507 and finally in 1517. For the Ethiopians, the end of the 15th century saw a period of conquest and expansion come to close, and one of defence begin.\nWest Africa.\nThe western Sahel and Sudan.\nc. 1250\u20131500.\nMali continued its expansion after the death of Sundiata. His son conquered Gajaaga and Takrur, and brought the key Saharan trading centres under his rule. The cessation of his reign culminated in a destructive civil war, only reconciled with a militaristic coup, after which Gao was conquered and the Tuareg subdued, cementing Mali's dominance over the trans-Saharan trade. In the 13th century Al-Hajj Salim Suwari, a Soninke Islamic scholar, pioneered the Suwarian tradition which sought to tolerate traditional religions, gaining popularity among West African Muslims. Mossi oral traditions tie the origins of the Mossi Kingdoms (located south of the Niger River) to the Mamprusi and Dagomba kingdoms in the forest regions, involving the Dagomba princess Yennenga.217, 224 Ouagadougou and Yatenga were the most powerful.\nIn 1312 Mansa Musa came to power in Mali after his predecessor had set out on an Atlantic voyage. Musa supposedly spent much of his early campaign preparing for his infamous \"hajj\" or pilgrimage to Mecca. Between 1324 and 1325 his entourage of over 10,000, and hundreds of camels, all carrying around 12 tonnes of gold in total, travelled 2700 miles, giving gifts to the poor along the way, and fostered good relations with the Mamluk sultan, garnering widespread attention in the Muslim world. On Musa's return, his general reasserted dominance over Gao and he commissioned a large construction program, building mosques and madrasas, with Timbuktu becoming a centre for trade and Islamic scholarship, however Musa features comparatively less than his predecessors in Mandinka oral traditions than in modern histories. Despite Mali's fame being attributed to its riches in gold, its prosperous economy was based on arable and pastoral farming, as well as crafts, and they traded commonly with the Akan, Dyula, and with Benin, Ife, and Nri in the forest regions.\nAmid a Malian mansa's attempt to coerce the empire back into financial shape after the lacklustre premiership of his predecessor, Mali's northwestern-most province broke away to form the Jolof Empire and the Serer kingdoms. Wolof tradition holds that the empire was founded by the wise Ndiadiane Ndiaye, and it later absorbed neighbouring kingdoms to form a confederacy of the Wolof kingdoms of Jolof, Cayor, Baol, and Waalo, and the Serer kingdoms of Sine and Saloum. In Mali after the death of Musa II in 1387, vicious conflict ensued within the Keita dynasty. In the 14th century Yatenga attacked and sacked Timbuktu and Oualata.80 The internal conflict weakened Mali's central authority. This provided an opportunity for the previously subdued Tuareg tribal confederations in the Sahara to rebel. Over the next few decades they captured the main trading cities of Timbuktu, Oualata, Nema, and possibly Gao, with some tribes forming the north-eastern Sultanate of Agadez, and with them all usurping Mali's dominance over the trans-Saharan trade.\nIn the 15th century, the Portuguese, following the development of the caravel, set up trading posts along the Atlantic coast, with Mali establishing formal commercial relations, and the Spanish soon following. In the early 15th century Diarra escaped Malian rule. Previously under Malian suzerainty and under pressure from the expansionist Jolof Empire, a Fula chief migrated to Futa Toro, founding Futa Kingui in the lands of Diarra circa 1450. Yatenga capitalised on Mali's decline and conquered Macina, and the old province of Wagadu. Meanwhile Gao, ruled by the Sonni dynasty, expanded, conquering Mema from Mali, and launched a \"jihad\" against Yatenga,81 in a struggle over the crumbling empire.\nCentral Africa.\nThe central Sahel.\nc. 1250-1500.\nIn northern Nigeria, the reign of Kano's Yaji I in the 14th century saw the introduction of Islam to the region via Wangara, and his conquest of Rano, after which the state continued to exist but never regained its sovereignty.\nWest Congo Basin.\nc. 1250\u20131500.\nBy the 13th century there were three main confederations of states in the western Congo Basin around Pool Malebo. The Seven Kingdoms of Kongo dia Nlaza, considered to be the oldest and most powerful, likely included Nsundi, Mbata, Mpangu, and possibly Kundi and Okanga. South of these was Mpemba which stretched from its capital in northern Angola 200\u00a0km north to the Congo River. It included various kingdoms such as Mpemba Kasi, its northernmost and remotest component, and Vunda. To its west across the Congo River was a confederation of three small states; Vungu (its leader), Kakongo, and Ngoyo.\nThe formation of the Kingdom of Kongo began in the late 13th century. Kongo oral traditions hold that Ntinu Wene (lit. \"King of the Kingdom\") crossed the Congo River from Vungu to conquer Mpemba Kasi, known as the \"Mother of Kongo\". The first kings ruled from Nsi a Kwilu, a valley and old religious centre, which produced iron and steel, and linked the copper and textile-producing north to the south. Around the 1350s Nimi Nzima established an alliance with the rulers of Mbata, who were looking to break away from the Seven Kingdoms, and agreed to secure each other's dynasties, making them known as the \"Grandfather of Kongo\".\nTradition holds that Nimi Nzima's son, Lukeni lua Nimi, wishing to aggrandise himself, built a fortress and blocked and taxed commerce. One day his pregnant aunt refused to pay the toll, and in a rage he killed her. While reprehensible, his action won him followers due to his determination and valour and allowed him to embark on conquests. To the south the market town of Mpangala, itself a sub unit of Vunda, was absorbed, with Vunda also styled as a Grandfather. This weakening of the Mpemba confederation precipitated its conquest and integration into the Kingdom of Kongo. Lukeni lua Nimi also conquered Kabunga in the west, whose leaders were regional religious leaders, not dissimilar from popes. From there Soyo and Mbamba were conquered.\nThe power and resources gained from these conquests allowed Kongo to expand north into Nsundi, which had multiple sub-units. Traditionally, a governor on Nsundi's western border forebode entry until they had fought a symbolic battle. Kongo conquered Nsundi and delegated it to a royal governor, who greatly expanded the territory, conquering Nsanga and Masinga. Northeast, Teke oral tradition holds that Mabiala Mantsi united the Bateke tribes, centralised his governance, and expanded using militaristic and diplomatic skill. Kongo's conquests eastward brought it into conflict with the formidable Teke Kingdom which halted their expansion. This expansion had primarily been done by allying and co-opting polities. By the late 15th century, Kongo had developed a new administrative system which would increase its centralisation, and after integrating Vunda, they set about conquering these polities and converting them into royal provinces.30\nSmall confederations, like Kisama, often put up spirited and successful resistance to either internal consolidation by aggressive components, or external conquest and integration.23 To the south around the highlands of Angola the Ambundu kingdoms of Ndongo and Matamba formed. The Dembos confederation sat between them and Kongo. Ndongo had come under tributary status to Kongo by the 16th century, and oral traditions collected in the 17th century hold their founder, Ngola Mussuri or Bumbambula, to be a blacksmith who came there from Kongo, and was elected king (\"Ngola\") due to his benevolence.57\nTo its east around Lake Mai-Ndombe, there emerged Mwene Muji, likely around 1400. Their 'empire' status is pending on further archaeological research. With a powerful riverine navy, they expanded along the Kasai, Lukenie, Kamtsha, Kwilu, and Wamba rivers, without venturing much into the interior, coming to dominate trade.\nIn the late 15th century, Kongo came into contact with the Portuguese. A Kongo delegation was invited to Lisbon in 1487, and relations were initially warm. A Portuguese priest mastered Kikongo and his input led to the baptism of Kongo's king and royal court. At the same time commercial relations developed. Trade in slaves was the most lucrative.52\nEast Congo Basin.\nc. 1250\u20131500.\nFurther southeast in the Upemba Depression, \"Lords of the land\" held priestly roles due to their special relationship with the spirits of the land and were widely recognised, holding sway over multiple villages and essentially ruling embryonic kingdoms. As lineages grew in size, authority was opportunistically incorporated diplomatically or by force, leading to the formation of states. Some of those of the southern savanna, such as the Luba-Katanga and Songye, had transitioned from being matrilineal to patrilineal by 1500, while others such as the Luba-Hemba and Chokwe remained so, making up the matrilineal belt. An early state formed between the Lualaba and Lomami rivers among the Luba-Katanga, around the 15th century, known as the Kingdom of Luba. Their oral traditions account their people's history and hold their first king, Nkongolo, as a conqueror.\nSouthern Africa.\nSouthern Great Lakes and the Zambezi and Limpopo basins.\nc. 1250-1500.\nBy 1250, Mapungubwe had a population of 5000, and produced textiles and ceramics. The centre of the settlement was the domain of men, and had an area for resolving disputes and making political decisions, while the outer zone was the domain of women, containing domestic complexes. The second king had their palace in the middle of the hill, and is called \"Tshidziwelele\" in Venda oral traditions. The king had many wives, with some living outside of the capital to help maintain the network of alliances. The economy was based on agriculture, and to make more productive use of the land, cattle (previously held as the primary identifier of wealth) were herded away from the capital and permitted to graze on other communities' land, forming social and political ties and increasing Mapungubwe's influence. A large amount of wealth was accumulated via tributes, which were paid in crops, animals, and sometimes rarer goods.163\nMeanwhile, at Great Zimbabwe agriculture and cattle played a key role in developing a vital social network, and served to \"enfranchise management of goods and services distributed as benefits within traditional political and social institutions\", while long-distance trade was crucial for the transformation of localised organisations into regional ones. This process rapidly advanced during the 13th century, which saw large dry masonry stone walls raised, and by 1250 Great Zimbabwe had become an important trade centre.\nThe events around Mapungubwe's collapse are unknown. It is plausible confidence was lost in the leadership amid the deepening material and spiritual divide between commoners and the king, and a breakdown in common purpose, provoking people to \"vote with their feet\". By 1300, trade routes had shifted north as merchants bypassed the Limpopo and Mapungubwe by travelling the Save River into the gold-producing interior, precipitating Mapungubwe's rapid decline and the dominance of Great Zimbabwe. Mapungubwe was abandoned as people scattered northwest and south. They didn't regroup.55\nGreat Zimbabwe's wealth was derived from cattle rearing, agriculture, and the domination of trade routes from the goldfields of the Zimbabwean Plateau to the Swahili coast. The kingdom taxed other rulers throughout the region and was composed of over 150 smaller zimbabwes, and likely covered 50,000\u00a0km2.7 The large cattle herd that supplied the city moved seasonally and was managed by the court, and salt, cattle, grain, and copper were traded as far north as the Kundelungu Plateau in present-day DR Congo.17 At Great Zimbabwe's centre was the Great Enclosure which housed royalty and had demarcated spaces for rituals. Commoners' homes were built out of mud on wooden frame structures, and within the second perimeter wall they surrounded the royalty. The institutionalisation of Great Zimbabwe's politico-religious ideology served to legitimise the position of the king (\"mambo\"), with a link between leaders, their ancestors, and God. The community incorporated dhaka pits into a complex water management system.\nAs with Mapungubwe, it is unclear to what extent coercion and conflict facilitated Great Zimbabwe's dominance. While the Great Enclosure served to display prestige and status, and to reinforce inequalities between elites and commoners, it likely also served to deter contestation for political power amid the close linkage between wealth accumulation and political authority, with rivals for power, such as district chiefs and regional governors, located outside the settlement in prestige enclosures. The perimeter walls also likely served a defensive purpose, indicating warfare was conventional.\nIt is unclear what caused Great Zimbabwe's decline. Shona oral tradition attributes Great Zimbabwe's demise to a salt shortage, which may be a figurative way of speaking of land depletion for agriculturalists or of the depletion of critical resources for the community.10 It is plausible the aquifer Great Zimbabwe sat on top of ran out of water, or the growing population contaminated the water. From the early 15th century, international trade began to decline amid a global economic downturn, reducing demand for gold, which adversely affected Great Zimbabwe. In response to this, elites expanded regional trading networks, resulting in greater prosperity for other settlements in the region.\nBy the late 15th century, the consequences of this decision began to manifest, as offshoots from Great Zimbabwe's royal family formed new dynasties, possibly as a result of losing succession disputes. According to oral tradition, Nyatsimba Mutota, a member of Great Zimbabwe's royal family, led part of the population north in search for salt to found the Mutapa Empire. It was believed that only their most recent ancestors would follow them, with older ancestors staying at Great Zimbabwe and providing protection there. Mutota is said to have found salt in the lands of the Tavara,204 and settled around the Ruya-Mazowe Basin, conquering and incorporating the pre-existing chiefdoms to control agricultural production and strategic resources. This placed the state at a key position in the gold and ivory trade. Angoche traders opened a new route along the Zambezi via Mutapa and Ingombe Ilede to reach the goldfields west of Great Zimbabwe, precipitating its decline and the rise of Khami (previously a Leopard's Kopje's chiefdom located close to the goldfields), the capital of the Kingdom of Butua.50 Butua's first \"mambo\" was Madabhale of the Torwa dynasty, who had the praise name \"Chibundule\" (meaning \"sounding of the war horn\").\nIn Mutapa, Mutota's son and successor, Nyanhewe Matope, moved the capital to Mount Fura and extended this new kingdom into an empire encompassing most of the lands between Tavara and the Indian Ocean. Matope's armies overran the Manyika and Tonga as well as the coastal Teve and Madanda. Meanwhile, Butua rapidly grew in size and wealth, and came to border the Mutapa Empire along the Sanyati River. There appear to have intermarriages between the Nembire dynasty of Mutapa and the Torwa dynasty of Butua. According to oral traditions, Changamire was likely a descendant of both dynasties. He had been appointed governor (\"amir\") of the southern portion of the Mutapa Empire (\"Guruhuswa\").46 In 1490, Changamire I rebelled against the \"Mwenemutapa\", his elder brother Nyahuma, and deposed him, reportedly with help from the Torwa. He ruled Mutapa for four years until he was killed by the rightful heir to the throne, reportedly his nephew. His son Changamire II continued the conflict,54 ruling the southern portion which broke away from the Mutapa Empire.46 Whether this breakaway state maintained independence or came back under the rule of the \"Mwenemutapa\" is unclear, as we don't hear of the Changamire dynasty again until the 17th century.54\nEarly Modern Africa (1800-1935).\nBetween 1878 and 1898, European states partitioned and conquered most of Africa. For 400 years, European nations had mainly limited their involvement to trading stations on the African coast, with few daring to venture inland. The Industrial Revolution in Europe produced several technological innovations which assisted them in overcoming this 400-year pattern. One was the development of repeating rifles, which were easier and quicker to load than muskets. Artillery was being used increasingly. In 1885, Hiram S. Maxim developed the maxim gun, the model of the modern-day machine gun. European states kept these weapons largely among themselves by refusing to sell these weapons to African leaders.\nAfrican germs took numerous European lives and deterred permanent settlements. Diseases such as yellow fever, sleeping sickness, yaws, and leprosy made Africa a very inhospitable place for Europeans. The deadliest disease was malaria, endemic throughout Tropical Africa. In 1854, the discovery of quinine and other medical innovations helped to make conquest and colonization in Africa possible.\nThere were strong motives for conquest of Africa. Raw materials were needed for European factories. Prestige and imperial rivalries were at play. Acquiring African colonies would show rivals that a nation was powerful and significant. These contextual factors forged the Scramble for Africa.\nIn the 1880s the European powers had carved up almost all of Africa (only Ethiopia and Liberia were independent). The Europeans were captivated by the philosophies of eugenics and Social Darwinism, and some attempted to justify all this by branding it civilising missions. Traditional leaders were incorporated into the colonial regimes as a form of indirect rule to extract human and natural resources and curb organized resistance. Colonial borders were drawn unilaterally by the Europeans, often cutting across bonds of kinship, language, culture, and established routes, and sometimes incorporating groups who previously had little in common. The threat to trade routes was mitigated by poor policing and African entrepreneurs (viewed as smugglers) who exploited the differing tax and legal schemes.\nContemporary Africa (1935-present).\nImperialism ruled until after World War II when forces of African nationalism grew stronger. In the 1950s and 1960s the colonial holdings became independent states. The process was usually peaceful but there were several long bitter bloody civil wars, as in Algeria, Kenya, and elsewhere. Across Africa the powerful new force of nationalism drew upon the advanced militaristic skills that natives learned during the world wars serving in the British, French, and other armies. It led to organizations that were not controlled by or endorsed by either the colonial powers nor the traditional local power structures who were viewed as collaborators. Nationalistic organizations began to challenge both the traditional and the new colonial structures, and finally displaced them. Leaders of nationalist movements took control when the European authorities evacuated; many ruled for decades or until they died. In recent decades, many African countries have undergone the triumph and defeat of nationalistic fervour, changing in the process the loci of the centralizing state power and patrimonial state.\nThe wave of decolonization of Africa started with Libya in 1951, although Liberia, South Africa, Egypt and Ethiopia were already independent. Many countries followed in the 1950s and 1960s, with a peak in 1960 with the Year of Africa, which saw 17 African nations declare independence, including a large part of French West Africa. Most of the remaining countries gained independence throughout the 1960s, although some colonizers (Portugal in particular) were reluctant to relinquish sovereignty, resulting in bitter wars of independence which lasted for a decade or more.\nThe last African countries to gain formal independence were Guinea-Bissau (1974), Mozambique (1975) and Angola (1975) from Portugal; Djibouti from France in 1977; Zimbabwe from the United Kingdom in 1980; and Namibia from South Africa in 1990. Eritrea later split off from Ethiopia in 1993. The nascent countries, despite some prior talk of redrawing borders, decided to keep their colonial borders in the Organisation of African Unity (OAU) conference of 1964 due to fears of civil wars and regional instability, and placed emphasis on Pan-Africanism, with the OAU later developing into the African Union. During the 1990s and early 2000s there were the First and Second Congo Wars, often termed the African World Wars.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14104", "revid": "1319207184", "url": "https://en.wikipedia.org/wiki?curid=14104", "title": "History of Oceania", "text": "The history of Oceania includes the history of Australia, Easter Island, Fiji, Hawaii, New Zealand, Papua New Guinea, Western New Guinea and other Pacific island nations.\nPrehistory.\nThe prehistory of Oceania is divided into the prehistory of each of its major areas: Australia, Melanesia, Micronesia, and Polynesia, and these vary greatly as to when they were first inhabited by humans \u2014 from 70,000 years ago (Near Oceania) to 3,000 years ago (Remote Oceania).\nAustralia.\nIndigenous Australians are the original inhabitants of the Australian continent and nearby islands. Indigenous Australians migrated from Africa to Asia around 70,000 years ago and arrived in Australia around 50,000 years ago. The Torres Strait Islanders are indigenous to the Torres Strait Islands, which are at the northernmost tip of Queensland near Papua New Guinea. The term \"Aboriginal\" is traditionally applied to only the indigenous inhabitants of mainland Australia and Tasmania, along with some of the adjacent islands, i.e.: the \"first peoples\". \"Indigenous Australians\" is an inclusive term used when referring to both Aboriginal and Torres Strait islanders.\nThe earliest definite human remains found to date are that of Mungo Man, which have been dated at about 40,000 years old, but the time of arrival of the ancestors of Indigenous Australians is a matter of debate among researchers, with estimates dating back as far as 125,000 years ago. There is great diversity among different Indigenous communities and societies in Australia, each with its own unique mixture of cultures, customs and languages. In present-day Australia these groups are further divided into local communities.\nMelanesia.\nThe first settlers of Australia, New Guinea, and the large islands just to the east arrived between 50,000 and 30,000 years ago, when Neanderthals still roamed Europe. The original inhabitants of the group of islands now named Melanesia were likely the ancestors of the present-day Papuan-speaking people. Migrating from Southeast Asia, they appear to have occupied these islands as far east as the main islands in the Solomon Islands (archipelago), including Makira and possibly the smaller islands farther to the east.\nParticularly along the north coast of New Guinea and in the islands north and east of New Guinea, the Austronesian peoples, who had migrated into the area somewhat more than 3,000 years ago, came into contact with these pre-existing populations of Papuan-speaking peoples. In the late 20th century, some academics proposed a long period of interaction that led to numerous complex changes in the peoples' genetics, languages, and cultures. Kayser, et al. proposed that, from this area, a very small group of people (speaking an Austronesian language) departed to the east to become the forebears of the Polynesian people.\nHowever, the theory is contradicted by the findings of a genetic study published by Temple University in 2008; based on genome scans and evaluation of more than 800 genetic markers among a wide variety of Pacific peoples, it found that neither Polynesians nor Micronesians have much genetic relation to Melanesians. Both groups are strongly related genetically to East Asians, particularly Taiwanese aborigines. It appeared that, having developed their sailing outrigger canoes, the Polynesian ancestors migrated from East Asia, moved through the Melanesian area quickly on their way, and kept going to eastern areas, where they settled. They left little genetic evidence in Melanesia.\nThe study found a high rate of genetic differentiation and diversity among the groups living within the Melanesian islands, with the peoples distinguished by island, language, topography, and geography among the islands. Such diversity developed over their tens of thousands of years of settlement before the Polynesian ancestors ever arrived at the islands. For instance, populations developed differently in coastal areas, as opposed to those in more isolated mountainous valleys.\nAdditional DNA analysis has taken research into new directions, as more human species have been discovered since the late 20th century. Based on his genetic studies of the Denisova hominin, an ancient human species discovered in 2010, Svante P\u00e4\u00e4bo claims that ancient human ancestors of the Melanesians interbred in Asia with these humans. He has found that people of New Guinea share 4\u20136% of their genome with the Denisovans, indicating this exchange. The Denisovans are considered cousin to the Neanderthals; both groups are now understood to have migrated out of Africa, with the Neanderthals going into Europe, and the Denisovans heading east about 400,000 years ago. This is based on genetic evidence from a fossil found in Siberia. The evidence from Melanesia suggests their territory extended into south Asia, where ancestors of the Melanesians developed.\nMelanesians of some islands are one of the few non-European peoples, and the only dark-skinned group of people outside Australia, known to have blond hair.\nMicronesia.\nMicronesia began to be settled several millennia ago, although there are competing theories about the origin and arrival of the first settlers. There are numerous difficulties with conducting archaeological excavations in the islands, due to their size, settlement patterns and storm damage. As a result, much evidence is based on linguistic analysis. The earliest archaeological traces of civilization have been found on the island of Saipan, dated to 1500 BCE or slightly before.\nThe ancestors of the Micronesians settled there over 4,000 years ago. A decentralized chieftain-based system eventually evolved into a more centralized economic and religious culture centered on Yap and Pohnpei. The prehistory of many Micronesian islands such as Yap are not known very well.\nOn Pohnpei, pre-colonial history is divided into three eras: \"Mwehin Kawa\" or \"Mwehin Aramas\" (Period of Building, or Period of Peopling, before c. 1100); \"Mwehin Sau Deleur\" (Period of the Lord of Deleur, c. 1100 to c. 1628); and \"Mwehin Nahnmwarki\" (Period of the Nahnmwarki, c. 1628 to c. 1885). Pohnpeian legend recounts that the Saudeleur rulers, the first to bring government to Pohnpei, were of foreign origin. The Saudeleur centralized form of absolute rule is characterized in Pohnpeian legend as becoming increasingly oppressive over several generations. Arbitrary and onerous demands, as well as a reputation for offending Pohnpeian deities, sowed resentment among Pohnpeians. The Saudeleur Dynasty ended with the invasion of Isokelekel, another semi-mythical foreigner, who replaced the Saudeleur rule with the more decentralized \"nahnmwarki\" system in existence today. Isokelekel is regarded as the creator of the modern Pohnpeian \"nahnmwarki\" social system and the father of the Pompeian people.\nConstruction of Nan Madol, a megalithic complex made from basalt lava logs in Pohnpei began as early as 1200 CE. Nan Madol is offshore of Temwen Island near Pohnpei, consists of a series of small artificial islands linked by a network of canals, and is often called the \"Venice of the Pacific\". It is located near the island of Pohnpei and was the ceremonial and political seat of the Saudeleur dynasty that united Pohnpei's estimated 25,000 people until its centralized system collapsed amid the invasion of Isokelekel. Isokelekel and his descendants initially occupied the stone city, but later abandoned it.\nThe first people of the Northern Mariana Islands navigated to the islands at some period between 4000 BCE to 2000 BCE from Southeast Asia. They became known as the Chamorros, and spoke an Austronesian language called Chamorro. The ancient Chamorro left a number of megalithic ruins, including Latte stone. The Refaluwasch or Carolinian people came to the Marianas in the 1800s from the Caroline Islands. Micronesian colonists gradually settled the Marshall Islands during the 2nd millennium BCE, with inter-island navigation made possible using traditional stick charts.\nPolynesia.\nLinguistic, archaeological, and human genetic evidence identifies the Polynesians as a subset of the sea-migrating Austronesian peoples, and tracing Polynesian languages places their prehistoric origins in the Malay Archipelago, and ultimately, in Taiwan. Between about 3000 and 1000 BCE speakers of Austronesian languages began spreading from Taiwan into Maritime Southeast Asia, as tribes thought to have travelled via South China about 8,000 years ago to the edges of western Micronesia and on into Melanesia, although they differ from the Han Chinese who now comprise the majority of people in China and Taiwan. There are three theories regarding the prehistoric spread of humans across the Pacific to Polynesia. Kayser \"et al.\" (2000) outline these well: \nIn the archaeological record there are well-defined traces of this expansion, allowing researchers to follow and date the path it took with some certainty. It is thought that by roughly 1400 BCE, \"Lapita peoples\" (so-named after their pottery tradition) appeared in the Bismarck Archipelago of northwest Melanesia. This culture is seen as having adapted and evolved through time and space since its emergence \"Out of Taiwan\". The Lapita people had given up rice production, for instance, after encountering and adapting to breadfruit in the Bird's Head area of New Guinea. In the end, the most eastern site for Lapita archaeological remains recovered so far has been through work on the archaeology in Samoa. The site is at Mulifanua on Upolu. The Mulifanua site, where 4,288 pottery shards have been found and studied, has a \"true\" age of c. 1000 BCE based on C14 dating. A 2010 study places the beginning of the human archaeological sequences of Polynesia in Tonga at 900 BCE, the small differences in dates with Samoa being due to differences in radiocarbon-dating technologies between 1989 and 2010, the Tongan site apparently predating the Samoan site by some few decades in real time.\nWithin a mere three or four centuries between about 1300 and 900 BCE, the Lapita archaeological culture spread 6,000 kilometres eastwards from the Bismarck Archipelago, until it reached as far as Fiji, Tonga, and Samoa. The area of Tonga, Fiji, and Samoa served as a gateway into the rest of the Pacific region now known as Polynesia. Ancient Tongan mythologies, as recorded by early European explorers, report the islands of 'Ata and Tongatapu as the first islands hauled to the surface from the deep ocean by Maui.\nThe \"Tu\u02bbi Tonga Empire\" or \"Tongan Empire\" in Oceania are descriptions sometimes given to Tongan expansionism and projected hegemony dating back to 950 CE, but at its peak during the period 1200\u20131500. While modern researchers and cultural experts attest to widespread Tongan influence and evidences of transoceanic trade and exchange of material and non-material cultural artifacts, empirical evidence of a \"political\" empire ruled for any length of time by successive rulers is lacking.\nModern archeology, anthropology and linguistic studies confirm widespread Tongan cultural influence ranging widely through East 'Uvea, Rotuma, Futuna, Samoa and Niue, parts of Micronesia (Kiribati, Pohnpei), Vanuatu, and New Caledonia and the Loyalty Islands, and while some academics prefer the term \"maritime chiefdom\", others argue that, while very different from examples elsewhere, \"...\"empire\" is probably the most convenient term.\"\nPottery art from Fijian towns shows that Fiji was settled before or around 3500 to 1000\u00a0BC, although the details of Pacific migration remain vague. It is believed that the Lapita people or the ancestors of the Polynesians settled the islands first but not much is known of what became of them after the Melanesians arrived; they may have had some influence on the new culture, and archaeological evidence shows that Polynesians would have then moved on to Tonga, Samoa, and even Hawai'i.\nThe first settlements in Fiji were started by voyaging traders and settlers from the west about 5000 years ago. Lapita pottery shards have been found at numerous excavations around the country. Aspects of Fijian culture are similar to the Melanesian culture of the western Pacific but have a stronger connection to the older Polynesian cultures. Stretching across from east to west, Fiji has been a nation of many languages. Fiji's history was one of settlement but also of mobility.\nOver the centuries, a unique Fijian culture developed. Constant warfare and cannibalism between warring tribes were quite rampant and very much part of everyday life. In later centuries, the reputation of the cannibal lifestyle deterred European sailors from going near Fijian waters, and Fiji acquired the name \"Cannibal Isles\"; as a result, Fiji remained unknown to the rest of the world.\nEarly European visitors to Easter Island recorded local oral traditions about the original settlers. In these traditions, Easter Islanders claimed that a chief Hotu Matu\ua78ca arrived on the island in one or two large canoes with his wife and extended family. They are believed to have been Polynesian. There is considerable uncertainty about the accuracy of this legend as well as about the date of settlement. Published literature suggests the island was settled around 300\u2013400 CE, or at about the time of the arrival of the earliest settlers in Hawaii.\nSome scientists say that Easter Island was not inhabited until 700\u2013800 CE. This date-range is based on glottochronological calculations and on three radiocarbon dates from charcoal that appears to have been produced during forest-clearance activities.\nMoreover, a recent study which included radiocarbon dates from what is thought to be very early material suggests that the island was settled as recently as 1200 CE. This seems to be supported by a 2006 study of the island's deforestation, which could have started around the same time. A large, now extinct, palm, \"Paschalococos disperta\" (related to the Chilean wine palm \"(Jubaea chilensis)\"), was one of the dominant trees as attested by fossil evidence; this species, unique to Easter Island, became extinct due to deforestation by the early Polynesian settlers.\nEuropean contact and exploration (1500s\u20131700s).\nIberian pioneers.\nEarly Iberian exploration.\nOceania was first explored by Europeans from the 16th century onwards. Portuguese navigators, between 1512 and 1526, reached the Moluccas (by Ant\u00f3nio de Abreu and Francisco Serr\u00e3o in 1512), Timor, the Aru Islands (Martim A. Melo Coutinho), the Tanimbar Islands, some of the Caroline Islands (by Gomes de Sequeira in 1525), and west Papua New Guinea (by Jorge de Menezes in 1526). In 1519 a Castilian ('Spanish') expedition led by Ferdinand Magellan sailed down the east coast of South America, found and sailed through the strait that bears his name and on 28 November 1520 entered the ocean which he named \"Pacific\". The three remaining ships, led by Magellan and his captains Duarte Barbosa and Jo\u00e3o Serr\u00e3o, then sailed north and caught the trade winds which carried them across the Pacific to the Philippines where Magellan was killed. One surviving ship led by Juan Sebasti\u00e1n Elcano returned west across the Indian Ocean and the other went north in the hope of finding the westerlies and reaching Mexico. Unable to find the right winds, it was forced to return to the East Indies. The Magellan-Elcano expedition achieved the first circumnavigation of the world and reached the Philippines, the Mariana Islands, and other islands of Oceania.\nOther large expeditions.\nFrom 1527 to 1595 a number of other large Spanish expeditions crossed the Pacific Ocean, leading to the discovery of the Marshall Islands and Palau in the North Pacific, as well as Tuvalu, the Marquesas, the Solomon Islands archipelago, the Cook Islands and the Admiralty Islands in the South Pacific.\nIn 1565, Spanish navigator Andr\u00e9s de Urdaneta found a wind system that would allow ships to sail eastward from Asia, back to the Americas. From then until 1815 the annual Manila galleons crossed the Pacific from Mexico to the Philippines and back, in the first transpacific trade route in history. Combined with the Spanish Atlantic or West Indies Fleet, the Manila galleons formed one of the first global maritime exchange in human history, linking Seville in Spain with Manila in the Philippines, via Mexico.\nLater, in the quest for Terra Australis, Spanish explorers in the 17th century discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Lu\u00eds Vaz de Torres. In 1668, the Spaniards founded a colony on Guam as a resting place for west-bound galleons. For a long time this was the only non-coastal European settlement in the Pacific.\nOceania during the Golden Age of Dutch exploration and discovery.\nEarly Dutch exploration.\nThe Dutch were the first non-natives to undisputedly explore and chart coastlines of Australia, Tasmania, New Zealand, Tonga, Fiji, Samoa, and Easter Island. Verenigde Oostindische Compagnie (or VOC) was a major force behind the (category; c. 1590s\u20131720s) and Netherlandish cartography (c. 1570s\u20131670s). In the 17th century, the VOC's navigators and explorers charted almost three-quarters of the Australian coastline, except the east coast.\nAbel Tasman's exploratory voyages.\nAbel Tasman was the first known European explorer to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands. His navigator Fran\u00e7ois Visscher, and his merchant Isaack Gilsemans, mapped substantial portions of Australia, New Zealand, Tonga and the Fijian islands.\nOn 24 November 1642 Abel Tasman sighted the west coast of Tasmania, north of Macquarie Harbour. He named his discovery Van Diemen's Land after Antonio van Diemen, Governor-General of the Dutch East Indies. then claimed formal possession of the land on 3 December 1642.\nAfter some exploration, Tasman had intended to proceed in a northerly direction but as the wind was unfavourable he steered east. On 13 December they sighted land on the north-west coast of the South Island, New Zealand, becoming the first Europeans to do so. Tasman named it \"Staten Landt\" on the assumption that it was connected to an island (Staten Island, Argentina) at the south of the tip of South America. Proceeding north and then east, he stopped to gather water, but one of his boats was attacked by M\u0101ori in a double hulled waka (canoes) and four of his men were attacked and killed by mere. As Tasman sailed out of the bay he was again attacked, this time by 11 waka. The waka approached the Zeehan which fired and hit one M\u0101ori who fell down. Canister shot hit the side of a waka.\nArcheological research has shown the Dutch had tried to land at a major agricultural area, which the M\u0101ori may have been trying to protect. Tasman named the bay \"Murderers' Bay\" (now known as Golden Bay) and sailed north, but mistook Cook Strait for a bight (naming it \"Zeehaen's Bight\"). Two names he gave to New Zealand landmarks still endure, Cape Maria van Diemen and Three Kings Islands, but \"Kaap Pieter Boreels\" was renamed by Cook 125 years later to Cape Egmont.\nEn route back to Batavia, Tasman came across the Tongan archipelago on 20 January 1643. While passing the Fiji islands, Tasman's ships came close to being wrecked on the dangerous reefs of the northeastern part of the Fiji group. He charted the eastern tip of Vanua Levu and Cikobia before making his way back into the open sea. He eventually turned northwest to New Guinea, and arrived at Batavia on 15 June 1643. For over a century after Tasman's voyages, until the era of James Cook, Tasmania and New Zealand were not visited by Europeans\u2014mainland Australia was visited, but usually only by accident.\nBritish exploration and Captain James Cook's voyages.\nFirst voyage (1768\u20131771).\nIn 1766 the Royal Society engaged James Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. The expedition sailed from England on 26 August 1768, rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of \"Terra Australis\".\nWith the help of a Tahitian named Tupaia, who had extensive knowledge of Pacific geography, Cook managed to reach New Zealand on 6 October 1769, leading only the second group of Europeans to do so (after Abel Tasman over a century earlier, in 1642). Cook mapped the complete New Zealand coastline, making only some minor errors (such as calling Banks Peninsula an island, and thinking Stewart Island / Rakiura was a peninsula of the South Island). He also identified Cook Strait, which separates the North Island from the South Island, and which Tasman had not seen.\nCook then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline. On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: \"\"\u2026and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not\".\" On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.\nAfter his departure from Botany Bay he continued northwards. After a grounding mishap on the Great Barrier Reef, the voyage continued, sailing through Torres Strait before returning to England via Batavia, the Cape of Good Hope, and Saint Helena.\nSecond voyage (1772\u20131775).\nIn 1772 the Royal Society commissioned Cook to search for the hypothetical Terra Australis again. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed by the Royal Society to lie further south.\nCook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, \"Resolution\" and \"Adventure\" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with M\u0101ori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71\u00b010'S on 31 January 1774.\nCook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.\nBefore returning to England, Cook made a final sweep across the South Atlantic from Cape Horn. He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.\nThird voyage (1776\u20131779).\nOn his last voyage, Cook again commanded HMS \"Resolution\", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a North-West Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to visit the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the \"Sandwich Islands\" after the fourth Earl of Sandwich\u2014the acting First Lord of the Admiralty.\nFrom the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.\nCook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the \"Makahiki\", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS \"Resolution\", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.\nAfter a month's stay, Cook resumed his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the \"Resolution\"'s foremast broke, so the ships returned to Kealakekua Bay for repairs. Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians. On 14 February 1779, at Kealakekua Bay, some Hawaiians took one of Cook's small boats. As thefts were quite common in Tahiti and the other islands, Cook would have taken hostages until the stolen articles were returned. He attempted to take as hostage the King of Hawai\u02bbi, Kalani\u02bb\u014dpu\u02bbu. The Hawaiians prevented this, and Cook's men had to retreat to the beach. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. Hawaiian tradition says that he was killed by a chief named Kalaimanokaho\u02bbowaha or Kana\u02bbina. The Hawaiians dragged his body away. Four of Cook's men were also killed and two others were wounded in the confrontation.\nThe esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.\nClerke assumed leadership of the expedition. Following the death of Clerke, \"Resolution\" and \"Discovery\" returned home in October 1780 commanded by John Gore, a veteran of Cook's first voyage, and Captain James King. After their arrival in England, King completed Cook's account of the voyage.\nColonialism.\nBritish colonialism.\nIn 1789 the Mutiny on the Bounty against William Bligh led to several of the mutineers escaping the Royal Navy and settling on Pitcairn Islands, which later became a British colony. Britain also established colonies in Australia in 1788, New Zealand in 1840 and Fiji in 1872, with much of Oceania being annexed by the British Empire.\nThe Gilbert Islands (now known as Kiribati) and the Ellice Islands (now known as Tuvalu) came under Britain's sphere of influence in the late 19th century. The Ellice Islands were administered as British protectorate by a Resident Commissioner from 1892 to 1916 as part of the British Western Pacific Territories (BWPT), and later as part of the Gilbert and Ellice Islands colony from 1916 to 1974.\nAmong the last islands in Oceania to be colonised was Niue (1900). In 1887, King Fata-a-iki, who reigned Niue from 1887 to 1896, offered to cede sovereignty to the British Empire, fearing the consequences of annexation by a less benevolent colonial power. The offer was not accepted until 1900. Niue was a British protectorate, but the UK's direct involvement ended in 1901 when New Zealand annexed the island.\nFrench colonialism.\nFrench Catholic missionaries arrived on Tahiti in 1834; their expulsion in 1836 caused France to send a gunboat in 1838. In 1842, Tahiti and Tahuata were declared a French protectorate, to allow Catholic missionaries to work undisturbed. The capital of Papeet\u0113 was founded in 1843. In 1880, France annexed Tahiti, changing the status from that of a protectorate to that of a colony.\nOn 24 September 1853, under orders from Napoleon III, Admiral Febvrier Despointes took formal possession of New Caledonia and Port-de-France (Noum\u00e9a) was founded 25 June 1854. A few dozen free settlers settled on the west coast in the following years. New Caledonia became a penal colony, and from the 1860s until the end of the transportations in 1897, about 22,000 criminals and political prisoners were sent to New Caledonia, among them many Communards, including Henri de Rochefort and Louise Michel. Between 1873 and 1876, 4,200 political prisoners were \"relegated\" in New Caledonia. Only forty of them settled in the colony, the rest returned to France after being granted amnesty in 1879 and 1880.\nIn the 1880s, France claimed the Tuamotu Archipelago, which formerly belonged to the P\u014dmare Dynasty, without formally annexing it. Having declared a protectorate over Tahuata in 1842, the French regarded the entire Marquesas Islands as French. In 1885, France appointed a governor and established a general council, thus giving it the proper administration for a colony. The islands of Rimatara and R\u016brutu unsuccessfully lobbied for British protection in 1888, so in 1889 they were annexed by France. Postage stamps were first issued in the colony in 1892. The first official name for the colony was \"\u00c9tablissements de l'Oc\u00e9anie\" (Settlements in Oceania); in 1903 the general council was changed to an advisory council and the colony's name was changed to \"\u00c9tablissements Fran\u00e7ais de l'Oc\u00e9anie\" (French Settlements in Oceania).\nSpanish colonialism.\nThe Spanish explorer Alonso de Salazar landed in the Marshall Islands in 1529. They were later named by Krusenstern, after English explorer John Marshall, who visited them together with Thomas Gilbert in 1788, en route from Botany Bay to Canton (two ships of the First Fleet).\nIn November 1770, Felipe Gonz\u00e1lez de Ahedo commanded an expedition from the Viceroyalty of Peru that searched for Davis Land and Madre de Dios Island and looked for foreign naval activities.\nThis expedition landed on \"Isla de San Carlos\" (Easter Island) and signed a treaty of annexation with Rapa Nui king Atamu Tekena.\nDutch colonialism.\nIn 1606 Lu\u00eds Vaz de Torres explored the southern coast of New Guinea from Milne Bay to the Gulf of Papua including Orangerie Bay which he named \"Bah\u00eda de San Lorenzo\". His expedition also discovered Basilaki Island naming it \"Tierra de San Buenaventura\", which he claimed for Spain in July 1606. On 18 October his expedition reached the western part of the island in present-day Indonesia, and also claimed the territory for the King of Spain.\nA successive European claim occurred in 1828, when the Netherlands formally claimed the western half of the island as Dutch New Guinea. In 1883, following a short-lived French annexation of New Ireland, the British colony of Queensland annexed south-eastern New Guinea. However, the Queensland government's superiors in the United Kingdom revoked the claim, and (formally) assumed direct responsibility in 1884, when Germany claimed north-eastern New Guinea as the protectorate of German New Guinea (also called Kaiser-Wilhelmsland).\nThe first Dutch government posts were established in 1898 and in 1902: Manokwari on the north coast, Fak-Fak in the west and Merauke in the south at the border with British New Guinea. The German, Dutch and British colonial administrators each attempted to suppress the still-widespread practices of inter-village warfare and headhunting within their respective territories.\nIn 1905 the British government transferred some administrative responsibility over south-east New Guinea to Australia (which renamed the area \"Territory of Papua\"); and in 1906, transferred all remaining responsibility to Australia. During World War I, Australian forces seized German New Guinea, which in 1920 became the Territory of New Guinea,\nto be administered by Australia under a League of Nations mandate. The territories under Australian administration became collectively known as The Territories of Papua and New Guinea (until February 1942).\nGerman colonialism.\nGermany established colonies in New Guinea in 1884 and Samoa in 1900.\nFollowing papal mediation and German compensation of $4.5 million, Spain recognized a German claim in 1885. Germany established a protectorate and set up trading stations on the islands of Jaluit and Ebon to carry out the flourishing copra (dried coconut meat) trade. Marshallese Iroij (high chiefs) continued to rule under indirect German colonial rule.\nAmerican colonialism.\nThe United States also expanded into the Pacific, beginning with Baker Island and Howland Island in 1857, and with Hawaii becoming a U.S. territory in 1898. Territorial disputes between the US, Germany and UK over Samoa led to the Tripartite Convention of 1899.\nSamoa aligned its interests with the United States in a Deed of Succession, signed by the \"Tui Man\u00fa\u02bba\" (supreme chief of Man\u00fa\u02bba) on 16 July 1904 at the Crown residence of the Tuimanu\u02bba called the \"Faleula\" in the place called Lalopua (from Official documents of the Tuimanu\u02bba government, 1893; Office of the Governor, 2004).\nCession followed the Tripartite Convention of 1899 that partitioned the eastern islands of Samoa (including Tutuila and the Man\u00fa\u02bba Group) from the western islands of Samoa (including \u02bbUpolu and Savai\u02bbi).\nJapanese colonialism.\nAt the beginning of World War I, Japan assumed control of the South Seas Mandate after annexing it from Germany. The Japanese headquarters was established at the German center of administration, Jaluit. On 31 January 1944, during World War II, American forces landed on Kwajalein atoll and U.S. Marines and Army troops later took control of the islands from the Japanese on 3 February, following intense fighting on Kwajalein and Enewetak atolls. In 1947, the United States, as the occupying power, entered into an agreement with the UN Security Council to administer much of Micronesia, including the Marshall Islands, as the Trust Territory of the Pacific Islands.\nDuring World War II, Japan occupied many Oceanic colonies by wresting control from western powers.\nSamoan Crisis 1887\u20131889.\nThe Samoan Crisis was a confrontation standoff between the United States, Imperial Germany, and the British Empire from 1887 to 1889 over control of the Samoan Islands during the Samoan Civil War.\nThe prime minister of the Kingdom of Hawaii, Walter M. Gibson, had long aimed to establishing an empire in the Pacific.\nIn 1887 his government sent the \"homemade battleship\" Kaimiloa to Samoa looking for an alliance against colonial powers.\nIt ended in suspicions from the German Navy and embarrassment for the conduct of the crew.\nThe 1889 incident involved three American warships, , and and three German warships, SMS \"Adler\", SMS \"Olga\", and SMS \"Eber\", keeping each other at bay over several months in Apia harbor, which was monitored by the British warship .\nThe standoff ended on 15 and 16 March when a cyclone wrecked all six warships in the harbor. \"Calliope\" was able to escape the harbor and survived the storm. Robert Louis Stevenson witnessed the storm and its aftermath at Apia and later wrote about what he saw. The Samoan Civil War continued, involving Germany, United States, and Britain, eventually resulting, via the Tripartite Convention of 1899, in the partition of the Samoan Islands into American Samoa and German Samoa.\nWorld War I.\nThe Asian and Pacific Theatre of World War I was a conquest of German colonial possession in the Pacific Ocean and China. The most significant military action was the Siege of Tsingtao in what is now China, but smaller actions were also fought at Battle of Bita Paka and Siege of Toma in German New Guinea.\nAll other German and Austrian possessions in Asia and the Pacific fell without bloodshed. Naval warfare was common; all of the colonial powers had naval squadrons stationed in the Indian or Pacific Oceans. These fleets operated by supporting the invasions of German held territories and by destroying the East Asia Squadron.\nOne of the first land offensives in the Pacific theatre was the Occupation of German Samoa in August 1914 by New Zealand forces. The campaign to take Samoa ended without bloodshed after over 1,000 New Zealanders landed on the German colony, supported by an Australian and French naval squadron.\nAustralian forces attacked German New Guinea in September 1914: 500 Australians encountered 300 Germans and native policemen at the Battle of Bita Paka; the Allies won the day and the Germans retreated to Toma. A company of Australians and a British warship besieged the Germans and their colonial subjects, ending with a German surrender.\nAfter the fall of Toma, only minor German forces were left in New Guinea and these generally capitulated once met by Australian forces. In December 1914, one German officer near Angorum attempted resist the occupation with thirty native police but his force deserted him after they fired on an Australian scouting party and he was subsequently captured.\nGerman Micronesia, the Marianas, the Carolines and the Marshall Islands also fell to Allied forces during the war.\nWorld War II.\nThe Pacific front saw major action during the Second World War, mainly between the belligerents Japan and the United States.\nThe attack on Pearl Harbor was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii, on the morning of 7 December 1941 (8 December in Japan). The attack led to the United States' entry into World War II.\nThe attack was intended as a preventive action in order to keep the U.S. Pacific Fleet from interfering with military actions the Empire of Japan was planning in South-East Asia against overseas territories of the United Kingdom, the Netherlands, and the United States. There were simultaneous Japanese attacks on the U.S.-held Philippines and on the British Empire in Malaya, Singapore, and Hong Kong.\nThe Japanese subsequently invaded New Guinea, the Solomon Islands and other Pacific islands. The Japanese were turned back at the Battle of the Coral Sea and the Kokoda Track campaign before they were finally defeated in 1945.\nSome of the most prominent Oceanic battlegrounds were the Solomon Islands campaign, the Air raids on Darwin, the Kokada Track, and the Borneo campaign.\nIn 1940 the administration of French Polynesia recognized the Free French Forces and many Polynesians served in World War II. Unknown at the time to French and Polynesians, the Konoe Cabinet in Imperial Japan on 16 September 1940 included French Polynesia among the many territories which were to become Japanese possessions in the post-war world\u2014though in the course of the war in the Pacific the Japanese were not able to launch an actual invasion of the French islands.\nSolomon Islands campaign.\nSome of the most intense fighting of the Second World War occurred in the Solomons. The most significant of the Allied Forces' operations against the Japanese Imperial Forces was launched on 7 August 1942, with simultaneous naval bombardments and amphibious landings on the Florida Islands at Tulagi and Red Beach on Guadalcanal.\nThe Guadalcanal campaign became an important and bloody campaign fought in the Pacific War as the Allies began to repulse Japanese expansion. Of strategic importance during the war were the coastwatchers operating in remote locations, often on Japanese held islands, providing early warning and intelligence of Japanese naval, army and aircraft movements during the campaign.\n\"The Slot\" was a name for New Georgia Sound, when it was used by the Tokyo Express to supply the Japanese garrison on Guadalcanal. Of more than 36,000 Japanese on Guadalcanal, about 26,000 were killed or missing, 9,000 died of disease, and 1,000 were captured.\nKokoda Track campaign.\nThe Kokoda Track campaign was a campaign consisting of a series of battles fought between July and November 1942 between Japanese and Allied\u2014primarily Australian\u2014forces in what was then the Australian territory of Papua. Following a landing near Gona, on the north coast of New Guinea, Japanese forces attempted to advance south overland through the mountains of the Owen Stanley Range to seize Port Moresby as part of a strategy of isolating Australia from the United States. Initially only limited Australian forces were available to oppose them, and after making rapid progress the Japanese South Seas Force clashed with under strength Australian forces at Awala, forcing them back to Kokoda. A number of Japanese attacks were subsequently fought off by the Australian Militia, yet they began to withdraw over the Owen Stanley Range, down the Kokoda Track.\nIn sight of Port Moresby itself, the Japanese began to run out of momentum against the Australians who began to receive further reinforcements. Having outrun their supply lines and following the reverses suffered by the Japanese at Guadalcanal, the Japanese were now on the defensive, marking the limit of the Japanese advance southwards. The Japanese subsequently withdrew to establish a defensive position on the north coast, but they were followed by the Australians who recaptured Kokoda on 2 November. Further fighting continued into November and December as the Australian and United States forces assaulted the Japanese beachheads, in what later became known as the Battle of Buna\u2013Gona.\nNuclear testing in Oceania.\nDue to its low population, Oceania was a popular location for atmospheric and underground nuclear tests. Tests were conducted in various locations by the United Kingdom (Operation Grapple and Operation Antler), the United States (Bikini atoll and the Marshall Islands) and France (Moruroa), often with devastating consequences for the inhabitants.\nFrom 1946 to 1958, the Marshall Islands served as the Pacific Proving Grounds for the United States, and was the site of 67 nuclear tests on various atolls. The world's first hydrogen bomb, codenamed \"Mike\", was tested at the Enewetak atoll in the Marshall Islands on 1 November (local date) in 1952, by the United States.\nIn 1954, fallout from the American Castle Bravo hydrogen bomb test in the Marshall Islands was such that the inhabitants of the Rongelap Atoll were forced to abandon their island. Three years later the islanders were allowed to return, but suffered abnormally high levels of cancer. They were evacuated again in 1985 and in 1996 given $45 million in compensation.\nA series of British tests were also conducted in the 1950s at Maralinga in South Australia, forcing the removal of the Pitjantjatjara and Yankunytjatjara peoples from their ancestral homelands.\nIn 1962, France's early nuclear testing ground of Algeria became independent and the atoll of Moruroa in the Tuamotu Archipelago was selected as the new testing site. Moruroa atoll became notorious as a site of French nuclear testing, primarily because tests were carried out there after most Pacific testing had ceased. These tests were opposed by most other nations in Oceania. The last atmospheric test was conducted in 1974, and the last underground test in 1996.\nFrench nuclear testing in the Pacific was controversial in the 1980s, in 1985 French agents caused the Sinking of the Rainbow Warrior in Auckland to prevent it from arriving at the test site in Moruroa. In September 1995, France stirred up widespread protests by resuming nuclear testing at Fangataufa atoll after a three-year moratorium. The last test was on 27 January 1996. On 29 January 1996, France announced that it would accede to the Comprehensive Test Ban Treaty, and no longer test nuclear weapons.\nFijian coups.\nFiji has suffered several coups d'\u00e9tat: military in 1987 and 2006 and civilian in 2000. All were ultimately due to ethnic tension between indigenous Fijians and Indo-Fijians, who originally came to the islands as indentured labor in the late nineteenth and early twentieth century. The 1987 coup followed the election of a multi-ethnic coalition, which Lieutenant Colonel Sitiveni Rabuka overthrew, claiming racial discrimination against ethnic Fijians. The coup was denounced by the United Nations and Fiji was expelled from the Commonwealth of Nations.\nThe 2000 coup was essentially a repeat of the 1987 affair, although it was led by civilian George Speight, apparently with military support. Commodore Frank Bainimarama, who was opposed to Speight, then took over and appointed a new Prime Minister. Speight was later tried and convicted for treason. Many indigenous Fijians were unhappy at the treatment of Speight and his supporters, feeling that the coup had been legitimate. In 2006 the Fijian parliament attempted to introduce a series of bills which would have, amongst other things, pardoned those involved in the 2000 coup. Bainimarama, concerned that the legal and racial injustices of the previous coups would be perpetuated, staged his own coup. It was internationally condemned, and Fiji again suspended from the Commonwealth.\nIn 2006 the then Australia Defence Minister, Brendan Nelson, warned Fijian officials of an Australian Naval fleet within proximity of Fiji that would respond to any attacks against its citizens.\nBougainville Civil War.\nThe Australian government estimated that anywhere between 15,000 and 20,000 people could have died in the Bougainville Civil War. More conservative estimates put the number of combat deaths as 1\u20132,000.\nFrom 1975, there were attempts by the Bougainville Province to secede from Papua New Guinea. These were resisted by Papua New Guinea primarily because of the presence in Bougainville of the Panguna mine, which was vital to Papua New Guinea's economy. The Bougainville Revolutionary Army began attacking the mine in 1988, forcing its closure the following year. Further BRA activity led to the declaration of a state of emergency and the conflict continued until about 2005, when successionist leader and self-proclaimed King of Bougainville Francis Ona died of malaria. Peacekeeping troops led by Australia have been in the region since the late 1990s, and a referendum on independence will be held in the 2010s.\nModern age.\nIn 1946, French Polynesians were granted French citizenship and the islands' status was changed to an overseas territory; the islands' name was changed in 1957 to Polyn\u00e9sie Fran\u00e7aise (French Polynesia).\nAustralia and New Zealand became dominions in the 20th century, adopting the Statute of Westminster Act in 1942 and 1947 respectively, marking their legislative independence from the United Kingdom. Hawaii became a U.S. state in 1959.\nSamoa became the first pacific nation to gain independence in 1962. Nauru became second in 1968, followed by Fiji and Tonga in 1970 and numerous other nations in the 1970s and 1980s. The South Pacific Forum was founded in 1971, which became the Pacific Islands Forum in 2000. Bougainville Island, geographically part of the Solomon Islands archipelago but politically part of Papua New Guinea, tried unsuccessfully to become independent in 1975, and a civil war followed in the early 1990s, with it later being granted autonomy.\nOn 1 May 1979, in recognition of the evolving political status of the Marshall Islands, the United States recognized the constitution of the Marshall Islands and the establishment of the Government of the Republic of the Marshall Islands. The constitution incorporates both American and British constitutional concepts.\nIn 1852, French Polynesia was granted partial internal autonomy; in 1984, the autonomy was extended. French Polynesia became a full overseas collectivity of France in 2004.\nBetween 2001 and 2007 Australia's Pacific Solution policy transferred asylum seekers to several Pacific nations, including the Nauru detention centre. Australia, New Zealand and other nations took part in the Regional Assistance Mission to Solomon Islands from 2003 after a request for aid.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14105", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=14105", "title": "Hanseatic League", "text": "1200s\u20131669 trade confederation in Northern Europe\nThe Hanseatic League, commonly called The Hansa, was a medieval commercial and defensive network of merchant guilds and market towns in Central and Northern Europe. Growing from L\u00fcbeck and a few other North German towns in the late 12th century, the League expanded between the 13th and 15th centuries and ultimately encompassed nearly 200 settlements across eight modern-day countries, ranging from what is now Estonia in the northeast to the Netherlands in the west, and extended inland as far south as Cologne.\nThe League began as a collection of loosely associated groups of German traders and towns aiming to expand their commercial interests, including protection against robbery. Over time, these arrangements evolved into the League, offering traders toll privileges and protection on affiliated territory and trade routes. Economic interdependence and familial connections among merchant families led to deeper political integration and the reduction of trade barriers. This gradual process involved standardizing trade regulations among Hanseatic Cities.\nDuring its time, the Hanseatic League dominated maritime trade in the North and Baltic Seas. It established a network of trading posts in numerous towns and cities, notably the Kontors in London (known as the Steelyard), Bruges, Bergen, and Novgorod, which became extraterritorial entities that enjoyed considerable legal autonomy. Hanseatic merchants operated private companies and were known for their access to commodities, and enjoyed privileges and protections abroad. The League's economic power enabled it to impose blockades and even wage war against kingdoms and principalities.\nEven at its peak, the Hanseatic League remained a loosely aligned confederation of cities. It lacked a permanent administrative body, a treasury, and a standing military force. The Grand Master of the Teutonic Order was often seen as the head of the Hanse (\"caput Hansae\"), both abroad and by some League members. The Teutonic Order was an official member of the Hanseatic League, unique as the only autonomous landed state to hold membership, while other members were cities or individual merchants. The Order and the Hanseatic League had a close economic and military interdependency, with many important Hanseatic trading ports falling within the Order's territories, and the Order itself played a role in protecting and organizing Baltic trade networks.\nIn the 14th century, the Hanseatic League instated an irregular negotiating \"diet\" that operated based on deliberation and consensus. By the mid-16th century, these weak connections left the Hanseatic League vulnerable, and it gradually unraveled as members merged into other realms or departed, ultimately disintegrating in 1669.\nThe League used a variety of vessel types for shipping across the seas and navigating rivers. The most emblematic type was the cog. Expressing diversity in construction, it was depicted on Hanseatic seals and coats of arms. By the end of the Middle Ages, the cog was replaced by types like the hulk, which later gave way to larger carvel ships.\nEtymology.\n is the Old High German word for a band or troop. This word was applied to bands of merchants traveling between the Hanseatic cities. in Middle Low German came to mean a society of merchants or a trader guild. Claims that it originally meant \"An-See\", or \"on the sea\", are incorrect.145\nHistory.\nExploratory trading ventures, raids, and piracy occurred throughout the Baltic Sea. The sailors of Gotland sailed up rivers as far away as Novgorod, which was a major Rus trade centre. Scandinavians led the Baltic trade before the League, establishing major trading hubs at Birka, Haithabu, and Schleswig by the 9th century CE. The later Hanseatic ports between Mecklenburg and K\u00f6nigsberg (present-day Kaliningrad) originally formed part of the Scandinavian-led Baltic trade system.\nAs the Hanseatic League was never formally founded, it lacks a date of foundation.2 Historians have traditionally traced its origins to the rebuilding of the north German town of L\u00fcbeck in 1159 by the powerful Henry the Lion, Duke of Saxony and Bavaria, after he had captured the area from Adolf II, Count of Schauenburg and Holstein. More recent scholarship has deemphasized L\u00fcbeck, viewing it as one of several regional trading centers, and presenting the League as the combination of a north German trading system oriented on the Baltic and a Rhinelandic trading system targeting England and Flanders.\nGerman cities speedily dominated trade in the Baltic during the 13th century, and L\u00fcbeck became a central node in the seaborne trade that linked the areas around the North and Baltic seas. L\u00fcbeck hegemony peaked during the 15th century.\nFoundation and early development.\nWell before the term \"Hanse\" appeared in a document in 1267, in different cities began to form guilds, or \"hansas\", with the intention of trading with overseas towns, especially in the economically less-developed eastern Baltic. This area could supply timber, wax, amber, resins, and furs, along with rye and wheat brought on barges from the hinterland to port markets. Merchant guilds formed in hometowns and destination ports as medieval corporations (\"universitates mercatorum\"), and despite competition increasingly cooperated to coalesce into the Hanseatic network of merchant guilds. The dominant language of trade was Middle Low German, which had a significant impact on the languages spoken in the area, particularly the larger Scandinavian languages, Estonian,288 and Latvian.\nVisby, on the island of Gotland, functioned as the leading center in the Baltic before the Hansa. Sailing east, Visby merchants established a trading post at Novgorod called \"Gutagard\" (also known as \"Gotenhof\") in 1080. In 1120, Gotland gained autonomy from Sweden and admitted traders from its southern and western regions.26 Thereafter, under a treaty with the Visby Hansa, northern German merchants made regular stops at Gotland. In the first half of the 13th century, they established their own trading station or \"Kontor\" in Novgorod, known as the Peterhof, up the river Volkhov.\nL\u00fcbeck soon became a base for merchants from Saxony and Westphalia trading eastward and northward; for them, because of its shorter and easier access route and better legal protections, it was more attractive than Schleswig.27 It became a transshipment port for trade between the North Sea and the Baltics. L\u00fcbeck also granted extensive trade privileges to Russian and Scandinavian traders.27\u201328 It was the main supply port for the Northern Crusades, improving its standing with various Popes. L\u00fcbeck gained imperial privileges to become a free imperial city in 1226, under Valdemar II of Denmark during the Danish dominion, as had Hamburg in 1189. Also in this period Wismar, Rostock, Stralsund, and Danzig received city charters.\nHansa societies worked to remove trade restrictions for their members. The earliest documentary mention (although without a name) of a specific German commercial federation dates between 1173 and 1175 (commonly misdated to 1157) in London. That year, the merchants of the Hansa in Cologne convinced King Henry II of England to exempt them from all tolls in London and to grant protection to merchants and goods throughout England.\nGerman colonists in the 12th and 13th centuries settled in numerous cities on and near the east Baltic coast, such as Elbing (Elbl\u0105g), Thorn (Toru\u0144), Reval (Tallinn), Riga, and Dorpat (Tartu), all of which joined the League, and some of which retain Hansa buildings and bear the style of their Hanseatic days. Most adopted L\u00fcbeck law, after the league's most prominent town. The law provided that they appeal in all legal matters to L\u00fcbeck's city council. Others, like Danzig from 1295 onwards, had Magdeburg law or its derivative, Culm law. \nOver the 13th century, older and wealthier long-distance traders increasingly chose to settle in their hometowns as trade leaders, transitioning from their previous roles as landowners. The growing number of settled merchants afforded long-distance traders greater influence over town policies. Coupled with an increased presence in the ministerial class, this elevated the status of merchants and enabled them to expand to and assert dominance over more cities.27\u201328 This decentralized arrangement was fostered by slow travel speeds: moving from Reval to L\u00fcbeck took between 4 weeks and, in winter, 4 months.202In 1241, L\u00fcbeck, which had access to the Baltic and North seas' fishing grounds, formed an alliance\u2014a precursor to the League\u2014with the trade city of Hamburg, which controlled access to the salt-trade routes from L\u00fcneburg. These cities gained control over most of the salt-fish trade, especially the Scania Market; Cologne joined them in the Diet of 1260. The towns raised their armies, with each guild required to provide levies when needed. The Hanseatic cities aided one another, and commercial ships often served to carry soldiers and their arms. The network of alliances grew to include a flexible roster of 70 to 170 cities.\nIn the West, cities of the Rhineland such as Cologne enjoyed trading privileges in Flanders and England. In 1266, King Henry III of England granted the L\u00fcbeck and Hamburg Hansa a charter for operations in England, initially causing competition with the Westphalians. But the Cologne Hansa and the Wendish Hansa joined in 1282 to form the Hanseatic colony in London, although they didn't completely merge until the 15th century. Novgorod was blockaded in 1268 and 1277/1278. Nonetheless, Westphalian traders continued to dominate trade in London and also Ipswich and Colchester, while Baltic and Wendish traders concentrated between King's Lynn and Newcastle upon Tyne.36 Much of the drive for cooperation came from the fragmented nature of existing territorial governments, which did not provide security for trade. Over the next 50 years, the merchant Hansa solidified with formal agreements for co-operation covering the west and east trade routes. Cities from the east modern-day Low Countries, but also Utrecht, Holland, Zealand, Brabant, Namur, and modern Limburg joined in participation over the thirteenth century.111 This network of Hanseatic trading guilds became called the \"Kaufmannshanse\" in historiography.\nCommercial expansion.\nThe League succeeded in establishing additional \"Kontors\" in Bruges (Flanders), Bryggen in Bergen (Norway), and London (England) beside the Peterhof in Novgorod. These trading posts were institutionalised by the first half of the 14th century (for Bergen and Bruges)6265 and, except for the Kontor of Bruges, became significant enclaves. The London \"Kontor\", the Steelyard, stood west of London Bridge near Upper Thames Street, on the site later occupied by Cannon Street station. It grew into a walled community with its warehouses, weigh house, church, offices, and homes.\nIn addition to the major \"Kontors\", individual ports with Hanseatic trading outposts or factories had a representative merchant and warehouse. Often they were not permanently manned. In Scania, Denmark, around 30 Hanseatic seasonal factories produced salted herring, these were called \"vitten\" and were granted legal autonomy to the extent that Burkhardt argues that they resembled a fifth kontor and would be seen as such if not for their early decline. In England, factories in Boston (the outpost was also called Stalhof), Bristol, Bishop's Lynn (later King's Lynn, which featured the sole remaining Hanseatic warehouse in England), Hull, Ipswich, Newcastle upon Tyne, Norwich, Scarborough, Yarmouth (now Great Yarmouth), and York, many of which were important for the Baltic trade and became centers of the textile industry in the late 14th century. Hansards and textile manufacturers coordinated to make fabrics meet local demand and fashion in the traders' hometowns. Outposts in Lisbon, Bordeaux, Bourgneuf, La Rochelle and Nantes offered the cheaper Bay salt. Ships that plied this trade sailed in the salt fleet. Trading posts operated in Flanders, Denmark-Norway, the Baltic interior, Upper Germany, Iceland, and Venice.\nHanseatic trade was not exclusively maritime, or even over water. Most Hanseatic towns did not have immediate access to the sea and many were linked to partners by river trade or even land trade. These formed an integrated network, while many smaller Hanseatic towns had their main trading activity in subregional trade. Internal Hanseatic trade was the Hanse's quantitatively largest and most important business. Trade over rivers and land was not tied to specific Hanseatic privileges, but seaports such as Bremen, Hamburg and Riga dominated trade on their rivers. This was not possible for the Rhine where trade retained an open character. Digging canals for trade was uncommon, although the Stecknitz Canal was built between L\u00fcbeck and Lauenburg from 1391 to 1398.\nMajor trade goods.\nStarting with trade in coarse woolen fabrics, the Hanseatic League increased both commerce and industry in northern Germany. As trade increased, finer woolen and linen fabrics, and even silks, were manufactured in northern Germany. The same refinement of products out of the cottage industry occurred in other fields, e.g. etching, wood carving, armor production, engraving of metals, and wood-turning.\nThe league primarily traded beeswax, furs, timber, resin (or tar), flax, honey, wheat, and rye from the east to Flanders and England with cloth, in particular broadcloth, (and, increasingly, manufactured goods) going in the other direction. Metal ore (principally copper and iron) and herring came south from Sweden, while the Carpathians were another important source of copper and iron, often sold in Thorn. Lubeck had a vital role in the salt trade; salt was acquired in L\u00fcneburg or shipped from France and Portugal and sold on Central European markets, taken to Scania to salt herring, or exported to Russia. Stockfish was traded from Bergen in exchange for grain; Hanseatic grain inflows allowed more permanent settlements further north in Norway. The league also traded beer, with beer from Hanseatic towns the most valued, and Wendish cities like L\u00fcbeck, Hamburg, Wismar, and Rostock developed export breweries for hopped beer.72141\nEconomic power and defense.\nThe Hanseatic League, at first the merchant hansas and eventually its cities, relied on power to secure protection and gain and preserve privileges. Bandits and pirates were persistent problems; during wars, these could be joined by privateers. Traders could be arrested abroad and their goods could be confiscated. The league sought to codify protection; internal treaties established mutual defense and external treaties codified privileges.53\nMany locals, merchant and noble alike, envied the League's power and tried to diminish it. For example, in London, local merchants exerted continuing pressure for the revocation of privileges. Most foreign cities confined Hanseatic traders to specific trading areas and their trading posts. The refusal of the Hansa to offer reciprocal arrangements to their counterparts exacerbated the tension.\nLeague merchants used their economic power to pressure cities and rulers. They called embargoes, redirected trade away from towns, and boycotted entire countries. Blockades were erected against Novgorod in 1268 and 1277/1278.58 Bruges was pressured by temporarily moving the Hanseatic emporium to Aardenburg from 1280 to 1282,58 from 1307 or 1308 to 1310 and in 1350,29 to Dordt in 1358 and 1388, and to Antwerp in 1436.68, 80, 92 Boycotts against Norway in 128428 and Flanders in 1358 nearly caused famines.68 They sometimes resorted to military action. Several Hanseatic cities maintained their warships and in times of need, repurposed merchant ships. Military action against political powers often involved an \"ad hoc\" coalition of stakeholders, called an alliance (\"tohopesate\").\nAs an essential part of protecting their investments, League members trained pilots and erected lighthouses, including K\u00f5pu Lighthouse. L\u00fcbeck erected in 1202 what may be northern Europe's first proper lighthouse in Falsterbo. By 1600 at least 15 lighthouses had been erected along the German and Scandinavian coasts, making it the best-lighted coast in the world, largely thanks to the Hansa.\nZenith.\nThe weakening of imperial power and imperial protection under the late Hohenstaufen dynasty forced the League to institutionalize a cooperating network of cities with a fluid structure, called the \"St\u00e4dtehanse\",27 but it never became a formal organization and the \"Kaufmannshanse\" continued to exist.28\u201329 This development was delayed by the conquest of Wendish cities by the Danish king Eric VI Menved or by their feudal overlords between 1306 and 1319 and the restriction of their autonomy. Assemblies of the Hanse towns met irregularly in L\u00fcbeck for a \"Hansetag\" (Hanseatic Diet) \u2013 starting either around 1300,59 or possibly 1356.66 Many towns chose not to attend nor to send representatives, and decisions were not binding on individual cities if their delegates were not included in the recesses; representatives would sometimes leave the Diet prematurely to give their towns an excuse not to ratify decisions. Only a few Hanseatic cities were free imperial cities or enjoyed comparable autonomy and liberties, but many temporarily escaped domination by local nobility.\nBetween 1361 and 1370, League members fought against Denmark in the Danish-Hanseatic War. Though initially unsuccessful with a Wendish offensive, towns from Prussia and the Netherlands, and eventually joined by Wendish towns, allied in the Confederation of Cologne in 1368, sacked Copenhagen and Helsingborg, and forced Valdemar IV, King of Denmark, and his son-in-law Haakon VI, King of Norway, to grant tax exemptions and influence over \u00d8resund fortresses for 15 years in the peace treaty of Stralsund in 1370. It extended privileges in Scania to the League, including Holland and Zeeland. The treaty marked the height of Hanseatic influence; for this period the League was called a \"Northern European great power\". The Confederation lasted until 1385, while the \u00d8resund fortresses were returned to Denmark that year.64, 70\u201373\nAfter Valdemar's heir Olav died, a succession dispute erupted over Denmark and Norway between Albert of Mecklenburg, King of Sweden and Margaret I, Queen of Denmark. This was further complicated when Swedish nobles rebelled against Albert and invited Margaret. Albert was taken prisoner in 1389, but hired privateers in 1392, the socalled Victual Brothers, who took Bornholm and Visby in his name. They and their descendants threatened maritime trade between 1392 and the 1430s. Under the 1395 release agreement for Albert, Stockholm was ruled from 1395 to 1398 by a consortium of 7 Hanseatic cities, and enjoyed full Hanseatic trading privileges. It went to Margaret in 1398.\nThe Victual Brothers controlled Gotland in 1398. It was conquered by the Teutonic Order with support from the Prussian towns and its privileges were restored.\nRise of rival powers.\nOver the 15th century, the League became further institutionalized. This was in part a response to challenges in governance and competition with rivals, but also reflected changes in trade. A slow shift occurred from loose participation to formal recognition/revocation. Another general trend was Hanseatic cities' increased legislation of their kontors abroad. Only the Bergen kontor grew more independent in this period.136\nIn Novgorod, after extended conflict since the 1380s, the League regained its trade privileges in 1392, agreeing to Russian trade privileges for Livonia and Gotland.\nIn 1424, all German traders of the \"Peterhof\" kontor in Novgorod were imprisoned and 36 of them died. Although rare, arrests and seizures in Novgorod were particularly violent.182 In response, and due to the ongoing war between Novgorod and the Teutonic Order, the League blockaded Novgorod and abandoned the \"Peterhof\" from 1443 to 1448.82\nAfter extended conflicts with the League from the 1370s, English traders gained trade privileges in the Prussian region via the treaties of Marienburg (the first in 1388, the last in 1409). Their influence increased, while the importance of Hanseatic trade in England decreased over the 15th century.98\nOver the 15th century, tensions between the Prussian region and the \"Wendish\" cities (L\u00fcbeck and its eastern neighbours) increased. L\u00fcbeck was dependent on its role as center of the Hansa; Prussia's main interest, on the other hand, was the export of bulk products such as grain and timber to England, the Low Countries and later on Spain and Italy.\nFrederick II, Elector of Brandenburg, tried to assert authority over the Hanseatic towns Berlin and C\u00f6lln in 1442 and blocked all Brandenburg towns from participating in Hanseatic diets. For some Brandenburg towns, this ended their Hanseatic involvement. In 1488, John Cicero, Elector of Brandenburg did the same to Stendal and Salzwedel in the Altmark.\nUntil 1394, Holland and Zeeland actively participated in the Hansa, but in 1395, their feudal obligations to Albert I, Duke of Bavaria prevented further cooperation. Consequently, their Hanseatic ties weakened, and their economic focus shifted. Between 1417 and 1432, this economic reorientation became even more pronounced as Holland and Zeeland gradually became part of the Burgundian State.\nThe city of L\u00fcbeck faced financial troubles in 1403, leading dissenting craftsmen to establish a supervising committee in 1405. This triggered a governmental crisis in 1408 when the committee rebelled and established a new town council. Similar revolts broke out in Wismar and Rostock, with new town councils established in 1410. The crisis was ended in 1418 by a compromise.\nEric of Pomerania succeeded Margaret in 1412 and sought to expand into Schleswig and Holstein levying tolls at the \u00d8resund. Hanseatic cities were divided initially; L\u00fcbeck tried to appease Eric while Hamburg supported the Schauenburg counts against him. This led to the Danish-Hanseatic War (1426\u20131435) and the Bombardment of Copenhagen (1428). The Treaty of Vordingborg renewed the League's commercial privileges in 1435, but the \u00d8resund tolls continued.\nEric of Pomerania was subsequently deposed and in 1438 L\u00fcbeck took control of the \u00d8resund toll, which caused tensions with Holland and Zeeland.265171 The Sound tolls, and a later attempt of L\u00fcbeck to exclude the English and Dutch merchants from Scania harmed the Scanian herring trade when the excluded regions began to develop their own herring industries.\nIn the Dutch\u2013Hanseatic War (1438\u20131441), a privateer war mostly waged by Wendish towns, the merchants of Amsterdam sought and eventually won free access to the Baltic. Although the blockade of the grain trade hurt Holland and Zeeland more than Hanseatic cities, it was against Prussian interest to maintain it.91\nIn 1454, the year of the marriage of Elisabeth of Austria to King-Grand Duke Casimir IV Jagiellon of Poland-Lithuania, the towns of the Prussian Confederation rose up against the dominance of the Teutonic Order and asked Casimir IV for help. Danzig, Thorn and Elbing became part of the autonomous province of Royal Prussia in the Kingdom of Poland by the Second Peace of Thorn.\nPoland in turn was heavily supported by the Holy Roman Empire through family connections and by military assistance under the Habsburgs. Krak\u00f3w, then the Polish capital, had a loose association with the Hansa.93 The lack of customs borders on the River Vistula after 1466 helped to gradually increase Polish grain exports, transported down the Vistula, from per year, in the late 15th century, to over in the 17th century. The Hansa-dominated maritime grain trade made Poland one of the main areas of its activity, helping Danzig to become the Hansa's largest city. Polish kings soon began to reduce the towns' political freedoms.36\nBeginning in the mid-15th century, the Griffin dukes of Pomerania were in constant conflict over control of the Pomeranian Hanseatic towns. While not successful at first, Bogislav X eventually subjugated Stettin and K\u00f6slin, curtailing the region's economy and independence.35\nA major Hansa economic advantage was its control of the shipbuilding market, mainly in L\u00fcbeck and Danzig. The League sold ships throughout Europe.\nThe economic crises of the late 15th century did not spare the Hansa. Nevertheless, its eventual rivals emerged in the form of territorial states. New vehicles of credit were imported from Italy.\nWhen Flanders and Holland became part of the Duchy of Burgundy, Burgund Dutch and Prussian cities increasingly excluded L\u00fcbeck from their grain trade in the 15th and 16th century. Burgund Dutch demand for Prussian and Livonian grain grew in the late 15th century. These trade interests differed from Wendish interests, threatening political unity, but also showed a trade where the Hanseatic system was impractical. Hollandish freight costs were much lower than the Hansa's, and the Hansa were excluded as middlemen. After naval wars between Burgundy and the Hanseatic fleets, Amsterdam gained the position of leading port for Polish and Baltic grain from the late 15th century onwards.\nNuremberg in Franconia developed an overland route to sell formerly Hansa-monopolised products from Frankfurt via Nuremberg and Leipzig to Poland and Russia, trading Flemish cloth and French wine in exchange for grain and furs from the east. The Hansa profited from the Nuremberg trade by allowing Nurembergers to settle in Hanseatic towns, which the Franconians exploited by taking over trade with Sweden as well. The Nuremberger merchant Albrecht Moldenhauer was influential in developing the trade with Sweden and Norway, and his sons Wolf and Burghard Moldenhauer established themselves in Bergen and Stockholm, becoming leaders of the local Hanseatic activities.\nKing Edward IV of England reconfirmed the league's privileges in the Treaty of Utrecht despite the latent hostility, in part thanks to the significant financial contribution the League made to the Yorkist side during the Wars of the Roses of 1455\u20131487. Tsar Ivan III of Russia closed the Hanseatic \"Kontor\" at Novgorod in 1494 and deported its merchants to Moscow, in an attempt to reduce Hanseatic influence on Russian trade.145 At the time, only 49 traders were at the Peterhof.99 The fur trade was redirected to Leipzig, taking out the Hansards;54 while the Hanseatic trade with Russia moved to Riga, Reval, and Pskov.100 When the Peterhof reopened in 1514, Novgorod was no longer a trade hub. In the same period, the burghers of Bergen tried to develop an independent intermediate trade with the northern population, against the Hansards' obstruction.144 The League's mere existence and its privileges and monopolies created economic and social tensions that often spilled onto rivalries between League members.\nEnd of the Hansa.\nThe development of transatlantic trade after the discovery of the Americas caused the remaining contours to decline, especially in Bruges, because it centered on other ports. It also changed business practice to short-term contracts and obsoleted the Hanseatic model of privileged guaranteed trade.\nThe trends of local feudal lords asserting control over towns and suppressing their autonomy, and of foreign rulers repressing Hanseatic traders continued in the next century. The city of Kiel was expelled from the League in 1518 for harbouring pirates.\nIn the Swedish War of Liberation 1521\u20131523, the Hanseatic League was successful in opposition to an economic conflict it had over the trade, mining, and metal industry in Bergslagen (the main mining area of Sweden in the 16th century) with Jakob Fugger (industrialist in the mining and metal industry) and his unfriendly business take-over attempt. Fugger allied with his financially dependent pope Leo X, Maximilian I, Holy Roman Emperor, and Christian II of Denmark/Norway. Both sides made costly investments in support of mercenaries to win the war. After the war, Gustav Vasa's Sweden and Frederick I's Denmark pursued independent policies and didn't support L\u00fcbeck's effort against Dutch trade.113\nHowever, L\u00fcbeck under J\u00fcrgen Wullenwever overextended in the Count's Feud in Scania and Denmark and lost influence in 1536 after Christian III's victory.144 L\u00fcbeck's attempts at forcing competitors out of the Sound eventually alienated even Gustav Vasa.113\u2013114 Its influence in the Nordic countries began to decline.\nThe Hanseatic towns of Guelders were obstructed in the 1530s by Charles II, Duke of Guelders. Charles, a strict Catholic, objected to Lutheranism, in his words \"Lutheran heresy\", of L\u00fcbeck and other north German cities. This frustrated but did not end the towns' Hanseatic trade and a small resurgence came later.\nLater in the 16th century, Denmark-Norway took control of the southern Baltic Sea. Sweden had regained control over its own trade, the \"Kontor\" in Novgorod had closed, and the \"Kontor\" in Bruges had become effectively moribund because the Zwin inlet was closing up.132 Finally, the growing political authority of the German princes constrained the independence of Hanse towns.\nThe league attempted to deal with some of these issues: it created the post of syndic in 1556 and elected Heinrich Sudermann to the position, who worked to protect and extend the diplomatic agreements of the member towns. In 1557 and 1579, revised agreements spelled out the duties of towns and some progress was made. The Bruges \"Kontor\" moved to Antwerp in 1520140\u2013154 and the Hansa attempted to pioneer new routes. However, the league proved unable to prevent the growing mercantile competition.\nIn 1567, a Hanseatic League agreement reconfirmed previous obligations and rights of league members, such as common protection and defense against enemies. The Prussian Quartier cities of Thorn, Elbing, K\u00f6nigsberg and Riga and Dorpat also signed. When pressed by the King of Poland\u2013Lithuania, Danzig remained neutral and would not allow ships running for Poland into its territory. They had to anchor somewhere else, such as at Pautzke (Puck).\nThe Antwerp \"Kontor\", moribund after the fall of the city, closed in 1593. In 1597 Queen Elizabeth I of England expelled the League from London, and the Steelyard closed and sequestered in 1598. The Kontor returned in 1606 under her successor, James I, but it could not recover. The Bergen \"Kontor\" continued until 1754; of all the \"Kontore\", only its buildings, the \"Bryggen\", survive.\nNot all states tried to suppress their cities' former Hanseatic links; the Dutch Republic encouraged its eastern former members to maintain ties with the remaining Hanseatic League. The States-General relied on those cities in diplomacy at the time of the Kalmar War.123\nThe Thirty Years' War was destructive for the Hanseatic League and members suffered heavily from both the imperials, the Danes and the Swedes. In the beginning, Saxon and Wendish faced attacks because of the desire of Christian IV of Denmark to control the Elbe and Weser. Pomerania had a major population decline. Sweden took Bremen-Verden (excluding the city of Bremen), Swedish Pomerania (including Stralsund, Greifswald, Rostock) and Swedish Wismar, preventing their cities from participating in the League, and controlled the Oder, Weser, and Elbe, and could levy tolls on their traffic. The league became increasingly irrelevant despite its inclusion in the Peace of Westphalia.43\nIn 1666, the Steelyard burned in the Great Fire of London. The Kontor-manager sent a letter to L\u00fcbeck appealing for immediate financial assistance for a reconstruction. Hamburg, Bremen, and L\u00fcbeck called for a Hanseatic Day in 1669. Only a few cities participated and those who came were reluctant to contribute financially to the reconstruction. It was the last formal meeting, unbeknownst to any of the parties. This date is often taken in retrospect as the effective end date of the Hansa, but the League never formally disbanded. It silently disintegrated.1922\nAftermath.\nThe Hanseatic League, however, lived on in the public mind. Leopold I even requested L\u00fcbeck to call a Tagfahrt to rally support for him against the Turks.192\nL\u00fcbeck, Hamburg, and Bremen continued to attempt common diplomacy, although interests had diverged by the Peace of Ryswick.192 Nonetheless, the Hanseatic Republics were able to jointly perform some diplomacy, such as a joint delegation to the United States in 1827, led by Vincent Rumpff; later the United States established a consulate to the \"Hanseatic and Free Cities\" from 1857 to 1862. Britain maintained diplomats to the Hanseatic Cities until the unification of Germany in 1871. The three cities also had a common \"Hanseatic\" representation in Berlin until 1920.192\nThree kontors remained as, often unused, Hanseatic property after the League's demise, as the Peterhof had closed in the 16th century. Bryggen was sold to Norwegian owners in 1754. The Steelyard in London and the Oostershuis in Antwerp were long impossible to sell. The Steelyard was finally sold in 1852 and the Oostershuis, closed in 1593, was sold in 1862.192\nHamburg, Bremen, and L\u00fcbeck remained as the only members until the League's formal end in 1862, on the eve of the 1867 founding of the North German Confederation and the 1871 founding of the German Empire under Kaiser Wilhelm I. Despite its collapse, they cherished the link to the Hanseatic League. Until German reunification, these three cities were the only ones that retained the words \"Hanseatic City\" in their official German names. Hamburg and Bremen continue to style themselves officially as \"free Hanseatic cities\", with L\u00fcbeck named \"Hanseatic City\". For L\u00fcbeck in particular, this anachronistic tie to a glorious past remained important in the 20th century. In 1937, the Nazi Party revoked its imperial immediacy through the Greater Hamburg Act. Since 1990, 24 other German cities have adopted this title.\nOrganization.\nThe Hanseatic League was a complex, loose-jointed constellation of protagonists pursuing their interests, which coincided in a shared program of economic domination in the Baltic region, and by no means a monolithic organization or a 'state within a state'. It gradually grew from a network of merchant guilds into a more formal association of cities, but never formed into a legal person.91\nLeague members were Low German-speaking, except for Dinant. Not all towns with Low German merchant communities were members (e.g., Emden, Memel (today Klaip\u0117da), Viborg (today Vyborg), and Narva). However, Hansards also came from settlements without German town law\u2014the premise for league membership was birth to German parents, subjection to German law, and commercial education. The league served to advance and defend its members' common interests: commercial ambitions such as enhancement of trade, and political ambitions such as ensuring maximum independence from the territorial rulers.\nLeague decisions and actions were taken via a consensus-based procedure. If an issue arose, members were invited to participate in a central meeting, the \"Tagfahrt\" (Hanseatic Diet, \"meeting ride\", sometimes also referred to as \"Hansetag\"), that may have begun around 1300,59 and were formalized since 1358 (or possibly 1356). The member communities then chose envoys (\"Ratssendeboten\") to represent their local consensus on the issue at the Diet. Not every community sent an envoy; delegates were often entitled to represent multiple communities. Consensus-building on local and \"Tagfahrt\" levels followed the Low Saxon tradition of \"Einung\", where consensus was defined as the absence of protest: after a discussion, the proposals that gained sufficient support were dictated to the scribe and passed as binding \"Rezess\" if the attendees did not object; those favoring alternative proposals unlikely to get sufficient support remained silent during this procedure. If consensus could not be established on a certain issue, it was found instead in the appointment of league members empowered to work out a compromise.\nThe League was characterised by legal pluralism and the diets could not issue laws. But the cities cooperated to achieve limited trade regulation, such as measures against fraud, or worked together on a regional level. Attempts to harmonize maritime law yielded a series of ordinances in the 15th and 16th centuries. The most extensive maritime ordinance was the \"Ship Ordinance and Sea Law\" of 1614, but it may not have been enforced.\nKontors.\nA Hanseatic \"Kontor\" was a settlement of Hansards organized in the mid-14th century as a private corporation that had its treasury, court, legislation, and seal. They operated like an early stock exchange. Kontors were first established to provide security, but also served to secure privileges and engage in diplomacy. The quality of goods was also examined at Kontors, increasing trade efficiency, and they served as bases to develop connections with local rulers and as sources of economic and political information.91 Most contours were also physical locations containing buildings that were integrated and segregated from city life to different degrees. The kontor of Bruges was an exception in this regard; it acquired buildings only as of the 15th century. Like the guilds, the \"Kontore\" were usually led by \"\u00c4lterm\u00e4nner\" (\"eldermen\", or English aldermen). The Stalhof, a special case, had a Hanseatic and an English alderman. In Novgorod the aldermen were replaced by a \"hofknecht\" in the 15th century. The contours statutes were read aloud to the present merchants once a year.\nIn 1347 the \"Kontor\" of Bruges modified its statute to ensure an equal representation of League members. To that end, member communities from different regions were pooled into three circles (\"Drittel\" (\"third [part]\"): the Wendish and Saxon Drittel, the Westphalian and Prussian Drittel as well as the Gotlandian, Livonian and Swedish Drittel). Merchants from each \"Drittel\" chose two aldermen and six members of the Eighteen Men Council (\"Achtzehnm\u00e4nnerrat\") to administer the \"Kontor\" for a set period.91, 101\nIn 1356, during a Hanseatic meeting in preparation for the first (or one of the first) \"Tagfahrt\", the League confirmed this statute. All trader settlements including the \"Kontors\" were subordinated to the Diet's decisions around this time, and their envoys received the right to attend and speak at Diets, albeit without voting power.91\nDrittel.\nThe league gradually divided the organization into three constituent parts called \"Drittel\" (German \"\"), as shown in the table below.\nThe \"Hansetag\" was the only central institution of the Hanseatic League. However, with the division into \"Drittel\", the members of the respective subdivisions frequently held a \"Dritteltage\" (\"\"Drittel\" meeting\") to work out common positions which could then be presented at a \"Hansetag\". On a more local level, league members also met, and while such regional meetings never crystallized into a Hanseatic institution, they gradually gained importance in the process of preparing and implementing the Diet's decisions.\nQuarters.\nFrom 1554, the division into \"Drittel\" was modified to reduce the circles' heterogeneity, to enhance the collaboration of the members on a regional level and thus to make the League's decision-making process more efficient.217 The number of circles rose to four, so they were called \"Quartiere\" (quarters):\nThis division was however not adopted by the \"Kontore\", who, for their purposes (like \"\u00c4lterm\u00e4nner\" elections), grouped League members in different ways (e.g., the division adopted by the Stalhof in London in 1554 grouped members into \"Dritteln\", whereby L\u00fcbeck merchants represented the Wendish, Pomeranian Saxon, and several Westphalian towns, Cologne merchants represented the Cleves, Mark, Berg and Dutch towns, while Danzig merchants represented the Prussian and Livonian towns).38\u201392\nHanseatic ships.\nVarious types of ships were used.\nCog.\nThe most used type, and the most emblematic, was the cog. The cog was a multi-purpose clinker-built ship with a carvel bottom, a stern rudder, and a square rigged mast. Most cogs were privately owned and were also used as warships. Cogs were built in various sizes and specifications and were used on both the seas and rivers. They could be outfitted with castles starting from the thirteenth century. The cog was depicted on many seals and several coats of arms of Hanseatic cities, like Stralsund, Elbl\u0105g and Wismar. Several shipwrecks have been found. The most notable wreck is the Bremen cog.35\u201336 It could carry a cargo of about 125 tons.20\nHulk.\nThe hulk began to replace the cog by 1400 and cogs lost their dominance to them around 1450.264\nThe hulk was a bulkier ship that could carry larger cargo; Elbl estimates they could carry up to 500 tons by the 15th century. It could be clinker or carvel-built.26464 No archeological evidence of a hulk has been found.\nCarvel.\nIn 1464, Danzig acquired a French carvel ship through a legal dispute and renamed it the Peter von Danzig. It was 40 m long and had three masts, one of the largest ships of its time. Danzig adopted carvel construction around 1470.44 Other cities shifted to carvels starting from this time. An example is the Jesus of L\u00fcbeck, later sold to England for use as a warship and slave ship.\nThe galleonlike carvel warship \"Adler von L\u00fcbeck\" was constructed by L\u00fcbeck for military use against Sweden during the Northern Seven Years' War (1563\u201370). Launched in 1566, it was never put to military use after the Treaty of Stettin. It was the biggest ship of its day at 78 m long and had four masts, including a bonaventure mizzen. It served as a merchant ship until it was damaged in 1581 on a return voyage from Lisbon and broken up in 1588.\nHanseatic cities.\nHansa Proper.\nIn the table below, the names listed in the column labeled Quarter have been summarised as follows: \nThe remaining column headings are as follows:\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;List of cities in Hansa Proper \n \n\"Kontore\".\nThe \"kontore\" were the major foreign trading posts of the League, not cities that were Hanseatic members, and are listed in the hidden table below.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;List of Hansa \"kontore\"\n \n\"Vitten\".\nThe \"vitten\" were significant foreign trading posts of the League in Scania, not cities that were Hanseatic members, they are argued by some to have been similar in status to the \"kontors\", and are listed in the hidden table below.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;List of Hansa \"vitten\"\n \nPorts with Hansa trading posts.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;List of ports with Hansa trading posts \nOther cities with a Hansa community.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;Other cities with a Hansa community \n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nLegacy.\nHistoriography.\nAcademic historiography of the Hanseatic League is considered to begin with Georg Sartorius, who started writing his first work in 1795 and founded the liberal historiographical tradition about the League. The German conservative nationalist historiographical tradition was first published with F.W. Barthold's \"Geschichte der Deutschen Hansa\" of 1853/1854. The conservative view was associated with Little German ideology and came to predominate from the 1850s until the end of the First World War. Hanseatic history was used to justify a stronger German navy and conservative historians drew a link between the League and the rise of Prussia as the leading German state. This climate deeply influenced the historiography of the Baltic trade.\nIssues of social, cultural and economic history became more important in German research after the First World War. But leading historian Fritz R\u00f6rig also promoted a National Socialist perspective. After the Second World War the conservative nationalist view was discarded, allowing exchanges between German, Swedish and Norwegian historians on the Hanseatic League's role in Sweden and Norway. Views on the League were strongly negative in the Scandinavian countries, especially Denmark, because of associations with German privilege and supremacy. Philippe Dollinger's book \"The German Hansa\" became the standard work in the 1960s. At that time, the dominant perspective became Ahasver von Brandt's view of a loosely aligned trading network. Marxist historians in the GDR were split on whether the League was a \"late feudal\" or \"proto-capitalist\" phenomenon.\nTwo museums in Europe are dedicated to the history of the Hanseatic League: the European Hansemuseum in L\u00fcbeck and the Hanseatic Museum and Sch\u00f8tstuene in Bergen.\nPopular views.\nFrom the 19th century, Hanseatic history was often used to promote a national cause in Germany. German liberals built a fictional literature around J\u00fcrgen Wullenwever, expressing fierce anti-Danish sentiment. Hanseatic subjects were used to propagate nation building, colonialism, fleet building and warfare, and the League was presented as a bringer of culture and pioneer of German expansion.\nThe preoccupation with a strong navy motivated German painters in the 19th century to paint supposedly Hanseatic ships. They used the traditions of maritime paintings and, not wanting Hanseatic ships to look unimpressive, ignored historical evidence to fictionalise cogs into tall two- or three-masted ships. The depictions were widely reproduced, such as on plates of Norddeutscher Lloyd. This misleading artistic tradition influenced public perception throughout the 20th century.\nIn the late 19th century, a social-critical view developed, where opponents of the League like the \"likedeelers\" were presented as heroes and liberators from economic oppression. This was popular from the end of the First World War into the 1930s, and survives in the St\u00f6rtebeker Festival on R\u00fcgen, founded as the R\u00fcgenfestspiele by the GDR.\nFrom the late 1970s, the Europeanness and cooperation of the Hanseatic League came to prominence in popular culture. It is associated with innovation, entrepreneurism and internationalness in economic circles. In this way it often used for tourism, city branding and commercial marketing.109 The League's unique governance structure has been identified as a precursor to the supranational model of the European Union.\nModern transnational organisations named after the Hanseatic League.\nUnion of Cities THE HANSA.\nIn 1979, Zwolle invited over 40 cities from West Germany, the Netherlands, Sweden and Norway with historic links to the Hanseatic League to sign the recesses of 1669, at Zwolle's 750 year city rights' anniversary in August of the next year. In 1980, those cities established a \"new Hanse\" in Zwolle, named \"St\u00e4dtebund Die Hanse\" (Union of Cities THE HANSA) in German and reinstituted the Hanseatic diets. This league is open to all former Hanseatic League members and cities that share a Hanseatic heritage.\nIn 2012, the city league had 187 members. This included twelve Russian cities, most notably Novgorod, and 21 Polish cities. No Danish cities had joined the Union although several qualify. The \"new Hanse\" fosters business links, tourism and cultural exchange.\nThe headquarters of the New Hansa is in L\u00fcbeck, Germany.\nDutch cities including Groningen, Deventer, Kampen, Zutphen and Zwolle, and a number of German cities including Bremen, Buxtehude, Demmin, Greifswald, Hamburg, L\u00fcbeck, L\u00fcneburg, Rostock, Salzwedel, Stade, Stendal, Stralsund, Uelzen and Wismar now call themselves \"Hanse\" cities (the German cities' car license plates are prefixed \"H\", e.g. \u2013\"HB\"\u2013 for \"Hansestadt Bremen\").\nEach year one of the member cities of the New Hansa hosts the Hanseatic Days of New Time international festival.\nIn 2006, King's Lynn became the first English member of the union of cities. It was joined by Hull in 2012 and Boston in 2016.\nNew Hanseatic League.\nIn February 2018, a new small group of EU countries formed a monetary and economic working group called the New Hanseatic League. Finance ministers from Denmark, Estonia, Finland, Ireland, Latvia, Lithuania, the Netherlands and Sweden signed its foundational document which set out the countries' \"shared views and values in the discussion on the architecture of the EMU\".\nOthers.\nThe legacy of the Hansa is reflected in several names: the German airline Lufthansa (lit. \"Air Hansa\"); F.C. Hansa Rostock, nickamed the Kogge or Hansa-Kogge; Hansa-Park, one of the biggest theme parks in Germany; Hanze University of Applied Sciences in Groningen, Netherlands; Hanze oil production platform, Netherlands; the Hansa Brewery in Bergen and the Hanse Sail in Rostock; Hanseatic Trade Center in Hamburg; DDG Hansa, which was a major German shipping company from 1881 until its bankruptcy and takeover by Hapag-Lloyd in 1980; the district of New Hanza City in Riga, Latvia; and Hansabank in Estonia, which was rebranded as Swedbank.\nExplanatory footnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14107", "revid": "14835592", "url": "https://en.wikipedia.org/wiki?curid=14107", "title": "Harvard (disambiguation)", "text": "Harvard University is a university in Cambridge, Massachusetts, US.\nHarvard may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14108", "revid": "29619079", "url": "https://en.wikipedia.org/wiki?curid=14108", "title": "Historical African place names", "text": "Names previously used for regions in Africa\nThis is a list of historical African place names. The names on the left are linked to the corresponding subregion(s) from History of Africa.\n*Northern Rhodesia - Zambia\n*Southern Rhodesia - Zimbabwe\n*(Southern Rhodesia was commonly referred to simply as Rhodesia from 1964 to 1980)"}
{"id": "14109", "revid": "13220696", "url": "https://en.wikipedia.org/wiki?curid=14109", "title": "Horror fiction", "text": "Literary genre\nHorror is a genre of speculative fiction that is intended to disturb, frighten, or scare an audience. Horror is often divided into the sub-genres of psychological horror and supernatural horror. Literary historian J. A. Cuddon, in 1984, defined the horror story as \"a piece of fiction in prose of variable length... which shocks, or even frightens the reader, or perhaps induces a feeling of repulsion or loathing\". Horror intends to create an eerie and frightening atmosphere for the reader. Often the central menace of a work of horror fiction can be interpreted as a metaphor for larger fears of a society.\nHistory.\nBefore 1000.\nThe horror genre has ancient origins, with roots in folklore and religious traditions focusing on death, the afterlife, evil, the demonic, and the principle of the thing embodied in the person. These manifested in stories of beings such as demons, witches, vampires, werewolves, and ghosts. Some early European horror-fiction were the Ancient Greeks and Ancient Romans. Mary Shelley's well-known 1818 novel about Frankenstein was greatly influenced by the story of Hippolytus, whom Asclepius revives from death. Euripides wrote plays based on the story, \"Hippolytos Kalyptomenos\" and \"Hippolytus\". In Plutarch's \"Parallel Lives\" in the account of Cimon, the author describes the spirit of a murderer, Damon, who himself was murdered in a bathhouse in Chaeronea.\nPliny the Younger (61 to c. 113) tells the tale of Athenodorus Cananites, who bought a haunted house in Athens. Athenodorus was cautious since the house seemed inexpensive. While writing a book on philosophy, he was visited by a ghostly figure bound in chains. The figure disappeared in the courtyard; the following day, the magistrates dug in the courtyard and found an unmarked grave.\nElements of the horror genre also occur in Biblical texts, notably in the Book of Revelation.\nAfter 1000.\nThe Witch of Berkeley by William of Malmesbury has been viewed as an early horror story. Werewolf stories were popular in medieval French literature. One of Marie de France's twelve lais is a werewolf story titled \"Bisclavret\".\nThe Countess Yolande commissioned a werewolf story titled \"Guillaume de Palerme\". Anonymous writers penned two werewolf stories, \"Biclarel\" and \"Melion\".\nMuch horror fiction derives from the cruellest personages of the 15th century. Dracula can be traced to the Prince of Wallachia Vlad III, whose alleged war crimes were published in German pamphlets. A 1499 pamphlet was published by Markus Ayrer, which is most notable for its woodcut imagery. The alleged serial-killer sprees of Gilles de Rais have been seen as the presumed inspiration for \"Bluebeard\". The motif of the vampiress is most notably derived from the real-life noblewoman and murderer, Elizabeth Bathory, and helped usher in the emergence of horror fiction in the 18th century, such as through L\u00e1szl\u00f3 Tur\u00f3czi's 1729 book \"Tragica Historia\".\n18th century.\nThe 18th century saw the gradual development of Romanticism and the Gothic horror genre. It drew on the written and material heritage of the Late Middle Ages, finding its form with Horace Walpole's seminal and controversial 1764 novel, \"The Castle of Otranto\". In fact, the first edition was published disguised as an actual medieval romance from Italy, discovered and republished by a fictitious translator. Once revealed as modern, many found it anachronistic, reactionary, or simply in poor taste, but it proved immediately popular.\n\"Otranto\" inspired \"Vathek\" (1786) by William Beckford, \"A Sicilian Romance\" (1790), \"The Mysteries of Udolpho\" (1794), \"The Italian\" (1796) by Ann Radcliffe, and \"The Monk\" (1797) by Matthew Lewis. A significant amount of horror fiction of this era was written by women and marketed towards a female audience, a typical scenario of the novels being a resourceful female menaced in a gloomy castle.\n19th century.\nThe Gothic tradition blossomed into the genre that modern readers today call horror literature in the 19th century. Influential works and characters that continue resonating in fiction and film today saw their genesis in the Brothers Grimm's \"H\u00e4nsel und Gretel\" (1812), Mary Shelley's \"Frankenstein; or, The Modern Prometheus\" (1818), John Polidori's \"The Vampyre\" (1819), Charles Maturin's \"Melmoth the Wanderer\" (1820), Washington Irving's \"The Legend of Sleepy Hollow\" (1820), Jane C. Loudon's \"\" (1827), Victor Hugo's \"The Hunchback of Notre-Dame\" (1831), Thomas Peckett Prest's \"Varney the Vampire\" (1847), the works of Edgar Allan Poe, the works of Sheridan Le Fanu, Robert Louis Stevenson's \"Strange Case of Dr Jekyll and Mr Hyde\" (1886), Oscar Wilde's \"The Picture of Dorian Gray\" (1890), Sir Arthur Conan Doyle's \"Lot No. 249\" (1892), H. G. Wells' \"The Invisible Man\" (1897), and Bram Stoker's \"Dracula\" (1897). Each of these works created an enduring icon of horror seen in later re-imaginings on the page, stage, and screen.\n20th century.\nA proliferation of cheap periodicals around the turn of the century led to a boom in horror writing. For example, Gaston Leroux serialized his \"Le Fant\u00f4me de l'Op\u00e9ra\" before it became a novel in 1910. One writer who specialized in horror fiction for mainstream pulps, such as \"All-Story Magazine,\" was Tod Robbins, whose fiction deals with themes of madness and cruelty. In Russia, the writer Alexander Belyaev popularized these themes in his story \"Professor Dowell's Head\" (1925), in which a mad doctor performs experimental head transplants and reanimations on bodies stolen from the morgue and which was first published as a magazine serial before being turned into a novel. Later, specialist publications emerged to give horror writers an outlet, prominent among them was \"Weird Tales\" and \"Unknown Worlds\".\nInfluential horror writers of the early 20th century made inroads in these mediums. Particularly, the venerated horror author H. P. Lovecraft, and his enduring Cthulhu Mythos transformed and popularized the genre of cosmic horror, and M. R. James is credited with redefining the ghost story in that era.\nThe serial murderer became a recurring theme. Yellow journalism and sensationalism of various murderers, such as Jack the Ripper, and lesser so, Carl Panzram, Fritz Haarman, and Albert Fish, all perpetuated this phenomenon. The trend continued in the postwar era, partly renewed after the murders committed by Ed Gein. In 1959, Robert Bloch, inspired by the murders, wrote \"Psycho\". The crimes committed in 1969 by the Manson Family influenced the slasher theme in horror fiction of the 1970s. In 1981, Thomas Harris wrote \"Red Dragon\", introducing Dr. Hannibal Lecter. In 1988, the sequel to that novel, \"The Silence of the Lambs\", was published.\nEarly cinema was inspired by many aspects of horror literature, and started a strong tradition of horror films and subgenres that continues to this day. Up until the graphic depictions of violence and gore on the screen commonly associated with 1960s and 1970s slasher films and splatter films, comic books such as those published by EC Comics (most notably \"Tales From The Crypt\") in the 1950s satisfied readers' quests for horror imagery that the silver screen could not provide. This imagery made these comics controversial, and as a consequence, they were frequently censored.\nThe modern zombie tale dealing with the motif of the living dead harks back to works including H. P. Lovecraft's stories \"Cool Air\" (1925), \"In The Vault\" (1926), and \"The Outsider\" (1926), and Dennis Wheatley's \"Strange Conflict\" (1941). Richard Matheson's novel \"I Am Legend\" (1954) influenced an entire genre of apocalyptic zombie fiction emblematized by the films of George A. Romero.\nIn the late 1960s and early 1970s, the enormous commercial success of three books \u2013 \"Rosemary's Baby\" (1967) by Ira Levin, \"The Exorcist\" by William Peter Blatty, and \"The Other\" by Thomas Tryon \u2013 encouraged publishers to begin releasing numerous other horror novels, thus creating a \"horror boom\".\nOne of the best-known late-20th century horror writers is Stephen King, known for \"Carrie\", \"The Shining\", \"It\", \"Misery\", and several dozen other novels and about 200 short stories. Beginning in the 1970s, King's stories have attracted a large audience, for which he was awarded by the U.S. National Book Foundation in 2003. Other popular horror authors of the period included Anne Rice, Shaun Hutson, Brian Lumley, Graham Masterton, James Herbert, Dean Koontz, Richard Laymon, Clive Barker, Ramsey Campbell, and Peter Straub.\n21st century.\nBest-selling book series of contemporary times exist in genres related to horror fiction, such as the Kitty Norville books by Carrie Vaughn that contain blend werewolf fiction and urban fantasy (2005 onward). Horror elements continue to expand outside the genre. The alternate history of more traditional historical horror in Dan Simmons's 2007 novel \"The Terror\" sits on bookstore shelves next to genre mash ups such as \"Pride and Prejudice and Zombies\" (2009), and historical fantasy and horror comics such as \"Hellblazer\" (1993 onward) and Mike Mignola's Hellboy (1993 onward). Horror also serves as one of the central genres in more complex modern works such as Mark Z. Danielewski's \"House of Leaves\" (2000), a finalist for the National Book Award. Like Danielewski, many authors have opted to publish their works online, with notable examples including \"Ben Drowned\" by Alex Hall and \"Candle Cove\" by Kris Straub. There are many horror novels for children and teens, such as R. L. Stine's Goosebumps series or \"The Monstrumologist\" by Rick Yancey. Additionally, many movies for young audiences, particularly animated ones, use horror aesthetics and conventions (for example, \"ParaNorman\"). These are what can be collectively referred to as \"children's horror\". Although it is unknown for sure why children enjoy these movies (as it seems counter-intuitive), it is theorized that it is, in part, grotesque monsters that fascinate kids. Tangential to this, the internalized impact of horror television programs and films on children is rather under-researched, especially when compared to the research done on the similar subject of violence in TV and film's impact on the young mind. What little research there is tends to be inconclusive on the impact that viewing such media has.\nCharacteristics.\nOne defining trait of the horror genre is that it provokes an emotional, psychological, or physical response within readers that causes them to react with fear. One of H. P. Lovecraft's most famous quotes about the genre is that: \"The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown.\" the first sentence from his seminal essay, \"Supernatural Horror in Literature\". Science fiction historian Darrell Schweitzer has stated, \"In the simplest sense, a horror story is one that scares us\" and \"the true horror story requires a sense of evil, not in necessarily in a theological sense; but the menaces must be truly menacing, life-destroying, and antithetical to happiness.\"\nIn her essay \"Elements of Aversion\", Elizabeth Barrette articulates the need by some for horror tales in a modern world:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The old \"fight or flight\" reaction of our evolutionary heritage once played a major role in the life of every human. Our ancestors lived and died by it. Then someone invented the fascinating game of civilization, and things began to calm down. Development pushed wilderness back from settled lands. War, crime, and other forms of social violence came with civilization and humans started preying on each other, but by and large daily life calmed down. We began to feel restless, to feel something missing: the excitement of living on the edge, the tension between hunter and hunted. So we told each other stories through the long, dark nights. when the fires burned low, we did our best to scare the daylights out of each other. The rush of adrenaline feels good. Our hearts pound, our breath quickens, and we can imagine ourselves on the edge. Yet we also appreciate the insightful aspects of horror. Sometimes a story intends to shock and disgust, but the best horror intends to rattle our cages and shake us out of our complacency. It makes us think, forces us to confront ideas we might rather ignore, and challenges preconceptions of all kinds. Horror reminds us that the world is not always as safe as it seems, which exercises our mental muscles and reminds us to keep a little healthy caution close at hand.\nIn a sense similar to the reason a person seeks out the controlled thrill of a roller coaster, readers in the modern era seek out feelings of horror and terror to feel a sense of excitement. However, Barrette adds that horror fiction is one of the few mediums where readers seek out a form of art that forces themselves to confront ideas and images they \"might rather ignore to challenge preconceptions of all kinds.\"\nThere are many theories as to why people enjoy being scared. For example, \"people who like horror films are more likely to score highly for openness to experience, a personality trait linked to intellect and imagination.\"\nIt is a now commonly accepted view that the horror elements of Dracula's portrayal of vampirism are metaphors for sexuality in a repressed Victorian era. But this is merely one of many interpretations of the metaphor of Dracula. Jack Halberstam postulates many of these in his essay \"Technologies of Monstrosity: Bram Stoker's Dracula\". He writes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[The] image of dusty and unused gold, coins from many nations and old unworn jewels, immediately connects Dracula to the old money of a corrupt class, to a kind of piracy of nations and to the worst excesses of the aristocracy.\nHalberstram articulates a view of Dracula as manifesting the growing perception of the aristocracy as an evil and outdated notion to be defeated. The depiction of a multinational band of protagonists using the latest technologies (such as a telegraph) to quickly share, collate, and act upon new information is what leads to the destruction of the vampire. This is one of many interpretations of the metaphor of only one central figure of the canon of horror fiction, as over a dozen possible metaphors are referenced in the analysis, from the religious to the antisemitic.\nNo\u00ebl Carroll's \"Philosophy of Horror\" postulates that a modern piece of horror fiction's \"monster\", villain, or a more inclusive menace must exhibit the following two traits:\nScholarship and criticism.\nIn addition to those essays and articles shown above, scholarship on horror fiction is almost as old as horror fiction itself. In 1826, the gothic novelist Ann Radcliffe published an essay distinguishing two elements of horror fiction, \"terror\" and \"horror.\" Whereas terror is a feeling of dread that takes place before an event happens, horror is a feeling of revulsion or disgust after an event has happened. Radcliffe describes terror as that which \"expands the soul and awakens the faculties to a high degree of life,\" whereas horror is described as that which \"freezes and nearly annihilates them.\"\nModern scholarship on horror fiction draws upon a range of sources. In their historical studies of the gothic novel, both Devendra Varma and S. L. Varnado make reference to the theologian Rudolf Otto, whose concept of the \"numinous\" was originally used to describe religious experience.\nA recent survey reports how often horror media is consumed:To assess frequency of horror consumption, we asked respondents the following question: \"In the past year, about how often have you used horror media (for example, horror literature, film, and video games) for entertainment?\" 11.3% said \"Never,\" 7.5% \"Once,\" 28.9% \"Several times,\" 14.1% \"Once a month,\" 20.8% \"Several times a month,\" 7.3% \"Once a week,\" and 10.2% \"Several times a week.\" Evidently, then, most respondents (81.3%) claimed to use horror media several times a year or more often. Unsurprisingly, there is a strong correlation between liking and frequency of use (r=.79, p&lt;.0001). \nAwards and associations.\nAchievements in horror fiction are recognized by numerous awards. The Horror Writers Association presents the Bram Stoker Awards for Superior Achievement, named in honor of Bram Stoker, author of the seminal horror novel \"Dracula\". The Australian Horror Writers Association presents annual Australian Shadows Awards. The International Horror Guild Award was presented annually to works of horror and dark fantasy from 1995 to 2008. The Shirley Jackson Awards are literary awards for outstanding achievement in the literature of psychological suspense, horror, and the dark fantastic works. Other important awards for horror literature are included as subcategories within general awards for fantasy and science fiction in such awards as the Aurealis Award.\nAlternative terms.\nSome writers of fiction normally classified as \"horror\" tend to dislike the term, considering it too lurid. They instead use the terms dark fantasy or Gothic fantasy for supernatural horror, or \"psychological thriller\" for non-supernatural horror.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14110", "revid": "38005489", "url": "https://en.wikipedia.org/wiki?curid=14110", "title": "Holomorphic function", "text": "Complex-differentiable (mathematical) function\nIn mathematics, a holomorphic function is a complex-valued function of one or more complex variables that is complex differentiable in a neighbourhood of each point in a domain in complex coordinate space &amp;NoBreak;&amp;NoBreak;. The existence of a complex derivative in a neighbourhood is a very strong condition: It implies that a holomorphic function is infinitely differentiable and locally equal to its own Taylor series (is \"analytic\"). Holomorphic functions are the central objects of study in complex analysis.\nThough the term \"analytic function\" is often used interchangeably with \"holomorphic function\", the word \"analytic\" is defined in a broader sense to denote any function (real, complex, or of more general type) that can be written as a convergent power series in a neighbourhood of each point in its domain. That all holomorphic functions are complex analytic functions, and vice versa, is a major theorem in complex analysis.\nHolomorphic functions are also sometimes referred to as \"regular functions\". A holomorphic function whose domain is the whole complex plane is called an entire function. The phrase \"holomorphic at a point &amp;NoBreak;&amp;NoBreak;\" means not just differentiable at &amp;NoBreak;&amp;NoBreak;, but differentiable everywhere within some close neighbourhood of &amp;NoBreak;&amp;NoBreak; in the complex plane.\nDefinition.\nGiven a complex-valued function &amp;NoBreak;&amp;NoBreak; of a single complex variable, the derivative of &amp;NoBreak;&amp;NoBreak; at a point &amp;NoBreak;&amp;NoBreak; in its domain is defined as the limit\nformula_1\nThis is the same definition as for the derivative of a real function, except that all quantities are complex. In particular, the limit is taken as the complex number &amp;NoBreak;&amp;NoBreak; tends to &amp;NoBreak;&amp;NoBreak;, and this means that the same value is obtained for any sequence of complex values for &amp;NoBreak;&amp;NoBreak; that tends to &amp;NoBreak;&amp;NoBreak;. If the limit exists, &amp;NoBreak;&amp;NoBreak; is said to be complex differentiable at &amp;NoBreak;&amp;NoBreak;. This concept of complex differentiability shares several properties with real differentiability: It is linear and obeys the product rule, quotient rule, and chain rule.\nA function is holomorphic on an open set &amp;NoBreak;&amp;NoBreak; if it is \"complex differentiable\" at \"every\" point of &amp;NoBreak;&amp;NoBreak;. A function &amp;NoBreak;&amp;NoBreak; is \"holomorphic\" at a point &amp;NoBreak;&amp;NoBreak; if it is holomorphic on some neighbourhood of &amp;NoBreak;&amp;NoBreak;.\nA function is \"holomorphic\" on some non-open set &amp;NoBreak;&amp;NoBreak; if it is holomorphic at every point of &amp;NoBreak;&amp;NoBreak;.\nA function may be complex differentiable at a point but not holomorphic at this point. For example, the function formula_2 \"is\" complex differentiable at &amp;NoBreak;&amp;NoBreak;, but \"is not\" complex differentiable anywhere else, esp. including in no place close to &amp;NoBreak;&amp;NoBreak; (see the Cauchy\u2013Riemann equations, below). So, it is \"not\" holomorphic at &amp;NoBreak;&amp;NoBreak;.\nThe relationship between real differentiability and complex differentiability is the following: If a complex function &amp;NoBreak;&amp;NoBreak; is holomorphic, then &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; have first partial derivatives with respect to &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak;, and satisfy the Cauchy\u2013Riemann equations:\nformula_3\nor, equivalently, the Wirtinger derivative of &amp;NoBreak;&amp;NoBreak; with respect to &amp;NoBreak;&amp;NoBreak;, the complex conjugate of &amp;NoBreak;&amp;NoBreak;, is zero:\nformula_4\nwhich is to say that, roughly, &amp;NoBreak;&amp;NoBreak; is functionally independent from &amp;NoBreak;&amp;NoBreak;, the complex conjugate of &amp;NoBreak;&amp;NoBreak;.\nIf continuity is not given, the converse is not necessarily true. A simple converse is that if &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; have \"continuous\" first partial derivatives and satisfy the Cauchy\u2013Riemann equations, then &amp;NoBreak;&amp;NoBreak; is holomorphic. A more satisfying converse, which is much harder to prove, is the Looman\u2013Menchoff theorem: if &amp;NoBreak;&amp;NoBreak; is continuous, &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; have first partial derivatives (but not necessarily continuous), and they satisfy the Cauchy\u2013Riemann equations, then &amp;NoBreak;&amp;NoBreak; is holomorphic.\nAn immediate useful consequence of the Cauchy Riemann Equations above is that the complex derivative can be defined explicitly in terms of real partial derivatives. If formula_5 is a complex function that is complex differentiable about a point formula_6 then (as we did earlier in the article) we can write formula_7 and then the complex derivative of the function can be written as formula_8 \nTerminology.\nThe term \"holomorphic\" was introduced in 1875 by Charles Briot and Jean-Claude Bouquet, two of Augustin-Louis Cauchy's students, and derives from the Greek \u1f45\u03bb\u03bf\u03c2 (\"h\u00f3los\") meaning \"whole\", and \u03bc\u03bf\u03c1\u03c6\u03ae (\"morph\u1e17\") meaning \"form\" or \"appearance\" or \"type\", in contrast to the term \"meromorphic\" derived from \u03bc\u03ad\u03c1\u03bf\u03c2 (\"m\u00e9ros\") meaning \"part\". A holomorphic function resembles an entire function (\"whole\") in a domain of the complex plane while a meromorphic function (defined to mean holomorphic except at certain isolated poles), resembles a rational fraction (\"part\") of entire functions in a domain of the complex plane. Cauchy had instead used the term \"synectic\".\nToday, the term \"holomorphic function\" is sometimes preferred to \"analytic function\". An important result in complex analysis is that every holomorphic function is complex analytic, a fact that does not follow obviously from the definitions. The term \"analytic\" is however also in wide use.\nProperties.\nBecause complex differentiation is linear and obeys the product, quotient, and chain rules, the sums, products and compositions of holomorphic functions are holomorphic, and the quotient of two holomorphic functions is holomorphic wherever the denominator is not zero. That is, if functions &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; are holomorphic in a domain &amp;NoBreak;&amp;NoBreak;, then so are &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, and &amp;NoBreak;&amp;NoBreak;. Furthermore, &amp;NoBreak;&amp;NoBreak; is holomorphic if &amp;NoBreak;&amp;NoBreak; has no zeros in &amp;NoBreak;&amp;NoBreak;; otherwise it is meromorphic.\nIf one identifies &amp;NoBreak;&amp;NoBreak; with the real plane &amp;NoBreak;&amp;NoBreak;, then the holomorphic functions coincide with those functions of two real variables with continuous first derivatives which solve the Cauchy\u2013Riemann equations, a set of two partial differential equations.\nEvery holomorphic function can be separated into its real and imaginary parts &amp;NoBreak;&amp;NoBreak;, and each of these is a harmonic function on &amp;NoBreak;&amp;NoBreak; (each satisfies Laplace's equation &amp;NoBreak;&amp;NoBreak;), with &amp;NoBreak;&amp;NoBreak; the harmonic conjugate of &amp;NoBreak;&amp;NoBreak;.\nConversely, every harmonic function &amp;NoBreak;&amp;NoBreak; on a simply connected domain &amp;NoBreak;&amp;NoBreak; is the real part of a holomorphic function: If &amp;NoBreak;&amp;NoBreak; is the harmonic conjugate of &amp;NoBreak;&amp;NoBreak;, unique up to a constant, then &amp;NoBreak;&amp;NoBreak; is holomorphic.\nCauchy's integral theorem implies that the contour integral of every holomorphic function along a loop vanishes:\nformula_9\nHere &amp;NoBreak;&amp;NoBreak; is a rectifiable path in a simply connected complex domain &amp;NoBreak;&amp;NoBreak; whose start point is equal to its end point, and &amp;NoBreak;&amp;NoBreak; is a holomorphic function.\nCauchy's integral formula states that every function holomorphic inside a disk is completely determined by its values on the disk's boundary. Furthermore: Suppose &amp;NoBreak;&amp;NoBreak; is a complex domain, &amp;NoBreak;&amp;NoBreak; is a holomorphic function and the closed disk formula_10 is completely contained in &amp;NoBreak;&amp;NoBreak;. Let &amp;NoBreak;&amp;NoBreak; be the circle forming the boundary of &amp;NoBreak;&amp;NoBreak;. Then for every &amp;NoBreak;&amp;NoBreak; in the interior of &amp;NoBreak;&amp;NoBreak;:\nformula_11\nwhere the contour integral is taken counter-clockwise.\nThe derivative &amp;NoBreak;&amp;NoBreak; can be written as a contour integral using Cauchy's differentiation formula:\nformula_12\nfor any simple loop positively winding once around &amp;NoBreak;&amp;NoBreak;, and\nformula_13\nfor infinitesimal positive loops &amp;NoBreak;&amp;NoBreak; around &amp;NoBreak;&amp;NoBreak;.\nIn regions where the first derivative is not zero, holomorphic functions are conformal: they preserve angles and the shape (but not size) of small figures.\nEvery holomorphic function is analytic. That is, a holomorphic function &amp;NoBreak;&amp;NoBreak; has derivatives of every order at each point &amp;NoBreak;&amp;NoBreak; in its domain, and it coincides with its own Taylor series at &amp;NoBreak;&amp;NoBreak; in a neighbourhood of &amp;NoBreak;&amp;NoBreak;. In fact, &amp;NoBreak;&amp;NoBreak; coincides with its Taylor series at &amp;NoBreak;&amp;NoBreak; in any disk centred at that point and lying within the domain of the function.\nFrom an algebraic point of view, the set of holomorphic functions on an open set is a commutative ring and a complex vector space. Additionally, the set of holomorphic functions in an open set &amp;NoBreak;&amp;NoBreak; is an integral domain if and only if the open set &amp;NoBreak;&amp;NoBreak; is connected. In fact, it is a locally convex topological vector space, with the seminorms being the suprema on compact subsets.\nFrom a geometric perspective, a function &amp;NoBreak;&amp;NoBreak; is holomorphic at &amp;NoBreak;&amp;NoBreak; if and only if its exterior derivative &amp;NoBreak;&amp;NoBreak; in a neighbourhood &amp;NoBreak;&amp;NoBreak; of &amp;NoBreak;&amp;NoBreak; is equal to &amp;NoBreak;&amp;NoBreak; for some continuous function &amp;NoBreak;&amp;NoBreak;. It follows from\nformula_14\nthat &amp;NoBreak;&amp;NoBreak; is also proportional to &amp;NoBreak;&amp;NoBreak;, implying that the derivative &amp;NoBreak;&amp;NoBreak; is itself holomorphic and thus that &amp;NoBreak;&amp;NoBreak; is infinitely differentiable. Similarly, &amp;NoBreak;&amp;NoBreak; implies that any function &amp;NoBreak;&amp;NoBreak; that is holomorphic on the simply connected region &amp;NoBreak;&amp;NoBreak; is also integrable on &amp;NoBreak;&amp;NoBreak;.\nFor a path &amp;NoBreak;&amp;NoBreak; from &amp;NoBreak;&amp;NoBreak; to &amp;NoBreak;&amp;NoBreak; lying entirely in &amp;NoBreak;&amp;NoBreak;, define\n&amp;NoBreak;&amp;NoBreak;\nIn light of the Jordan curve theorem and the generalized Stokes' theorem, &amp;NoBreak;&amp;NoBreak; is independent of the particular choice of path &amp;NoBreak;&amp;NoBreak;, and thus &amp;NoBreak;&amp;NoBreak; is a well-defined function on &amp;NoBreak;&amp;NoBreak; having &amp;NoBreak;&amp;NoBreak;, or equivalently &amp;NoBreak;&amp;NoBreak;.\nExamples.\nAll polynomial functions in &amp;NoBreak;&amp;NoBreak; with complex coefficients are entire functions (holomorphic in the whole complex plane &amp;NoBreak;&amp;NoBreak;), and so are the exponential function &amp;NoBreak;&amp;NoBreak; and the trigonometric functions &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; (cf. Euler's formula). The principal branch of the complex logarithm function &amp;NoBreak;&amp;NoBreak; is holomorphic on the domain &amp;NoBreak;}&amp;NoBreak;. The square root function can be defined as &amp;NoBreak;&amp;NoBreak; and is therefore holomorphic wherever the logarithm &amp;NoBreak;&amp;NoBreak; is. The reciprocal function &amp;NoBreak;}&amp;NoBreak; is holomorphic on &amp;NoBreak;}&amp;NoBreak;. (The reciprocal function, and any other rational function, is meromorphic on &amp;NoBreak;&amp;NoBreak;.)\nAs a consequence of the Cauchy\u2013Riemann equations, any real-valued holomorphic function must be constant. Therefore, the absolute value &lt;math&gt; the argument &amp;NoBreak;&amp;NoBreak;, the real part &amp;NoBreak;&amp;NoBreak; and the imaginary part &amp;NoBreak;&amp;NoBreak; are not holomorphic. Another typical example of a continuous function which is not holomorphic is the complex conjugate &amp;NoBreak;&amp;NoBreak; (The complex conjugate is antiholomorphic.)\nSeveral variables.\nThe definition of a holomorphic function generalizes to several complex variables in a straightforward way. A function &amp;NoBreak;&amp;NoBreak; in &amp;NoBreak;&amp;NoBreak; complex variables is analytic at a point &amp;NoBreak;&amp;NoBreak; if there exists a neighbourhood of &amp;NoBreak;&amp;NoBreak; in which &amp;NoBreak;&amp;NoBreak; is equal to a convergent power series in &amp;NoBreak;&amp;NoBreak; complex variables;\nthe function &amp;NoBreak;&amp;NoBreak; is holomorphic in an open subset &amp;NoBreak;&amp;NoBreak; of &amp;NoBreak;&amp;NoBreak; if it is analytic at each point in &amp;NoBreak;&amp;NoBreak;. Osgood's lemma shows (using the multivariate Cauchy integral formula) that, for a continuous function &amp;NoBreak;&amp;NoBreak;, this is equivalent to &amp;NoBreak;&amp;NoBreak; being holomorphic in each variable separately (meaning that if any &amp;NoBreak;&amp;NoBreak; coordinates are fixed, then the restriction of &amp;NoBreak;&amp;NoBreak; is a holomorphic function of the remaining coordinate). The much deeper Hartogs' theorem proves that the continuity assumption is unnecessary: &amp;NoBreak;&amp;NoBreak; is holomorphic if and only if it is holomorphic in each variable separately.\nMore generally, a function of several complex variables that is square integrable over every compact subset of its domain is analytic if and only if it satisfies the Cauchy\u2013Riemann equations in the sense of distributions.\nFunctions of several complex variables are in some basic ways more complicated than functions of a single complex variable. For example, the region of convergence of a power series is not necessarily an open ball; these regions are logarithmically convex Reinhardt domains, the simplest example of which is a polydisk. However, they also come with some fundamental restrictions. Unlike functions of a single complex variable, the possible domains on which there are holomorphic functions that cannot be extended to larger domains are highly limited. Such a set is called a domain of holomorphy.\nA complex differential &amp;NoBreak;&amp;NoBreak;-form &amp;NoBreak;&amp;NoBreak; is holomorphic if and only if its antiholomorphic Dolbeault derivative is zero: &amp;NoBreak;&amp;NoBreak;.\nExtension to functional analysis.\nThe concept of a holomorphic function can be extended to the infinite-dimensional spaces of functional analysis. For instance, the Fr\u00e9chet or Gateaux derivative can be used to define a notion of a holomorphic function on a Banach space over the field of complex numbers.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14113", "revid": "44840730", "url": "https://en.wikipedia.org/wiki?curid=14113", "title": "History of Algeria", "text": "Much of the history of Algeria has taken place on the fertile coastal plain of North Africa, which is often called the Maghreb. North Africa served as a transit region for people moving towards Europe or the Middle East, thus, the region's inhabitants have been influenced by populations from other areas, including the Carthaginians, Romans, and Vandals. The region was conquered by the Muslims in the early 8th century AD, but broke off from the Umayyad Caliphate after the Berber Revolt of 740. During the Ottoman period, Algeria became an important state in the Mediterranean sea which led to many naval conflicts. The last significant events in the country's recent history have been the Algerian War and Algerian Civil War.\nPrehistory.\nEvidence of the early human occupation of Algeria is demonstrated by the discovery of 1.8 million year old Oldowan stone tools found at Ain Hanech in 1992. In 1954 fossilised \"Homo erectus\" bones were discovered by C. Arambourg at Ternefine that are 700,000 years old. Neolithic civilization (marked by animal domestication and subsistence agriculture) developed in the Saharan and Mediterranean Maghrib between 6000 and 2000 BC. This type of economy, richly depicted in the Tassili n'Ajjer cave paintings in southeastern Algeria, predominated in the Maghrib until the classical period.\nNumidia.\nNumidia (Berber: \"Inumiden\"; 202\u201340 BC) was the ancient kingdom of the Numidians located in northwest Africa, initially comprising the territory that now makes up modern-day Algeria, but later expanding across what is today known as Tunisia, Libya, and some parts of Morocco. The polity was originally divided between the Massylii in the east and the Masaesyli in the west. During the Second Punic War (218\u2013201 BC), Masinissa, king of the Massylii, defeated Syphax of the Masaesyli to unify Numidia into one kingdom. The kingdom began as a sovereign state and later alternated between being a Roman province and a Roman client state.\nNumidia, at its largest extent, was bordered by Mauretania to the west, at the Moulouya River, Africa to the east (also exercising control over Tripolitania), the Mediterranean Sea to the north, and the Sahara to the south. It was one of the first major states in the history of Algeria and the Berbers.\nWar With Rome.\nBy 112 BC, Jugurtha resumed his war with Adherbal. He incurred the wrath of Rome in the process by killing some Roman businessmen who were aiding Adherbal. After a brief war with Rome, Jugurtha surrendered and received a highly favourable peace treaty, which raised suspicions of bribery once more. The local Roman commander was summoned to Rome to face corruption charges brought by his political rival Gaius Memmius. Jugurtha was also forced to come to Rome to testify against the Roman commander, where Jugurtha was completely discredited once his violent and ruthless past became widely known, and after he had been suspected of murdering a Numidian rival.\nWar broke out between Numidia and the Roman Republic and several legions were dispatched to North Africa under the command of the Consul Quintus Caecilius Metellus Numidicus. The war dragged out into a long and seemingly endless campaign as the Romans tried to defeat Jugurtha decisively. Frustrated at the apparent lack of action, Metellus' lieutenant Gaius Marius returned to Rome to seek election as Consul. Marius was elected, and then returned to Numidia to take control of the war. He sent his Quaestor Sulla to neighbouring Mauretania in order to eliminate their support for Jugurtha. With the help of Bocchus I of Mauretania, Sulla captured Jugurtha and brought the war to a conclusive end. Jugurtha was brought to Rome in chains and was placed in the Tullianum.\nJugurtha was executed by the Romans in 104 BC, after being paraded through the streets in Gaius Marius' Triumph.\nIndependence.\nThe Greek historians referred to these peoples as \"\u039d\u03bf\u03bc\u03ac\u03b4\u03b5\u03c2\" (i.e. Nomads), which by Latin interpretation became \"Numidae\" (but cf. also the correct use of \"Nomades\"). Historian Gabriel Camps, however, disputes this claim, favoring instead an African origin for the term.\nThe name appears first in Polybius (second century BC) to indicate the peoples and territory west of Carthage including the entire north of Algeria as far as the river Mulucha (Muluya), about 160 kilometres (100\u00a0mi) west of Oran.\nThe Numidians were composed of two great tribal groups: the Massylii in eastern Numidia, and the Masaesyli in the west. During the first part of the Second Punic War, the eastern Massylii, under their king Gala, were allied with Carthage, while the western Masaesyli, under king Syphax, were allied with Rome. The Kingdom of Masaesyli under Syphax extended from the Moulouya river to Oued Rhumel.\nHowever, in 206 BC, the new king of the eastern Massylii, Masinissa, allied himself with Rome, and Syphax of the Masaesyli switched his allegiance to the Carthaginian side. At the end of the war, the victorious Romans gave all of Numidia to Masinissa of the Massylii. At the time of his death in 148 BC, Masinissa's territory extended from the Moulouya to the boundary of the Carthaginian territory, and also southeast as far as Cyrenaica to the gulf of Sirte, so that Numidia entirely surrounded Carthage (Appian, \"Punica\", 106) except towards the sea. Furthermore, after the capture of Syphax the king in modern day Morocco with his capital based in Tingis, Bokkar, had become a vassal of Massinissa. Massinissa had also penetrated as far south beyond the Atlas to the Gaetuli and Fezzan was part of his domain.\nIn 179 B.C. Masinissa had received a golden crown from the inhabitants of Delos as he had offered them a shipload of grain. A statue of Masinissa was set up in Delos in honour of him as well as an inscription dedicated to him in Delos by a native from Rhodes. His sons too had statues of them erected on the island of Delos and the King of Bithynia, Nicomedes, had also dedicated a statue to Masinissa.\nAfter the death of the long-lived Masinissa around 148 BC, he was succeeded by his son Micipsa. When Micipsa died in 118 BC, he was succeeded jointly by his two sons Hiempsal I and Adherbal and Masinissa's illegitimate grandson, Jugurtha, who was very popular among the Numidians. Hiempsal and Jugurtha quarrelled immediately after the death of Micipsa. Jugurtha had Hiempsal killed, which led to open war with Adherbal.\nPhoenician traders arrived on the North African coast around 900 BC and established Carthage (in present-day Tunisia) around 800 BC. During the classical period, Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others.\nThe Carthaginian state declined because of successive defeats by the Romans in the Punic Wars, and in 146 BC, the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew.\nBy the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. After that, king Masinissa managed to unify Numidia under his rule.\nRoman empire.\nChristianity arrived in the 2nd century. By the end of the 4th century, the settled areas had become Christianized, and some Berber tribes had converted en masse.\nAfter the fall of the Western Roman Empire, Algeria came under the control of the Vandal Kingdom. Later, the Eastern Roman Empire (also known as the Byzantine Empire) conquered Algeria from the Vandals, incorporating it into the Praetorian prefecture of Africa and later the Exarchate of Africa.\nMedieval Muslim Algeria.\nFrom the 8th century Umayyad conquest of North Africa led by Musa bin Nusayr, Arab colonization started. The 11th century invasion of migrants from the Arabian peninsula brought oriental tribal customs. The introduction of Islam and Arabic had a profound impact on North Africa. The new religion and language introduced changes in social and economic relations, and established links with the Arab world through acculturation and assimilation.\nThe second Arab military expeditions into the Maghreb, between 642 and 669, resulted in the spread of Islam. The Umayyads (a Muslim dynasty based in Damascus from 661 to 750) recognised that the strategic necessity of dominating the Mediterranean dictated a concerted military effort on the North African front. By 711 Umayyad forces helped by Berber converts to Islam had conquered all of North Africa. In 750 the Abbasids succeeded the Umayyads as Muslim rulers and moved the caliphate to Baghdad. Under the Abbasids, Berber Kharijites Sufri Banu Ifran were opposed to Umayyad and Abbasids. After, the Rustumids (761\u2013909) actually ruled most of the central Maghrib from Tahirt, southwest of Algiers. The imams gained a reputation for honesty, piety, and justice, and the court of Tahirt was noted for its support of scholarship. The Rustumid imams failed, however, to organise a reliable standing army, which opened the way for Tahirt's demise under the assault of the Fatimid dynasty.\nThe Fatimids left the rule of most of Algeria to the Zirids and Hammadid (972\u20131148), a Berber dynasty that centered significant local power in Algeria for the first time, but who were still at war with Banu Ifran (kingdom of Tlemcen) and Maghraoua (942-1068). This period was marked by constant conflict, political instability, and economic decline. Following a large incursion of Arab Bedouin from Egypt beginning in the first half of the 11th century, the use of Arabic spread to the countryside, and sedentary Berbers were gradually Arabised.\nThe Almoravid (\"those who have made a religious retreat\") movement developed early in the 11th century among the Sanhaja Berbers of southern Morocco. The movement's initial impetus was religious, an attempt by a tribal leader to impose moral discipline and strict adherence to Islamic principles on followers. But the Almoravid movement shifted to engaging in military conquest after 1054. By 1106, the Almoravids had conquered the Maghreb as far east as Algiers and Morocco, and Spain up to the Ebro River.\nLike the Almoravids, the Almohads (\"unitarians\") found their inspiration in Islamic reform. The Almohads took control of Morocco by 1146, captured Algiers around 1151, and by 1160 had completed the conquest of the central Maghrib. The zenith of Almohad power occurred between 1163 and 1199. For the first time, the Maghrib was united under a local regime, but the continuing wars in Spain overtaxed the resources of the Almohads, and in the Maghrib their position was compromised by factional strife and a renewal of tribal warfare.\nIn the central Maghrib, the Abdalwadid founded a dynasty that ruled the Kingdom of Tlemcen in Algeria. For more than 300 years, until the region came under Ottoman suzerainty in the 16th century, the Zayanids kept a tenuous hold in the central Maghrib. Many coastal cities asserted their autonomy as municipal republics governed by merchant oligarchies, tribal chieftains from the surrounding countryside, or the privateers who operated out of their ports. Nonetheless, Tlemcen, the \"pearl of the Maghrib,\" prospered as a commercial center.\nBerber dynasties.\nAccording to historians of the Middle Ages, the Berbers were divided into two branches, both going back to their ancestors Mazigh. The two branches, called Botr and Barn\u00e8s were divided into tribes, and each Maghreb region is made up of several tribes. The large Berber tribes or peoples are Sanhaja, Houara, Zenata, Masmuda, Kutama, Awarba, Barghawata ... etc. Each tribe is divided into sub tribes. All these tribes had independent and territorial decisions.\nSeveral Berber dynasties emerged during the Middle Ages: - In North and West Africa, in Spain (al-Andalus), Sicily, Egypt, as well as in the southern part of the Sahara, in modern-day Mali, Niger, and Senegal. The medieval historian Ibn Khaldun described the follying Berber dynasties: Zirid, Banu Ifran, Maghrawa, Almoravid, Hammadid, Almohad Caliphate, Marinid, Zayyanid, Wattasid, Meknes, Hafsid dynasty, Fatimids.\nThe invasion of the Banu Hilal Arab tribes in the 11th century sacked Kairouan, and the area under Zirid control was reduced to the coastal region, and the Arab conquests fragmented into petty Bedouin emirates.\nMaghrawa Dynasty.\nThe Maghrawa or Meghrawa (Arabic: \u0627\u0644\u0645\u063a\u0631\u0627\u0648\u064a\u0648\u0646) were a large Zenata Berber tribal confederation whose cradle and seat of power was the territory located on the Chlef in the north-western part of today's Algeria, bounded by the Ouarsenis to the south, the Mediterranean Sea to the north and Tlemcen to the west. They ruled these areas on behalf of the \"Umayyad\" Caliphate of Cordoba at the end of the 10th century and during the first half of the 11th century. The Maghrawa confederation of zanata Berbers supposedly originated in the region of modern Algeria between Tlemcen and Tenes.The confederation of Maghrawa were the majority people of the central Maghreb among the Zenata (Gaetuli). Both nomadic and sedentary, the Maghrawa lived under the command of Maghrawa chiefs or Zenata. Algiers has been the territory of the Maghrawa since ancient times. The name Maghrawa was transcribed into Greek by historians. The great kingdom of the Maghrawa was located between Algiers, Cherchell, T\u00e9n\u00e8s, Chlef, Miliana and M\u00e9d\u00e9a. The Maghrawa imposed their domination in the Aur\u00e8s.['] Chlef and its surroundings were populated by the Maghrawa according to Ibn Khaldun. The Maghrawa settled and extended their domination throughout the Dahra and beyond Miliana to the Tafna wadi near Tlemcen,['] and were found as far away as Mali.\nThe Maghrawa were one of the first Berber tribes to submit to Islam in the 7th century. They supported Uqba ibn Nafi in his campaign to the Atlantic in 683. They defected from Sunni Islam and became Kharijite Muslims from the 8th century, and allied first with the Idrisids, and, from the 10th century on, with the Umayyads of C\u00f3rdoba in Al-Andalus. As a result, they were caught up in the Umayyad-Fatimid conflict in Morocco and Algeria. Although they won a victory over the allies of the Fatimids in 924, they soon allied with them. When they switched back to the side of C\u00f3rdoba, the Zirids briefly took control over most of Morocco, and ruled on behalf of the Fatimids. In 976/977 the Maghrawa conquered Sijilmasa from the Banu Midrar, and in 980 were able to drive the Miknasa out of Sijilmasa as well.\nThe Maghrawa reached their peak under Ziri ibn Atiyya (to 1001), who achieved supremacy in Fez under Umayyad suzerainty, and expanded their territory at the expense of the Banu Ifran in the northern Maghreb \u2013 another Zenata tribe whose alliances had shifted often between the Fatimids and the Umayyads of C\u00f3rdoba. Ziri ibn Atiyya conquered as much as he could of what is now northern Morocco and was able to achieve supremacy in Fez by 987. In 989 he defeated his enemy, Abu al-Bah\u0101r, which resulted in Ziri ruling from Zab to Sous Al-Aqsa, in 991 achieving supremacy in the western Maghreb. As a result of his victory he was invited to C\u00f3rdoba by Ibn Abi 'Amir al-Mansur (also Latinized as Almanzor), the regent of Caliph Hisham II and \"de facto\" ruler of the Caliphate of C\u00f3rdoba. Ziri brought many gifts and Al-Mansur housed him in a lavish palace, but Ziri soon returned to North Africa. The Banu Ifran took advantage of his absence and, under Yadd\u016b, managed to capture Fez.[\"\"] After a bloody struggle, Ziri reconquered Fez in 993 and displayed Yadd\u016b's severed head on its walls.\nA period of peace followed, in which Ziri founded the city of Oujda in 994 and made it his capital. However, Ziri was loyal to the Umayyad caliphs in Cordoba and increasingly resented the way that Ibn Abi 'Amir was holding Hisham II captive while progressively usurping his power. In 997 Ziri rejected Ibn Abi 'Amir's authority and declared himself a direct supporter of Caliph Hisham II. Ibn Abi 'Amir sent an invasion force to Morocco. After three unsuccessful months, Ibn Abi 'Amir's army was forced to retreat to the safety of Tangiers, so Ibn Abi 'Amir sent a powerful reinforcements under his son Abd al-Malik. The armies clashed near Tangiers, and in this battle, Ziri was stabbed by an African soldier who reported to Abd al-Malik that he had seriously wounded the Zenata leader. Abd al-Malik pressed home the advantage, and the wounded Ziri fled, hotly pursued by the Caliph's army. The inhabitants of Fez would not let him enter the city, but opened the gates to Abd al-Malik on 13 October 998. Ziri fled to the Sahara, where he rallied the Zenata tribes and overthrew the unpopular remnants of the Idrisid dynasty at Tiaret. He was able to expand his territory to include Tlemcen and other parts of western Algeria, this time under Fatimid protection. Ziri died in 1001 of the after-effects of the stab wounds. He was succeeded by his son Al-Mu'izz, who made peace with Al-Mansur, and regained possession of all his father's former territories.\nA revolt against the Andalusian Umayyads was put down by Ibn Abi 'Amir, although the Maghrawa were able to regain power in Fez. Under the succeeding rulers al-Muizz (1001\u20131026), Hamman (1026\u20131039) and Dunas (1039), they consolidated their rule in northern and central Morocco.\nInternal power struggles after 1060 enabled the Almoravid dynasty to conquer the Maghrawa realm in 1070 and put an end to their rule. In the mid 11th century the Maghrawa still controlled most of Morocco, notably most of the Sous and Draa River area as well as Aghmat, Fez and Sijilmasa. Later, Zenata power declined. The Maghrawa and Banu Ifran began oppressing their subjects, shedding their blood, violating their women, breaking into homes to seize food and depriving traders of their goods. Anyone who tried to ward them off was killed.\nZirid Dynasty.\nThe Zirid dynasty (), Banu Ziri (), or the Zirid state () was a Sanhaja Berber dynasty from modern-day Algeria which ruled the central Maghreb from 972 to 1014 and Ifriqiya (eastern Maghreb) from 972 to 1148.\nDescendants of Ziri ibn Manad, a military leader of the Fatimid Caliphate and the eponymous founder of the dynasty, the Zirids were emirs who ruled in the name of the Fatimids. The Zirids gradually established their autonomy in Ifriqiya through military conquest until officially breaking with the Fatimids in the mid-11th century. The rule of the Zirid emirs opened the way to a period in North African history where political power was held by Berber dynasties such as the Almoravid dynasty, Almohad Caliphate, Zayyanid dynasty, Marinid Sultanate and Hafsid dynasty.\nUnder Buluggin ibn Ziri the Zirids extended their control westwards and briefly occupied Fez and much of present-day Morocco after 980, but encountered resistance from the local Zenata Berbers who gave their allegiance to the Caliphate of Cordoba. To the east, Zirid control was extended over Tripolitania after 978 and as far as Ajdabiya (in present-day Libya). One member of the dynastic family, Zawi ibn Ziri, revolted and fled to al-Andalus, eventually founding the Taifa of Granada in 1013, after the collapse of the Caliphate of Cordoba. Another branch of the Zirids, the Hammadids, broke away from the main branch after various internal disputes and took control of the territories of the central Maghreb after 1015. The Zirids proper were then designated as Badicides and occupied only Ifriqiya between 1048 and 1148. They were based in Kairouan until 1057, when they moved the capital to Mahdia on the coast. The Zirids of Ifriqiya also intervened in Sicily during the 11th century, as the Kalbids, the dynasty who governed the island on behalf of the Fatimids, fell into disorder.\nThe Zirids of Granada surrendered to the Almoravids in 1090, but the Badicides and the Hammadids remained independent during this time. Sometime between 1041 and 1051 the Zirid ruler al-Mu'izz ibn Badis renounced the Fatimid Caliphs and recognized the Sunni Muslim Abbasid Caliphate. In retaliation, the Fatimids instigated the migration of the Banu Hilal tribe to the Maghreb, dealing a serious blow to Zirid power in Ifriqiya. In the 12th century, the Hilalian invasions combined with the attacks of the Normans of Sicily along the coast further weakened Zirid power. The last Zirid ruler, al-Hasan, surrendered Mahdia to the Normans in 1148, thus ending independent Zirid rule. The Almohad Caliphate conquered the central Maghreb and Ifriqiya by 1160, ending the Hammadid dynasty in turn and finally unifying the whole of the Maghreb.\nOrigins and establishment.\nThe Zirids were Sanhaja Berbers, from the sedentary Talkata tribe, originating from the area of modern Algeria. In the 10th century this tribe served as vassals of the Fatimid Caliphate, an Isma'ili Shi'a state that challenged the authority of the Sunni Abbasid caliphs. The progenitor of the Zirid dynasty, Ziri ibn Manad (r. 935\u2013971) was installed as governor of the central Maghreb (roughly north-eastern Algeria today) on behalf of the Fatimids, guarding the western frontier of the Fatimid Caliphate. With Fatimid support Ziri founded his own capital and palace at 'Ashir, south-east of Algiers, in 936. He proved his worth as a key ally in 945, during the Kharijite rebellion of Abu Yazid, when he helped break Abu Yazid's siege of the Fatimid capital, Mahdia. After playing this valuable role, he expanded 'Ashir with a new palace circa 947. In 959 he aided Jawhar al-Siqili on a Fatimid military expedition which successfully conquered Fez and Sijilmasa in present-day Morocco. On their return home to the Fatimid capital they paraded the emir of Fez and the \u201cCaliph\u201d Ibn Wasul of Sijilmasa in cages in a humiliating manner. After this success, Ziri was also given Tahart to govern on behalf of the Fatimids. He was eventually killed in battle against the Zanata in 971.\nWhen the Fatimids moved their capital to Egypt in 972, Ziri's son Buluggin ibn Ziri (r. 971\u2013984) was appointed viceroy of Ifriqiya. He soon led a new expedition west and by 980 he had conquered Fez and most of Morocco, which had previously been retaken by the Umayyads of Cordoba in 973. He also led a successful expedition to Barghawata territory, from which he brought back a large number of slaves to Ifriqiya. In 978 the Fatimids also granted Buluggin overlordship of Tripolitania (in present-day Libya), allowing him to appoint his own governor in Tripoli. In 984 Buluggin died in Sijilmasa from an illness and his successor decided to abandon Morocco in 985.\nBuluggin's successors and the first divisions.\nAfter Buluggin's death, rule of the Zirid state passed to his son, Al-Mansur ibn Buluggin (r. 984\u2013996), and continued through his descendants. However, this alienated the other sons of Ziri ibn Manad who now found themselves excluded from power. In 999 many of these brothers launched a rebellion in 'Ashir against Badis ibn al-Mansur (r. 996\u20131016), Buluggin's grandson, marking the first serious break in the unity of the Zirids. The rebels were defeated in battle by Hammad ibn Buluggin, Badis' uncle, and most of the brothers were killed. The only remaining brother of stature, Zawi ibn Ziri, led the remaining rebels westwards and sought new opportunity in al-Andalus under the Umayyads Caliphs of Cordoba, the former enemies of the Fatimids and Zirids. He and his followers eventually founded an independent kingdom in al-Andalus, the \"Taifa\" of Granada, in 1013.\nAfter 1001 Tripolitania broke away under the leadership of Fulful ibn Sa'id ibn Khazrun, a Maghrawa leader who founded the Banu Khazrun dynasty, which endured until 1147. Fulful fought a protracted war against Badis ibn al-Mansur and sought outside help from the Fatimids and even from the Umayyads of Cordoba, but after his death in 1009 the Zirids were able to retake Tripoli for a time. The region nonetheless remained effectively under control of the Banu Khazrun, who fluctuated between practical autonomy and full independence, often playing the Fatimids and the Zirids against each other. The Zirids finally lost Tripoli to them in 1022.\nBadis appointed Hammad ibn Buluggin as governor of 'Ashir and the western Zirid territories in 997. He gave Hammad a great deal of autonomy, allowing him to campaign against the Zanata and control any new territories he conquered. Hammad constructed his own capital, the Qal'at Bani Hammad, in 1008, and in 1015 he rebelled against Badis and declared himself independent altogether, while also recognizing the Abbasids instead of the Fatimids as caliphs. Badis besieged Hammad's capital and nearly subdued him, but died in 1016 shortly before this could be accomplished. His son and successor, al-Mu'izz ibn Badis (r. 1016\u20131062), defeated Hammad in 1017, which forced the negotiation of a peace agreement between them. Hammad resumed his recognition of the Fatimids as caliphs but remained independent, forging a new Hammadid state which controlled a large part of present-day Algeria thereafter.\nApogee in Ifriqiya.\nThe Zirid period of Ifriqiya is considered a high point in its history, with agriculture, industry, trade and learning, both religious and secular, all flourishing, especially in their capital, Qayrawan (Kairouan). The early reign of al-Mu'izz ibn Badis (r. 1016\u20131062) was particularly prosperous and marked the height of their power in Ifriqiya. In the eleventh century, when the question of Berber origin became a concern, the dynasty of al-Mu'izz started, as part of the Zirids' propaganda, to emphasize its supposed links to the Himyarite kings as a title to nobility, a theme that was taken the by court historians of the period. Management of the area by later Zirid rulers was neglectful as the agricultural economy declined, prompting an increase in banditry among the rural population. The relationship between the Zirids their Fatimid overlords varied - in 1016 thousands of Shiites died in rebellions in Ifriqiya, and the Fatimids encouraged the defection of Tripolitania from the Zirids, but nevertheless the relationship remained close. In 1049 the Zirids broke away completely by adopting Sunni Islam and recognizing the Abbasids of Baghdad as rightful Caliphs, a move which was popular with the urban Arabs of Kairouan.\nIn Sicily the Kalbids continued to govern on behalf of the Fatimids but the island descended into political disarray during the 11th century, inciting the Zirids to intervene on the island. In 1025 (or 1021), al-Mu'izz ibn Badis sent a fleet of 400 ships to the island in response to the Byzantines reconquering Calabria (in southern Italy) from the Muslims, but the fleet was lost in a powerful storm off the coast of Pantelleria. In 1036, the Muslim population of the island request aid from al-Mu'izz to overthrow the Kalbid emir Ahmad ibn Yusuf al-Akhal, whose rule they considered flawed and unjust. The request also contained a pledge to recognize al-Mu'izz as their ruler. Al-Mu'izz, eager to expand his influence after the fragmentation of Zirid North Africa, accepted and sent his son, 'Abdallah, to the island with a large army. Al-Akhal, who had been in negotiations with the Byzantines, requested help from them. A Byzantine army intervened and defeated the Zirid army on the island, but it then withdrew to Calabria, allowing 'Abdallah to finish off al-Akhal. Al-Akhal was besieged in Palermo and killed in 1038. 'Abdallah was subsequently forced to withdraw from the island, either due to the ever-divided Sicilians turning against him or due to another Byzantine invasion in 1038, led by George Maniakes. Another Kalbid amir, al-Hasan al-Samsam, was elected to govern Sicily, but Muslim rule there disintegrated into various petty factions leading up to the Norman conquest of the island in the second half of the 11th century.\nHilalian invasions and withdrawal to Mahdia.\nThe Zirids renounced the Fatimids and recognized the Abbasid Caliphs in 1048-49, or sometime between 1041 and 1051. In retaliation, the Fatimids sent the Arab tribes of the Banu Hilal and the Banu Sulaym to the Maghreb. The Banu Sulaym settled first in Cyrenaica, but the Banu Hilal continued towards Ifriqiya. The Zirids attempted to stop their advance towards Ifriqiya, they sent 30,000 Sanhaja cavalry to meet the 3,000 Arab cavalry of Banu Hilal in the Battle of Haydaran of 14 April 1052. Nevertheless, the Zirids were decisively defeated and were forced to retreat, opening the road to Kairouan for the Hilalian Arab cavalry. The resulting anarchy devastated the previously flourishing agriculture, and the coastal towns assumed a new importance as conduits for maritime trade and bases for piracy against Christian shipping, as well as being the last holdout of the Zirids. The Banu Hilal invasions eventually forced al-Mu'izz ibn Badis to abandon Kairouan in 1057 and move his capital to Mahdia, while the Banu Hilal largely roamed and pillaged the interior of the former Zirid territories.\nAs a result of the Zirid withdrawal, various local principalities emerged in different areas. In Tunis, the shaykhs of the city elected Abd al-Haqq ibn Abd al-Aziz ibn Khurasan (r. 1059-1095) as local ruler. He founded the local Banu Khurasan dynasty that governed the city thereafter, alternately recognizing the Hammadids or the Zirids as overlords depending on the circumstances. In Qabis (Gab\u00e8s), the Zirid governor, al-Mu'izz ibn Muhammad ibn Walmiya remained loyal until 1062 when, outraged by the expulsion of his two brothers from Mahdia by al-Mu'izz ibn Badis, he declared his independence and placed himself under the protection of Mu'nis ibn Yahya, a chief of Banu Hilal. Sfaqus (Sfax) was declared independent by the Zirid governor, Mansur al-Barghawati, who was murdered and succeeded by his cousin Hammu ibn Malil al-Barghawati.\nAl-Mui'zz ibn Badis was succeeded by his son, Tamim ibn al-Mu'izz (r. 1062-1108), who spent much of his reign attempting to restore Zirid power in the region. In 1063 he repelled a siege of Mahdia by the independent ruler of Sfax while also capturing the important port of Sus (Sousse). Meanwhile, the Hammadid ruler al-Nasir ibn 'Alannas (r. 1062-1088) began to intervene in Ifriqiya around this time, having his sovereignty recognized in Sfax, Tunis, and Kairouan. Tamim organized a coalition with some of the Banu Hilal and Banu Sulaym tribes and succeeded in inflicting a heavy defeat on al-Nasir at the Battle of Sabiba in 1065. The war between the Zirids and Hammadids continued until 1077, when a truce was negotiated, sealed by a marriage between Tamim and one of al-Nasir's daughters. In 1074 Tamim sent a naval expedition to Calabria where they ravaged the Italian coasts, plundered Nicotera and enslaved many of its inhabitants. The next year (1075) another Zirid raid resulted in the capture of Mazara in Sicily; however, the Zirid emir rethought his involvement in Sicily and decided to withdraw, abandoning what they had briefly held. In 1087, the Zirid capital, Mahdia, was sacked by the Pisans. According to Ettinghausen, Grabar, and Jenkins-Madina, the Pisa Griffin is believed to have been part of the spoils taken during the sack. In 1083 Mahdia was besieged by a chief of the Banu Hilal, Malik ibn 'Alawi. Unable to take the city, Malik instead turned to Kairouan and captured that city, but Tamim marched out with his entire army and defeated the Banu Hilal forces, at which point he also brought Kairouan back under Zirid control. He went on to capture Gab\u00e8s in 1097 and Sfax in 1100. Gab\u00e8s, however, soon declared itself independent again under the leadership of the Banu Jami', a family from the Riyahi branch of the Banu Hilal.\nTamim's son and successor, Yahya ibn Tamim (r. 1108-1116), formally recognized the Fatimid caliphs again and received an emissary from Cairo in 1111. He captured an important fortress near Carthage called Iqlibiya and his fleet launched raids against Sardinia and Genoa, bringing back many captives. He was assassinated in 1116 and succeeded by his son, 'Ali ibn Yahya (r. 1116-1121). 'Ali continued to recognize the Fatimids, receiving another embassy from Cairo in 1118. He imposed his authority on Tunis, but failed to recapture Gab\u00e8s from its local ruler, Rafi' ibn Jami', whose counterattack he then had to repel from Mahdia. He was succeeded by his son al-Hasan in 1121, the last Zirid ruler.\nEnd of Zirid rule.\nDuring the 1130s and 1140s the Normans of Sicily began to capture cities and islands along the coast of Ifriqiya. Jerba was captured in 1135 and Tripoli was captured in 1146. In 1148, the Normans captured Sfax, Gab\u00e8s, and Mahdia. In Mahdia, the population was weakened by years of famine and the bulk of the Zirid army was away on another campaign when the Norman fleet, commanded by George of Antioch, arrived off the coast. Al-Hasan decided to abandon the city, leaving it to be occupied, which effectively ended the Zirid dynasty's rule. Al-Hasan fled to the citadel of al-Mu'allaqa near Carthage and stayed there for a several months. He planned to flee to the Fatimid court in Egypt but the Norman fleet blocked his way, so instead he headed west, making for the Almohad court of 'Abd al-Mu'min in Marrakesh. He obtained permission from Yahya ibn al-'Aziz, the Hammadid ruler, to cross his territory, but after entering Hammadid territory he was detained and placed under house arrest in Algiers. When 'Abd al-Mu'min captured Algiers in 1151, he freed al-Hasan, who accompanied him back to Marrakesh. Later, when 'Abd al-Mu'min conquered Mahdia in 1160, placing all of Ifriqiya under Almohad rule, al-Hasan was with him. 'Abd al-Mu'min appointed him governor of Mahdia, where he remained, residing in the suburb of Zawila, until 'Abd al-Mu'min's death in 1163. The new Almohad caliph, Abu Ya'qub Yusuf, subsequently ordered him to come back to Marrakesh, but al-Hasan died along the way in Tamasna in 1167.\nHammadid Dynasty.\nThe Hammadid dynasty (Arabic: \u0627\u0644\u062d\u0645\u0651\u0627\u062f\u064a\u0648\u0646) was a branch of the Sanhaja Berber dynasty that ruled an area roughly corresponding to north-eastern modern Algeria between 1008 and 1152. The state reached its peak under Nasir ibn Alnas during which it was briefly the most important state in Northwest Africa.\nThe Hammadid dynasty's first capital was at Qalaat Beni Hammad. It was founded in 1007, and is now a UNESCO World Heritage Site. When the area was sacked by the Banu Hilal tribe, the Hammadids moved their capital to B\u00e9ja\u00efa in 1090.\nAlmohad Caliphate.\nThe Almohad Caliphate (; or or from\n) was a North African Berber Muslim empire founded in the 12th century. At its height, it controlled much of the Iberian Peninsula (Al Andalus) and North Africa (the Maghreb).\nThe Almohad doctrine was founded by Ibn Tumart among the Berber Masmuda tribes, but the Almohad caliphate and its ruling dynasty were founded after his death by Abd al-Mu'min al-Gumi, which was born in the Hammadid region of Tlemcen, Algeria. Around 1120, Ibn Tumart first established a Berber state in Tinmel in the Atlas Mountains. Under Abd al-Mu'min (r. 1130\u20131163) they succeeded in overthrowing the ruling Almoravid dynasty governing Morocco in 1147, when he conquered Marrakesh and declared himself caliph. They then extended their power over all of the Maghreb by 1159. Al-Andalus soon followed, and all of Muslim Iberia was under Almohad rule by 1172.\nThe turning point of their presence in the Iberian Peninsula came in 1212, when Muhammad III, \"al-Nasir\" (1199\u20131214) was defeated at the Battle of Las Navas de Tolosa in the Sierra Morena by an alliance of the Christian forces from Castile, Aragon and Navarre. Much of the remaining territories of al-Andalus were lost in the ensuing decades, with the cities of C\u00f3rdoba and Seville falling to the Christians in 1236 and 1248 respectively.\nThe Almohads continued to rule in Africa until the piecemeal loss of territory through the revolt of tribes and districts enabled the rise of their most effective enemies, the Marinids, from northern Morocco in 1215. The last representative of the line, Idris al-Wathiq, was reduced to the possession of Marrakesh, where he was murdered by a slave in 1269; the Marinids seized Marrakesh, ending the Almohad domination of the Western Maghreb.\nOrigins.\nThe Almohad movement originated with Ibn Tumart, a member of the Masmuda, a Berber tribal confederation of the Atlas Mountains of southern Morocco. At the time, Morocco, western Algeria and Spain (al-Andalus), were under the rule of the Almoravids, a Sanhaja Berber dynasty. Early in his life, Ibn Tumart went to Spain to pursue his studies, and thereafter to Baghdad to deepen them. In Baghdad, Ibn Tumart attached himself to the theological school of al-Ash'ari, and came under the influence of the teacher al-Ghazali. He soon developed his own system, combining the doctrines of various masters. Ibn Tumart's main principle was a strict unitarianism (\"tawhid\"), which denied the independent existence of the attributes of God as being incompatible with His unity, and therefore a polytheistic idea. Ibn Tumart represented a revolt against what he perceived as anthropomorphism in Muslim orthodoxy. His followers would become known as the \"al-Muwa\u1e25\u1e25id\u016bn\" (\"Almohads\"), meaning those who affirm the unity of God.\nAfter his return to the Maghreb c.\u00a01117, Ibn Tumart spent some time in various Ifriqiyan cities, preaching and agitating, heading riotous attacks on wine-shops and on other manifestations of laxity. He laid the blame for the latitude on the ruling dynasty of the Almoravids, whom he accused of obscurantism and impiety. He also opposed their sponsorship of the Maliki school of jurisprudence, which drew upon consensus (\"ijma\") and other sources beyond the Qur'an and Sunnah in their reasoning, an anathema to the stricter Zahirism favored by Ibn Tumart. His antics and fiery preaching led fed-up authorities to move him along from town to town. After being expelled from Bejaia, Ibn Tumart set up camp in Mellala, in the outskirts of the city, where he received his first disciples \u2013 notably, al-Bashir (who would become his chief strategist) and Abd al-Mu'min (a Zenata Berber, who would later become his successor).\nIn 1120, Ibn Tumart and his small band of followers proceeded to Morocco, stopping first in Fez, where he briefly engaged the Maliki scholars of the city in debate. He even went so far as to assault the sister of the Almoravid emir \u02bfAli ibn Yusuf, in the streets of Fez, because she was going about unveiled, after the manner of Berber women. After being expelled from Fez, he went to Marrakesh, where he successfully tracked down the Almoravid emir Ali ibn Yusuf at a local mosque, and challenged the emir, and the leading scholars of the area, to a doctrinal debate. After the debate, the scholars concluded that Ibn Tumart's views were blasphemous and the man dangerous, and urged him to be put to death or imprisoned. But the emir decided merely to expel him from the city.\nIbn Tumart took refuge among his own people, the Hargha, in his home village of Igiliz (exact location uncertain), in the Sous valley. He retreated to a nearby cave, and lived out an ascetic lifestyle, coming out only to preach his program of puritan reform, attracting greater and greater crowds. At length, towards the end of Ramadan in late 1121, after a particularly moving sermon, reviewing his failure to persuade the Almoravids to reform by argument, Ibn Tumart 'revealed' himself as the true Mahdi, a divinely guided judge and lawgiver, and was recognized as such by his audience. This was effectively a declaration of war on the Almoravid state.\nOn the advice of one of his followers, Omar Hintati, a prominent chieftain of the Hintata, Ibn Tumart abandoned his cave in 1122 and went up into the High Atlas, to organize the Almohad movement among the highland Masmuda tribes. Besides his own tribe, the Hargha, Ibn Tumart secured the adherence of the Ganfisa, the Gadmiwa, the Hintata, the Haskura, and the Hazraja to the Almohad cause. Around 1124, Ibn Tumart erected the ribat of Tinmel, in the valley of the Nfis in the High Atlas, an impregnable fortified complex, which would serve both as the spiritual center and military headquarters of the Almohad movement.\nFor the first eight years, the Almohad rebellion was limited to a guerilla war along the peaks and ravines of the High Atlas. Their principal damage was in rendering insecure (or altogether impassable) the roads and mountain passes south of Marrakesh \u2013 threatening the route to all-important Sijilmassa, the gateway of the trans-Saharan trade. Unable to send enough manpower through the narrow passes to dislodge the Almohad rebels from their easily defended mountain strong points, the Almoravid authorities reconciled themselves to setting up strongholds to confine them there (most famously the fortress of Tasgh\u00eem\u00fbt that protected the approach to Aghmat, which was conquered by the Almohads in 1132), while exploring alternative routes through more easterly passes.\nIbn Tumart organized the Almohads as a commune, with a minutely detailed structure. At the core was the \"Ahl ad-d\u0101r\" (\"House of the Mahdi:), composed of Ibn Tumart's family. This was supplemented by two councils: an inner Council of Ten, the Mahdi's privy council, composed of his earliest and closest companions; and the consultative Council of Fifty, composed of the leading \"sheikh\"s of the Masmuda tribes. The early preachers and missionaries (\"\u1e6dalaba\" and \"huff\u0101\u1e93\") also had their representatives. Militarily, there was a strict hierarchy of units. The Hargha tribe coming first (although not strictly ethnic; it included many \"honorary\" or \"adopted\" tribesmen from other ethnicities, e.g. Abd al-Mu'min himself). This was followed by the men of Tinmel, then the other Masmuda tribes in order, and rounded off by the black fighters, the \"\u02bbab\u012bd\". Each unit had a strict internal hierarchy, headed by a \"mohtasib\", and divided into two factions: one for the early adherents, another for the late adherents, each headed by a \"mizwar\" (or \"amzwaru\"); then came the \"sakkakin\" (treasurers), effectively the money-minters, tax-collectors, and bursars, then came the regular army (\"jund\"), then the religious corps \u2013 the muezzins, the \"hafidh\" and the \"hizb\" \u2013 followed by the archers, the conscripts, and the slaves. Ibn Tumart's closest companion and chief strategist, al-Bashir, took upon himself the role of \"political commissar\", enforcing doctrinal discipline among the Masmuda tribesmen, often with a heavy hand.\nIn early 1130, the Almohads finally descended from the mountains for their first sizeable attack in the lowlands. It was a disaster. The Almohads swept aside an Almoravid column that had come out to meet them before Aghmat, and then chased their remnant all the way to Marrakesh. They laid siege to Marrakesh for forty days until, in April (or May) 1130, the Almoravids sallied from the city and crushed the Almohads in the bloody Battle of al-Buhayra (named after a large garden east of the city). The Almohads were thoroughly routed, with huge losses. Half their leadership was killed in action, and the survivors only just managed to scramble back to the mountains.\nIbn Tumart died shortly after, in August 1130. That the Almohad movement did not immediately collapse after such a devastating defeat and the death of their charismatic Mahdi, is likely due to the skills of his successor, Abd al-Mu'min. Ibn Tumart's death was kept a secret for three years, a period which Almohad chroniclers described as a \"ghayba\" or \"occultation\". This period likely gave Abd al-Mu'min time to secure his position as successor to the political leadership of the movement. Although a Zenata Berber from Tagra (Algeria), and thus an alien among the Masmuda of southern Morocco, Abd al-Mu'min nonetheless saw off his principal rivals and hammered wavering tribes back to the fold. In an ostentatious gesture of defiance, in 1132, if only to remind the emir that the Almohads were not finished, Abd al-Mu'min led an audacious night operation that seized Tasgh\u00eem\u00fbt fortress and dismantled it thoroughly, carting off its great gates back to Tinmel. Three years after Ibn Tumart's death he was officially proclaimed \"Caliph\".\nIn order to neutralise the Masmudas, to whom he was a stranger, Abd al-Mumin relied on his tribe of origin, the Kumiyas (a Berber tribe from Orania), which he integrated massively into the army and within the Almohad power. He thus appointed his son as his successor and his other children as governors of the provinces of the Caliphate. The Kumiyas would later form the bodyguard of Abd al Mumin and his successor. In addition, he also relied on Arabs, representatives of the great Hilalian families, whom he deported to Morocco to weaken the influence of the Masmuda sheikhs. These moves have the effect of advancing the Arabisation of the future Morocco.\nAl-Andalus.\nAbd al-Mu'min then came forward as the lieutenant of the Mahdi Ibn Tumart. Between 1130 and his death in 1163, Abd al-Mu'min not only rooted out the Almoravids, but extended his power over all northern Africa as far as Egypt, becoming amir of Marrakesh in 1147.\nAl-Andalus followed the fate of Africa. Between 1146 and 1173, the Almohads gradually wrested control from the Almoravids over the Moorish principalities in Iberia. The Almohads transferred the capital of Muslim Iberia from C\u00f3rdoba to Seville. They founded a great mosque there; its tower, the Giralda, was erected in 1184 to mark the accession of Ya'qub I. The Almohads also built a palace there called Al-Muwarak on the site of the modern day Alc\u00e1zar of Seville.\nThe Almohad princes had a longer and more distinguished career than the Almoravids. The successors of Abd al-Mumin, Abu Yaqub Yusuf (Yusuf I, ruled 1163\u20131184) and Abu Yusuf Yaqub al-Mansur (Ya\u02bbq\u016bb I, ruled 1184\u20131199), were both able men. Initially their government drove many Jewish and Christian subjects to take refuge in the growing Christian states of Portugal, Castile, and Aragon. Ultimately they became less fanatical than the Almoravids, and Ya'qub al-Mansur was a highly accomplished man who wrote a good Arabic style and protected the philosopher Averroes. In 1190\u20131191, he campaigned in southern Portugal and won back territory lost in 1189. His title of \"al-Man\u1e63\u016br\" (\"the Victorious\") was earned by his victory over Alfonso VIII of Castile in the Battle of Alarcos (1195).\nFrom the time of Yusuf II, however, the Almohads governed their co-religionists in Iberia and central North Africa through lieutenants, their dominions outside Morocco being treated as provinces. When Almohad emirs crossed the Straits it was to lead a jihad against the Christians and then return to Morocco.\nHolding years.\nIn 1212, the Almohad Caliph Muhammad 'al-Nasir' (1199\u20131214), the successor of al-Mansur, after an initially successful advance north, was defeated by an alliance of the four Christian kings of Castile, Arag\u00f3n, Navarre, and Portugal, at the Battle of Las Navas de Tolosa in the Sierra Morena. The battle broke the Almohad advance, but the Christian powers remained too disorganized to profit from it immediately.\nBefore his death in 1213, al-Nasir appointed his young ten-year-old son as the next caliph Yusuf II \"al-Mustansir\". The Almohads passed through a period of effective regency for the young caliph, with power exercised by an oligarchy of elder family members, palace bureaucrats and leading nobles. The Almohad ministers were careful to negotiate a series of truces with the Christian kingdoms, which remained more-or-less in place for next fifteen years (the loss of Alc\u00e1cer do Sal to the Kingdom of Portugal in 1217 was an exception).\nIn early 1224, the youthful caliph died in an accident, without any heirs. The palace bureaucrats in Marrakesh, led by the \"wazir\" Uthman ibn Jam'i, quickly engineered the election of his elderly grand-uncle, Abd al-Wahid I 'al-Makhlu', as the new Almohad caliph. But the rapid appointment upset other branches of the family, notably the brothers of the late al-Nasir, who governed in al-Andalus. The challenge was immediately raised by one of them, then governor in Murcia, who declared himself Caliph Abdallah al-Adil. With the help of his brothers, he quickly seized control of al-Andalus. His chief advisor, the shadowy Abu Zayd ibn Yujjan, tapped into his contacts in Marrakesh, and secured the deposition and assassination of Abd al-Wahid I, and the expulsion of the al-Jami'i clan.\nThis coup has been characterized as the pebble that finally broke al-Andalus. It was the first internal coup among the Almohads. The Almohad clan, despite occasional disagreements, had always remained tightly knit and loyally behind dynastic precedence. Caliph al-Adil's murderous breach of dynastic and constitutional propriety marred his acceptability to other Almohad \"sheikhs\". One of the recusants was his cousin, Abd Allah al-Bayyasi (\"the Baezan\"), the Almohad governor of Ja\u00e9n, who took a handful of followers and decamped for the hills around Baeza. He set up a rebel camp and forged an alliance with the hitherto quiet Ferdinand III of Castile. Sensing his greater priority was Marrakesh, where recusant Almohad \"sheikh\"s had rallied behind Yahya, another son of al-Nasir, al-Adil paid little attention to this little band of misfits.\nZayyanid Dynasty.\nThe Kingdom of Tlemcen or Zayyanid Kingdom of Tlemcen () was a Berber kingdom in what is now the northwest of Algeria. Its territory stretched from Tlemcen to the Chelif bend and Algiers, and at its zenith reached Sijilmasa and the Moulouya River in the west, Tuat to the south and the Soummam in the east.\nThe Tlemcen Kingdom was established after the demise of the Almohad Caliphate in 1236, and later fell under Ottoman rule in 1554. It was ruled by sultans of the Zayyanid dynasty. The capital of the Tlemcen kingdom centred on Tlemcen, which lay on the primary east\u2013west route between Morocco and Ifriqiya. The kingdom was situated between the realm of the Marinids the west, centred on Fez, and the Hafsids to the east, centred on Tunis.\nTlemcen was a hub for the north\u2013south trade route from Oran on the Mediterranean coast to the Western Sudan. As a prosperous trading centre, it attracted its more powerful neighbours. At different times the kingdom was invaded and occupied by the Marinids from the west, by the Hafsids from the east, and by Aragonese from the north. At other times, they were able to take advantage of turmoil among their neighbours: during the reign of Abu Tashfin I (r. 1318\u20131337) the Zayyanids occupied Tunis and in 1423, under the reign of Abu Malek, they briefly captured Fez.287 In the south the Zayyanid realm included Tuat, Tamentit and the Draa region which was governed by Abdallah Ibn Moslem ez Zerdali, a sheikh of the Zayyanids.\nRise to power (13th century).\nThe \"B\u0101nu \u02bfabd \u0101l-W\u0101d\", also called the \"B\u0101nu Ziy\u0101n\" or Zayyanids after Yaghmurasen Ibn Zyan, the founder of the dynasty, were leaders of a Berber group who had long been settled in the Central Maghreb. Although contemporary chroniclers asserted that they had a noble Arab origin, he reportedly spoke in Zenati dialect and denied the lineage that genealogists had attributed to him. The town of Tlemcen, called Pomaria by the Romans, is about 806m above sea level in fertile, well-watered country.\nTlemcen was an important centre under the Almoravid dynasty and its successors the Almohad Caliphate, who began a new wall around the town in 1161.\nYaghmurasen ibn Zayyan (1235\u201383) of the \"B\u0101nu \u02bfabd \u0101l-W\u0101d\" was governor of Tlemcen under the Almohads. He inherited leadership of the family from his brother in 1235. When the Almohad empire began to fall apart, in 1235, Yaghmurasen declared his independence. The city of Tlemcen became the capital of one of three successor states, ruled for centuries by successive Ziyyanid sultans. Its flag was a white crescent pointing upwards on a blue field. The kingdom covered the less fertile regions of the Tell Atlas. Its people included a minority of settled farmers and villagers, and a majority of nomadic herders.\nYaghmurasen was able to maintain control over the rival Berber groups, and when faced with the outside threat of the Marinid dynasty, he formed an alliance with the Emir of Granada and the King of Castile, Alfonso X. According to Ibn Khaldun, \"he was the bravest, most dreaded and honourable man of the 'Abd-la-Wadid family. No one looked after the interest of his people, maintained the influence of the kingdom and managed the state administration better than he did.\" In 1248 he defeated the Almohad Caliph in the Battle of Oujda during which the Almohad Caliph was killed. In 1264 he managed to conquer Sijilmasa, therefore bringing Sijilmasa and Tlemcen, the two most important outlets for trans-Saharan trade under one authority. Sijilmasa remained under his control for 11 years. Before his death he instructed his son and heir Uthman to remain on the defensive with the Marinid kingdom, but to expand into Hafsid territory if possible.\n14th century.\nFor most of its history the kingdom was on the defensive, threatened by stronger states to the east and the west. The nomadic Arabs to the south also took advantage of the frequent periods of weakness to raid the centre and take control of pastures in the south.\nThe city of Tlemcen was several times attacked or besieged by the Marinids, and large parts of the kingdom were occupied by them for several decades in the fourteenth century.\nThe Marinid Abu Yaqub Yusuf an-Nasr besieged Tlemcen from 1299 to 1307. During the siege he built a new town, al-Mansura, diverting most of the trade to this town. The new city was fortified and had a mosque, baths and palaces. The siege was raised when Abu Yakub was murdered in his sleep by one of his eunuchs.\nWhen the Marinids left in 1307, the Zayyanids promptly destroyed al-Mansura. The Zayyanid king Abu Zayyan I died in 1308 and was succeeded by Abu Hammu I (r. 1308\u20131318). Abu Hammu was later killed in a conspiracy instigated by his son and heir Abu Tashufin I (r. 1318\u20131337). The reigns of Abu Hammu I and Abu Tashufin I marked the second apogee of the Zayyanids, a period during which they consolidated their hegemony in the central Maghreb. Tlemcen recovered its trade and its population grew, reaching about 100,000 by around the 1330s. Abu Tashufin initiated hostilities against Ifriqiya while the Marinids were distracted by their internal struggles. He besieged B\u00e9ja\u00efa and sent an army into Tunisia that defeated the Hafsid king Abu Yahya Abu Bakr II, who fled to Constantine while the Zayyanids occupied Tunis in 1325.\nThe Marinid sultan Abu al-Hasan (r. 1331\u20131348) cemented an alliance with Hafsids by marrying a Hafsid princess. Upon being attacked by the Zayyanids again, the Hafsids appealed to Abu al-Hasan for help, providing him with an excuse to invade his neighbour. The Marinid sultan initiated a siege of Tlemcen in 1335 and the city fell in 1337. Abu Tashufin died during the fighting. Abu al-Hasan received delegates from Egypt, Granada, Tunis and Mali congratulating him on his victory, by which he had gained complete control of the trans-Saharan trade. In 1346 the Hafsid Sultan, Abu Bakr, died and a dispute over the succession ensued. In 1347 Abu al-Hasan annexed Ifriqiya, briefly reuniting the Maghrib territories as they had been under the Almohads.\nHowever, Abu al-Hasan went too far in attempting to impose more authority over the Arab tribes, who revolted and in April 1348 defeated his army near Kairouan. His son, Abu Inan Faris, who had been serving as governor of Tlemcen, returned to Fez and declared that he was sultan. Tlemcen and the central Maghreb revolted. The Zayyanid Abu Thabit I (1348-1352) was proclaimed king of Tlemcen. Abu al-Hasan had to return from Ifriqiya by sea. After failing to retake Tlemcen and being defeated by his son, Abu al-Hasan died in May 1351. In 1352 Abu Inan Faris recaptured Tlemcen. He also reconquered the central Maghreb. He took B\u00e9ja\u00efa in 1353 and Tunis in 1357, becoming master of Ifriqiya. In 1358 he was forced to return to Fez due to Arab opposition, where he fell sick and was killed.\nThe Zayyanid king Abu Hammu Musa II (r. 1359\u20131389) next took the throne of Tlemcen. He pursued an expansionist policy, pushing towards Fez in the west and into the Chelif valley and B\u00e9ja\u00efa in the east. He had a long reign punctuated by fighting against the Marinids or various rebel groups. The Marinids reoccupied Tlemcen in 1360 and in 1370. In both cases, the Marinids found they were unable to hold the region against local resistance. Abu Hammu attacked the Hafsids in B\u00e9ja\u00efa again in 1366, but this resulted in Hafsid intervention in the kingdom's affairs. The Hafsid sultan released Abu Hammu's cousin, Abu Zayyan, and helped him in laying claim to the Zayyanid throne. This provoked an internecine war between the two Zayyanids until 1378, when Abu Hammu finally captured Abu Zayyan in Algiers.\nThe historian Ibn Khaldun lived in Tlemcen for a period during the generally prosperous reign of Abu Hammu Musa II, and helped him in negotiations with the nomadic Arabs. He said of this period, \"Here [in Tlemcen] science and arts developed with success; here were born scholars and outstanding men, whose glory penetrated into other countries.\" Abu Hammu was deposed by his son, Abu Tashfin II (1389\u201394), and the state went into decline.\nDecline (late 14th and 15th centuries).\nIn the late 14th century and the 15th century, the state was increasingly weak and became intermittently a vassal of Hafsid Ifriqiya, Marinid Morocco or the Crown of Aragon. In 1386 Abu Hammu moved his capital to Algiers, which he judged less vulnerable, but a year later his son, Abu Tashufin, overthrew him and took him prisoner. Abu Hammu was sent on a ship towards Alexandria but he escaped along the way when the ship stopped in Tunis. In 1388 he recaptured Tlemcen, forcing his son to flee. Abu Tashufin sought refuge in Fez and enlisted the aid of the Marinids, who sent an army to occupy Tlemcen and reinstall him on the throne. As a result, Abu Tashufin and his successors recognized the suzerainty of the Marinids and paid them an annual tribute.\nDuring the reign of the Marinid sultan Abu Sa'id, the Zayyanids rebelled on several occasions and Abu Sa'id had to reassert his authority. After Abu Sa'id's death in 1420 the Marinids were plunged into political turmoil. The Zayyanid emir, Abu Malek, used this opportunity to throw off Marinid authority and captured Fez in 1423. Abu Malek installed Muhammad, a Marinid prince, as a Zayyanid vassal in Fez.287 The Wattasids, a family related to the Marinids, continued to govern from Sal\u00e9, where they proclaimed Abd al-Haqq II, an infant, as the successor to the Marinid throne, with Abu Zakariyya al-Wattasi as regent. The Hafsid sultan, Abd al-Aziz II, reacted to Abu Malek's rising influence by sending military expeditions westward, installing his own Zayyanid client king (Abu Abdallah II) in Tlemcen and pursuing Abu Malek to Fez. Abu Malek's Marinid puppet, Muhammad, was deposed and the Wattasids returned with Abd al-Haqq II to Fez, acknowledging Hafsid suzerainty.287 The Zayyanids remained vassals of the Hafsids until the end of the 15th century, when the Spanish expansion along the coast weakened the rule of both dynasties.\nBy the end of the 15th century the Kingdom of Aragon had gained effective political control, intervening in the dynastic disputes of the amirs of Tlemcen, whose authority had shrunk to the town and its immediate neighbourship. When the Spanish took the city of Oran from the kingdom in 1509, continuous pressure from the Berbers prompted the Spanish to attempt a counterattack against the city of Tlemcen (1543), which was deemed by the Papacy to be a crusade. The Spanish under Martin of Angulo had also suffered a prior defeat in 1535 when they attempted to install a client ruler in Tlemcen. The Spanish failed to take the city in the first attack, but the strategic vulnerability of Tlemcen caused the kingdom's weight to shift toward the safer and more heavily fortified corsair base at Algiers.\nTlemcen was captured in 1551 by the Ottoman Empire under Hassan Pasha. The last Zayyanid sultan's son escaped to Oran, then a Spanish possession. He was baptized and lived a quiet life as Don Carlos at the court of Philip II of Spain.\nUnder the Ottoman Empire Tlemcen quickly lost its former importance, becoming a sleepy provincial town. The failure of the kingdom to become a powerful state can be explained by the lack of geographical or cultural unity, the constant internal disputes and the reliance on irregular Arab-Berber nomads for the military.\nChristian conquest of Spain.\nThe final triumph of the 700-year Christian conquest of Spain was marked by the fall of Granada in 1492. Christian Spain imposed its influence on the Maghrib coast by constructing fortified outposts and collecting tribute. But Spain never sought to extend its North African conquests much beyond a few modest enclaves. Privateering was an age-old practice in the Mediterranean, and North African rulers engaged in it increasingly in the late 16th and early 17th centuries because it was so lucrative. Until the 17th century the Barbary pirates used galleys, but a Dutch renegade of the name of Zymen Danseker taught them the advantage of using sailing ships.\nAlgeria became the privateering city-state par excellence, and two privateer brothers were instrumental in extending Ottoman influence in Algeria. At about the time Spain was establishing its presidios in the Maghrib, the Muslim privateer brothers Aruj and Khair ad Din\u2014the latter known to Europeans as Barbarossa, or Red Beard\u2014were operating successfully off Tunisia. In 1516 Aruj moved his base of operations to Algiers but was killed in 1518. Khair ad Din succeeded him as military commander of Algiers, and the Ottoman sultan gave him the title of beglerbey (provincial governor).\nSpanish enclaves.\nThe Spanish expansionist policy in North Africa began with the Catholic Monarchs and the regent Cisneros, once the \"Reconquista\" in the Iberian Peninsula was finished. That way, several towns and outposts in the Algerian coast were conquered and occupied: Mers El K\u00e9bir (1505), Oran (1509), Algiers (1510) and Bugia (1510). The Spanish conquest of Oran was won with much bloodshed: 4,000 Algerians were massacred, and up to 8,000 were taken prisoner. For about 200 years, Oran's inhabitants were virtually held captive in their fortress walls, ravaged by famine and plague; Spanish soldiers, too, were irregularly fed and paid.\nThe Spaniards left Algiers in 1529, Bujia in 1554, Mers El K\u00e9bir and Oran in 1708. The Spanish returned in 1732 when the armada of the Duke of Montemar was victorious in the Battle of A\u00efn-el-Turk and retook Oran and Mers El K\u00e9bir; the Spanish massacred many Muslim soldiers. In 1751, a Spanish adventurer, named John Gascon, obtained permission, and vessels and fireworks, to go against Algiers, and set fire, at night, to the Algerian fleet. The plan, however, miscarried. In 1775, Charles III of Spain sent a large force to attack Algiers, under the command of Alejandro O'Reilly (who had led Spanish forces in crushing French rebellion in Louisiana), resulting in a disastrous defeat. The Algerians suffered 5,000 casualties. The Spanish navy bombarded Algiers in 1784; over 20,000 cannonballs were fired, much of the city and its fortifications were destroyed and most of the Algerian fleet was sunk.\nOran and Mers El K\u00e9bir were held until 1792, when they were sold by the king Charles IV to the Bey of Algiers.\nRegency of Algiers.\nThe Regency of Algiers () was a state in North Africa lasting from 1516 to 1830, until it was conquered by the French. Situated between the regency of Tunis in the east, the Sultanate of Morocco (from 1553) in the west and Tuat as well as the country south of In Salah in the south (and the Spanish and Portuguese possessions of North Africa), the Regency originally extended its borders from La Calle in the east to Trara in the west and from Algiers to Biskra, and afterwards spread to the present eastern and western borders of Algeria.\nIt had various degrees of autonomy throughout its existence, in some cases reaching complete independence, recognized even by the Ottoman sultan. The country was initially governed by governors appointed by the Ottoman sultan (1518\u20131659), rulers appointed by the Odjak of Algiers (1659\u20131710), and then Deys elected by the Divan of Algiers from (1710-1830).\nEstablishment.\nFrom 1496, the Spanish conquered numerous possessions on the North African coast: Melilla (1496), Mers El K\u00e9bir (1505), Oran (1509), Bougie (1510), Tripoli (1510), Algiers, Shershell, Dellys, and Tenes. The Spaniards later led unsuccessful expeditions to take Algiers in the Algiers expedition in 1516, 1519 and another failed expedition in 1541.\nAround the same time, the Ottoman privateer brothers Oru\u00e7 and Hayreddin\u2014both known to Europeans as Barbarossa, or \"Red Beard\"\u2014were operating successfully off Tunisia under the Hafsids. In 1516, Oru\u00e7 moved his base of operations to Algiers. He asked for the protection of the Ottoman Empire in 1517, but was killed in 1518 during his invasion of the Zayyanid Kingdom of Tlemcen. Hayreddin succeeded him as military commander of Algiers.\nIn 1551 Hasan Pasha, the son of Hayreddin defeated the Spanish-Moroccan armies during a campaign to recapture Tlemcen, thus cementing Ottoman control in western and central Algeria.\nAfter that, the conquest of Algeria sped up. In 1552 Salah Rais, with the help of some Kabyle kingdoms, conquered Touggourt, and established a foothold in the Sahara.\nIn the 1560s eastern Algeria was centralized, and the power struggle which had been present ever since the Emirate of B\u00e9ja\u00efa collapsed came to an end.\nDuring the 16th, 17th, and early 18th century, the Kabyle Kingdoms of Kuku and Ait Abbas managed to maintain their independence repelling Ottoman attacks several times, notably in the First Battle of Kalaa of the Beni Abbes. This was mainly thanks to their ideal position deep inside the Kabylia Mountains and their great organisation, and the fact that unlike in the West and East where collapsing kingdoms such as Tlemcen or B\u00e9ja\u00efa were present, Kabylia had two new and energetic emirates.\nBase in the war against Spain.\nHayreddin Barbarossa established the military basis of the regency. The Ottomans provided a supporting garrison of 2,000 Turkish troops with artillery. He left Hasan Agha in command as his deputy when he had to leave for Constantinople in 1533. The son of Barbarossa, Hasan Pashan was in 1544 when his father retired, the first governor of the Regency to be directly appointed by the Ottoman Empire. He took the title of \"beylerbey\". Algiers became a base in the war against Spain, and also in the Ottoman conflicts with Morocco.\n\"Beylerbeys\" continued to be nominated for unlimited tenures until 1587. After Spain had sent an embassy to Constantinople in 1578 to negotiate a truce, leading to a formal peace in August 1580, the Regency of Algiers was a formal Ottoman territory, rather than just a military base in the war against Spain. At this time, the Ottoman Empire set up a regular Ottoman administration in Algiers and its dependencies, headed by \"Pashas\", with 3-year terms to help considate Ottoman power in the Maghreb.\nMediterranean privateers.\nDespite the end of formal hostilities with Spain in 1580, attacks on Christian and especially Catholic shipping, with slavery for the captured, became prevalent in Algiers and were actually the main industry and source of revenues of the Regency.\nIn the early 17th century, Algiers also became, along with other North African ports such as Tunis, one of the bases for Anglo-Turkish piracy. There were as many as 8,000 renegades in the city in 1634. (Renegades were former Christians, sometimes fleeing the law, who voluntarily moved to Muslim territory and converted to Islam.) Hayreddin Barbarossa is credited with tearing down the Pe\u00f1\u00f3n of Algiers and using the stone to build the inner harbor.\nA contemporary letter states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"The infinity of goods, merchandise jewels and treasure taken by our English pirates daily from Christians and carried to Algire and Tunis to the great enriching of Mores and Turks and impoverishing of Christians\"\nPrivateers and slavery of Christians originating from Algiers were a major problem throughout the centuries, leading to regular punitive expeditions by European powers. Spain (1567, 1775, 1783), Denmark (1770), France (1661, 1665, 1682, 1683, 1688), England (1622, 1655, 1672), all led naval bombardments against Algiers. Abraham Duquesne fought the Barbary pirates in 1681 and bombarded Algiers between 1682 and 1683, to help Christian captives.\nPolitical Turmoil (1659-1713).\nThe Agha period.\nIn 1659 the Janissaries of the Odjak of Algiers took over the country, and removed the local Pasha with the blessing of the Ottoman Sultan. From there on a system of dual leaders was in place. There was first and foremost the Agha, elected by the Odjak, and the Pasha appointed by the Ottoman Sublime Porte, whom was a major cause of unrest. Of course, this duality was not stable. All of the Aghas were assassinated, without an exception. Even the first Agha was killed after only 1 year of rule. Thanks to this the Pashas from Constantinople were able to increase the power, and reaffirm Turkish control over the region. In 1671, the Rais, the pirate captains, elected a new leader, Mohamed Trik. The Janissaries also supported him, and started calling him the Dey, which means Uncle in Turkish.\nEarly Dey period (1671-1710).\nIn the early Dey period the country worked similarly to before, with the Pasha still holding considerable powers, but instead of the Janissaries electing their own leaders freely, other factions such as the Taifa of Rais also wanted to elect the deys. Mohammed Trik, taking over during a time instability was faced with heavy issues. Not only were the Janissaries on a rampage, removing any leaders for even the smallest mistakes (even if those leaders were elected by them), but the native populace was also restless. The conflicts with European powers didn't help this either. In 1677, following an explosion in Algiers and several attempts at his life, Mohammed escaped to Tripoli leaving Algiers to Baba Hassan. Just 4 years into his rule he was already at war with one of the most powerful countries in Europe, the Kingdom of France. In 1682 France bombarded Algiers for the first time. The Bombardment was inconclusive, and the leader of the fleet Abraham Duquesne failed to secure the submission of Algiers. The next year, Algiers was bombarded again, this time liberating a few slaves. Before a peace treaty could be signed though, Baba Hassan was deposed and killed by a Rais called Mezzo Morto H\u00fcseyin. Continuing the war against France he was defeated in a naval battle in 1685, near Cherchell, and at last a French Bombardment in 1688 brought an end to his reign, and the war. His successor, Hadj Chabane was elected by the Ra\u00efs. He defeated Morocco in the Battle of Moulouya and defeated Tunis as well. He went back to Algiers, but he was assassinated in 1695 by the Janissaries whom once again took over the country. From there on Algiers was in turmoil once again. Leaders were assassinated, despite not even ruling for a year, and the Pasha was still a cause of unrest. The only notable event during this time of unrest was the recapture of Oran and Mers-el-K\u00e9bir from the Spanish.\nCoup of Baba Ali Chaouche, and independence.\nBaba Ali Chaouche, also written as Chaouch, took over the country, ending the rule of the Janissaries. The Pasha attempted to resist him, but instead he was sent home, and told to never come back, and if he did he will be executed. He also sent a letter to the Ottoman sultan declaring that Algiers will from then on act as an independent state, and will not be an Ottoman vassal, but an ally at best. The Sublime Porte, enraged, tried to send another Pasha to Algiers, whom was then sent back to Constantinople by the Algerians. This marked the \"de facto\" independence of Algiers from the Ottoman Empire.\nDanish\u2013Algerian War.\nIn the mid-1700s Dano-Norwegian trade in the Mediterranean expanded. In order to protect the lucrative business against piracy, Denmark\u2013Norway had secured a peace deal with the states of Barbary Coast. It involved paying an annual tribute to the individual rulers and additionally to the States.\nIn 1766, Algiers had a new ruler, dey Baba Mohammed ben-Osman. He demanded that the annual payment made by Denmark-Norway should be increased, and he should receive new gifts. Denmark\u2013Norway refused the demands. Shortly after, Algerian pirates hijacked three Dano-Norwegian ships and allowed the crew to be sold as slaves.\nThey threatened to bombard the Algerian capital if the Algerians did not agree to a new peace deal on Danish terms. Algiers was not intimidated by the fleet, the fleet was of 2 frigates, 2 bomb galiot and 4 ship of the line.\nAlgerian-Sharifian War.\nIn the west, the Algerian-Cherifian conflicts shaped the western border of Algeria.\nThere were numerous battles between the Regency of Algiers and the Sharifian Empires for example: the campaign of Tlemcen in 1551, the campaign of Tlemcen in 1557, the Battle of Moulouya and the Battle of Chelif. The independent Kabyle Kingdoms also had some involvement, the Kingdom of Beni Abbes participated in the campaign of Tlemcen in 1551 and the Kingdom of Kuku provided Zwawa troops for the capture of Fez in 1576 in which Abd al-Malik was installed as an Ottoman vassal ruler over the Saadi Dynasty. The Kingdom of Kuku also participated in the capture of Fez in 1554 in which Salih Rais defeated the Moroccan army and conquered Morocco up until Fez, adding these territories to the Ottoman crown and placing Ali Abu Hassun as the ruler and vassal to the Ottoman sultan. In 1792 the Regency of Algiers managed to take possession of the Moroccan Rif and Oujda, which they then abandoned in 1795 for unknown reasons.\nBarbary Wars.\nDuring the early 19th century, Algiers again resorted to widespread piracy against shipping from Europe and the young United States of America, mainly due to internal fiscal difficulties, and the damage caused by the Napoleonic Wars. This in turn led to the First Barbary War and Second Barbary War, which culminated in August 1816 when Lord Exmouth executed a naval bombardment of Algiers, the biggest, and most successful one. The Barbary Wars resulted in a major victory for the American, British, and Dutch Navy.\nPolitical status.\n1516-1567.\nIn between 1516 and 1567, the rulers of the Regency were chosen by the Ottoman sultan. During the first few decades, Algiers was completely aligned with the Ottoman Empire, although it later gained a certain level of autonomy as it was the westernmost province of the Ottoman Empire, and administering it directly would have been problematic.\n1567-1710.\nDuring this period a form of dual leadership was in place, with the Aghas sharing power and influence with a Pasha appointed by the Ottoman sultan from Constantinople. After 1567, the Deys became the main leaders of the country, although the Pashas still retained some power.\n1710-1830.\nAfter a coup by Baba Ali Chaouch, the political situation of Algiers became complicated.\nRelation with the Ottoman Empire.\nSome sources describe it as completely independent from the Ottomans, albeit the state was still nominally part of the Ottoman Empire.\nCur Abdy, dey of Algiers shouted at an Ottoman envoy for claiming that the Ottoman Padishah was the king of Algiers (\"King of Algiers? King of Algiers? If he is the King of Algiers then who am I?\").\nDespite the Ottomans having no influence in Algiers, and the Algerians often ignoring orders from the Ottoman sultan, such as in 1784. In some cases Algiers also participated in the Ottoman Empire's wars, such as the Russo-Turkish War (1787\u20131792), albeit this was not common, and in 1798 for example Algiers sold wheat to the French Empire campaigning in Egypt against the Ottomans through two Jewish traders.\nIn some cases, Algiers was declared to be a country rebelling against the holy law of Islam by the Ottoman Caliph. This usually meant a declaration of war by the Ottomans against the Deylik of Algiers. This could happen due to many reasons. For example, under the rule of Haji Ali Dey, Algerian pirates regularly attacked Ottoman shipments, and Algiers waged war against the Beylik of Tunis, despite several protests by the Ottoman Porte, which resulted in a declaration of war.\nIt can be thus said that the relationship between the Ottoman Empire and Algiers mainly depended on what the Dey at the time wanted. While in some cases, if the relationship between the two was favorable, Algiers did participate in Ottoman wars, Algiers otherwise remained completely autonomous from the rest of the Empire similar to the other Barbary States.\nFrench rule.\n19th century colonialism.\nNorth African boundaries have shifted during various stages of the conquests. The borders of modern Algeria were expanded by the French, whose colonization began in 1830 (French invasion began on July 5), though it was not fully conquered and pacified until 1903. To benefit French colonists (many of whom were not in fact of French origin but Italian, Maltese, and Spanish) and nearly the entirety of whom lived in urban areas, northern Algeria was eventually organized into overseas departments of France, with representatives in the French National Assembly. France controlled the entire country, but the traditional Muslim population in the rural areas remained separated from the modern economic infrastructure of the European community.\nAs a result of what the French considered an insult to the French consul in Algiers by the Day in 1827, France blockaded Algiers for three years. In 1830, France invaded and occupied the coastal areas of Algeria, citing a diplomatic incident as casus belli. Hussein Dey went into exile. French colonization then gradually penetrated southwards, and came to have a profound impact on the area and its populations. The European conquest, initially accepted in the Algiers region, was soon met by a rebellion, led by Abdel Kadir, which took roughly a decade for the French troops to put down. By 1848 nearly all of northern Algeria was under French control, and the new government of the French Second Republic declared the occupied lands an integral part of France. Three \"civil territories\"\u2014Algiers, Oran, and Constantine\u2014were organized as French d\u00e9partements (local administrative units) under a civilian government. During the \"Pacification of Algeria\", which lasted until 1903, the French perpetrated atrocities which included mass executions of civilians and prisoners and the use of concentration camps; many estimates indicates that the native Algerian population fell by one-third in the years between the French invasion and the end of fighting in the mid-1870s due to warfare, disease and starvation. Various governments and scholars consider France's actions in Algeria as constituting a genocide.\nNapoleon III set up a Project of Arab kingdom in Algeria between 1860 and 1870. His goal was to take Algeria out of legal limbo and make it a kingdom associated with France before his project was abandoned by the Third Republic.\nIn addition to enduring the affront of being ruled by a foreign, non-Muslim power, many Algerians lost their lands to the new government or to colonists. Traditional leaders were eliminated, coopted, or made irrelevant, and the traditional educational system was largely dismantled; social structures were stressed to the breaking point. From 1856, native Muslims and Jews were viewed as French subjects not citizens.\nHowever, in 1865, Napoleon III allowed them to apply for full French citizenship, a measure that few took, since it involved renouncing the right to be governed by \"sharia\" law in personal matters, and was considered a kind of apostasy; in 1870, the Cr\u00e9mieux Decree made French citizenship automatic for Jewish natives, a move which largely angered many Muslims, which resulted in the Jews being seen as the accomplices of the colonial power by anti-colonial Algerians. Nonetheless, this period saw progress in health, some infrastructures, and the overall expansion of the economy of Algeria, as well as the formation of new social classes, which, after exposure to ideas of equality and political liberty, would help propel the country to independence.\nDuring the colonization France focused on eradicating the local culture by destroying hundreds years old palaces and important buildings. It is estimated that around half of Algiers, a city founded in the 10th century, was destroyed. Many segregatory laws were levied against the Algerians and their culture.\nRise of Algerian nationalism and French resistance.\nA new generation of Islamic leadership emerged in Algeria at the time of World War I and grew to maturity during the 1920s and 1930s. Various groups were formed in opposition to French rule, most notable the National Liberation Front (FLN) and the National Algerian Movement.\n\"Colons\" (colonists), or, more popularly, \"pieds noirs\" (literally, black feet) dominated the government and controlled the bulk of Algeria's wealth. Throughout the colonial era, they continued to block or delay all attempts to implement even the most modest reforms. But from 1933 to 1936, mounting social, political, and economic crises in Algeria induced the indigenous population to engage in numerous acts of political protest. The government responded with more restrictive laws governing public order and security. Algerian Muslims rallied to the French side at the start of World War II as they had done in World War I. But the colons were generally sympathetic to the collaborationist Vichy regime established following France's defeat by Nazi Germany. After the fall of the Vichy regime in Algeria (November 11, 1942) as a result of Operation Torch, the Free French commander in chief in North Africa slowly rescinded repressive Vichy laws, despite opposition by colon extremists.\nIn March 1943, Muslim leader Ferhat Abbas presented the French administration with the Manifesto of the Algerian People, signed by 56 Algerian nationalist and international leaders. The manifesto demanded an Algerian constitution that would guarantee immediate and effective political participation and legal equality for Muslims. Instead, the French administration in 1944 instituted a reform package, based on the 1936 Viollette Plan, that granted full French citizenship only to certain categories of \"meritorious\" Algerian Muslims, who numbered about 60,000. In April 1945 the French had arrested the Algerian nationalist leader Messali Hadj. On May 1 the followers of his Parti du Peuple Alg\u00e9rien (PPA) participated in demonstrations which were violently put down by the police. Several Algerians were killed. The tensions between the Muslim and colon communities exploded on May 8, 1945, V-E Day, causing the S\u00e9tif and Guelma massacre. When a Muslim march was met with violence, marchers rampaged. The army and police responded by conducting a prolonged and systematic ratissage (literally, raking over) of suspected centers of dissidence. According to official French figures, 1,500 Muslims died as a result of these countermeasures. Other estimates vary from 6,000 to as high as 45,000 killed. Many nationalists drew the conclusion that independence could not be won by peaceful means, and so started organizing for violent rebellion.\nIn August 1947, the French National Assembly approved the government-proposed Organic Statute of Algeria. This law called for the creation of an Algerian Assembly with one house representing Europeans and \"meritorious\" Muslims and the other representing the remaining 8 million or more Muslims. Muslim and colon deputies alike abstained or voted against the statute but for diametrically opposed reasons: the Muslims because it fell short of their expectations and the colons because it went too far.\nAlgerian War of Independence (1954\u20131962).\nThe Algerian War of Independence (1954\u20131962), brutal and long, was the most recent major turning point in the country's history. Although often fratricidal, it ultimately united Algerians and seared the value of independence and the philosophy of anticolonialism into the national consciousness.\nIn the early morning hours of November 1, 1954, the National Liberation Front (Front de Lib\u00e9ration Nationale\u2014FLN) launched attacks throughout Algeria in the opening salvo of a war of independence. An important watershed in this war was the massacre of Pieds-Noirs civilians by the FLN near the town of Philippeville in August 1955. Which prompted Jacques Soustelle into calling for more repressive measures against the rebels. The French authorities claimed that 1,273 \"guerrillas\" died in what Soustelle admitted were \"severe\" reprisals. The FLN subsequently, giving names and addresses, claimed that 12,000 Muslims were killed. After Philippeville, all-out war began in Algeria. The FLN fought largely using guerrilla tactics whilst the French counter-insurgency tactics often included severe reprisals and repression.\nEventually, protracted negotiations led to a cease-fire signed by France and the FLN on March 18, 1962, at Evian, France. The Evian accords also provided for continuing economic, financial, technical, and cultural relations, along with interim administrative arrangements until a referendum on self-determination could be held. The Evian accords guaranteed the religious and property rights of French settlers, but the perception that they would not be respected led to the exodus of one million \"pieds-noirs\" and \"harkis\".\nAbusive tactics of the French Army remains a controversial subject in France to this day. Deliberate illegal methods were used, such as beatings, mutilations, hanging by the feet or hands, torture by electroshock, waterboarding, sleep deprivation and sexual assaults, among others. French war crimes against Algerian civilians were also committed, including indiscriminate shootings of civilians, bombings of villages suspected of helping the ALN, rape, disembowelment of pregnant women, imprisonment without food in small cells (some of which were small enough to impede lying down), throwing prisoners out of helicopters to their death or into the sea with concrete on their feet, and burying people alive.\nThe FLN also committed many atrocities, both against French pieds-noirs and against fellow Algerians whom they deemed as supporting the French. These crimes included killing unarmed men, women and children, rape and disembowelment or decapitation of women and murdering children by slitting their throats or banging their heads against walls.\nBetween 350,000 and 1 million Algerians are estimated to have died during the war, and more than 2 million, out of a total Muslim population of 9 or 10 million, were made into refugees or forcibly relocated into government-controlled camps. Much of the countryside and agriculture was devastated, along with the modern economy, which had been dominated by urban European settlers (the \"pied-noirs\"). French sources estimated that at least 70,000 Muslim civilians were killed or abducted and presumed killed, by the FLN during the Algerian War. Nearly one million people of mostly French, Spanish and Italian descent left the country at independence due to the privileges that they lost as settlers and their unwillingness to be on equal footing with indigenous Algerians along with them left most Algerians of Jewish descent and those Muslim Algerians who had supported a French Algeria (\"harkis\"). 30\u2013150,000 pro-French Muslims were also killed in Algeria by FLN in post-war reprisals.\nIndependent Algeria.\nBen Bella presidency (1962\u201365).\nThe Algerian independence referendum was held in French Algeria on 1 July 1962, passing with 99.72% of the vote. As a result, France declared Algeria independent on 3 July. On 8 September 1963, the first Algerian constitution was adopted by nationwide referendum under close supervision by the National Liberation Front (FLN). Later that month, Ahmed Ben Bella was formally elected the first president of Algeria for a five-year term after receiving support from the FLN and the military, led by Colonel Houari Boum\u00e9di\u00e8ne.\nHowever, the war for independence and its aftermath had severely disrupted Algeria's society and economy. In addition to the destruction of much of Algeria's infrastructure, an exodus of the upper-class French and European \"colons\" from Algeria deprived the country of most of its managers, civil servants, engineers, teachers, physicians, and skilled workers. The homeless and displaced numbered in the hundreds of thousands, many suffering from illness, and some 70 percent of the workforce was unemployed. The months immediately following independence witnessed the pell-mell rush of Algerians and government officials to claim the property and jobs left behind by the European \"colons\". For example in the 1963 March Decrees, President Ben Bella declared all agricultural, industrial, and commercial properties previously owned and operated by Europeans vacant, thereby legalizing confiscation by the state.\nThe military played an important role in Ben Bella's administration. Since the president recognized the role that the military played in bringing him to power, he appointed senior military officers as ministers and other important positions within the new state, including naming Colonel Boum\u00e9di\u00e8ne as defence minister. These military officials played a core role into implementing the country's security and foreign policy.\nUnder the new constitution, Ben Bella's presidency combined the functions of chief of state and head of government with those of supreme commander of the armed forces. He formed his government without needing legislative approval and was responsible for the definition and direction of its policies. There was no effective institutional check on the president's powers. As a result, opposition leader Hocine A\u00eft-Ahmed quit the National Assembly in 1963 to protest the increasingly dictatorial tendencies of the regime and formed a clandestine resistance movement, the Socialist Forces Front (\"Front des Forces Socialistes\"\u2014FFS), dedicated to overthrowing the Ben Bella regime by force.\nLate summer 1963 saw sporadic incidents attributed to the FFS, but more serious fighting broke out a year later, and the army moved quickly and in force to crush a rebellion. Minister of Defense Boum\u00e9di\u00e8ne had no qualms about sending the army to put down regional uprisings because he felt they posed a threat to the state. However, President Ben Bella attempted to co-opt allies from among these regional leaders in order to undermine the ability of military commanders to influence foreign and security policy. Tensions consequently built between Boum\u00e9di\u00e8ne and Ben Bella, and in 1965 the military removed Ben Bella in a coup d'\u00e9tat, replacing him with Boum\u00e9di\u00e8ne as head of state.\nThe 1965 coup and the Boum\u00e9dienne military regime.\nOn 19 June 1965, Houari Boum\u00e9di\u00e8ne deposed Ahmed Ben Bella in a military coup d'\u00e9tat that was both swift and bloodless. Ben Bella \"disappeared\", and would not be seen again until he was released from house arrest in 1980 by Boum\u00e9di\u00e8ne's successor, Colonel Chadli Bendjedid. Boum\u00e9di\u00e8ne immediately dissolved the National Assembly and suspended the 1963 constitution. Political power resided in the Nation Council of the Algerian Revolution (\"Conseil National de la R\u00e9volution Alg\u00e9rienne\"\u2014CNRA), a predominantly military body intended to foster cooperation among various factions in the army and the party.\nHouari Boum\u00e9di\u00e8ne's position as head of government and of state was initially insecure, partly because of his lack of a significant power base outside of the armed forces. He relied strongly on a network of former associates known as the Oujda group, named after Boum\u00e9di\u00e8ne's posting as National Liberation Army (\"Arm\u00e9e de Lib\u00e9ration Nationale\"\u2014ALN) leader in the Moroccan border town of Oujda during the war years, but he could not fully dominate his fractious regime. This situation may have accounted for his deference to collegial rule.\nOver Boum\u00e9di\u00e8ne's 11-year reign as Chairman of the CNRA, the council introduced two formal mechanisms: the People's Municipal Assembly (\"Assembl\u00e9e Populaires Communales\") and the People's Provincial Assembly (\"Assembl\u00e9e Populaires de Wilaya\") for popular participation in politics. Under Boum\u00e9di\u00e8ne's rule, leftist and socialist concepts were merged with Islam.\nBoum\u00e9di\u00e8ne also used Islam to opportunistically consolidate his power. On one hand, he made token concessions and cosmetic changes to the government to appear more Islamic, such as putting Islamist Ahmed Taleb Ibrahimi in charge of national education in 1965 and adopting policies criminalizing gambling, establishing Friday as the national holiday, and dropping plans to introduce birth control to paint an Islamic image of the new government. But on the other hand, Boum\u00e9di\u00e8ne's government also progressively repressed Islamic groups, such as by ordering the dissolution of Al Qiyam.\nFollowing attempted coups\u2014most notably that of chief-of-staff Col. in December 1967\u2014and a failed assassination attempt on 25 April 1968, Boum\u00e9di\u00e8ne consolidated power and forced military and political factions to submit. He took a systematic, authoritarian approach to state building, arguing that Algeria needed stability and an economic base before building any political institutions.\nEleven years after Boum\u00e9di\u00e8ne took power, after much public debate, a long-promised new constitution was promulgated in November 1976. The constitution restored the National Assembly and gave it legislative, consent, and oversight functions. Boum\u00e9di\u00e8ne was later elected president with 95 percent of the cast votes.\nBendjedid rule (1978\u201392), the 1992 Coup d'\u00c9tat and the rise of the civil war.\nBoum\u00e9di\u00e8ne's death on 27 December 1978 set off a struggle within the FLN to choose a successor. A deadlock occurred between two candidates was broken when Colonel Chadli Bendjedid, a moderate who had collaborated with Boum\u00e9di\u00e8ne in deposing Ahmed Ben Bella, was sworn in on February 9, 1979. He was re-elected in 1984 and 1988. After the violent 1988 October Riots, a new constitution was adopted in 1989 that eradicated the Algerian one-party state by allowing the formation of political associations in addition to the FLN. It also removed the armed forces, which had run the government since the days of Boum\u00e9di\u00e8ne, from a role in the operation of the government.\nAmong the scores of parties that sprang up under the new constitution, the militant Islamic Salvation Front (\"Front Islamique du Salut\"\u2014FIS) was the most successful, winning a majority of votes in the June 1990 municipal elections, as well as the first stage of the December national legislative elections.\nThe surprising first round of success for the fundamentalist FIS party in the December 1991 balloting caused the army to discuss options to intervene in the election. Officers feared that an Islamist government would interfere with their positions and core interests in economic, national security, and foreign policy, since the FIS has promised to make a fundamental re-haul of the social, political, and economic structure to achieve a radical Islamist agenda. Senior military figures, such as Defence Minister Khaled Nezzar, Chief of the General Staff Abdelmalek Guenaizia, and other leaders of the navy, Gendarmerie, and security services, all agreed that the FIS should be stopped from gaining power at the polling box. They also agreed that Bendjedid would need to be removed from office due to his determination to uphold the country's new constitution by continuing with the second round of ballots.\nOn 11 January 1992, Bendjedid announced his resignation on national television, saying it was necessary to \"protect the unity of the people and the security of the country\". Later that same day, the High Council of State (\"Haut Comit\u00e9 d'Etat\"\u2014HCE), which was composed of five people (including Khaled Nezzar, , Ali Kafi, Mohamed Boudiaf and Ali Haroun), was appointed to carry out the duties of the president.\nThe new government, led by Sid Ahmed Ghozali, banned all political activity at mosques and began stopping people from attending prayers at popular mosques. The FIS was legally dissolved by Interior Minister Larbi Belkheir on 9 February for attempting \"insurrections against the state\". A state of emergency was also declared and extraordinary powers, such as curtailing the right to associate, were granted to the regime.\nBetween January and March, a growing number of FIS militants were arrested by the military, including Abdelkader Hachani and his successors, Othman Aissani and Rabah Kebir. Following the announcement to dissolve the FIS and implement a state of emergency on 9 February, the Algerian security forces used their new emergency powers to conduct large scale arrests of FIS members and housed them in 5 \"detention centers\" in the Sahara. Between 5,000 (official number) and 30,000 (FIS number) people were detained.\nThis crackdown led to a fundamental Islamic insurgency, resulting in the continuous and brutal 10 year-long Algerian Civil War. During the civil war, the secular state apparatus nonetheless allowed elections featuring pro-government and moderate religious-based parties. The civil war lasted from 1991 to 2002.\nCivil War and Bouteflika (1992\u20132019).\nAfter Chadli Bendjedid resigned from the presidency in the military coup of 1992, a series of figureheads were selected by the military to assume the presidency, as officers were reluctant to assume public political power even though they had manifested control over the government. Additionally, the military's senior leaders felt a need to give a civilian face to the new political regime they had hastily constructed in the aftermath of Benjedid's ousting and the termination of elections, preferring a friendlier non-military face to front the regime.\nThe first such head of state was Mohamed Boudiaf, who was appointed president of the High Council of State (HCE) in February 1992 after a 27-year exile in Morocco. However, Boudiaf quickly came to odds with the military when attempts by Boudiaf to appoint his own staff or form a political party were viewed with suspicion by officers. Boudiaf also launched political initiatives, such as a rigorous anti-corruption campaign in April 1992 and the sacking of Khaled Nezzar from his post as Defence Minister, which were seen by the military as an attempt to remove their influence in the government. The former of these initiatives was especially hazardous to the many senior military officials who had benefited massively and illegally from the political system for years. In the end, Boudiaf was assassinated in June 1992 by one of his bodyguards with Islamist sympathies.\nAli Kafi briefly assumed the HCE presidency after Boudiaf's death, before Liamine Z\u00e9roual was appointed as a long-term replacement in 1994. However, Z\u00e9roual only remained in office for four years before he announced his retirement, as he quickly became embroiled in a clan warfare within the upper classes of the military and fell out with groups of the more senior generals. After this Abdelaziz Bouteflika, Boum\u00e9di\u00e8ne's foreign minister, succeeded as the president.\nAs the Algerian civil war wound to a close, presidential elections were held again in April 1999. Although seven candidates qualified for election, all but Abdelaziz Bouteflika, who had the support of the military as well as the National Liberation Front (FLN), withdrew on the eve of the election amid charges of electoral fraud and interference from the military. Bouteflika went on to win with 70 percent of the cast votes.\nDespite the purportedly democratic elections, the civilian government immediately after the 1999 elections only acted as a sort of 'hijab' over the true government, mostly running day-to-day businesses, while the military still largely ran the country behind the scenes. For example, ministerial mandates to individuals were only granted with the military's approval, and different factions of the military invested in various political parties and the press, using them as pawns to gain influence.\nHowever, the military's influence over politics decreased gradually, leaving Bouteflika with more authority on deciding policy. One reason for this was that the senior commanders who had dominated the political scene during the 1960s and 1970s started to retire. Bouteflika's former experience as Boum\u00e9di\u00e8ne's foreign minister earned him connections that rejuvenated Algeria's international reputation, which had been tarnished in the early 1990s due to the civil war. On the domestic front, Bouteflika's policy of \"national reconciliation\" to bring a close to civilian violence earned him a popular mandate that helped him to win further presidential terms in 2004, 2009 and 2014.\nIn 2010, journalists gathered to demonstrate for press freedom and against Bouteflika's self-appointed role as editor-in-chief of Algeria's state television station. In February 2011, the government rescinded the state of emergency that had been in place since 1992 but still banned all protest gatherings and demonstrations. However, in April 2011, over 2,000 protesters defied the official ban and took to the streets of Algiers, clashing with police forces. These protests can be seen as a part of the Arab Spring, with protesters noting that they were inspired by the recent Egyptian revolution, and that Algeria was a police state that was \"corrupt to the bone\".\nIn 2019, after 20 years in office, Bouteflika announced in February that he would seek a fifth term of office. This sparked widespread discontent around Algeria and protests in Algiers. Despite later attempts at saying he would resign after his term finished in late April, Bouteflika resigned on 2 April, after the chief of the army, Ahmed Gaid Salah, made a declaration that he was \"unfit for office\". Despite Gaid Salah being loyal to Bouteflika, many in the military identified with civilians, as nearly 70 percent of the army are civilian conscripts who are required to serve for 18 months. Also, since demonstrators demanded a change to the whole governmental system, many army officers aligned themselves with demonstrators in the hopes of surviving an anticipated revolution and retaining their positions.\nAfter Bouteflika (2019-).\nAfter the resignation of Abdelaziz Bouteflika on 9 April 2019, the President of the Council of the Nation Abdelkader Bensalah became acting president of Algeria.\nFollowing the presidential election on 12 December 2019, Abdelmadjid Tebboune was elected president after taking 58% of the votes, beating the candidates from both main parties, the National Liberation Front and the Democratic National Rally.\nOn the eve of the first anniversary of the Hirak Movement, which led to the resignation of former president Bouteflika, President Abdelmadjid Tebboune announced in a statement to the Algerian national media that 22 February would be declared the Algerian \"National Day of Fraternity and Cohesion between the People and Its Army for Democracy.\" In the same statement, Tebboune spoke in favor of the Hirak Movement, saying that \"the blessed Hirak has preserved the country from a total collapse\", and that he had \"made a personal commitment to carry out all of the [movement's] demands.\" On 21 and 22 February 2020, masses of demonstrators (with turnout comparable to well-established Algerian holidays like the Algerian Day of Independence) gathered to honor the anniversary of the Hirak Movement and the newly established national day.\nIn an effort to contain the COVID-19 pandemic, Tebboune announced on 17 March 2020 that \"marches and rallies, whatever their motives\" would be prohibited. But after protesters and journalists were arrested for participating in such marches, Tebboune faced accusations of attempting to \"silence Algerians.\" Notably, the government's actions were condemned by Amnesty International, which said in a statement that \"when all eyes [...] are on the management of the COVID-19 pandemic, the Algerian authorities are devoting time to speeding up the prosecution and trial of activists, journalists, and supporters of the Hirak movement.\" The National Committee for the Liberation of Detainees (\"Comit\u00e9 national pour la lib\u00e9ration des d\u00e9tenus\"\u2014CNLD) estimated that around 70 prisoners of conscience were imprisoned by 2 July 2020 and that several of the imprisoned were arrested for Facebook posts.\nOn 28 December 2019, the then-recently inaugurated President Tebboune met with Ahmed Benbitour, the former Algerian Head of Government, with whom he discussed the \"foundations of the new Republic.\" On 8 January 2020, Tebboune established a \"commission of experts\" composed of 17 members (a majority of which were professors of constitutional law) responsible for examining the previous constitution and making any necessary revisions. Led by Ahmed Laraba, the commission was required to submit its proposals to Tebboune directly within the following two months. In a letter to Laraba on the same day, Tebboune outlined seven axes around which the commission should focus its discussion. These areas of focus included strengthening citizens' rights, combating corruption, consolidating the balance of powers in the Algerian government, increasing the oversight powers of parliament, promoting the independence of the judiciary, furthering citizens' equality under the law, and constitutionalizing elections. Tebboune's letter also included a call for an \"immutable and intangible\" two-term limit to anyone serving as president \u2014 a major point of contention in the initial Hirak Movement protests, which were spurred by former president Abdelaziz Bouteflika's announcement to run for a fifth term.\nThe preliminary draft revision of the constitution was publicly published on 7 May 2020, but the Laraba Commission (as the \"commission of experts\" came to be known) was open to additional proposals from the public until 20 June. By 3 June, the commission had received an estimated 1,200 additional public proposals. After all revisions were considered by the Laraba Commission, the draft was introduced to the Cabinet of Algeria (Council of Ministers).\nThe revised constitution was adopted in the Council of Ministers on 6 September, in the People's National Assembly on 10 September, and in the Council of the Nation on 12 September. The constitutional changes were approved in the 1 November 2020 referendum, with 66.68% of voters participating in favour of the changes.\nOn 16 February 2021, mass protests and a wave of nationwide rallies and peaceful demonstrations against the government of Abdelmadjid Tebboune began. In May 2021, Algeria prohibited any protests that do not have prior approval by authorities.\nIn September 2024, President Tebboune won a second term with a landslide 84.3 percent of the vote, although his opponents called the results fraud.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14114", "revid": "37015224", "url": "https://en.wikipedia.org/wiki?curid=14114", "title": "History of Zimbabwe", "text": "Until roughly 2,000 years ago, what would become Zimbabwe was populated by ancestors of the San people. Bantu inhabitants of the region arrived and developed ceramic production in the area. A series of trading empires emerged, including the Kingdom of Mapungubwe and Kingdom of Zimbabwe. In the 1880s, the British South Africa Company began its activities in the region, leading to the colonial era in Southern Rhodesia.\nIn 1965, the colonial government declared itself independent as Rhodesia, but largely failed to secure international recognition and faced sustained internal opposition in the Rhodesian Bush War.\nAfter fifteen years of war, following the Lancaster House Agreement of 1979 there was a transition to internationally recognised majority rule in 1980. The United Kingdom, which had never recognised Rhodesian independence, briefly imposed direct rule in order to grant independence on 18 April that year as the new country of Zimbabwe. In the 2000s Zimbabwe's economy began to deteriorate due to various factors, including the imposition of economic sanctions by Western countries led by the United Kingdom and widespread corruption in government. Economic instability caused many Zimbabweans to emigrate. Prior to its recognized independence as Zimbabwe in 1980, the nation had been known by several names: Rhodesia, Southern Rhodesia, and Zimbabwe Rhodesia.\nPre-Colonial era (150,000 BCE \u2013 1852 CE).\nPrehistory.\nBy 150,000 BC, \"Homo sapiens\" had migrated to the region now known as Zimbabwe from East Africa. Prior to the arrival of Bantu speakers in present-day Zimbabwe the region was populated by ancestors of the San people. The first Bantu-speaking farmers arrived during the Bantu expansion around 2000 years ago.\nThese Bantu speakers were the makers of early Iron Age pottery belonging to the Silver Leaves or Matola tradition, of the third to fifth centuries A.D., found in southeast Zimbabwe. This tradition was part of the eastern stream of Bantu expansion (sometimes called Kwale) which originated west of the Great Lakes, spreading to the coastal regions of southeastern Kenya and north eastern Tanzania, and then southwards to Mozambique, south eastern Zimbabwe and Natal. More substantial in numbers in Zimbabwe were the makers of the Ziwa and Gokomere ceramic wares, of the fourth century A.D. Their early Iron Age ceramic tradition belonged to the highlands facies of the eastern stream, which moved inland to Malawi and Zimbabwe. Imports of beads have been found at Gokomere and Ziwa sites, possibly in return for gold exported to the coast.\nA later phase of the Gokomere culture was the Zhizo in southern Zimbabwe. Zhizo communities settled in the Shashe-Limpopo area in the tenth century. Their capital there was Schroda (just across the Limpopo River from Zimbabwe). Many fragments of ceramic figurines have been recovered from there, including figures of animals and birds, and also fertility dolls. The inhabitants produced ivory bracelets and other ivory goods. Imported beads found there and at other Zhizo sites, are evidence of trade, probably of ivory and skins, with traders on the Indian Ocean coast.\nPottery belonging to a western stream of Bantu expansion (sometimes called Kalundu) has been found at sites in northeastern Zimbabwe, dating back to the seventh century. (The western stream originated in the same area as the eastern stream: both belong to the same style system, called by Phillipson the Chifumbadze system, which has general acceptance by archaeologists.) The terms eastern and western streams represent the expansion of the Bantu-speaking peoples in terms of their culture. Another question is about the branches of the Bantu languages which they spoke. It seems that the makers of the Ziwa/Gokomere wares were not the ancestral speakers of the Shona languages of today's Zimbabwe, who did not arrive in there until around the tenth century, from south of the Limpopo river, and whose ceramic culture belonged to the western stream. The linguist and historian Ehret believes that in view of the similarity of the Ziwa/Gokomere pottery to the Nkope of the ancestral Nyasa language speakers, the Ziwa/Gokomere people spoke a language closely related to the Nyasa group. Their language, whatever it was, was superseded by the ancestral Shona languages, although Ehret says that a set of Nyasa words occur in central Shona dialects today.\nThe evidence that the ancestral Shona speakers came from South Africa is that the ceramic styles associated with Shona speakers in Zimbabwe from the thirteenth to the seventeenth centuries can be traced back to western stream (Kalunndu) pottery styles in South Africa. The Ziwa /Gokomere and Zhizo traditions were superseded by Leopards Kopje and Gumanye wares of the Kalundu tradition from the tenth century.\nAlthough the western stream Kalundu tradition was ancestral to Shona ceramic wares, the closest relationships of the ancestral Shona language according to many linguists were with a southern division of eastern Bantu \u2013 such languages as the southeastern languages (Nguni, Sotho-Tswana, Tsonga), Nyasa and Makwa. While it may well be the case that the people of the western stream spoke a language belonging to a wider Eastern Bantu division, it is a puzzle which remains to be resolved that they spoke a language most closely related to the languages just mentioned, all of which are today spoken in southeastern Africa.\nAfter the Shona speaking people moved into the present day Zimbabwe many different dialects developed over time in the different parts of the country. Among these was Kalanga.\nIt is believed that Kalanga speaking societies first emerged in the middle Limpopo valley in the early 12th century before moving on to the Zimbabwean highlands. The Zimbabwean plateau eventually became the centre of subsequent Kalanga states. The Kingdom of Mapungubwe was the first in a series of sophisticated trade states developed in Zimbabwe by the time of the first European explorers from Portugal. They traded in gold, ivory and copper for cloth and glass. From about 1250 until 1450, Mapungubwe was eclipsed by the Kingdom of Zimbabwe. This Kalanga state further refined and expanded upon Mapungubwe's stone architecture, which survives to this day at the ruins of the kingdom's capital of Great Zimbabwe. From c.\u20091450\u20131760, Zimbabwe gave way to the Kingdom of Mutapa. This Kalanga state ruled much of the area that is known as Zimbabwe today, and parts of central Mozambique. It is known by many names including the Mutapa Empire, also known as Mwenemutapa was known for its gold trade routes with Arabs and the Portuguese. Ant\u00f3nio Fernandes, a Portuguese explorer, first entered the area in 1511 from Sofala and encountered the Manyika people. He returned in 1513 and explored the northern region of the territory, coming into contact with Chikuyo Chisamarengu, the ruler of Mutapa. In the early 17th century, Portuguese settlers destroyed the trade and began a series of wars which left the empire in near collapse. As a direct response to Portuguese aggression in the interior, a new Kalanga state emerged called the Rozvi Empire. Relying on centuries of military, political and religious development, the Rozvi (which means \"destroyers\") removed the Portuguese from the Zimbabwe plateau by force of arms. The Rozvi continued the stone building traditions of the Zimbabwe and Mapungubwe kingdoms while adding guns to its arsenal and developing a professional army to protect its trade routes and conquests. Around 1821, the Zulu general Mzilikazi of the Khumalo clan successfully rebelled from King Shaka and created his own clan, the Ndebele. The Ndebele fought their way northwards into the Transvaal, leaving a trail of destruction in their wake and beginning an era of widespread devastation known as the Mfecane. When Boer trekkers converged on the Transvaal in 1836, they drove the tribe even further northward.\nAfter losing their remaining South African lands in 1840, Mzilikazi and his tribe permanently settled the southwest of present-day Zimbabwe in what became known as Matabeleland, establishing Bulawayo as their capital. Mzilikazi then organised his society into a military system with regimental kraals, similar to those of Shaka, which was stable enough to repel further Boer incursions. During the pre-colonial period, the Ndebele social structure was stratified. It was composed mainly of three social groups, abeZansi, Enhla and Amahole. The Zansi were the ruling class of the original Khumalo people who migrated from south of Limpopo with Mzilikazi. The Enhla and Amahole groups were made up of other tribes and ethnics who had been incorporated into the empire during the migration. However, with the passage of time, this stratification has slowly disappeared The Ndebele people have for long ascribed to the worship of Unkunkulu as their supreme being. Their religious life in general, rituals, ceremonies, practices, devotion and loyalty revolves around the worship of this Supreme Being. However, with the popularisation of Christianity and other religions, Ndebele traditional religion is now uncommon.\nMzilikazi died in 1868 and, following a violent power struggle, was succeeded by his son, Lobengula. King Mzilikazi had established the Ndebele Kingdom, with Shona subjects paying tribute to him. The nascent kingdom encountered European powers for the first time and Lobengula signed various treaties with the various nations jostling for power in the region, playing them off one another in order to preserve the sovereignty of his kingdom and gain the aid of the Europeans should the kingdom become involved in a war.\nColonial era (1890\u20131980).\nIn the 1880s, British diamond magnate Cecil Rhodes' British South Africa Company (BSAC) started to make inroads into the region. In 1898, the name Southern Rhodesia was adopted. In 1888, Rhodes obtained a concession for mining rights from King Lobengula of the Ndebele peoples. Cecil Rhodes presented this concession to persuade the British government to grant a royal charter to his British South Africa Company over Matabeleland, and its subject states such as Mashonaland. Rhodes sought permission to negotiate similar concessions covering all territory between the Limpopo River and Lake Tanganyika, then known as 'Zambesia'. In accordance with the terms of aforementioned concessions and treaties, Cecil Rhodes promoted the immigration of white settlers into the region, as well as the establishment of mines, primarily to extract the diamond ores present. In 1895 the BSAC adopted the name 'Rhodesia' for the territory of Zambesia, in honour of Cecil Rhodes. In 1898, 'Southern Rhodesia' became the official denotation for the region south of the Zambezi, which later became Zimbabwe. The region to the north was administered separately by the BSAC and later named Northern Rhodesia (now Zambia).\nThe Shona waged unsuccessful wars (known as Chimurenga) against encroachment upon their lands by clients of BSAC and Cecil Rhodes in 1896 and 1897. Following the failed insurrections of 1896\u201397 the Ndebele and Shona groups became subject to Rhodes's administration thus precipitating European settlement en masse in the new colony.\nThe colony's first formal constitution was drafted in 1899, and copied various pieces of legislation directly from that of the Union of South Africa; Rhodesia was meant to be, in many ways, a shadow colony of the Cape. Many within the administrative framework of the BSAC assumed that Southern Rhodesia, when its \"development\" was \"suitably advanced\", would \"take its rightful place as a member of\" the Union of South Africa after the Second Boer War (1898\u20131902), when the four South African colonies joined under the auspices of one flag and began to work towards the creation of a unified administrative structure. The territory was made open to white settlement, and these settlers were then in turn given considerable administrative powers, including a franchise that, while on the surface non-racial, ensured \"a predominantly European electorate\" which \"operated to preclude Great Britain from modifying her policy in Southern Rhodesia and subsequently treating it as a territory inhabited mainly by Africans whose interests should be paramount and to whom British power should be transferred\".\nWorld War I.\nAs a British territory, Southern Rhodesia immediately joined World War I after the UK declared war on the Central Powers, in August 1914. Rhodesia was noted for its patriotic zeal in joining the war. The main priority of British forces in Southern Africa was the capture of the German colony of South-West Africa, modern-day Namibia. A Rhodesian unit was sent to guard Victoria Falls from a possible German invasion via the Caprivi Strip. Meanwhile, a force was sent to assist British forces in South Africa suppressing the pro-German Maritz Rebellion. Afterwards, Rhodesians participated in the invasion of German South-West Africa.\nFollowing the British victory over German forces in Southern Africa, many Rhodesian units, mostly white, were sent to the Western Front in Europe, where they took part in major battles of the war. A small number of Rhodesian soldiers saw action in the Salonika Front in Macedonia, and some even joined the Royal Flying Corps. Other Rhodesian regiments were sent to participate in the invasion of German East Africa, now Tanzania, in early 1915. A guerrilla war in the German colony began, and the fierce fighting and disease devastated the 2nd Rhodesian Regiment, leading to more regiments of native Africans being raised. By November 1918, the Central Powers surrendered to the Allies, ending World War I.\nSouthern Rhodesia became a self-governing British colony in October 1923, subsequent to a referendum held the previous year. The British government took full command of the British South Africa Company's holdings, including both Northern and Southern Rhodesia. Northern Rhodesia retained its status as a colonial protectorate; Southern Rhodesia was given responsible self-government \u2013 with limitations and still annexed to the crown as a colony. Many studies of the country see it as a state that operated independently within the Commonwealth; nominally under the rule of the Crown, but technically able to do as it pleased. And in theory, Southern Rhodesia was able to govern itself, draft its own legislation, and elect its own parliamentary leaders. But in reality, this was self-government subject to supervision. Until the white minority settler government's declaration of unilateral independence in 1965, London remained in control of the colony's external affairs, and all legislation was subject to approval from the United Kingdom Government and the Queen.\nIn 1930, the Land Apportionment Act divided rural land along racial lines, creating four types of land: white-owned land that could not be acquired by Africans; purchase areas for those Africans who could afford to purchase land; Tribal Trust Lands designated as the African reserves; and Crown lands owned by the state, reserved for future use and public parks. Fifty one percent of the land was given to approximately 50,000 white inhabitants, with 29.8 per cent left for over a million Africans.\nMany Rhodesians served on behalf of the United Kingdom during World War II, mainly in the East African Campaign against Axis forces in Italian East Africa.\nIn 1953, the British government consolidated the two colonies of Rhodesia with Nyasaland (now Malawi) in the ill-fated Federation of Rhodesia and Nyasaland which was dominated by Southern Rhodesia. This move was heavily opposed by the residents of Nyasaland, who feared coming under the domination of white Rhodesians. In 1962, however, with growing African nationalism and general dissent, the British government declared that Nyasaland had the right to secede from the Federation; soon afterwards, they said the same for Northern Rhodesia.\nAfter African-majority governments had assumed control in neighbouring Northern Rhodesia and in Nyasaland, the white-minority Southern Rhodesian government led by Ian Smith made a Unilateral Declaration of Independence (UDI) from the United Kingdom on 11 November 1965. The United Kingdom deemed this an act of rebellion, but did not re-establish control by force. The white minority government declared itself a republic in 1970. A civil war ensued, with Joshua Nkomo's ZAPU and Robert Mugabe's ZANU using assistance from the governments of Zambia and Mozambique. Although Smith's declaration was not recognised by the United Kingdom nor any other foreign power, Southern Rhodesia dropped the designation \"Southern\", and claimed nation status as the Republic of Rhodesia in 1970 although this was not recognised internationally.\nIndependence and the 1980s.\nThe country gained official independence as Zimbabwe on 18 April 1980. The government held independence celebrations in Rufaro stadium in Salisbury, the capital. Lord Christopher Soames, the last Governor of Southern Rhodesia, watched as Charles, Prince of Wales, gave a farewell salute and the Rhodesian Signal Corps played \"God Save the Queen\". Many foreign dignitaries also attended, including Prime Minister Indira Gandhi of India, President Shehu Shagari of Nigeria, President Kenneth Kaunda of Zambia, President Seretse Khama of Botswana, and Prime Minister Malcolm Fraser of Australia, representing the Commonwealth of Nations. Bob Marley sang 'Zimbabwe', a song he wrote, at the government's invitation in a concert at the country's independence festivities.\nPresident Shagari pledged $15 million at the celebration to train Zimbabweans in Zimbabwe and expatriates in Nigeria. Mugabe's government used part of the money to buy newspaper companies owned by South Africans, increasing the government's control over the media. The rest went to training students in Nigerian universities, government workers in the Administrative Staff College of Nigeria in Badagry, and soldiers in the Nigerian Defence Academy in Kaduna. Later that year Mugabe commissioned a report by the BBC on press freedom in Zimbabwe. The BBC issued its report on 26 June, recommending the privatisation of the Zimbabwe Broadcasting Corporation and its independence from political interests.\nMugabe's government changed the capital's name from Salisbury to Harare on 18 April 1982 in celebration of the second anniversary of independence. The government renamed the main street in the capital, Jameson Avenue, in honour of Samora Machel, President of Mozambique.\nIn 1992, a World Bank study indicated that more than 500 health centres had been built since 1980. The percentage of children vaccinated increased from 25% in 1980 to 67% in 1988 and life expectancy increased from 55 to 59 years. Enrollment increased by 232 per cent one year after primary education was made free and secondary school enrolment increased by 33 per cent in two years. These social policies lead to an increase in the debt ratio. Several laws were passed in the 1980s in an attempt to reduce wage gaps. However, the gaps remained considerable. In 1988, the law gave women, at least in theory, the same rights as men. Previously, they could only take a few personal initiatives without the consent of their father or husband.\nThe new Constitution provided for an executive President as Head of State with a Prime Minister as Head of Government. Reverend Canaan Banana served as the first President. In government amended the Constitution in 1987 to provide for an Executive President and abolished the office of Prime Minister. The constitutional changes came into effect on 1 January 1988 with Robert Mugabe as president. The bicameral Parliament of Zimbabwe had a directly elected House of Assembly and an indirectly elected Senate, partly made up of tribal chiefs. The Constitution established two separate voters rolls, one for the black majority, who had 80% of the seats in Parliament, and the other for whites and other ethnic minorities, such as Coloureds, people of mixed race, and Asians, who held 20%. The government amended the Constitution in 1986, eliminating the voter rolls and replacing the white seats with seats filled by nominated members. Many white MPs joined ZANU which then reappointed them. In 1990 the government abolished the Senate and increased the House of Assembly's membership to include members nominated by the President.\nPrime Minister Mugabe kept Peter Walls, the head of the army, in his government and put him in charge of integrating the Zimbabwe People's Revolutionary Army (ZIPRA), Zimbabwe African National Liberation Army (ZANLA), and the Rhodesian Army. While Western media outlets praised Mugabe's efforts at reconciliation with the white minority, tension soon developed. On 17 March 1980, after several unsuccessful assassination attempts Mugabe asked Walls, \"Why are your men trying to kill me?\" Walls replied, \"If they were my men you would be dead.\" BBC News interviewed Walls on 11 August 1980. He told the BBC that he had asked British Prime Minister Margaret Thatcher to annul the 1980 election prior to the official announcement of the result on the grounds that Mugabe used intimidation to win the election. Walls said Thatcher had not replied to his request. On 12 August British government officials denied that they had not responded, saying Antony Duff, Deputy Governor of Salisbury, told Walls on 3 March that Thatcher would not annul the election.\nMinister of Information Nathan Shamuyarira said the government would not be \"held ransom by racial misfits\" and told \"all those Europeans who do not accept the new order to pack their bags.\" He also said the government continued to consider taking \"legal or administrative action\" against Walls. Mugabe, returning from a visit with United States President Jimmy Carter in New York City, said, \"One thing is quite clear\u2014we are not going to have disloyal characters in our society.\" Walls returned to Zimbabwe after the interview, telling Peter Hawthorne of \"Time\" magazine, \"To stay away at this time would have appeared like an admission of guilt.\" Mugabe drafted legislation that would exile Walls from Zimbabwe for life and Walls moved to South Africa.\nEthnic divisions soon came back to the forefront of national politics. Tension between ZAPU and ZANU erupted with guerrilla activity starting again in Matabeleland in south-western Zimbabwe. Nkomo (ZAPU) left for exile in Britain and did not return until Mugabe guaranteed his safety. In 1982 government security officials discovered large caches of arms and ammunition on properties owned by ZAPU, accusing Nkomo and his followers of plotting to overthrow the government. Mugabe fired Nkomo and his closest aides from the cabinet. Seven MPs, members of the Rhodesian Front, left Smith's party to sit as \"independents\" on 4 March 1982, signifying their dissatisfaction with his policies. As a result of what they saw as persecution of Nkomo and his party, PF-ZAPU supporters, army deserters began a campaign of dissidence against the government. Centring primarily in Matabeleland, home of the Ndebeles who were at the time PF-ZAPU's main followers, this dissidence continued through 1987. It involved attacks on government personnel and installations, armed banditry aimed at disrupting security and economic life in the rural areas, and harassment of ZANU-PF members.\nBecause of the unsettled security situation immediately after independence and democratic sentiments, the government kept in force a \"state of emergency\". This gave the government widespread powers under the \"Law and Order Maintenance Act,\" including the right to detain persons without charge which it used quite widely. In 1983 to 1984 the government declared a curfew in areas of Matabeleland and sent in the army in an attempt to suppress members of the Ndebele tribe. The pacification campaign, known as the Gukuruhundi, or strong wind, resulted in at least 20,000 civilian deaths perpetrated by an elite, North Korean-trained brigade, known in Zimbabwe as the Gukurahundi.\nZANU-PF increased its majority in the 1985 elections, winning 67 of the 100 seats. The majority gave Mugabe the opportunity to start making changes to the constitution, including those with regard to land restoration. Fighting did not cease until Mugabe and Nkomo reached an agreement in December 1987 whereby ZAPU became part of ZANU-PF and the government changed the constitution to make Mugabe the country's first executive president and Nkomo one of two vice-presidents.\n1990s.\nElections in March 1990 resulted in another overwhelming victory for Mugabe and his party, which won 117 of the 120 election seats. Election observers estimated voter turnout at only 54% and found the campaign neither free nor fair, though balloting met international standards. Unsatisfied with a \"de facto\" one-party state, Mugabe called on the ZANU-PF Central Committee to support the creation of a \"de jure\" one-party state in September 1990 and lost. The government began further amending the constitution. The judiciary and human rights advocates fiercely criticised the first amendments enacted in April 1991 because they restored corporal and capital punishment and denied recourse to the courts in cases of compulsory purchase of land by the government. The general health of the civilian population also began to significantly flounder and by 1997 25% of the population of Zimbabwe had been infected by HIV, the AIDS virus.\nDuring the 1990s students, trade unionists, and workers often demonstrated to express their discontent with the government. Students protested in 1990 against proposals for an increase in government control of universities and again in 1991 and 1992 when they clashed with police. Trade unionists and workers also criticised the government during this time. In 1992 police prevented trade unionists from holding anti-government demonstrations. In 1994 widespread industrial unrest weakened the economy. In 1996 civil servants, nurses, and junior doctors went on strike over salary issues.\nOn 9 December 1997 a national strike paralysed the country. Mugabe was panicked by demonstrations by ZANLA ex-combatants, war veterans, who had been the heart of incursions 20 years earlier in the Bush War. He agreed to pay them large gratuities and pensions, which proved to be a wholly unproductive and unbudgeted financial commitment. The discontent with the government spawned draconian government crackdowns which in turn started to destroy both the fabric of the state and of society. This in turn brought with it further discontent within the population. Thus a vicious downward spiral commenced.\nAlthough many whites had left Zimbabwe after independence, mainly for neighbouring South Africa, those who remained continued to wield disproportionate control of some sectors of the economy, especially agriculture. In the late-1990s whites accounted for less than 1% of the population but owned 70% of arable land. Mugabe raised this issue of land ownership by white farmers. In a calculated move, he began forcible land redistribution, which brought the government into headlong conflict with the International Monetary Fund. Amid a severe drought in the region, the police and military were instructed not to stop the invasion of white-owned farms by the so-called 'war veterans' and youth militia. This led to a mass migration of White Zimbabweans out of Zimbabwe. At present almost no arable land is in the possession of white farmers.\nThe economy during the 1980s and 1990s.\nThe economy was run along corporatist lines with strict governmental controls on all aspects of the economy. Controls were placed on wages, prices and massive increases in government spending resulting in significant budget deficits. This experiment met with very mixed results and Zimbabwe fell further behind the first world and unemployment. Some market reforms in the 1990s were attempted. A 40 per cent devaluation of the Zimbabwean dollar was allowed to occur and price and wage controls were removed. These policies also failed at that time. Growth, employment, wages, and social service spending contracted sharply, inflation did not improve, the deficit remained well above target, and many industrial firms, notably in textiles and footwear, closed in response to increased competition and high real interest rates. The incidence of poverty in the country increased during this time.\n1999 to 2000.\nHowever, Zimbabwe began experiencing a period of considerable political and economic upheaval in 1999. Opposition to President Mugabe and the ZANU-PF government grew considerably after the mid-1990s in part due to worsening economic and human rights conditions brought about by the seizure of farmland owned by white farmers and economic sanctions imposed by Western countries in response. The Movement for Democratic Change (MDC) was established in September 1999 as an opposition party founded by trade unionist Morgan Tsvangirai.\nThe MDC's first opportunity to test opposition to the Mugabe government came in February 2000, when a referendum was held on a draft constitution proposed by the government. Among its elements, the new constitution would have permitted President Mugabe to seek two additional terms in office, granted government officials immunity from prosecution, and authorised government seizure of white-owned land. The referendum was handily defeated. Shortly thereafter, the government, through a loosely organised group of war veterans, some of the so-called war veterans judging from their age were not war veterans as they were too young to have fought in the chimurenga, sanctioned an aggressive land redistribution program often characterised by forced expulsion of white farmers and violence against both farmers and farm employees.\nParliamentary elections held in June 2000 were marred by localised violence, electoral irregularities, and government intimidation of opposition supporters. Nonetheless, the MDC succeeded in capturing 57 of 120 seats in the National Assembly.\n2002.\nPresidential elections were held in March 2002. In the months leading up to the poll, ZANU-PF, with the support of the army, security services, and especially the so-called 'war veterans', \u2013 very few of whom actually fought in the Second Chimurenga against the Smith regime in the 1970s \u2013 set about wholesale intimidation and suppression of the MDC-led opposition. Despite strong international criticism, these measures, together with organised subversion of the electoral process, ensured a Mugabe victory . The government's behaviour drew strong criticism from the EU and the US, which imposed limited sanctions against the leading members of the Mugabe regime. Since the 2002 election, Zimbabwe has suffered further economic difficulty and growing political chaos.\n2003\u20132005.\nDivisions within the opposition MDC had begun to fester early in the decade, after Morgan Tsvangirai (the president of the MDC) was lured into a government sting operation that videotaped him talking of Mr. Mugabe's removal from power. He was subsequently arrested and put on trial on treason charges. This crippled his control of party affairs and raised questions about his competence. It also catalysed a major split within the party. In 2004 he was acquitted, but not until after suffering serious abuse and mistreatment in prison. The opposing faction was led by Welshman Ncube who was the general secretary of the party. In mid-2004, vigilantes loyal to Mr. Tsvangirai began attacking members who were mostly loyal to Ncube, climaxing in a September raid on the party's Harare headquarters in which the security director was nearly thrown to his death.\nAn internal party inquiry later established that aides to Tsvangirai had tolerated, if not endorsed, the violence. Divisive as the violence was, it was a debate over the rule of law that set off the party's final break-up in November 2005. These division severely weakened the opposition. In addition the government employed its own operatives to both spy on each side and to undermine each side via acts of espionage. Zimbabwean parliamentary election, 2005 were held in March 2005 in which ZANU-PF won a two-thirds majority, were again criticised by international observers as being flawed. Mugabe's political operatives were thus able to weaken the opposition internally and the security apparatus of the state was able to destabilise it externally by using violence in anti-Mugabe strongholds to prevent citizens from voting. Some voters were 'turned away' from polling station despite having proper identification, further guaranteeing that the government could control the results. Additionally Mugabe had started to appoint judges sympathetic to the government, making any judicial appeal futile. Mugabe was also able to appoint 30 of the members of parliament.\nAs Senate elections approached further opposition splits occurred. Ncube's supporters argued that the M.D.C. should field a slate of candidates; Tsvangirai's argued for a boycott. When party leaders voted on the issue, Ncube's side narrowly won, but Mr. Tsvangirai declared that as president of the party he was not bound by the majority's decision. Again the opposition was weakened. As a result, the elections for a new Senate in November 2005 were largely boycotted by the opposition. Mugabe's party won 24 of the 31 constituencies where elections were held amid low voter turnout. Again, evidence surfaced of voter intimidation and fraud. \nIn May 2005 the government began Operation Murambatsvina. It was officially billed to rid urban areas of illegal structures, illegal business enterprises, and criminal activities. In practice its purpose was to punish political opponents. The UN estimates 700,000 people have been left without jobs or homes as a result. Families and traders, especially at the beginning of the operation, were often given no notice before police destroyed their homes and businesses. Others were able to salvage some possessions and building materials but often had nowhere to go, despite the government's statement that people should be returning to their rural homes. Thousands of families were left unprotected in the open in the middle of Zimbabwe's winter., . The government interfered with non-governmental organisation (NGO) efforts to provide emergency assistance to the displaced in many instances. Some families were removed to transit camps, where they had no shelter or cooking facilities and minimal food, supplies, and sanitary facilities. The operation continued into July 2005, when the government began a program to provide housing for the newly displaced.\nHuman Rights Watch said the evictions had disrupted treatment for people with HIV/AIDS in a country where 3,000 die from the disease each week and about 1.3 million children have been orphaned. The operation was \"the latest manifestation of a massive human rights problem that has been going on for years\", said Amnesty International. As of September 2006, housing construction fell far short of demand, and there were reports that beneficiaries were mostly civil servants and ruling party loyalists, not those displaced. The government campaign of forced evictions continued in 2006, albeit on a lesser scale.\nIn September 2005 Mugabe signed constitutional amendments that reinstituted a national senate (abolished in 1987) and that nationalised all land. This converted all ownership rights into leases. The amendments also ended the right of landowners to challenge government expropriation of land in the courts and marked the end of any hope of returning any land that had been hitherto grabbed by armed land invasions. Elections for the senate in November resulted in a victory for the government. The MDC split over whether to field candidates and partially boycotted the vote. In addition to low turnout there was widespread government intimidation. The split in the MDC hardened into factions, each of which claimed control of the party. The early months of 2006 were marked by food shortages and mass hunger. The sheer extremity of the siltation was revealed by the fact that in the courts, state witnesses said they were too weak from hunger to testify.\n2006 to 2007.\nIn August 2006 runaway inflation forced the government to replace its existing currency with a revalued one. In December 2006, ZANU-PF proposed the \"harmonisation\" of the parliamentary and presidential election schedules in 2010; the move was seen by the opposition as an excuse to extend Mugabe's term as president until 2010.\nMorgan Tsvangirai was badly beaten on 12 March 2007 after being arrested and held at Machipisa Police Station in the Highfield suburb of Harare. The event garnered an international outcry and was considered particularly brutal and extreme, even considering the reputation of Mugabe's government. Kolawole Olaniyan, Director of Amnesty International's Africa Programme said \"We are very concerned by reports of continuing brutal attacks on opposition activists in Zimbabwe and call on the government to stop all acts of violence and intimidation against opposition activists\".\nThe economy has shrunk by 50% from 2000 to 2007. In September 2007 the inflation rate was put at almost 8,000%, the world's highest. There are frequent power and water outages. Harare's drinking water became unreliable in 2006 and as a consequence dysentery and cholera swept the city in December 2006 and January 2007. Unemployment in formal jobs is running at a record 80%. There was widespread hunger, manipulated by the government so that opposition strongholds suffer the most. Availability of bread was severely constrained after a poor wheat harvest and the closure of all bakeries.\nThe country, which used to be one of Africa's richest, became one of its poorest. Many observers now view the country as a 'failed state'. The settlement of the Second Congo War brought back Zimbabwe's substantial military commitment, although some troops remain to secure the mining assets under their control. The government lacks the resources or machinery to deal with the ravages of the HIV/AIDS pandemic, which affects 25% of the population. With all this and the forced and violent removal of white farmers in a brutal land redistribution program, Mugabe has earned himself widespread scorn from the international arena.\nThe regime has managed to cling to power by creating wealthy enclaves for government ministers, and senior party members. For example, Borrowdale Brook, a suburb of Harare is an oasis of wealth and privilege. It features mansions, manicured lawns, full shops with fully stocked shelves containing an abundance of fruit and vegetables, big cars and a golf club give is the home to President Mugabe's out-of-town retreat.\nZimbabwe's bakeries shut down in October 2007 and supermarkets warned that they would have no bread for the foreseeable future due to collapse in wheat production after the seizure of white-owned farms. The ministry of agriculture has also blamed power shortages for the wheat shortfall, saying that electricity cuts have affected irrigation and halved crop yields per acre. The power shortages are because Zimbabwe relies on Mozambique for some of its electricity and that due to an unpaid bill of $35 million Mozambique had reduced the amount of electrical power it supplies. On 4 December 2007, The United States imposed travel sanctions against 38 people with ties to President Mugabe because they \"played a central role in the regime's escalated human rights abuses.\"\nOn 8 December 2007, Mugabe attended a meeting of EU and African leaders in Lisbon, prompting UK Prime Minister Gordon Brown to decline to attend. While German chancellor Angela Merkel criticised Mugabe with her public comments, the leaders of other African countries offered him statements of support.\nDeterioration of the educational system.\nThe educational system in Zimbabwe, which was once regarded as among the best in Africa, went into crisis in 2007 because of the country's economic meltdown. One foreign reporter witnessed hundreds of children at Hatcliffe Extension Primary School in Epworth, west of Harare, writing in the dust on the floor because they had no exercise books or pencils. The high school exam system unravelled in 2007. Examiners refused to mark examination papers when they were offered just Z$79 a paper, enough to buy three small candies. Corruption has crept into the system and may explain why in January 2007 thousands of pupils received no marks for subjects they had entered, while others were deemed \"excellent\" in subjects they had not sat. However, as of late the education system has recovered and is still considered the best in Southern Africa.\n2008.\n2008 elections.\nZimbabwe held a presidential election along with a 2008 parliamentary election of 29 March. The three major candidates were incumbent President Robert Mugabe of the Zimbabwe African National Union \u2013 Patriotic Front (ZANU-PF), Morgan Tsvangirai of the Movement for Democratic Change \u2013 Tsvangirai (MDC-T), and Simba Makoni, an independent. As no candidate received an outright majority in the first round, a second round was held on 27 June 2008 between Tsvangirai (with 47.9% of the first round vote) and Mugabe (43.2%). Tsvangirai withdrew from the second round a week before it was scheduled to take place, citing violence against his party's supporters. The second round went ahead, despite widespread criticism, and led to victory for Mugabe.\nBecause of Zimbabwe's dire economic situation the election was expected to provide President Mugabe with his toughest electoral challenge to date. Mugabe's opponents were critical of the handling of the electoral process, and the government was accused of planning to rig the election; Human Rights Watch said that the election was likely to be \"deeply flawed\". After the first round, but before the counting was completed, Jose Marcos Barrica, the head of the Southern African Development Community observer mission, described the election as \"a peaceful and credible expression of the will of the people of Zimbabwe.\"\nNo official results were announced for more than a month after the first round. The failure to release results was strongly criticised by the MDC, which unsuccessfully sought an order from the High Court to force their release. An independent projection placed Tsvangirai in the lead, but without the majority needed to avoid a second round. The MDC declared that Tsvangirai won a narrow majority in the first round and initially refused to participate in any second round. ZANU-PF has said that Mugabe will participate in a second round; the party alleged that some electoral officials, in connection with the MDC, fraudulently reduced Mugabe's score, and as a result a recount was conducted.\nAfter the recount and the verification of the results, the Zimbabwe Electoral Commission (ZEC) announced on 2 May that Tsvangirai won 47.9% and Mugabe won 43.2%, thereby necessitating a run-off, which was to be held on 27 June 2008. Despite Tsvangirai's continuing claims to have won a first round majority, he refused to participate in the second round. The period following the first round was marked by serious political violence caused by ZANU-PF. ZANU-PF blamed the MDC supporters for perpetrating this violence; Western governments and prominent Western organisations have blamed ZANU-PF for the violence which seems very likely to be true. On 22 June 2008, Tsvangirai announced that he was withdrawing from the run-off, describing it as a \"violent sham\" and saying that his supporters risked being killed if they voted for him. The second round nevertheless went ahead as planned with Mugabe as the only actively participating candidate, although Tsvangirai's name remained on the ballot. Mugabe won the second round by an overwhelming margin and was sworn in for another term as president on 29 June.\nThe international reaction to the second round have varied. The United States and states of the European Union have called for increased sanctions. On 11 July, the United Nations Security Council voted to impose sanctions on the Zimbabwe; Russia and China vetoed. The African Union has called for a \"government of national unity.\"\nPreliminary talks to set up conditions for official negotiations began between leading negotiators from both parties on 10 July, and on 22 July, the three party leaders met for the first time in Harare to express their support for a negotiated settlement of disputes arising out of the presidential and parliamentary elections. Negotiations between the parties officially began on 25 July and are currently proceeding with very few details released from the negotiation teams in Pretoria, as coverage by the media is barred from the premises where the negotiations are taking place. The talks were mediated by South African President Thabo Mbeki.\nOn 15 September 2008, the leaders of the 14-member Southern African Development Community witnessed the signing of the power-sharing agreement, brokered by South African leader Thabo Mbeki. With symbolic handshake and warm smiles at the Rainbow Towers hotel, in Harare, Mugabe and Tsvangirai signed the deal to end the violent political crisis. As provided, Robert Mugabe will remain president, Morgan Tsvangirai will become prime minister, ZANU-PF and the MDC will share control of the police, Mugabe's Zanu (PF) will command the Army, and Arthur Mutambara becomes deputy prime minister.\nMarange diamond fields massacre.\nIn November 2008 the Air Force of Zimbabwe was sent, after some police officers began refusing orders to shoot the illegal miners at Marange diamond fields. Up to 150 of the estimated 30,000 illegal miners were shot from helicopter gunships. In 2008 some Zimbabwean lawyers and opposition politicians from Mutare claimed that Shiri was the prime mover behind the military assaults on illegal diggers in the diamond mines in the east of Zimbabwe. Estimates of the death toll by mid-December range from 83 reported by the Mutare City Council, based on a request for burial ground, to 140 estimated by the (then) opposition Movement for Democratic Change - Tsvangirai party.\n2009 to present.\n2009\u20132025.\nIn January 2009, Morgan Tsvangirai announced that he would do as the leaders across Africa had insisted and join a coalition government as prime minister with his nemesis, President Robert Mugabe . On 11 February 2009 Tsvangirai was sworn in as the Prime Minister of Zimbabwe. By 2009 inflation had peaked at 500 billion % per year under the Mugabe government and the Zimbabwe currency was worthless. The opposition shared power with the Mugabe regime between 2009 and 2013, Zimbabwe switched to using the US dollar as currency and the economy improved reaching a growth rate of 10% per year.\nIn 2013 the Mugabe government won an election which The Economist described as \"rigged,\" doubled the size of the civil service and embarked on \"...misrule and dazzling corruption.\" However, the United Nations, African Union and SADC endorsed the elections as free and fair.\nBy 2016 the economy had collapsed, nationwide protests took place throughout the country and the finance minister admitted \"Right now we literally have nothing.\"\nThere was the introduction of bond notes to literally fight the biting cash crisis and liquidity crunch. Special Historical bonds was created to help the economy but never seen the light and was kept by the then President Robert Mugabe. Cash became scarce on the market in the year 2017.\nOn Wednesday 15 November 2017 the military placed President Mugabe under house arrest and removed him from power. The military stated that the president was safe. The military placed tanks around government buildings in Harare and blocked the main road to the airport. Public opinion in the capital favored the dictators removal although they were uncertain about his replacement with another dictatorship. The Times reported that Emmerson Mnangagwa helped to orchestrate the coup. He had recently been sacked by Mr Mugabe so that the path could be smoothed for Grace Mugabe to replace her husband. A Zimbabwean army officer, Major General Sibusiso Moyo, went on television to say the military was targeting \"criminals\" around President Mugabe but not actively removing the president from power. However the head of the African Union described it as such.\nUgandan writer Charles Onyango-Obbo stated on Twitter \"If it looks like a coup, walks like a coup and quacks like a coup, then it's a coup\". Naunihal Singh, an assistant professor at the U.S. Naval War College and author of a book on military coups, described the situation in Zimbabwe as a coup. He tweeted that \"'The President is safe' is a classic coup catch-phrase\" of such an event.\nRobert Mugabe resigned 21 November 2017. Second Vice-president Phelekezela Mphoko became the Acting President. Former Vice-president and new ZANU-PF -leader, Emmerson Mnangagwa, was sworn in as president on 24 November 2017.\n2018\u20132019.\nGeneral elections were held on 30 July 2018 to elect the president and members of both houses of parliament. Ruling party ZANU-PF won the majority of seats in parliament, incumbent President Emmerson Mnangagwa was declared the winner after receiving 50.8% of votes. The opposition accused the government of rigging the vote. In subsequent riots by MDC supporters, the army opened fire and killed three people, while three others died of their injuries the following day.\nIn January 2019 following a 130% increase in the price of fuel thousands of Zimbabweans protested and the government responded with a coordinated crackdown that resulted in hundreds of arrests and multiple deaths.\nIn September 2019, former president Robert Mugabe died in Singapore, aged 95.\nIn September 2023, Zimbabwe signed control over almost 20% of the country's land to the carbon offset company Blue Carbon.\nEconomic statistics 2021\nGDP growth in Zimbabwe is projected to reach 3.9% in 2021, a significant improvement after a two-year recession, according to the World Bank Zimbabwe Economic Update.\n2023 Zimbabwean general election.\nIn August 2025, President Emmerson Mnangagwa won a second term in an outcome of the election rejected by the opposition and questioned by observers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14115", "revid": "372290", "url": "https://en.wikipedia.org/wiki?curid=14115", "title": "History of Russia", "text": " \nThe history of Russia begins with the histories of the East Slavs. The traditional start date of specifically Russian history is the establishment of the Rus' state in the north in the year 862, ruled by Varangians. In 882, Prince Oleg of Novgorod seized Kiev, uniting the northern and southern lands of the Eastern Slavs under one authority, moving the governance center to Kiev by the end of the 10th century, and maintaining northern and southern parts with significant autonomy from each other. The state adopted Christianity from the Byzantine Empire in 988, beginning the synthesis of Byzantine, Slavic and Scandinavian cultures that defined Russian culture for the next millennium. Kievan Rus' ultimately disintegrated as a state due to the Mongol invasions in 1237\u20131240. After the 13th century, Moscow emerged as a significant political and cultural force, driving the unification of Russian territories. By the end of the 15th century, many of the petty principalities around Moscow had been united with the Grand Duchy of Moscow, which took full control of its own sovereignty under Ivan the Great.\nIvan the Terrible transformed the Grand Duchy into the Tsardom of Russia in 1547. However, the death of Ivan's son Feodor I without issue in 1598 created a succession crisis and led Russia into a period of chaos and civil war known as the Time of Troubles, ending with the coronation of Michael Romanov as the first Tsar of the Romanov dynasty in 1613. During the rest of the seventeenth century, Russia completed the exploration and conquest of Siberia, claiming lands as far as the Pacific Ocean by the end of the century. Domestically, Russia faced numerous uprisings of the various ethnic groups under their control, as exemplified by the Cossack leader Stenka Razin, who led a revolt in 1670\u20131671. In 1721, in the wake of the Great Northern War, Tsar Peter the Great renamed the state as the Russian Empire; he is also noted for establishing St. Petersburg as the new capital of his Empire, and for his introducing Western European culture to Russia. In 1762, Russia came under the control of Catherine the Great, who continued the westernizing policies of Peter the Great, and ushered in the era of the Russian Enlightenment. Catherine's grandson, Alexander I, repulsed an invasion by the French Emperor Napoleon, leading Russia into the status of one of the great powers.\nPeasant revolts intensified during the nineteenth century, culminating with Alexander II abolishing Russian serfdom in 1861. In the following decades, reform efforts such as the Stolypin reforms of 1906\u20131914, the constitution of 1906, and the State Duma (1906\u20131917) attempted to open and liberalize the economy and political system, but the emperors refused to relinquish autocratic rule and resisted sharing their power. A combination of economic breakdown, mismanagement over Russia's involvement in World War I, and discontent with the autocratic system of government triggered the Russian Revolution in 1917. The end of the monarchy initially brought into office a coalition of liberals and moderate socialists, but their failed policies led to the October Revolution. In 1922, Soviet Russia, along with the Ukrainian SSR, Byelorussian SSR, and Transcaucasian SFSR signed the Treaty on the Creation of the USSR, officially merging all four republics to form the Soviet Union as a single state. Between 1922 and 1991 the history of Russia essentially became the history of the Soviet Union. During this period, the Soviet Union was one of the victors in World War II after recovering from a surprise invasion in 1941 by Nazi Germany and its collaborators, which had previously signed a non-aggression pact with the Soviet Union. The Soviet Union's network of satellite states in Eastern Europe, which were brought into its sphere of influence in the closing stages of World War II, helped the country become a superpower competing with fellow superpower the United States and other Western countries in the Cold War.\nBy the mid-1980s, with the weaknesses of Soviet economic and political structures becoming acute, Mikhail Gorbachev embarked on major reforms, which eventually led to the weakening of the communist party and dissolution of the Soviet Union, leaving Russia again on its own and marking the start of the history of post-Soviet Russia. The Russian Soviet Federative Socialist Republic renamed itself as the Russian Federation and became the primary successor state to the Soviet Union. Russia retained its nuclear arsenal but lost its superpower status. Scrapping the central planning and state-ownership of property of the Soviet era in the 1990s, new leaders, led by President Vladimir Putin, took political and economic power after 2000 and engaged in an assertive foreign policy. Coupled with economic growth, Russia has since regained significant global status as a world power. Russia's 2014 annexation of the Crimean Peninsula led to economic sanctions imposed by the United States and the European Union. Russia's 2022 invasion of Ukraine led to significantly expanded sanctions. Under Putin's leadership, corruption in Russia is rated as the worst in Europe, and Russia's human rights situation has been increasingly criticized by international observers.\nPrehistory.\nThe first human settlement on the territory of Russia dates back to the Oldowan period in the early Lower Paleolithic. About 2 million years ago, representatives of \"Homo erectus\" migrated from Western Asia to the North Caucasus (archaeological site of Kermek on the Taman Peninsula). At Bogatyri/Sinyaya balka, in a skull of \"Elasmotherium caucasicum\", which lived 1.5\u20131.2 million years ago, a stone tool was found. 1.5-million-year-old Oldowan flint tools have been discovered in the Dagestan Akusha region of the north Caucasus, demonstrating the presence of early humans in the territory of present-day Russia.\nFossils of Denisovans in Russia date to about 110,000 years ago. DNA from a bone fragment found in Denisova Cave, belonging to a female who died about 90,000 years ago, shows that she was a hybrid of a Neanderthal mother and a Denisovan father. Russia was also home to some of the last surviving Neanderthals - the partial skeleton of a Neanderthal infant in Mezmaiskaya cave in Adygea showed a carbon-dated age of only 45,000 years. In 2008, Russian archaeologists from the Institute of Archaeology and Ethnology of Novosibirsk, working at the site of Denisova Cave in the Altai Mountains of Siberia, uncovered a 40,000-year-old small bone fragment from the fifth finger of a juvenile hominin, which DNA analysis revealed to be a previously unknown species of human, which was named the Denisova hominin.\nThe first trace of \"Homo sapiens\" on the large expanse of Russian territory dates back to 45,000 years, in central Siberia (Ust'-Ishim man). The discovery of some of the earliest evidence for the presence of anatomically modern humans found anywhere in Europe was reported in 2007 from the Kostenki archaeological site near the Don River in Russia (dated to at least 40,000 years ago) and at Sungir (34,600 years ago). Humans reached Arctic Russia (Mamontovaya Kurya) by 40,000 years ago.\nDuring the prehistoric eras the vast steppes of Southern Russia were home to tribes of nomadic pastoralists. (In classical antiquity, the Pontic Steppe was known as \"Scythia\".) Remnants of these long-gone steppe cultures were discovered in the course of the 20th century in such places as Ipatovo, Sintashta, Arkaim, and Pazyryk.\nAntiquity.\nIn the later part of the 8th century BC, Greek merchants brought classical civilization to the trade emporiums in Tanais and Phanagoria. Gelonus was described by Herodotus as a huge (Europe's biggest) earth- and wood-fortified grad inhabited around 500 BC by Heloni and Budini. In 513 BC, the king of the Achaemenid Empire, Darius I, launched a military campaign around the Black Sea into Scythia, modern-day Ukraine, eventually reaching the Tanais river (now known as the Don).\nGreeks, mostly from the city-state of Miletus, would colonize large parts of modern-day Crimea and the Sea of Azov during the seventh and sixth centuries BC, eventually unifying into the Bosporan Kingdom by 480 BC, and would be incorporated into the large Kingdom of Pontus in 107 BC. The Kingdom would eventually be conquered by the Roman Republic, and the Bosporan Kingdom would become a client state of the Roman Empire. At about the 2nd century AD Goths migrated to the Black Sea, and in the 3rd and 4th centuries AD, a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom was also overwhelmed by successive waves of nomadic invasions, led by warlike tribes which would often move on to Europe, as was the case with the Huns and Turkish Avars.\nIn the second millennium BC, the territories between the Kama and the Irtysh Rivers were the home of a Proto-Uralic-speaking population that had contacts with Proto-Indo-European speakers from the south. The woodland population is the ancestor of the modern Ugrian inhabitants of Trans-Uralia. Other researchers say that the Khanty people originated in the south Ural steppe and moved northwards into their current location about 500 AD.\nA Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas through to the 8th century. Noted for their laws, tolerance, and cosmopolitanism, the Khazars were the main commercial link between the Baltic and the Muslim Abbasid empire centered in Baghdad. They were important allies of the Eastern Roman Empire, and waged a series of successful wars against the Arab Caliphates.\nEarly history.\nEarly Slavs.\nSome of the ancestors of the modern Russians were the Slavic tribes, whose original home is thought by some scholars to have been the Pripet Marshes. The Early East Slavs gradually settled Western Russia in two waves: one moving from Kiev (present-day Ukraine) towards present-day Suzdal and Murom and another from Polotsk (present-day Belarus) towards Novgorod and Rostov.\nFrom the 7th century onwards, East Slavs constituted the bulk of the population in Western Russia and slowly conquered and assimilated the native Finnic and Baltic tribes, such as the Merya, the Muromians, and the Meshchera.\nThere existed a political hierarchy north of the middle Dnieper as early as 825-850, and perhaps even earlier. Indeed, according to the \"Annales Bertiniani\", some Rh\u014ds (as they were known by the Byzantines) accompanied a Byzantine embassy to the court of the Frankish king, Louis the Pious, asking assistance in returning to their homeland. Their ruler was ascribed a title akin to the Khazars' (Chaganus; Khagan), but they yet claimed belonging to the 'people of the Swedes'. The specific facts related to this political hierarchy, however, are subject to numerous contradicting historical interpretations.\nKievan Rus' (862\u20131240).\nScandinavian Norsemen, known as Vikings in Western Europe and Varangians in the East, combined piracy and trade throughout Northern Europe. In the mid-9th century, they began to venture along the waterways from the eastern Baltic to the Black and Caspian Seas. According to the legendary Calling of the Varangians, recorded in several Rus' chronicles such as the \"Novgorod First Chronicle\" and \"Primary Chronicle\", the Varangians Rurik, Sineus and Truvor were invited in the 860s to restore order in three towns \u2013 either Novgorod (most texts) or Staraya Ladoga (\"Hypatian Codex\"); Beloozero; and Izborsk (most texts) or \"Slovensk\" (\"Pskov Third Chronicle\"), respectively. Their successors allegedly moved south and extended their authority to Kiev, which had been previously dominated by the Khazars.\nThus, the first East Slavic state, Rus', emerged in the 9th century along the Dnieper River valley. A coordinated group of princely states with a common interest in maintaining trade along the river routes, Kievan Rus' controlled the trade route for furs, wax, and slaves between Scandinavia and the Byzantine Empire along the Volkhov and Dnieper Rivers.\nBy the end of the 10th century, the minority Norse military aristocracy had merged with the native Slavic population, which also absorbed Greek Christian influences in the course of the multiple campaigns to loot Tsargrad, or Constantinople. One such campaign claimed the life of the foremost Slavic druzhina leader, Svyatoslav I, who was renowned for having crushed the power of the Khazars on the Volga.\nKievan Rus' is important for its introduction of a Slavic variant of the Eastern Orthodox religion, dramatically deepening a synthesis of Byzantine and Slavic cultures that defined Russian culture for the next thousand years. The region adopted Christianity in 988 by the official act of public baptism of Kiev inhabitants by Prince Vladimir I. Some years later the first code of laws, Russkaya Pravda, was introduced by Yaroslav the Wise. From the onset, the Kievan princes followed the Byzantine example and kept the Church dependent on them.\nBy the 11th century, particularly during the reign of Yaroslav the Wise, Kievan Rus' displayed an economy and achievements in architecture and literature superior to those that then existed in the western part of the continent. Compared with the languages of European Christendom, the Russian language was little influenced by the Greek and Latin of early Christian writings. This was because Church Slavonic was used directly in liturgy instead.\nA nomadic Turkic people, the Kipchaks (also known as the Cumans), replaced the earlier Pechenegs as the dominant force in the south steppe regions neighbouring to Rus' at the end of the 11th century and founded a nomadic state in the steppes along the Black Sea (Desht-e-Kipchak). Repelling their regular attacks, especially in Kiev, was a heavy burden for the southern areas of Rus'. The nomadic incursions caused a massive influx of Slavs to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.\nKievan Rus' ultimately disintegrated as a state because of in-fighting between members of the princely family that ruled it collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod in the north, and Halych-Volhynia in the south-west. Conquest by the Mongol Golden Horde in the 13th century was the final blow. Kiev was destroyed. Halych-Volhynia would eventually be absorbed into the Polish\u2013Lithuanian Commonwealth, while the Mongol-dominated Vladimir-Suzdal and independent Novgorod Republic, two regions on the periphery of Kiev, would establish the basis for the modern Russian nation.\nMongol invasion and vassalage (1223\u20131480).\nThe invading Mongols accelerated the fragmentation of the Rus'. In 1223, the disunited southern princes faced a Mongol raiding party at the Kalka River and were soundly defeated. In 1237\u20131238 the Mongols burnt down the city of Vladimir (4 February 1238) and other major cities of northeast Russia, routed the Russians at the Sit' River, and then moved west into Poland and Hungary. By then they had conquered most of the Russian principalities. Only the Novgorod Republic escaped occupation and continued to flourish in the orbit of the Hanseatic League.\nThe impact of the Mongol invasion on the territories of Kievan Rus' was uneven. The advanced city culture was almost completely destroyed. As older centers such as Kiev and Vladimir never recovered from the devastation of the initial attack, the new cities of Moscow, Tver and Nizhny Novgorod began to compete for hegemony in the Mongol-dominated Rus' principalities under the suzerainty of the Golden Horde. Although a coalition of Rus' princes led by Dmitry Donskoy defeated Mongol warlord Mamai at Kulikovo in 1380, forces of the new khan Tokhtamysh and his Rus' allies immediately sacked Moscow in 1382 as punishment for resisting Mongol authority. Mongol domination of the Rus' principalities, along with tax collection by various overlords such as the Crimean Khans, continued into the early 16th century, despite later claims of Muscovite bookmen that the indecisive standoff at the Ugra in 1480 had signified \"the end of the Tatar yoke\" and the \"liberation of Russia\".\nThe Mongols dominated the lower reaches of the Volga and held Russia in sway from their western capital at Sarai, one of the largest cities of the medieval world. The princes had to pay tribute to the Mongols of the Golden Horde, commonly called Tatars; but in return they received charters authorizing them to act as deputies to the khans. In general, the princes were allowed considerable freedom to rule as they wished, while the Russian Orthodox Church even experienced a spiritual revival.\nThe Mongols left their impact on the Russians in such areas as military tactics and transportation. Under Mongol occupation, Muscovy also developed its postal road network, census, fiscal system, and military organization.\nAt the same time, Prince of Novgorod, Alexander Nevsky, managed to repel the offensive of the Northern Crusades against Novgorod from the West. Despite this, becoming the Grand Prince, Alexander declared himself a vassal to the Golden Horde, not having the strength to resist its power.\nGrand Duchy of Moscow (1283\u20131547).\nRise of Moscow.\n Daniil Aleksandrovich, the youngest son of Alexander Nevsky, founded the principality of Moscow (known as Muscovy in English), which first cooperated with and ultimately expelled the Tatars from Russia. Well-situated in the central river system of Russia and surrounded by protective forests and marshes, Moscow was at first only a vassal of Vladimir, but soon it absorbed its parent state.\nA major factor in the ascendancy of Moscow was the cooperation of its rulers with the Mongol overlords, who granted them the title of Grand Prince of Moscow and made them agents for collecting the Tatar tribute from the Russian principalities. The principality's prestige was further enhanced when it became the center of the Russian Orthodox Church. Its head, the Metropolitan, fled from Kiev to Vladimir in 1299 and a few years later established the permanent headquarters of the Church in Moscow under the original title of Kiev Metropolitan.\nBy the middle of the 14th century, the power of the Mongols was declining, and the Grand Princes felt able to openly oppose the Mongol yoke. In 1380, at Battle of Kulikovo on the Don River, the Mongols were defeated, and although this hard-fought victory did not end Tatar rule of Russia, it did bring great fame to the Grand Prince Dmitry Donskoy. Moscow's leadership in Russia was now firmly based and by the middle of the 14th century its territory had greatly expanded through purchase, war, and marriage.\nIvan III, the Great.\nIn the 15th century, the grand princes of Moscow continued to consolidate Russian land to increase their population and wealth. The most successful practitioner of this process was Ivan III, who laid the foundations for a Russian national state. Ivan competed with his powerful northwestern rival, the Grand Duchy of Lithuania, for control over some of the semi-independent Upper Principalities in the upper Dnieper and Oka River basins.\nThrough the defections of some princes, border skirmishes, and a long war with the Novgorod Republic, Ivan III was able to annex Novgorod and Tver. As a result, the Grand Duchy of Moscow tripled in size under his rule. During his conflict with Pskov, a monk named Filofei (Philotheus of Pskov) composed a letter to Ivan III, with the prophecy that the latter's kingdom would be the Third Rome. The Fall of Constantinople and the death of the last Greek Orthodox Christian emperor contributed to this new idea of Moscow as \"New Rome\" and the seat of Orthodox Christianity, as did Ivan's 1472 marriage to Byzantine Princess Sophia Palaiologina.\nUnder Ivan III, the first central government bodies were created in Russia: Prikaz. The Sudebnik was adopted, the first set of laws since the 11th century. The double-headed eagle was adopted as the coat of arms of Russia.\nIvan proclaimed his absolute sovereignty over all Russian princes and nobles. Refusing further tribute to the Tatars, Ivan initiated a series of attacks that opened the way for the complete defeat of the declining Golden Horde, now divided into several Khanates and hordes. Ivan and his successors sought to protect the southern boundaries of their domain against attacks of the Crimean Tatars and other hordes. To achieve this aim, they sponsored the construction of the Great Abatis Belt and granted manors to nobles, who were obliged to serve in the military. The manor system provided a basis for an emerging cavalry-based army.\nIn this way, internal consolidation accompanied outward expansion of the state. By the 16th century, the rulers of Moscow considered the entire Russian territory their collective property. Various semi-independent princes still claimed specific territories, but Ivan III forced the lesser princes to acknowledge the grand prince of Moscow and his descendants as unquestioned rulers with control over military, judicial, and foreign affairs. Gradually, the Russian ruler emerged as a powerful, autocratic ruler, a tsar. The first Russian ruler to officially crown himself \"Tsar\" was Ivan IV.\nIvan III tripled the territory of his state, ended the dominance of the Golden Horde over the Rus', renovated the Moscow Kremlin, and laid the foundations of the Russian state. Biographer Fennell concludes that his reign was \"militarily glorious and economically sound,\" and especially points to his territorial annexations and his centralized control over local rulers. However, Fennell argues that his reign was also \"a period of cultural depression and spiritual barrenness. Freedom was stamped out within the Russian lands. By his bigoted anti-Catholicism Ivan brought down the curtain between Russia and the west. For the sake of territorial aggrandizement he deprived his country of the fruits of Western learning and civilization.\"\nTsardom of Russia (1547\u20131721).\nIvan IV, the Terrible.\nThe development of the Tsar's autocratic powers reached a peak during the reign of Ivan IV (1547\u20131584), known as \"Ivan the Terrible\". He strengthened the position of the monarch to an unprecedented degree, as he ruthlessly subordinated the nobles to his will, exiling or executing many on the slightest provocation. Nevertheless, Ivan is often seen as a farsighted statesman who reformed Russia as he promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor), curbed the influence of the clergy, and introduced local self-management in rural regions. Tsar also created the first regular army in Russia: Streltsy.\nHis long Livonian War (1558\u20131583) for control of the Baltic coast and access to the sea trade ultimately proved a costly failure. Ivan managed to annex the Khanates of Kazan, Astrakhan, and Siberia. These conquests complicated the migration of aggressive nomadic hordes from Asia to Europe via the Volga and Urals. Through these conquests, Russia acquired a significant Muslim Tatar population and emerged as a multiethnic and multiconfessional state. Also around this period, the mercantile Stroganov family established a firm foothold in the Urals and recruited Russian Cossacks to colonise Siberia.\nIn the later part of his reign, Ivan divided his realm in two. In the zone known as the \"oprichnina\", Ivan's followers carried out a series of bloody purges of the feudal aristocracy (whom he suspected of treachery after prince Andrey Kurbsky's betrayal), culminating in the Massacre of Novgorod in 1570. This combined with the military losses, epidemics, and poor harvests so weakened Russia that the Crimean Tatars were able to sack central Russian regions and burn down Moscow in 1571. However, in 1572 the Russians defeated the Crimean Tatar army at the Battle of Molodi and Ivan abandoned the \"oprichnina\".\nAt the end of Ivan IV's reign the Polish\u2013Lithuanian and Swedish armies carried out a powerful intervention in Russia, devastating its northern and northwest regions.\nTime of Troubles.\nThe death of Ivan's childless son Feodor was followed by a period of civil wars and foreign intervention known as the Time of Troubles (1606\u201313). Extremely cold summers (1601\u20131603) wrecked crops, which led to the Russian famine of 1601\u20131603 and increased the social disorganization. Boris Godunov's reign ended in chaos, civil war combined with foreign intrusion, devastation of many cities and depopulation of the rural regions. The country rocked by internal chaos also attracted several waves of interventions by the Polish\u2013Lithuanian Commonwealth.\nDuring the Polish\u2013Muscovite War (1605\u20131618), Polish\u2013Lithuanian forces reached Moscow and installed the impostor False Dmitriy I in 1605, then supported False Dmitry II in 1607. The decisive moment came when a combined Russian-Swedish army was routed by the Polish forces under hetman Stanis\u0142aw \u017b\u00f3\u0142kiewski at the Battle of Klushino on 4 July\u00a0[O.S. 24 June]\u00a01610. As the result of the battle, the Seven Boyars, a group of Russian nobles, deposed the tsar Vasily Shuysky on 27 July\u00a0[O.S. 17 July]\u00a01610, and recognized the Polish prince W\u0142adys\u0142aw IV Vasa as the Tsar of Russia on 6 September\u00a0[O.S. 27 August]\u00a01610. The Poles occupied Moscow on 21 September\u00a0[O.S. 11 September]\u00a01610. Moscow revolted but riots there were brutally suppressed and the city was set on fire.\nThe crisis provoked a patriotic national uprising against the invasion, both in 1611 and 1612. A volunteer army, led by the merchant Kuzma Minin and prince Dmitry Pozharsky, expelled the foreign forces from the capital on 4 November\u00a0[O.S. 22 October]\u00a01612.\nThe Russian statehood survived the \"Time of Troubles\" and the rule of weak or corrupt Tsars because of the strength of the government's central bureaucracy. Government functionaries continued to serve, regardless of the ruler's legitimacy or the faction controlling the throne. However, the Time of Troubles caused the loss of much territory to the Polish\u2013Lithuanian Commonwealth in the Russo-Polish war, as well as to the Swedish Empire in the Ingrian War.\nAccession of the Romanovs and early rule.\nIn February 1613, after the chaos and expulsion of the Poles from Moscow, a national assembly elected Michael Romanov, the young son of Patriarch Filaret, to the throne. The Romanov dynasty ruled Russia until 1917.\nThe immediate task of the new monarch was to restore peace. Fortunately for Moscow, its major enemies, the Polish\u2013Lithuanian Commonwealth and Sweden, were engaged in a bitter conflict with each other, which provided Russia the opportunity to make peace with Sweden in 1617 and to sign a truce with the Polish\u2013Lithuanian Commonwealth in 1619.\nRecovery of lost territories began in the mid-17th century, when the Khmelnitsky Uprising (1648\u20131657) in Ukraine against Polish rule brought about the Treaty of Pereyaslav between Russia and the Ukrainian Cossacks. In the treaty, Russia granted protection to the Cossacks state in Left-bank Ukraine, formerly under Polish control. This triggered a prolonged Russo-Polish War (1654\u20131667), which ended with the Treaty of Andrusovo, where Poland accepted the loss of Left-bank Ukraine, Kiev and Smolensk.\nThe Russian conquest of Siberia, begun at the end of the 16th century, continued in the 17th century. By the end of the 1640s, the Russians reached the Pacific Ocean, the Russian explorer Semyon Dezhnev, discovered the strait between Asia and America. Russian expansion in the Far East faced resistance from Qing China. After the war between Russia and China, the Treaty of Nerchinsk was signed, delimiting the territories in the Amur region.\nRather than risk their estates in more civil war, the boyars cooperated with the first Romanovs, enabling them to finish the work of bureaucratic centralization. Thus, the state required service from both the old and the new nobility, primarily in the military. In return, the tsars allowed the boyars to complete the process of enserfing the peasants.\nIn the preceding century, the state had gradually curtailed peasants' rights to move from one landlord to another. With the state now fully sanctioning serfdom, runaway peasants became state fugitives, and the power of the landlords over the peasants \"attached\" to their land had become almost complete. Together, the state and the nobles placed an overwhelming burden of taxation on the peasants, whose rate was 100 times greater in the mid-17th century than it had been a century earlier. Likewise, middle-class urban tradesmen and craftsmen were assessed taxes, and were forbidden to change residence. All segments of the population were subject to military levy and special taxes.\nRiots among peasants and citizens of Moscow at this time were endemic and included the Salt Riot (1648), Copper Riot (1662), and the Moscow Uprising (1682). By far the greatest peasant uprising in 17th-century Europe erupted in 1667. As the free settlers of South Russia, the Cossacks, reacted against the growing centralization of the state, serfs escaped from their landlords and joined the rebels. The Cossack leader Stenka Razin led his followers up the Volga River, inciting peasant uprisings and replacing local governments with Cossack rule. The tsar's army finally crushed his forces in 1670; a year later Stenka was captured and beheaded. Yet, less than half a century later, the strains of military expeditions produced another revolt in Astrakhan, ultimately subdued.\nRussian Empire (1721\u20131917).\nPopulation.\nMuch of Russia's expansion occurred in the 17th century, culminating in the first Russian colonisation of the Pacific in the mid-17th century, the Russo-Polish War (1654\u20131667) that incorporated left-bank Ukraine, and the Russian conquest of Siberia. Poland was divided in the 1790\u20131815 era, with much of the land and population going to Russia. Most of the 19th century growth came from adding territory in Asia, south of Siberia.\nPeter the Great.\nPeter the Great (Peter I, 1672\u20131725) brought centralized autocracy into Russia and played a major role in bringing his country into the European state system. Russia was now the largest country in the world, stretching from the Baltic Sea to the Pacific Ocean. The vast majority of the land was unoccupied, and travel was slow. Much of its expansion had taken place in the 17th century, culminating in the first Russian settlement of the Pacific in the mid-17th century, the reconquest of Kiev, and the pacification of the Siberian tribes. However, a population of only 14 million was stretched across this vast landscape. With a short growing season, grain yields trailed behind those in the West and potato farming was not yet widespread. As a result, the great majority of the population workforce was occupied with agriculture. Russia remained isolated from the sea trade and its internal trade, communication and manufacturing were seasonally dependent.\nPeter reformed the Russian army and created the Russian navy. Peter's first military efforts were directed against the Ottoman Turks. His aim was to establish a Russian foothold on the Black Sea by taking the town of Azov. His attention then turned to the north. Peter still lacked a secure northern seaport except at Archangel on the White Sea, whose harbor was frozen nine months a year. Access to the Baltic was blocked by Sweden, whose territory enclosed it on three sides. Peter's ambitions for a \"window to the sea\" led him in 1699 to make a secret alliance with the Polish\u2013Lithuanian Commonwealth and Denmark against Sweden resulting in the Great Northern War.\nThe war ended in 1721 when an exhausted Sweden sued for peace with Russia. Peter acquired four provinces situated south and east of the Gulf of Finland, thus securing his coveted access to the sea. There, in 1703, he had already founded the city that was to become Russia's new capital, Saint Petersburg. Russian intervention in the Commonwealth marked, with the Silent Sejm, the beginning of a 200-year domination of that region by the Russian Empire. In celebration of his conquests, Peter assumed the title of emperor, and the Russian Tsardom officially became the Russian Empire in 1721.\nPeter re-organized his government based on the latest Western models, molding Russia into an absolutist state. He replaced the old \"boyar\" Duma (council of nobles) with a Senate, in effect a supreme council of state. The countryside was also divided into new provinces and districts. Peter told the senate that its mission was to collect taxes. In turn tax revenues tripled over the course of his reign.\nAdministrative Collegia (ministries) were established in St. Petersburg, to replace the old governmental departments. In 1722, Peter promulgated his famous Table of ranks. As part of the government reform, the Orthodox Church was partially incorporated into the country's administrative structure, in effect making it a tool of the state. Peter abolished the patriarchate and replaced it with a collective body, the Holy Synod, led by a lay government official. Peter continued and intensified his predecessors' requirement of state service for all nobles.\nBy then, the once powerful Persian Safavid Empire to the south was heavily declining. Taking advantage, Peter launched the Russo-Persian War (1722\u20131723), known as \"The Persian Expedition of Peter the Great\" by Russian histographers, in order to be the first Russian emperor to establish Russian influence in the Caucasus and Caspian Sea region. After considerable success and the capture of many provinces and cities in the Caucasus and northern mainland Persia, the Safavids were forced to hand over the territories to Russia. However, by 12 years later, all the territories were ceded back to Persia, which was now led by the charismatic military genius Nader Shah, as part of the Treaty of Resht and Treaty of Ganja and the Russo-Persian alliance against the Ottoman Empire, the common neighbouring rivalling enemy.\nPeter the Great died in 1725, leaving an unsettled succession, but Russia had become a great power by the end of his reign. Peter I was succeeded by his second wife, Catherine I (1725\u20131727), who was merely a figurehead for a powerful group of high officials, then by his minor grandson, Peter II (1727\u20131730), then by his niece, Anna (1730\u20131740), daughter of Tsar Ivan V. The heir to Anna was soon deposed in a coup and Elizabeth, daughter of Peter I, ruled from 1741 to 1762. During her reign, Russia took part in the Seven Years' War.\nCatherine the Great.\nNearly 40 years passed before a comparably ambitious ruler appeared. Catherine II, \"the Great\" (r. 1762\u20131796), was a German princess who married the German heir to the Russian crown. Catherine overthrew him in a coup in 1762, becoming queen regnant. Catherine enthusiastically supported the ideals of The Enlightenment, thus earning the status of an enlightened despot. She patronized the arts, science and learning. She contributed to the resurgence of the Russian nobility that began after the death of Peter the Great. Catherine promulgated the Charter to the Gentry reaffirming rights and freedoms of the Russian nobility and abolishing mandatory state service. She seized control of all the church lands, drastically reduced the size of the monasteries, and put the surviving clergy on a tight budget.\nCatherine spent heavily to promote an expansive foreign policy. She extended Russian political control over the Polish\u2013Lithuanian Commonwealth with actions, including the support of the Targowica Confederation. The cost of her campaigns, plus the oppressive social system that required serfs to spend almost all their time laboring on the land of their lords, provoked a major peasant uprising in 1773. Inspired by a Cossack named Yemelyan Pugachev, with the emphatic cry of \"Hang all the landlords!\", the rebels threatened to take Moscow until Catherine crushed the rebellion. Like the other enlightened despots of Europe, Catherine made certain of her own power and formed an alliance with the nobility.\nCatherine successfully waged two wars (1768\u20131774, 1787\u20131792) against the decaying Ottoman Empire and advanced Russia's southern boundary to the Black Sea. In 1775 she liquidated the Zaporozhian Sich, and on the former lands of the Ukrainian Cossacks in the places of theirs settlements was created Novorossiya Governorate, in which new cities were formed: Yekaterinoslav (1776), Yelisavetgrad, Kherson (1778), Odessa (1794). Russia annexed Crimea in 1783 and created the Black Sea fleet. Then, by allying with the rulers of Austria and Prussia, she incorporated the territories of the Polish\u2013Lithuanian Commonwealth, where after a century of Russian rule non-Catholic, mainly Orthodox population prevailed during the Partitions of Poland, pushing the Russian frontier westward into Central Europe.\nIn accordance to Russia's treaty with the Georgians to protect them against any new invasion of their Persian suzerains and further political aspirations, Catherine waged a new war against Persia in 1796 after they had again invaded Georgia and established rule over it about a year prior, and had expelled the newly established Russian garrisons in the Caucasus.\nIn 1798\u20131799, Russian troops participated in the anti-French coalition, the troops under the command of Alexander Suvorov defeated the French in Northern Italy.\nRuling the Empire (1725\u20131825).\nRussian emperors of the 18th century professed the ideas of Enlightened absolutism. However, Westernization and modernization affected only the upper classes of Russian society, while the bulk of the population, consisting of peasants, remained in a state of serfdom. Powerful Russians resented their privileged positions and alien ideas. The backlash was especially severe after the Napoleonic wars. It produced a powerful anti-western campaign that \"led to a wholesale purge of Western specialists and their Russian followers in universities, schools, and government service\".\nThe mid-18th century was marked by the emergence of higher education in Russia. The first two major universities Saint Petersburg State University and Moscow State University were opened. Russian exploration of Siberia and the Far East continued. Great Northern Expedition laid the foundation for the development of Alaska by the Russians. By the end of the 18th century, Alaska became a Russian colony (Russian America). In the early 19th century, Alaska was used as a base for the First Russian circumnavigation. In 1819\u20131821, Russian sailors discovered Antarctica during an Antarctic expedition.\nRussia was in a continuous state of financial crisis. While revenue rose from 9 million rubles in 1724 to 40 million in 1794, expenses grew more rapidly, reaching 49 million in 1794. The budget was allocated 46% to the military, 20% to government economic activities, 12% to administration, and 9% for the Imperial Court in St. Petersburg. The deficit required borrowing, primarily from Amsterdam; 5% of the budget was allocated to debt payments. Paper money was issued to pay for expensive wars, thus causing inflation. 18th-century Russia remained \"a poor, backward, overwhelmingly agricultural, and illiterate country\".\nAlexander I and victory over Napoleon.\nBy the time of her death in 1796, Catherine's expansionist policy had made Russia a major European power. Alexander I continued this policy, wresting Finland from the weakened kingdom of Sweden in 1809 and Bessarabia from the Ottomans in 1812. His key advisor was a Polish nobleman Adam Jerzy Czartoryski.\nAfter Russian armies liberated allied Georgia from Persian occupation in 1802, they clashed with Persia over control and consolidation over Georgia, as well as the Iranian territories that comprise modern-day Azerbaijan and Dagestan. They also became involved in the Caucasian War against the Caucasian Imamate and Circassia. In 1813, the war with Persia concluded with a Russian victory, forcing Qajar Iran to cede swaths of its territories in the Caucasus to Russia, which drastically increased its territory in the region. To the south-west, Russia tried to expand at the expense of the Ottoman Empire, using Georgia at its base for the Caucasus and Anatolian front.\nIn European policy, Alexander I switched Russia back and forth four times in 1804\u20131812 from neutral peacemaker to anti-Napoleon to an ally of Napoleon, winding up in 1812 as Napoleon's enemy. In 1805, he joined Britain in the War of the Third Coalition against Napoleon, but after the massive defeat at the Battle of Austerlitz he switched and formed an alliance with Napoleon by the Treaty of Tilsit (1807) and joined Napoleon's Continental System. He fought a small-scale naval war against Britain, 1807\u20131812.\nThe alliance collapsed by 1810. Russia's economy had been hurt by Napoleon's Continental System, which cut off trade with Britain. As Esdaile notes, \"Implicit in the idea of a Russian Poland was, of course, a war against Napoleon\". Schroeder says Poland was the root cause of the conflict but Russia's refusal to support the Continental System was also a factor.\nThe invasion of Russia was a catastrophe for Napoleon and his 450,000 invasion troops. One major battle was fought at Borodino; casualties were very high, but it was indecisive, and Napoleon was unable to engage and defeat the Russian armies. He tried to force the Tsar to terms by capturing Moscow at the onset of winter, even though he had lost most of his men. Instead, the Russians retreated, burning crops and food supplies in a scorched earth policy that multiplied Napoleon's logistic problems: 85%\u201390% of Napoleon's soldiers died from disease, cold, starvation or ambush by peasant guerrillas. As Napoleon's forces retreated, Russian troops pursued them into Central and Western Europe, defeated Napoleon's army in the Battle of the Nations and finally captured Paris. Of a total population of around 43 million people, Russia lost about 1.5 million in the year 1812; of these about 250,000 to 300,000 were soldiers and the rest peasants and serfs.\nAfter the defeat of Napoleon, Alexander presided over the redrawing of the map of Europe at the Congress of Vienna (1814\u20131815), which made him the king of Congress Poland. He formed the Holy Alliance with Austria and Prussia, to suppress revolutionary movements in Europe that he saw as immoral threats to legitimate Christian monarchs. He helped Austria's Klemens von Metternich in suppressing all national and liberal movements.\nAlthough the Russian Empire would play a leading role on behalf of conservatism as late as 1848, its retention of serfdom precluded economic progress of any significant degree. As West European economic growth accelerated during the Industrial Revolution, sea trade and colonialism which had begun in the second half of the 18th century, Russia began to lag ever farther behind, undermining its ability to field strong armies.\nNicholas I and the Decembrist Revolt.\nRussia's great power status obscured the inefficiency of its government, the isolation of its people, and its economic backwardness. Following the defeat of Napoleon, Alexander I was willing to discuss constitutional reforms, and though a few were introduced, no thoroughgoing changes were attempted.\nThe tsar was succeeded by his younger brother, Nicholas I (1825\u20131855), who at the onset of his reign was confronted with an uprising. The background of this revolt lay in the Napoleonic Wars, when a number of well-educated Russian officers traveled in Europe in the course of the military campaigns, where their exposure to the liberalism of Western Europe encouraged them to seek change on their return. The result was the Decembrist Revolt (December 1825), the work of a small circle of liberal nobles and army officers who wanted to install Nicholas' brother as a constitutional monarch. But the revolt was easily crushed, leading Nicholas to turn away from liberal reforms and champion the reactionary doctrine \"Orthodoxy, Autocracy, and Nationality\".\nIn 1826\u20131828, Russia fought another war against Persia. Russia lost almost all of its recently consolidated territories during the first year but regained them and won the war on highly favourable terms. At the 1828 Treaty of Turkmenchay, Russia gained Armenia, Nakhchivan, Nagorno-Karabakh, Azerbaijan, and I\u011fd\u0131r. In the 1828\u20131829 Russo-Turkish War Russia invaded northeastern Anatolia and occupied the strategic Ottoman towns of Erzurum and G\u00fcm\u00fc\u015fhane and, posing as protector and saviour of the Greek Orthodox population, received extensive support from the region's Pontic Greeks. After a brief occupation, the Russian imperial army withdrew into Georgia. By the 1830s, Russia had conquered all Persian territories and major Ottoman territories in the Caucasus.\nIn 1831, Nicholas crushed the November Uprising in Poland. The Russian autocracy gave Polish artisans and gentry reason to rebel in 1863 by assailing the national core values of language, religion, and culture. The resulting January Uprising was a massive Polish revolt, which also was crushed. France, Britain and Austria tried to intervene in the crisis but were unable. The Russian patriotic press used the Polish uprising to unify the Russian nation, claiming it was Russia's God-given mission to save Poland and the world. Poland was punished by losing its distinctive political and judicial rights, with Russianization imposed on its schools and courts.\nRussian Army.\nTsar Nicholas I (reigned 1825\u20131855) lavished attention on his army. In a nation of 60\u201370 million people, it included a million men. They had outdated equipment and tactics, but the tsar took pride in its smartness on parade. The cavalry horses, for example, were only trained in parade formations, and did poorly in battle. He put generals in charge of most of his civilian agencies regardless of their qualifications. The Army became the vehicle of upward social mobility for noble youths from non-Russian areas, such as Poland, the Baltic, Finland and Georgia. On the other hand, many miscreants, petty criminals and undesirables were punished by local officials by enlisting them for life in the Army. Village oligarchies controlled employment, conscription for the army, and local patronage; they blocked reforms and sent the most unpromising peasant youth to the army. The conscription system was unpopular with people, as was the practice of forcing peasants to house the soldiers for six months of the year.\nFinally the Crimean War at the end of his reign showed the world that Russia was militarily weak, technologically backward, and administratively incompetent. Despite his ambitions toward the south and Ottoman Empire, Russia had not built its railroad network in that direction, and communications were poor. The bureaucracy was riddled with corruption and inefficiency and was unprepared for war. The Navy was weak and technologically backward; the Army, although very large, was good only for parades, suffered from colonels who pocketed their men's pay, poor morale, and was even more out of touch with the latest technology. The nation's leaders realized that reforms were urgently needed.\nRussian society in the first half of 19th century.\nThe early 19th century is the time when Russian literature becomes an independent and very striking phenomenon.\nWesternizers favored imitating Western Europe while others renounced the West and called for a return of the traditions of the past. The latter path was championed by Slavophiles, who heaped scorn on the \"decadent\" West. The Slavophiles were opponents of bureaucracy and preferred the collectivism of the medieval Russian \"mir\", or village community, to the individualism of the West. A forerunner of the Westernizer movement was Pyotr Chaadayev. He exposed the cultural isolation of Russia, from the perspective of Western Europe, in his \"Philosophical Letters\" of 1831. He cast doubt on the greatness of the Russian past, and ridiculed Orthodoxy for failing to provide a sound spiritual basis for the Russian mind. He called on Russia to emulate Western Europe, especially in rational and logical thought, its progressive spirit, its leadership in science, and indeed its leadership on the path to freedom. Vissarion Belinsky and Alexander Herzen were prominent Westernizers.\nCrimean War.\nSince the war against Napoleon, Russia had become deeply involved in the affairs of Europe, as part of the \"Holy Alliance.\" The Holy Alliance was formed to serve as the \"policeman of Europe.\" However, to maintain the alliance required large armies. Prussia, Austria, Britain and France (the other members of the alliance) lacked large armies and needed Russia to supply the required numbers, which fit the philosophy of Nicholas I. The Tsar sent his army into Hungary in 1849 at the request of the Austrian Empire and broke the revolt there, while preventing its spread to Russian Poland. The Tsar cracked down on any signs of internal unrest.\nRussia expected that in exchange for supplying the troops to be the policeman of Europe, it should have a free hand in dealing with the decaying Ottoman Empire\u2014the \"sick man of Europe.\" In 1853, Russia invaded Ottoman-controlled areas leading to the Crimean War. Britain and France came to the rescue of the Ottomans. After a grueling war fought largely in Crimea, with very high death rates from disease, the allies won.\nHistorian Orlando Figes points to the long-term damage Russia suffered:\nThe demilitarization of the Black Sea was a major blow to Russia, which was no longer able to protect its vulnerable southern coastal frontier against the British or any other fleet... The destruction of the Russian Black Sea Fleet, Sevastopol and other naval docks was a humiliation. No compulsory disarmament had ever been imposed on a great power previously... The Allies did not really think that they were dealing with a European power in Russia. They regarded Russia as a semi-Asiatic state...In Russia itself, the Crimean defeat discredited the armed services and highlighted the need to modernize the countries defenses, not just in the strictly military sense, but also through the building of railways, industrialization, sound finances and so on...The image many Russians had built up of their country \u2013 the biggest, richest and most powerful in the world \u2013 had suddenly been shattered. Russia's backwardness had been exposed...The Crimean disaster had exposed the shortcomings of every institution in Russia \u2013 not just the corruption and incompetence of the military command, the technological backwardness of the army and navy, or the inadequate roads and lack of railways the accounted for the chronic problems of supply, but the poor condition and illiteracy of the serfs who made up the armed forces, the inability of the serf economy to sustain a state of war against industrial powers, and the failures of autocracy itself.\nAlexander II and the abolition of serfdom.\nWhen Alexander II came to the throne in 1855, the demand for reform was widespread. The most pressing problem confronting the Government was serfdom. In 1859, there were 23 million serfs (out of a total population of 67\u00a0million). In anticipation of civil unrest that could ultimately foment a revolution, Alexander II chose to preemptively abolish serfdom with the emancipation reform in 1861. Emancipation brought a supply of free labor to the cities, stimulated industry, and the middle class grew in number and influence. The freed peasants had to buy land, allotted to them, from the landowners with state assistance. The Government issued special bonds to the landowners for the land that they had lost, and collected a special tax from the peasants, called redemption payments, at a rate of 5% of the total cost of allotted land yearly. All the land turned over to the peasants was owned collectively by the \"mir\", the village community, which divided the land among the peasants and supervised the various holdings.\nAlexander was responsible for numerous reforms besides abolishing serfdom. He reorganized the judicial system, setting up elected local judges, abolishing capital punishment, promoting local self-government through the zemstvo system, imposing universal military service, ending some of the privileges of the nobility, and promoting the universities.\nIn foreign policy, he sold Alaska to the United States in 1867. He modernized the military command system. He sought peace, and joined with Germany and Austria in the League of the Three Emperors that stabilized the European situation. The Russian Empire expanded in Siberia and in the Caucasus and made gains at the expense of China. Faced with an uprising in Poland in 1863, he stripped that land of its separate Constitution and incorporated it directly into Russia. To counter the rise of a revolutionary and anarchistic movements, he sent thousands of dissidents into exile in Siberia and was proposing additional parliamentary reforms when he was assassinated in 1881.\nIn the late 1870s Russia and the Ottoman Empire again clashed in the Balkans. The Russo-Turkish War was popular among the Russian people, who supported the independence of their fellow Orthodox Slavs, the Serbs and the Bulgarians. Russia's victory in this war allowed a number of Balkan states to gain independence: Romania, Serbia, Montenegro. In addition, Bulgaria de facto became independent. However, the war increased tension with Austria-Hungary, which also had ambitions in the region. The Tsar was disappointed by the results of the Congress of Berlin in 1878, but abided by the agreement.\nDuring this period Russia expanded its empire into Central Asia, conquering the khanates of Kokand, Bukhara, and Khiva, as well as the Trans-Caspian region. Russia's advance in Asia led to British fears that the Russians planned aggression against British India. Before 1815 London worried Napoleon would combine with Russia to do that in one mighty campaign. After 1815 London feared Russia alone would do it step by step. However historians report that the Russians never had any intention to move against India.\nRussian society in the second half of 19th century.\nIn the 1860s, a movement known as Nihilism developed in Russia. A term originally coined by Ivan Turgenev in his 1862 novel \"Fathers and Sons\", Nihilists favoured the destruction of human institutions and laws, based on the assumption that they are artificial and corrupt. At its core, Russian nihilism was characterized by the belief that the world lacks comprehensible meaning, objective truth, or value. For some time, many Russian liberals had been dissatisfied by what they regarded as the empty discussions of the intelligentsia. The Nihilists questioned all old values and shocked the Russian establishment. They became involved in the cause of reform and became major political forces. Their path was facilitated by the previous actions of the Decembrists, who revolted in 1825, and the financial and political hardship caused by the Crimean War, which caused many Russians to lose faith in political institutions. Russian nihilists created the manifesto \"Catechism of a Revolutionary\".\nAfter the Nihilists failed to convert the aristocracy and landed gentry to the cause of reform, they turned to the peasants. Their campaign became known as the \"Narodnk\" (\"Populist\") movement. It was based on the belief that the common people had the wisdom and peaceful ability to lead the nation.\nAs the \"Narodnik\" movement gained momentum, the government moved to extirpate it. In response to the growing reaction of the government, a radical branch of the Narodniks advocated and practiced terrorism. One after another, prominent officials were shot or killed by bombs. This represented the ascendancy of anarchism in Russia as a powerful revolutionary force. Finally, after several attempts, Alexander II was assassinated by anarchists in 1881, on the very day he had approved a proposal to call a representative assembly to consider new reforms in addition to the abolition of serfdom designed to ameliorate revolutionary demands.\nThe end of the 19th century and the beginning of the 20th is known as the Silver Age of Russian culture. The Silver Age was dominated by the artistic movements of Russian Symbolism, Acmeism, and Russian Futurism, many poetic schools flourished, including the Mystical Anarchism tendency within the Symbolist movement. The Russian avant-garde was a large, influential wave of modern art that flourished in Russian Empire and Soviet Union, approximately from 1890 to 1930\u2014although some have placed its beginning as early as 1850 and its end as late as 1960.\nAutocracy and reaction under Alexander III.\nUnlike his father, the new tsar Alexander III (1881\u20131894) was throughout his reign a staunch reactionary who revived the maxim of \"Orthodoxy, Autocracy, and National Character\". A committed Slavophile, Alexander III believed that Russia could be saved from chaos only by shutting itself off from the subversive influences of Western Europe. In his reign Russia concluded the union with republican France to contain the growing power of Germany, completed the conquest of Central Asia, and exacted important territorial and commercial concessions from China.\nThe tsar's most influential adviser was Konstantin Pobedonostsev, tutor to Alexander III and his son Nicholas, and procurator of the Holy Synod from 1880 to 1895. He taught his royal pupils to fear freedom of speech and press and to hate democracy, constitutions, and the parliamentary system. Under Pobedonostsev, revolutionaries were hunted down and a policy of Russification was carried out.\nNicholas II and new revolutionary movement.\nAlexander was succeeded by his son Nicholas II (1894\u20131918). The Industrial Revolution, which began to exert a significant influence in Russia, was meanwhile creating forces that would finally overthrow the tsar. Politically, these opposition forces organized into three competing parties: The liberal elements among the industrial capitalists and nobility, who wanted peaceful social reform and a constitutional monarchy, founded the Constitutional Democratic party or \"Kadets\" in 1905. Followers of the Narodnik tradition established the Socialist-Revolutionary Party or \"Esers\" in 1901, advocating the distribution of land among the peasants who worked it. A third radical group founded the Russian Social Democratic Labour Party or \"RSDLP\" in 1898; this party was the primary exponent of Marxism in Russia. Gathering their support from the radical intellectuals and the urban working class, they advocated complete social, economic and political revolution.\nIn 1903, the RSDLP split into two wings: the radical Bolsheviks, led by Vladimir Lenin, and the relatively moderate Mensheviks, led by Yuli Martov. The Mensheviks believed that Russian socialism would grow gradually and peacefully and that the tsar's regime should be succeeded by a democratic republic. The Bolsheviks advocated the formation of a small elite of professional revolutionaries, subject to strong party discipline, to act as the vanguard of the proletariat in order to seize power by force.\nAt the beginning of the 20th century, Russia continued its expansion in the Far East; Chinese Manchuria was in the zone of Russian interests. Russia took an active part in the intervention of the great powers in China to suppress the Boxer rebellion. During this war, Russia occupied Manchuria, which caused a clash of interests with Japan. In 1904, the Russo-Japanese War began, which ended extremely unfavourably for Russia.\nRevolution of 1905.\nThe disastrous performance of the Russian armed forces in the Russo-Japanese War was a major blow to the Russian State and increased the potential for unrest.\nIn January 1905, an incident known as \"Bloody Sunday\" occurred when Father Gapon led an enormous crowd to the Winter Palace in Saint Petersburg to present a petition to the tsar. When the procession reached the palace, Cossacks opened fire, killing hundreds. The Russian masses were so aroused over the massacre that a general strike was declared demanding a democratic republic. This marked the beginning of the Russian Revolution of 1905. Soviets (councils of workers) appeared in most cities to direct revolutionary activity.\nIn October 1905, Nicholas reluctantly issued the October Manifesto, which conceded the creation of a national Duma (legislature) to be called without delay. The right to vote was extended, and no law was to go into force without confirmation by the Duma. The moderate groups were satisfied; but the socialists rejected the concessions as insufficient and tried to organize new strikes. By the end of 1905, there was disunity among the reformers, and the tsar's position was strengthened.\nWorld War I.\nOn 28 June 1914, Bosnian Serbs assassinated Archduke Franz Ferdinand of Austro-Hungary. Austro-Hungary issued an ultimatum to Serbia, which it considered a Russian client-state. Russia had no treaty obligation to Serbia, and most Russian leaders wanted to avoid war. But in that crisis they had the support of France, and believed that supporting Serbia was important for Russia's credibility and for its goal of a leadership role in the Balkans. Tsar Nicholas II mobilised Russian forces on 30 July 1914 to defend Serbia. Christopher Clark states: \"The Russian general mobilisation [of 30 July] was one of the most momentous decisions of the July crisis\". Germany responded with its own mobilisation and declaration of War on 1 August 1914. At the opening of hostilities, the Russians took the offensive against both Germany and Austria-Hungary.\nThe very large but poorly led and under-equipped Russian army fought tenaciously. Casualties were enormous. In the 1914 campaign, Russian forces defeated Austro-Hungarian forces in the Battle of Galicia. The success of the Russian army forced the German army to withdraw troops from the western front to the Russian front. However, victories in Poland by the Central Powers in the 1915 campaign, led to a major retreat of the Russian army. In 1916, the Russians again dealt a powerful blow to the Austrians during the Brusilov offensive.\nBy 1915, morale was worsening. Many recruits were sent to the front unarmed. Nevertheless, the Russian army fought on, and tied down large numbers of Germans and Austrians. When the homefront showed an occasional surge of patriotism, the tsar and his entourage failed to exploit it for military benefit. The Russian army neglected to rally the ethnic and religious minorities that were hostile to Austria, such as Poles. The tsar refused to cooperate with the national legislature, the Duma, and listened less to experts than to his wife, who was in thrall to her chief advisor, the holy man Grigori Rasputin. More than two million refugees fled.\nRepeated military failures and bureaucratic ineptitude soon turned large segments of the population against the government. The German and Ottoman fleets prevented Russia from importing urgently needed supplies through the Baltic and Black seas. By mid-1915 the impact of the war was demoralizing. Food and fuel were in short supply, casualties kept occurring, and inflation was mounting. Strikes increased among factory workers, and the peasants, who wanted land reforms, were restless. Meanwhile, elite distrust of the regime was deepened by reports that Rasputin was gaining influence; his assassination in late 1916 ended the scandal but did not restore the autocracy's prestige.\nRussian Civil War (1917\u20131922).\nRussian Revolution.\nIn late February (3 March 1917), a strike occurred in a factory in the capital Petrograd (Saint Petersburg). On 23 February (8 March) 1917, thousands of female textile workers walked out of their factories protesting the lack of food and calling on other workers to join them. Within days, nearly all the workers in the city were idle, and street fighting broke out. The tsar ordered the Duma to disband, ordered strikers to return to work, and ordered troops to shoot at demonstrators in the streets. His orders triggered the February Revolution, especially when soldiers sided with the strikers. On 2 March, Nicholas II abdicated.\nTo fill the vacuum of authority, the Duma declared a Provisional Government, headed by Prince Lvov, which was collectively known as the Russian Republic. Meanwhile, the socialists in Petrograd organized elections among workers and soldiers to form a soviet (council) of workers' and soldiers' deputies, as an organ of popular power that could pressure the \"bourgeois\" Provisional Government.\nIn July, following a series of crises that undermined their authority with the public, the head of the Provisional Government resigned and was succeeded by Alexander Kerensky, who was more progressive than his predecessor but not radical enough for the Bolsheviks or many Russians discontented with the deepening economic crisis and the war. The socialist-led soviet in Petrograd joined with soviets that formed throughout the country to create a national movement.\nThe German government provided over 40 million gold marks to subsidize Bolshevik publications and activities subversive of the tsarist government, especially focusing on disgruntled soldiers and workers. In April 1917 Germany provided a special sealed train to carry Vladimir Lenin back to Russia from his exile in Switzerland. After many behind-the-scenes maneuvers, the soviets seized control of the government in November 1917 and drove Kerensky and his moderate provisional government into exile, in the events that would become known as the October Revolution.\nBolshevik figures such as Anatoly Lunacharsky, Moisei Uritsky and Dmitry Manuilsky agreed that Lenin\u2019s influence on the Bolshevik party was decisive but the October insurrection was carried out according to Trotsky\u2019s, not to Lenin\u2019s plan.\nWhen the national Constituent Assembly (elected in December 1917) refused to become a rubber stamp of the Bolsheviks, it was dissolved by Lenin's troops and all vestiges of democracy were removed. With the handicap of the moderate opposition removed, Lenin was able to free his regime from the war problem by the harsh Treaty of Brest-Litovsk (1918) with Germany. Russia lost much of her western borderlands. However, when Germany was defeated the Soviet government repudiated the Treaty.\nRussian Civil War.\nThe Bolshevik grip on power was by no means secure, and a lengthy struggle broke out between the new regime and its opponents, which included the Socialist Revolutionaries, the anti-Bolshevik White movement, and large numbers of peasants. At the same time the Allied powers sent several expeditionary armies to support the anti-Communist forces in an attempt to force Russia to rejoin the world war. The Bolsheviks fought against both these forces and national independence movements in the former Russian Empire. By 1921, they had defeated their internal enemies and brought most of the newly independent states under their control, with the exception of Finland, the Baltic States, the Moldavian Democratic Republic (which elected to unite with Romania), and Poland (with whom they had fought the Polish\u2013Soviet War). Finland also annexed the region Pechenga of the Russian Kola Peninsula; Soviet Russia and allied Soviet republics conceded the parts of its territory to Estonia (Petseri County and Estonian Ingria), Latvia (Pytalovo), and Turkey (Kars). Poland incorporated the contested territories of Western Belarus and Western Ukraine, the former parts of the Russian Empire (except Galicia) east to Curzon Line.\nBoth sides regularly committed brutal atrocities against civilians. During the civil war era for example, Petlyura and Denikin's forces massacred 100,000 to 150,000 Jews in Ukraine and southern Russia. Hundreds of thousands of Jews were left homeless and tens of thousands became victims of serious illness. These massacres are now referred to as the White Terror (Russia).\nEstimates for the total number of people killed during the Red Terror carried out by the Bolsheviks vary widely. One source asserts that the total number of victims could be 1.3 million, whereas others give estimates ranging from 10,000 in the initial period of repression to 140,000 and an estimate of 28,000 executions per year from December 1917 to February 1922. The most reliable estimations for the total number of killings put the number at about 100,000, whereas others suggest a figure of 200,000.\nThe Russian economy was devastated by the war, with factories and bridges destroyed, cattle and raw materials pillaged, mines flooded and machines damaged. The droughts of 1920 and 1921, as well as the 1921 famine, worsened the disaster still further. Disease had reached pandemic proportions, with 3,000,000 dying of typhus alone in 1920. Millions more also died of widespread starvation. By 1922 there were at least 7,000,000 street children in Russia as a result of nearly ten years of devastation from the Great War and the civil war. Another one to two million people, known as the White \u00e9migr\u00e9s, fled Russia, many were evacuated from Crimea in the 1920, some through the Far East, others west into the newly independent Baltic countries. These \u00e9migr\u00e9s included a large percentage of the educated and skilled population.\nSoviet Union (1922\u20131991).\nCreation of the Soviet Union.\nThe Soviet Union, established in December 1922 by the leaders of the Russian Communist Party, was roughly coterminous with Russia before the Treaty of Brest-Litovsk. At that time, the new nation included four constituent republics: the Russian SFSR, the Ukrainian SSR, the Belarusian SSR, and the Transcaucasian SFSR.\nThe constitution, adopted in 1924, established a federal system of government based on a pyramid of soviets in each constituent republic which culminated in the All-Union Congress of Soviets. However, while it appeared that the congress exercised sovereign power, this body was actually governed by the Communist Party, which in turn was controlled by the Politburo from Moscow.\nWar Communism and the New Economic Policy.\nThe period from the consolidation of the Bolshevik Revolution in 1917 until 1921 is known as the period of war communism. Land, all industry, and small businesses were nationalized, and the money economy was restricted. Strong opposition soon developed. The peasants wanted cash payments for their products and resented having to surrender their surplus grain to the government as a part of its civil war policies. Confronted with peasant opposition, Lenin began a strategic retreat from war communism known as the New Economic Policy (NEP). The peasants were freed from wholesale levies of grain and allowed to sell their surplus produce in the open market. Commerce was stimulated by permitting private retail trading. The state continued to be responsible for banking, transportation, heavy industry, and public utilities.\nAlthough the left opposition among the Communists criticized the rich peasants, or kulaks, who benefited from the NEP, the program proved highly beneficial and the economy revived. The NEP would later come under increasing opposition from within the party following Lenin's death in early 1924.\nChanges to Russian society.\nAs the Russian Empire included during this period not only the region of Russia, but also today's territories of Ukraine, Belarus, Poland, Lithuania, Estonia, Latvia, Finland, Moldavia and the Caucasian and Central Asian countries, it is possible to examine the firm formation process in all those regions. One of the main determinants of firm creation for given regions of Russian Empire might be urban demand of goods and supply of industrial and organizational skill.\nWhile the Russian economy was being transformed, the social life of the people underwent equally drastic changes. The Family Code of 1918 granted women equal status to men, and permitted a couple to take either the husband or wife's name. Divorce no longer required court procedure,\nand to make women completely free of the responsibilities of childbearing, abortion was made legal as early as 1920. As a side effect, the emancipation of women increased the labor market. Girls were encouraged to secure an education and pursue a career. Communal nurseries were set up for child care, and efforts were made to shift the center of people's social life from the home to educational and recreational groups, the soviet clubs.\nThe Soviet government pursued a policy of eliminating illiteracy (Likbez). After industrialization, massive urbanization began. In the field of national policy in the 1920s, the Korenizatsiya was carried out. However, from the mid-30s, the Stalinist government returned to the tsarist policy of Russification of the outskirts. In particular, the languages of all the nations of the USSR were transcribed into the Cyrillic alphabet in the process known as Cyrillization.\nIndustrialization and collectivization.\nThe years from 1929 to 1939 comprised a tumultuous decade in Soviet history\u2014a period of massive industrialization and internal struggles as Joseph Stalin established near total control over Soviet society, wielding virtually unrestrained power. Following Lenin's death Stalin wrestled to gain control of the Soviet Union with rival factions in the Politburo, especially Leon Trotsky's. By 1928, with the Trotskyists either exiled or rendered powerless, Stalin was ready to put a radical programme of industrialisation into action.\nIn 1929, Stalin proposed the first five-year plan. Abolishing the NEP, it was the first of a number of plans aimed at swift accumulation of capital resources through the buildup of heavy industry, the collectivization of agriculture, and the restricted manufacture of consumer goods. For the first time in history a government controlled all economic activity. The rapid growth of production capacity and the volume of production of heavy industry was of great importance for ensuring economic independence from western countries and strengthening the country's defense capability. At this time, the Soviet Union made the transition from an agrarian country to an industrial one.\nAs a part of the plan, the government took control of agriculture through the state and collective farms (\"kolkhozes\"). By a decree of February 1930, about one million individual peasants (\"kulaks\") were forced off their land. Many peasants strongly opposed regimentation by the state, often slaughtering their herds when faced with the loss of their land. In some sections they revolted, and countless peasants deemed \"kulaks\" by the authorities were executed. The combination of bad weather, deficiencies of the hastily established collective farms, and massive confiscation of grain precipitated a serious famine, and several million peasants died of starvation, mostly in Ukraine, Kazakhstan and parts of southwestern Russia. The deteriorating conditions in the countryside drove millions of desperate peasants to the rapidly growing cities, fueling industrialization, and vastly increasing Russia's urban population.\nStalinist repression.\nThe NKVD gathered in tens of thousands of Soviet citizens to face arrest, deportation, or execution. Of the six original members of the 1920 Politburo who survived Lenin, all were purged by Stalin. Old Bolsheviks who had been loyal comrades of Lenin, high officers in the Red Army, and directors of industry were liquidated in the Great Purges. Purges in other Soviet republics also helped centralize control in the USSR.\nStalin destroyed the opposition in the party consisting of the old Bolsheviks during the Moscow trials. The NKVD under the leadership of Stalin's commissar Nikolai Yezhov carried out a series of massive repressive operations against the kulaks and various national minorities in the USSR. During the Great Purges of 1937\u201338, about 700,000 people were executed.\nPenalties were introduced, and many citizens were prosecuted for fictitious crimes of sabotage and espionage. The labor provided by convicts working in the labor camps of the Gulag system became an important component of the industrialization effort, especially in Siberia. An estimated 18 million people passed through the Gulag system, and perhaps another 15\u00a0million had experience of some other form of forced labor.\nAfter the partition of Poland in 1939, the NKVD executed 20,000 captured Polish officers in the Katyn massacre. In the late 30s - first half of the 40s, the Stalinist government carried out massive deportations of various nationalities. A number of ethnic groups were deported from their settlement to Central Asia.\nSoviet Union on the international stage.\nThe Soviet Union viewed the 1933 accession of fervently anti-Communist Hitler to power in Germany with alarm, especially since Hitler proclaimed the Drang nach Osten as one of the major objectives in his vision of the German strategy of Lebensraum. The Soviets supported the republicans of Spain who struggled against fascist German and Italian troops in the Spanish Civil War. In 1938\u20131939, the Soviet Union successfully fought against Imperial Japan in the Soviet\u2013Japanese border conflicts in the Russian Far East, which led to Soviet-Japanese neutrality and the tense border peace that lasted until August 1945.\nIn 1938, Germany annexed Austria and, together with major Western European powers, signed the Munich Agreement following which Germany, Hungary and Poland divided parts of Czechoslovakia between themselves. German plans for further eastward expansion, as well as the lack of resolve from Western powers to oppose it, became more apparent. Despite the Soviet Union strongly opposing the Munich deal and repeatedly reaffirming its readiness to militarily back commitments given earlier to Czechoslovakia, the Western Betrayal led to the end of Czechoslovakia and further increased fears in the Soviet Union of a coming German attack. This led the Soviet Union to rush the modernization of its military industry and to carry out its own diplomatic maneuvers. In 1939, the Soviet Union signed the Molotov\u2013Ribbentrop Pact: a non-aggression pact with Nazi Germany dividing Eastern Europe into two separate spheres of influence. Following the pact, the USSR normalized relations with Nazi Germany and resumed Soviet\u2013German trade.\nWorld War II.\nOn 17 September 1939, the Red Army invaded eastern Poland, stating as justification the \"need to protect Ukrainians and Belarusians\" there, after the \"cessation of existence\" of the Polish state. As a result, the Belarusian and Ukrainian Soviet republics' western borders were moved westward, and the new Soviet western border was drawn close to the original Curzon line. In the meantime negotiations with Finland over a Soviet-proposed land swap that would redraw the Soviet-Finnish border further away from Leningrad failed, and in December 1939 the USSR invaded Finland, beginning a campaign known as the Winter War (1939\u20131940), with the goal of annexing Finland into the Soviet Union. The war took a heavy death toll on the Red Army and the Soviets failed to conquer Finland, but forced Finland to sign the Moscow Peace Treaty and cede the Karelian Isthmus and Ladoga Karelia. In summer 1940 the USSR issued an ultimatum to Romania forcing it to cede the territories of Bessarabia and Northern Bukovina. At the same time, the Soviet Union also occupied the three formerly independent Baltic states (Estonia, Latvia and Lithuania).\nThe peace with Germany was tense, as both sides were preparing for the military conflict, and abruptly ended when the Axis forces led by Germany swept across the Soviet border on 22 June 1941. By the autumn the German army had seized Ukraine, laid a siege of Leningrad, and threatened to capture the capital, Moscow, itself. Despite the fact that in December 1941 the Red Army threw off the German forces from Moscow in a successful counterattack, the Germans retained the strategic initiative for approximately another year and held a deep offensive in the south-eastern direction, reaching the Volga and the Caucasus. However, two major German defeats in Stalingrad and Kursk proved decisive and reversed the course of the entire World War as the Germans never regained the strength to sustain their offensive operations and the Soviet Union recaptured the initiative for the rest of the conflict. By the end of 1943, the Red Army had broken through the German siege of Leningrad and liberated much of Ukraine, much of Western Russia and moved into Belarus. During the 1944 campaign, the Red Army defeated German forces in a series of offensive campaigns known as Stalin's ten blows. By the end of 1944, the front had moved beyond the 1939 Soviet frontiers into eastern Europe. Soviet forces drove into eastern Germany, capturing Berlin in May 1945. The war with Germany thus ended triumphantly for the Soviet Union.\nAs agreed at the Yalta Conference, three months after the Victory Day in Europe the USSR launched the Soviet invasion of Manchuria, defeating the Japanese troops in neighboring Manchuria, the last Soviet battle of World War II.\nAlthough the Soviet Union was victorious in World War II, the war resulted in around 26\u201327\u00a0million Soviet deaths (estimates vary) and had devastated the Soviet economy in the struggle. Some 70,000 settlements were destroyed. The occupied territories suffered from the ravages of German occupation and deportations of slave labor by Germany. Thirteen million Soviet citizens became victims of the repressive policies of Germany and its allies in occupied territories, where people died because of mass murders, famine, absence of medical aid and slave labor. The Holocaust, carried out by German \"Einsatzgruppen\" along with local collaborators, resulted in almost complete annihilation of the Jewish population over the entire territory temporarily occupied by Germany and its allies. During the occupation, the Leningrad region lost around a quarter of its population, Soviet Belarus lost from a quarter to a third of its population, and 3.6\u00a0million Soviet prisoners of war (of 5.5\u00a0million) died in German camps.\nCold War.\nCollaboration among the major Allies had won the war and was supposed to serve as the basis for postwar reconstruction and security. USSR became one of the founders of the UN and a permanent member of the UN Security Council. However, the conflict between Soviet and U.S. national interests, known as the Cold War, came to dominate the international stage.\nThe Cold War emerged from a conflict between Stalin and U.S. President Harry Truman over the future of Eastern Europe during the Potsdam Conference in the summer of 1945. Stalin's goal was to establish a buffer zone of states between Germany and the Soviet Union. Truman charged that Stalin had betrayed the Yalta agreement. With Eastern Europe under Red Army occupation, Stalin was also biding his time, as his own atomic bomb project was steadily and secretly progressing.\nIn April 1949 the United States sponsored the North Atlantic Treaty Organization (NATO), a mutual defense pact. The Soviet Union established an Eastern counterpart to NATO in 1955, dubbed the Warsaw Pact. The division of Europe into Western and Soviet blocks later took on a more global character, especially after 1949, when the U.S. nuclear monopoly ended with the testing of a Soviet bomb and the Communist takeover in China.\nThe foremost objectives of Soviet foreign policy were the maintenance and enhancement of national security and the maintenance of hegemony over Eastern Europe. The Soviet Union maintained its dominance over the Warsaw Pact through crushing the Hungarian Revolution of 1956, suppressing the Prague Spring in Czechoslovakia in 1968, and supporting the suppression of the Solidarity movement in Poland in the early 1980s. The Soviet Union opposed the United States in a number of proxy conflicts all over the world, including the Korean War and Vietnam War.\nAs the Soviet Union continued to maintain tight control over its sphere of influence in Eastern Europe, the Cold War gave way to \"D\u00e9tente\" and a more complicated pattern of international relations in the 1970s. The nuclear race continued, the number of nuclear weapons in the hands of the USSR and the United States reached a menacing scale, giving them the ability to destroy the planet multiple times. Less powerful countries had more room to assert their independence, and the two superpowers were partially able to recognize their common interest in trying to check the further spread and proliferation of nuclear weapons in treaties such as SALT I, SALT II, and the Anti-Ballistic Missile Treaty.\nU.S.\u2013Soviet relations deteriorated following the beginning of the nine-year Soviet\u2013Afghan War in 1979 and the 1980 election of Ronald Reagan, a staunch anti-communist, but improved as the communist bloc started to unravel in the late 1980s. With the collapse of the Soviet Union in 1991, Russia lost the superpower status that it had won in the Second World War.\nDe-Stalinization and the era of stagnation.\nNikita Khrushchev solidified his position in a speech before the Twentieth Congress of the Communist Party in 1956 detailing Stalin's atrocities.\nIn 1964, Khrushchev was impeached by the Communist Party's Central Committee, charging him with a host of errors that included Soviet setbacks such as the Cuban Missile Crisis. After a period of collective leadership led by Leonid Brezhnev, Alexei Kosygin and Nikolai Podgorny, Brezhnev took Khrushchev's place as Soviet leader. Brezhnev emphasized heavy industry, instituted the Soviet economic reform of 1965, and also attempted to ease relationships with the United States. Soviet science and industry peaked in the Khrushchev and Brezhnev years. The world's first nuclear power plant was established in 1954 in Obninsk, and the Baikal Amur Mainline was built. In the 1950s the USSR became a leading producer and exporter of petroleum and natural gas. In 1980 Moscow hosted the Summer Olympic Games.\nWhile all modernized economies were rapidly moving to computerization after 1965, the USSR fell behind. Moscow's decision to copy the IBM 360 of 1965 proved a decisive mistake for it locked scientists into an antiquated system they were unable to improve. They had enormous difficulties in manufacturing the necessary chips reliably and in quantity, in programming workable and efficient programs, in coordinating entirely separate operations, and in providing support to computer users.\nOne of the greatest strengths of Soviet economy was its vast supplies of oil and gas; world oil prices quadrupled in 1973\u20131974, and rose again in 1979\u20131981, making the energy sector the chief driver of the Soviet economy, and was used to cover multiple weaknesses. At one point, Soviet Premier Alexei Kosygin told the head of oil and gas production, \"things are bad with bread. Give me 3 million tons [of oil] over the plan.\" Former prime minister Yegor Gaidar, an economist looking back three decades, in 2007 wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The hard currency from oil exports stopped the growing food supply crisis, increased the import of equipment and consumer goods, ensured a financial base for the arms race and the achievement of nuclear parity with the United States, and permitted the realization of such risky foreign-policy actions as the war in Afghanistan.\nSoviet space program.\nThe Soviet space program, founded by Sergey Korolev, was especially successful. On 4 October 1957, the Soviet Union launched the first satellite, Sputnik. On 12 April 1961, Yuri Gagarin became the first human to travel into space in the Soviet spaceship Vostok 1. Other achievements of Russian space program include: the first photo of the far side of the Moon; exploration of Venus; the first spacewalk by Alexei Leonov; first female spaceflight by Valentina Tereshkova. In 1970 and 1973, the world's first planetary rovers were sent to the moon: Lunokhod 1 and Lunokhod 2. More recently, the Soviet Union produced the world's first space station, Salyut, which in 1986 was replaced by Mir, the first consistently inhabited long-term space station, that served from 1986 to 2001.\nPerestroika and Glasnost.\nTwo developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. After the rapid succession of Yuri Andropov and Konstantin Chernenko, Mikhail Gorbachev implemented perestroika in an attempt to modernize Soviet communism, and made significant changes in the party leadership. However, Gorbachev's social reforms led to unintended consequences. His policy of \"glasnost\" facilitated public access to information after decades of government repression, and social problems received wider public attention, undermining the Communist Party's authority. \"Glasnost\" allowed ethnic and nationalist disaffection to reach the surface, and many constituent republics, especially the Baltic republics, Georgian SSR and Moldavian SSR, sought greater autonomy, which Moscow was unwilling to provide. In the revolutions of 1989 the USSR lost its allies in Eastern Europe. Gorbachev's attempts at economic reform were not sufficient, and the Soviet government left intact most of the fundamental elements of communist economy. Suffering from low pricing of petroleum and natural gas, the ongoing war in Afghanistan, and outdated industry and pervasive corruption, the Soviet planned economy proved to be ineffective, and by 1990 the Soviet government had lost control over economic conditions. Due to price control, there were shortages of almost all products. Control over the constituent republics was also relaxed, and they began to assert their national sovereignty.\nThe tension between Soviet Union and Russian SFSR authorities came to be personified in the power struggle between Gorbachev and Boris Yeltsin. Squeezed out of Union politics by Gorbachev in 1987, Yeltsin, who represented himself as a committed democrat, presented a significant opposition to Gorbachev's authority. In a remarkable reversal of fortunes, he gained election as chairman of the Russian republic's new Supreme Soviet in May 1990.\nPriority over Soviet Union laws and negotiations on a new Treaty.\nThe following month, Yeltsin secured legislation giving Russian laws priority over Soviet laws. Article 5 of the Declaration of State Sovereignty of the Russian Soviet Federative Socialist Republic establishes the full authority of the RSFSR, with the exception of those which it voluntarily transfers to the Union of Soviet Socialist Republics, as well as the supremacy of the Constitution of the RSFSR and the laws of the RSFSR over the entire territory of the RSFSR. Acts of the Union of SSR which conflict with the sovereign rights of the RSFSR shall be suspended by the Republic on its territory. And also Yeltsin withholding two-thirds of the budget. In the first Russian presidential election in 1991 Yeltsin became president of the Russian SFSR. At last Gorbachev attempted to restructure the Soviet Union into a less centralized state. However, on 19 August 1991, a coup against Gorbachev was attempted. The coup faced wide popular opposition and collapsed in three days, but disintegration of the Union became imminent. The Russian government took over most of the Soviet Union government institutions on its territory. Because of the dominant position of Russians in the Soviet Union, most gave little thought to any distinction between Russia and the Soviet Union before the late 1980s. In the Soviet Union, only Russian SFSR lacked its own republic-level Communist Party branch, trade union councils, Academy of Sciences, and the like.\nSoviet coup attempt, the Transition Period and the end of the Soviet Union.\nThe Communist Party of the Soviet Union was banned in Russia in 1991, although no lustration has ever taken place, and many of its members became top Russian officials. However, as the Soviet government was still opposed to immediate market reforms, the economic situation continued to deteriorate.\nOn 24 September, RSFSR State Secretary Gennady Burbulis arrived to Boris Yeltsin, who was on vacation at the Black Sea coast. He brought a document \u201cRussia's Strategy for the Transition Period\u201d, which later received the unofficial name \u201cBurbulis Memorandum\u201d. The \u201cmemorandum\u201d contained an analysis of the situation in the country, proposals on what should be done without delay, prepared by Yegor Gaidar's group. The document concluded that Russia should take the course of economic independence with a \u201csoft\u201d, \u201ctemporary\u201d political alliance with other republics, i.e. to create not a declared, but a truly independent state of Russia. 30 years later, Burbulis recalled that the Burbulis Memorandum was the reform concept of Gaidar's group: There was not any secrecy. First Yegor Gaidar made a report at the State Council of the RSFSR, and then Burbulis spoke at the State Council and said he would make a report for Yeltsin.\nAs the Kommersant newspaper wrote on 7 October 1991, a series of conflicts occurred in the RSFSR government during preparations for the signing of the Treaty on the Economic Community. In his speech to members of the Russian parliament, RSFSR State Secretary Gennady Burbulis declared Russia's special role as the legal successor to the Soviet Union. Accordingly, the ways of drafting agreements with the republics should be determined by the Russian leadership. Instead of the planned order, he suggested signing a political agreement first, followed by an economic one. The newspaper suggested that Burbulis' goal was to persuade Yeltsin not to sign the agreement as it stands at the time. Yegor Gaidar, Alexander Shokhin and Konstantin Kagalovsky were named as the developers of the statement made by Burbulis. In the same time, a group of \"isolationist patriots\" consisting of Mikhail Maley, Nikolai Fedorov, Alexander Shokhin, Igor Lazarev and Mikhail Poltoranin criticized Ivan Silaev and Yevgeny Saburov for wanting to preserve the Soviet Union.\nThe Treaty on Economic Community was signed in Moscow on 18 October 1991 in a single copy in the Russian language by the competent representatives, including Boris Yeltsin.\nBy December 1991, the shortages had resulted in the introduction of food rationing in Moscow and Saint Petersburg for the first time since World War II. Russia received humanitarian food aid from abroad. After the Belavezha Accords, the Supreme Soviet of Russia withdrew Russia from the Soviet Union on 12 December. The Soviet Union officially ended on 25 December 1991, and the Russian Federation (formerly the Russian Soviet Federative Socialist Republic) took power on 26 December. The Russian government lifted price control on 2 January 1992. Prices rose dramatically, but shortages disappeared.\nRussian Federation (1991\u2013present).\nIndependent country and the Commonwealth.\nPost-Soviet countries have signed a series of treaties and agreements to settle the legacy of the former Soviet Union multilaterally and bilaterally in particular status in international organizations, nuclear weapons and debts and assets and general agreements.\nLiberal reforms of the 1990s.\nAlthough Yeltsin came to power on a wave of optimism, he never recovered his popularity after endorsing Yegor Gaidar's \"shock therapy\" of ending Soviet-era price controls, drastic cuts in state spending, and an open foreign trade regime in early 1992 (\"see\" Russian economic reform in the 1990s). The reforms immediately devastated the living standards of much of the population. In the 1990s Russia suffered an economic downturn that was, in some ways, more severe than the United States or Germany had undergone six decades earlier in the Great Depression. Hyperinflation hit the ruble, due to monetary overhang from the days of the planned economy.\nMeanwhile, the profusion of small parties and their aversion to coherent alliances left the legislature chaotic. During 1993, Yeltsin's rift with the parliamentary leadership led to the September\u2013October 1993 constitutional crisis. The crisis climaxed on 3 October, when Yeltsin chose a radical solution to settle his dispute with parliament: he called up tanks to shell the Russian White House, blasting out his opponents. As Yeltsin was taking the unconstitutional step of dissolving the legislature, Russia came close to a serious civil conflict. Yeltsin was then free to impose the current Russian constitution with strong presidential powers, which was approved by referendum in December 1993. The cohesion of the Russian Federation was also threatened when the republic of Chechnya attempted to break away, leading to the First and Second Chechen Wars.\nEconomic reforms also consolidated a semi-criminal oligarchy with roots in the old Soviet system. Advised by Western governments, the World Bank, and the International Monetary Fund, Russia embarked on the largest and fastest privatization ever to reform the fully nationalized Soviet economy. By mid-decade, retail, trade, services, and small industry was in private hands. Most big enterprises were acquired by their old managers, engendering a new rich (Russian tycoons) in league with criminal mafias or Western investors. Corporate raiders such as Andrei Volgin engaged in hostile takeovers of corrupt corporations by the mid-1990s.\nBy the mid-1990s Russia had a system of multiparty electoral politics. But it was harder to establish a representative government because of the struggle between president and parliament and the anarchic party system.\nMeanwhile, the central government had lost control of the localities, bureaucracy, and economic fiefdoms, and tax revenues had collapsed. Still in a deep depression, Russia's economy was hit further by the financial crash of 1998. At the end of 1999, Yeltsin made a surprise announcement of his resignation, leaving the government in the hands of the Prime Minister Vladimir Putin.\nEra of Putin.\nIn 2000, the new acting president won the presidential election on 26 March and won in a landslide four years later. The Second Chechen war ended with the victory of Russia. After the 11 September terrorist attacks, there was a rapprochement between Russia and the United States. Putin created a system of guided democracy in Russia by subjugating parliament, suppressing independent media and placing major oil and gas companies under state control.\nInternational observers were alarmed by moves in late 2004 to further tighten the presidency's control over parliament, civil society, and regional officeholders. In 2008, Dmitri Medvedev, Putin's head of staff, was elected president. In 2012, Putin became president again, prompting massive protests in Moscow.\nRussia's long-term problems include a shrinking workforce, rampant corruption, and underinvestment in infrastructure. Nevertheless, reversion to a socialist command economy seemed almost impossible. The economic problems are aggravated by massive capital outflows, as well as extremely difficult conditions for doing business, due to pressure from the security forces \"Siloviki\" and government agencies.\nDue to high oil prices, from 2000 to 2008, Russia's GDP at PPP doubled. Although high oil prices and a relatively cheap ruble initially drove this growth, since 2003 consumer demand and, more recently, investment have played a significant role. Russia is well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry. Russia hosted the 2014 Winter Olympic Games in Sochi.\nIn 2014, following a controversial referendum, in which separation was favored by a large majority of voters according to official results, the Russian leadership announced the accession of Crimea into the Russian Federation, thus starting the Russo-Ukrainian War. Following Russia's annexation of Crimea and alleged Russian interference in the war in eastern Ukraine, international sanctions were imposed on Russia.\nOn 4 December 2011, elections to the State Duma were held, as a result of which United Russia won for the third time in a row. The official voting results caused significant protests in the country; a number of political scientists and journalists noted various falsifications on election day. In 2012, according to another pre-election agreement, a \"castling\" took place; Putin again became president and Dmitry Medvedev took over as chairman of the government, after which the protests acquired an anti-Putin orientation, but soon began to decline.\nSince 2015, Russia has been conducting military intervention in Syria in support of the Bashar al-Assad regime.\nIn 2018, Putin was re-elected for a fourth presidential term.\nIn 2022, Russia launched the invasion of Ukraine, which was denounced by NATO and the European Union. They aided Ukraine and imposed massive International sanctions during the 2022 Russian invasion of Ukraine. A leading banker in Moscow said the damage might take a decade to recover, as half of its international trade has been lost. Despite international opposition, Russia officially annexed the Donetsk People's Republic and the Luhansk People's Republic, along with most of the Kherson and Zaporizhzhia Oblasts on 30 September. The United Nations have reported that Russia has committed war crimes during the invasion.\nIn March 2023, Russia adopted a Eurasianist, anti-Western foreign policy strategy detailed in a document approved by Putin. The document defined Russia as a \"unique country-civilization and a vast Eurasian and Euro-Pacific power\" that seeks to create a \"Greater Eurasian Partnership\" by pursuing close relations with China, India, countries of the Islamic World and rest of the Global South (Latin America and sub-Saharan Africa). On 23 June 2023, the Wagner Group, a Russian paramilitary organization led by Yevgeny Prigozhin, rebelled against the government. As of August 2023, the total number of Russian and Ukrainian soldiers killed or wounded during the Russian invasion of Ukraine was nearly 500,000.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14117", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=14117", "title": "History of Christianity", "text": "The history of Christianity begins with Jesus, an itinerant Jewish preacher and teacher, who was crucified in Jerusalem c.\u2009AD\u00a030\u201333. His followers proclaimed that he was the incarnation of God and had risen from the dead. In the two millennia since, Christianity has spread across the world, becoming the world's largest religion with over two billion adherents worldwide.\nInitially, Christianity was a mostly urban grassroots movement. Its religious text was written in the first century. A formal church government developed, and it grew to over a million adherents by the third century. Constantine the Great issued the Edict of Milan legalizing it in 315. Christian art, architecture, and literature blossomed during the fourth century, but competing theological doctrines led to divisions. The Nicene Creed of 325, the Nestorian schism, the Church of the East and Oriental Orthodoxy resulted. While the Western Roman Empire ended in 476, its successor states and its eastern compatriot\u2014the Byzantine Empire\u2014remained Christian.\nAfter the fall of Rome in 476, western monks preserved culture and provided social services. Early Muslim conquests devastated many Christian communities in the Middle East and North Africa, but Christianization continued in Europe and Asia and helped form the states of Eastern Europe. The 1054 East\u2013West Schism saw the Byzantine Empire's Eastern Orthodoxy and Western Europe's Catholic Church separate. In spite of differences, the East requested western military aid against the Turks, resulting in the Crusades. Gregorian reform led to a more centralized and bureaucratic Catholicism. Faced with internal and external challenges, the church fought heresy and established courts of inquisition. Artistic and intellectual advances among western monks played a part in the Renaissance of the 12th century and the later Scientific Revolution.\nIn the 14th century, the Western Schism and several European crises led to the 16th-century Reformation when Protestantism formed. Reformation Protestants advocated for religious tolerance and the separation of church and state and impacted economics. Quarrelling royal houses took sides precipitating the European wars of religion. Christianity spread with the colonization of the Americas, Australia, and New Zealand. Different parts of Christianity influenced the Age of Enlightenment, American and French Revolutions, the Industrial Revolution, and the Atlantic slave trade. Some Protestants created biblical criticism while others responded to rationalism with Pietism and religious revivals that created new denominations. Nineteenth century missionaries laid the linguistic and cultural foundation for many nations. \nIn the twentieth century, Christianity declined in most of the Western world but grew in the Global South, particularly Southeast Asia and Sub-Saharan Africa. In the twenty first century, Christianity has become the most diverse and pluralistic of the world's religions embracing over 3000 of the world's languages.\nEarly Christianity (c. 27 \u2013 fourth century).\nFirst century.\nChristianity began with Jesus of Nazareth, a Jewish man and itinerant preacher in Galilee and the Roman province of Judea during the first century. Much about Jesus is uncertain, but his crucifixion c.\u200930 is well attested. The religious, social, and political climate in both regions was extremely diverse and characterized by turmoil with numerous religious and political movements. One such movement, Jewish messianism, promised a messianic redeemer descended from Israel's ancient king, David, who would save Israel. Those who followed Jesus, called disciples, saw him as that Messiah.\nJesus was a prophetic figure who proclaimed the coming kingdom of God. Incarnation, the belief that God (or the Word of God) was embodied in Jesus, and resurrection, the belief that after his crucifixion, he rose from the dead, were Christianity's earliest beliefs. Its earliest rituals were baptism, a rite of initiation, and the communal Eucharist, a celebration of the new covenant at Jesus' last meal before death.\nThe first Christians were predominantly Jewish. They gathered in small groups inside private homes where the typical setting for worship was the communal meal. Elders (called presbyters or bishops) oversaw the small groups, providing for the economic requirements of the meal and charitable distributions. Women comprised significant numbers of Christianity's earliest members. Religion had appeal because women could attain greater freedom through religious activities than Roman customs otherwise permitted. The Pauline epistles recognize their presence in early Christian congregations. Christianity most likely began in Jerusalem with fewer than 1000 believers, which grew to approximately one hundred small household churches, each with an average of seventy members, by the year 100.\nOf the original believers, Jesus kept twelve disciples close to him who became known as the Apostles. Saul of Tarsus, who became Paul the Apostle, was a Jewish Pharisee who had not known Jesus and persecuted early Christians. According to his own account, his life turned in the opposite direction after experiencing a vision of Christ on the road to Damascus. Driven by belief and characterized by passion, the twelve Apostles and Paul identified evangelism as a task to be undertaken, which prompted them to travel through foreign lands sharing their message. Christianity was largely an urban religion that spread along the trade and travel routes into the Jewish diaspora and beyond. The largest cities in the Roman Empire, such as Rome, Alexandria, Antioch, Ephesus, and Carthage, all had Christian congregations by the end of the first century.\nDespite martyrs such as Stephen, the movement grew, reaching Antioch where converts were first called Christian by non-Christians. From Antioch, Barnabas and Paul went to Cyprus, then Asia Minor, where the gospel was received by both Jewish and non-Jewish people. The conversion of Gentiles led to disputes with a group who desired observance of Mosaic law including circumcision. James, brother of Jesus, called the Council of Jerusalem (c.\u200950) which determined that converts should avoid \"pollution of idols, fornication, things strangled, and blood\" but should not be required to follow other aspects of Jewish Law (KJV, Acts 15:20\u201321). As Christianity grew in the Gentile world, it underwent a gradual separation from Judaism. Disagreements over Jewish law, progenitors of Rabbinic Judaism, and insurrections against Rome, contributed to this separation. Nevertheless, Jewish Christianity remained influential in Palestine, Syria, and Asia Minor into the second and third centuries.\nIn the early centuries, the languages most used to spread Christianity were Greek, Syriac (a form of Aramaic), and Latin. Christian writings in Koine Greek, including the four gospels (the accounts of Jesus' ministry), letters of Paul, and letters attributed to other early Christian leaders, were written in the first century and had considerable authority, even in the formative period. Letters sent by Paul the Apostle to Christian communities were circulating in collected form by the end of the first century.\nAnte-Nicene period (100\u2013312).\nThe Christian faith spread east into Syria and Mesopotamia where the population spoke Aramaic, not Greek. Aramaic Christians were in Adiabene (northern Iraq) by the second century. By the second century Christianity was in North Africa, and by the third century, it had spread across the Mediterranean region, from Greece and Anatolia into the Balkans in the East, and as far as Roman Britain in the northwest.\nChristianity's ideology, combined with its social impact, were pivotal to this growth. Christianity offered people new ways of thinking. For example, the idea that the power of God was manifested through Jesus in a reversal of power challenged Roman concepts of hierarchy. In sociologist Rodney Stark's view, Christianity grew because it constituted an \"intense community\" which provided a unique \"sense of belonging\". However, early Christianity demonstrates both inclusion and exclusion. Belief in Jesus was the crucial and defining characteristic for becoming a Christian, and early Christianity was highly inclusive toward anyone who expressed such belief. Ancient philosophy Professor Danny Praet writes that believers were also highly exclusive; they were separated from unbelievers by a strong social boundary based on belief rather than ritual in the traditional Roman fashion. \nWomen are prominent in the Pauline epistles and early Christian art. Church rolls from the second century list groups of women \"exercising the office of widow\".\nMuch of the most virulent anti-Christian criticism of this period was linked to \"female initiative\", which may have contributed to the sporadic but increasing persecution. \nChristians were persecuted by the Roman empire because they did not uphold fundamental beliefs of Roman society and their withdrawal from public religion made them targets of suspicion and rumor. For most of its early centuries, Christianity was tolerated, and episodes of persecution were local. Emperor Nero's persecution of Christians during the mid-1st century was confined to Rome. There were no empire-wide persecutions until the 250s. Official persecution reached its height under Diocletian in 303\u2013311. \nBy 200, Christian numbers had grown to over 200,000 people, and communities with an average size of 500\u20131000 people existed in approximately 200\u2013400 towns. By 250, Christianity had grown to over a million. House churches were then succeeded by buildings designed to be churches, complete with assembly rooms, classrooms, and dining rooms. A more formal church government developed at different times in different locations. Bishops were essential to this development, and they rose in power and influence as they began to preside over larger areas with multiple churches.\nChristian sects, cults, and movements rose during the second and third centuries. Gnostic texts challenged the physical nature of Jesus, Montanism suggested that the apostles could be superseded, and Monarchianism emphasized the unity of God over the Trinity. The four gospels and the letters of Paul were generally regarded as authoritative, but other writings, such as the Book of Revelation and the epistles to the Hebrews, James, and 1 John, were assigned different degrees of authority. In the face of such diversity, unity was provided by the shared scriptures and bishops.\nThe fluidity of the New Testament in the first century does not seem to have affected belief in the Trinity as it connected to Christology and salvation. Christianity's central mystery, the Trinity, defines the Holy Spirit, Father, and Son as one God in three persons. However, there is an evolution of thought in the Patristic writings, then the development of the canon, and later in the theological controversies of the fourth century, that shaped the concept's development and gradually created a more technical Trinitarian vocabulary.\nThere are few remnants of early Christian art, but the oldest, dated between 200 and 400, have been found in the catacombs of Rome. It typically fused Graeco-Roman style and Christian symbolism: the most common image was Jesus as the good shepherd.\nLate antiquity (313 \u2013 c. 600).\nLate Antiquity was an age of change in which Christianity became a permitted religion, then a favored one that transformed in every capacity. In 313, the emperor Constantine, a self-declared Christian, issued the Edict of Milan expressing tolerance for all religions. Thereafter, he supported Christianity by giving bishops judicial power and establishing them as legally equal to polytheistic priests. He devoted personal and public funds to building churches and endowed them with funds to support their clergy. There were churches in the majority of Roman cities by the end of the fourth century. \nChristian art, architecture, and literature blossomed under Constantine. The basilica, a type of Roman municipal court hall, became the model for Christian architecture. Frescoes, mosaics, statues, and paintings blended classical and Christian styles. Similarly, a hybrid form of poetry written in classical styles with Christian concepts emerged. In the late fourth century, Jerome was commissioned to translate the Greek biblical texts into the Latin language; this translation was called the Vulgate. Church Fathers of this period, such as Augustine of Hippo, John Chrysostom, Gregory of Nyssa, Athanasius of Alexandria, Basil of Caesarea, Gregory of Nazianzus, Cyril of Alexandria, and Ambrose of Milan, wrote vast numbers of works.\nThe ascetic ideal of these early Church Fathers was also embraced by monasticism, which had begun earlier in Syria, and was key to the development of Christianity. In Late Antiquity, monastic communities became associated with the urban holy places in Palestine, Cappadocia, Italy, Gaul, and Roman North Africa. In the 370s, Basil the Great founded the Basileias, a monastic community in Caesarea (Mazaca) which developed the first health care system for the poor, a forerunner of modern public hospitals.\nBefore the fourth century, Judaism had been an approved religion, while Christianity was persecuted as an illegal superstition; during the fourth century, Christianity became favored by emperors and Judaism came to be seen as similar to heresy. Still, Augustine of Hippo argued that Jews should not be killed or forcibly converted; they should be left alone because they preserved the teachings of the Old Testament and were \"living witnesses\" of the New Testament. Aside from the Visigothic Kingdom, Jews and Christians peacefully coexisted, for the most part, into the High Middle Ages.\nConstantine and his successors attempted to fit the church into their political program. Church leaders responded with the first fully articulated limitation on secular authority based on the church as a separate entity, arguing that the church was not part of the empire so much as the empire was part of the universal church. During this period, the successors to Peter as Bishop of Rome (known as the Pope) had limited influence, and they lacked the power to break free of secular involvement in church affairs. However, papal influence rose as eastern patriarchs looked to the Pope to resolve disagreements.\nGeographical spread.\nChristianity grew rapidly throughout this period. Christians in Persia, (present-day Iraq), were deeply persecuted in Late Antiquity, but their numbers still grew. In the fourth century the percentage of Christians was as high in the Sasanian Empire as in the Roman Empire. A form of Christianity made inroads among Arabs in Palestine, Yemen, and Arabia. Even as the Huns, Ostrogoths, Visigoths, and Vandals caused havoc in the Roman Empire in the fourth and fifth centuries, many of them converted to Christianity. Syria was home to a thriving theological school. The gospel was first brought to Central Asia and China by Syriac-speaking missionaries.\nChristian institutions in Asia or East Africa never developed the kind of influence that the European churches and Byzantium held. Even so, in 301, the Kingdom of Armenia became the first nation to adopt Christianity as its state religion, soon followed by Caucasian Albania and the East African Kingdom of Aksum. Christianity, a minority faith in Britain since the second century, began to be displaced by Anglo-Saxon paganism in the fifth century. However, this process reversed after the Gregorian mission of 597. In the early fifth century, missionaries began converting Ireland.\nReligious violence.\nScholars have traditionally interpreted the many Late Antique writings alleging violent acts by Christians against paganism as evidence of a widespread historical reality. This has been questioned in recent decades using modern archaeology. For example, temple destruction is attested in 43 cases in the written sources, but only four are supported by archaeological evidence. These studies indicate narrated violence was misattributed or over-reported, and though there were violent incidents, they were few, local, limited, and at the ordinary level of occurrence in Roman society. In addition, violent writings, traditionally seen as encouraging violent acts, were composed after, not before, events. Many are hagiography depicting the Christian God defeating the pagan gods in Heaven. Michael Gaddis says these writings were used to connect Christianity's heavenly \"triumph\" with the new identity Christians wrote for themselves as 'victors'.\nReligious violence between pagans and Christians may not have been a general phenomenon, but from the time of Constantine, there was virulent legal hostility toward certain pagan practices. Blood sacrifice, which had been a central rite of virtually all religious groups in the pre-Christian Mediterranean, disappeared by the end of the fourth century due to hostile imperial laws. Still, polytheism remained active into the fifth century, and in some places, into the ninth, even though popular support for the polytheistic religions had been in decline since the second century BC.\nConstantine generally supported resolving religious disputes through debate, not force, but in 304, Donatists formed a schism in North Africa, refusing, often violently, to accept back into the church those who had apostatized during Diocletian's persecution. The emperor attempted to impose public order through force, but it was ineffective, and in 321, Constantine decided no more punishment would be given to Donatists, but their Catholic victims would become venerated as Christian martyrs. In 408, Augustine defended the government's violent response asserting that coercion could not produce genuine conversion, but it could soften resistance and make conversion possible. According to Peter Brown, Augustine thus \"provided the theological foundation for the justification of medieval persecution\".\nHeresies, schisms and councils.\nRegional variants of Christianity produced diverse and sometimes competing theologies. Ancient Christians identified any practice or doctrine which differed from apostolic tradition as heresy. The number of laws directed at heresy indicate it was a much higher priority than paganism for Christians of this period.\nFor decades, Arianism embroiled the entire church, laity (non-clergy) and clergy alike, in arguing whether Jesus' divinity was equal to the Father's. The First Council of Nicaea in 325 attempted to resolve the controversy with the Nicene Creed, but some refused to accept it. Along the Eastern Mediterranean, where Christian factions struggled without resolution, Christian communities were weakened, affecting their long-term survival.\nDebate became a primary method of competition between pagans and Christians. Persuasion, rhetoric and polemics centered on the true meaning of logos (the word). Pagans asserted its correct meaning was allegorical and could be found in ancient myths and poetics. Christians asserted Jesus as the living word in their first true ontologies.\nBiblical commentators between 300 and 600 focused more on aiding ordinary Christians whose main concern was sin and salvation. Christian baptism was distinctive and demonstrated how Christians understood these concepts in terms of the death of Christ. As theology evolved, it held to the paradox of God's incarnation, as well as the decisive human contribution to redemption seen in Jesus.\nChristian scriptures were formalized as the New Testament and distinguished from the Old Testament by the fourth century. Despite agreement on these texts, differences between East and West were becoming evident. The West was solidly Nicean while the East was largely Arian. The West condemned Roman culture as sinful and resisted state control, whereas the East harmonized with Greek culture and aimed for unanimity between church and state. The marriage of clerics was accepted in the East but forbidden in the West. The East advocated sharing the government of the church between five church leaders, arguing that the Patriarchs of Constantinople, Alexandria, Antioch, and Jerusalem were equal to the Pope. Rome asserted that successors to Peter had superiority.\nControversies over how Jesus' human and divine natures coexisted peaked when Nestorius declared Mary as the mother of Jesus' humanity, not his divinity, thereby giving Jesus two distinct natures. This led to a series of ecumenical councils: the Council of Ephesus was the church's third council, and it condemned Nestorius. Held in 431, the church in the Persian Empire refused to recognize its authority. This led to the first separation between East and West. Two groups, one mostly Persian and the other Syrian, separated from Catholicism; Persians became the Church of the East (also known as the Assyrian, Nestorian, or Persian Church), while the majority of Christians in Syria and Mesopotamia became the Syrian Orthodox Church (Jacobite). This cut off the flourishing school of Syrian Semitic Christian theologians and writers from the rest of Christendom. The Church of the East lay almost entirely outside the Byzantine Empire. It became the principal Church in Asia in the Middle Ages.\nIn 451, the fourth council was the influential Council of Chalcedon. While most of Christianity accepted the Chalcedonian Definition, which emphasizes that the Son is \"one person in two natures,\" there were those who found that description too close to the duality of Nestorianism, so after 484, they separated into Oriental Orthodoxy that sees only \"One Nature of God the Incarnate Logos\".\nAfter 476.\nFor five centuries after the fall of the Western Roman Empire in 476, Western culture and civilization were primarily preserved and passed on by monks. Those in the Eastern Roman Empire continued to see themselves as a Roman Empire with an emperor, a civil government, and a large army.\nThe religious policies of the Eastern Roman Emperor Justinian I (r.\u2009527\u00a0\u2013\u00a0565) reflected his conviction that the unity of the Empire presupposed unity of faith: he persecuted pagans and religious minorities, purging the government and church bureaucracies of those who disagreed with him. Justinian contributed to cultural development, and integrated Christian concepts with Roman law in his \", which remains the basis of civil law in many modern states.\nIn Gaul, the Frankish king Clovis I converted to Catholicism; his kingdom became the dominant polity in the West in 507, gradually converting into a Christian kingdom over the next centuries. Papal influence rose as the church increasingly relied on Rome to resolve disagreements. Pope Gregory I gained prestige and power for the papacy by leading the response to invasion by the Lombards in 592 and 593, reforming the clergy, standardizing music in worship, sending out missionaries, and founding new monasteries. Until 751, the Pope remained a subject of the Byzantine emperor.\nEarly Middle Ages (c. 600\u20131000).\nBy the early 600s, Christianity had spread around the Mediterranean. However, between 632 and 750, Islamic caliphates conquered the Middle East, North Africa, and the Iberian Peninsula. Most urban Asian churches disappeared, but some Christian communities in remote areas survived. In the same period, war on multiple fronts contributed to the Eastern Roman Empire becoming the independent Byzantine Empire. Until the eighth century, most of Western Europe remained largely impoverished, politically fragmented, and dependent on the church.\nDuring this period, invasion, deportation, and neglect left some communities without a church, allowing Christianity to syncretize with local pagan traditions. Nevertheless, \"Christendom\", the notion of all Christians united as a polity, emerged at the end of this age.\nMonasticism and art.\nUntil the end of the Early Middle Ages, Western culture was preserved and passed on primarily by monks known as \"regular clergy\" because they followed a : a rule. The rule included chastity, obedience and poverty sought through prayer, memorization of scripture, celibacy, fasting, manual labour, and almsgiving. A monastery's location (whether it was remote or by a city), the monastic order it belonged to, its economic resources, and its local leadership determined whether it focused primarily on charitable work or spiritual pursuits and self-sufficiency.\nMonasteries served as orphanages and inns for travelers, and many provided food for those in need. They supported literacy, practiced classical arts and crafts, and copied and preserved ancient texts in their scriptoria and libraries. Dedicated monks created illuminated manuscripts. From the sixth to the eighth centuries, most schools were connected to monasteries, but methods of teaching an illiterate populace could also include mystery plays, vernacular sermons, saints' lives in epic form, and artwork.\nThis was an age of uncertainty, and the role of relics and holy men able to provide special access to the divine became increasingly important. Donations funding prayers for the dead provided an ongoing source of wealth. Monasteries became increasingly organized, gradually establishing their own authority as separate from political and familial authorities, thereby revolutionizing social history. Medical practice was highly important, and medieval monasteries were known for their public hospitals, hospices, and contributions to medicine. The sixth-century Rule of Saint Benedict has had extensive influence.\nThe East developed an approach to sacred art unknown in the West, adapting ancient portraiture in icons as intercessors between God and humankind. In the 720s, the Byzantine Emperor Leo banned the pictorial representation of Christ, saints, and biblical scenes, and destroyed much early representational art. The West condemned the Byzantine iconoclasm of Leo and some of his successors. By the tenth and early eleventh centuries, Byzantine culture began to recover its artistic heritage.\nRegional differences.\nEastern Europe had been exposed to Christianity during Roman rule, but it was Byzantine Christianity, brought by the ninth-century saints Cyril and Methodius, that was integral to the formation of its modern states. Dukes and kings used the new faith to solidify their position and promote unity, while some directly enforced it with new laws, building churches, and establishing monasteries. The brothers developed the Glagolitic alphabet to translate the Bible into the local language. Their disciples then developed the Cyrillic script, which spread literacy and became the cultural and religious foundation for all Slavic nations.\nIn 635, the Church of the East brought Christianity into China. Emperor Taizong decreed that the Christian faith was allowed and its license was copied onto the Sianfu stele. It spread into northwestern China, Khotan, Turfan, and south of Lake Balkash in southeastern Kazakhstan, but its growth was halted in 845 by Emperor Wuzong of Tang who favoured Taoism. The Church of the East evangelized all along the Silk Road and was instrumental in converting some of the Mongolic and Turkic peoples. After 700, when much of Christianity was declining, there were flourishing Christian societies along all the main trade routes of Asia, South India, the Nubian kingdoms, Ethiopia, and the Caucasus region.\nIn Western Europe, canon law was instrumental in developing key norms concerning oaths of loyalty, homage, and fidelity. These norms were incorporated into civil law where traces remain. Within the tenets of feudalism, the church created a new model of consecrated kingship unknown in the East, and in 800, Clovis' descendant Charlemagne became its recipient when Pope Leo III crowned him emperor. Charlemagne engaged in a number of reforms which began the Carolingian Renaissance, a period of intellectual and cultural revival. His crowning set the precedent that only a pope could crown a Western emperor enabling popes to claim emperors derived their power from God through them. The Papacy became free from Byzantine control, and the former lands of the Exarchate became States of the church. However, the papacy was still in need of aid and protection, so the Holy Roman emperors often used that need to attempt domination of the Papacy and the Papal States. In Rome, the papacy came under the control of the city's aristocracy.\nIn Russia, the baptism of Vladimir of Kiev in 989 is traditionally associated with the conversion of the Kievan Rus'. Their new religious structure included dukes maintaining control of a financially-dependent church. Monasticism was the dominant form of piety for both peasants and elites who identified as Christian while retaining many pre-Christian practices.\nViking raids in the ninth and tenth centuries destroyed many churches and monasteries, inadvertently leading to reform. Patrons competed in rebuilding so that \"by the mid-eleventh century, a wealthy, unified, better-organized, better-educated, more spiritually sensitive Latin Church\" resulted. There was another rise in papal power in the tenth century when William IX, Duke of Aquitaine, and other powerful lay founders of monasteries, placed their institutions under the protection of the papacy.\nHigh Middle Ages (c. 1000\u20131300).\nMembership in the Christendom of this age began with baptism at birth. Every follower was supposed to have some knowledge of the Apostles' Creed and the Lord's Prayer, to rest on Sunday and feast days, attend mass, fast at specified times, take communion at Easter, pay various fees for the needy, and receive last rites at death. From 1198\u20131216, Pope Innocent III raised the papacy's influence to its greatest height.\nThe High Middle Ages saw the formation of several fundamental doctrines, such as the seven sacraments, the just reward for labour, \"the terms of Christian marriage, the nature of clerical celibacy and the appropriate lifestyle for priests\". Heresy was more precisely defined. Purgatory became an official doctrine. In 1215, confession became required for all. The rosary was created after veneration of Mary, mother of Jesus became a central aspect of the period.\nBeginning at Cluny Abbey (910), which used Romanesque architecture to convey a sense of awe and wonder and inspire obedience, monasteries gained influence through the Cluniac Reforms. However, their cultural and religious dominance began to decline in the mid-eleventh century when secular clergy, who were not members of religious orders, rose in influence. Monastery schools lost influence as cathedral schools spread, independent schools arose, and universities formed as self-governing corporations chartered by popes and kings. Canon and civil law became professionalized, and a new literate elite formed, further displacing monks. Throughout this period, the clergy and the laity became \"more literate, more worldly, and more self-assertive\".\nCentralization, expulsions and Investiture.\nThe reform of Pope Gregory VII (1073\u20131085) began \"a new period in church history\" by pressing for an end to simony (the sale of church offices), the enforcement of clerical celibacy, and the establishment of papal supremacy. Previously, the power of kings and emperors had been (at least partly) founded on connection to the sacred. Gregorian Reform intended to divest Western rule of that sacramental character, free the church from state control, and establish the preeminence of the church. The reform process reinforced the pope's temporal power, enabling a reorganization of the administration of the Papal States which brought a substantial increase in wealth, consolidated territory, centralized authority, and established a bureaucracy.\nIn the preceding era of raids by Muslim pirates and Viking warriors, church leaders had been forced to seek protection by nobles who then saw it as their right to control the institutions they protected. In 1061, Pope Nicholas II moved to protect the papacy from secular control by establishing that popes could only be elected by a College of Cardinals, however, both the nobles and the church still claimed the right to appoint bishops. This led to the Investiture Controversy, a conflict between the Holy Roman Emperor Henry IV and Pope Gregory VII over the secular appointment of bishops and abbots and control of their revenues in the Holy Roman Empire. For the church, ending lay investiture would support independence from the state, encourage reform, and provide better pastoral care. For the kings, ending lay investiture meant the power of the Holy Roman Emperor and the European nobility would be reduced.\nThe \"\" of 1075 declared that the pope alone could invest bishops. Disobedience to the Pope became equated with heresy; when Henry IV rejected the decree, he was excommunicated, which contributed to a civil war. A similar controversy occurred in England. Struggles over division of power between church and state continued throughout the medieval era.\nSchism, crusade, spread, and retraction.\nThe Church of the East, which had separated after Chalcedon, survived against the odds with help from Byzantium. At the height of its expansion in the thirteenth century, the Church of the East stretched from Syria to eastern China and from Siberia to southern India and southern Asia. Along with geographical separation, there had long been many cultural differences, geopolitical disagreements, and a lack of respect between east and west. Their second separation took place in 1054 when the church within the Byzantine Empire formed Byzantine Eastern Orthodoxy, which thereafter remained in communion with the Ecumenical Patriarchate of Constantinople, not the Pope.\nChristianity was declining in Mesopotamia and inner Iran. As churches in Egypt, Syria, and Iraq became subject to fervently Islamic militaristic regimes, Christians were designated as \"dhimmi\", a status that guaranteed their protection but enforced their legal inferiority. Different communities adopted various survival strategies: some withdrew from interaction, others converted to Islam, and others sought outside help. The Byzantine emperor Alexios I Komnenos asked Pope Urban II for help with the Seljuk Turks in 1081, and in 1095, Urban asked European Christians to \"go to the aid of their brethren\" in counterattack against the inroads of Islam.\nUrban's message had great popular appeal. Drawing on powerful and prevalent aspects of folk religion, it connected pilgrimage, charity, and absolution with a willingness to fight. It gave ordinary Christians a tangible means of expressing brotherhood with the East and carried a sense of historical responsibility. Tens of thousands answered. Among the first was Peter the Hermit who led the People's Crusade to a disastrous end in 1096. Eight Crusades, which lasted from 1096 to 1272, had little to no overall military success, failed as a religious endeavor, contributed to the development of national identities in European nations and, eventually, increased division with the East. Scholars struggle with no agreement on estimates of how many died.\nThe cult of chivalry, which upheld the ideal of the Christian knight, emerged with powerful and wide-spread social and cultural influence before its decline during the 1400s. Another significant effect of the Crusades was the invention of the indulgence.\nThe Christianization of Scandinavia occurred in two stages: first, in the ninth century, missionaries operated without secular support; then, a secular ruler would begin to oversee Christianization in their territory until an organized ecclesiastical network was established. By 1350, Scandinavia was an integral part of Western Christendom.\nRenaissance, science and technology.\nThe Christian wars of reconquest, which lasted over 200 years, had begun in Italy in 915 and in Spain in 1009 to retake territory lost to Muslims, causing fleeing Muslims in Sicily and Spain to leave behind their libraries. Between 1150 and 1200, monks searched those libraries and found the works of Aristotle, Euclid, and other ancient writers.\nThe West's rediscovery of the complete works of Aristotle led to the Renaissance of the twelfth century. It also created conflict between faith and reason, resolved by a revolution in thought called scholasticism. The scholastic writings of Thomas Aquinas impacted Catholic theology and influenced secular philosophy and law into the modern day.\nMonks revived the scientific study of natural phenomena, which laid the necessary foundation that eventually led to the Scientific Revolution in the West. There was no parallel Renaissance in the East.\nByzantine art exerted a powerful influence on Western art in the twelfth and thirteenth centuries. Gothic architecture, intended to inspire contemplation of the divine, began in the same centuries.\nThe Cistercian movement was a wave of monastic reform after 1098. Cistercians were instrumental in promoting technological advancement and were among the best industrialists of the Middle Ages. Of the 740 twelfth-century Cistercian monasteries, nearly all possessed a water wheel used to develop innovative hydraulic engineering techniques, water circulation systems for central heating, produce olive oil or forge metal and produce iron. They taught and practiced advanced farming techniques such as crop rotation and were skilled metallurgists.\nChallenges and repression.\nThe twelfth century saw a change in the goal of a monk from contemplative to active reformer. Among these new activist preachers was Dominic who founded the Dominican Order and was significant in opposing Catharism. In 1209, Pope Innocent III and King Philip II of France initiated the Albigensian Crusade against Catharism. The campaign took a political turn when the king's army seized and occupied strategic lands of nobles who had not supported the heretics but had instead been in the good graces of the Church. It ended in 1229 with a treaty which brought the region under the rule of the French king, creating southern France, while Catharism continued until 1350.\nThe Medieval Inquisition, which lasted from 1184 to the 1230s, was initiated by Innocent III in response to increasing concerns over heresy and public disorder. These courts were established when someone was accused, then after prosecution, the court was dissolved. Unlike the later modern inquisitions, these medieval courts did not have the power of prosecution on their own; they were dependent upon the civil courts. Though these courts had no joint leadership, nor joint organization, the Dominican Order held the primary responsibility for conducting inquisitions. Between 8,000 and 40,000 people were brought to interrogation and sentencing, and death sentences were relatively rare. The penalty imposed most often was an act of penance which might include public confession.\nBishops were the lead inquisitors, but they did not possess absolute power, nor were they universally supported. Inquisition became stridently contested as public opposition grew and riots against the Dominicans occurred. The Fourth Lateran Council of 1215 empowered inquisitors to search out moral and religious \"crimes\" even when there was no accuser. In theory, this granted them extraordinary powers. In practice, without sufficient local secular support, their task became so overwhelmingly difficult that inquisitors were endangered and some were murdered.\nFrom 1170-80, the Jewish philosopher Moses ben Maimon (commonly known as Maimonides) wrote his fourteen-volume code of Jewish law and ethics, titled the \"Mishneh Torah\". A turning point in Jewish-Christian relations occurred when the Talmud was put \"on trial\" in 1239 by the French King Louis IX and Pope Gregory IX because of contents that mocked the central figures of Christianity. Talmudic Judaism came to be seen as so different from biblical Judaism that old Augustinian obligations to leave the Jews alone no longer applied. A rhetoric with elaborate stories casting Jews as enemies accused of ritual murder, blood libel, and desecration of the Christian eucharist host grew among ordinary folk. The spread of the Black Death led to attacks on Jewish communities by people who blamed them for the epidemic. Jews often acted as financial agents for the nobility, providing them loans with interest while being exempt from certain financial obligations. This attracted jealousy and resentment. Count Emicho of Leiningen massacred Jews in search of supplies and protection money, while the York massacre of 1190 also appears to have originated in a conspiracy by local leaders to liquidate their debts.\nAs newly centralized states demanded greater cultural conformity from their citizens, canon laws that left out Christianity's earlier principles of equity and inclusivity were created. The medieval church never officially repudiated Augustine's doctrine of protecting the Jews, but legal restrictions increasingly enabled treating them as outsiders. Throughout the medieval era, local rulers evicted Jews from their lands and confiscated property.\nThe nobility of Eastern Europe prioritized subduing the Balts, the last major polytheistic population in Europe, over crusading in the Holy Land. In 1147, the \"Divina dispensatione\" gave these nobles indulgences for the first of the Northern Crusades, which intermittently continued, with and without papal support, until 1316. The clergy pragmatically accepted the forced conversions the nobles perpetrated despite continued theological emphasis on voluntary conversion.\nRenaissance and Reformation (c. 1300\u20131650).\nDivision in the West.\nThe many calamities of the \"long fourteenth century\", which included plague, famine, wars, and social unrest, led European people to believe the end of the world was imminent. This belief ran throughout society and became intertwined with anti-clerical and anti-papal sentiments. Criticism of the church became an integral part of late medieval European life, and was expressed in both secular and religious writings, and movements of heresy or internal reform. Most attempts at reform between 1300 and 1500 failed.\nIn 1309, Pope Clement V fled Rome's factional politics by moving to Avignon in southern France. By leaving Rome and the \"seat of Peter\" behind, this Avignon Papacy, consisting of seven successive popes, unintentionally diminished papal prestige and power. Pope Gregory XI returned to Rome in 1377. After Gregory's death the following year, the papal conclave elected Urban VI to succeed him, but the French cardinals disapproved and elected Robert of Geneva instead. This began the Western Schism, during which there was more than one pope. In 1409, the Council of Pisa's attempted resolution resulted in the election of a third separate pope. The schism was finally resolved in 1417, with the election of Pope Martin V.\nThroughout the Late Middle Ages, the church faced powerful challenges and vigorous political confrontations. The English scholastic philosopher John Wycliffe (1320\u20131384) urged the church to embrace its original simplicity, give up its property and wealth, end subservience to secular politics, and deny papal authority. Wycliffe's teachings were condemned as heresy, but he was allowed to live out the last two years of his life in his home parish. In 1382, the first English translation of the Bible, known as Wycliffe's Bible, was published. Wycliffe's teachings influenced the Czech theologian Jan Hus (1369\u20131415) who also spoke out against what he saw as corruption in the church. Hus was convicted of heresy and burned at the stake. This was the impetus for the Bohemian Reformation and led to the Hussite Wars.\nMeanwhile, a vernacular religious culture called the \"Devotio Moderna\" attempted to work toward a pious society of ordinary people. Through the Dutch scholar Desiderius Erasmus Roterodamus (1466\u20131536), Christian humanism grew and impacted literature and education. Between 1525 and 1534, William Tyndale used the Vulgate and Greek texts from Erasmus to create the Tyndale Bible. King James commissioned the King James Version in 1604, using all previous versions in Latin, Greek, and English as sources. It was published in 1611.\nEast and Renaissance.\nIn 14th-century Byzantium, St. Gregory Palamas, defended hesychast spirituality and the Orthodox understanding of God against the criticisms of Barlaam a Calabrian humanist philosopher, by writing his most influential work, \"Triads\", in 1341.\nA reunion agreement between the Orthodox and Catholic churches in 1452 was negated by the Fall of Constantinople to the Ottoman Empire in 1453, which sealed off Orthodoxy from the West for more than a century. Islamic law did not acknowledge the Byzantine church as an institution, but a concern for societal stability allowed it to survive. Financial handicaps, constant upheaval, simony, and corruption impoverished many, and made conversion an attractive solution. This led to the state confiscating churches and turning them into mosques. The patriarchate became a part of the Ottoman system under Suleiman the Magnificent (1520\u20131566), and by the end of the sixteenth century, widespread desperation and low morale had produced crisis and decline. When Cyril I Loukaris (1572 \u2013 1638) became Patriarch in 1620, he began leading the church toward renewal. A shared hostility towards Catholicism led Cyril to reach out to the Protestants of Europe and to be deeply impacted by their Reformation doctrines. Protestant pressure produced the Lukaris Confession embracing Calvinism.\nThe flight of Eastern Christians from Constantinople, as well as the manuscripts they carried with them, were important factors in stimulating literary renaissance in the West. The Catholic Church became a leading patron of art and architecture, commissioning work and supporting renowned artists. Even while fifteenth-century popes struggled to reestablish papal authority, the Renaissance Papacy transformed Rome by rebuilding St. Peter's Basilica and establishing the city as a prestigious centre of learning. Reformation Protestants condemned these popes as corrupt for their lack of chastity, nepotism, and selling \"hats and indulgences\".\nIn Russia, Ivan III of Russia adopted the style of the Byzantine imperial court to gain support among the Rus' elite who saw themselves as the new 'chosen' and Moscow as the New Jerusalem. Jeremias II (1536\u20131595), the first Orthodox patriarch to visit north-eastern Europe, founded the Orthodox Patriarchate of Russia during his journey.\nThe sixteenth-century success of Christianity in Japan was followed by severe repression, such as the crucifixion of the 26 Martyrs of Japan.\nColonialism and missions.\nColonialism, which began in the fifteenth century, originated either on a militaristic/political path, a commercial one, or with settlers who wanted land. Christian missionaries soon followed with their own separate agenda. Relations between missionaries and colonialist companies, politicians, settlers, and traders were often antagonistic, because mission and colonial interests were in opposition to each other. Missionaries promoted human development and provided healthcare and education which colonial governments were unwilling or unable to provide.\nBetween 1500 and 1800, Catholic Christianity gained followers worldwide through missionaries from the Spanish, Portuguese, and French empires. During the Hispanic colonization of the Americas, Latin America largely became a New World form of Iberian Catholicism, while the merging of native and Spanish traditions also created a multitude of indigenous Christianities. \nMissionaries relied on colonial governments for protection, transportation, and status, so many of them cooperated with and benefitted from colonialism. Many accepted the social views of the day which saw Western culture as superior; they encouraged the adoption of European practices and values to the detriment of indigenous customs and the disruption of local societies. Some missionaries participated in forced relocation programs and boarding school systems that separated children from their families and cultures. In Greater Syria during WWI, French missionaries used their local contacts to supply intelligence to French authorities. Jesuits tried to suppress the trade in Amerindian slaves in the Caribbean, but became one of the largest holders of black slaves.\n\"There were intense theological, moral, and juridical debates about the status and nature of human beings throughout [this] period. In the Spanish dominions, forceful denunciations of the ill-treatment of indigenous peoples in the Americas sometimes prompted new laws and measures aimed at regulating and controlling these abuses.\" This led many missionaries to openly oppose colonialism. Some actively worked to maintain the rights of indigenous peoples, advocated for their protection, and opposed oppressive colonial policies. These missionaries respected local cultural structures, maintained local language and customs, and advocated for a \"self-supporting, self-governing, and self-propogating church\". Missionaries like John Mackenzie fought for equal legal protection and protected native lands. Some missionaries can be seen as the forerunners of today's human-rights-advocates smuggling out reports of colonial abuses to their media contacts willing to expose injustices. Religious societies such as the Moravians and the Quakers opposed slavery and worked toward abolition.\nWomen, witch frenzy, and Modern Inquisition.\nWomen in the Middle Ages were considered incapable of moral judgment and authority. However, there were women who became distinguished leaders of nunneries, exercising the same powers and privileges as their male counterparts, such as Hildegard of Bingen (d. 1179), Elisabeth of Sch\u00f6nau (d. 1164/65), and Marie d'Oignies (d. 1213). Hildegard began writing the first of her three-volume theology in 1141.\nAlthough the Catholic Church had long ruled that witches did not exist, the conviction that witches were both real and malevolent developed throughout fifteenth-century European society. No single cause of the \"witch frenzy\" which followed is known, although the Little Ice Age is thought to have been a factor. In Finnish scholar Marko Nenonen's view: \"Most likely... there was no single economic, social or ideological (not even political) factor leading to witch-hunts. One has to face the fact that behind most accusations [there were] personal and private motives, of a very malicious nature...\" Between 100,000 and 200,000 people were accused most often by fellow villagers. Approximately 80% of the accused were women; most were acquitted; most trials were civil trials. Inquisitions lessened the impact by requiring strict evidence. From 1561 to 1670, it is estimated that between 40,000 and 50,000 people were executed.\nBetween 1478 and 1542, the Spanish and Portuguese inquisitions were initially authorized by the church but soon became state institutions. Authorized by Pope Sixtus IV in 1478, the Spanish Inquisition was established to combat fears that Jewish converts were conspiring with Muslims to sabotage the new state. Five years later, a papal bull conceded control of the Spanish Inquisition to Spanish monarchs, making it the first national, unified, centralized institution of the nascent Spanish state. The monarchy centralized state power by absorbing and adapting military orders, Inquisitorial courts and police organizations for political purposes.\nThe Portuguese Inquisition, controlled by a state board of directors, incorporated anti-Judaism before the end of the fifteenth century. Many of these forcibly converted Jews, known as New Christians, fled to Portuguese colonies in India, where they subsequently suffered as targets of the Goa Inquisition. The bureaucratic and intellectual Roman Inquisition, best known for its condemnation of Galileo, served the papacy's political aims in Italy.\nReformation.\nSupported by secular and canon law, the fourteenth century was among the most oppressive for minorities in Western Europe. Protests against the church led to the Protestant Reformation which began in 1517 when the Catholic monk Martin Luther nailed his \"Ninety-five Theses\" to the church door in Wittenberg. Luther challenged the nature of the church's role in society and its authority. For Catholics, authority meant the Pope. For the protesters, authority was found in the priesthood of believers and in Scripture. Luther asserted there were two realms of human existence, the secular and the sacred, that neither should be allowed to dominate the other, and only secular authority had the right to use force. Edicts issued at the Diet of Worms in 1521 condemned Luther.\nAfter protracted and acrimonious struggle, three religious traditions emerged alongside Roman Catholicism: the Lutheran, Reformed, and Anglican traditions. Reformed churches, formed by followers of theologian John Calvin, argued that the church had the right to function without interference from the state, and they advocated for a constitutional representative government in both the church and in society. Puritans and other Dissenter groups in England, Huguenots in France, \"Beggars\" in Holland, Covenanters in Scotland who produced Presbyterianism, and Pilgrim Fathers of New England are Reformed churches that trace their theological roots to Calvin. The Anglican church was first created as the Church of England by Henry VIII (1491 \u2013 1547) who severed it from papal authority and appointed himself Supreme Head of the Church of England. Henry preserved Catholic doctrine and the church's established role in society.\nThe Roman Catholic Church responded in the Counter-Reformation, spearheaded by ten reforming popes between 1534 to 1605. The Council of Trent (1545\u20131563) answered each Protestant claim, and laid the foundation of modern Catholic policies. New monastic orders were formed, including the Society of Jesus \u2013 the \"Jesuits\" \u2013 who adopted military-style discipline and strict loyalty to the Pope. Monastic reform also led to the Spanish mystics and the French school of spirituality, as well as the Uniate church which used Eastern liturgy but recognized the authority of Rome.\nQuarreling royal houses, already involved in dynastic disagreements, became polarized into the two religious camps. In 1562, France became the centre of a series of wars, of which the largest and most destructive was the Thirty Years' War (1618\u20131648). While some scholars argue that these wars were varieties of the just war tradition for religious liberty and freedom, most historians argue that the wars were also about nationalistic state-building and economics.\nModern period (1650\u20131945).\nIdeological movements.\nThe era of political absolutism followed the breakdown of Christian universalism in Europe. Abuses from absolutist Catholic kings gave rise to a virulent critique of Christianity that first emerged among the more extreme Protestant reformers in the 1680s as an aspect of the Age of Enlightenment. For 200 years, Protestants had been arguing for religious toleration, and by the 1690s, secular thinkers were rethinking the state's reasons for persecution, and they too began advocating for religious toleration. Concepts of freedom of religion, speech, and thought began being established in the West.\nSecularisation spread at every level of European society. Pioneered by Protestants, Biblical criticism advocated historicism and rationalism to make study of the Bible more scholarly and secular in the 1700s. In reaction to rationalism, pietism, a holiness movement within Lutheranism, began in Europe and spread to the Thirteen Colonies where it contributed to the First Great Awakening, a religious revival of the 1700s. Pietist Moravians came to Georgia in 1732 where they influenced John Wesley, an Anglican missionary in Savannah. After returning to England, Wesley began preaching in open-air meetings, leading to the creation of the Methodist church. In the colonies, Presbyterians and Baptists contributed to revival, and to divisions over it, which formed political parties and lent crucial support for the American Revolution. Some radical revolutionaries violently sought the dechristianization of France during the French Revolution leading the Eastern Orthodox Church to reject Enlightenment ideas as too dangerous to embrace.\nThe rise of Protestantism contributed to the conceptualization of human capital, development of the Protestant work ethic, the European state system, modern capitalism in Northern Europe, and overall economic growth. However, urbanization and industrialisation created a plethora of new social problems. In Europe and North America, both Protestants and Catholics provided massive aid to the poor, supported family welfare, and offered medicine and education.\nNineteenth and twentieth centuries.\nThe Second Great Awakening - a religious revival of the 1800s\u20131830s - produced Mormonism, Restorationism, and the Holiness movement. Mormons preached the restoration of first-century Christianity and sought to create a religious utopia. Restorationists, such as the Churches of Christ, Jehovah's Witnesses, and Seventh Day Adventists, also focused on restoring practices of the early church. The Holiness movement focused on avoiding sin. It contributed to the later development of Pentecostalism, typified by the 1906 Azusa Street Revival, by combining Restorationism with the goal of sanctification defined as a deeper spiritual experience. \nThis revival focused on evidencing conversion through active moral reform in areas such as women's rights, temperance, literacy, and the abolition of slavery. The pursuit of women's rights established \"prayer, worship, and biblical exegesis as weapons of political warfare\". Women were involved in temperance reform from the early 1800s. Concern for women who suffered at the hands of drunkards was a recurrent theme of temperance literature which used moral persuasion to effect change. In Maine of 1851, the power of the state to effect immediate social change overshadowed such efforts, and temperance became prohibition: a political movement. The Woman's Christian Temperance Union (WCTU) was founded in 1874; many supporters went on to contribute to the women's rights movement.\nThe 300-year-old trans-Atlantic slave trade, in which some Christians had participated, had always garnered moral objections, and by the eighteenth century, individual Quakers, Methodists, Presbyterians, and Baptists began a written campaign against it. Congregations led by black preachers kept abolitionism alive into the early nineteenth century when some American Protestants organized the first anti-slavery societies. This ideological opposition eventually ended the trans-Atlantic slave trade, changing economic and human history on three continents.\nThe Third Great Awakening began in 1857 and took root throughout the world, especially in English-speaking countries, contributing to a surge of missionary zeal. Nineteenth-century Protestant missionaries, many of them women, played a significant role in shaping nations and societies. They translated the Bible into local languages, generating a written grammar, a lexicon of native traditions, and a dictionary of the local language. These were used to teach in missionary schools, resulting in the spread of literacy and indigenization. According to historian Lamin Sanneh, Protestant missionaries thus stimulated the \"largest, most diverse and most vigorous movement of cultural renewal\" in African history.\nLiberal Christians embraced seventeenth-century rationalism, but its disregard of faith and ritual in maintaining Christianity led to its decline. Fundamentalist Christianity rose in the early 1900s as a reaction against modern rationalism. By 1930, Protestant fundamentalism in America appeared to be dying. However, in the second half of the 1930s, a theology against liberalism that also included a reevaluation of Reformation teachings began uniting moderates of both sides.\nWorld War I profoundly impacted Christianity. In response, in 1938, the World Council of Churches (WCC) formed to address social issues, create cooperation, and open a dialogue among Christians on a global scale. The WCC played an important role in the Universal Declaration of Human Rights in 1948.\nThe Roman Catholic Church became increasingly centralized, conservative, and focused on loyalty to the Pope. As Nazism rose, Pope Pius XI declared the irreconcilability of the Catholic position with totalitarian fascist states that placed the nation above God. Most leaders and members of the largest Protestant church in Germany, the German Evangelical Church, supported the Nazi Party when they came to power in 1933. About a third of German Protestants formed the Confessing Church which opposed Nazism; its members were harassed, arrested, and otherwise targeted. In Poland, Catholic priests were arrested and Polish priests and nuns were executed en masse.\nRussian Orthodoxy.\nThe church reform of Peter I of Russia in the early 1700s placed the Orthodox authorities under the control of the emperor. Russian emperors continually involved the church in campaigns of russification, contributing to antisemitism. The communist revolutionaries who established the Soviet Union saw the Church as an enemy of the people and part of the monarchy. The communist Soviet Union heavily persecuted the Russian Orthodox Church, executing up to 8,000 people by 1922. The League of Militant Atheists adopted a five-year plan in 1932 \"aimed at the total eradication of religion by 1937\". Despite this, the Orthodox Church continued to contribute to theology and culture.\nAfter World War II.\nWorldwide.\nBefore 1945, about a third of the people in the world were Christians, and about 80% of them lived in Europe, Russia, and the Americas. In 2025, 31% of adults around the world declare themselves Christian, but they are no longer concentrated in the West. Christianity has declined in Europe. Between 2010 and 2015, the number of European Christians who died outnumbered births by nearly 6 million. From 2019 to 2024, the Christian share of the adult population in the United States stayed between 60% and 64%. Even so, it is estimated that fewer than a quarter of the world's Christians will live in its western locations by 2060.\nAfter WWII, decolonization strengthened the indigenization efforts of Christian missionaries, leading to explosive growth in the churches of many former colonies. In 1900, about six and a half percent of the total population of Africa were Christian; Christians numbered just under nine million out of the total population of 140 million. By 1960, this increased to just under 21%: 60 million out of a total population of 286.7 million. By 2005, the percentage of Christians was about half of the continent's population, which remained consistent into 2022. According to PEW, religion is very important to people in Africa, the Middle East, South Asia, and Latin America where populations are growing and are likely to continue to grow. This is shifting the geographic center of Christianity to sub-Saharan Africa where more than forty percent of the world's Christians are projected to live by 2060.\nChristianity in Southeast and East Asia, especially Korea, grew faster after colonialism. Rapid expansion began in the 1980s. The Council on Foreign Relations reports that the number of Chinese Protestants has grown by an average of 10% annually since 1979, with growth especially prominent among young people.\nWith the Fall of the Eastern Bloc, Christianity expanded in some Eastern European countries while declining in others. Catholic countries have displayed secularization, while Orthodox countries have experienced a revival of church participation. Orthodox Christianity made a partial resurgence in the former Soviet Union after 1991 and continues to be an important element of national identity for many citizens there.\nIn the first quarter of the twenty-first century, Christianity is present in all seven continents and a multitude of different cultures. Diverse and pluralist, it embraces over three thousand of the world's languages through \"Bible translation, prayer, litergy, hymns, and literature\". Most Christians live outside North America and Western Europe; white Christians are a global minority, and slightly over half of worldwide Christians are female. In 2017, PEW reported that Christianity is the world's largest religion with roughly 2.4 billion followers, equal to 31.2% of the world's population.\nModern movements.\nThe Second Vatican Council (Vatican II), from 1962-1965, brought about numerous reforms, liturgical changes, promoted the involvement of laypeople, and improved relations with other Christian denominations. \nIn 1992, the Catholic Church and the Lutheran World Federation signed the Joint Declaration on the Doctrine of Justification. Roman Catholic ecumenical goals are to re-establish full communion amongst all the various Christian churches, but there is no agreement amongst evangelicals. There is, however, a trend at the local level toward discussion, pulpit exchanges, and shared social action. Less than 40% of Orthodox Christians favor reconciliation with the Roman Catholic Church. Orthodox Christians of the Greek, Russian and Balkans branches tend to be more conservative on most issues than Protestants and Catholics.\nIn the last quarter of the twentieth century, Christianity faced the challenges of secularism and a changing moral climate concerning sexual ethics, gender, and exclusivity, leading to a decline in church attendance in the West. In a 2018 PEW survey of 27 countries, the majority of nations had more residents claim that the role of religion has decreased over the preceding twenty years than said it had increased. However, people in Southeast Asian and Sub-Saharan African countries reported the opposite trend, suggesting that secularization is a region-specific trend.\nIn 2000, approximately one-quarter of all Christians worldwide were part of Pentecostalism and its associated movements. By 2025, Pentecostals are expected to constitute one-third of the nearly three billion Christians worldwide, making it the largest branch of Protestantism and fastest-growing Christian movement.\nThe three main branches of Eastern Christianity are the Eastern Orthodox Church, Oriental Orthodox Communion, and Eastern Catholic Church. Roughly half of Eastern Orthodox Christians live in formerly Eastern Bloc countries. Its oldest communities in Jerusalem, Antioch, Alexandria, Constantinople, and Georgia, are decreasing due to forced migration from religious persecution. In 2020, 57 countries had \"very high\" levels of government restrictions on religion, banning or giving preferential treatment to particular groups, prohibiting conversions, and limiting preaching. As of 2022, Christians were harassed in 166 countries, compared to Muslims in 148 and Jews in 90. Anti-Christian persecution has become a consistent human rights concern. \nThe multiple wars of the twentieth century brought questions of theodicy to the forefront. For the first time since the pre-Constantinian era, Christian pacifism became an alternative to war. The Holocaust forced many to realize that supersessionism, the belief that Christians had replaced the Jews as God's chosen people, can lead to hatred, ethnocentrism, and racism. Supersessionism was never an official doctrine or universally accepted, and supersessionist texts are increasingly challenged.\nFor theologians writing after 1945, theology became dependent on context. Liberation theology was combined with the social gospel, redefining social justice, and exposing institutionalized sin to aid Latin American poor, but its context limited its application in other environments. Different historical and socio-political situations produced black theology and feminist theology. Combining Christianity with questions of civil rights, aspects of the Black Power movement, and responses to black Muslims produced a black theology that spread to the United Kingdom and parts of Africa, confronting apartheid in South Africa. The feminist movement of the mid-twentieth century began with an anti-Christian ethos but soon developed an influential feminist theology dedicated to transforming churches and society. Feminist theology developed at the local level through movements such as the womanist theology of African-American women, the \"mujerista\" theology of Hispanic women, and Asian feminist theology.\nIn the mid to late 1990s, postcolonial theology emerged globally from multiple sources. It analyzes structures of power and ideology to recover what colonialism erased or suppressed in indigenous cultures.\nModern motivation toward missions has declined in some denominations. The missionary movement of the twenty-first century has become a multi-cultural, multi-faceted global network of NGOs, volunteer doctors, short-term student volunteers, and traditional long-term bilingual, bicultural professionals who focus on evangelism and local development.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nEncyclopedia &amp; web sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;"}
{"id": "14119", "revid": "6781", "url": "https://en.wikipedia.org/wiki?curid=14119", "title": "Melody dominated homophony", "text": ""}
{"id": "14120", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=14120", "title": "Historic list of cities of Europe", "text": ""}
{"id": "14121", "revid": "1319782098", "url": "https://en.wikipedia.org/wiki?curid=14121", "title": "Hertz", "text": "SI unit for frequency\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nThe hertz (symbol: Hz) is the unit of frequency in the International System of Units (SI), often described as being equivalent to one event (or cycle) per second. The hertz is an SI derived unit whose formal expression in terms of SI base units is 1/s or s\u22121, meaning that one hertz is one per second or the reciprocal of one second. It is used only in the case of periodic events. It is named after Heinrich Rudolf Hertz (1857\u20131894), the first person to provide conclusive proof of the existence of electromagnetic waves. For high frequencies, the unit is commonly expressed in multiples: kilohertz (kHz), megahertz (MHz), gigahertz (GHz), terahertz (THz).\nSome of the unit's most common uses are in the description of periodic waveforms and musical tones, particularly those used in radio- and audio-related applications. It is also used to describe the clock speeds at which computers and other electronics are driven. The units are sometimes also used as a representation of the energy of a photon, via the Planck relation \"E\"\u00a0=\u00a0\"h\u03bd\", where \"E\" is the photon's energy, \"\u03bd\" is its frequency, and \"h\" is the Planck constant.\nDefinition.\nThe hertz is defined as one per second for periodic events. The International Committee for Weights and Measures defined the second as \"the duration of periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom\" and then adds: \"It follows that the hyperfine splitting in the ground state of the caesium 133 atom is exactly , \"\u03bd\"hfs Cs =.\" The dimension of the unit hertz is 1/time (T\u22121). Expressed in base SI units, the unit is the reciprocal second (1/s).\nIn English, \"hertz\" is also used as the plural form. As an SI unit, Hz can be prefixed; commonly used multiples are kHz (kilohertz, ), MHz (megahertz, ), GHz (gigahertz, ) and THz (terahertz, ). One hertz (i.e. one per second) simply means \"one periodic event occurs per second\" (where the event being counted may be a complete cycle); means \"one hundred periodic events occur per second\", and so on. The unit may be applied to any periodic event\u2014for example, a clock might be said to tick at , or a human heart might be said to beat at .\nThe occurrence rate of aperiodic or stochastic events is expressed in \"reciprocal second\" or \"inverse second\" (1/s or s\u22121) in general or, in the specific case of radioactivity, in becquerels. Whereas (one per second) specifically refers to one cycle (or periodic event) per second, (also one per second) specifically refers to one radionuclide event per second on average.\nEven though frequency, angular velocity, angular frequency and radioactivity all have the dimension T\u22121, of these only frequency is expressed using the unit hertz. Thus a disc rotating at 60 revolutions per minute (rpm) is said to have an angular velocity of 2\u03c0\u00a0rad/s and a frequency of rotation of . The correspondence between a frequency \"f\" with the unit hertz and an angular velocity \"\u03c9\" with the unit radians per second is\nformula_1 and formula_2\nThe hertz is named after Heinrich Hertz. As with every SI unit named after a person, its symbol starts with an upper case letter (Hz), but when written in full, it follows the rules for capitalisation of a common noun; i.e., \"hertz\" becomes capitalised at the beginning of a sentence and in titles but is otherwise in lower case.\nHistory.\nThe hertz is named after the German physicist Heinrich Hertz (1857\u20131894), who made important scientific contributions to the study of electromagnetism. The name was established by the International Electrotechnical Commission (IEC) in 1935. It was adopted by the General Conference on Weights and Measures (CGPM) (\"Conf\u00e9rence g\u00e9n\u00e9rale des poids et mesures\") in 1960, replacing the previous name for the unit, \"cycles per second\" (cps), along with its related multiples, primarily \"kilocycles per second\" (kc/s) and \"megacycles per second\" (Mc/s), and occasionally \"kilomegacycles per second\" (kMc/s). The term \"cycles per second\" was largely replaced by \"hertz\" by the 1970s.\nIn some usage, the \"per second\" was omitted, so that \"megacycles\" (Mc) was used as an abbreviation of \"megacycles per second\" (that is, megahertz (MHz)).\nApplications.\nSound and vibration.\nSound is a traveling longitudinal wave, which is an oscillation of pressure. Humans perceive the frequency of a sound as its pitch. Each musical note corresponds to a particular frequency. An infant's ear is able to perceive frequencies ranging from to ; the average adult human can hear sounds between and . The range of ultrasound, infrasound and other physical vibrations such as molecular and atomic vibrations extends from a few femtohertz into the terahertz range and beyond.\nElectromagnetic radiation.\nElectromagnetic radiation is often described by its frequency\u2014the number of oscillations of the perpendicular electric and magnetic fields per second\u2014expressed in hertz.\nRadio frequency radiation is usually measured in kilohertz (kHz), megahertz (MHz), or gigahertz (GHz), with the latter known as microwaves. Light is electromagnetic radiation that is even higher in frequency, and has frequencies in the range of tens of terahertz (THz, infrared) to a few petahertz (PHz, ultraviolet), with the visible spectrum being 400\u2013790\u00a0THz. Electromagnetic radiation with frequencies in the low terahertz range (intermediate between those of the highest normally usable radio frequencies and long-wave infrared light) is often called terahertz radiation. Even higher frequencies exist, such as that of X-rays and gamma rays, which can be measured in exahertz (EHz).\nFor historical reasons, the frequencies of light and higher frequency electromagnetic radiation are more commonly specified in terms of their wavelengths or photon energies: for a more detailed treatment of this and the above frequency ranges, see \"Electromagnetic spectrum\".\nGravitational waves.\nCurrent observations of gravitational waves are conducted in the 30\u20137000\u00a0Hz range by laser interferometers like LIGO, and the nanohertz (1\u20131000\u00a0nHz) range by pulsar timing arrays. Future space-based detectors are planned to fill in the gap, with LISA operating from 0.1\u201310\u00a0mHz (with some sensitivity from 10\u00a0\u03bcHz to 100\u00a0mHz), and DECIGO in the 0.1\u201310\u00a0Hz range.\nComputers.\nIn computers, most central processing units (CPU) are labeled in terms of their clock rate expressed in megahertz () or gigahertz (). This specification refers to the frequency of the CPU's master clock signal. This signal is nominally a square wave, which is an electrical voltage that switches between low and high logic levels at regular intervals. As the hertz has become the primary unit of measurement accepted by the general populace to determine the performance of a CPU, many experts have criticized this approach, which they claim is an easily manipulable benchmark. Some processors use multiple clock cycles to perform a single operation, while others can perform multiple operations in a single cycle. For personal computers, CPU clock speeds have ranged from approximately in the late 1970s (Atari, Commodore, Apple computers) to up to in IBM Power microprocessors.\nVarious computer buses, such as the front-side bus connecting the CPU and northbridge, also operate at various frequencies in the megahertz range.\nSI multiples.\nHigher frequencies than the International System of Units provides prefixes for are believed to occur naturally in the frequencies of the quantum-mechanical vibrations of massive particles, although these are not directly observable and must be inferred through other phenomena. By convention, these are typically not expressed in hertz, but in terms of the equivalent energy, which is proportional to the frequency by the factor of the Planck constant.\nUnicode.\nThe CJK Compatibility block in Unicode contains characters for common SI units for frequency. These are intended for compatibility with East Asian character encodings, and not for use in new documents (which would be expected to use Latin letters, e.g. \"MHz\").\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14123", "revid": "1226705653", "url": "https://en.wikipedia.org/wiki?curid=14123", "title": "Heroic couplet", "text": "Rhyming pair of lines in iambic pentameter\nA heroic couplet is a traditional form for English poetry, commonly used in epic and narrative poetry, and consisting of a rhyming pair of lines in iambic pentameter. Use of the heroic couplet was pioneered by Geoffrey Chaucer in the \"Legend of Good Women\" and the \"Canterbury Tales\", and generally considered to have been perfected by John Dryden and Alexander Pope in the Restoration Age and early 18th century respectively. \nExample.\nA frequently-cited example illustrating the use of heroic couplets is this passage from \"Cooper's Hill\" by John Denham, part of his description of the Thames:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;&lt;poem&gt;\nO could I flow like thee, and make thy stream\nMy great example, as it is my theme!\nThough deep yet clear, though gentle yet not dull;\nStrong without rage, without o'erflowing full.\n&lt;/poem&gt;\nHistory.\nThe term \"heroic couplet\" is sometimes reserved for couplets that are largely \"closed\" and self-contained, as opposed to the enjambed couplets of poets like John Donne. The heroic couplet is often identified with the English Baroque works of John Dryden and Alexander Pope, who used the form for their translations of the epics of Virgil and Homer, respectively. Major poems in the closed couplet, apart from the works of Dryden and Pope, are Samuel Johnson's \"The Vanity of Human Wishes\", Oliver Goldsmith's \"The Deserted Village\", and John Keats's \"Lamia\". The form was immensely popular in the 18th century. The looser type of couplet, with occasional enjambment, was one of the standard verse forms in medieval narrative poetry, largely because of the influence of the Canterbury Tales.\nVariations.\nEnglish heroic couplets, especially in Dryden and his followers, are sometimes varied by the use of the occasional alexandrine, or hexameter line, and triplet. Often these two variations are used together to heighten a climax. The breaking of the regular pattern of rhyming pentameter pairs brings about a sense of poetic closure. Here are two examples from Book IV of Dryden's translation of the \"Aeneid\".\n\"Alexandrine\".\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;&lt;poem&gt;\nHer lofty courser, in the court below,\nWho his majestic rider seems to know, \nProud of his purple trappings, paws the ground, \nAnd champs the golden bit, and spreads the foam around.\n&lt;/poem&gt;\u2014\u200a\n\"Alexandrine and Triplet\".\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;&lt;poem&gt;\nMy Tyrians, at their injur\u2019d queen\u2019s command, \nHad toss\u2019d their fires amid the Trojan band; \nAt once extinguish\u2019d all the faithless name; \nAnd I myself, in vengeance of my shame,\nHad fall\u2019n upon the pile, to mend the fun\u2019ral flame.\n&lt;/poem&gt;\u2014\u200a\nModern use.\nTwentieth-century authors have occasionally made use of the heroic couplet, often as an allusion to the works of poets of previous centuries. An example of this is Vladimir Nabokov's novel \"Pale Fire\", the second section of which is a 999-line, 4-canto poem largely written in loose heroic couplets with frequent enjambment. Here is an example from the first canto:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;&lt;poem&gt;\nAnd then black night. That blackness was sublime.\nI felt distributed through space and time:\nOne foot upon a mountaintop. One hand\nUnder the pebbles of a panting strand,\nOne ear in Italy, one eye in Spain,\nIn caves, my blood, and in the stars, my brain.\n&lt;/poem&gt;\u2014\u200a\nThe use of heroic couplets in translations of Greco-Roman epics has also inspired translations of non-Western works into English. In 2021, Vietnamese translator Nguyen Binh published a translation of the Vietnamese epic poem \"Tale of Ki\u1ec1u\", in which the \"l\u1ee5c b\u00e1t\" couplets of the original were rendered into heroic couplets. Binh named John Dryden and Alexander Pope as major influences on their work, which also mimicked the spelling of Dryden and Pope's translations to evoke the medieval air of the Vietnamese original. An example of the heroic couplet translation can be found below:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;&lt;poem&gt;\nOne mounted, one released the other\u2019s coat,\nThe autumn maples dyed with roads remote.\nRed miles cast dust upon the faring steed; \nHe disappear\u2019d behind the berry mead. \nOne stay\u2019d as shadow through the hours of night,\nOne left alone for great miles out of sight.\nWho had cut up the rounded moon in two, \nHalf shining cushions, half on miles that grew?\n&lt;/poem&gt;\u2014\u200a\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14126", "revid": "25859633", "url": "https://en.wikipedia.org/wiki?curid=14126", "title": "Homosexuals", "text": ""}
{"id": "14127", "revid": "7407280", "url": "https://en.wikipedia.org/wiki?curid=14127", "title": "H\u00f6\u00f0r", "text": "Norse deity\nH\u00f6\u00f0r ( , Latin Hotherus; often anglicized as Hod, Hoder, or Hodur) is a god in Norse mythology. The blind son of Odin, he is tricked and guided by Loki into shooting a mistletoe arrow which was to slay the otherwise invulnerable Baldr.\nAccording to the \"Prose Edda\" and the \"Poetic Edda\", the goddess Frigg, Baldr's mother, made everything in existence swear never to harm Baldr, except for the mistletoe, which she found too unimportant to ask (alternatively, which she found too young to demand an oath from). The gods amused themselves by trying weapons on Baldr and seeing them fail to do any harm. Loki, the mischief-maker, upon finding out about Baldr's one weakness, made a spear from mistletoe, and helped H\u00f6\u00f0r shoot it at Baldr. In reaction to this, Odin and Rindr gave birth to V\u00e1li, who grew to adulthood within a day and slew H\u00f6\u00f0r.\nThe Danish historian Saxo Grammaticus recorded an alternative version of this myth in his \"Gesta Danorum\". In this version, the mortal hero H\u00f8therus and the demi-god \"Balderus\" compete for the hand of Nanna. Ultimately, H\u00f8therus slays Balderus.\nName.\nAccording to scholar Andy Orchard, the theonym \"H\u01eb\u00f0r\" can be translated as 'warrior'. Jan de Vries and Vladimir Orel write that is comparable with Old Norse \"h\u01eb\u00f0\" ('war, slaughter'), and related to Old English \"hea\u00f0u-de\u00f3r\" ('brave, stout in war'), from Proto-Germanic \"*ha\u00feuz\" ('battle'; cf. Old High German \"hadu\"-, Old Saxon \"hathu\"-\",\" Old Frisian \"-had\", Burgundian *\"ha\u00feus\").\nAttestations.\nThe \"Prose Edda\".\nIn the \"Gylfaginning\" part of Snorri Sturluson's Prose Edda H\u00f6\u00f0r is introduced in an ominous way.\nH\u00f6\u00f0r is not mentioned again until the prelude to Baldr's death is described. All things except the mistletoe (believed to be harmless) have sworn an oath not to harm Baldr, so the \u00c6sir throw missiles at him for sport.\nThe \"Gylfaginning\" does not say what happens to H\u00f6\u00f0r after this. In fact it specifically states that Baldr cannot be avenged, at least not immediately.\nIt does seem, however, that H\u00f6\u00f0r ends up in Hel one way or another for the last mention of him in \"Gylfaginning\" is in the description of the post-Ragnar\u00f6k world.\nSnorri's source of this knowledge is clearly \"V\u00f6lusp\u00e1\" as quoted below.\nIn the \"Sk\u00e1ldskaparm\u00e1l\" section of the Prose Edda several kennings for H\u00f6\u00f0r are related.\nNone of those kennings, however, are actually found in surviving skaldic poetry. Neither are Snorri's kennings for V\u00e1li, which are also of interest in this context.\nIt is clear from this that Snorri was familiar with the role of V\u00e1li as H\u00f6\u00f0r's slayer, even though he does not relate that myth in the \"Gylfaginning\" prose. Some scholars have speculated that he found it distasteful, since H\u00f6\u00f0r is essentially innocent in his version of the story.\nThe \"Poetic Edda\".\nH\u00f6\u00f0r is referred to several times in the Poetic Edda, always in the context of Baldr's death. The following strophes are from \"V\u00f6lusp\u00e1\".\nThis account seems to fit well with the information in the Prose Edda, but here the role of Baldr's avenging brother is emphasized.\nBaldr and H\u00f6\u00f0r are also mentioned in \"V\u00f6lusp\u00e1\"'s description of the world after Ragnar\u00f6k.\nThe poem \"Vaf\u00fer\u00fa\u00f0nism\u00e1l\" informs us that the gods who survive Ragnar\u00f6k are Vi\u00f0arr, V\u00e1li, M\u00f3\u00f0i and Magni with no mention of H\u00f6\u00f0r and Baldr.\nThe myth of Baldr's death is also referred to in another Eddic poem, \"Baldrs draumar\".\nH\u00f6\u00f0r is not mentioned again by name in the Eddas. He is, however, referred to in \"V\u00f6lusp\u00e1 in skamma\".\nSkaldic poetry.\nH\u00f6\u00f0r appears in both the Poetic Edda and Prose Edda. The name of H\u00f6\u00f0r occurs several times in skaldic poetry as a part of warrior-kennings. Thus \"H\u00f6\u00f0r brynju\", \"H\u00f6\u00f0r of byrnie\", is a warrior and so is \"H\u00f6\u00f0r v\u00edga\", \"H\u00f6\u00f0r of battle\". Some scholars have found the fact that the poets should want to compare warriors with H\u00f6\u00f0r to be incongruous with Snorri's description of him as a blind god, unable to harm anyone without assistance. It is possible that this indicates that some of the poets were familiar with other myths about H\u00f6\u00f0r than the one related in \"Gylfaginning\" \u2013 perhaps some where H\u00f6\u00f0r has a more active role. On the other hand, the names of many gods occur in kennings and the poets might not have been particular in using any god name as a part of a kenning.\n\"Gesta Danorum\".\nIn \"Gesta Danorum\" by Saxo Grammaticus, Hotherus is a human hero of the Danish and Swedish royal lines. He is the son of Hothbrodd (or Hodbrodd) and brother of Athisl, both Kings of Sweden before him. Hotherus himself became ruler of both Sweden and Denmark after the death of the usurper Hiartuar, but most of the story about him as related in \"Gesta Danorum\" relates to his early life before becoming king.\nHotherus is gifted in swimming, archery, fighting and music and Nanna, daughter of King Gevarus falls in love with him. But at the same time Balderus, son of Othinus, has caught sight of Nanna bathing and fallen violently in love with her. He resolves to slay Hotherus, his rival. Out hunting, Hotherus is led astray by a mist and meets wood-maidens who control the fortunes of war. They warn him that Balderus has designs on Nanna but also tell him that he shouldn't attack him in battle since he is a demigod. Hotherus goes to consult with King Gevarus and asks him for his daughter. The king replies that he would gladly favour him but that Balderus has already made a like request and he does not want to incur his wrath. Gevarus tells Hotherus that Balderus is invincible but that he knows of one weapon which can defeat him, a sword kept by Mimingus, the satyr of the woods. Mimingus also has another magical artifact, a bracelet that increases the wealth of its owner. Riding through a region of extraordinary cold in a carriage drawn by reindeer, Hotherus captures the satyr with a clever ruse and forces him to yield his artifacts.\nHearing about Hotherus's artifacts, Gelderus, king of Saxony, equips a fleet to attack him. Gevarus warns Hotherus of this and tells him where to meet Gelderus in battle. When the battle is joined, Hotherus and his men save their missiles while defending themselves against those of the enemy with a testudo formation. With his missiles exhausted, Gelderus is forced to sue for peace. He is treated mercifully by Hotherus and becomes his ally. Hotherus then gains another ally with his eloquent oratory by helping King Helgo of H\u00e5logaland win a bride. Meanwhile, Balderus enters the country of King Gevarus armed and sues for Nanna. Gevarus tells him to learn Nanna's own mind. Balderus addresses her with cajoling words but is refused. Nanna tells him that because of the great difference in their nature and stature, since he is a demigod, they are not suitable for marriage.\nAs news of Balderus's efforts reaches Hotherus, he and his allies resolve to attack Balderus. A great naval battle ensues where the gods fight on the side of Balderus. Thoro in particular shatters all opposition with his mighty club. When the battle seems lost, Hotherus manages to hew Thoro's club off at the haft and the gods are forced to retreat. Gelderus perishes in the battle and Hotherus arranges a funeral pyre of vessels for him. After this battle Hotherus finally marries Nanna. Balderus is not completely defeated and shortly afterwards returns to defeat Hotherus in the field. But Balderus's victory is without fruit for he is still without Nanna. Lovesick, he is harassed by phantoms in Nanna's likeness and his health deteriorates so that he cannot walk but has himself drawn around in a cart.\nHotherus learned of the death of King Rolf Kraki, whose father had slain Hotherus' father Hodbrodd. He took a fleet to Denmark and was appointed king. Shortly afterwards, he also heard of the death of his brother Athisl, and also became king of Sweden.\nAfter a while Hotherus and Balderus have their third battle and again Hotherus is forced to retreat. Weary of life because of his misfortunes, he plans to retire and wanders into the wilderness. In a cave he comes upon the same maidens he had met at the start of his career. Now they tell him that he can defeat Balderus if he gets a taste of some extraordinary food which had been devised to increase the strength of Balderus. Encouraged by this, Hotherus returns from exile and once again meets Balderus in the field. After a day of inconclusive fighting, he goes out during the night to spy on the enemy. He finds where Balderus's magical food is prepared and plays the lyre for the maidens preparing it. While they don't want to give him the food, they bestow on him a belt and a girdle which secure victory. Heading back to his camp, Hotherus meets Balderus and plunges his sword into his side. Despite realising that it was a mortal wound, Balderus insists on being carried back into battle on a litter. After three days, Balderus dies from his wound.\nMany years later, Bous, the son of Othinus and Rinda, returns to avenge his brother by killing Hotherus. Hotherus foresees that he will die in the battle and asks the assembly of elders to pass the kingship to his son Rorik, which they do. Hotherus faces Bous in battle and is killed, but Bous also dies the next day from his wounds.\n\"Gesta Danorum p\u00e5 dansk\u00e6\".\n\"Gesta Danorum p\u00e5 dansk\u00e6\" is an Old Danish work based, in part on Saxo's \"Gesta Danorum\" and another Latin chronicle called the \"Chronicon Lethrense\". It contains a second, briefer euhemerized account of H\u00f6\u00f0r's slaying of Balder, as follows:\nAfter this, Hother's son Rorik Slengeborre, aka Rake, became king.\n\"Hversu Noregr bygg\u00f0ist\".\nH\u00f6\u00f0r appears in the genealogies of \"Hversu Noregr bygg\u00f0ist\" (\"How Norway was inhabited\"). In this, he is the ruler of Ha\u00f0aland and the father of H\u00f6ddbroddr (instead of being H\u00f6ddbroddr's son, as in the \"Gesta Danorum\"). H\u00f6ddbroddr's descendants are described for a further six generations, and include Hromund Gripsson.\nH\u00f6\u00f0r's parentage is not explicitly given in this text, but he may be the same as Haukr, the second legitimate son of Raum the Old by his wife Hilda, daughter of Gudrod the Old. This is because Raum the Old had four sons by Hilda and this statement is followed by four lineages springing from four men, who are otherwise the four sons of Raum and Hilda in the same order except that H\u00f6\u00f0r takes the place of Haukr.\nRydberg's theories.\nAccording to the Swedish mythologist and romantic poet Viktor Rydberg, the story of Baldr's death was taken from \"H\u00fasdr\u00e1pa\", a poem composed by Ulfr Uggason around 990 AD at a feast thrown by the Icelandic Chief \u00d3l\u00e1fr H\u00f6skuldsson to celebrate the finished construction of his new home, Hjar\u00f0arholt, the walls of which were filled with symbolic representations of the Baldr myth among others. Rydberg suggested that H\u00f6\u00f0r was depicted with eyes closed and Loki guiding his aim to indicate that Loki was the true cause of Baldr's death and H\u00f6\u00f0r was only his \"blind tool.\" Rydberg theorized that the author of the \"Gylfaginning\" then mistook the description of the symbolic artwork in the \"H\u00fasdr\u00e1pa\" as the actual tale of Baldr's death.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14128", "revid": "50464812", "url": "https://en.wikipedia.org/wiki?curid=14128", "title": "Herat", "text": "City in Herat Province, Afghanistan\nHerat, also known as Harat or Hirat, and historically known as Hira, Harew, Haraiva, Areion, and Horeiva, is an oasis city and the third-largest city in Afghanistan. In 2020, it had an estimated population of 574,276. It is the capital of Herat Province, situated south of the Paropamisus Mountains (\"Selseleh-ye Saf\u0113d K\u014dh\") in the fertile valley of the Hari River in the western part of the country. An ancient civilization on the Silk Road between West Asia, Central Asia, and South Asia, it is a regional hub in the country's west.\nHerat dates back to Avestan times and was traditionally known for its wine. The city has a number of historic sites, including the Herat Citadel and the Musalla Complex. During the Middle Ages, Herat became one of the important cities of Khorasan, as it was known as the \"Pearl of Khorasan\". After its conquest by Tamerlane, the city became an important center of intellectual and artistic life in the Islamic world. Under the rule of Shah Rukh, the city served as the focal point of the Timurid Renaissance, whose glory is thought to have matched Florence of the Italian Renaissance as the center of a cultural rebirth. After the fall of the Timurid Empire, Herat has been governed by various Afghan rulers since the early 18th century. In 1716, the Abdali Afghans inhabiting the city revolted and formed their own Sultanate, the Sadozai Sultanate of Herat. They were conquered by the Afsharid Persia in 1732.\nAfter Nader Shah's death and Ahmad Shah Durrani's rise to power in 1747, Herat separated from Persia and became part of Afghanistan. It became an independent city-state in the first half of the 19th century, facing several Qajar Iranian invasions until being incorporated into Afghanistan in 1863. Pashtunzadagan, Darwazekhosh, Ghorian, GhaderGij (QaderGij) and Gozargah are some of the neighborhoods of Herat within the city limits. The roads from Herat to Iran (through the border town of Islam Qala) and Turkmenistan (through the border town of Torghundi) are still strategically important. As the gateway to Iran, it collects a high amount of customs revenue for Afghanistan. It also has an international airport. Following the 2001 war, the city had been relatively safe from Taliban insurgent attacks. In 2021, it was announced that Herat would be listed as a UNESCO World Heritage Site. On 12 August 2021, the city was seized by Taliban fighters as part of the Taliban's summer offensive.\nThe area of Herat, along with areas like Piranshahr, Damghan and Aleppo, are noted to be sites for archaeological interests and exploration.\nHistory.\nAncient\nHerat is first recorded in ancient times, but its precise date of foundation is unknown. Under the Persian Achaemenid Empire (550\u2013330 BC), the surrounding district was known by the Old Persian name of \"Haraiva\" (\ud800\udfc3\ud800\udfbc\ud800\udfa1\ud800\udfba), and in classical sources, the region was correspondingly known as Areia (Aria). In the Zoroastrian collection of Avesta, the district is referred as \"Haroiva\". The name of the district and its principal town is a derivative from that of the local river, the Herey River (from Old Iranian \"Harayu\", meaning \"with velocity\"), which goes through the district and ends south of Herat. The naming of a region and its principal town after the main river is a common feature in this part of the world\u2014 compare the adjoining districts/rivers/towns of Arachosia and Bactria.\nThe district \"Aria\" of the Achaemenid Empire is mentioned in the provincial lists that are included in various royal inscriptions, for instance, in the Behistun inscription of Darius I (ca. 520 BC). Representatives from the district are depicted in reliefs, e.g., at the royal Achaemenid tombs of Naqsh-e Rustam and Persepolis. They are wearing Scythian-style dress (with a tunic and trousers tucked into high boots) and a twisted Bashlyk that covers their head, chin and neck.\nHamdallah Mustawfi, composer of the 14th-century geographical work \"Nuzhat al-Qulub\" writes that:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nHerodotus described Herat as \"the breadbasket of Central Asia\". At the time of Alexander the Great in 330 BC, Aria was obviously an important district. It was administered by a satrap called Satibarzanes, who was one of the three main Persian officials in the East of the Empire, together with the satrap Bessus of Bactria and Barsaentes of Arachosia. In late 330 BC, Alexander captured the Arian capital that was called Artacoana. The town was rebuilt and the citadel was constructed. Afghanistan became part of the Seleucid Empire.\nHowever, most sources suggest that Herat was predominantly Zoroastrian. It became part of the Parthian Empire in 167 BC. In the Sasanian period (226\u2013652), \ud802\udf67\ud802\udf65\ud802\udf69\ud802\udf65 \"Har\u0113v\" is listed in an inscription on the Ka'ba-i Zartosht at Naqsh-e Rustam; and \"Hariy\" is mentioned in the Pahlavi catalogue of the provincial capitals of the empire. Around 430, the town is also listed as having a Christian community, with a bishop from the Church of the East.\nIn the last two centuries of Sasanian rule, Aria (Herat) was of great strategic importance in the endless wars between the Sasanians, the Chionites, and the Hephthalites, who had been settled in the northern section of Afghanistan since the late 4th century.\nConversion to Islam.\nAt the time of the Arab invasion in the middle of the 7th century, the Sasanian central power seemed already largely nominal in the province in contrast with the role of the Hephthalites tribal lords, who were settled in the Herat region and in the neighboring districts, mainly in pastoral B\u0101dghis and in Qohest\u0101n. It must be underlined, however, that Herat remained one of the three Sasanian mint centers in the east, the other two beings Balkh and Marv. The Hephthalites from Herat and some unidentified Turks opposed the Arab forces in a battle of Qohest\u0101n in 651-52 AD, trying to block their advance on Nish\u0101pur, but they were defeated.\nWhen the Arab armies appeared in Khor\u0101s\u0101n in the 650s, Her\u0101t was counted among the twelve capital towns of the Sasanian Empire. The Arab army under the general command of Ahnaf ibn Qais in its conquest of Khor\u0101s\u0101n in 652 seems to have avoided Her\u0101t. The city eventually submitted to the Arabs, since shortly afterward, an Arab governor is mentioned there. A treaty was drawn in which the regions of B\u0101dghis and Bushanj were included. Like many other places in Khor\u0101s\u0101n, Her\u0101t rebelled and had to be re-conquered several times.\nAnother power that was active in the area in the 650s was Tang dynasty China which had embarked on a campaign that culminated in the Conquest of the Western Turks. By 659\u2013661, the Tang claimed a tenuous suzerainty over Herat, the westernmost point of Chinese power in its long history. This hold however would be ephemeral with local Turkish tribes rising in rebellion in 665 and driving out the Tang.\nIn 702, Yazid ibn al-Muhallab defeated certain Arab rebels, followers of Ibn al-Ash'ath, and forced them out of Herat. The city was the scene of conflicts between different groups of Muslims and Arab tribes in the disorders leading to the establishment of the Abbasid Caliphate. Herat was also a center of the followers of Ustadh Sis.\nIn 870, Ya'qub ibn al-Layth al-Saffar, the founder of the Saffarid dynasty, conquered Herat and the rest of the nearby regions in the name of Islam.&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...Arab armies carrying the banner of Islam came out of the west to defeat the Sasanians in 642 AD and then they marched with confidence to the east. On the western periphery of the Afghan area, the princes of Herat and Seistan gave way to rule by Arab governors but in the east, in the mountains, cities submitted only to rise in revolt, and the hastily converted returned to their old beliefs once the armies passed. The harshness and avariciousness of Arab rule produced such unrest, however, that once the waning power of the Caliphate became apparent, native rulers once again established themselves independent. Among these, the Saffarids of Seistan shone briefly in the Afghan area. The fanatic founder of this dynasty, the coppersmith's apprentice Yaqub ibn Layth Saffari, came forth from his capital at Zaranj in 870 AD and marched through Bost, Kandahar, Ghazni, Kabul, Bamiyan, Balkh and Herat, conquering in the name of Islam.\u2014\u200a\nPearl of Khorasan.\nThe region of Her\u0101t was under the rule of King Nuh II, the seventh of the Samanid line\u2014at the time of Seb\u00fck Tigin and his older son, Mahmud of Ghazni. The governor of Her\u0101t was a noble by the name of \"Faik\", who was appointed by Nuh III. It is said that Faik was a powerful but insubordinate governor of Nuh III and had been punished by Nuh III. Faik made overtures to Bogra Khan and Ughar Khan of Khorasan. Bogra Khan answered Faik's call, came to Her\u0101t, and became its ruler. The Samanids fled, betrayed at the hands of Faik to whom the defense of Her\u0101t had been entrusted by Nuh III. In 994, Nuh III invited Alptegin to come to his aid. Alptegin, along with Mahmud of Ghazni, defeated Faik and annexed Her\u0101t, Nishapur and Tous.\nHerat was a great trading center strategically located on trade routes from the Mediterranean to India or China. The city was noted for its textiles during the Abbasid Caliphate, according to many references by geographers. Her\u0101t also had many learned sons such as Ans\u0101r\u012b. The city is described by Estakhri and Ibn Hawqal in the 10th century as a prosperous town surrounded by strong walls with plenty of water sources, extensive suburbs, an inner citadel, a congregational mosque, and four gates, each gate opening to a thriving market place. The government building was outside the city at a distance of about a mile in a place called Khor\u0101s\u0101n\u0101b\u0101d. A church was still visible in the countryside northeast of the town on the road to Balkh, and farther away on a hilltop stood a flourishing fire temple, called Sereshk, or Arshak according to Mustawfi.\nHerat was a part of the Taherid dominion in Khor\u0101s\u0101n until the rise of the Saffarids in Sist\u0101n under Ya'qub-i Laith in 861, who, in 862, started launching raids on Herat before besieging and capturing it on 16 August 867, and again in 872. The Saffarids succeeded in expelling the Taherids from Khorasan in 873.\nThe S\u0101m\u0101nid dynasty was established in Transoxiana by three brothers, Nuh, Yahy\u0101, and Ahmad. Ahmad S\u0101m\u0101ni opened the way for the Samanid dynasty to the conquest of Khor\u0101s\u0101n, including Her\u0101t, which they were to rule for one century. The centralized Samanid administration served as a model for later dynasties. The Samanid power was destroyed in 999 by the Qarakhanids, who were advancing on Transoxiana from the northeast, and by the Ghaznavids, former Samanid retainers, attacking from the southeast.\nGhaznavid Era\nSultan Ma\u1e25mud of Ghazni officially took control of Khor\u0101s\u0101n in 998. Herat was one of the six Ghaznavid mints in the region. In 1040, Herat was captured by the Seljuk Empire. During this change of power in Herat, there was supposedly a power vacuum which was filled by Abdullah Awn, who established a city-state and made an alliance with Mahmud of Ghazni. Yet, in 1175, it was captured by the Ghurids of Ghor and then came under the Khawarazm Empire in 1214. According to the account of Mustawfi, Herat flourished especially under the Ghurid dynasty in the 12th century. Mustawfi reported that there were \"359 colleges in Herat, 12,000 shops all fully occupied, 6,000 bath-houses; besides caravanserais and mills, also a darwish convent and a fire temple\". There were about 444,000 houses occupied by a settled population. The men were described as \"warlike and carry arms\", and they were Sunni Muslims. The great mosque of Her\u0101t was built by Ghiyasuddin Ghori in 1201. In this period Her\u0101t became an important center for producing metal goods, especially in bronze, often decorated with elaborate inlays in precious metals.\nMongols\nThe Mongol Empire laid siege to Herat twice. The first siege resulted in the surrender of the city, the slaughter of the local sultan's army of 12,000, and the appointment of two governors, one Mongol and one Muslim. The second, prompted by a rebellion against Mongol rule, lasted seven months and ended in June 1222 with, according to one account, the beheading of the entire population of 1,600,000 people by the victorious Mongols, such that \"no head was left on a body, nor body with a head.\"\nThe city remained in ruins from 1222 to about 1236. In 1244, a local prince Shams al-Din Kart was named ruler of Her\u0101t by the Mongol governor of Khor\u0101s\u0101n and in 1255 he was confirmed in his rule by the founder of the Il-Khan dynasty Hulagu. Shamsuddin Kart founded a new dynasty and his successors, especially Fakhruddin Kart and Ghiyasuddin Kart, built many mosques and other buildings. The members of this dynasty were great patrons of literature and the arts. By this time Her\u0101t became known as the \"pearl of Khorasan\".&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Rumi\u2014\u200a\nTimur took Herat in 1380 and he brought the Kartid dynasty to an end a few years later. The city reached its greatest glory under the Timurid princes, especially Sultan Husayn Bayqara who ruled Herat from 1469 until 4 May 1506. His chief minister, the poet and author in Persian and Turkish, Mir Ali-Shir Nava'i was a great builder and patron of the arts. Under the Timurids, Herat assumed the role of the main capital of an empire that extended in the West as far as central Persia. As the capital of the Timurid empire, it boasted many fine religious buildings and was famous for its sumptuous court life and musical performance and its tradition of miniature paintings. On the whole, the period was one of relative stability, prosperity, and development of economy and cultural activities. It began with the nomination of Shahrokh, the youngest son of Timur, as governor of Herat in 1397. The reign of Shahrokh in Herat was marked by intense royal patronage, building activities, and the promotion of manufacturing and trade, especially through the restoration and enlargement of the Herat's b\u0101z\u0101r. The present Musallah Complex, and many buildings such as the madrasa of Gawhar Shad, Ali Shir mah\u0101l, many gardens, and others, date from this time. The village of Gazar Gah, over two km northeast of Herat, contained a shrine that was enlarged and embellished under the Timurids. The tomb of the poet and mystic Khw\u0101jah Abdull\u0101h Ans\u0101r\u012b (d. 1088), was first rebuilt by Shahrokh about 1425, and other famous men were buried in the shrine area.\nIn the summer of 1458, the Qara Qoyunlu under Jahan Shah advanced as far as Herat, but had to turn back soon because of a revolt by his son Hasan Ali and also because Abu Said's march on Tabriz.\nIn 1507, Herat was occupied by the Uzbeks but after much fighting the city was taken by Shah Isma'il, the founder of the Safavid dynasty, in 1510 and the Shamlu Qizilbash assumed the governorship of the area. Under the Safavids, Herat was again relegated to the position of a provincial capital, albeit one of particular importance. At the death of Shah Isma'il the Uzbeks again took Herat and held it until Shah Tahmasp retook it in 1528. The Persian king, Shah Abbas the Great was born in Herat, and in Safavid texts, Herat is referred to as \"a'zam-i bil\u0101d-i \u012br\u0101n\", meaning \"the greatest of the cities of Iran\". In the 16th century, all future Safavid Persian rulers, from Tahmasp I to Abbas I, were governors of Herat in their youth.\nModern history (1500-2023).\nBy the early 18th century Herat was governed by the Abdali Afghans. After Nader Shah's death in 1747, Ahmad Shah Durrani took possession of the city and became part of the Durrani Empire.\nIn 1793, Herat became independent for several years when Afghanistan underwent a civil war between different sons of Timur Shah. The Iranians had multiple wars with Herat between 1801 and 1837 (1804, 1807, 1811, 1814, 1817, 1818, 1821, 1822, 1825, 1833). The Iranians besieged the city in 1837, but the British helped the Heratis in repelling them. In 1856, they invaded again, and briefly managed to take the city on 25 October; it led directly to the Anglo-Persian War. In 1857 hostilities between the Iranians and the British ended after the Treaty of Paris was signed, and the Persian troops withdrew from Herat in September 1857. Afghanistan conquered Herat on 26 May 1863, under Dost Muhammad Khan, two weeks before his death.\nThe famous Musalla of Gawhar Shah of Herat, a large Islamic religious complex consisting of five minarets, several mausoleums along with mosques and madrasas was dynamited during the Panjdeh incident to prevent their usage by the advancing Russian forces. Some emergency preservation work was carried out at the site in 2001 which included building protective walls around the Gawhar Shad Mausoleum and Sultan Husain Madrasa, repairing the remaining minaret of Gawhar Shad's Madrasa, and replanting the mausoleum garden.\nIn the aftermath of the Afghan Civil War (1928\u20131929), Herat was the last stronghold of Saqqawist resistance, holding out until 1931 when it was retaken by forces loyal to Mohammad Nadir Shah.\nIn the 1960s, engineers from the United States built Herat Airport, which was used by the Soviet forces during the Democratic Republic of Afghanistan in the 1980s. Even before the Soviet invasion at the end of 1979, there was a substantial presence of Soviet advisors in the city with their families.\nBetween 10\u201320 March 1979, the Afghan Army in Herat under the control of commander Ismail Khan mutinied. Thousands of protesters took to the streets against the Khalq communist regime's oppression led by Nur Mohammad Taraki. The new rebels led by Khan managed to oust the communists and take control of the city for 3 days, with some protesters murdering any Soviet advisers and targeting women without headscarves, dubbed \"s\u0101rluchi\". This shocked the government, who blamed the new administration of Iran following the Iranian Revolution for influencing the uprising. Reprisals by the government followed, and between 3,000 and 24,000 people (according to different sources) were killed, in what is called the 1979 Herat uprising, or in Persian as the \"Qiam-e Herat\". The city itself was recaptured with by the Afghan Army\u2019s 4th and 15th Armoured Brigades, detachments of the Afghan Commando Forces and the Afghan Air Force but at the cost of thousands of civilians killed. This rebellion was the first of its kind since the Third Anglo-Afghan War in 1919, and was the bloodiest event preceding the Soviet\u2013Afghan War.\nHerat received damage during the Soviet\u2013Afghan War, especially its western side. The province as a whole was one of the worst-hit. In April 1983, a series of Soviet bombings damaged half of the city and killed around 3,000 civilians, described as \"extremely heavy, brutal and prolonged\". Ismail Khan was the leading mujahideen commander in Herat fighting against the Soviet-backed government.\nAfter the communist government's collapse in 1992, Khan joined the new government and he became governor of Herat Province. The city was relatively safe and it was recovering and rebuilding from the damage caused in the Soviet\u2013Afghan War. However, on 5 September 1995, the city was captured by the Taliban without much resistance, forcing Khan to flee. Herat became the first Persian-speaking city to be captured by the Taliban. The Taliban's strict enforcement of laws confining women at home and closing girls' schools alienated Heratis who are traditionally more liberal and educated, like the Kabulis, than other urban populations in the country. Two days of anti-Taliban protests occurred in December 1996 which was violently dispersed and led to the imposition of a curfew. In May 1999, a rebellion in Herat was crushed by the Taliban, who blamed Iran for causing it.\nAfter the U.S. invasion of Afghanistan, on 12 November 2001, it was captured from the Taliban by forces loyal to the Northern Alliance and Ismail Khan returned to power (see Battle of Herat). The state of the city was reportedly much better than that of Kabul. In 2004, Mirwais Sadiq, Aviation Minister of Afghanistan and the son of Ismail Khan, was ambushed and killed in Her\u0101t by a local rival group. More than 200 people were arrested under suspicion of involvement.\nIn 2005, the International Security Assistance Force (ISAF) began establishing bases in and around the city. Its main mission was to train the Afghan National Security Forces (ANSF) and help with the rebuilding process of the country. Regional Command West, led by Italy, assisted the Afghan National Army (ANA) 207th Corps. Herat was one of the first seven areas that transitioned security responsibility from NATO to Afghanistan. In July 2011, the Afghan security forces assumed security responsibility from NATO.\nDue to their close relations, Iran began investing in the development of Herat's power, economy and education sectors. In the meantime, the United States built a consulate in Herat to help further strengthen its relations with Afghanistan. In addition to the usual services, the consulate works with the local officials on development projects and with security issues in the region.\nOn 12 August 2021, the city was captured by the Taliban during the 2021 Taliban offensive.\nGeography.\nClimate.\nHerat has a cold semi-arid climate (K\u00f6ppen climate classification \"BSk\"). Precipitation is very low, and mostly falls in winter. Although Her\u0101t is approximately lower than Kandahar, the summer climate is more temperate, and the climate throughout the year is far from disagreeable, although winter temperatures are comparably lower. From May to September, the wind blows from the northwest with great force. The winter is tolerably mild; snow melts rather quickly, and even on the mountains does not lie long. The eastern reaches of the Hari River, including the rapids, are frozen hard in the winter, and people travel on it as on a road.\nPlaces of interest.\nOf the more than dozen minarets that once stood in Her\u0101t, many have been toppled from war and neglect over the past century. Recently, however, everyday traffic threatens many of the remaining unique towers by shaking the very foundations they stand on. Cars and trucks that drive on a road encircling the ancient city rumble the ground every time they pass these historic structures. UNESCO personnel and Afghan authorities have been working to stabilize the Fifth Minaret.\nDemographics.\nThe population of Herat numbered approximately 592,902 in 2021. The majority of Herat's population is Tajik, at 85%, followed by a relatively large minority of Pashtuns at 10%, and small minorities of Hazaras (2%), Uzbeks (2%), and Turkmens (1%).98% of Herat is Sunni and 2% is Shia. The city has high residential density clustered around the core of the city. However, vacant plots account for a higher percentage of the city (21%) than residential land use (18%) and agricultural is the largest percentage of total land use (36%).\nThe city once had a Jewish community. About 280 families lived in Herat as of 1948, but most of them moved to Israel that year, and the community disappeared by 1992. There are four former synagogues in the city's old quarter, which were neglected for decades and fell into disrepair. In the late 2000s, the buildings of the synagogues were renovated by the Aga Khan Trust for culture, and at this time, three of them were turned into schools and nurseries, the Jewish community having vanished. In 2022, the Taliban government approved conservation work on the Yu Aw Synagogue, located in Herat's old city. The Jewish cemetery is being taken care of by Jalil Ahmed Abdelaziz.\nEconomy.\nTransport.\nAir.\nHerat International Airport was built by engineers from the United States in the 1960s and was used by the Soviet Armed Forces during the Soviet\u2013Afghan War in the 1980s. It was bombed in late 2001 during Operation Enduring Freedom but had been rebuilt within the next decade. The runway of the airport has been extended and upgraded and as of August 2014 there were regularly scheduled direct flights to Delhi, Dubai, Mashad, and various airports in Afghanistan. At least five airlines operated regularly scheduled direct flights to Kabul.\nRail.\nRail connections to and from Herat were proposed many times, during \"The Great Game\" of the 19th century and again in the 1970s and 1980s, but nothing came to life. In February 2002, Iran and the Asian Development Bank announced funding for a railway connecting Torbat-e Heydarieh in Iran to Herat. This was later changed to begin in Khaf in Iran, a railway for both cargo and passengers, with work on the Iranian side of the border starting in 2006. Construction is underway in the Afghan side and it was estimated to be completed by March 2018. There is also the prospect of an extension across Afghanistan to Sher Khan Bandar.\nRoad.\nThe AH76 highway connects Herat to Maymana and the north. The AH77 connects it east towards Chaghcharan and north towards Mary in Turkmenistan. Highway 1 (part of Asian highway AH1) links it to Islam Qala to the northwest, and south via the Kandahar\u2013Herat Highway to Kandahar.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14129", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=14129", "title": "Haithabu", "text": ""}
{"id": "14130", "revid": "40443892", "url": "https://en.wikipedia.org/wiki?curid=14130", "title": "Hedeby", "text": "Danish Viking Age trading settlement\nHedeby (, Old Norse: \"Hei\u00f0ab\u00fdr\", German: \"Haithabu\") was an important Danish Viking Age (8th to the 11th centuries) trading settlement near the southern end of the Jutland Peninsula, now in the Schleswig-Flensburg district of Schleswig-Holstein, Germany. Around 965, chronicler Ibrahim ibn Yaqub visited Hedeby and described it as \"a very large city at the very end of the world's ocean.\"\nDue to its unique position between the Frankish Empire and the Danish Kingdom, the settlement developed as a trading centre at the head of a narrow, navigable inlet known as the Schlei, which connects to the Baltic Sea. The location was favorable because there is a short portage of less than 15\u00a0km to the Treene River, which flows into the Eider with its North Sea estuary, making it a convenient place where goods and ships could be pulled on a corduroy road overland for an almost uninterrupted seaway between the Baltic and the North Sea and avoid a dangerous and time-consuming circumnavigation of Jutland, providing Hedeby with a role similar to later L\u00fcbeck. Hedeby was the second largest Nordic town during the Viking Age, after Upp\u00e5kra in present-day southern Sweden. The city of Schleswig was later founded on the other side of the Schlei. Hedeby was abandoned after its destruction in 1066.\nHedeby was rediscovered in the late 19th century and excavations began in 1900. The Hedeby Viking Museum was opened next to the site in 1985. Because of its historical importance during the Viking Age and exceptional preservation, Hedeby and the nearby defensive earthworks of the Danevirke were inscribed on the UNESCO World Heritage List in 2018.\nHedeby is mentioned in Hans Christian Andersen's fairy tale \"The Marsh King's Daughter\".\nSince 2018, Hedeby has been a UNESCO World Heritage Site.\nName.\nThe Old Norse name \"Hei\u00f0a-b\u00fdr\" simply translates to \"heath-settlement\" (\"hei\u00f0r\" \"heath\" and \"b\u00fdr\" = \"yard; settlement, village, town\"). The name is recorded in numerous spelling variants.\nSources from the 9th and 10th century AD also attest to the names \"Sliesthorp\" and \"Sliaswich\" (cf. \"-thorp\" vs. \"-wich\"), and the town of Schleswig still exists 3\u00a0km north of Hedeby. However, \u00c6thelweard claimed in his Latin translation of the Anglo-Saxon Chronicle that the Saxons used \"Slesuuic\" and the Danes \"Haithaby\" to refer to the same town.\nHistory.\nOrigins.\nHedeby is first mentioned in the Frankish chronicles of Einhard (804), who was in the service of Charlemagne, as a place Charlemagne stayed in the summer of 804, at the end of the Saxon Wars. In 808 the Danish king Godfred (Lat. Godofredus) destroyed a competing Slav trade centre named Reric, and it is recorded in the Frankish chronicles that he resettled the merchants from there to Hedeby. This may have provided the initial impetus for the town to further develop.\nThe same sources record that Godfred strengthened the Danevirke, an earthen wall that stretched across the south of the Jutland peninsula. The Danevirke joined the defensive walls of Hedeby to form an east\u2013west barrier across the peninsula, from the marshes in the west to the Schlei inlet leading into the Baltic in the east.\nThe town itself was surrounded on its three landward sides (north, west, and south) by earthworks. At the end of the 9th century the northern and southern parts of the town were abandoned for the central section. Later a 9-metre (29-ft) high semi-circular wall was erected to guard the western approaches to the town. On the eastern side, the town was bordered by the innermost part of the Schlei inlet and the bay of Haddebyer Noor.\nRise.\nHedeby became a principal marketplace because of its geographical location on the major trade routes between the Frankish Empire and Scandinavia (north-south), and between the Baltic and the North Sea (east-west). Between 800 and 1000 the growing economic power of the Vikings led to its dramatic expansion as a major trading centre. Along with Birka and Schleswig, Hedeby's prominence as a major international trading hub served as a foundation of the Hanseatic League that would emerge by the 12th century.\nHedeby played an important role in the international Viking slave trade between Europe and Byzantines as well as the Islamic world. People taken captive during the Viking raids across Eastern Europe could be sold to Moorish Spain via the Dublin slave trade or transported to Hedeby or Br\u00e4nn\u00f6 in Scandinavia and from there via the Volga trade route to Russia, where Slavic slaves and furs were sold to Muslim merchants in exchange for Arab silver \"dirham\" and silk, which have been found in Birka, Wollin and Dublin; initially this trade route between Europe and the Abbasid Caliphate passed via the Khazar Kaghanate, but from the early 10th-century onward it went via Volga Bulgaria and from there by caravan to Khwarazm, to the Samanid slave market in Central Asia and finally via Iran to the Abbasid Caliphate.\nThe following indicates the importance achieved by the town:\nA Swedish dynasty founded by Olof the Brash is said to have ruled Hedeby during the last decades of the 9th century and the first part of the 10th century. This was told to Adam of Bremen by the Danish king Sweyn Estridsson, and it is supported by three runestones found in Denmark. Two of them were raised by the mother of Olof's grandson Sigtrygg Gnupasson. The third runestone, discovered in 1796, is from Hedeby, the \"Stone of Eric\" (). It is inscribed with Norwegian-Swedish runes. It is, however, possible that Danes also occasionally wrote with this version of the younger futhark.\nLifestyle.\nLife was short and crowded in Hedeby. The small houses were clustered tightly together in a grid, with the east\u2013west streets leading down to jetties in the harbour.\nWhile Hedeby primarily served as a trade emporium, archaeological evidence demonstrates that it had produced many goods locally. Discovery and analysis of excavated artifacts reveal that tools such as spindle whorls, spindle rods, loom weights, and bone needles were standardized products. The distribution of these various tools demonstrates that there was a wide range of textiles produced at Hedeby, ranging from coarse fabric for sailcloth and outer-garments, to fine worsted wool fabric for higher quality clothes. More than 340,000 pieces related to comb making, tools for working leather, remains of ironworking and goldsmithing, and mercury from fire gilding were also found. There was also evidence found for the presence of a glass furnace active in the site from the period of 850 to 900. A total of 7,700 decorative beads have been unearthed in Hedeby, although it is likely that a small percentage of those were produced in situ. The presence of these artifacts at the site indicate that Hedeby had a robust local economy that produced a wide variety of goods, likely for domestic use and for trade at the sites markets.\nAnalysis of some of Hedeby\u2019s burial sites provide evidence for the existence of an aristocracy. Graves that are lavishly furnished with jewelry, commodities, weapons and armor set apart from more humble inhumation sites indicate an established degree of stratification among Hedeby\u2019s society.\nThe trade and production of beads was tied to a robust fashion within Hedeby. Beads made of varying materials such as carnelian, rock crystal, amber, jet, silver, brass, bronze, and mosaic glass have been found in the harbor excavation sites, burials, and throughout the settlement. Dating of these finds reveals that there was a change in style roughly every 10\u201335 years within the settlement.\nAl-Tartushi, a late 10th-century traveller from al-Andalus, provides one of the most colourful and often quoted descriptions of life in Hedeby. Al-Tartushi was from Cordoba in Spain, which had a significantly more wealthy and comfortable lifestyle than Hedeby. While Hedeby may have been significant by Scandinavian standards, Al-Tartushi was unimpressed:\n\"Slesvig (Hedeby) is a very large town at the extreme end of the world ocean... The inhabitants worship Sirius, except for a minority of Christians who have a church of their own there... He who slaughters a sacrificial animal puts up poles at the door to his courtyard and impales the animal on them, be it a piece of cattle, a ram, billy goat or a pig so that his neighbours will be aware that he is making a sacrifice in honour of his god. The town is poor in goods and riches. People eat mainly fish which exist in abundance. Babies are thrown into the sea for reasons of economy. The right to divorce belongs to the women... Artificial eye make-up is another peculiarity; when they wear it their beauty never disappears, indeed it is enhanced in both men and women. Further: Never did I hear singing fouler than that of these people, it is a rumbling emanating from their throats, similar to that of a dog but even more bestial.\"\nDestruction.\nThe town was sacked in 1050 by King Harald Hardrada of Norway during a conflict with King Sweyn II of Denmark. He set the town on fire by sending several burning ships into the harbour, the charred remains of which were found at the bottom of the Schlei during recent excavations. An unnamed Norwegian \"skald\" in Harald's army, quoted by Snorri Sturluson, describes the sack as follows:\n\"All Hedeby was burned from end to end out of anger, and that one can call a valiant deed, I believe.\"\n \"There is hope that we will do harm to Sveinn; I was on the rampart of the stronghold last night before dawn; high flame burst from the houses.\"\nIn 1066 the town was sacked and burned by West Slavs. Following the destruction, Hedeby was slowly abandoned. People moved across the Schlei inlet, which separates the two peninsulas of Angeln and Schwansen, to the growing town of Schleswig. Hedeby\u2019s royal tolls and levies were transferred to the town by the monarchy.\nArchaeology.\n20th-century archaeology.\nAfter the settlement was abandoned, rising waters contributed to the complete disappearance of all visible structures on the site. It was even forgotten where the settlement had been. This proved to be fortunate for later archaeological work at the site.\nThe exact location of the site was rediscovered by Sophus Muller in 1897. Archaeological work began at the site in 1900 after the rediscovery of the settlement with small-scale excavations by Johanna Mestorf. Excavations were conducted for the next 15 years, and additionally in 1921. These early efforts would result in over 350 small trenches being dug, and the discovery of a burial site within the rampart dating from earlier in the site's history, they were led by Wilhelm Splieth and Friedrich Norr.\nFurther excavations were carried out between 1930 and 1939 by Nazi Germany\u2019s Ahnenerbe, the pseudoscientific organization within the SS under Herbert Jankuhn. The results of Jankuhn\u2019s discoveries were never published in detail. What has been published shows that this period saw the digging of several trial trenches, discovering a group of ten chamber burials, a cremation burial site, and two inhumation graves.\nExcavation in 1956 found more inhumation and cremation burials south of the rampart, which prompted many large-scale excavations. Klaus Raddatz, Heiko Steuer, and Konrad Weidemann investigated much of the cemetery site at that time, but their findings have not been published in detail.\nIn 1963, Torsten Capelle and Kurt Schietzel conducted further work on the site, they were the source of the youngest find at the site, with an excavated well dated to 1020 A.D. by dendrochronology.\nArchaeological work on the site was productive for two main reasons: that the site had never been built on since its destruction some 840 years earlier, and that the permanently waterlogged ground had preserved wood and other perishable materials. The embankments surrounding the settlement were excavated, and the harbour was partially dredged, during which the wrecks of multiple Viking ships were discovered, including the Hedeby 1. Despite all this work, only 5% of the settlement (and only 1% of the harbour) has as yet been investigated.\nThe most important finds resulting from the excavations are now on display in the adjoining Hedeby Viking Museum.\n21st-century archaeology.\nWork has continued on the site since the earlier projects.\nIn 2002 a large scale geophysical project was started by teams from Marburg, Munich and Vienna. Over the course of three weeks, a total of ca 29 ha in and around the semi-circular rampart were analysed using Fluxgate, Caesium magnetometer and ground-penetrating radar.\nFurther work continued in 2003 when the \"Arch\u00e4olgisches Landesmuseum\" began a metal detector survey with the help of the \"Bornholmske Amat\u00f8rarkaologer\" and a group from Schleswig-Holstein. Throughout their work, 11,500 metal finds were collected and catalogued with a D-GPS system.\nIn 2005 an ambitious archaeological reconstruction program was initiated on the original site. Based on the results of archaeological analyses, exact copies of some of the original Viking houses have been built.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14131", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=14131", "title": "Hazaras", "text": "Persian-speaking ethnic group mainly in Afghanistan\nThe Hazaras (; ) are an ethnic group and a principal component of Afghanistan's population. They are one of the largest ethnic groups in Afghanistan, primarily residing in the Hazaristan (Hazarajat) region in central Afghanistan. Hazaras also form significant minority communities in Pakistan, mainly in Quetta, and in Iran, primarily in Mashhad. They speak Dari and Hazaragi, dialects of Persian. Dari, also known as Dari Persian, is an official language of Afghanistan, alongside Pashto.\nBetween 1888 and 1893, more than half of the Hazara population was massacred under the Emirate of Afghanistan, and they have faced persecution at various times over the past decades. Widespread ethnic discrimination, religious persecution, organized attacks by terrorist groups, harassment, and arbitrary arrest for various reasons have affected Hazaras. There have been numerous cases of torture of Hazara women, land and home seizures, deliberate economic restrictions, economic marginalization of the Hazara region and appropriation of Hazara agricultural fields and pastures leading to their forced displacement from Afghanistan.\nEtymology.\nThe etymology of the word \"Hazara\" is disputed, with differing opinions on its origin.\nNasir Khusraw Balkhi, the 11th-century Persian-language poet and scholar, refers to the word \"Hazara\" ( ) in his poetry:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Hazaran qaul-e khob-o naghz-o barik\"\"Azo yaband chon tar-e Hazara\"Translation:\"It is from wisdom that spring thousands of fine and thoughtful words\"\"As does music from the strings of a Hazara tar\"\nOne of the earliest mentions of the Hazaras appears in the \"Baburnama\", written by Babur, the founder of the Mughal Empire, in the early 16th century. The text specifically refers to prominent Hazara tribes, including the Sultan Masudi and Turkoman Hazaras.\nIn their native language, the Hazaras refer to themselves as \"Azra\" ( ) or ( ).\nOrigin.\nDespite being one of the principal population groups in Afghanistan, the origins of the Hazara people have not been fully reconstructed. Genetic and linguistic analyses describe Hazaras as an ethnically mixed group, with varying degrees of ancestry linked to contemporary Turkic, Mongolic, and Iranic populations. The physical characteristics of some Hazaras and Char Aimaks are Mongolian, likely a legacy of the Mongol invasion. Additionally, the Hazaras share common racial traits, physical features, and a strong resemblance to the Turkic populations of Central Asia. Babur, the founder of the Mughal Empire in the early 16th century, mentioned the Hazaras in the \"Baburnama,\" referring to some as \"Turkoman Hazaras.\"\nOver the centuries, various Mongol (Turco-Mongol) and Turkic groups, notably the Qara'unas, Chagatai Turco-Mongols, Ilkhanate, and Timurids, merged with local indigenous Turkic and Iranic populations. Scholars agree that the Hazaras are the result of this historical blending, representing a unique ethnogenesis shaped by Turkic, Mongolic, and Iranic influences.\nAlthough the Hazaras are a mix of multiple distinct ethnicities, a number of researchers focus on their Mongolic component. Some authors, including Elizabeth Emaline Bacon, Barbara A. West, Yuri Averyanov, and Elbrus Sattsayev, refer to them as \"Hazara Mongols\". Scholars such as Vasily Bartold, \u00c1rmin V\u00e1mb\u00e9ry, Vadim Masson, Vadim Romodin, Ilya Petrushevsky, Allah Rakha, Fatima, Min-Sheng Peng, Atif Adan, Rui Bi, Memona Yasmin, and Yong-Gang Yao have written about the historical use of the Mongolian language by the Hazaras. According to Sayed Askar Mousavi, the term \"Moghol Hazaras\" has not been found in historical documents, and no scholars have encountered \"Mogholi-speaking Hazaras\". However, 19th-century Hungarian orientalist \u00c1rmin V\u00e1mb\u00e9ry, who personally traveled through Afghanistan, reported that some Hazara groups in the region of Herat still spoke a Mongolic dialect in his time. He also noted that the Hazaras preserved distinct Mongolian physical traits and cultural features. Similarly, the Mughal emperor Babur, in his memoirs (Baburnama), mentioned that some Hazara communities spoke Mongolian. These historical observations are frequently cited by scholars who support a Mongol origin of the Hazaras.\nAccording to historian Lutfi Temirkhanov, Mongolian detachments left in Afghanistan by Genghis Khan or his successors became the foundational layer of Hazara ethnogenesis. Sayed Askar Mousavi, however, questions the theory that these military units were permanently settled by direct order of Genghis Khan or his commanders in what is now Hazarajat. He argues that no known primary sources support such a claim, and views this interpretation as lacking historical foundation. Nevertheless, a number of other historians maintain that Mongol military garrisons were indeed left behind in the region following the 13th-century invasions, and that these settlements played a significant role in the ethnogenesis of the Hazara people. According to Rashid al-Din, the Mongols established permanent military units across Central Asia, including in Khorasan. He specifically mentions a commander named Tumay, who was stationed as a military governor (amir) in Khorasan, indicating long-term garrison activity in the area.\nIn the Ghilji neighborhood, Hazaras are called Mongols. In turn, the Qarluq, Khalaj, and Turkoman peoples also contributed to the ethnogenesis of the Hazaras, with tribal names such as Qarluq and Turkoman still present among them today. Evidence for the Mongol influence in Hazara ethnogenesis includes linguistic data, historical sources, toponymy, and population genetics studies. However, alternative theories have also been proposed, highlighting the complex origins of the Hazara people.\nSome historians argue that the Bamiyan Buddha statues constructed around the 5th and 6th centuries and noted for their resemblance to the Hazaras in facial features and appearance, suggest the deep historical roots of the Hazara people in the central regions of present-day Afghanistan.\nWithout taking a definitive stance, some scholars consider it historically plausible that the origins of the Hazara people are rooted in Mongolic and Turkic groups who gradually entered the mountainous regions between Persia, Central Asia, and India from the 13th to 15th centuries, intermixing with local populations and adopting their language. Additionally, earlier Turko-Mongolic groups such as the Hephthalites, who inhabited the region in the 5th and 6th centuries, may have also contributed to Hazara ethnogenesis.\nHistory.\nA mention of the Hazaras appears in Babur's \"Baburnama\" in the early 16th century, particularly referring to tribes such as the Sultan Masaudi Hazaras, Turkoman Hazaras, and Kedi Hazaras.\nIt is reported that the Hazaras embraced Shia Islam between the end of the 16th century and the beginning of the 17th century, during the Safavid period. In the 18th century, Hazara men, together with individuals from other ethnic groups, were enlisted into the army of Ahmad Shah Durrani.\n19th century.\nDuring the second reign of Dost Mohammad Khan in the 19th century, Hazaras from Hazarajat were taxed for the first time. However, for the most part, they managed to maintain their regional autonomy until the 1892 Battle of Uruzgan and the subsequent subjugation by Abdur Rahman, which began in the late 19th century.\nWhen the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, Abdur Rahman set a goal to bring Hazaristan, Turkistan, and Kafiristan under his control. He launched several campaigns in Hazaristan in response to resistance from the Hazaras, during which his forces committed atrocities. The southern part of Hazaristan was spared, as its inhabitants accepted his rule, while other regions rejected Abdur Rahman and supported his uncle, Sher Ali Khan. In response, Abdur Rahman waged war against the tribal leaders who opposed his policies and rule. This conflict is known as the Hazara Uprisings.\nThese campaigns had a catastrophic impact on the demographics of the Hazaras, resulting in the massacre of over sixty percent of the total Hazara population, with many being displaced and exiled from their own lands. The Hazara lands were distributed among loyalist villagers from nearby non-Hazara communities. The repression following the uprising has been characterized as genocide or ethnic cleansing in the history of modern Afghanistan.\nAfter these massacres, Abdul Rahman forced many Hazara families from the Hazara areas of Uruzgan and other parts of Hazaristan to leave their hometowns and ancestral lands, prompting many Hazaras to flee to neighboring countries such as Central Asia, Iran, British India, Iraq, and Syria. Those Hazaras living in the northern Hindu Kush migrated to Tsarist Russia, primarily settling in the southern cities, while some moved to Iran. Over time, many Hazaras living in Tsarist Russian regions lost their language, culture, and ethnic identity due to the similarities in racial background and physical appearance of the local population, leading them to assimilate. The fleeing Hazaras settled in former Tsarist Russia regions, including Uzbekistan, Tajikistan, Turkmenistan, Kazakhstan, and Dagestan. Meanwhile, the Hazaras from northwestern Afghanistan migrated to Iran, settling in neighborhoods in and around Mashhad, where they later became known as Khawari or Barbari. Another group of Hazaras from the southeastern regions of Afghanistan moved to British India, where they reside in Quetta (present-day Pakistan) and parts of present-day India. Additionally, some Hazaras settled in Syria and Iraq. Unlike those who migrated to Tsarist Russia, the Hazaras in Pakistan, India, Iran, Syria, and Iraq were unable to integrate fully due to differences in physical appearance, allowing them to retain their language, culture, and ethnic identity.\n20th and 21st century.\nIn 1901, Habibullah Khan, Abdur Rahman's eldest son and successor, granted amnesty to the Hazaras and invited those exiled by his predecessor to return. However, few returned, settling instead in Turkistan and Balkh province, as they had lost their previous lands. The Hazaras continued to face social, economic, and political discrimination throughout most of the 20th century. In 1933, Muhammad Nadir Shah, the King of Afghanistan, was assassinated by Abdul Khaliq Hazara, a school student. The Afghan government later captured and executed him, along with several of his family members.\nMistrust of the central government among the Hazaras and local uprisings persisted. In particular, from 1945 to 1946, during Zahir Shah's rule, a revolt led by Ibrahim Khan, known as \"Ibrahim Gawsawar,\" erupted in response to new taxes that were imposed exclusively on the Hazaras. Meanwhile, the Kuchis were not only exempted from these taxes but also received allowances from the Afghan government. The angry rebels began capturing and killing government officials. In response, the central government sent a force to subdue the region and subsequently removed the taxes.\nThe repressive policies of the People's Democratic Party of Afghanistan (PDPA) after the Saur Revolution in 1978 led to uprisings throughout the country. Fearing Iranian influence, the Hazaras were particularly persecuted. In October 1979, President Hafizullah Amin published a list of 12,000 victims of the Taraki government, among whom were 7,000 Hazaras who had been shot in the notorious Pul-e-Charkhi prison.\nDuring the Soviet-Afghan War, the Hazarajat region did not experience as much heavy fighting as other parts of Afghanistan. Most of the Hazara mujahideen engaged in combat against the Soviets in regions on the periphery of Hazarajat. There was a division between the Tanzeem Nasle Nau Hazara, a party based in Quetta comprising Hazara nationalists and secular intellectuals, and the Islamist parties in Hazarajat. By 1979, the Hazara Islamist groups had already liberated Hazarajat from the central Soviet-backed Afghan government and subsequently took full control of the region away from the secularists. By 1984, the Islamist dominance in Hazarajat was complete. As the Soviets withdrew in 1989, the Islamist groups recognized the need to broaden their political appeal and shifted their focus toward Hazara nationalism. This shift led to the establishment of Hizbe-Wahdat, an alliance of all Hazara resistance groups, except for Harakat-e Islami.\nIn 1992, with the fall of Kabul, Harakat-e Islami sided with Burhanuddin Rabbani's government, while Hizb-e Wahdat aligned with the opposition. Hizb-e Wahdat was eventually forced out of Kabul in 1995 when the Taliban captured the city and killed their leader, Abdul Ali Mazari. Following the Taliban's capture of Kabul in 1996, all Hazara groups united with the Northern Alliance against this common enemy. However, despite fierce resistance, Hazarajat fell to the Taliban in 1998. The Taliban isolated Hazarajat from the rest of the world, even preventing the United Nations from delivering food to the provinces of Bamyan, Ghor, Maidan Wardak, and Daikundi.\nIn 1997, a revolt broke out among the Hazaras in Mazar-e Sharif when they refused to be disarmed by the Taliban, resulting in the deaths of 600 Taliban fighters in the subsequent fighting. In retaliation, the Taliban adopted the genocidal policies reminiscent of Abdur Rahman Khan's era. In 1998, six thousand Hazaras were killed in the north, with the intent of carrying out ethnic cleansing against the Hazara population. In March 2001, the two giant Buddhas of Bamiyan were destroyed, despite widespread international condemnation.\nHazaras have also played a significant role in the creation of Pakistan. One notable Hazara was Qazi Muhammad Isa of the Sheikh Ali tribe, who was a close friend of Muhammad Ali Jinnah; they met for the first time while studying in London. Qazi Muhammad Isa was the first person from his native province of Balochistan to obtain a Bar-at-Law degree and played a key role in establishing the All-India Muslim League in Balochistan.\nThough Hazaras played a role in the anti-Soviet movement, some Hazaras also participated in the new communist government, which actively courted Afghan minorities. Sultan Ali Kishtmand, a Hazara, served as the Prime Minister of Afghanistan from 1981 to 1990, with a brief interruption in 1988. The Ismaili Hazaras of Baghlan Province likewise supported the communists, and their \"pir\" (religious leader), Jaffar Naderi, led a pro-Communist militia in the region.\nDuring the following years, the Hazaras suffered severe oppression, and numerous ethnic massacres, genocides, and pogroms were carried out by the predominantly Pashtun Taliban. These events have been documented by organizations such as Human Rights Watch.\nFollowing the September 11, 2001 attacks in the United States, American and Coalition forces invaded Afghanistan. After the fall of the Taliban, many Hazaras emerged as important figures in the country. Hazaras pursued higher education, enrolled in the army, and held various top government positions. Notable Hazaras in leadership roles included Vice Presidents, ministers, and governors, such as Karim Khalili, Sarwar Danish, Sima Samar, Muhammad Mohaqiq, Habiba Sar\u0101bi, Abdul Haq Shafaq, Sayed Anwar Rahmati, Qurban Ali Urozgani, Muhammad Arif Shah Jahan, Mahmoud Baligh, Mohammad Eqbal Munib, and Mohammad Asim Asim. Azra Jafari, the mayor of Nili, Daikundi, became the first female mayor in Afghanistan. Other notable Hazaras include Sultan Ali Keshtmand, Abdul Wahed Sar\u0101bi, Akram Yari, Ghulam Ali Wahdat, Sayed Mustafa Kazemi, Ghulam Husain Naseri, Abbas Noyan, Daoud Naji, Abbas Ibrahim Zada, Ramazan Bashardost, Ahmad Shah Ramazan, Ahmad Behzad, Nasrullah Sadiqi Zada Nili, Fahim Hashimi, Maryam Monsef, and others.\nAlthough Afghanistan has historically been one of the poorest countries in the world, the Hazarajat region remained underdeveloped due to past government neglect. Since the ousting of the Taliban in late 2001, billions of dollars had been invested in Afghanistan for reconstruction, and several large-scale projects began in August 2012. For instance, more than 5,000 kilometers of road pavement have been completed across the country, with little done in central Afghanistan (Hazarajat). Conversely, the Band-e Amir in Bamyan Province became the first national park in Afghanistan. A road from Kabul to Bamyan was also constructed, along with new police stations, government institutions, hospitals, and schools in Bamyan, Daikundi, and other provinces predominantly inhabited by Hazaras. Additionally, the first ski resort in Afghanistan was established in Bamyan Province.\nDiscrimination is evident in the treatment of Kuchis (Pashtun nomads who historically migrate from region to region depending on the season), who are allowed to use the pastures of Hazarajat during the summer months. It is believed that this practice began during the rule of Abdur Rahman Khan. Living in mountainous Hazarajat, where arable farmland is scarce, the Hazara people rely on these pasture lands for their livelihood during the long and harsh winters. In 2007, some Kuchi nomads entered parts of Hazarajat to graze their livestock. When the local Hazaras resisted, a clash ensued, resulting in several deaths on both sides from gunfire. Such events continue to occur, even after the central government, including President Hamid Karzai, was compelled to intervene. In late July 2012, a Hazara police commander in Uruzgan Province reportedly rounded up and killed nine Pashtun civilians in retaliation for the deaths of two local Hazaras. The Afghan government is currently investigating this matter.\nPresident Hamid Karzai's efforts after the Peace Jirga to negotiate a deal with Taliban leaders caused deep unease among Afghanistan's minority communities, who had fought the Taliban the longest and suffered the most during their rule. Leaders of the Tajik, Uzbek, and Hazara communities vowed to resist any return of the Taliban to power, recalling the large-scale massacres of Hazara civilians during the Taliban's previous rule. The 2021 Kabul school bombing targeted a girls' school in Dashte Barchi, a predominantly Hazara area in western Kabul. The Dashte Barchi district had frequently been attacked by the Islamic State \u2013 Khorasan Province.\nFollowing the fall of Kabul to the Taliban in 2021, which marked the end of the war in Afghanistan, concerns were raised about whether the Taliban would reimpose the persecution of Hazaras as they did in the 1990s. An academic at Melbourne's La Trobe University stated that \"The Hazaras are very fearful that the Taliban will likely reinstate the policies of the 1990s,\" despite Taliban reassurances that they would not revert to their previous oppressive practices. On 6 September 2022, Human Rights Watch reported that since the Taliban took over Afghanistan in August 2021, ISIS\u2013K has claimed responsibility for 13 attacks against Hazaras and has been linked to at least three more, resulting in the deaths and injuries of at least 700 people. The Islamic State affiliate has repeatedly targeted Hazaras and other religious minorities at mosques, schools, and workplaces.\nGenetics.\nGenetically, the Hazaras have a mix of West Eurasian and East Eurasian components. Genetic data shows that Hazaras in Afghanistan cluster closely with the Uzbek population, while both groups are notably distinct from Afghanistan's Tajik and Pashtun populations. There is evidence of both paternal and maternal connections to Turkic, Mongolic, and Iranic populations.\nThe frequency of ancestral components among the Hazaras varies according to tribal affiliation. They show a high genetic affinity to present-day Turkic populations of Central Asia and East Asia, as well as to Mongolic populations. In terms of their overall genetic makeup, approximately 49% of the average gene pool of the Hazaras is derived from East Asian sources, around 48% from European sources, and approximately 0.17%, 0.47%, and 2.30% from African, Oceanian, and Amerindian sources, respectively. The genetic makeup of the Hazaras is similar to that of Uzbek, Uyghur, Kazakh, Kyrgyz, and Mongol populations.\nGenetic analyses using methods such as pairwise genetic distances, multidimensional scaling (MDS), principal component analysis (PCA), and phylogenetic reconstruction have shown that the Hazaras are genetically closer to Turkic-speaking populations \u2013 such as the Uyghur, Kazakh, and Kyrgyz of northwest China \u2013 than to Mongolians, East-Asian or Indo-Iranian populations. Additional analyses, including f3, f4, f4-ratio, qpWave, and qpAdm, indicate that while Hazaras share substantial genetic components with East Asian populations, approximately 57.8% of their ancestry can be traced to Mongolian-related sources. According Guanglin He, genetic studies further suggest that the Hazaras have undergone admixture with local and neighboring populations, resulting in their present-day East\u2013West Eurasian mixed genetic profile, which developed after their divergence from Mongolian groups.\nPaternal haplogroups.\nThe most common paternal DNA haplogroups among Hazaras from Afghanistan are the East Eurasian haplogroup C-M217 (33.33%) and the West Eurasian haplogroup R1a1a-M17 (6.67%), followed by the West Eurasian haplogroups J2-M172 and L-M20. Some Hazaras were also found to belong to the haplogroups E1b1b1-M35, L-M20, and H-M69, which they share with Tajiks, Pashtuns, and Indian populations. Additionally, one individual with the haplogroup B-M60, typically found in Eastern Africa, was identified.\nHaplogroup C2 (previously known as the C3-Star cluster) is the most frequent haplogroup among Pakistani and Afghan Hazaras. Pakistani Hazaras have a high frequency of haplogroup C-M217 at approximately 40% (10/25) and haplogroup R1b at around 32% (8/25). A relatively high frequency of R1b has also been found among Eastern Russian Tatars and Bashkirs, and all three groups are thought to be associated with the Golden Horde. Haplogroup C-M217, or C2, is the most common haplogroup in Mongol and Kazakh populations. According to Sabitov, studies indicate that Y-DNA haplogroup C2 among Hazaras is linked to the expansion of the Mongols, and, as noted by Zhabagin, this genetic marker supports the Mongolian origin of the Hazaras. According to Volkov, the Turkic genetic lineage is associated with haplogroup R1b, which is most likely linked to populations of the Dasht-i-Kipchak region who were recruited from the eastern wing of the Jochid Ulus for H\u00fcleg\u00fc's campaign into Iran.\nMaternal haplogroups.\nThe Hazaras share approximately 35% of their maternal haplogroups with contemporary East Asian populations, while about 65% are shared with West Eurasian populations. Overall, the Hazaras predominantly have West Eurasian mtDNA.\nDemographics.\nMost unbiased sources estimate that Hazaras make up approximately 9% of Afghanistan's total population. Some sources, however, claim the figure is as low as 3%, while others suggest it could be as high as 20%. The World Hazara Council even claims that around 8 to 10 million Hazaras reside in Afghanistan and the Hazara population worldwide (especially in Pakistan, Iran, and other countries, largely due to historical migration and displacement) could be as high as 14 million in 2024, which would make up to a quarter of the total Afghan population and is not backed up by any unbiased source. During the Hazara uprisings between 1888 and 1893, over 60 percent of their population was massacred and forcibly displaced. Consequently, they lost a substantial portion of their ancestral lands to non-Hazaras\u2014territory that, if retained, could have nearly doubled their current land holdings.\nGeographic distribution.\nAfghanistan.\nThe Hazaras are among the largest ethnic groups in Afghanistan, predominantly settled in the central regions known as Hazaristan (Hazarajat), with a significant presence throughout the country. Their population within Afghanistan is estimated to range from 8 to 10 million.\nUntil the 1880s, the Hazaras maintained full autonomy and controlled all of Hazarajat. The central government in Kabul had not yet succeeded in bringing them under its rule.\nCentral Asia.\nAfter the massacre and genocide of the Hazaras by Abdur Rahman from 1888 to 1893, many Hazaras migrated to Central Asian regions under Tsarist Russian occupation, including Uzbekistan, Tajikistan, Turkmenistan, and Kazakhstan, with a significant number settling in Samarkand and Bukhara. Over time, many Hazaras living in these regions lost their accent, language, and ethnic identity due to the similarities in racial structure and appearance with the local populations, leading to their assimilation.\nPakistan.\nDuring the period of British colonial rule in the Indian subcontinent in the 19th century, Hazaras worked in coal mines, road construction, and other working-class jobs during the winter months in various cities of what is now Pakistan. The earliest record of Hazaras in Pakistan dates back to Broadfoot's Sappers Company, which was established in 1835 in Quetta and also participated in the First Anglo-Afghan War. Additionally, some Hazaras worked on agricultural farms in Sindh and contributed to the construction of the Sukkur Barrage. In 1962, the government of Pakistan officially recognized the Hazaras as one of the country's ethnic groups.\nMost Pakistani Hazaras are native to Balochistan. Localities in the city of Quetta with prominent Hazara populations include Hazara Town and Mariabad. The literacy level among the Hazara community in Pakistan is relatively high compared to that of Hazaras in Afghanistan, and they have integrated well into the local society's social dynamics. Saira Batool, a Hazara woman, was one of the first female pilots in the Pakistan Air Force. Other notable Hazaras include Qazi Muhammad Isa, General Musa Khan, who served as the fourth Commander-in-Chief of the Pakistan Army from 1958 to 1968, Air Marshal Sharbat Ali Changezi, who served in the Pakistan Air Force from 1949 to 1987, Hussain Ali Yousafi, the slain chairman of the Hazara Democratic Party, and Sayed Nasir Ali Shah, a Member of the National Assembly from Quetta, along with his father Haji Sayed Hussain Hazara, who was a senator and member of the Pakistan Parliament during the Zia-ul-Haq era.\nDespite this, Hazaras are often targeted by militant groups such as Lashkar-e-Jhangvi and others. Activists report that at least 800 to 1,000 Hazaras have been killed since 1999, and the pace is quickening. According to Human Rights Watch, more than one hundred have been murdered in and around Quetta since January. The political representation of the community is served by the Hazara Democratic Party, a secular liberal democratic party headed by Abdul Khaliq Hazara.\nIran.\nThe Hazara people in Iran are also referred to as Kh\u0101wari () or Barbari (). Over many years, due to political unrest in Afghanistan, some Hazaras have migrated to Iran. Before Iran was forced to relinquish the Herat region according to the Treaty of Paris in 1857 during the reign of Naser al-Din Shah, the country possessed a much larger part of Greater Khorasan. One of the tribes that roamed this area prior to the cession was the Hazaras. After the border between Iran and Afghanistan was drawn, the tribe settled on both sides of the border. The leadership of this tribe at the end of the Qajar period and during the Pahlavi period was held by Muhammad Yusuf Khan Hazara, known as \"Sulat al-Sultanah Hazara.\" He was a Sunni Hazara, a politician, and the first Sunni representative in the Iranian Parliament, as well as the only Sunni Iranian to represent Mashhad in the history of Iran's legislatures.\nIndia.\nThe Attarwala claim descent from Hazaras who mainly inhabit the state of Gujarat, India. They are descended from a group of Mughal soldiers who were initially settled in Agra during the rule of Mughal Emperor Jahangir. According to their recorded documents, they then migrated to Ahmedabad via Gwalior, Ratlam, and Godhra. This migration followed their participation in the community during the 1857 Indian War of Independence. Once settled in Gujarat, the community took up the occupation of manufacturing perfumes known as ittars. The term \"attarwala\" means \"manufacturer of perfumes.\" A second migration occurred in 1947 from Agra after the partition of India, with some members immigrating to Pakistan, while others joined their co-ethnics in Ahmedabad.\nDiaspora.\nAlessandro Monsutti argues in his recent anthropological book that migration is a traditional way of life for the Hazara people, referring to the seasonal and historical migrations that have never ceased and do not seem to be dictated solely by emergencies such as war. Due to decades of conflict in Afghanistan and sectarian violence in Pakistan, many Hazaras have left their communities and settled in Australia, New Zealand, Canada, the United States, the United Kingdom, and particularly the Northern European countries such as Sweden and Denmark. Some migrate as exchange students, while others do so through human smuggling, which sometimes costs them their lives. Since 2001, about 1,000 people have died at sea while attempting to reach Australia by boat from Indonesia, many of whom were Hazaras. A notable case was the Tampa affair, in which a shipload of refugees, mostly Hazaras, was rescued by the Norwegian freighter MV \"Tampa\" and subsequently sent to Nauru.\nCulture and society.\nHazara culture is a rich tapestry of customs, traditions, behaviors, beliefs, and norms that have evolved over centuries. This culture has developed through a series of interactions with and responses to the surrounding peoples and environments, ultimately shaping it into a distinct cultural identity. Today, Hazara culture stands out for its unique heritage, incorporating elements from both Central Asia and South Asia while maintaining its own distinctiveness. Outside of Hazarajat, many Hazara communities have embraced aspects of the local cultures in which they reside, often blending elements of Afghan Tajiks and Pashtuns traditions. However, in Hazarajat, the heart of Hazara culture, many of the original customs and traditions remain intact. These are more closely aligned with those of Central Asia than with the Afghan Tajiks, preserving the distinct cultural legacy of the Hazara people. Traditionally, the Hazara people have been highland farmers, skillfully cultivating the mountainous regions of their homeland. While most Hazaras live in permanent homes, certain groups, such as the Aimaq Hazara, continue to maintain a semi-nomadic lifestyle. These communities often live in felt yurts rather than traditional dwellings.\nBefore the conquest of Hazarajat by the Afghan ruler Abdur Rahman between 1888 and 1893, the Hazara society was structured a feudal system. The social hierarchy was dominated by influential landowners and powerful figures, such as Khan, Beig, Arbab, Mir, or Malik, who held authority over the land and society. Below them, the Clerics (Mullahs) and Sayyids held the second tier. The economy of Hazara was largely centered on agriculture and livestock, which formed the foundation of the region's prosperity.\nAttire.\nHazara attire plays a significant role in upholding the cultural, traditional, and social identity of the Hazara ethnicity. These garments are primarily handcrafted, reflecting the community's rich heritage. In Afghanistan, Hazara clothing is sewn in various regions, with a particular focus on the central provinces.\nMale clothing.\nHazara men traditionally wear a barak (also known as barag) alongside a hat, with the barak being a key element of Hazara clothing. This soft, thick garment is crafted from the first wool sheared from special sheep raised in the Hazarajat region, making it both luxurious and durable. Beyond its regal appearance, the Hazara barak serves a practical function as a warm winter garment. Its unique properties make it resistant to moisture, allowing it to stay dry even in snow and rain. Additionally, the softness of the fabric is believed to reduce muscle pain and offer relief for joint discomfort. In contemporary times, however, the perahan o tunban has become the most common attire among Hazara men, often worn with a hat or turban.\nFemale clothing.\nThe traditional clothing of Hazara women includes a pleated skirt with a tunban or undergarment. The lower tunbans are made from fabrics such as flowered chits, while the upper skirts are crafted from finer materials like velvet, zari, or net, often adorned with a border or decoration at the bottom. The women's shirt is calf-length, with a close collar and long sleeves, featuring slits on both sides that fit over the skirts, which are appreciated for their modesty in accordance with Islamic customs. Hazara women's clothing varies according to social, economic, and age factors. Young Hazara women typically wear outfits made from different fabrics in vibrant colors and cheerful designs, complemented by beautiful and colorful chadors. In contrast, older women prefer darker fabrics with simple black and white patterns. Hazara women's chadors or head coverings are often embellished with ornaments, typically made of silver or gold, and sometimes paired with a hat. The adornments on their clothing include silver or gold necklace with colorful beads, buttons, bangles, and silver or gold bracelets.\nHeadgear.\nHazara people have a rich tradition of wearing distinct headgear, with styles varying for men and women. These hats and caps come in various forms, with some made from animal skin, while others are crafted from barak. Additionally, some Hazara men wear the traditional Khorasan turban.\nCuisine.\nThe Hazara cuisine is deeply influenced by Central Asian, South Asian, and Persian culinary traditions. Despite these influences, the Hazaras have developed a distinctive food culture, with unique dishes, cooking techniques, and flavors specific to their community. Hospitality plays a central role in their dining etiquette, and it is customary to prepare special meals when hosting guests.\nLanguage.\nThe Hazaras speak Dari and Hazaragi, eastern dialects of the Persian language.\nAccording to the \"Encyclopaedia of Islam\", Hazaragi is a dialect of Persian infused with many Turkic and some Mongolic words or loanwords. The \"Encyclop\u00e6dia Britannica\" describes Hazaragi as an eastern variety of Persian containing numerous Mongolic and Turkic words. Similarly, \"Encyclopaedia Iranica\" notes that Hazaras speak a Persian dialect with many Turkic and some Mongolic words. Other sources describe the Hazara population as speaking Persian with some Mongolic words. An \"Iranica\" article on the language of Hazaras states that the dialect consists of three linguistic layers: (1) pre-Mongol Persian, with its own substratum; (2) Mongolian; and (3) modern Tajiki, preserving elements of both (1) and (2). The primary difference between Persian and Hazaragi lies in the accent. Despite these variations, Hazaragi remains mutually intelligible with Dari, the official language of Afghanistan.\nAccording to Dr. Lutfi Temirkhanov, a Doctor of Sciences, the ancestors of the Hazaras were originally Mongol-speaking. However, following their resettlement, they began to intermingle with Persian- and Turkic-speaking populations. Temirkhanov explains, \"Hordes of Mongol princes and feudal lords found themselves in a Persian-speaking environment; they mixed with them, were influenced by Persian-Tajik culture, and gradually adopted the Persian language.\" Sayed Askar Mousavi, however, questions the theory that these military units were permanently settled by direct order of Genghis Khan or his commanders in what is now Hazarajat. He argues that no known primary sources support such a claim, and views this interpretation as lacking historical foundation. Nevertheless, a number of other historians maintain that Mongol military garrisons were indeed left behind in the region following the 13th-century invasions, and that these settlements played a significant role in the ethnogenesis of the Hazara people. According to Rashid al-Din, the Mongols established permanent military units across Central Asia, including in Khorasan. He specifically mentions a commander named Tumay, who was stationed as a military governor (amir) in Khorasan, indicating long-term garrison activity in the area.\nSome sources indicate that in the 16th century, during the time of Babur, some Hazaras still spoke a Mongolian language. According to the \"Great Russian Encyclopedia\" and other sources, some Hazaras continued to speak Mongolian until the 19th century. Temirkhanov notes that Mongolic words make up about 10% of the Hazara vocabulary. Approximately 20 percent of the Hazara vocabulary consists of Turkic and Mongolic words, with the proportion of each varying by source; some studies report a predominance of Turkic terms, while others highlight a stronger Mongolic influence.\nAccording to Sayed Askar Mousavi, the term \"Moghol Hazaras\" has not been found in historical documents, and no scholars have encountered \"Mogholi-speaking Hazaras\". However, 19th-century Hungarian orientalist \u00c1rmin V\u00e1mb\u00e9ry, who personally traveled through Afghanistan, reported that some Hazara groups in the region of Herat still spoke a Mongolic dialect in his time. He also noted that the Hazaras preserved distinct Mongolian physical traits and cultural features. Similarly, the Mughal emperor Babur, in his memoirs (Baburnama), mentioned that some Hazara communities spoke Mongolian. These historical observations are frequently cited by scholars who support a Mongol origin of the Hazaras.\nAccording to Efimov, examples of vocabulary in Hazaragi that reflect Turkic influence include ata (\"father\"), ka\u1e6da (\"big, large\"), qara (\"black\"), kunda (\"plow\"), q\u014d\u0161 (\"eyebrow\"), while words of Mongolic origin include b\u00eari (\"bride\"), ala\u1e21a (\"palm of the hand\"), qula\u1e21ay (\"thief\"), xatun (\"wife, woman\"), \u014d\u1e21il (\"village\"), and others.\nReligion.\nHazaras predominantly practice Islam, with most adhering to Shi'a Islam, a significant portion following Sunni Islam, and smaller groups practicing Isma'ili and Non-denominational Islam. The majority of Afghanistan's population practices Sunni Islam, which may have contributed to the discrimination Hazaras face.\nShia Hazaras.\nThere is no definitive theory regarding the acceptance of Shi'a Islam by the majority of Hazaras. It is possible that most Hazaras adopted Shi'a Islam in the early 16th century, during the initial years of the Safavid dynasty.\nSunni Hazaras.\nSunni Hazaras have practiced Sunni Islam for a long time, predating the Afghan Amir, Abdul Rahman's occupation of Hazara lands. However, some were forcefully converted from Shi'a to Sunni Islam following Abdur Rahman's occupation and the Hazara genocide. In Afghanistan, they primarily inhabit the provinces of Baghlan, Badghis, Ghor, Kunduz, Panjshir, Bamyan, Badakhshan, Parwan, and Kabul.\nSher Muhammad Khan Hazara, a Sunni Hazara and chieftain of the Hazaras of Qala-e-Naw, Badghis, was a warlord who participated in the Sunni coalition that defended Herat in 1837. He was also one of those who defeated British forces around Qandahar and in the Maiwand desert during the First Anglo-Afghan War (1838\u20131842).\nDuring the 1996-2001 Afghan Civil War, the Taliban also forcefully converted Shia Hazaras into Sunni.\nIsma'ili Hazaras.\nIsma'ili Hazaras primarily reside in the provinces of Kabul, Parwan, Baghlan, Bamyan, Maidan Wardak, Samangan, and Zabul. They have historically been separated from other Hazaras due to religious beliefs and political reasons.\nHazara tribes.\nThe Hazara people are organized into various tribes. Some prominent Hazara tribes include Sheikh Ali, Jaghori, Jaghatu, Qara Baghi, Muhammad Khwaja, Behsudi, Dai Mirdad, Turkmani, Uruzgani, Daikundi, Daizangi, Daichopan, Daizinyat, Qarlugh, Aimaq Hazara, and others.\nArt.\nWriters and poets.\nSome well-known Hazara writers and poets include Faiz Muhammad Kateb, Amir Khosrow Dehlavi, Ismael Balkhi, Hassan Poladi, Kazim Yazdani, Ali Mohaqiq Nasab, Kamran Mir Hazar, Basir Ahang, Sayed Askar Mousavi, Ali Baba Taj, Sayed Abutalib Mozaffari, Rahnaward Zaryab, and Aziz Royesh, among others.\nMusic.\nMany Hazara musicians are widely recognized for their skill in playing the dambura, a native lute instrument also found in other Central Asian countries such as Kazakhstan, Uzbekistan, and Tajikistan. Notable Hazara musicians and dambura players include Sarwar Sarkhosh, Dawood Sarkhosh, Safdar Tawakoli, and Sayed Anwar Azad, among others. Revolutionary hymns are particularly common in Hazara dambura music, with Sarwar Sarkhosh being the first singer to popularize them. His main message centered on the uprising of the younger generation and the fight against oppression. Additionally, the ghaychak, a traditional field instrument, is played similarly to a fiddle. Its resonance bowl is typically made from walnuts or berries, and its strings are metal, making it one of the stringed instruments in Hazara music.\nRenowned Pakistani musician Nusrat Fateh Ali Khan is also a descendant of the Hazaras of Afghanistan.\nCinema.\nSome well-known Hazara actors and actresses are Hussain Sadiqi, Abid Ali Nazish, Shamila Shirzad, Nikbakht Noruz, and others.\nSports.\nMany Hazaras engage in various sports, including football, volleyball, wrestling, martial arts, boxing, karate, taekwondo, judo, wushu, Jujitsu, cricket, tennis, and more. Pahlawan Ebrahim Khedri, a 62\u00a0kg wrestler, was the national champion in Afghanistan for two decades. Another famous Hazara wrestler, Wakil Hussain Allahdad, was killed in the suicide bombing in Dashte Barchi, Kabul, on 22 April 2018.\nRohullah Nikpai, won a bronze medal in Taekwondo at the 2008 Beijing Olympics, defeating world champion Juan Antonio Ramos of Spain 4\u20131 in the playoff final. This achievement marked Afghanistan's first-ever Olympic medal. He then won a second Olympic medal for Afghanistan at the London 2012 Games.\nAnother notable Hazara athlete, Sayed Abdul Jalil Waiz, was the first badminton player to represent Afghanistan in the Asian Junior Championships in 2005, where he secured the first win for his country against Iraq with scores of 15\u201313 and 15\u20131. He has participated in several international championships since 2005, achieving victories against competitors from Australia, the Philippines, and Mongolia. Hamid Rahimi is a Hazara boxer from Afghanistan who currently lives in Germany. Hussain Sadiqi is a Hazara Australian martial artist who won an award for the best fight scene in an Australian-made action movie.\nHazara football players include Zohib Islam Amiri, who currently plays for the Afghanistan national football team; Moshtaq Yaqoubi, an Afghan-Finnish footballer who plays for HIFK; Mustafa Amini, a Hazara Australian footballer who plays as a midfielder for Danish Superliga club AGF and the Australian national team; Rahmat Akbari, an Australian footballer who plays as a midfielder for Brisbane Roar. Other notable players include Rohullah Iqbalzada, Omran Haydary, Zelfy Nazary, Moshtaq Ahmadi, and Zahra Mahmoodi.\nSome Hazaras from Pakistan have also excelled in sports and received numerous awards, particularly in boxing, football, and field hockey.\nPakistani Hazara Abrar Hussain, a former Olympic boxer, served as the deputy director-general of the Pakistan Sports Board. He represented Pakistan three times at the Olympics and won a gold medal at the 1990 Asian Games in Beijing. Another Hazara boxer from Pakistan is Haider Ali, a Commonwealth Games gold medalist and Olympian who is currently retired.\nFormer captain of the Pakistan national football team, Qayyum Changezi, was the second Pakistani footballer to score a hat trick in an international game. New Hazara youngsters are emerging in football in Pakistan, mostly from Quetta, including Muhammad Ali and Rajab Ali Hazara.\nAnother notable figure is Kulsoom Hazara, a celebrated Pakistani karate champion who has earned numerous gold, silver, and bronze medals at both national and international levels. She has also been honored with the prestigious Pride of Pakistan Award. Other notable Hazara athletes in karate include Nargis Hameedullah, who made history as the first Pakistani woman to win a bronze medal at the Asian Games in karate, and Shahida Abbasi, a gold medalist and the first Pakistani woman to represent the country internationally in kata.\nCultural sports.\nThe cultural sports of the Hazara people are those that have been passed down through generations from their ancestors.\nBuzkashi.\nBuzkashi is a Central Asian sport in which horse-mounted players attempt to place a goat or calf carcass into a goal. It is the national sport of Afghanistan and is one of the cultural sports of the Hazara people, who continue to practice this sport in Afghanistan.\nTirand\u0101zi.\nTirand\u0101zi is a form of archery and an ancient cultural sport of the Hazaras.\nPahlawani.\nPahlawani, or Kushti, is a traditional wrestling sport practiced by the Hazaras. It has a long history in Afghanistan and is particularly significant among the Hazara community. During holidays, Pahlawani fields are set up for competitions, which are held across different age groups. This cultural sport features its own unique techniques. Due to its ancient roots and familiarity, Pahlawani has been passed down from generation to generation among the Hazaras.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14132", "revid": "1304395678", "url": "https://en.wikipedia.org/wiki?curid=14132", "title": "Hawala", "text": "Informal currency transfer system\nHawala or hewala ( , meaning \"transfer\" or sometimes \"trust\"), originating in India as havala (), also known as ' in Persian, and ' or in Somali, is a popular and informal value transfer system based on the performance and honour of a huge network of money brokers (known as \"hawaladars\"). \nThey operate outside of, or parallel to, traditional banking, financial channels and remittance systems. The system requires a minimum of two hawaladars that take care of the \"transaction\" without the movement of cash or telegraphic transfer. While hawaladars are spread throughout the world, they are primarily located in the Middle East, North Africa, the Horn of Africa and the Indian subcontinent. Hawala follows Islamic traditions, but its use is not limited to Muslims.\nOrigins.\nThe hawala system originated in India. In 2003 Hawala as a legal concept was documented, finding evidence of Hawala reaching back to 1327, in a publication by Matthias Schramm and Markus Taube, with the title \"Evolution and institutional foundation of the hawala financial system\". \nIt has been speculated that \"Hawala\" itself influenced the development of the agency in common law and in civil laws, such as the \"aval\" in French law, the in Portuguese law, and the \"avallo\" in Italian law. The words \"aval\" and \"avallo\" bear a similarity to \"hawala,\" and the context of intensive trade between Italian cities and the Muslim world suggests a possible link. The transfer of debt was \"not permissible under Roman law but became widely practiced in medieval Europe, especially in commercial transactions\", potentially borrowing from hawala. Agency was also \"an institution unknown to Roman law\" as no \"individual could conclude a binding contract on behalf of another as his agent\". On the other hand, Islamic law and the later common law \"had no difficulty in accepting agency as one of its institutions in the field of contracts and of obligations in general\". The claims about the Islamic origins of hawala have later been challenged by Cinar.\nRegulation.\nFollowing the September 11 attacks in 2001, international organizations responsible for counterterrorism and enforcing laws against money laundering have directed their efforts on identifying problems within the hawala, as well as other remittance systems. The First International Conference on Hawala in May 2002 published the \"Regulatory Frameworks for Hawala and Other Remittance Systems\". The International Monetary Fund (IMF) contributed a chapter, in which informal value transfer systems were considered. According to the IMF, countries with limited financial services experience macroeconomic consequences because residents rely heavily on informal fund transfer systems. Informal value transfer systems share common characteristics, including anonymity and lack of regulation or official scrutiny. Therefore informal value transfer systems may be susceptible to use by criminal organizations for money laundering and terrorist financing.\nProcedure.\nIn the most basic variant of the hawala system, money is transferred via a network of hawala brokers, or \"hawaladars\", without actually moving money. According to the author Sam Vaknin, there are large hawaladar operators with networks of middlemen in cities across many countries, but most hawaladars are small businesses who work at hawala as a sideline or moonlighting operation.\nIn general, the process of hawala operates as follows:\nThe unique feature of the system is that no promissory instruments are exchanged between the hawala brokers: the transaction takes place entirely on the honour system. As the system does not depend on the legal enforceability of claims, it can operate even in the absence of a legal and juridical environment. Trust and extensive use of connections are the components that distinguish it from other remittance systems. Hawaladar networks are often based on membership in the same family, village, clan or ethnic group, and cheating is punished by effective excommunication and the loss of honour, which lead to severe economic hardship.\nInformal records are produced of individual transactions, and a running tally of the amount owed by one broker to another is kept. Settlements of debts between hawala brokers can take a variety of forms (e.g., goods, services, properties, transfers of employees, etc.), and need not take the form of direct cash transactions.\nIn addition to commissions, hawala brokers often earn their profits through bypassing official exchange rates. Generally, the funds enter the system in the source country's currency and leave the system in the recipient country's currency. As settlements often take place without any foreign exchange transactions, they can be made at other than official exchange rates.\nHawala is attractive to customers because it provides a fast and convenient transfer of funds, usually with a far lower commission than that charged by banks. Its advantages are most pronounced when the receiving country applies unprofitable exchange rate regulations or when the banking system in the receiving country is less complex (e.g., due to differences in the legal environment in places such as Afghanistan, Yemen, and Somalia). Moreover, in some parts of the world, it is the only option for legitimate fund transfers. It has been used even by aid organizations in areas in which it is the best-functioning institution.\nRegional variants.\nDubai has been prominent for decades as a welcoming hub for hawala transactions worldwide.\nSouth Asia.\nHundis.\nThe \"hundi\" is a financial instrument that developed on the Indian sub-continent for use in trade and credit transactions. Hundis are used as a form of remittance instrument to transfer money from place to place, as a form of credit instrument or IOU to borrow money and as a bill of exchange in trade transactions. The Reserve Bank of India describes the Hundi as \"an unconditional order in writing made by a person directing another to pay a certain sum of money to a person named in the order\".\nHorn of Africa.\nAccording to the CIA, with the dissolution of Somalia's formal banking system, many informal money transfer operators arose to fill the void. It estimates that such \"hawaladars\", \"xawilaad\" or \"xawala\" brokers are now responsible for the transfer of up to $1.6 billion per year in remittances to the country, most coming from working Somalis outside Somalia. Such funds have in turn had a stimulating effect on local business activity.\nWest Africa.\nThe 2012 Tuareg rebellion left Northern Mali without an official money transfer service for months. The coping mechanisms that appeared were patterned on the hawala system.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14133", "revid": "45925219", "url": "https://en.wikipedia.org/wiki?curid=14133", "title": "Hydroponics", "text": "Growing plants without soil using nutrients in water\nHydroponics is a type of horticulture and a subset of hydroculture which involves growing plants, usually crops or medicinal plants, without soil, by using water-based mineral nutrient solutions in an artificial environment. Terrestrial or aquatic plants may grow freely with their roots exposed to the nutritious liquid or the roots may be mechanically supported by an inert medium such as perlite, gravel, or other substrates.\nDespite inert media, roots can cause changes of the rhizosphere pH and root exudates can affect rhizosphere biology and physiological balance of the nutrient solution when secondary metabolites are produced in plants. Transgenic plants grown hydroponically allow the release of pharmaceutical proteins as part of the root exudate into the hydroponic medium.\nThe nutrients used in hydroponic systems can come from many different organic or inorganic sources, including fish excrement, duck manure, purchased chemical fertilizers, or artificial standard or hybrid nutrient solutions.\nIn contrast to field cultivation, plants are commonly grown hydroponically in a greenhouse or contained environment on inert media, adapted to the controlled-environment agriculture (CEA) process. Plants commonly grown hydroponically include tomatoes, peppers, cucumbers, strawberries, lettuces, and cannabis, usually for commercial use, as well as \"Arabidopsis thaliana\", which serves as a model organism in plant science and genetics.\nHydroponics offers many advantages, notably a decrease in water usage in agriculture. To grow of tomatoes using\nHydroponic cultures lead to highest biomass and protein production compared to other growth substrates, of plants cultivated in the same environmental conditions and supplied with equal amounts of nutrients.\nHydroponics is not only used on earth, but has also proven itself in plant production experiments in Earth orbit.\nHistory.\nThe earliest published work on growing terrestrial plants without soil was the 1627 book \"Sylva Sylvarum\" or 'A Natural History' by Francis Bacon, printed a year after his death. As a result of his work, water culture became a popular research technique. In 1699, John Woodward published his water culture experiments with spearmint. He found that plants in less-pure water sources grew better than plants in distilled water. By 1842, a list of nine elements believed to be essential for plant growth had been compiled, and the discoveries of German botanists Julius von Sachs and Wilhelm Knop, in the years 1859\u20131875, resulted in a development of the technique of soilless cultivation. To quote von Sachs directly: \"In the year 1860, I published the results of experiments which demonstrated that land plants are capable of absorbing their nutritive matters out of watery solutions, without the aid of soil, and that it is possible in this way not only to maintain plants alive and growing for a long time, as had long been known, but also to bring about a vigorous increase of their organic substance, and even the production of seed capable of germination.\" Growth of terrestrial plants without soil in mineral nutrient solutions was later called \"solution culture\" in reference to \"soil culture\". It quickly became a standard research and teaching technique in the 19th and 20th centuries and is still widely used in plant nutrition science.\nAround the 1930s plant nutritionists investigated diseases of certain plants, and thereby, observed symptoms related to existing soil conditions such as salinity or nutrient deficiency. In this context, water culture experiments were undertaken with the hope of delivering similar symptoms under controlled laboratory conditions. This approach forced by Dennis Robert Hoagland led to innovative model systems (e.g., green algae Nitella) and standardized nutrient recipes playing an increasingly important role in modern plant physiology. In 1929, William Frederick Gericke of the University of California at Berkeley began publicly promoting that the principles of solution culture be used for agricultural crop production. He first termed this cultivation method \"aquiculture\" created in analogy to \"agriculture\" but later found that the cognate term aquaculture was already applied to culture of aquatic organisms. Gericke created a sensation by growing tomato vines high in his backyard in mineral nutrient solutions rather than soil. He then introduced the term \"Hydroponics\", water culture, in 1937, proposed to him by , a phycologist with an extensive education in the classics. Hydroponics is derived from neologism \u03c5\u03b4\u03c1\u03c9\u03c0\u03bf\u03bd\u03b9\u03ba\u03ac (derived from Greek \u03cd\u03b4\u03c9\u03c1=water and \u03c0\u03bf\u03bd\u03ad\u03c9=cultivate), constructed in analogy to \u03b3\u03b5\u03c9\u03c0\u03bf\u03bd\u03b9\u03ba\u03ac (derived from Greek \u03b3\u03b1\u03af\u03b1=earth and \u03c0\u03bf\u03bd\u03ad\u03c9=cultivate), geoponica, that which concerns agriculture, replacing, \u03b3\u03b5\u03c9-, earth, with \u1f51\u03b4\u03c1\u03bf-, water.\nDespite initial successes, however, Gericke realized that the time was not yet ripe for the general technical application and commercial use of hydroponics for producing crops. He also wanted to make sure all aspects of hydroponic cultivation were researched and tested before making any of the specifics available to the public. Reports of Gericke's work and his claims that hydroponics would revolutionize plant agriculture prompted a huge number of requests for further information. Gericke had been denied use of the university's greenhouses for his experiments due to the administration's skepticism, and when the university tried to compel him to release his preliminary nutrient recipes developed at home, he requested greenhouse space and time to improve them using appropriate research facilities. While he was eventually provided greenhouse space, the university assigned Hoagland and Arnon to re-evaluate Gericke's claims and show his formula held no benefit over soil grown plant yields, a view held by Hoagland. Because of these irreconcilable conflicts, Gericke left his academic position in 1937 in a climate that was politically unfavorable and continued his research independently in his greenhouse. In 1940, Gericke, whose work is considered to be the basis for all forms of hydroponic growing, published the book, \"Complete Guide to Soilless Gardening\". Therein, for the first time, he published his basic formulas involving the macro- and micronutrient salts for hydroponically grown plants.\nAs a result of research of Gericke's claims by order of the Director of the \"California Agricultural Experiment Station\" of the University of California, Claude Hutchison, Dennis Hoagland and Daniel Arnon wrote a classic 1938 agricultural bulletin, \"The Water Culture Method for Growing Plants Without Soil\", one of the most important works on solution culture ever, which made the claim that hydroponic crop yields were no better than crop yields obtained with good-quality soils. Ultimately, crop yields would be limited by factors other than mineral nutrients, especially light and aeration of the culture medium. However, in the introduction to his landmark book on soilless cultivation, published two years later, Gericke pointed out that the results published by Hoagland and Arnon in comparing the yields of experimental plants in sand, soil and solution cultures, were based on several systemic errors (\"...these experimenters have made the mistake of limiting the productive capacity of hydroponics to that of soil. Comparison can be only by growing as great a number of plants in each case as the fertility of the culture medium can support.\").\nFor example, the Hoagland and Arnon study did not adequately appreciate that hydroponics has other key benefits compared to soil culture including the fact that the roots of the plant have constant access to oxygen and that the plants have access to as much or as little water and nutrients as they need. This is important as one of the most common errors when cultivating plants is over- and underwatering; hydroponics prevents this from occurring as large amounts of water, which may drown root systems in soil, can be made available to the plant in hydroponics, and any water not used, is drained away, recirculated, or actively aerated, eliminating anoxic conditions in the root area. In soil, a grower needs to be very experienced to know exactly how much water to feed the plant. Too much and the plant will be unable to access oxygen because air in the soil pores is displaced, which can lead to root rot; too little and the plant will undergo water stress or lose the ability to absorb nutrients, which are typically moved into the roots while dissolved, leading to nutrient deficiency symptoms such as chlorosis or fertilizer burn. Eventually, Gericke's advanced ideas led to the implementation of hydroponics into commercial agriculture while Hoagland's views and helpful support by the University prompted Hoagland and his associates to develop several new formulas (recipes) for mineral nutrient solutions, universally known as Hoagland solution.\nOne of the earliest successes of hydroponics occurred on Wake Island, a rocky atoll in the Pacific Ocean used as a refueling stop for Pan American Airlines. Hydroponics was used there in the 1930s to grow vegetables for the passengers. Hydroponics was a necessity on Wake Island because there was no soil, and it was prohibitively expensive to airlift in fresh vegetables.\nFrom 1943 to 1946, Daniel I. Arnon served as a major in the United States Army and used his prior expertise with plant nutrition to feed troops stationed on barren Ponape Island in the western Pacific by growing crops in gravel and nutrient-rich water because there was no arable land available.\nIn the 1960s, Allen Cooper of England developed the nutrient film technique. The Land Pavilion at Walt Disney World's EPCOT Center opened in 1982 and prominently features a variety of hydroponic techniques.\nIn recent decades, NASA has done extensive hydroponic research for its Controlled Ecological Life Support System (CELSS) and Advanced Life Support (ALS) programs. Hydroponics research mimicking space environments will need further study for different gravity environments, for example u-gravity in Low Earth Orbit, 1/6 g on the Moon, and 1/3 g on Mars. Ray Wheeler, a plant physiologist at Kennedy Space Center's Space Life Science Lab, believes that hydroponics will allow water and nutrient recycling needed for space travel and eventual bioregenerative life support systems where plants are used to produce oxygen and food, while removing carbon dioxide.\nAs of 2017, Canada had hundreds of acres of large-scale commercial hydroponic greenhouses, producing tomatoes, peppers and cucumbers.\nDue to technological advancements within the industry and numerous economic factors, the global hydroponics market is forecast to grow from US$226.45 million in 2016 to US$724.87 million by 2023.\nTechniques.\nHydroponic systems typically fall into two irrigation categories: sub-irrigation systems, where nutrient solution is supplied from below and roots absorb moisture upward (e.g., deep water culture, ebb-and-flow), and top-irrigation systems, where nutrient solution is applied from above through drip emitters or sprayers (e.g., nutrient film technique, aeroponics).Hydroponic techniques aim to simultaneously optimize the water, nutrient and oxygen supply to the plant roots. For all techniques, most hydroponic reservoirs are now built of plastic, but other materials have been used, including concrete, glass, metal, vegetable solids, and wood. The containers should exclude light to prevent algae and fungal growth in the hydroponic medium.\nStatic solution culture.\nIn static solution culture, plants are grown in containers of nutrient solution, such as glass Mason jars (typically, in-home applications), pots, buckets, tubs, or tanks. The solution is usually gently aerated but may be un-aerated. If un-aerated, the solution level is kept low enough that enough roots are above the solution so they get adequate oxygen. A hole is cut (or drilled) in the top of the reservoir for each plant; if it is a jar or tub, it may be its lid, but otherwise, cardboard, foil, paper, wood or metal may be put on top. A single reservoir can be dedicated to a single plant, or to various plants. Reservoir size can be increased as plant size increases. A home-made system can be constructed from food containers or glass canning jars with aeration provided by an aquarium pump, aquarium airline tubing, aquarium valves or even a biofilm of green algae on the glass, through photosynthesis. Clear containers can also be covered with aluminium foil, butcher paper, black plastic, or other material to eliminate the effects of negative phototropism. The nutrient solution is changed either on a schedule, such as once per week, or when the concentration drops below a certain level as determined with an electrical conductivity meter. Whenever the solution is depleted below a certain level, either water or fresh nutrient solution is added. A Mariotte's bottle, or a float valve, can be used to automatically maintain the solution level. In raft solution culture, plants are placed in a sheet of buoyant plastic that is floated on the surface of the nutrient solution. That way, the solution level never drops below the roots.\nContinuous-flow solution culture.\nIn continuous-flow solution culture, the nutrient solution constantly flows past the roots. It is much easier to automate than the static solution culture because sampling and adjustments to the temperature, pH, and nutrient concentrations can be made in a large storage tank that has potential to serve thousands of plants. A popular variation is the nutrient film technique or NFT, whereby a very shallow stream of water containing all the dissolved nutrients required for plant growth is recirculated in a thin layer past a bare root mat of plants in a watertight channel, with an upper surface exposed to air. As a consequence, an abundant supply of oxygen is provided to the roots of the plants. A properly designed NFT system is based on using the right channel slope, the right flow rate, and the right channel length. The main advantage of the NFT system over other forms of hydroponics is that the plant roots are exposed to adequate supplies of water, oxygen, and nutrients. In all other forms of production, there is a conflict between the supply of these requirements, since excessive or deficient amounts of one results in an imbalance of one or both of the others. NFT, because of its design, provides a system where all three requirements for healthy plant growth can be met at the same time, provided that the simple concept of NFT is always remembered and practised. The result of these advantages is that higher yields of high-quality produce are obtained over an extended period of cropping. A downside of NFT is that it has very little buffering against interruptions in the flow (e.g., power outages). But, overall, it is probably one of the more productive techniques.\nThe same design characteristics apply to all conventional NFT systems. While slopes along channels of 1:100 have been recommended, in practice it is difficult to build a base for channels that is sufficiently true to enable nutrient films to flow without ponding in locally depressed areas. As a consequence, it is recommended that slopes of 1:30 to 1:40 are used. This allows for minor irregularities in the surface, but, even with these slopes, ponding and water logging may occur. The slope may be provided by the floor, benches or racks may hold the channels and provide the required slope. Both methods are used and depend on local requirements, often determined by the site and crop requirements.\nFor nutrient film technique (NFT) systems, recommended flow rates are commonly around 1 L/min per gully to provide sufficient nutrient replenishment to the root film. At planting, rates may be half this and the upper limit of 2 L/min appears about the maximum. Flow rates beyond these extremes are often associated with nutritional problems. Depressed growth rates of many crops have been observed when channels exceed 12 meters in length. On rapidly growing crops, tests have indicated that, while oxygen levels remain adequate, nitrogen may be depleted over the length of the gully. As a consequence, channel length should not exceed 10\u201315 meters. In situations where this is not possible, the reductions in growth can be eliminated by placing another nutrient feed halfway along the gully and halving the flow rates through each outlet.\nAeroponics.\nAeroponics is a system wherein roots are continuously or discontinuously kept in an environment saturated with fine drops (a mist or aerosol) of nutrient solution. The method requires no substrate and entails growing plants with their roots suspended in a deep air or growth chamber with the roots periodically wetted with a fine mist of atomized nutrients. Excellent aeration is the main advantage of aeroponics.\nAeroponic techniques have proven to be commercially successful for propagation, seed germination, seed potato production, tomato production, leaf crops, and micro-greens. Since inventor Richard Stoner commercialized aeroponic technology in 1983, aeroponics has been implemented as an alternative to water intensive hydroponic systems worldwide. A major limitation of hydroponics is the fact that of water can only hold of air, no matter whether aerators are utilized or not.\nAnother distinct advantage of aeroponics over hydroponics is that any species of plants can be grown in a true aeroponic system because the microenvironment of an aeroponic can be finely controlled. Another limitation of hydroponics is that certain species of plants can only survive for so long in water before they become waterlogged. In contrast, suspended aeroponic plants receive 100% of the available oxygen and carbon dioxide to their roots zone, stems, and leaves, thus accelerating biomass growth and reducing rooting times. NASA research has shown that aeroponically grown plants have an 80% increase in dry weight biomass (essential minerals) compared to hydroponically grown plants. Aeroponics also uses 65% less water than hydroponics. NASA concluded that aeroponically grown plants require \u00bc the nutrient input compared to hydroponics. Unlike hydroponically grown plants, aeroponically grown plants will not suffer transplant shock when transplanted to soil, and offers growers the ability to reduce the spread of disease and pathogens.\nAeroponics is also widely used in laboratory studies of plant physiology and plant pathology. Aeroponic techniques have been given special attention from NASA since a mist is easier to handle than a liquid in a zero-gravity environment.\nFogponics.\nFogponics is a derivation of aeroponics wherein the nutrient solution is aerosolized by a diaphragm vibrating at ultrasonic frequencies. Solution droplets produced by this method tend to be 5\u201310\u00a0\u03bcm in diameter, smaller than those produced by forcing a nutrient solution through pressurized nozzles, as in aeroponics. The smaller size of the droplets allows them to diffuse through the air more easily, and deliver nutrients to the roots without limiting their access to oxygen.\nPassive sub-irrigation.\nPassive sub-irrigation, also known as passive hydroponics, semi-hydroponics, or \"hydroculture\", is a method wherein plants are grown in an inert porous medium that moves water and fertilizer to the roots by capillary action from a separate reservoir as necessary, reducing labor and providing a constant supply of water to the roots. In the simplest method, the pot sits in a shallow solution of fertilizer and water or on a capillary mat saturated with nutrient solution. The various hydroponic media available, such as expanded clay and coconut husk, contain more air space than more traditional potting mixes, delivering increased oxygen to the roots, which is important in epiphytic plants such as orchids and bromeliads, whose roots are exposed to the air in nature. Additional advantages of passive hydroponics are the reduction of root rot.\nEbb and flow (flood and drain) sub-irrigation.\nIn its simplest form, nutrient-enriched water is pumped into containers with plants in a growing medium such as Expanded clay aggregate At regular intervals, a simple timer causes a pump to fill the containers with nutrient solution, after which the solution drains back down into the reservoir. This keeps the medium regularly flushed with nutrients and air.\nRun-to-waste.\nIn a run-to-waste system, nutrient and water solution is periodically applied to the medium surface. The method was invented in Bengal in 1946; for this reason it is sometimes referred to as \"The Bengal System\".\nThis method can be set up in various configurations. In its simplest form, a nutrient-and-water solution is manually applied one or more times per day to a container of inert growing media, such as rockwool, perlite, vermiculite, coco fibre, or sand. In a slightly more complex system, it is automated with a delivery pump, a timer and irrigation tubing to deliver nutrient solution with a delivery frequency that is governed by the key parameters of plant size, plant growing stage, climate, substrate, and substrate conductivity, pH, and water content.\nIn a commercial setting, watering frequency is multi-factorial and governed by computers or PLCs.\nCommercial hydroponics production of large plants like tomatoes, cucumber, and peppers uses one form or another of run-to-waste hydroponics.\nDeep water culture.\nThe hydroponic method of plant production by means of suspending the plant roots in a solution of nutrient-rich, oxygenated water. Traditional methods favor the use of plastic buckets and large containers with the plant contained in a net pot suspended from the centre of the lid and the roots suspended in the nutrient solution.\nThe solution is oxygen saturated by an air pump combined with porous stones. With this method, the plants grow much faster because of the high amount of oxygen that the roots receive. The Kratky Method is similar to deep water culture, but uses a non-circulating water reservoir.\nTop-fed deep water culture.\n\"Top-fed\" deep water culture is a technique involving delivering highly oxygenated nutrient solution direct to the root zone of plants. While deep water culture involves the plant roots hanging down into a reservoir of nutrient solution, in top-fed deep water culture the solution is pumped from the reservoir up to the roots (top feeding). The water is released over the plant's roots and then runs back into the reservoir below in a constantly recirculating system. As with deep water culture, there is an airstone in the reservoir that pumps air into the water via a hose from outside the reservoir. The airstone helps add oxygen to the water. Both the airstone and the water pump run 24 hours a day.\nThe biggest advantage of top-fed deep water culture over standard deep water culture is increased growth during the first few weeks. With deep water culture, there is a time when the roots have not reached the water yet. With top-fed deep water culture, the roots get easy access to water from the beginning and will grow to the reservoir below much more quickly than with a deep water culture system. Once the roots have reached the reservoir below, there is not a huge advantage with top-fed deep water culture over standard deep water culture. However, due to the quicker growth in the beginning, grow time can be reduced by a few weeks.\nAdvantages.\nHydrozones lie at the intersection of urban agriculture innovations, environmental concerns, and biodiversity conservation efforts. Notable examples include specialized botanical gardens, cultivation facilities for threatened endemic species, and domestic spaces for advanced horticulture enthusiasts.\nRotary.\nA rotary hydroponic garden is a style of commercial hydroponics created within a circular frame which rotates continuously during the entire growth cycle of whatever plant is being grown.\nWhile system specifics vary, systems typically rotate once per hour, giving a plant 24 full turns within the circle each 24-hour period. Within the center of each rotary hydroponic garden can be a high intensity grow light, designed to simulate sunlight, often with the assistance of a mechanized timer.\nEach day, as the plants rotate, they are periodically watered with a hydroponic growth solution to provide all nutrients necessary for robust growth. Due to the plants continuous fight against gravity, plants typically mature much more quickly than when grown in soil or other traditional hydroponic growing systems. Because rotary hydroponic systems have a small size, they allow for more plant material to be grown per area of floor space than other traditional hydroponic systems.\nRotary hydroponic systems should be avoided in most circumstances, mainly because of their experimental nature and their high costs for finding, buying, operating, and maintaining them.\nVertical farming.\nSome benefits of vertical farming include that plants grown with this technique can take place inside, be stacked up in layers, and can take advantage of soilless plant-growing techniques such as hydroponics.\nEnvironmental benefits.\nHydroponic farming offers several environmental benefits when compared to traditional agriculture. The most significant of these is reduced water consumption and controlled nutrient usage. Hydroponic systems can use up to 90% less water when compared to conventional farming. Also, in hydroponic systems, water and nutrients are recirculated in a controlled environment, eliminating runoff and the discharge of pollutants into local waterways.\nBy using hydroponics to grow crops indoors or in greenhouses, land use is minimized, reserving arable soil and land for other purposes. Also, utilizing the controlled environment created for hydroponic farming reduces the need for pesticides and other chemicals. This is due to the fact that many pests and diseases in farming are soil-borne. Since hydroponics uses other substrates, eliminating soil use, these farming obstacles are reduced.\nUsing hydroponics systems that grow vertically in a space-efficient manner also makes cultivating crops in urban areas possible. However, These systems can use large amounts of energy due to the use of water filtration systems and artificial lighting. Due to this, the carbon footprint of a hydroponic farm can vary depending on factors like the energy source, local climate, and the scale of the operation. Using renewable energy sources such as solar panels has the possibility of making hydroponic farms more sustainable.\nResource use.\nHydroponic systems use less water than traditional farming due to the system's ability to recirculate water rather than absorb it from the soil or lose it to evaporation. Nutrients are also efficiently delivered to plant roots, minimizing nutrient waste and lowering the cost of fertilizing crops.\nHigher yields and faster growth.\nBecause plants are provided with water, nutrients, and light in a controlled environment, hydroponics allows crops to grow faster and potentially yield more within the same or smaller footprint. Some studies show increases of up to 20\u201330% in crop yield when compared to traditional farming methods.\nYear-round production.\nBecause hydroponic crops can be grown indoors in controlled environments, crops are not dependent on growing seasons or climate. Additionally, extreme weather conditions such as drought and freezing temperatures are less impactful to crops. This stabilizes production and allows hydroponics to produce crops more consistently year-round than traditional farming.\nPest and disease control.\nSince hydroponics uses substrates instead of soil as the base for root growth, soil-borne diseases and pests are eliminated. This reduces the use of chemical pesticides and lowers crop maintenance costs.\nUrban crop growth.\nHydroponic gardens can be set up in urban areas with little to no arable land. They can be constructed on rooftops, in warehouses, or other available space. This provides the opportunity for urban neighborhoods to have crops grown closer to them, allowing for closer delivery and fresher produce for consumers.\nSubstrates (growing support materials).\nDifferent media are appropriate for different growing techniques.\nRock wool.\nRock wool (mineral wool) is the most widely used medium in hydroponics. Rock wool is an inert substrate suitable for both run-to-waste and recirculating systems. Rock wool is made from molten rock, basalt or 'slag' that is spun into bundles of single filament fibres, and bonded into a medium capable of capillary action, and is, in effect, protected from most common microbiological degradation. Rock wool is typically used only for the seedling stage, or with newly cut clones, but can remain with the plant base for its lifetime. Rock wool has many advantages and some disadvantages. The latter being the possible skin irritancy (mechanical) whilst handling (1:1000). Flushing with cold water usually brings relief. Advantages include its proven efficiency and effectiveness as a commercial hydroponic substrate. Most of the rock wool sold to date is a non-hazardous, non-carcinogenic material, falling under Note Q of the European Union Classification Packaging and Labeling Regulation (CLP).\nMineral wool products can be engineered to hold large quantities of water and air that aid root growth and nutrient uptake in hydroponics; their fibrous nature also provides a good mechanical structure to hold the plant stable. The naturally high pH of mineral wool makes them initially unsuitable to plant growth and requires \"conditioning\" to produce a wool with an appropriate, stable pH.\nExpanded clay aggregate.\nBaked clay pellets are suitable for hydroponic systems in which all nutrients are carefully controlled in water solution. The clay pellets are inert, pH-neutral, and do not contain any nutrient value.\nThe clay is formed into round pellets and fired in rotary kilns at . This causes the clay to expand, like popcorn, and become porous. It is light in weight, and does not compact over time. The shape of an individual pellet can be irregular or uniform depending on brand and manufacturing process. The manufacturers consider expanded clay to be an ecologically sustainable and re-usable growing medium because of its ability to be cleaned and sterilized, typically by washing in solutions of white vinegar, chlorine bleach, or hydrogen peroxide (H2O2), and rinsing completely.\nAnother view is that clay pebbles are best not re-used even when they are cleaned, due to root growth that may enter the medium. Breaking open a clay pebble after use can reveal this growth.\nGrowstones.\nGrowstones, made from glass waste, have both more air and water retention space than perlite and peat. This aggregate holds more water than parboiled rice hulls. Growstones by volume consist of 0.5 to 5% calcium carbonate \u2013 for a standard 5.1\u00a0kg bag of Growstones that corresponds to 25.8 to 258 grams of calcium carbonate. The remainder is soda-lime glass.\nCoconut coir.\nCoconut coir, also known as coir peat, is a natural byproduct derived from coconut processing. The outer husk of a coconut consists of fibers which are commonly used to make a myriad of items ranging from floor mats to brushes. After the long fibers are used for those applications, the dust and short fibers are merged to create coir. Coconuts absorb high levels of nutrients throughout their life cycle, so the coir must undergo a maturation process before it becomes a viable growth medium. This process removes salt, tannins and phenolic compounds through substantial water washing. Contaminated water is a byproduct of this process, as three hundred to six hundred liters of water per one cubic meter of coir are needed. Additionally, this maturation can take up to six months and one study concluded the working conditions during the maturation process are dangerous and would be illegal in North America and Europe. Despite requiring attention, posing health risks and environmental impacts, coconut coir has impressive material properties. When exposed to water, the brown, dry, chunky and fibrous material expands nearly three or four times its original size. This characteristic combined with coconut coir's water retention capacity and resistance to pests and diseases make it an effective growth medium. Used as an alternative to rock wool, coconut coir offers optimized growing conditions.\nRice husks.\nParboiled rice husks (PBH) are an agricultural byproduct that would otherwise have little use. They decay over time, and allow drainage, and even retain less water than growstones. A study showed that rice husks did not affect the effects of plant growth regulators.\nPerlite.\nPerlite is a volcanic rock that has been superheated into very lightweight expanded glass pebbles. It is used loose or in plastic sleeves immersed in the water. It is also used in potting soil mixes to decrease soil density. It does contain a high amount of fluorine which could be harmful to some plants. Perlite has similar properties and uses to vermiculite but, in general, holds more air and less water and is buoyant.\nVermiculite.\nLike perlite, vermiculite is a mineral that has been superheated until it has expanded into light pebbles. Vermiculite holds more water than perlite and has a natural \"wicking\" property that can draw water and nutrients in a passive hydroponic system. If too much water and not enough air surrounds the plants roots, it is possible to gradually lower the medium's water-retention capability by mixing in increasing quantities of perlite.\nPumice.\nLike perlite, pumice is a lightweight, mined volcanic rock that finds application in hydroponics.\nSand.\nSand is cheap and easily available. However, it is heavy, does not hold water very well, and it must be sterilized between uses.\nGravel.\nThe same type that is used in aquariums, though any small gravel can be used, provided it is washed first. Indeed, plants growing in a typical traditional gravel filter bed, with water circulated using electric powerhead pumps, are in effect being grown using gravel hydroponics, also termed \"nutriculture\". Gravel is inexpensive, easy to keep clean, drains well and will not become waterlogged. However, it is also heavy, and, if the system does not provide continuous water, the plant roots may dry out.\nWood fiber.\nWood fibre, produced from steam friction of wood, is an efficient organic substrate for hydroponics. It has the advantage that it keeps its structure for a very long time. Wood wool (i.e. wood slivers) have been used since the earliest days of the hydroponics research. However, more recent research suggests that wood fibre may have detrimental effects on \"plant growth regulators\".\nSheep wool.\nWool from shearing sheep is a little-used yet promising renewable growing medium. In a study comparing wool with peat slabs, coconut fibre slabs, perlite and rockwool slabs to grow cucumber plants, sheep wool had a greater air capacity of 70%, which decreased with use to a comparable 43%, and water capacity that increased from 23% to 44% with use. Using sheep wool resulted in the greatest yield out of the tested substrates, while application of a biostimulator consisting of humic acid, lactic acid and Bacillus subtilis improved yields in all substrates.\nBrick shards.\nBrick shards have similar properties to gravel. They have the added disadvantages of possibly altering the pH and requiring extra cleaning before reuse.\nPolystyrene packing peanuts.\nPolystyrene packing peanuts are inexpensive, readily available, and have excellent drainage. However, they can be too lightweight for some uses. They are used mainly in closed-tube systems. Note that non-biodegradable polystyrene peanuts must be used; biodegradable packing peanuts will decompose into a sludge. Plants may absorb styrene and pass it to their consumers; this is a possible health risk.\nNutrient solutions.\nInorganic hydroponic solutions.\nThe formulation of hydroponic solutions is an application of plant nutrition, with nutrient deficiency symptoms mirroring those found in traditional soil based agriculture. However, the underlying chemistry of hydroponic solutions can differ from soil chemistry in many significant ways. Important differences include:\nAs in conventional agriculture, nutrients should be adjusted to satisfy Liebig's law of the minimum for each specific plant variety. Nevertheless, generally acceptable concentrations for nutrient solutions exist, with minimum and maximum concentration ranges for most plants being somewhat similar. Most nutrient solutions are mixed to have concentrations between 1,000 and 2,500 ppm. Acceptable concentrations for the individual nutrient ions, which comprise that total ppm figure, are summarized in the following table. For essential nutrients, concentrations below these ranges often lead to nutrient deficiencies while exceeding these ranges can lead to nutrient toxicity. Optimum nutrition concentrations for plant varieties are found empirically by experience or by plant tissue tests.\nOrganic hydroponic solutions.\nOrganic fertilizers can be used to supplement or entirely replace the inorganic compounds used in conventional hydroponic solutions. However, using organic fertilizers introduces a number of challenges that are not easily resolved. Examples include:\nNevertheless, if precautions are taken, organic fertilizers can be used successfully in hydroponics.\nOrganically sourced macronutrients.\nExamples of suitable materials, with their average nutritional contents tabulated in terms of percent dried mass, are listed in the following table.\nOrganically sourced micronutrients.\nMicronutrients can be sourced from organic fertilizers as well. For example, composted pine bark is high in manganese and is sometimes used to fulfill that mineral requirement in conventional hydroponic solutions. To satisfy requirements for National Organic Programs, pulverized, unrefined minerals (e.g. Gypsum, Calcite, and glauconite) can also be added to satisfy a plant's nutritional needs.\nAdditives.\nCompounds can be added in both organic and conventional hydroponic systems to improve nutrition acquisition and uptake by the plant\".\" Chelating agents and humic acid have been shown to increase nutrient uptake. Additionally, plant growth promoting rhizobacteria (PGPR), which are regularly utilized in field and greenhouse agriculture, have been shown to benefit hydroponic plant growth development and nutrient acquisition. Some PGPR are known to increase nitrogen fixation. While nitrogen is generally abundant in hydroponic systems with properly maintained fertilizer regimens, \"Azospirillum\" and \"Azotobacter\" genera can help maintain mobilized forms of nitrogen in systems with higher microbial growth in the rhizosphere. Traditional fertilizer methods often lead to high accumulated concentrations of nitrate within plant tissue at harvest. \"Rhodopseudo-monas palustris\" has been shown to increase nitrogen use efficiency, increase yield, and decrease nitrate concentration by 88% at harvest compared to traditional hydroponic fertilizer methods in leafy greens. Many \"Bacillus\" spp., \"Pseudomonas\" spp. and \"Streptomyces\" spp. convert forms of phosphorus in the soil that are unavailable to the plant into soluble anions by decreasing soil pH, releasing phosphorus bound in chelated form that is available in a wider pH range, and mineralizing organic phosphorus.\nSome studies have found that \"Bacillus\" inoculants allow hydroponic leaf lettuce to overcome high salt stress that would otherwise reduce growth. This can be especially beneficial in regions with high electrical conductivity or salt content in their water source. This could potentially avoid costly reverse osmosis filtration systems while maintaining high crop yield.\nTools.\nCommon equipment.\nManaging nutrient concentrations, oxygen saturation, and pH values within acceptable ranges is essential for successful hydroponic horticulture. Common tools used to manage hydroponic solutions include:\nEquipment.\nChemical equipment can also be used to perform accurate chemical analyses of nutrient solutions. Examples include:\nUsing chemical equipment for hydroponic solutions can be beneficial to growers of any background because nutrient solutions are often reusable. Because nutrient solutions are virtually never completely depleted, and should never be due to the unacceptably low osmotic pressure that would result, re-fortification of old solutions with new nutrients can save growers money and can control point source pollution, a common source for the eutrophication of nearby lakes and streams.\nSoftware.\nAlthough pre-mixed concentrated nutrient solutions are generally purchased from commercial nutrient manufacturers by hydroponic hobbyists and small commercial growers, several tools exist to help anyone prepare their own solutions without extensive knowledge about chemistry. The free and open source tools HydroBuddy and HydroCal have been created by professional chemists to help any hydroponics grower prepare their own nutrient solutions. The first program is available for Windows, Mac and Linux while the second one can be used through a simple JavaScript interface. Both programs allow for basic nutrient solution preparation although HydroBuddy provides added functionality to use and save custom substances, save formulations and predict electrical conductivity values.\nMixing solutions.\nOften mixing hydroponic solutions using individual salts is impractical for hobbyists or small-scale commercial growers because commercial products are available at reasonable prices. However, even when buying commercial products, multi-component fertilizers are popular. Often these products are bought as three part formulas which emphasize certain nutritional roles. For example, solutions for vegetative growth (i.e. high in nitrogen), flowering (i.e. high in potassium and phosphorus), and micronutrient solutions (i.e. with trace minerals) are popular. The timing and application of these multi-part fertilizers should coincide with a plant's growth stage. For example, at the end of an annual plant's life cycle, a plant should be restricted from high nitrogen fertilizers. In most plants, nitrogen restriction inhibits vegetative growth and helps induce flowering.\nAdditional improvements.\nGrowrooms.\nWith pest problems reduced and nutrients constantly fed to the roots, productivity in hydroponics is high; however, growers can further increase yield by manipulating a plant's environment by constructing sophisticated growrooms.\nCO2 enrichment.\nTo increase yield further, some sealed greenhouses inject CO2 into their environment to help improve growth and plant fertility.\nCrops grown.\nHydroponic crops are chosen based on the market demand for a crop, environmental suitability, growth cycles, root structures, and plant growth characteristics that make them good candidates for soilless cultivation. Leafy greens like lettuce, spinach, and kale are grown because they have a short growing cycle of as little as 30-50 days, have high market value, and require minimal space.\nVegetable crops like tomatoes, peppers, and cucumbers are also widely grown in hydroponics. These crops are successful due to a hydroponic system's ability to precisely control each crop's temperature, humidity, and light requirements. This control optimizes crop yield and maintains the quality and nutrition of the plants grown.\nCulinary herbs such as basil, mint, cilantro, rosemary, and parsley are popular to grow using hydroponics due to their demand from consumers and profitability. Most herbs also grow quickly, with a growth cycle of 25-40 days per harvest for herbs like basil, mint, and cilantro. Longer growing herbs such as rosemary can be harvested multiple times from the same plant. This eliminates the need for new seedlings and extends the production window for each plant.\nUrban hydroponics.\nUrban hydroponics refers to the use of soilless cultivation systems in city environments, including apartments, rooftops, and other indoor spaces. These systems have been explored in response to challenges related to urban food access. According to the United States Department of Agriculture, more than 19 million people in the United States live in low-income neighborhoods with limited access to supermarkets or large grocery stores, areas often referred to as food deserts. In these contexts, hydroponic systems can be used as a method to grow vegetables such as lettuce, spinach, and herbs indoors.\nUrban hydroponic systems are generally designed to operate within small spaces. Commercially available units include vertical towers, countertop kits, and window-mounted structures, many of which rely on artificial lighting and water circulation to deliver nutrients directly to plant roots. These configurations are intended to support plant growth in environments with limited natural sunlight and no access to soil.\nSome hydroponic systems are marketed toward individual consumers and households. While larger systems may require significant investment, smaller-scale models are available for individual use. Reports suggest that such systems can produce a range of crops year-round, though yield and cost-efficiency vary by setup and user experience.\nHydroponic systems have also been studied for their resource efficiency. In urban settings, locally grown produce using hydroponics may reduce the need for long-distance transportation of produce, though the overall environmental impact depends on multiple factors, including energy use for lighting and climate control.\nAs global urbanization continues, hydroponics has been included in urban food systems and localized agriculture discussions. The United Nations projects that by 2050, nearly 70 percent of the global population will live in urban areas, a demographic trend contributing to interest in alternative food production methods such as hydroponics.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14134", "revid": "35056011", "url": "https://en.wikipedia.org/wiki?curid=14134", "title": "Humanist (disambiguation)", "text": "Humanist may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "14135", "revid": "24465790", "url": "https://en.wikipedia.org/wiki?curid=14135", "title": "Henry Purcell", "text": "English composer (1659\u20131695)\nHenry Purcell (, rare: ; c.\u200910 September 1659 \u2013 21 November 1695) was an English composer and organist of the middle Baroque era. He composed more than 100 songs, a tragic opera \"Dido and Aeneas\", and wrote incidental music to a version of Shakespeare's \"A Midsummer Night's Dream\" called \"The Fairy Queen\". \nPurcell's musical style was uniquely English, although it incorporated Italian and French elements. Purcell is generally considered to be one of the greatest English composers.\nLife and work.\nEarly life.\nPurcell was born in St Ann's Lane, Old Pye Street, Westminster, in 1659. Henry Purcell Senior, whose older brother Thomas Purcell was a musician, was a gentleman of the Chapel Royal and sang at the coronation of King Charles II of England. Henry the elder had three sons: Edward, Henry and Daniel. Daniel Purcell, the youngest of the brothers, was also a prolific composer who wrote the music for much of the final act of \"The Indian Queen\" after his brother Henry's death. The family lived just a few hundred yards west of Westminster Abbey from 1659 onwards.\nAfter his father's death in 1664, Purcell was placed under the guardianship of his uncle Thomas, who showed him great affection and kindness. Thomas arranged for Henry to be admitted as a chorister. Henry studied first under Captain Henry Cooke, Master of the Children, and afterwards under Cooke's successor Pelham Humfrey, who was a pupil of Lully. The composer Matthew Locke was a family friend and, particularly with his semi-operas, probably also had a musical influence on the young Purcell. Henry was a chorister in the Chapel Royal until his voice broke in 1673 when he became assistant to the organ-builder John Hingston, who held the post of keeper of wind instruments to the King.\nEarly career.\nPurcell is said to have been composing at nine years old, but the earliest work that can be certainly identified as his is an ode for the King's birthday, written in 1670, when he was eleven. The dates for his compositions are often uncertain, despite considerable research. It is assumed that the three-part song \"Sweet tyranness, I now resign\" was written by him as a child. After Humfrey's death, Purcell continued his studies under John Blow. He attended Westminster School and in 1676 was appointed copyist at Westminster Abbey. Henry Purcell's earliest anthem, \"Lord, who can tell\", was composed in 1678. It is a psalm that is prescribed for Christmas Day and also to be read at morning prayer on the fourth day of the month.\nIn 1679, he wrote songs for John Playford's \"Choice Ayres, Songs and Dialogues\" and an anthem, the name of which is unknown, for the Chapel Royal. From an extant letter written by Thomas Purcell we learn that this anthem was composed for the exceptionally fine voice of the Rev. John Gostling, then at Canterbury, but afterwards a gentleman of His Majesty's Chapel. Purcell wrote several anthems at different times for Gostling's extraordinary basso profondo voice, which is known to have had a range of at least two full octaves, from D below the bass staff to the D above it. The dates of very few of these sacred compositions are known; perhaps the most notable example is the anthem \"They that go down to the sea in ships.\" In gratitude for the providential escape of King Charles II from shipwreck, Gostling, who had been of the royal party, put together some verses from the Psalms in the form of an anthem and requested Purcell to set them to music. The challenging work opens with a passage which traverses the full extent of Gostling's range, beginning on the upper D and descending two octaves to the lower.\nDido and Aeneas.\nBetween 1680 and 1688 Purcell wrote music for seven plays. The composition of his chamber opera \"Dido and Aeneas\", which forms a very important landmark in the history of English dramatic music, has been attributed to this period, and its earliest production may well have predated the documented one of 1689. It was written to a libretto furnished by Nahum Tate, and performed in 1689 in cooperation with Josias Priest, a dancing master and the choreographer for the Dorset Garden Theatre. Priest's wife kept a boarding school for young gentlewomen, first in Leicester Fields and afterwards at Chelsea, where the opera was performed. It is occasionally considered the first genuine English opera, though that title is usually given to Blow's \"Venus and Adonis\": as in Blow's work, the action does not progress in spoken dialogue but in Italian-style recitative. Each work runs to less than one hour. At the time, \"Dido and Aeneas\" never found its way to the theatre, though it appears to have been very popular in private circles. It is believed to have been extensively copied, but only one song was printed by Purcell's widow in \"Orpheus Britannicus\", and the complete work remained in manuscript until 1840 when it was printed by the Musical Antiquarian Society under the editorship of Sir George Macfarren. The composition of \"Dido and Aeneas\" gave Purcell his first chance to write a sustained musical setting of a dramatic text. It was his only opportunity to compose a work in which the music carried the entire drama. The story of \"Dido and Aeneas\" derives from the original source in Virgil's epic the \"Aeneid\". During the early part of 1679, he produced two important works for the stage, the music for Nathaniel Lee's \"Theodosius\", and Thomas d'Urfey's \"Virtuous Wife\". \nIn 1679, Blow, who had been appointed organist of Westminster Abbey 10 years before, resigned his office in favour of Purcell. Purcell now devoted himself almost entirely to the composition of sacred music, and for six years severed his connection with the theatre. He had probably written his two important stage works before taking up his new office.\nWestminster Abbey and Chapel Royal.\nSoon after Purcell's marriage in 1682, on the death of Edward Lowe, he was appointed organist of the Chapel Royal, an office which he was able to hold simultaneously with his position at Westminster Abbey. His eldest son was born in this same year, but he was short-lived. His first printed composition, \"Twelve Sonatas\", was published in 1683. For some years after this, he was busy in the production of sacred music, odes addressed to the king and royal family, and other similar works. In 1685, he wrote two of his finest anthems, \"I was glad\" and \"My heart is inditing,\" for the coronation of King James II. In 1690 he composed a setting of the birthday ode for Queen Mary, \"Arise, my muse\" and four years later wrote one of his most elaborate, important and magnificent works \u2013 a setting for another birthday ode for the Queen, written by Nahum Tate, entitled \"Come Ye Sons of Art\".\nTheatre music.\nIn 1687, he resumed his connection with the theatre by furnishing the music for John Dryden's tragedy \"Tyrannick Love\". In this year, Purcell also composed a march and passepied called \"Quick-step\", which became so popular that Lord Wharton adapted the latter to the verses of \"Lillibullero\". In or before January 1688, Purcell composed his anthem \"Blessed are they that fear the Lord\" by the express command of the King. A few months later, he wrote the music for D'Urfey's play, \"The Fool's Preferment\". In 1690, he composed the music for Betterton's adaptation of Fletcher and Massinger's \"Prophetess\" (afterwards called \"Dioclesian\") and Dryden's \"Amphitryon\". In 1691, he wrote the music for what is sometimes considered his dramatic masterpiece, \"King Arthur, or The British Worthy\". In 1692, he composed \"The Fairy-Queen\" (an adaptation of Shakespeare's \"A Midsummer Night's Dream\"), the score of which (his longest for theatre) was rediscovered in 1901 and published by the Purcell Society. \"The Indian Queen\" followed in 1695, in which year he also wrote songs for Dryden and Davenant's version of Shakespeare's \"The Tempest\" (recently, this has been disputed by music scholars), probably including \"Full fathom five\" and \"Come unto these yellow sands\". \"The Indian Queen\" was adapted from a tragedy by Dryden and Sir Robert Howard. In these semi-operas (another term for which at the time was \"dramatic opera\"), the main characters of the plays do not sing but speak their lines: the action moves in dialogue rather than recitative. The related songs are sung \"for\" them by singers, who have minor dramatic roles.\nLast works.\nPurcell's \"Te Deum\" and \"Jubilate Deo\" were written for Saint Cecilia's Day, 1694, the first English \"Te Deum\" ever composed with orchestral accompaniment. This work was annually performed at St Paul's Cathedral until 1712, after which it was performed alternately with Handel's \"Utrecht Te Deum and Jubilate\" until 1743, when both works were replaced by Handel's \"Dettingen Te Deum\".\nHe composed an anthem and two elegies for Queen Mary II's funeral, his \"Funeral Sentences and Music for the Funeral of Queen Mary\". Besides the operas and semi-operas already mentioned, Purcell wrote the music and songs for Thomas d'Urfey's \"The Comical History of Don Quixote\", \"Bonduca\", \"The Indian Queen\" and others, a vast quantity of sacred music, and numerous odes, cantatas, and other miscellaneous pieces. The quantity of his instrumental chamber music is minimal after his early career, and his keyboard music consists of an even more minimal number of harpsichord suites and organ pieces. In 1693, Purcell composed music for two comedies: \"The Old Bachelor\", and \"The Double Dealer\". Purcell also composed for five other plays within the same year. In July 1695, Purcell composed an ode for the Duke of Gloucester for his sixth birthday. The ode is titled \"Who can from joy refrain?\" Purcell's four-part sonatas were issued in 1697. In the final six years of his life, Purcell wrote music for forty-two plays.\nDeath.\nPurcell died on 21 November 1695 at his home in Marsham Street, at the height of his career. He is believed to have been 35 or 36 years old at the time. The cause of his death is unclear: one theory is that he caught a chill after returning home late from the theatre one night to find that his wife had locked him out. Another is that he succumbed to tuberculosis. The beginning of Purcell's will reads:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the name of God Amen. I, Henry Purcell, of the City of Westminster, gentleman, being dangerously ill as to the constitution of my body, but in good and perfect mind and memory (thanks be to God) do by these presents publish and declare this to be my last Will and Testament. And I do hereby give and bequeath unto my loving wife, Frances Purcell, all my estate both real and personal of what nature and kind soever...\nPurcell is buried adjacent to the organ in Westminster Abbey. The music that he had earlier composed for Queen Mary's funeral was performed during his funeral. Purcell was universally mourned as \"a very great master of music\".\u00a0 Following his death, the officials at Westminster honoured him by unanimously voting that he be buried with no expense spared in the north aisle of the Abbey. His epitaph reads: \"Here lyes Henry Purcell Esq., who left this life and is gone to that Blessed Place where only His harmony can be exceeded.\"\nPurcell and his wife Frances had six children, four of whom died in infancy. His wife, as well as his son Edward (1689\u20131740) and daughter Frances, survived him. His wife Frances died in 1706, having published a number of her husband's works, including the now-famous collection called \"Orpheus Britannicus\", in two volumes, printed in 1698 and 1702, respectively. Edward was appointed organist of St Clement's, Eastcheap, London, in 1711 and was succeeded by his son Edward Henry Purcell (died 1765). Both men were buried in St Clement's near the organ gallery.\nLegacy.\nNotable compositions.\nPurcell worked in many genres, both in works closely linked to the court, such as symphony song, to the Chapel Royal, such as the symphony anthem, and the theatre.\nAmong Purcell's most notable works are his opera \"Dido and Aeneas\" (1688), his semi-operas \"Dioclesian\" (1690), \"King Arthur\" (1691), \"The Fairy-Queen\" (1692) and \"Timon of Athens\" (1695), as well as the compositions \"Hail! Bright Cecilia\" (1692), \"Come Ye Sons of Art\" (1694) and \"Funeral Sentences and Music for the Funeral of Queen Mary\" (1695).\nIn 2025, a lost work, the song \"As Soon as Day Began To Peep\" from Thomas D'Urfey's 1691 play \"Love for Money\", was rediscovered in the Worcestershire County Archives; it was unknown to modern scholars. Announced at the same time was the rediscovery in Norfolk County Archives of manuscripts of three keyboard works in Purcell's own hand, including early versions of his G minor suite, different from the published work.\nInfluence and reputation.\nAfter his death, Purcell was honoured by many of his contemporaries, including his old friend John Blow, who wrote \"An Ode, on the Death of Mr. Henry Purcell (Mark how the lark and linnet sing)\" with text by his old collaborator, John Dryden. William Croft's 1724 setting for the Burial Service was written in the style of \"the great Master\". Croft preserved Purcell's setting of \"Thou knowest Lord\" (Z 58) in his service, for reasons \"obvious to any artist\"; it has been sung at every British state funeral ever since. More recently, the English poet Gerard Manley Hopkins wrote a famous sonnet entitled simply \"Henry Purcell\", with a headnote reading: \"The poet wishes well to the divine genius of Purcell and praises him that, whereas other musicians have given utterance to the moods of man's mind, he has, beyond that, uttered in notes the very make and species of man as created both in him and in all men generally.\"\nPurcell also had a strong influence on the composers of the English musical renaissance of the early 20th century, most notably Benjamin Britten, who arranged many of Purcell's vocal works for voice(s) and piano in \"Britten's Purcell Realizations\", including from \"Dido and Aeneas\", and whose \"The Young Person's Guide to the Orchestra\" is based on a theme from Purcell's \"Abdelazar\". Stylistically, the aria \"I know a bank\" from Britten's opera \"A Midsummer Night's Dream\" is clearly inspired by Purcell's aria \"Sweeter than Roses\", which Purcell originally wrote as part of incidental music to Richard Norton's \"Pausanias, the Betrayer of His Country\".\nIn a 1940 interview Ignaz Friedman stated that he considered Purcell as great as Bach and Beethoven. In Victoria Street, Westminster, England, there is a bronze monument to Purcell, sculpted by Glynn Williams and unveiled in 1995 to mark the 300th anniversary of his death. In 2009, Purcell was selected by the Royal Mail for their \"Eminent Britons\" commemorative postage stamp issue.\nA Purcell Club was founded in London in 1836 for promoting the performance of his music but was dissolved in 1863. In 1876 a Purcell Society was founded, which published new editions of his works. A modern-day Purcell Club has been created, and provides guided tours and concerts in support of Westminster Abbey.\nToday there is a Henry Purcell Society of Boston, which performs his music in live concert. There is a Purcell Society in London, which collects and studies Purcell manuscripts and musical scores, concentrating on producing revised versions of the scores of all his music. Purcell's works have been catalogued by Franklin Zimmerman, who gave them a number preceded by Z.\nSo strong was his reputation that a popular wedding processional was incorrectly attributed to Purcell for many years. The so-called \"Purcell's Trumpet Voluntary\" was in fact written around 1700 by a British composer named Jeremiah Clarke as the \"Prince of Denmark's March\".\nIn popular culture.\nMusic for the Funeral of Queen Mary was reworked by Wendy Carlos for the title music of the 1971 film by Stanley Kubrick, \"A Clockwork Orange\". The 1973 \"Rolling Stone\" review of Jethro Tull's \"A Passion Play\" compared the musical style of the album with that of Purcell. In 2009 Pete Townshend of The Who, an English rock band that established itself in the 1960s, identified Purcell's harmonies, particularly the use of suspension and resolution (Townshend has mentioned Chaconne from The Gordian Knot Untied) that he had learned from producer Kit Lambert, as an influence on the band's music (in songs such as \"Won't Get Fooled Again\" (1971), \"I Can See for Miles\" (1967) and the very Purcellian intro to \"Pinball Wizard\"). Purcell's music was widely featured as background music in the Academy Award winning 1979 film \"Kramer vs. Kramer\", with a soundtrack on CBS Masterworks Records. The 1995 film \"England, My England\" tells the story of an actor who is himself writing a play about Purcell's life and music, and features many of his compositions.\nIn the 21st century, the soundtrack of the 2005 film version of \"Pride and Prejudice\" features a dance titled \"A Postcard to Henry Purcell\". This is a version by composer Dario Marianelli of Purcell's \"Abdelazar\" theme. In the German-language 2004 movie, \"Downfall\", the music of Dido's Lament is used repeatedly as Nazi Germany collapses. The 2012 film \"Moonrise Kingdom\" contains Benjamin Britten's version of the Rondeau in Purcell's \"Abdelazar\" created for his 1946 \"The Young Person's Guide to the Orchestra\". In 2013, the Pet Shop Boys released their single \"Love Is a Bourgeois Construct\" incorporating one of the same ground basses from \"King Arthur\" used by Michael Nyman in his \"The Draughtsman's Contract\" score. Olivia Chaney performs her adaptation of \"There's Not a Swain\" on her CD \"The Longest River\". The song \"Music for a while\" from Purcell's incidental music to \"Oedipus,\" Z. 583 was included in the soundtrack of the 2018 film \"The Favourite,\" along with the second movement of his Trumpet Sonata in D major, Z. 850, performed by the English Baroque Soloists, conducted by Sir John Eliot Gardiner.\n\"What Power Art Thou\" (from King Arthur, or The British Worthy (Z. 628), a semi-opera in five acts with music by Purcell and a libretto by John Dryden) is featured in \"The Crown\".\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14136", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=14136", "title": "Hydrophobe", "text": "Molecule or surface that has no attraction to water\nIn chemistry, hydrophobicity is the chemical property of a molecule (called a hydrophobe) that is seemingly repelled from a mass of water. In contrast, \"hydrophiles\" are attracted to water.\nHydrophobic molecules tend to be nonpolar and, thus, prefer other neutral molecules and nonpolar solvents. Because water molecules are polar, hydrophobes do not dissolve well among them. Hydrophobic molecules in water often cluster together, forming micelles. Water on hydrophobic surfaces will exhibit a high contact angle.\nExamples of hydrophobic molecules include the alkanes, oils, fats, and greasy substances in general. Hydrophobic materials are used for oil removal from water, the management of oil spills, and chemical separation processes to remove non-polar substances from polar compounds. \nThe term \"hydrophobic\"\u2014which comes from the Ancient Greek (), \"having a fear of water\", constructed from grc \" \u1f55\u03b4\u03c9\u03c1 (h\u00fad\u014dr)\"\u00a0'water' and grc \" \u03c6\u03cc\u03b2\u03bf\u03c2 (ph\u00f3bos)\"\u00a0'fear'\u2014is often used interchangeably with \"lipophilic\", \"fat-loving\". However, the two terms are not synonymous. While hydrophobic substances are usually lipophilic, there are exceptions, such as the silicones and fluorocarbons.\nChemistry.\nFor small solutes, the hydrophobic interaction is mostly an entropic effect originating from the disruption of the highly dynamic hydrogen bonds between molecules of liquid water by the nonpolar solute, causing the water to compensate by forming a clathrate-like cage structure around the non-polar molecules. This structure is more highly ordered than free water molecules due to the water molecules arranging themselves to interact as much as possible with themselves, and thus results in a lower entropic state at the interface. This causes non-polar molecules to clump together to reduce the surface area exposed to water and thereby increase the entropy of the system. Thus, the two immiscible phases (hydrophilic vs. hydrophobic) will change so that their corresponding interfacial area will be minimal. This effect can be visualized in the phenomenon called phase separation.\nFor larger nonpolar solutes that cannot be adequately \"clathrated\" by the hydrogen-bond network of water, the disruption of these bonds becomes inevitable, leading to a high enthalpic cost. Under ambient conditions, this transition from an entropy-dominated regime to one governed by enthalpy occurs at around ~1 nm in size, reflecting a shift in hydration free energy behavior from scaling with the solute volume to depending on the exposed surface area.\nIn this context, a quantitative molecular definition of hydrophobicity has been proposed, based on the energetic cost for a system to induce hydrogen-bond defects in its hydration shell. According to this approach, a system is considered hydrophobic if it cannot compensate for the missing hydrogen bonds with an energy at least as favorable as the cost of generating such a defect in pure water, a value known as the Defect Interaction Threshold (DIT), estimated at approximately \u22126 kJ/mol (around 30% of the typical energy of a hydrogen bond). This criterion coincides with the classical 90\u00b0 contact angle threshold, thus providing a molecular justification for the transition to hydrophobic behavior.\nAdditionally, the DIT helps determine the regimes of filling, partial filling, and drying in nanoconfined water, depending on how many of the water molecule's interaction sites (among its four tetrahedral sites) exceed this threshold. This analysis for quantifying hydrophobicity or wetting can be performed using a structural indicator, the V4S index, which reveals the existence of two inherently preferential interaction states for water.\nSuperhydrophobicity.\nSuperhydrophobic surfaces, such as the leaves of the lotus plant, are those that are extremely difficult to wet. The contact angles of a water droplet exceeds 150\u00b0. This is referred to as the lotus effect, and is primarily a physical property related to interfacial tension, rather than a chemical property.\nTheory.\nIn 1805, Thomas Young defined the contact angle \"\u03b8\" by analyzing the forces acting on a fluid droplet resting on a solid surface surrounded by a gas.\nformula_1\nwhere\nformula_2 = Interfacial tension between the solid and gas\nformula_3 = Interfacial tension between the solid and liquid\nformula_4 = Interfacial tension between the liquid and gas\n\"\u03b8\" can be measured using a contact angle goniometer.\nWenzel determined that when the liquid is in intimate contact with a microstructured surface, \"\u03b8\" will change to \"\u03b8\"W*\nformula_5\nwhere \"r\" is the ratio of the actual area to the projected area. Wenzel's equation shows that microstructuring a surface amplifies the natural tendency of the surface. A hydrophobic surface (one that has an original contact angle greater than 90\u00b0) becomes more hydrophobic when microstructured \u2013 its new contact angle becomes greater than the original. However, a hydrophilic surface (one that has an original contact angle less than\u00a090\u00b0) becomes more hydrophilic when microstructured \u2013 its new contact angle becomes less than the original.\nCassie and Baxter found that if the liquid is suspended on the tops of microstructures, \"\u03b8\" will change to \"\u03b8\"CB*:\nformula_6\nwhere \"\u03c6\" is the area fraction of the solid that touches the liquid. Liquid in the Cassie\u2013Baxter state is more mobile than in the Wenzel state.\nWe can predict whether the Wenzel or Cassie\u2013Baxter state should exist by calculating the new contact angle with both equations. By a minimization of free energy argument, the relation that predicted the smaller new contact angle is the state most likely to exist. Stated in mathematical terms, for the Cassie\u2013Baxter state to exist, the following inequality must be true.\nformula_7\nA recent alternative criterion for the Cassie\u2013Baxter state asserts that the Cassie\u2013Baxter state exists when the following 2 criteria are met:1) Contact line forces overcome body forces of unsupported droplet weight and 2) The microstructures are tall enough to prevent the liquid that bridges microstructures from touching the base of the microstructures.\nA new criterion for the switch between Wenzel and Cassie-Baxter states has been developed recently based on surface roughness and surface energy. The criterion focuses on the air-trapping capability under liquid droplets on rough surfaces, which could tell whether Wenzel's model or Cassie-Baxter's model should be used for certain combination of surface roughness and energy.\nContact angle is a measure of static hydrophobicity, and contact angle hysteresis and slide angle are dynamic measures. Contact angle hysteresis is a phenomenon that characterizes surface heterogeneity. When a pipette injects a liquid onto a solid, the liquid will form some contact angle. As the pipette injects more liquid, the droplet will increase in volume, the contact angle will increase, but its three-phase boundary will remain stationary until it suddenly advances outward. The contact angle the droplet had immediately before advancing outward is termed the advancing contact angle. The receding contact angle is now measured by pumping the liquid back out of the droplet. The droplet will decrease in volume, the contact angle will decrease, but its three-phase boundary will remain stationary until it suddenly recedes inward. The contact angle the droplet had immediately before receding inward is termed the receding contact angle. The difference between advancing and receding contact angles is termed contact angle hysteresis and can be used to characterize surface heterogeneity, roughness, and mobility. Surfaces that are not homogeneous will have domains that impede motion of the contact line. The slide angle is another dynamic measure of hydrophobicity and is measured by depositing a droplet on a surface and tilting the surface until the droplet begins to slide. In general, liquids in the Cassie\u2013Baxter state exhibit lower slide angles and contact angle hysteresis than those in the Wenzel state.\nSoil science.\nSoil tends to become hydrophobic in response to wildfires. Depending on the severity of the fire, this can lead to more precipitation being rendered as surface runoff, traveling over the surface without infiltrating into the soil.\nResearch and development.\nDettre and Johnson discovered in 1964 that the superhydrophobic lotus effect phenomenon was related to rough hydrophobic surfaces, and they developed a theoretical model based on experiments with glass beads coated with paraffin or TFE telomer. The self-cleaning property of superhydrophobic micro-nanostructured surfaces was reported in 1977. Perfluoroalkyl, perfluoropolyether, and RF plasma -formed superhydrophobic materials were developed, used for electrowetting and commercialized for bio-medical applications between 1986 and 1995. Other technology and applications have emerged since the mid-1990s. A durable superhydrophobic hierarchical composition, applied in one or two steps, was disclosed in 2002 comprising nano-sized particles \u2264 100 nanometers overlaying a surface having micrometer-sized features or particles \u2264\u00a0100 micrometers. The larger particles were observed to protect the smaller particles from mechanical abrasion.\nIn recent research, superhydrophobicity has been reported by allowing alkylketene dimer (AKD) to solidify into a nanostructured fractal surface. Many papers have since presented fabrication methods for producing superhydrophobic surfaces including particle deposition, sol-gel techniques, plasma treatments, vapor deposition, and casting techniques. Current opportunity for research impact lies mainly in fundamental research and practical manufacturing. Debates have recently emerged concerning the applicability of the Wenzel and Cassie\u2013Baxter models. In an experiment designed to challenge the surface energy perspective of the Wenzel and Cassie\u2013Baxter model and promote a contact line perspective, water drops were placed on a smooth hydrophobic spot in a rough hydrophobic field, a rough hydrophobic spot in a smooth hydrophobic field, and a hydrophilic spot in a hydrophobic field. Experiments showed that the surface chemistry and geometry at the contact line affected the contact angle and contact angle hysteresis, but the surface area inside the contact line had no effect. An argument that increased jaggedness in the contact line enhances droplet mobility has also been proposed.\nMany hydrophobic materials found in nature rely on Cassie's law and are biphasic on the submicrometer level with one component air. The lotus effect is based on this principle. Inspired by it, many functional superhydrophobic surfaces have been prepared.\nAn example of a bionic or biomimetic superhydrophobic material in nanotechnology is nanopin film.\nOne study presents a vanadium pentoxide surface that switches reversibly between superhydrophobicity and superhydrophilicity under the influence of UV radiation. According to the study, any surface can be modified to this effect by application of a suspension of rose-like V2O5 particles, for instance with an inkjet printer. Once again hydrophobicity is induced by interlaminar air pockets (separated by 2.1 nm distances). The UV effect is also explained. UV light creates electron-hole pairs, with the holes reacting with lattice oxygen, creating surface oxygen vacancies, while the electrons reduce V5+ to V3+. The oxygen vacancies are met by water, and it is this water absorbency by the vanadium surface that makes it hydrophilic. By extended storage in the dark, water is replaced by oxygen and hydrophilicity is once again lost.\nA significant majority of hydrophobic surfaces have their hydrophobic properties imparted by structural or chemical modification of a surface of a bulk material, through either coatings or surface treatments. That is to say, the presence of molecular species (usually organic) or structural features results in high contact angles of water. In recent years, rare earth oxides have been shown to possess intrinsic hydrophobicity. The intrinsic hydrophobicity of rare earth oxides depends on surface orientation and oxygen vacancy levels, and is naturally more robust than coatings or surface treatments, having potential applications in condensers and catalysts that can operate at high temperatures or corrosive environments.\nApplications and potential applications.\nHydrophobic concrete has been produced since the mid-20th century.\nActive recent research on superhydrophobic materials might eventually lead to more industrial applications.\nA simple routine of coating cotton fabric with silica or titania particles by sol-gel technique has been reported, which protects the fabric from UV light and makes it superhydrophobic.\nAn efficient routine has been reported for making polyethylene superhydrophobic and thus self-cleaning. 99% of dirt on such a surface is easily washed away.\nPatterned superhydrophobic surfaces also have promise for lab-on-a-chip microfluidic devices and can drastically improve surface-based bioanalysis.\nIn pharmaceuticals, hydrophobicity of pharmaceutical blends affects important quality attributes of final products, such as drug dissolution and hardness. Methods have been developed to measure the hydrophobicity of pharmaceutical materials.\nThe development of hydrophobic passive daytime radiative cooling (PDRC) surfaces, whose effectiveness at solar reflectance and thermal emittance is predicated on their cleanliness, has improved the \"self-cleaning\" of these surfaces. Scalable and sustainable hydrophobic PDRCs that avoid VOCs have further been developed.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14142", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=14142", "title": "Harley-Davidson", "text": "American motorcycle manufacturer\nHarley-Davidson, Inc. (H-D, or simply Harley) is an American motorcycle manufacturer and lifestyle brand headquartered in Milwaukee, Wisconsin. Founded in 1903, it is one of two major American motorcycle manufacturers to survive the Great Depression along with its historical rival, Indian Motorcycle Company. The company has survived numerous ownership arrangements, subsidiary arrangements, periods of poor economic health and product quality, and intense global competition to become an iconic brand widely known for its loyal following. There are owner clubs and events worldwide, as well as a company-sponsored, brand-focused museum.\nHarley-Davidson is noted for a style of customization that gave rise to the chopper motorcycle style. The company traditionally marketed heavyweight, air-cooled cruiser motorcycles with engine displacements greater than 700\u00a0cc, but it has broadened its offerings to include more contemporary VRSC (2002) and middle-weight Street (2014) platforms.\nHarley-Davidson manufactures its motorcycles at factories in York, Pennsylvania; Menomonee Falls, Wisconsin; Tomahawk, Wisconsin; Manaus, Brazil; and Rayong, Thailand. The company markets its products worldwide, and also licenses and markets merchandise under the Harley-Davidson brand, among them apparel, home d\u00e9cor and ornaments, accessories, toys, scale models of its motorcycles, and video games based on its motorcycle line and the community.\nHistory.\nIn 1901, -year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cubic inches (116\u00a0cc) and four-inch (102\u00a0mm) flywheels designed for use in a regular pedal-bicycle frame. Over the next two years, he and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend Henry Melk. It was finished in 1903 with the help of Arthur's brother Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance, and they wrote off their first motor-bicycle as a valuable learning experiment.\nThe three began work on a new and improved machine with an engine of 24.74 cubic inches (405\u00a0cc) with flywheels weighing . Its advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle designed by Joseph Merkel, later of Flying Merkel fame. The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. They also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.\nThe prototype of the new loop-frame Harley-Davidson was assembled in a shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. Edward Hildebrand rode it and placed fourth in the race.\nIn January 1905, the company placed small advertisements in the \"Automobile and Cycle Trade Journal\" offering bare Harley-Davidson engines to the do-it-yourself trade. By April, they were producing complete motorcycles on a very limited basis. That year, Harley-Davidson dealer Carl H. Lang of Chicago sold three bikes from the five built in the Davidson backyard shed. Years later, the company moved the original shed to the Juneau Avenue factory where it stood for many decades as a tribute.\nIn 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a single-story wooden structure. The company produced about 50 motorcycles that year.\nIn 1907, William S. Harley graduated from the University of Wisconsin\u2013Madison with a degree in mechanical engineering. That year, they expanded the factory with a second floor and later with facings and additions of Milwaukee pale yellow (\"cream\") brick. With the new facilities, production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since. In 1907, William A. Davidson quit his job as tool foreman for the Milwaukee Road railroad and joined the Motor Company.\nProduction in 1905 and 1906 were all single-cylinder models with 26.84-cubic-inch (440\u00a0cc) engines. In February 1907, they displayed a prototype model at the Chicago Automobile Show with a 45-degree V-Twin engine. Very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cubic inches (880\u00a0cc) and produced about . This gave about double the power of the first singles, and top speed was about . Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.\nIn 1911, the company introduced an improved V-Twin model with a displacement of 49.48 cubic inches (811\u00a0cc) and mechanically operated intake valves, as opposed to the \"automatic\" intake valves used on earlier V-Twins that opened by engine vacuum. It was smaller than earlier twins but gave better performance. After 1913, the majority of bikes produced by Harley-Davidson were V-Twin models.\nIn 1912, Harley-Davidson introduced their patented \"Ful-Floteing Seat\", which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight, and more than of travel was available. Harley-Davidson used seats of this type until 1958.\nBy 1913, the yellow brick factory had been demolished and a new five-story structure had been built on the site which took up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and dominated motorcycle racing after 1914. Production that year swelled to 16,284 machines.\nWorld War I.\nIn 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time that it was adopted for military issue, first with the British Model H produced by Triumph Engineering Co Ltd in 1915. The U.S. military purchased over 20,000 motorcycles from Harley-Davidson.\nHarley-Davidson launched a line of bicycles in 1917 in hopes of recruiting more domestic customers for its motorcycles. Models included the traditional diamond frame men's bicycle, a step-through frame 3\u201318 \"Ladies Standard\", and a 5\u201317 \"Boy Scout\" for youth. The effort was discontinued in 1923 because of disappointing sales. The bicycles were built for Harley-Davidson in Dayton, Ohio by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.\n1920s.\nBy 1920 Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced and dealers in 67 countries. In 1921, Otto Walker set a record on a Harley-Davidson as the first motorcycle to win a race at an average speed greater than .\nHarley-Davidson put several improvements in place during the 1920s, such as a new 74\u00a0cubic inch (1,212.6 \u00a0cc) V-Twin introduced in 1921, and the \"teardrop\" gas tank in 1925. They added a front brake in 1928, although only on the J/JD models. In the late summer of 1929, Harley-Davidson introduced its 45-cubic-inch (737\u00a0cc) flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the \"D\" model produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to it as the \"three cylinder Harley\" because the generator was upright and parallel to the front cylinder. In 1929, Vivian Bales drove a record 5,000 miles across the United States and Canada on a D-model.\nGreat Depression.\nThe Great Depression began a few months after the introduction of their model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.\nIn order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.\nAlfred Rich Child opened a production line in Japan in the mid-1930s with the VL. The Japanese license-holder, Sankyo Seiyaku Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.\nAn flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.\nIn 1936, the 61E and 61EL models with the \"Knucklehead\" OHV engines were introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.\nBy 1937, all Harley-Davidson flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the \"Knucklehead\" OHV engine. The revised V and VL models were renamed U and UL, the VH and VLH to be renamed UH and ULH, and the R to be renamed W.\nIn 1941, the 74-cubic-inch \"Knucklehead\" was introduced as the F and the FL. The flathead UH and ULH models were discontinued after 1941, while the 74-cubic-inchU &amp; UL flathead models were produced up to 1948.\nWorld War II.\nOne of only two American motorcycle manufacturers to survive the Great Depression (the other being the Indian Motorcycle Manufacturing Company), Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.\nHarley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its WL line, called the WLA. The A in this case stood for \"Army\". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. Some 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy \"E\" Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.\nShipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1949 to 1952 for use in the Korean War.\nThe U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley-Davidson largely copied the BMW engine and drive train and produced the shaft-driven 750\u00a0cc 1942 and '43 Harley-Davidson XA. This shared no dimensions, no parts or no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100\u00a0\u00b0F (56\u00a0\u00b0C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general-purpose vehicle, and the WLA\u00a0\u2013 already in production\u00a0\u2013 was sufficient for its limited police, escort, and courier roles. Only ~1,000 were made and the XA never went into full production. \nSmall: Hummer, Sportcycle and Aermacchi.\nAs part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as \"Hummers\" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.\nIn 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250\u00a0cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350\u00a0cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.\nAfter the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.\nHarley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Italian Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125\u00a0cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250\u00a0cc two-stroke SS-250 replaced the four-stroke 350\u00a0cc Sprint in 1974.\nHarley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva, owned by the Castiglioni family.\nTarnished reputation.\nIn 1952, following their application to the U.S. Tariff Commission for a 40 percent tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.\nIn 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and cost-cutting produced lower-quality bikes. Simultaneously, the Japanese \"big four\" manufacturers (Honda, Kawasaki, Suzuki, and Yamaha) revolutionized the North American market by introducing what the motoring press would call the Universal Japanese Motorcycle. In comparison, Harley-Davidson's bikes were expensive and inferior in performance, handling, and quality. Sales and quality declined, and the company almost went bankrupt. The \"Harley-Davidson\" name was mocked as \"Hardly Ableson\", \"Hardly Driveable\", and \"Hogly Ferguson\",\nand the nickname \"Hog\" became pejorative.\nIn 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley-Davidson with Confederate-specific paint and details.\nRestructuring and revival.\nIn 1981, AMF sold the company to a group of 13 investors led by Vaughn Beals and Willie G. Davidson for $80\u00a0million. The new management team improved product quality, introduced new technologies, and adopted just-in-time inventory management. These operational and product improvements were matched with a strategy of seeking tariff protection for large-displacement motorcycles in the face of intense competition with Japanese manufacturers. These protections were granted by the Reagan administration in 1983, giving Harley-Davidson time to implement their new strategies.&lt;ref name=\"7/83 US Imposes 45% Tariff on Imported Motorcycles\"&gt;&lt;/ref&gt;\nRevising stagnated product designs was a crucial centerpiece of Harley-Davidson's turnaround strategy. Rather than trying to mimic popular Japanese designs, the new management deliberately exploited the \"retro\" appeal of Harley motorcycles, building machines that deliberately adopted the look and feel of their earlier bikes and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.\nHarley-Davidson bought the \"Sub Shock\" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.\nIn response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.\nThe \"Sturgis\" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.\nFat Boy, Dyna, and Harley-Davidson museum.\nBy 1990, with the introduction of the \"Fat Boy\", Harley-Davidson once again became the sales leader in the heavyweight (over 750\u00a0cc) market. At the time of the Fat Boy model introduction, a false etymology spread that \"Fat Boy\" was a combination of the names of the atomic bombs Fat Man and Little Boy. This has been debunked, as the name \"Fat Boy\" actually comes from the observation that the motorcycle is somewhat wider than other bikes when viewed head-on.\n1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1994. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR2, FXR3 &amp; FXR4).\nHarley-Davidson celebrated their 100th anniversary on September 1, 2003 with a large event and concert featuring performances from Elton John, The Doobie Brothers, Kid Rock, and Tim McGraw.\nConstruction started on the $75\u00a0million, 130,000 square-foot (12,000\u00a0m2) Harley-Davidson Museum in the Menomonee Valley of Milwaukee, Wisconsin on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, caf\u00e9 and meeting space.\nOverseas operations.\nEstablished in 1918, the oldest continuously operating Harley-Davidson dealership outside of the United States is in Australia. Sales in Japan started in 1912 then in 1929, Harley-Davidsons were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.\nIn 1998, the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.\nIn August 2009, Harley-Davidson launched Harley-Davidson India and started selling motorcycles there in 2010. The company established the subsidiary in Gurgaon, near Delhi, in 2011 and created an Indian dealer network. On September 24, 2020, Harley Davidson announced that it would discontinue its sales and manufacturing operations in India due to weak demand and sales. The move involves $75 million in restructuring costs, 70 layoffs and the closure of its Bawal plant in northern India.\nBuell Motorcycle Company.\nHarley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought 49 percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.\nIn an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production. The Buell Blast was the training vehicle for the Harley-Davidson Rider's Edge New Rider Course from 2000 until May 2014, when the company re-branded the training academy and started using the Harley-Davidson Street 500 motorcycles. In those 14 years, more than 350,000 participants in the course learned to ride on the Buell Blast.\nOn October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately, in order to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.\nClaims of stock price manipulation.\nDuring its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42\u00a0million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.\nProblems with Police Touring models.\nStarting around 2000, several police departments started reporting problems with high-speed instability on the Harley-Davidson Touring motorcycles. A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high-speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006. The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.\n2007 strike.\nOn February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania, went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.\nThe day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.\nHarley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.\nMV Agusta Group.\nOn July 11, 2008, Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for US$109 million (\u20ac70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.\nOn October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta. Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni \u2013 a member of the family that had purchased Aermacchi from H-D in 1978 \u2013 for a reported 3 euros, ending the transaction in the first week of August 2010. Castiglioni was MV Agusta's former owner, and had been MV Agusta's chairman since Harley-Davidson bought it in 2008. As part of the deal, Harley-Davidson put $26M into MV Agusta's accounts, essentially giving Castiglioni $26M to take the brand.\n2008 financial crisis.\nThe 2008 financial crisis and 2008\u20132010 automotive industry crisis affected also the motorcycle industry. According to Interbrand, the value of the Harley-Davidson brand fell by 43 percent to $4.34\u00a0billion in 2009. The fall in value is believed to be connected to the 66 percent drop in the company profits in two-quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54\u00a0million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25 percent of its total workforce (around 3,500 employees). The company announced on September 14, 2010, that it would remain in Wisconsin.\nMotorcycle engines.\nThe classic Harley-Davidson engines are V-twin engines, with a 45\u00b0 angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.\nThis 45\u00b0 angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy \"potato-potato\" sound so strongly linked to the Harley-Davidson brand.\nTo simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively \"wasting a spark\". The exhaust note is basically a throaty growling sound with some popping. The 45\u00b0 design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315\u00b0 later, then there is a 405\u00b0 gap until the first cylinder fires again, giving the engine its unique sound.\nHarley-Davidson has used various ignition systems, including the early points and condenser system on Big Twins and Sportsters up to 1978, a magneto ignition system used on some 1958 to 1969 Sportsters, an early electronic with centrifugal mechanical advance weights on all models from mid-1978 until 1979, and a later electronic with a transistorized ignition control module (more familiarly known as a black box or a brain) on all models 1980 to present.\nStarting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.\nIn 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the \"Harley Sound\". This research resulted in the bikes that were introduced in compliance with EU standards for 1998.\nOn February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: \"The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use\". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to register a sound trademark.\nRevolution engine.\nThe Revolution engine is based on the VR-1000 Superbike race program, developed by Harley-Davidson's Powertrain Engineering with Porsche helping to make the engine suitable for street use. It is a liquid cooled, dual overhead cam, internally counterbalanced 60\u00a0degree V-twin engine with a displacement of 69\u00a0cubic\u00a0inch (1,130\u00a0cc), producing at 8,250\u00a0rpm at the crank, with a redline of 9,000\u00a0rpm. It was introduced for the new VRSC (V-Rod) line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model. The Revolution marks Harley's first collaboration with Porsche since the V4 Nova project, which, like the V-Rod, was a radical departure from Harley's traditional lineup until it was cancelled by AMF in 1981 in favor of the Evolution engine.\nA 1,250\u00a0cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250\u00a0cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims at the crank for the 2008 VRSCAW model. The VRXSE \"Destroyer\" dragbike is equipped with a stroker (75\u00a0mm crank) Screamin' Eagle 79\u00a0cubic\u00a0inch (1,300\u00a0cc) Revolution Engine, producing , and more than .\n750\u00a0cc and 500\u00a0cc versions of the Revolution engine are used in Harley-Davidson's Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.\nD\u00fcsseldorf-Test.\nAn extreme endurance test of the Revolution engine was performed in a dynamometer installation at the Harley-Davidson factory in Milwaukee, simulating the German Autobahn (highways without general speed limit) between the Porsche research and development center in Weissach, near Stuttgart to D\u00fcsseldorf. An undisclosed number of samples of engines failed, until an engine successfully passed the 500-hour nonstop run. This was the benchmark for the engineers to approve the start of production for the Revolution engine, which was documented in the Discovery channel special \"Harley-Davidson: Birth of the V-Rod\", October 14, 2001.\nSingle-cylinder engines.\nThe first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum, based on the DeDion-Bouton pattern. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.\nSingle-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.\nModel families.\nModern Harley-branded motorcycles fall into one of seven model families: Touring, Softail, Dyna, Sportster, Vrod, Street and LiveWire. These model families are distinguished by the frame, engine, suspension, and other characteristics.\nTouring.\nTouring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, \"e.g.\", FLHR (Road King) and FLTR (Road Glide).\nThe touring family, also known as \"dressers\" or \"baggers\", includes Road King, Road Glide, Electra Glide and Street Glide models offered in various trims. The Road Kings have a \"retro cruiser\" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the \"Batwing\" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the \"Sharknose\". The Sharknose includes a unique, dual front headlight.\nTouring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.\nThe frame was modified for the 1993 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.\nIn 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.\nIn 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.\nFor the 2009 model year, Harley-Davidson redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, front wheels for all but the FLHRC Road King Classic, and a 2\u20131\u20132 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.\nAlso released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103-cubic-inch (1,690\u00a0cc) engine exclusive to the trike.\nIn 2014, Harley-Davidson released a redesign for specific touring bikes and called it \"Project Rushmore\". Changes include a new 103CI High Output engine, one handed easy open saddlebags and compartments, a new Boom! Box Infotainment system with either 4.3-inch (10\u00a0cm) or 6.5-inch (16.5\u00a0cm) screens featuring touchscreen functionality [6.5-inch (16.5\u00a0cm) models only], Bluetooth (media and phone with approved compatible devices), available GPS and SiriusXM, Text-to-Speech functionality (with approved compatible devices) and USB connectivity with charging. Other features include ABS with Reflex linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.\nSoftail.\nThese big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the \"hardtail\" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with \"Heritage\" styling that incorporate design cues from throughout their history and used to offer \"Springer\" front ends on these Softail models from the factory.\nSoftail models utilize the big-twin engine (F) and the Softail chassis (ST).\nDyna.\nDyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.\nPrior to 2006, Dyna models typically featured a narrow, XL-style 39mm front fork and front wheel, as well as footpegs which the manufacturer indicated with the letter \"X\" in the model designation. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which featured thicker 41mm forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2\u20131\u20132 exhaust, twin headlamps, a 180\u00a0mm rear tire, and, for the first time in the Dyna lineup, a 130\u00a0mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation with floorboards, detachable painted hard saddlebags, touring windshield, headlight nacelle and a wide front tire with full fender. The new front end resembled the big-twin FL models from 1968 to 1971.\nThe Dyna family used the 88-cubic-inch (1,440\u00a0cc) twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cubic inches (1,570\u00a0cc) as the factory increased the stroke to . For the 2012 model year, the manufacturer began to offer Dyna models with the 103-cubic-inch (1,690\u00a0cc) upgrade. All Dyna models use a rubber-mounted engine to isolate engine vibration. Harley discontinued the Dyna platform in 2017 for the 2018 model year, having been replaced by a completely redesigned Softail chassis; some of the existing models previously released by the company under the Dyna nameplate have since been carried over to the new Softail line.\nDyna models utilize the big-twin engine (F), footpegs noted as (X) with the exception of the 2012 FLD Switchback, a Dyna model which used floorboards as featured on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to 2016, all Dyna models have designations that begin with FXD, \"e.g.\", FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).\nSportster.\nIntroduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883\u00a0cc or 1,200\u00a0cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.\nUp until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.\nIn the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.\nIn the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce , four-piston dual front disc brakes, and an aluminum swing arm. \"Motorcyclist\" featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their \"First Ride\" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.\nOne possible reason for the delayed availability in the United States was that Harley-Davidson had to obtain the \"XR1200\" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.\nIn 2021, Harley-Davidson launched the Sportster S model, with a 121\u00a0hp engine and 228\u00a0kg ready-to-ride weight. The Sportster S was one of the first Harleys to come with cornering-ABS and lean-sensitive traction control. The Sportster S is also the first model under the Sportster nameplate since 1957 to receive a completely new engine.\nExcept for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100\u00a0cc to 1,200\u00a0cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883\u00a0cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.\nVRSC.\nIntroduced in 2001 and produced until 2017, the VRSC muscle bike family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming muscle bike/power cruiser segment, the \"V-Rod\" makes use of the revolution engine that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.\nIn 2008, Harley added the anti-lock braking system as a factory-installed option on all VRSC models. Harley also increased the displacement of the stock engine from , which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.\nVRSC models include:\nVRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240\u00a0mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII (CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, \"e.g.\", VRSCDX denotes the Night Rod Special.\nVRXSE.\nThe VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300\u00a0cc \"stroked\" incarnation, featuring a 75\u00a0mm crankshaft, 105\u00a0mm Pistons, and 58\u00a0mm throttle bodies.\nThe V-Rod Destroyer is not a street-legal motorcycle. As such, it uses \"X\" instead of \"SC\" to denote a non-street bike. \"SE\" denotes a CVO Special Edition.\nStreet.\nThe Street, Harley-Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike at a cheaper price. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on February 5, 2014. The Street 750 weighs 218\u00a0kg and has a ground clearance of 144\u00a0mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.\nThe Street 750 uses an all-new, liquid-cooled, 60\u00b0 V-twin engine called the Revolution X. In the Street 750, the engine displaces and produces 65\u00a0Nm at 4,000\u00a0rpm. A six speed transmission is used.\nThe Street 750 and the smaller-displacement Street 500 have been available since late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.\nLiveWire.\nHarley-Davidson's \"LiveWire\", released in 2019, is their first electric vehicle. The high-voltage battery provides a minimum city range of 98 miles (158\u00a0km). The LiveWire targets a different type of customer than their classic V-twin powered motorcycles.\nIn March 2020, a Harley-Davidson LiveWire was used to break the 24-hour distance record for an electric motorcycle. The bike traveled a reported 1,723\u00a0km (1,079 miles) in 23 hours and 48 minutes. The LiveWire offers a Level 1 slow recharge, which uses a regular wall outlet to refill an empty battery overnight, or a quick Level 3 DC Fast Charge. The Fast Charge fills the battery most of the way in about 40 minutes. Swiss rider Michel von Tell used the Level 3 charging to make the 24-hour ride.\nIn December 2021, the company announced that LiveWire was to be spun-off from parent Harley Davidson, set to go public in the first half of 2022 as a special-purpose acquisition company (SPAC) with the value estimated to be $1.77 billion.\nCustom Vehicle Operations.\nCustom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.\nEnvironmental record.\nThe Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an \"environmental warranty\". The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the One Clean-Up Program. This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.\nPaul Gotthold, Director of Operations for the EPA, congratulated the motor company:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Harley-Davidson has taken their environmental responsibilities very seriously and has already made substantial progress in the investigation and cleanup of past contamination. Proof of Harley's efforts can be found in the recent EPA determination that designates the Harley property as 'under control' for cleanup purposes. This determination means that there are no serious contamination problems at the facility. Under the new One Cleanup Program, Harley, EPA, and PADEP will expedite the completion of the property investigation and reach a final solution that will permanently protect human health and the environment.\nHarley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth \"protection to the purchaser (Harley-Davidson) against environmental risks\".\nIn August 2016, Harley-Davidson settled with the EPA for $12\u00a0million, without admitting wrongdoing, over the sale of after-market \"super tuners\". Super tuners were devices, marketed for competition, which enabled increased performance of Harley-Davidson products. However, the devices also modified the emission control systems, producing increased hydrocarbon and nitrogen oxide. Harley-Davidson is required to buy back and destroy any super tuners which do not meet Clean Air Act requirements and spend $3\u00a0million on air pollution mitigation.\nBrand culture.\nAccording to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. However, by 2006, only 15 percent of Harley buyers were under 35, and as of 2005, the median age had risen to 46.7. In 2008, Harley-Davidson stopped disclosing the average age of riders; at this point it was 48 years old.\nIn 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.\nMany Harley-Davidson Clubs exist nowadays around the world; the oldest one, founded in 1928, is in Prague.\nHarley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5 percent of the company's net revenue ($41\u00a0million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.\nFrom its founding, Harley-Davidson had worked to brand its motorcycles as respectable and refined products, with ads that showed what motorcycling writer Fred Rau called \"refined-looking ladies with parasols, and men in conservative suits as the target market\". The 1906 Harley-Davidson's effective, and polite, muffler was emphasized in advertisements with the nickname \"The Silent Gray Fellow\". That began to shift in the 1960s, partially in response to the clean-cut motorcyclist portrayed in Honda's \"You meet the nicest people on a Honda\" campaign, when Harley-Davidson sought to draw a contrast with Honda by underscoring the more working-class, macho, and even a little anti-social attitude associated with motorcycling's dark side. With the 1971 FX Super Glide, the company embraced, rather than distanced itself from, chopper style and the counterculture custom Harley scene. Their marketing cultivated the \"bad boy\" image of biker and motorcycle clubs, and to a point, even outlaw or one-percenter motorcycle clubs.\nOrigin of \"Hog\" nickname.\nBeginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the \"hog boys\", consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product, taking advantage of the long-standing nickname by turning \"hog\" into the acronym HOG, for Harley Owners Group. Harley-Davidson attempted to trademark \"hog\", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, New York, in 1999, when the appellate panel ruled that \"hog\" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.\nOn August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.\nBobbers.\nHarley-Davidson FL \"big twins\" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called \"bobbers\" or sometimes \"choppers\", because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of motorcycle clubs like the Hells Angels, referred to stock FLs as \"garbage wagons\".\nHarley Owners Group.\nHarley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,\nand other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.\nHOG members typically spend 30 percent more than other Harley owners on such items as clothing and Harley-Davidson-sponsored events.\nIn 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.\nToday, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.\nHOG benefits include organized group rides, exclusive products and product discounts, https://, and the Hog Tales newsletter. A one-year full membership is included with the purchase of a new, unregistered Harley-Davidson.\nIn 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.\n3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters.\nFactory tours and museum.\nHarley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.\nDue to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin, was closed in 2009.\nHistoric register designations.\nSome of the company's buildings have been listed on state and national historic registers, including:\nAnniversary celebrations.\nBeginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the \"Ride Home\". This new tradition has continued every five years, and is referred to unofficially as \"Harleyfest\", in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28\u201331, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th-anniversary celebration was held on August 29\u201331, 2013. The 115th anniversary was held in Prague, Czech Republic, the home country of the oldest existing Harley Davidson Club, on July 5\u20138, 2018 and attracted more than 100,000 visitors and 60,000 bikes.\nThe 120th anniversary was held in Budapest, Hungary, with the parade on June 24.\nLabor Hall of Fame.\nWilliam S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson Sr were, in 2004, inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.\nTelevision drama.\nThe company's origins were dramatized in a 2016 miniseries entitled \"Harley and the Davidsons\", starring Robert Aramayo as William Harley, Bug Hall as Arthur Davidson and Michiel Huisman as Walter Davidson, and premiered on the Discovery Channel as a \"three-night event series\" on September 5, 2016.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14143", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=14143", "title": "Harappan Civilisation", "text": ""}
{"id": "14144", "revid": "461300", "url": "https://en.wikipedia.org/wiki?curid=14144", "title": "Hiberno-English", "text": "Dialect of English spoken in Ireland\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nHiberno-English or Irish English (IrE), also formerly sometimes called Anglo-Irish, is the set of dialects of the English language native to the island of Ireland. In both the Republic of Ireland and Northern Ireland, English is the first language in everyday use and, alongside the Irish language, one of two official languages (with Ulster Scots, in Northern Ireland, being yet another local language). \nThe writing standards of Irish English, such as its spelling, align with British English. But the diverse accents and some of the grammatical structures and vocabulary of Irish English are unique, including certain notably conservative phonological features and vocabulary: those that are no longer common in the dialects of England or North America. It shows significant influences from the Irish language and also, in the north, the Scots language.\nPhonologists today often divide Irish English into four or five overarching dialects or accents: Ulster or Northern Irish accents, Western and Southern Irish accents (like Cork accents), various Dublin accents, and a non-regional standard accent (outside of Ulster) whose features have been developing since the late 1970s.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nMiddle English, as well as a small elite that spoke Anglo-Norman, was brought to Ireland as a result of the Anglo-Norman invasion of Ireland in the late 12th century. The remnants of which survived as the Yola language and Fingallian dialects, which is not mutually comprehensible with Modern English. A second wave of the English language was brought to Ireland in the 16th-century Elizabethan Early Modern period, making that variety of English spoken in Ireland the oldest outside of Great Britain. It remains more conservative today than many other dialects of English in terms of phonology and vocabulary.\nInitially, during the Anglo-Norman period in Ireland, English was mainly spoken in an area known as the Pale around Dublin, with largely the Irish language spoken throughout the rest of the country. Some small pockets of speakers remained, who predominantly continued to use the English of that time. Because of their sheer isolation, these dialects developed into later, now-extinct, English-related varieties, known as Yola in Wexford and Fingallian in Fingal, Dublin. These were no longer mutually intelligible with other English varieties. By the Tudor period, Irish culture and language had regained most of the territory lost to the invaders: even in the Pale, \"all the common folk\u2026 for the most part are of Irish birth, Irish habit, and of Irish language\".\nThe Tudor conquest and colonisation of Ireland in the 16th century led to a second wave of immigration by English speakers, along with the forced suppression and decline in the status and use of the Irish language. By the mid-19th century, English had become the majority language spoken in the country. It has retained this status to the present day, with even those whose first language is Irish being fluent in English as well. Today, there is little more than one per cent of the population who speaks the Irish language natively, though it is required to be taught in all state-funded schools. Of the 40% of the population who self-identified as speaking some Irish in 2016, 4% speak Irish daily outside the education system.\nA German traveller, Ludolf von M\u00fcnchhausen, visited the Pale in Dublin in 1591. He says of the pale in regards to the language spoken there: \"Little Irish is spoken; there are even some people here who cannot speak Irish at all\". He may be mistaken, but if this account is true, the language of Dublin in the 1590s was English, not Irish.\nAnd yet again, Albert Jouvin travelled to Ireland in 1668; he says of the pale and the east coast, \"In the inland parts of Ireland, they speak a particular language, but in the greatest part of the towns and villages on the sea coast, only English is spoken\".\u00a0'A Tour of Ireland in 1775', by Richard Twiss says of the language spoken in Dublin \"as at present almost all the peasants speak the English language, they converse with as much propriety as any persons of their class in England.\"\nIn On Early English Pronunciation, Part V, an early dialect study on English, Alexander John Ellis included some samples of Hiberno-English dialect from the Forth and Bargy baronies in County Wexford. Writing in the late 19th century, Ellis seems to have been unaware that English had been spoken in parts of Ireland, especially in Ulster, for centuries.\nUlster English.\nUlster English, or Northern Irish English, here refers collectively to the varieties of the Ulster province, including Northern Ireland and neighbouring counties outside of Northern Ireland, which has been influenced by Ulster Irish as well as the Scots language, brought over by Scottish settlers during the Plantation of Ulster. Its main subdivisions are Mid-Ulster English, South Ulster English and Ulster Scots, the latter of which is arguably a separate language. \nUlster varieties distinctly pronounce:\nWestern and Southern Irish English.\nWestern and Southern Irish English is a collection of broad varieties of Ireland's West Region and Southern Region. Accents of both regions are known for:\nThe subset, South-West Irish English (often known, by specific county, as Cork English, Kerry English, or Limerick English), features two additional defining characteristics of its own. One is the pin\u2013pen merger: the raising of \"dress\" to when before or (as in \"again\" or \"pen\"). The other is the intonation pattern of a slightly higher pitch followed by a significant drop in pitch on stressed long-vowel syllables (across multiple syllables or even within a single one), which is popularly heard in rapid conversation, by speakers of other English dialects, as a noticeable kind of undulating \"sing-song\" pattern.\nDublin English.\nDublin English is highly internally diverse and refers collectively to the Irish English varieties immediately surrounding and within the metropolitan area of Dublin. Modern-day Dublin English largely lies on a phonological continuum, ranging from a more traditional, lower-prestige, local urban accent on the one end, to a more recently developing, higher-prestige, non-local, regional and even supra-regional accent on the other end. Most of the latter characteristics of Dublin English first emerged in the late 1980s and early 1990s. \nThe accent that most strongly uses the traditional working-class features has been labelled by the linguist Raymond Hickey as \"local Dublin English\". Most speakers from Dublin and its suburbs have accent features falling variously along the entire middle, as well as the newer end of the spectrum, which together form what is called \"non-local Dublin English\". It is spoken by middle- and upper-class natives of Dublin and the greater eastern Irish region surrounding the city.\nIn the most general terms, all varieties of Dublin English have the following identifying sounds that are often distinct from the rest of Ireland, pronouncing:\nLocal Dublin English.\nLocal Dublin English (or popular Dublin English) is a traditional, broad, working-class variety spoken in the Republic of Ireland's capital city of Dublin. It is the only Irish English variety that in earlier history was non-rhotic; but today it is weakly rhotic. Known for diphthongisation of the GOAT and FACE vowels, the local Dublin accent is also known for a phenomenon called \"vowel breaking\", in which MOUTH, PRICE, GOOSE and FLEECE in closed syllables are \"broken\" into two syllables, approximating , , , and , respectively.\nAdvanced Dublin English.\nEvolving as a fashionable outgrowth of the mainstream non-local Dublin English, advanced Dublin English, also new Dublin English or formerly fashionable Dublin English, is a youthful variety that originally began in the early 1990s among the \"avant-garde\" and now those aspiring to a non-local \"urban sophistication\". Advanced Dublin English itself, first associated with affluent and middle-class inhabitants of southside Dublin, is probably now spoken by a majority of Dubliners born since the 1980s. \nAdvanced Dublin English can have a fur\u2013fair merger, horse\u2013hoarse, and witch\u2013which mergers, while resisting the traditionally Irish English cot\u2013caught merger. This accent has since spread south to parts of east County Wicklow, west to parts of north County Kildare, and parts of south County Meath. The accent can be heard among the middle to upper classes in most major cities in the Republic today.\nStandard Irish English.\nSupraregional Southern Irish English, sometimes, simply Supraregional Irish English or Standard Irish English, refers to a variety spoken particularly by educated and middle- or higher-class Irish people, crossing regional boundaries throughout all of the Republic of Ireland, except the north. A mainstream middle-class variety of Dublin English of the early- to mid-twentieth century is the direct influence and catalyst for this variety, coming about by the suppression of certain markedly Irish features, and retention of other Irish features, as well as the adoption of certain standard British (i.e., non-Irish) features. \nThe result is a configuration of features that is still unique. In other words, this accent is not simply a wholesale shift towards British English. Most speakers born in the 1980s or later are showing fewer features of this late-twentieth-century mainstream supraregional form and more characteristics aligning with a rapidly-spreading advanced Dublin accent. See more above, under \"Non-local Dublin English\".\nIreland's supraregional dialect pronounces:\nOverview of pronunciation and phonology.\nThe following charts list the vowels typical of each Irish English dialect as well as the several distinctive consonants of Irish English, according to the linguist Raymond Hickey. Phonological characteristics of overall Irish English are given as well as categorisations into five major divisions of Hiberno-English: Ulster; West and South-West Ireland; local Dublin; advanced Dublin; and supraregional (southern) Ireland. Features of mainstream non-local Dublin English fall on a range between what Hickey calls \"local Dublin\" and \"advanced Dublin\".\nMonophthongs.\nThe following monophthongs are defining characteristics of Irish English:\nFootnotes:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^1 In southside Dublin's once-briefly fashionable \"Dublin 4\" (or \"Dortspeak\") accent, the \" and broad \" set becomes rounded as .\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^2 In South-West Ireland, before or is raised to.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^3 Due to the phenomenon of \"vowel breaking\" in local Dublin accents, and may be realised as and in closed syllables.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^4 Ulster-English lacks \"Happy\"-tensing, meaning that the final vowel takes the diaphoneme instead of . In this context, said vowel is often realised as .\nOther notes:\nDiphthongs.\nThe following diphthongs are defining characteristics of Irish English:\nFootnotes:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^1 \nDue to the phenomenon of \"vowel breaking\" local Dublin accents, and may be realised as and in closed syllables.\nConsonants.\nThe consonants of Hiberno-English mostly align with the typical English consonant sounds. but a few Irish English consonants have distinctive, varying qualities. The following consonant features are defining characteristics of Hiberno-English: \nFootnotes:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^1 In traditional, conservative Ulster English, and are palatalised before an open front vowel.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^2 Local Dublin features consonant cluster reduction, so that plosives occurring after fricatives or sonorants may be left unpronounced, resulting, for example, in \"poun(d)\" and \"las(t)\".\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^3 In extremely traditional and conservative accents (e.g. M\u00edche\u00e1l \u00d3 Muircheartaigh and Jackie Healy-Rae), prevocalic can also be an alveolar flap, . may be guttural (uvular, ) in north-east Leinster.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^4 \u27e8\u03b8\u0320\u27e9 is used here to represent the voiceless alveolar non-sibilant fricative, sometimes known as a \"slit fricative\", which is apico-alveolar.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^5 Overall, and are being increasingly merged in supraregional Irish English, for example, making \"wine\" and \"whine\" homophones, as in most varieties of English around the world.\nVowel + \u27e8r\u27e9 combinations.\nThe following vowels + \u27e8r\u27e9 create combinations that are defining characteristics of Hiberno-English: \nFootnotes:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^1 In southside Dublin's \"Dublin 4\" (or \"Dortspeak\") accent, is realised as .\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^2 In non-local Dublin's more recently emerging (or \"advanced Dublin\") accent, and may both be realised more rounded as .\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^3 The NURSE mergers have not occurred in local Dublin, West/South-West, and other very conservative and traditional Irish English dialects, including in Ulster, all of which retain a two-way distinction between as in \"earn\" versus as in \"urn\". Contrarily, most English dialects worldwide have merged and before the consonant . For instance, in the case of non-local Dublin, supraregional, and younger Irish accents, the merged sequence is phonetically . But for those accents that retain the more conservative distinction, the distribution of and is as follows: occurs when spelled \u27e8ur\u27e9 and \u27e8or\u27e9 (e.g. \"urn\" and \"word\"), \u27e8ir\u27e9 after alveolar stops (e.g. \"dirt\"), and after labial consonants (e.g. \"fern\"); is occurs in all other situations. There are apparent exceptions to these rules; John C. Wells describes \"prefer\" and \"per\" as , despite the vowel in question following a labial in both cases. The distribution of versus is listed below in some example words:\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^4 In a rare few local Dublin varieties that are non-rhotic, is either lowered to or backed and raised to .\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^5 The distinction between and is widely preserved in Ireland, so that, for example, \"horse\" and \"hoarse\" are not merged in most Irish English dialects; but they are usually merged in Belfast and advanced Dublin.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^6 In local Dublin may be realised as . For some speakers may merge with .\nVocabulary.\nLoan words from Irish.\nA number of Irish language loan words are used in Hiberno-English, particularly in an official state capacity. For example, the head of government is the Taoiseach, the deputy head is the T\u00e1naiste, the parliament is the Oireachtas and its lower house is D\u00e1il \u00c9ireann. Less formally, people also use loan words in day-to-day speech, although this has been on the wane in recent decades and among the young.\nDerived words from Irish.\nAnother group of Hiberno-English words are those \"derived\" from the Irish language. Some are words in English that have entered into general use, while others are unique to Ireland. These words and phrases are often Anglicised versions of words in Irish or direct translations into English. In the latter case, they often give meaning to a word or phrase that is generally not found in wider English use.\nDerived words from Old and Middle English.\nAnother class of vocabulary found in Hiberno-English are words and phrases common in Old and Middle English, but which have since become obscure or obsolete in the modern English language generally. Hiberno-English has also developed particular meanings for words that are still in common use in English generally.\nOther words.\nIn addition to the three groups above, there are also additional words and phrases whose origin is disputed or unknown. While this group may not be unique to Ireland, their usage is not widespread, and could be seen as characteristic of Irish English.\nGrammar and syntax.\nThe syntax of the Irish language is quite different from that of English. Various aspects of Irish syntax have influenced Hiberno-English, though many of these idiosyncrasies are disappearing in suburban areas and among the younger population.\nAnother feature of Hiberno-English that sets it apart is the retention of words and phrases from Old and Middle English that are not retained otherwise in Modern English.\nFrom Irish.\nReduplication.\nReduplication is an alleged trait of Hiberno-English strongly associated with Stage Irish and Hollywood films.\nYes and no.\nIrish has no words that directly translate as 'yes' or 'no', and instead repeats the verb used in the question, negated if necessary, to answer. Hiberno-English uses \"yes\" and \"no\" less frequently than other English dialects as speakers can repeat the verb, positively or negatively, instead of (or in redundant addition to) using \"yes\" or \"no\".\nThis is not limited only to the verb \"to be\": it is also used with \"to have\" when used as an auxiliary; and, with other verbs, the verb \"to do\" is used. This is most commonly used for intensification, especially in Ulster English.\nRecent past construction.\nIrish indicates recency of an action by adding \"after\" to the present continuous (a verb ending in \"-ing\"), a construction known as the \"hot news perfect\" or \"after perfect\". The idiom for \"I had done X when I did Y\" is \"I was after doing X when I did Y\", modelled on the Irish usage of the compound prepositions , , and : \u2009/\u2009\u2009/\u2009.\nA similar construction is seen where exclamation is used in describing a recent event:\nWhen describing less astonishing or significant events, a structure resembling the German perfect can be seen:\nThis correlates with an analysis of \"H1 Irish\" proposed by Adger &amp; Mitrovic, in a deliberate parallel to the status of German as a V2 language.\nRecent past construction has been directly adopted into Newfoundland English, where it is common in both formal and casual register. In rural areas of the Avalon peninsula, where Newfoundland Irish was spoken until the early 20th century, it is the grammatical standard for describing whether or not an action has occurred.\nReflection for emphasis.\nThe reflexive version of pronouns is often used for emphasis or to refer indirectly to a particular person, etc., according to context. \"Herself\", for example, might refer to the speaker's boss or to the woman of the house. Use of \"herself\" or \"himself\" in this way can imply status or even some arrogance of the person in question. Note also the indirectness of this construction relative to, for example, \"She's coming now\". This reflexive pronoun can also be used in a more neutral sense to describe a person's spouse or partner \u2013 \"I was with himself last night\" or \"How's herself doing?\"\nPrepositional pronouns.\nThere are some language forms that stem from the fact that there is no verb \"to have\" in Irish. Instead, possession is indicated in Irish by using the preposition \"at\", (in Irish, ). To be more precise, Irish uses a prepositional pronoun that combines 'at' and 'me' to create . In English, the verb \"to have\" is used, along with a \"with me\" or \"on me\" that derives from . This gives rise to the frequent\nSomebody who can speak a language \"has\" a language, in which Hiberno-English has borrowed the grammatical form used in Irish.\nWhen describing something, many Hiberno-English speakers use the term \"in it\" where \"there\" would usually be used. This is due to the Irish word fulfilling both meanings.\nAnother idiom is this thing or that thing described as \"this man here\" or \"that man there\", which also features in Newfoundland English in Canada.\nConditionals have a greater presence in Hiberno-English due to the tendency to replace the simple present tense with the conditional (would) and the simple past tense with the conditional perfect (would have).\nBring and take: Irish use of these words differs from that of British English because it follows the Irish grammar for and . English usage is determined by direction; a person determines Irish usage. So, in English, one takes \"\"from\" here \"to\" there\", and brings it \"\"to\" here \"from\" there\". In Irish, a person takes only when accepting a transfer of possession of the object from someone else\u00a0\u2013 and a person brings at all other times, irrespective of direction (to or from).\nTo be.\nThe Irish equivalent of the verb \"to be\" has two present tenses, one (the present tense proper or \") for cases which are generally true or are true at the time of speaking and the other (the habitual present or \") for repeated actions. Thus, \"you are [now, or generally]\" is , but \"you are [repeatedly]\" is . Both forms are used with the verbal noun (equivalent to the English present participle) to create compound tenses. This is similar to the distinction between and in Spanish or the use of the \"habitual be\" in African-American Vernacular English.\nThe corresponding usage in English is frequently found in rural areas, especially County Mayo and County Sligo in the west of Ireland and County Wexford in the south-east, inner-city Dublin and Cork city along with border areas of the North and Republic. In this form, the verb \"to be\" in English is similar to its use in Irish, with a \"does be/do be\" (or \"bees\", although less frequently) construction to indicate the continuous, or habitual, present:\nFrom Old and Middle English.\nIn old-fashioned usage, \"it is\" can be freely abbreviated \"'tis\", even as a standalone sentence. This also allows the double contraction \"'tisn't\", for \"it is not\".\nIrish has separate forms for the second person singular () and the second person plural ().\nMirroring Irish, and almost every other Indo-European language, the plural \"you\" is also distinguished from the singular in Hiberno-English, normally by use of the otherwise archaic English word \"ye\" ; the word \"yous\" (sometimes written as \"youse\") also occurs, but primarily only in Dublin and across Ulster. In addition, in some areas in Leinster, north Connacht and parts of Ulster, the hybrid word \"ye-s\", pronounced \"yiz\", may be used. The pronunciation differs with that of the northwestern being and the Leinster pronunciation being .\nThe word \"ye\", \"yis\" or \"yous\", otherwise archaic, is still used in place of \"you\" for the second-person plural, e.g. \"Where are yous going?\" \"Ye'r\", \"Yisser\" or \"Yousser\" are the possessive forms.\nThe verb \"mitch\" is very common in Ireland, indicating being truant from school. This word appears in Shakespeare (though he wrote in Early Modern English rather than Middle English), but is seldom heard these days in British English, although pockets of usage persist in some areas (notably South Wales, Devon, and Cornwall). In parts of Connacht and Ulster the \"mitch\" is often replaced by the verb \"scheme\", while in Dublin it is often replaced by \"on the hop/bounce\".\nAnother usage familiar from Shakespeare is the inclusion of the second person pronoun after the imperative form of a verb, as in \"Wife, go you to her ere you go to bed\" (Romeo and Juliet, Act III, Scene IV). This is still common in Ulster: \"Get youse your homework done or you're no goin' out!\". In Munster, you will still hear children being told, \"Up to bed, let ye\" , although wider English uses similar constructions such as \"Up to bed you go\".\nFor influence from Scotland, see Ulster Scots and Ulster English.\nOther grammatical influences.\n\"Now\" is often used at the end of sentences or phrases as a semantically empty word, completing an utterance without contributing any apparent meaning. Examples include \"Bye now\" (= \"Goodbye\"), \"There you go now\" (when giving someone something), \"Ah now!\" (expressing dismay), \"Hold on now\" (= \"wait a minute\"), \"Now then\" as a mild attention-getter, etc. This usage is universal among English dialects, but occurs more frequently in Hiberno-English. It is also used in the manner of the Italian 'prego' or German 'bitte', for example, a barman might say \"Now, Sir.\" when delivering drinks.\n\"So\" is often used for emphasis (\"I can speak Irish, so I can\"), or it may be tacked onto the end of a sentence to indicate agreement, where \"then\" would often be used in Standard English (\"Bye so\", \"Let's go so\", \"That's fine so\", \"We'll do that so\"). The word is also used to contradict a negative statement (\"You're not pushing hard enough\" \u2013 \"I am so!\"). (This contradiction of a negative is also seen in American English, though not as often as \"I am too\", or \"Yes, I am\".) The practice of indicating emphasis with \"so\" and including reduplicating the sentence's subject pronoun and auxiliary verb (is, are, have, has, can, etc.) such as in the initial example, is particularly prevalent in more northern dialects such as those of Sligo, Mayo and the counties of Ulster.\n\"Sure/Surely\" is often used as a tag word, emphasising the obviousness of the statement, roughly translating as but/and/well/indeed. It can be used as \"to be sure\" (but the other stereotype of \"Sure and \u2026\" is not actually used in Ireland.) Or \"Sure, I can just go on Wednesday\", \"I will not, to be sure.\" The word is also used at the end of sentences (primarily in Munster), for instance, \"I was only here five minutes ago, sure!\" and can express emphasis or indignation. In Ulster, the reply \"Aye, surely\" may be given to show strong agreement.\n\"To\" is often omitted from sentences where it would exist in British English. For example, \"I'm not allowed go out tonight\", instead of \"I'm not allowed \"to\" go out tonight\".\n\"Will\" is often used where British English would use \"shall\" or American English \"should\" (as in \"Will I make us a cup of tea?\"). The distinction between \"shall\" (for first-person simple future, and second- and third-person emphatic future) and \"will\" (second- and third-person simple future, first-person emphatic future), maintained by many in England, does not exist in Hiberno-English, with \"will\" generally used in all cases.\n\"Once\" is sometimes used in a different way from how it is used in other dialects; in this usage, it indicates a combination of logical and causal conditionality: \"I have no problem laughing at myself once the joke is funny.\" Other dialects of English would probably use \"if\" in this situation.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14147", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=14147", "title": "Harmonic analysis", "text": "Study of superpositions in mathematics\nHarmonic analysis is a branch of mathematics concerned with investigating the connections between a function and its representation in frequency. The frequency representation is found by using the Fourier transform for functions on unbounded domains such as the full real line or by Fourier series for functions on bounded domains, especially periodic functions on finite intervals. Generalizing these transforms to other domains is generally called Fourier analysis, although the term is sometimes used interchangeably with harmonic analysis. Harmonic analysis has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis, spectral analysis, and neuroscience.\nThe term \"harmonics\" originated from the Ancient Greek word \"harmonikos\", meaning \"skilled in music\". In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes. Still, the term has been generalized beyond its original meaning.\nDevelopment of harmonic analysis.\nHistorically, harmonic functions first referred to the solutions of Laplace's equation. This terminology was extended to other special functions that solved related equations, then to eigenfunctions of general elliptic operators, and nowadays harmonic functions are considered as a generalization of periodic functions in function spaces defined on manifolds, for example as solutions of general, not necessarily elliptic, partial differential equations including some boundary conditions that may imply their symmetry or periodicity.\nFourier analysis.\nThe classical Fourier transform on R\"n\" is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution \"f\", we can attempt to translate these requirements into the Fourier transform of \"f\". The Paley\u2013Wiener theorem is an example. The Paley\u2013Wiener theorem immediately implies that if \"f\" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported (i.e., if a signal is limited in one domain, it is unlimited in the other). This is an elementary form of an uncertainty principle in a harmonic-analysis setting.\nFourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis. There are four versions of the Fourier transform, dependent on the spaces that are mapped by the transformation:\nAs the spaces mapped by the Fourier transform are, in particular, subspaces of the space of tempered distributions it can be shown that the four versions of the Fourier transform are particular cases of the Fourier transform on tempered distributions.\nAbstract harmonic analysis.\nAbstract harmonic analysis is primarily concerned with how real or\ncomplex-valued functions (often on very general domains) can be studied using symmetries such\nas translations or rotations (for instance via the Fourier transform and its relatives); this field is of\ncourse related to real-variable harmonic analysis, but is perhaps closer in spirit to representation theory and functional analysis.\nOne of the most modern branches of harmonic analysis, having its roots in the mid-20th century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups.\nOne of the major results in the theory of functions on abelian locally compact groups is called Pontryagin duality. \nHarmonic analysis studies the properties of that duality. Different generalization of Fourier transforms attempts to extend those features to different settings, for instance, first to the case of general abelian topological groups and second to the case of non-abelian Lie groups.\nHarmonic analysis is closely related to the theory of unitary group representations for general non-abelian locally compact groups. For compact groups, the Peter\u2013Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the valuable properties of the classical Fourier transform in terms of carrying convolutions to pointwise products or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.\nIf the group is neither abelian nor compact, no general satisfactory theory is currently known (\"satisfactory\" means at least as strong as the Plancherel theorem). However, many specific cases have been analyzed, for example, SL\"n\". In this case, representations in infinite dimensions play a crucial role.\nApplied harmonic analysis.\nMany applications of harmonic analysis in science and engineering begin with the idea or hypothesis that a phenomenon or signal is composed of a sum of individual oscillatory components. Ocean tides and vibrating strings are common and simple examples. The theoretical approach often tries to describe the system by a differential equation or system of equations to predict the essential features, including the amplitude, frequency, and phases of the oscillatory components. The specific equations depend on the field, but theories generally try to select equations that represent significant principles that are applicable.\nThe experimental approach is usually to acquire data that accurately quantifies the phenomenon. For example, in a study of tides, the experimentalist would acquire samples of water depth as a function of time at closely enough spaced intervals to see each oscillation and over a long enough duration that multiple oscillatory periods are likely included. In a study on vibrating strings, it is common for the experimentalist to acquire a sound waveform sampled at a rate at least twice that of the highest frequency expected and for a duration many times the period of the lowest frequency expected.\nFor example, the top signal at the right is a sound waveform of a bass guitar playing an open string corresponding to an A note with a fundamental frequency of 55\u00a0Hz. The waveform appears oscillatory, but it is more complex than a simple sine wave, indicating the presence of additional waves. The different wave components contributing to the sound can be revealed by applying a mathematical analysis technique known as the Fourier transform, shown in the lower figure. There is a prominent peak at 55\u00a0Hz, but other peaks at 110\u00a0Hz, 165\u00a0Hz, and at other frequencies corresponding to integer multiples of 55\u00a0Hz. In this case, 55\u00a0Hz is identified as the fundamental frequency of the string vibration, and the integer multiples are known as harmonics.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14148", "revid": "1319771667", "url": "https://en.wikipedia.org/wiki?curid=14148", "title": "Home run", "text": "Four-base hit resulting in a run by the batter in baseball\nIn baseball, a home run (abbreviated HR) is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home plate safely in one play without any errors being committed by the defensive team. A home run is usually achieved by hitting the ball over the outfield fence between the foul poles (or hitting either foul pole) without the ball touching the field.\nInside-the-park home runs, when the batter reaches home safely while the baseball is in play on the field, are infrequent. In very rare cases, a fielder attempting to catch a ball in flight may misplay it and knock it over the outfield fence, resulting in a home run.\nAn official scorer will credit the batter with a hit, a run scored, and a run batted in (RBI), as well as an RBI for each runner on base. The pitcher is recorded as having given up a hit and a run, with additional runs charged for each base-runner that scores.\nHome runs are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams\u2014hence the old saying, \"Home run hitters drive Cadillacs, and singles hitters drive Fords\" (coined, circa 1948, by veteran pitcher Fritz Ostermueller, by way of mentoring his young teammate, Ralph Kiner).\nNicknames for a home run include \"homer\", \"round-tripper\", \"four-bagger\", \"big fly\", \"goner\" \"dinger\", \"long ball\", \"jack\", \"quadruple\", \"moon shot\", \"bomb\", \"tater\", and \"blast\", while a player hitting a home run may be said to have \"gone deep\" or \"gone yard\".\nTypes of home runs.\nOut of the park.\nA home run is most often scored when the ball is hit over the outfield wall between the foul poles (in fair territory) before it touches the ground (in flight), and without being caught or deflected back onto the field by a fielder. A batted ball is also a home run if it touches either a foul pole or its attached screen before touching the ground, as the foul poles are by definition in fair territory. Additionally, many major-league ballparks have ground rules stating that a batted ball in flight that strikes a specified location or fixed object is a home run; this usually applies to objects that are beyond the outfield wall but are located such that it may be difficult for the umpire to judge.\nIn professional baseball, a batted ball that goes over the outfield wall \"after\" touching the ground (i.e. a ball that bounces over the outfield wall) becomes an automatic double. This is colloquially referred to as a \"ground rule double\" even though it is uniform across all of Major League Baseball, per MLB rules 5.05(a)(6) through 5.05(a)(9).\nA fielder is allowed to reach over the wall to try to catch the ball as long as his feet are on or over the field during the attempt, and if the fielder successfully catches the ball while it is in flight the batter is out, even if the ball had already passed the vertical plane of the wall. However, since the fielder is not part of the field, a ball that bounces off a fielder (including his glove) and over the wall without touching the ground is still a home run. A fielder may not deliberately throw his glove, cap, or any other equipment or apparel to stop or deflect a fair ball, and an umpire may award a home run to the batter if a fielder does so on a ball that, in the umpire's judgment, would have otherwise been a home run (this is rare in modern professional baseball).\nA home run accomplished in any of the above manners is an automatic home run. The ball is dead, even if it rebounds back onto the field (e.g., from striking a foul pole), and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so will not cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).\nInside-the-park home run.\nAn inside-the-park home run is a rare play in which a batter rounds all four bases for a home run without the baseball leaving the field of play. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.\nIn the early days of baseball, outfields were much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.\nModern outfields are much less spacious and more uniformly designed than in the game's early days. Therefore, inside-the-park home runs are now rare. They usually occur when a fast runner hits the ball deep into the outfield and the ball bounces in an unexpected direction away from the nearest outfielder (e.g., off a divot in the field or off the outfield wall), the nearest outfielder is injured on the play and cannot get to the ball, or an outfielder misjudges the flight of the ball in a way that he cannot quickly recover from the mistake (e.g., by diving and missing). The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.\nIf any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored. Instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.\nAn example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&amp;T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball that caromed off the right-center field wall in the opposite direction from where National League right fielder Ken Griffey Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history and led to Suzuki being named the game's Most Valuable Player.\nNumber of runs batted in.\nHome runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is never called a \"one-run homer\", but rather a solo home run, solo homer, or \"solo shot\". With one runner on base, two runs score (the base-runner and the batter), and thus the home run is often called a two-run homer or two-run shot. Similarly, a home run with two runners on base is a three-run homer or three-run shot.\nThe term \"four-run homer\" is never used. Instead, it's called a \"grand slam\". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.\nGrand slam.\nA grand slam occurs when the bases are \"loaded\" (that is, base runners are at first, second, and third base) and the batter hits a home run. According to \"The Dickson Baseball Dictionary\", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.\nOn July 25, 1956, Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9\u20138 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.\nOn April 23, 1999, Fernando Tat\u00eds Sr. made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tat\u00eds also set a Major League record with 8 RBI in one inning.\nOn July 29, 2003, against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate; he hit three home runs in that game, and his two grand slams were in consecutive at-bats.\nOn August 25, 2011, the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually won the game 22\u20139, after trailing 7\u20131.\nOn October 25th, 2024, Freddie Freeman made history by hitting a walk-off grand slam in the bottom of the 10th inning in Game 1 of the World Series, leading the Los Angeles Dodgers to a 6-3 victory over the New York Yankees. Freeman is the only player in MLB history to do so in the World Series.\nSpecific situation home runs.\nThese types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.\nWalk-off home run.\nA walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the losing team has to \"walk off\" the field.\nTwo World Series have ended via the \"walk-off\" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a ninth-inning solo home run in the seventh game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a ninth-inning three-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.\nSuch a home run can also be called a \"sudden death\" or \"sudden victory\" home run. That usage has lessened as \"walk-off home run\" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death home run would most likely be the \"Shot Heard 'Round the World\" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants, along with many other game-ending home runs that famously ended some of the most important and suspenseful baseball games.\nA walk-off home run over the fence is an exception to baseball's one-run rule. Normally if the home team is tied or behind in the ninth or extra innings, the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches first base or the moment the runner touches home plate\u2014whichever happens last. However, this is superseded by the \"ground rule\", which provides automatic doubles (when a ball-in-play hits the ground first and then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.\nLeadoff home run.\nA leadoff home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB (major league Baseball), Rickey Henderson holds the career record with 81 lead-off home runs.\n Craig Biggio holds the National League career record with 53, fourth overall following Henderson, George Springer with 61, and Alfonso Soriano with 54. As of August 26, 2025, Springer holds the career record among active players, with 61 leadoff home runs, which also ranks second all-time.\nIn 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.\nBack-to-back.\nWhen consecutive batters hit home runs, it's referred to as back-to-back home runs. The home runs are still considered back-to-back even if the batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back.\nFour home runs in a row have only occurred eleven times in Major League Baseball history. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on July 2, 2022, when the St. Louis Cardinals hit four in a row against the Philadelphia Phillies. Nolan Arenado, Nolan Gorman, Juan Yepez, and Dylan Carlson hit consecutive home runs during the first inning off starting pitcher Kyle Gibson.\nOn June 9, 2019, the Washington Nationals hit four in a row against the San Diego Padres in Petco Park as Howie Kendrick, Trea Turner, Adam Eaton and Anthony Rendon homered off pitcher Craig Stammen. Stammen became the fifth pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963, Chase Wright on April 22, 2007, Dave Bush on August 10, 2010, and Michael Blazek on July 27, 2017.\nOn August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9\u20132. In this game, Jim Thome, Paul Konerko, Alexei Ram\u00edrez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez hit their home runs against Joel Peralta, while Uribe did it off Rob Tejeda.\nOn April 22, 2007, the Boston Red Sox were trailing the New York Yankees 3\u20130 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit consecutive home runs to put them up 4\u20133. They eventually went on to win the game 7\u20136 after a three-run home run by Mike Lowell in the bottom of the seventh inning. On September 18, 2006, trailing 9\u20135 to the San Diego Padres in the ninth inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the tenth, the Dodgers won the game in the bottom of the tenth, on a walk-off two-run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his home run was the second of the four.\nOn September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8\u20136. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians. The Indians returned the favor in Game One of the 2016 American League Division Series.\nTwice in MLB history, two brothers have hit back-to-back home runs. On April 23, 2013, brothers Melvin Upton Jr. (formerly B.J. Upton) and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.\nSimple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a home run, he might have his concentration broken and might alter his normal approach in an attempt to \"make up for it\" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved \"Babe Ruth's called shot\" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.\nIn Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back home runs in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.\nAnother notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey Sr. and Ken Griffey Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.\nOn May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.\nOn June 19, 2012, Jos\u00e9 Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnaci\u00f3n for a lead change in each instance.\nOn July 23, 2017, Whit Merrifield, Jorge Bonifacio, and Eric Hosmer of the Kansas City Royals hit back-to-back-to-back home runs in the fourth inning against the Chicago White Sox. The Royals went on to win the game 5\u20134.\nOn June 20, 2018, George Springer, Alex Bregman, and Jose Altuve of the Houston Astros hit back-to-back-to-back home runs in the sixth inning against the Tampa Bay Rays. The Astros went on to win the game 5\u20131.\nOn April 3, 2018, the St. Louis Cardinals began the game against the Milwaukee Brewers with back-to-back home runs from Dexter Fowler and Tommy Pham. Then in the bottom of the ninth, with two outs and the Cardinals leading 4\u20133, Christian Yelich homered to tie the game; and Ryan Braun hit the next pitch for a walk-off home run. This is the only major league game to begin and end with back-to-back home runs.\nOn May 5, 2019, Eugenio Suarez, Jesse Winker and Derek Dietrich of the Cincinnati Reds, hit back-to-back-to-back home runs on three straight pitches against Jeff Samardzija of the San Francisco Giants in the bottom of the first inning.\nOn October 30, 2021, Dansby Swanson and Jorge Soler hit back-to-back home runs for the Atlanta Braves off Houston Astros pitcher Cristian Javier to give the Braves a 3\u20132 lead in the bottom of the seventh in Game 4 of the World Series.\nOn October 18, 2024, Aaron Judge and Giancarlo Stanton hit back-to-back quadruples to take the lead in a game of the American League Championship Series.\nOn March 29, 2025, New York Yankees batters Paul Goldschmidt, Cody Bellinger, and Aaron Judge became the first trio to hit back-to-back-to-back home runs on the first three pitches of a game. They accomplished this feat against Milwaukee Brewers pitcher Nestor Cortes.\nOn April 29, 2025, New York Yankees batters Trent Grisham, Aaron Judge, and Ben Rice hit back-to-back-to-back home runs in the top of the first inning off Baltimore Orioles pitcher Kyle Gibson. The Yankees became the first team to hit three consecutive homers to start a game twice in one season.\nConsecutive home runs by one batter.\nThe record for consecutive home runs by a batter under any circumstances is four. Of the sixteen players (through 2012) who have hit four in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.\nBases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, four in four games played, during September 17\u201322, 1957, for the Red Sox. Williams hit a pinch-hit home run on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had four walks interspersed among his four homers.\nIn World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at-bats, off three different pitchers (4th inning: Hooten; 5th inning: Sosa; 8th inning: Hough). He had also hit one in his last at-bat of the previous game, giving him four home runs on four consecutive swings. The four in a row set the record for consecutive homers across two Series games.\nIn Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series.\nNomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: three home runs in two innings, on July 23, 2002, for the Boston Red Sox.\nHome run cycle.\nAn offshoot of hitting for the cycle, a \"home run cycle\" is when a player hits a solo home run, two-run home run, three-run home run, and grand slam all in one game. This is an extremely rare feat, as it requires the batter not only to hit four home runs in the game but also to hit the home runs with a specific number of runners already on base. This is largely dependent on circumstances outside of the player's control, such as teammates' ability to get on base, and the order in which the player comes to bat in any particular inning. A further variant of the home run cycle would be the \"natural home run cycle\", should a batter hit the home runs in the specific order listed above.\nA home run cycle has never occurred in MLB, which has only had 21 instances of a player hitting four home runs in a game. Though multiple home run cycles have been recorded in collegiate baseball, there have been two known home run cycles in a professional baseball game: one belongs to Tyrone Horne, playing for the Arkansas Travelers in a Double-A level Minor League Baseball game against the San Antonio Missions on July 27, 1998, and the other was accomplished by Chandler Redmond of the Springfield Cardinals, of the Texas League in a game against the Amarillo Sod Poodles on August 10, 2022.\nMajor league players have come close to hitting a home run cycle, two notable examples being Scooter Gennett of the Cincinnati Reds on June 6, 2017, and Mark Whiten of the St. Louis Cardinals on September 7, 1993. Gennett hit four home runs against the St. Louis Cardinals. He hit a grand slam in the third inning, a two-run home run in the fourth inning, a solo home run in the sixth inning, and a two-run home run in the eighth inning. He had an opportunity for a three-run home run in the first inning but drove in one run with a single in that at-bat. \nWhiten hit a grand slam in the first inning, a three-run home run in both the sixth and seventh inning, and a two-run home run in the ninth inning. In doing so, he also tied the MLB record for total RBIs in a game with 12.\nOn Sept 8, 2024 Pavin Smith of the Arizona Diamondbacks hit 3 consecutive home runs versus the Houston Astros to have a shot at the Home Run Cycle. He hit a 3 run hr in the 2nd, a Grand Slam in the 3rd &amp; a solo HR in the 5th. He struck out with a man on base in his final at-bat.\nHistory.\nIn the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home \"run\" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century because in baseball's early days, a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called \"manufacturing runs\" or \"small ball\".\nThe home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly the prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the \"inside game\" to the \"power game\" in order to keep up.\nBefore , Major League Baseball considered a fair ball that bounced over an outfield fence to be a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became automatic doubles (often referred to as a ground rule double). The last \"bounce\" home run in MLB was hit by Al L\u00f3pez of the Brooklyn Robins on September 12, 1930, at Ebbets Field. A carryover of the old rule is that if a player deflects a ball over the outfield fence in fair territory without it touching the ground, it is a home run, per MLB rule 5.05(a)(9). Additionally, MLB rule 5.05(a)(5) still stipulates that a ball hit over a fence in fair territory that is less than from home plate \"shall entitle the batter to advance to second base only\", as some early ballparks had short dimensions.\nAlso until circa 1931, the ball had to go not only over the fence in fair territory, but it had to land in the bleachers in fair territory or still be visibly fair when disappearing from view. The rule stipulated \"fair when last seen\" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second \"foul pole\" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal \"touch down\" of the ball in the end zone but now only requires the \"breaking of the [vertical] plane\" of the goal line, in baseball the ball needs only \"break the plane\" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).\nBabe Ruth's 60th home run in 1927 was somewhat controversial because it landed barely in fair territory in the stands down the right field line. Ruth lost many home runs in his career due to the when-last-seen rule. Bill Jenkinson, in \"The Year Babe Ruth Hit 104 Home Runs\", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.\nFurther, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to \"force\" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.\nIn the 2020s, it has become increasingly popular for Major League teams to celebrate home runs using some sort of prop. For example, allowing the player to wear or hold an item, such as a hat, helmet, jacket, sword, or trident.\nRecords.\nMajor League Baseball keeps running totals of all-time home runs by the team, including teams no longer active (before 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in all of MLB history (according to the then-current definition of the major leagues, which excluded Negro league baseball) with a grand slam on September 8, 2008. Sheffield had hit the MLB's 249,999th home run against Gio Gonz\u00e1lez in his previous at-bat.\nThe all-time, verified professional baseball record for career home runs for one player, excluding the U.S. Negro leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.\nIn Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&amp;T Park off pitcher Mike Bacsik. Only eight other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Albert Pujols (703), Alex Rodriguez (696), Willie Mays (660), Ken Griffey Jr. (630), Jim Thome (612), and Sammy Sosa (609). Giancarlo Stanton holds the record for currently active MLB players with 453 as of the end of the 2025 season.\nThe single-season record is 73, set by Barry Bonds in 2001. Other notable single-season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, Aaron Judge, with 62 home runs in 2022, and Sammy Sosa and Mark McGwire, who hit 66 and 70 respectively, in 1998.\nNegro league slugger Josh Gibson's Baseball Hall of Fame plaque says he hit \"almost 800\" home runs in his career. The \"Guinness Book of World Records\" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, \"Baseball\", states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record-keeping in the Negro leagues. The 1993 edition of the MacMillan \"Baseball Encyclopedia\" attempted to compile a set of Negro league records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 \"official\" Negro League games they were able to account for in his 17-year career, about one home run every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or one home run every 3.5 games. The large gap in the numbers for Gibson reflects the fact that Negro League clubs played relatively far fewer league games and many more \"barnstorming\" or exhibition games during the course of a season, than did the major league clubs of that era.\nOther legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit \"the longest home run ever\" at an estimated distance of , although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season three times), Ken Griffey Jr. and Eddie Mathews. In 1987, Joey Meyer of the minor league Denver Zephyrs hit the longest verifiable home run in professional baseball history. The home run was measured at a distance of and was hit inside Denver's Mile High Stadium. On May 6, 1964, Chicago White Sox outfielder Dave Nicholson hit a home run officially measured at 573 feet that either bounced atop the left-field roof of Comiskey Park or entirely cleared it. Major League Baseball's longest verifiable home run distance is about , by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.\nThe location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The spot sits outside American Family Field, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th home run landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured home run in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.\nMay 2019 saw 1,135 MLB home runs, the highest ever number of home runs in a single month in Major League Baseball history. During this month, 44.5% of all runs scored were the result of a home run, breaking the previous record of 42.3%.\nIn postseason play, the most home runs hit by a player for a career is Manny Ramirez, who hit 29. Jose Altuve (23), Bernie Williams (22), Derek Jeter (20), and Kyle Schwarber (20) are the only other players to hit twenty postseason home runs. Rounding out the top ten as of the end of the 2021 season are Albert Pujols (19), George Springer (19), Carlos Correa (18), Reggie Jackson (18), Mickey Mantle (18, all in the World Series), and Nelson Cruz (18). As for most home runs in one postseason, Randy Arozarena holds the record with ten, done in the 2020 postseason.\nInstant replay.\nReplays \"to get the call right\" have been used extremely sporadically in the past, but the use of instant replay to determine \"boundary calls\"\u2014home runs and foul balls\u2014was not officially allowed until 2008.\nIn a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.\nIn November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:\nOn August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008, in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.\nAbout two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Pe\u00f1a of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Pe\u00f1a a home run.\nAside from the two aforementioned reviews at Tampa Bay, the replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.\nOn October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that hit a camera protruding over the wall and into the field of play in deep right field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14149", "revid": "22312364", "url": "https://en.wikipedia.org/wiki?curid=14149", "title": "Harappa", "text": "Archaeological site in Punjab, Pakistan\nHarappa () is an archaeological site in Punjab, Pakistan, about west of Sahiwal, that takes its name from a modern village near the former course of the Ravi River. The Ravi now runs to the north. \nThe city of Harappa is believed to have had as many as 23,500 residents and occupied about with clay brick houses at its greatest extent during the Mature Harappan phase (2600 BC \u2013 1900\u00a0BC), which is considered large for its time. \nThe ancient city of Harappa was heavily damaged under British rule when bricks from the ruins were used as track ballast to construct the Lahore\u2013Multan Railway. The current village of Harappa is less than from the ancient site. Although modern Harappa has a legacy railway station from the Raj period, it is a small crossroads town of 15,000 people today. In 2004, the site was added to the tentative list for UNESCO World Heritage Sites. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artefacts during the early stages of building work.\nHistory.\nThe Harappan Civilization has its earliest roots in cultures such as that of Mehrgarh, approximately 6000 BC. The two greatest cities, Mohenjo-daro and Harappa, emerged c.\u20092600 BC along the Indus River valley in Punjab and Sindh. The civilization, with a possible writing system, urban centres, drainage infrastructure and diversified social and economic system, was rediscovered in the 1920s also after excavations at Mohenjo-daro in Sindh near Larkana, and Harappan cities, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in the East Punjab, to Gujarat in the southeast, and Balochistan, Pakistan in the southwest, have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railway used bricks from the ruins for track ballast, an abundance of artefacts has nevertheless been recovered.\nDue to falling sea-levels, certain regions were abandoned during the late Harappan period. In its later stages, the Harappan civilization lost features such as writing and hydraulic engineering. As a result, the Ganges Valley settlement gained prominence and Ganges' cities developed.\nThe earliest recognisably Harappan sites date to 3500 BC. This early phase lasts till around 2600 BC. The civilization's mature phase lasted from 2600 BC to 2000 BC. This is when the great cities were at their height. Then, from around 2000 BC, there was a steady disintegration that lasted till 1400 BC \u2013 what is usually called Late Harappan. There is no sign that the Harappan cities were laid waste by invaders. The evidence strongly points to natural causes. A number of studies show that the area which is today the Thar Desert was once far wetter and that the climate gradually became drier.\nCulture and economy.\nThe Indus Valley civilization was basically an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Elam and Sumer in southern Mesopotamia. Both Mohenjo-daro and Harappa are generally characterised as having \"differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers.\" Although such similarities have given rise to arguments for the existence of a standardised system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.\nThe weights and measures of the Indus Valley Civilisation, on the other hand, were highly standardised, and conformed to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for the identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. \"Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, was domesticated,\" as well as \"fowl for fighting\". Wheel-made pottery\u2014some of it adorned with animal and geometric motifs\u2014has been found in profusion at all the major Indus sites. A centralised administration for each city, though not the whole civilisation, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy. Harappans had many trade routes along the Indus River that went as far as the Persian Gulf, Mesopotamia, and Egypt. Some of the most valuable things traded were carnelian and lapis lazuli.\nWhat is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Examinations of Harappan skeletons have often found wounds that are likely to have been inflicted in battle. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (an ossuary located south-east of the city walls). Furthermore, rates of craniofacial trauma and infection increased through time demonstrating that the civilisation collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety.\nTrade.\nThe Harappans had traded with ancient Mesopotamia, especially Elam, among other areas. Cotton textiles and agricultural products were the primary trading objects. The Harappan merchants also had procurement colonies in Mesopotamia as well, which served as trading centres. They also traded extensively with people living in southern India, near modern-day Karnataka, to procure gold and copper from them.\nArchaeology.\nSignificance.\nHarappa is the type site of the Bronze Age Indus Valley Civilisation (\"IVC\"), as it was the first IVC site to be excavated by the Archaeological Survey of India during the British Raj, although its significance did not become manifest until the discovery of Mohenjo-daro in Sindh, Pakistan, some years later. For this reason, IVC is sometimes called the \"Harappan civilisation,\" a term more commonly used by the Archaeological Survey of India after decolonization in 1947. The discovery of Harappa and, soon afterwards, Mohenjo-Daro, two major urban IVC settlements, was the culmination of work that had begun after the founding of the Archaeological Survey of India in 1861.\nChronology.\nThe excavators of the site have proposed the following chronology of Harappa's occupation:\nPeriod 1 occupation was thought to be around 7 to 10 hectares, but following excavations and findings of pottery in Mound E, along with previously found Mound AB pottery, suggest Ravi/Hakra phase would have been extended, together in both mounds, to 25 hectares.\nPeriod 2, Kot Diji phase, was extended in the same two mounds, AB and E, covering over 27 hectares.\nIn Period 3, Harappa phase, the settlement reached 150 hectares.\nBy far the most exquisite and obscure artefacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world and the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascribing of Indus Valley Civilisation iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence for such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area.\nThe area of the late Harappan period consisted of the areas of the Daimabad, Maharashtra, and Badakshan regions of Afghanistan. The area covered by this civilisation would have been very large with a distance of around .\nSymbols similar to the Indus script.\nClay and stone tablets unearthed at Harappa, which were carbon-dated 3300\u20133200\u00a0BC, contain trident-shaped and plant-like markings. \"It is a big question as to if we can call what we have found true writing, but we have found symbols that have similarities to what became Indus script\", said Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. These primitive symbols are placed slightly earlier than the primitive writing of the Sumerians of Mesopotamia, dated c.3100\u00a0BC. These markings have similarities to what later became Indus Script which has not been completely deciphered yet.\nIn February 2006, a school teacher in the village of Sembian-Kandiyur in Tamil Nadu, discovered a stone celt (tool) with an inscription estimated to be up to 3,500 years old. Indian epigraphist Iravatham Mahadevan postulated that the four signs were in the Indus script and called the find \"the greatest archaeological discovery of a century in Tamil Nadu\". Based on this evidence, he went on to suggest that the language used in the Indus Valley was of Dravidian origin. However, the absence of a Bronze Age in South India, contrasted with the knowledge of bronze making techniques in the Indus Valley cultures, calls into question the validity of this hypothesis.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14151", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=14151", "title": "Hate site", "text": ""}
{"id": "14153", "revid": "19225282", "url": "https://en.wikipedia.org/wiki?curid=14153", "title": "Hendecasyllable", "text": "Poetic line of eleven syllables\nIn poetry, a hendecasyllable (as an adjective, hendecasyllabic) is a line of eleven syllables. The term may refer to several different poetic meters, the older of which are quantitative and used chiefly in classical (Ancient Greek and Latin) poetry, and the newer of which are syllabic or accentual-syllabic and used in medieval and modern poetry.\nClassical.\nIn classical poetry, \"hendecasyllable\" or \"hendecasyllabic\" may refer to any of three distinct 11-syllable Aeolic meters, used first in Ancient Greece and later, with little modification, by Roman poets.\nAeolic meters are characterized by an Aeolic base codice_1 followed by a choriamb codice_2; where codice_3 = a long syllable, codice_4 = a short syllable, and codice_5 = an anceps, that is, a syllable either long or short. The three Aeolic hendecasyllables (with base and choriamb in bold) are:\nPhalaecian hendecasyllable.\n \u00d7 \u00d7 \u2013 u u \u2013 u \u2013 u \u2013 \u2013\nThis line is named after Phalaecus, a minor Hellenistic poet who used it in epigrams; though he did not invent it, since it had earlier been used by Sappho and Anacreon. \nThe Phalaecian hendecasyllable was a favorite of Catullus; it was also very frequently used by Martial. An example from Catullus is the first poem in his collection, given below. The translation attempts to convey some sense of the rhythm, substituting English stress for Latin length.\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Catullus: \"Catullus 1\", lines 1-4\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nThe aeolic base (i.e., the first two syllables of the line) with codice_6 is by far the most common in Catullus, and in later poets such as Statius and Martial was the only one used, but occasionally Catullus uses codice_7 or codice_8 as in lines 2 and 4 above. There is usually a caesura in the line after the 5th or 6th syllable.\nIn the first part of his poetry collection, Catullus uses the Phalaecian hendecasyllable as given above in 41 poems. In addition, in two of his poems (55 and 58b) Catullus uses a variation of the metre, in which the 4th and 5th syllables can sometimes be contracted into a single long syllable. In poem 55 there are twelve decasyllables and ten normal lines:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Catullus: \"Catullus 55\", lines 1-2\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nPoem 58b is thought by some scholars to be a fragment which was formerly part of this one; although others think it an independent poem.\nAlcaic hendecasyllable.\n \u00d7 \u2013 u \u2013 \u00d7 \u2013 u u \u2013 u \u2013\nHere the Aeolic base is truncated to a single anceps. This meter typically appears as the first two lines of an Alcaic stanza. (For an English example, see \u00a7English, below.)\nSapphic hendecasyllable.\n \u2013 u \u2013 \u00d7 \u2013 u u \u2013 u \u2013 \u2013\nAgain, the Aeolic base is truncated. This meter typically appears as the first three lines of a Sapphic stanza, though it was also sometimes used in stichic verse, for example by Seneca and Boethius. Sappho wrote many of the stanzas subsequently named after her, for example (with formal equivalent, substituting English stress for Greek length):\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Sappho: Fragment 31, lines 1-4\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nItalian.\nThe hendecasyllable () is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called \"endecasillabo a minore\", or lesser hendecasyllable, and has the first hemistich equivalent to a \"quinario\"; the second is called \"endecasillabo a maiore\", or greater hendecasyllable, and has a \"settenario\" as the first hemistich.\nThere is a strong tendency for hendecasyllabic lines to end with feminine rhymes (causing the total number of syllables to be eleven, hence the name), but ten-syllable lines (\"Ci\u00f2 che 'n grembo a Benaco star non pu\u00f2\") and twelve-syllable lines (\"Ergasto mio, perch\u00e9 solingo e tacito\") are encountered as well. Lines of ten or twelve syllables are more common in rhymed verse; \"versi sciolti\", which rely more heavily on a pleasant rhythm for effect, tend toward a stricter eleven-syllable format. As a novelty, lines longer than twelve syllables can be created by the use of certain verb forms and affixed enclitic pronouns (\"Ottima \u00e8 l'acqua; ma le piante abbeverinosene.\").\nAdditional accents beyond the two mandatory ones provide rhythmic variation and allow the poet to express thematic effects. A line in which accents fall consistently on even-numbered syllables (\"Al c\u00f2r gent\u00ecl remp\u00e0ira s\u00e8mpre am\u00f3re\") is called iambic (\"giambico\") and may be a greater or lesser hendecasyllable. This line is the simplest, commonest and most musical but may become repetitive, especially in longer works. Lesser hendecasyllables often have an accent on the seventh syllable (\"f\u00e0tta di gi\u00f2co in fig\u00f9ra d'am\u00f3re\"). Such a line is called dactylic (\"dattilico\") and its less pronounced rhythm is considered particularly appropriate for representing dialogue. Another kind of greater hendecasyllable has an accent on the third syllable (\"Se Merc\u00e9 fosse am\u00ecca a' miei dis\u00ecri\") and is known as anapestic (\"anapestico\"). This sort of line has a crescendo effect and gives the poem a sense of speed and fluidity.\nIt is considered improper for the lesser hendecasyllable to use a word accented on its antepenultimate syllable (\"parola sdrucciola\") for its mid-line stress. A line like \"Pi\u00f9 non sfav\u00ecllano quegli \u00f2cchi n\u00e9ri\", which delays the caesura until after the sixth syllable, is not considered a valid hendecasyllable.\nMost classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme systems used include terza rima, ottava, sonnet and canzone, and some verse forms use a mixture of hendecasyllables and shorter lines. From the early 16th century onward, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. This is known as \"verso sciolto\". An early example is \"Le Api\" (\"the bees\") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525 (with formal equivalent paraphrase which mirrors the original's syllabic counts, varied caesurae, and line- and hemistich-final stress profiles):\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Rucellai: \"Le Api\", lines 1-11\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\u2014adapted from Leigh Hunt's blank verse translation\nLike other early Italian-language tragedies, the \"Sophonisba\" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the \"Canti\" of Giacomo Leopardi, where hendecasyllables are alternated with \"settenari\".\nPolish.\nThe hendecasyllabic metre () was very popular in Polish poetry, especially in the seventeenth and eighteenth centuries, owing to strong Italian literary influence. It was used by Jan Kochanowski, Piotr Kochanowski (who translated \"Jerusalem Delivered\" by Torquato Tasso), Sebastian Grabowiecki, Wespazjan Kochowski and Stanis\u0142aw Herakliusz Lubomirski. The greatest Polish Romantic poet, Adam Mickiewicz, set his poem Gra\u017cyna in this measure. The Polish hendecasyllable is widely used when translating English blank verse.\nThe eleven-syllable line is normally a line of 5+6 syllables with medial caesura, primary stresses on the fourth and tenth syllables, and feminine endings on both half-lines. Although the form can accommodate a fully iambic line, there is no such tendency in practice, word stresses falling variously on any of the initial syllables of each half-line.\n o o o S s | o o o o S s\n o=any syllable, S=stressed syllable, s=unstressed syllable\nA popular form of Polish literature that employs the hendecasyllable is the Sapphic stanza: 11/11/11/5.\nThe Polish hendecasyllable is often combined with an 8-syllable line: 11a/8b/11a/8b. Such a stanza was used by Mickiewicz in his ballads, as in the following example (with formal equivalent paraphrase):\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Adam Mickiewicz: \"\u015awite\u017a\", lines 1-4\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nPortuguese.\nThe hendecasyllable () is a common meter in Portuguese poetry. The best-known Portuguese poem composed in hendecasyllables is Lu\u00eds de Cam\u00f5es' \"Lusiads\", which begins as follows:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Cam\u00f5es: \"Os Lus\u00edadas\", Canto I, lines 1-8\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\u2014trans. Sir Richard Fanshawe\nIn Portuguese, the hendecasyllable meter is often called \"decasyllable\" (\"decass\u00edlabo\"), even when the work in question uses overwhelmingly feminine rhymes (as is the case with the \"Lusiads\"). This is due to Portuguese prosody considering verses to end at the last stressed syllable, thus the aforementioned verses are effectively decasyllabic according to Portuguese scansion.\nSpanish.\nThe hendecasyllable () is less pervasive in Spanish poetry than in Italian or Portuguese, but it is commonly used with Italianate verse forms like sonnets and ottava rima (as found, for example, in Alonso de Ercilla's epic \"La Araucana\").\nSpanish dramatists often use hendecasyllables in tandem with shorter lines like heptasyllables, as can be seen in Rosaura's opening speech from Calder\u00f3n's \"La vida es sue\u00f1o\":\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014Calder\u00f3n: \"La vida es sue\u00f1o\" I.i.1-8 \n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\u2014trans. Denis Florence Mac-Carthy\nEnglish.\nThe term \"hendecasyllable\" most often refers to an imitation of Greek or Latin metrical lines, notably by Alfred Tennyson, Swinburne, and Robert Frost (\"For Once, Then, Something\"). Contemporary American poets Annie Finch (\"Lucid Waking\") and Patricia Smith (\"The Reemergence of the Noose\") have published recent examples. In English, which lacks phonemic length, poets typically substitute stressed syllables for \"long\", and unstressed syllables for \"short\". Tennyson, however, attempted to maintain the quantitative features of the meter (while supporting them with concurrent stress) in his Alcaic stanzas, the first two lines of which are Alcaic hendecasyllables:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\u2014\u200a\nOccasionally \"hendecasyllable\" is used to denote a line of iambic pentameter with a feminine ending, as in the first line of John Keats's \"Endymion\": \"A thing of beauty is a joy for ever\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14155", "revid": "45203278", "url": "https://en.wikipedia.org/wiki?curid=14155", "title": "Hebrides", "text": "Archipelago off the west coast of Scotland\nThe Hebrides ( ; , ; ) are the largest archipelago in the United Kingdom, off the west coast of the Scottish mainland. The islands fall into two main groups, based on their proximity to the mainland: the Inner and Outer Hebrides.\nThese islands have a long history of occupation (dating back to the Mesolithic period), and the culture of the inhabitants has been successively influenced by the cultures of Celtic-speaking, Norse-speaking, and English-speaking peoples. This diversity is reflected in the various names given to the islands, which are derived from the different languages that have been spoken there at various points in their history.\nThe Hebrides are where much of Scottish Gaelic literature and Gaelic music has historically originated. Today, the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. The Hebrides have less biodiversity than mainland Scotland, but a significant number of seals and seabirds.\nThe islands have a combined area of , and, as of 2011[ [update]], a combined population of around 45,000.\nGeology, geography and climate.\nThe Hebrides have a diverse geology, ranging in age from Precambrian strata that are amongst the oldest rocks in Europe, to Paleogene igneous intrusions. Raised shore platforms in the Hebrides have been identified as strandflats, possibly formed during the Pliocene period and later modified by the Quaternary glaciations.\nThe Hebrides can be divided into two main groups, separated from one another by the Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides form a chain of more than 100 islands and small skerries located about west of mainland Scotland. Among them, 15 are inhabited. The main inhabited islands include Lewis and Harris, North Uist, Benbecula, South Uist, and Barra.\nA complication is that there are various descriptions of the scope of the Hebrides. The \"Collins Encyclopedia of Scotland\" describes the Inner Hebrides as lying \"east of the Minch\". This definition would encompass all offshore islands, including those that lie in the sea lochs, such as and , which might not ordinarily be described as \"Hebridean\". However, no formal definition exists.\nIn the past, the Outer Hebrides were often referred to as the \"Long Isle\" (). Today, they are also sometimes known as the \"Western Isles\", although this phrase can also be used to refer to the Hebrides in general.\nThe Hebrides have a cool, temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides, the average temperature is 6\u00a0\u00b0C (44\u00a0\u00b0F) in January and 14\u00a0\u00b0C (57\u00a0\u00b0F) in the summer. The average annual rainfall in Lewis is , and there are between 1,100 and 1,200 hours of sunshine \"per annum\" (13%). The summer days are relatively long, and May through August is the driest period.\nEtymology.\nThe earliest surviving written references to the islands were made circa 77\u00a0AD by Pliny the Elder in his \"Natural History\": He states that there are 30 ', and makes a separate reference to ', which Watson (1926) concluded refers unequivocally to the Outer Hebrides. About 80 years after Pliny the Elder, in 140\u2013150\u00a0AD, Ptolemy (drawing on accounts of the naval expeditions of ) writes that there are five ' (possibly meaning the Inner Hebrides) and '. Later texts in classical Latin, by writers such as , use the forms ' and '.\nThe name ' (used by Ptolemy) may be pre-Celtic. Ptolemy calls Islay \"\", and the use of the letter \"p\" suggests a Brythonic or Pictish tribal name, , because the root is not Gaelic. Woolf (2012) has suggested that ' may be \"an Irish attempt to reproduce the word ' phonetically, rather than by translating it\", and that the tribe's name may come from the root ', meaning \"horse\". Watson (1926) also notes a possible relationship between ' and the ancient Irish Ulaid tribal name ', and also the personal name of a king (recorded in the \"Silva Gadelica\").\nThe names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic, but the roots of several other names for Hebrides islands may have a pre-Celtic origin. Adomn\u00e1n, a 7th-century abbot of Iona, records Colonsay as \"Colosus\" and Tiree as \"Ethica\", and both of these may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is \"\" in Old Norse. Various suggestions have been made as to possible meanings of the name in Norse (for example, \"song house\"), but the name is not of Gaelic origin, and the Norse provenance is questionable.\nThe earliest comprehensive written list of Hebridean island names was compiled by Donald Monro in 1549. This list also provides the earliest written reference to the names of some of the islands.\nThe derivations of all the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.\nOuter Hebrides.\nLewis and Harris is the largest island in Scotland and the third largest of the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. The island does not have a single common name in either English or Gaelic and is referred to as \"Lewis and Harris\", \"Lewis with Harris\", \"Harris with Lewis\" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, \"Erimon\" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the ( \"desert\". The origin of Uist () is similarly unclear.\nInner Hebrides.\nThere are various examples of earlier names for Inner Hebridean islands that were Gaelic, but these names have since been completely replaced. For example, Adomn\u00e1n records \"Sainea\", \"Elena\", \"Ommon\" and \"Oideacha\" in the Inner Hebrides. These names presumably passed out of usage in the Norse era, and the locations of the islands they refer to are not clear. As an example of the complexity: Rona may originally have had a Celtic name, then later a similar-sounding Norse name, and then still later a name that was essentially Gaelic again, but with a Norse \"\u00f8y\" or \"ey\" ending. (See Rona, below.)\nUninhabited islands.\nThe names of uninhabited islands follow the same general patterns as the inhabited islands. (See the list, below, of the ten largest islands in the Hebrides and their outliers.)\nThe etymology of the name \"St Kilda\", a small archipelago west of the Outer Hebrides, and the name of its main island, \"Hirta,\" is very complex. No saint is known by the name of Kilda, so various other theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name \"St Kilda\" first appears on a Dutch map dated 1666, and that it may derive from the Norse phrase ' (\"sweet wellwater\") or from a mistaken Dutch assumption that the spring ' was dedicated to a saint. (' is a tautological placename, consisting of the Gaelic and Norse words for \"well\", i.e., \"well well\"). Similarly unclear is the origin of the Gaelic for \"Hirta\", ', ', or ' a name for the island that long pre-dates the name \"St Kilda\". Watson (1926) suggests that it may derive from the Old Irish word ' (\"death\"), possibly a reference to the often lethally dangerous surrounding sea. Maclean (1977) notes that an Icelandic saga about an early 13th-century voyage to Ireland refers to \"the islands of '\", which means \"stags\" in Norse, and suggests that the outline of the island of Hirta resembles the shape of a stag, speculating that therefore the name \"Hirta\" may be a reference to the island's shape.\nThe etymology of the names of small islands may be no less complex and elusive. In relation to , Robert Louis Stevenson believed that \"black and dismal\" was one translation of the name, noting that \"as usual, in Gaelic, it is not the only one.\"\nHistory.\nPrehistory.\nThe Hebrides were settled during the Mesolithic era around 6500\u00a0BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on is dated to 8590 \u00b195 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.\nCeltic era.\nIn 55\u00a0BC, the Greek historian Diodorus Siculus wrote that there was an island called \"Hyperborea\" (which means \"beyond the North Wind\"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.\nA traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before 83\u00a0AD. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.\nThe first written records of native life begin in the 6th century AD, when the founding of the kingdom of D\u00e1l Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of D\u00e1l Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cen\u00e9l Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.\nNorth of D\u00e1l Riata, the Inner and Outer Hebrides were nominally under Pictish control, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: \"As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.\"\nNorwegian control.\nViking raids began on Scottish shores towards the end of the 8th century, and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis \"fire played high in the heaven\" as \"flame spouted from the houses\" and that in the Uists \"the king dyed his sword red in blood\".\nThe Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Gael kinsman of the Manx royal house.\nFollowing the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.\nScottish control.\nAs the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.\nThe Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.\nIn 1598, King James VI authorised some \"Gentleman Adventurers\" from Fife to civilise the \"most barbarous Isle of Lewis\". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on in . The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.\nEarly British era.\nWith the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen \"came out\" in support of the Jacobite Earl of Mar in the 1715 and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but over the following century the clan system was broken up and islands of the Hebrides became a series of landed estates.\nThe early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Clachan Bridge improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the Clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic.\nAs , a Gaelic poet from South Uist, wrote for his countrymen who were obliged to leave the Hebrides in the late 18th century, emigration was the only alternative to \"sinking into slavery\" as the Gaels had been unfairly dispossessed by rapacious landlords. In the 1880s, the \"Battle of the Braes\" involved a demonstration against unfair land regulation and eviction, stimulating the calling of the Napier Commission. Disturbances continued until the passing of the 1886 Crofters' Act.\nLanguage.\nThe residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.\nIt is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived from Ireland due to the growing influence of the kingdom of D\u00e1l Riata from the 6th century AD onwards, and became the dominant language of the southern Hebrides at that time. For a few centuries, the military might of the ' meant that Old Norse was prevalent in the Hebrides. North of , the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was ', which means \"Southern Isles\"; in contrast to the \"\", or \"Northern Isles\" of Orkney and Shetland.\nSouth of , Gaelic place names are more common, and after the 13th century, Gaelic became the main language of the entire Hebridean archipelago. Due to Scots and English being favoured in government and the educational system, the Hebrides have been in a state of diglossia since at least the 17th century. The Highland Clearances of the 19th century accelerated the language shift away from Scottish Gaelic, as did increased migration and the continuing lower status of Gaelic speakers. Nevertheless, as late as the end of the 19th century, there were significant populations of monolingual Gaelic speakers, and the Hebrides still contain the highest percentages of Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where a slim majority speak the language. The Scottish Gaelic college, , is based on Skye and Islay.\nIronically, given the status of the Western Isles as the last Gaelic-speaking stronghold in Scotland, the Gaelic language name for the islands \u2013 \"\" \u2013 means \"isles of the foreigners\"; from the time when they were under Norse colonisation.\nModern economy.\nFor those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless, emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.\nThere were, however, continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.\nThe widespread immigration of mainlanders, particularly non-Gaelic speakers, has been a subject of controversy.\nAgriculture practised by crofters remained popular in the 21st century in the Hebrides; crofters own a small property but often share a large common grazing area. Various types of funding are available to crofters to help supplement their incomes, including the \"Basic Payment Scheme, the suckler beef support scheme, the upland sheep support scheme and the Less Favoured Area support scheme\". One reliable source discussed the Crofting Agricultural Grant Scheme (CAGS) in March 2020:the scheme \"pays up to \u00a325,000 per claim in any two-year period, covering 80% of investment costs for those who are under 41 and have had their croft less than five years. Older, more established crofters can get 60% grants\".\nMedia and the arts.\nMusic.\nMany contemporary Gaelic musicians have roots in the Hebrides, including vocalist and multi-instrumentalist Julie Fowlis (North Uist), Catherine-Ann MacPhee (Barra), Kathleen MacInnes of the band Capercaillie (South Uist), and Ishbel MacAskill (Lewis). All of these singers have composed their own music in Scottish Gaelic, with much of their repertoire stemming from Hebridean vocal traditions, such as ' (\"mouth music\", similar to Irish lilting) and ' (waulking songs). This tradition includes many songs composed by little-known or anonymous poets, well-before the 1800s, such as \", \", \" and \". Several of Runrig's songs are inspired by the archipelago; Calum and were raised on North Uist and Donnie Munro on Skye.\nLiterature.\nThe Gaelic poet spent much of his life in the Hebrides and often referred to them in his poetry, including in ' and '. The best known Gaelic poet of her era, (Mary MacPherson, 1821\u201398), embodied the spirit of the land agitation of the 1870s and 1880s. This, and her powerful evocation of the Hebrides\u2014she was from Skye\u2014has made her among the most enduring Gaelic poets. Allan MacDonald (1859\u20131905), who spent his adult life on Eriskay and South Uist, composed hymns and verse in honour of the Blessed Virgin, the Christ Child, and the Eucharist. In his secular poetry, MacDonald praised the beauty of Eriskay and its people. In his verse drama, ' (\"The Old Wives' Parliament\"), he lampooned the gossiping of his female parishioners and local marriage customs.\nIn the 20th century, Murdo Macfarlane of Lewis wrote ', a well-known poem about the Gaelic revival in the Outer Hebrides. Sorley MacLean, the most respected 20th-century Gaelic writer, was born and raised on Raasay, where he set his best known poem, ', about the devastating effect of the Highland Clearances. , raised on South Uist and described by MacLean as \"one of the few really significant living poets in Scotland, writing in any language\" (West Highland Free Press, October 1992) wrote the Scottish Gaelic-language novel \"\" which was voted in the Top Ten of the 100 Best-Ever Books from Scotland.\nVirginia Woolf's \"To The Lighthouse\" is set on the Isle of Skye, part of the Inner Hebrides.\nNatural history.\nIn some respects the Hebrides lack biodiversity in comparison to mainland Britain; for example, there are only half as many mammalian species. However, these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The latter was re-introduced to R\u00f9m in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.\nRed deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland. Colonies of seals are found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, orcas, basking sharks, porpoises and dolphins are among the sealife that can be seen.\nHeather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.\nLoch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.\nHedgehogs are not native to the Outer Hebrides\u2014they were introduced in the 1970s to reduce garden pests\u2014and their spread poses a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 due to protests. Trapped animals were relocated to the mainland.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences and footnotes.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "14157", "revid": "206898667", "url": "https://en.wikipedia.org/wiki?curid=14157", "title": "Human rights law", "text": ""}
{"id": "14158", "revid": "21054272", "url": "https://en.wikipedia.org/wiki?curid=14158", "title": "HMS Dreadnought", "text": "Several ships and one submarine of the Royal Navy have borne the name HMS \"Dreadnought\" in the expectation that they would \"dread nought\", i.e. \"fear nothing\". The 1906 ship, which revolutionized battleship design, became one of the Royal Navy's most famous vessels; battleships built after her were referred to as \"dreadnoughts\", and earlier battleships became known as pre-dreadnoughts.\nBattle honours.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n List of ships with the same or similar names\nThis article includes a with the same or similar names. If an [ internal link] for a specific ship led you here, you may wish to change the link to point directly to the intended ship article, if one exists."}
{"id": "14159", "revid": "29937276", "url": "https://en.wikipedia.org/wiki?curid=14159", "title": "Hartmann Schedel", "text": "German historian, cartographer, physician and humanist (1440\u20131514)\nHartmann Schedel (13 February 1440 \u2013 28 November 1514) was a German historian, physician, humanist, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. \nSchedel is best known for his writing the text for the \"Nuremberg Chronicle\", known as \"Schedelsche Weltchronik\" (English: \"Schedel's World Chronicle\"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446\u20131520) and Sebastian Kammermeister (1446\u20131503). Maps in the \"Chronicle\" were the first ever illustrations of many cities and countries.\nWith the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books had previously been rare and very expensive.\nSchedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14160", "revid": "1292102636", "url": "https://en.wikipedia.org/wiki?curid=14160", "title": "Hexameter", "text": "Metrical line of verses consisting of six feet\nHexameter is a metrical line of verses consisting of six feet (a \"foot\" here is the pulse, or major accent, of words in an English line of poetry; in Greek as well as in Latin a \"foot\" is not an accent, but describes various combinations of syllables). It was the standard epic metre in classical Greek and Latin literature, such as in the \"Iliad\", \"Odyssey\" and \"Aeneid\". Its use in other genres of composition include Horace's satires, Ovid's \"Metamorphoses,\" and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by Phemonoe, daughter of Apollo and the first Pythia of Delphi.\nClassical hexameter.\nIn classical hexameter, the six feet follow these rules:\nA short syllable (\u222a) is a syllable with a short vowel and no consonant at the end. A long syllable (\u2014) is a syllable that either has a long vowel, one or more consonants at the end (or a long consonant), or both. Spaces between words are not counted in syllabification, so for instance \"cat\" is a long syllable (\u2014) if said in isolation, but \"cat attack\" in combination would be syllabified as short-short-long: \"\"ca\", \"ta\", \"tack\"\" (\u222a\u222a \u2014).\nVariations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.\nApplication.\nAlthough the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) include Ancient Greek, Latin, Lithuanian and Hungarian.\nWhile the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's \"Poly-Olbion\" (1612) in couplets of iambic hexameter. An example from Drayton (marking the six feet on each line):\nNor a/ny o/ther wold / like Cot/swold e/ver sped,\nSo rich / and fair / a vale / in for/tuning / to wed.\nIn the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.\nSeveral attempts were made in the 19th century to naturalise the dactylic hexameter to English \u2014 by Henry Wadsworth Longfellow, Arthur Hugh Clough, and others \u2014 none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.\nIn the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem \"\"Metai\" (The Seasons)\" is considered the most successful hexameter text in Lithuanian as yet.\nFor dactylic hexameter poetry in Hungarian language, see Dactylic hexameter#In Hungarian.\nAlbert Meyer (1893\u20131962) used a natural form of hexameter in his translation of some verses from Homer's \"Odyssey\" into the Swiss dialect of Bern.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14161", "revid": "1808194", "url": "https://en.wikipedia.org/wiki?curid=14161", "title": "Holy Hand Grenade of Antioch", "text": ""}
{"id": "14162", "revid": "49521346", "url": "https://en.wikipedia.org/wiki?curid=14162", "title": "Timeline of Polish history", "text": "This is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of prime ministers of Poland.\n Centuries: 5th\u00a0\u00b7 6th\u00a0\u00b7 7th\u00a0\u00b7 8th\u00a0\u00b7 9th\u00a0\u00b7 10th\u00a0\u00b7 11th\u00a0\u00b7 12th\u00a0\u00b7 13th\u00a0\u00b7 14th\u00a0\u00b7 15th\u00a0\u00b7 16th\u00a0\u00b7 17th\u00a0\u00b7 18th\u00a0\u00b7 19th\u00a0\u00b7 20th\u00a0\u00b7 21st\u00a0\u00b7 See also\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "14163", "revid": "21327649", "url": "https://en.wikipedia.org/wiki?curid=14163", "title": "Hamitic languages", "text": ""}
{"id": "14168", "revid": "26074453", "url": "https://en.wikipedia.org/wiki?curid=14168", "title": "Himalia", "text": "Himalia may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
