{"id": "9588", "revid": "12530063", "url": "https://en.wikipedia.org/wiki?curid=9588", "title": "Extraterrestrial life", "text": "Life that does not originate on Earth\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in astronomy\nCould life have arisen elsewhere?What are the requirements for life?Are there exoplanets like Earth?How likely is the evolution of intelligent life?\nMore unsolved problems in astronomy\nExtraterrestrial life, or alien life (colloquially, aliens), is life that originates from another world rather than on Earth. No extraterrestrial life has yet been scientifically or conclusively detected. Such life might range from simple forms such as prokaryotes to intelligent beings, possibly bringing forth civilizations that might be far more, or far less, advanced than humans. The Drake equation speculates about the existence of sapient life elsewhere in the universe. The science of extraterrestrial life is known as astrobiology.\nSpeculation about inhabited worlds beyond Earth dates back to antiquity. Early Christian writers, including Augustine, discussed ideas from thinkers like Democritus and Epicurus about countless worlds in the vast universe. Pre-modern writers typically assumed extraterrestrial \"worlds\" were inhabited by living beings. William Vorilong, in the 15th century, acknowledged the possibility Jesus could have visited extraterrestrial worlds to redeem their inhabitants. In 1440, Nicholas of Cusa suggested Earth is a \u201cbrilliant star\u201d; he theorized that all celestial bodies, even the Sun, could host life. Descartes wrote that there were no means to prove the stars were not inhabited by \"intelligent creatures\", but their existence was a matter of speculation.\nIn comparison to the life-abundant Earth, the vast majority of intrasolar and extrasolar planets and moons have harsh surface conditions and disparate atmospheric chemistry, or lack an atmosphere. However, there are many extreme and chemically harsh ecosystems on Earth that do support forms of life and are often hypothesized to be the origin of life on Earth. Examples include life surrounding hydrothermal vents, acidic hot springs, and volcanic lakes, as well as halophiles and the deep biosphere.\nSince the mid-20th century, researchers have searched for extraterrestrial life and intelligence. Solar system studies focus on Venus, Mars, Europa, and Titan, while exoplanet discoveries now total 6,022 confirmed planets in 4,490 systems as of October 2025. Depending on the category of search, methods range from analysis of telescope and specimen data to radios used to detect and transmit interstellar communication. Interstellar travel remains largely hypothetical, with only the \"Voyager 1\" and \"Voyager 2\" probes confirmed to have entered the interstellar medium. The concept of extraterrestrial life, especially intelligent life, has greatly influenced culture and fiction. A key debate centers on contacting extraterrestrial intelligence: some advocate active attempts, while others warn it could be risky, given humanity\u2019s history of exploiting less advanced societies.\nContext.\nInitially, after the Big Bang, the universe was too hot to allow life. It is estimated that the temperature of the universe was around 10 billion Kelvin at the one-second mark. Roughly 15 million years later, it cooled to temperate levels, though the elements of organic life were yet nonexistent. The only freely available elements at that point were hydrogen and helium. Carbon and oxygen (and later, water) would not appear until 50 million years later, created through stellar fusion. At that point, the difficulty for life to appear was not the temperature, but the scarcity of free heavy elements. Planetary systems emerged, and the first organic compounds may have formed in the protoplanetary disk of dust grains that would eventually create rocky planets like Earth. Although Earth was in a molten state after its birth and may have burned any organics that fell on it, it would have been more receptive once it cooled down. Once the right conditions on Earth were met, life started by a chemical process known as abiogenesis. Alternatively, life may have formed less frequently, then spread\u2014by meteoroids, for example\u2014between habitable planets in a process called panspermia.\nDuring most of its stellar evolution, stars combine hydrogen nuclei to make helium nuclei by stellar fusion, and the comparatively lighter weight of helium allows the star to release the extra energy. The process continues until the star uses all of its available fuel, with the speed of consumption being related to the size of the star. During its last stages, stars start combining helium nuclei to form carbon nuclei. The larger stars can further combine carbon nuclei to create oxygen and silicon, oxygen into neon and sulfur, and so on until iron. Ultimately, the star blows much of its content back into the stellar medium, where it would join clouds that would eventually become new generations of stars and planets. Many of those materials are the raw components of life on Earth. As this process takes place in all the universe, said materials are ubiquitous in the cosmos and not a rarity from the Solar System.\nEarth is a planet in the Solar System, a planetary system formed by a star at the center, the Sun, and the objects that orbit it: other planets, moons, asteroids, and comets. The sun is part of the Milky Way, a galaxy. The Milky Way is part of the Local Group, a galaxy group that is in turn part of the Laniakea Supercluster. The universe is composed of all similar structures in existence. The immense distances between celestial objects are a difficulty for studying extraterrestrial life. So far, humans have only set foot on the Moon and sent robotic probes to other planets and moons in the Solar System. Although probes can withstand conditions that may be lethal to humans, the distances cause time delays: the \"New Horizons\" took nine years after launch to reach Pluto. No probe has ever reached extrasolar planetary systems. The \"Voyager 2\" left the Solar System at a speed of 50,000 kilometers per hour; if it headed towards the Alpha Centauri system, the closest one to Earth at 4.4 light years, it would reach it in 100,000 years. Under current technology, such systems can only be studied by telescopes, which have limitations. It is estimated that dark matter has a larger amount of combined matter than stars and gas clouds, but as it plays no role in the stellar evolution of stars and planets, it is usually not taken into account by astrobiology. \nThere is an area around a star, the circumstellar habitable zone or \"Goldilocks zone\", wherein water may be at the right temperature to exist in liquid form at a planetary surface. This area is neither too close to the star, where water would become steam, nor too far away, where water would be frozen as ice. However, although useful as an approximation, planetary habitability is complex and defined by several factors. Being in the habitable zone is not enough for a planet to be habitable, not even to actually have such liquid water. Venus is located in the solar system's habitable zone, but does not have liquid water because of the conditions of its atmosphere. Jovian planets or gas giants are not considered habitable even if they orbit close enough to their stars as hot Jupiters, due to crushing atmospheric pressures. The actual distances for the habitable zones vary according to the type of star, and even the solar activity of each specific star influences the local habitability. The type of star also defines the time the habitable zone will exist, as its presence and limits will change along with the star's stellar evolution.\nThe Big Bang occurred 13.8 billion years ago, the Solar System was formed 4.6 billion years ago, and the first hominids appeared 6 million years ago. Life on other planets may have started, evolved, given birth to extraterrestrial intelligences, and perhaps even faced a planetary extinction event millions or billions of years ago. When considered from a cosmic perspective, the brief times of existence of Earth's species may suggest that extraterrestrial life may be equally fleeting under such a scale.\nDuring a period of about 7 million years, from about 10 to 17 million years after the Big Bang, the background temperature was between , allowing the possibility of liquid water if any planets existed. Avi Loeb (2014) speculated that primitive life might in principle have appeared during this window, which he called \"the Habitable Epoch of the Early Universe\".\nLife on Earth is quite ubiquitous across the planet and has adapted over time to almost all the available environments in it, extremophiles and the deep biosphere thrive at even the most hostile ones. As a result, it is inferred that life in other celestial bodies may be equally adaptive. However, the origin of life is unrelated to its ease of adaptation and may have stricter requirements. A celestial body may not have any life on it, even if it were habitable.\nLikelihood of existence.\nIt is unclear if life, and more importantly, intelligent life in the cosmos is ubiquitous or rare. The hypothesis of ubiquitous extraterrestrial life relies on three main ideas. The first one, the size of the universe, allows for plenty of planets to have a similar habitability to Earth, and the age of the universe gives enough time for a long process analog to the history of Earth to happen there. The second is that the substances that make life, such as carbon and water, are ubiquitous in the universe. The third is that the physical laws are universal, which means that the forces that would facilitate or prevent the existence of life would be the same ones as on Earth. According to this argument, made by scientists such as Carl Sagan and Stephen Hawking, it would be improbable for life \"not\" to exist somewhere else other than Earth. This argument is embodied in the Copernican principle, which states that Earth does not occupy a unique position in the Universe, and the mediocrity principle, which states that there is nothing special about life on Earth.\nOther authors consider instead that life in the cosmos, or at least multicellular life, may actually be rare. The Rare Earth hypothesis maintains that life on Earth is possible because of a series of factors that range from the location in the galaxy and the configuration of the Solar System to local characteristics of the planet, and that it is unlikely that another planet simultaneously meets all such requirements. The proponents of this hypothesis consider that very little evidence suggests the existence of extraterrestrial life and that, at this point, it is just a desired result and not a reasonable scientific explanation for any gathered data.\nDrake equation.\nIn 1961, astronomer and astrophysicist Frank Drake devised the Drake equation as a way to stimulate scientific dialogue at a meeting on the search for extraterrestrial intelligence (SETI). The Drake equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. The Drake equation is:\nformula_1\nwhere:\n\"N\" = the number of Milky Way galaxy civilizations communicating across interplanetary space\nand\n\"R\"* = the rate of formation of stars suitable for intelligent life in our galaxy\n\"f\"\"p\" = the fraction of those stars that have planets\n\"n\"\"e\" = the average number of planets that can potentially support life\n\"f\"\"l\" = the fraction of planets that actually support life\n\"f\"\"i\" = the fraction of planets with life that evolves to become intelligent life (civilisations)\n\"f\"\"c\" = the fraction of civilizations that develop a technology to broadcast detectable signs of their existence into space\n\"L\" = the length of time over which such civilizations broadcast detectable signals into space\nDrake's proposed estimates are as follows, but numbers on the right side of the equation are agreed as speculative and open to substitution:\nformula_2\nThe Drake equation has proved controversial since, although it is written as a math equation, none of its values were known at the time. Although some values may eventually be measured, others are based on social sciences and are not knowable by their very nature. This does not allow one to make noteworthy conclusions from the equation.\nBased on observations from the Hubble Space Telescope, there are nearly 2 trillion galaxies in the observable universe. It is estimated that at least ten percent of all Sun-like stars have a system of planets. In other words, there are stars with planets orbiting them in the observable universe. Even if it is assumed that only one out of a billion of these stars has planets supporting life, there would be some 6.25 billion life-supporting planetary systems in the observable universe. A 2013 study based on results from the \"Kepler\" spacecraft estimated that the Milky Way contains at least as many planets as it does stars, resulting in 100\u2013400 billion exoplanets. The Nebular hypothesis that explains the formation of the Solar System and other planetary systems would suggest that those can have several configurations, and not all of them may have rocky planets within the habitable zone. \nThe apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilisations and the lack of evidence for such civilisations is known as the Fermi paradox. Dennis W. Sciama claimed that life's existence in the universe depends on various fundamental constants. Zhi-Wei Wang and Samuel L. Braunstein suggest that a random universe capable of supporting life is likely to be just barely able to do so, giving a potential explanation to the Fermi paradox.\nBiochemical basis.\nIf extraterrestrial life exists, it could range from simple microorganisms and multicellular organisms similar to animals or plants, to complex alien intelligences akin to humans. When scientists talk about extraterrestrial life, they consider all those types. Although it is possible that extraterrestrial life may have other configurations, scientists use the hierarchy of lifeforms from Earth for simplicity, as it is the only one known to exist.\nThe first basic requirement for life is an environment with non-equilibrium thermodynamics, which means that the thermodynamic equilibrium must be broken by a source of energy. The traditional sources of energy in the cosmos are the stars, such as for life on Earth, which depends on the energy of the sun. However, there are other alternative energy sources, such as volcanoes, plate tectonics, and hydrothermal vents. There are ecosystems on Earth in deep areas of the ocean that do not receive sunlight, and take energy from black smokers instead. Magnetic fields and radioactivity have also been proposed as sources of energy, although they would be less efficient ones.\nLife on Earth requires water in a liquid state as a solvent in which biochemical reactions take place. It is highly unlikely that an abiogenesis process can start within a gaseous or solid medium: the atom speeds, either too fast or too slow, make it difficult for specific ones to meet and start chemical reactions. A liquid medium also allows the transport of nutrients and substances required for metabolism. Sufficient quantities of carbon and other elements, along with water, might enable the formation of living organisms on terrestrial planets with a chemical make-up and temperature range similar to that of Earth. Life based on ammonia rather than water has been suggested as an alternative, though this solvent appears less suitable than water. It is also conceivable that there are forms of life whose solvent is a liquid hydrocarbon, such as methane, ethane or propane.\nAnother unknown aspect of potential extraterrestrial life would be the chemical elements that would compose it. Life on Earth is largely composed of carbon, but there could be other hypothetical types of biochemistry. A replacement for carbon would need to be able to create complex molecules, store information required for evolution, and be freely available in the medium. To create DNA, RNA, or a close analog, such an element should be able to bind its atoms with many others, creating complex and stable molecules. It should be able to create at least three covalent bonds: two for making long strings and at least a third to add new links and allow for diverse information. Only nine elements meet this requirement: boron, nitrogen, phosphorus, arsenic, antimony (three bonds), carbon, silicon, germanium and tin (four bonds). As for abundance, carbon, nitrogen, and silicon are the most abundant ones in the universe, far more than the others. On Earth's crust the most abundant of those elements is silicon, in the Hydrosphere it is carbon and in the atmosphere, it is carbon and nitrogen. Silicon, however, has disadvantages over carbon. The molecules formed with silicon atoms are less stable, and more vulnerable to acids, oxygen, and light. An ecosystem of silicon-based lifeforms would require very low temperatures, high atmospheric pressure, an atmosphere devoid of oxygen, and a solvent other than water. The low temperatures required would add an extra problem, the difficulty to kickstart a process of abiogenesis to create life in the first place. Norman Horowitz, head of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions from 1965 to 1976 considered that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival of life on other planets. However, he also considered that the conditions found on Mars were incompatible with carbon based life. \nEven if extraterrestrial life is based on carbon and uses water as a solvent, like Earth life, it may still have a radically different biochemistry. Life is generally considered to be a product of natural selection. It has been proposed that to undergo natural selection a living entity must have the capacity to replicate itself, the capacity to avoid damage/decay, and the capacity to acquire and process resources in support of the first two capacities. Life on Earth may have started with an RNA world and later evolved to its current form, where some of the RNA tasks were transferred to DNA and proteins. Extraterrestrial life may still be stuck using RNA, or evolve into other configurations. It is unclear if our biochemistry is the most efficient one that could be generated, or which elements would follow a similar pattern. However, it is likely that, even if cells had a different composition to those from Earth, they would still have a cell membrane. Life on Earth jumped from prokaryotes to eukaryotes and from unicellular organisms to multicellular organisms through evolution. So far no alternative process to achieve such a result has been conceived, even if hypothetical. Evolution requires life to be divided into individual organisms, and no alternative organisation has been satisfactorily proposed either. At the basic level, membranes define the limit of a cell, between it and its environment, while remaining partially open to exchange energy and resources with it.\nThe evolution from simple cells to eukaryotes, and from them to multicellular lifeforms, is not guaranteed. The Cambrian explosion took place thousands of millions of years after the origin of life, and its causes are not fully known yet. On the other hand, the jump to multicellularity took place several times, which suggests that it could be a case of convergent evolution, and so likely to take place on other planets as well. Palaeontologist Simon Conway Morris considers that convergent evolution would lead to kingdoms similar to our plants and animals, and that many features are likely to develop in alien animals as well, such as bilateral symmetry, limbs, digestive systems and heads with sensory organs. Scientists from the University of Oxford analysed it from the perspective of evolutionary theory and wrote in a study in the International Journal of Astrobiology that aliens may be similar to humans. The planetary context would also have an influence: a planet with higher gravity would have smaller animals, and other types of stars can lead to non-green photosynthesizers. The amount of energy available would also affect biodiversity, as an ecosystem sustained by black smokers or hydrothermal vents would have less energy available than those sustained by a star's light and heat, and so its lifeforms would not grow beyond a certain complexity. There is also research in assessing the capacity of life for developing intelligence. It has been suggested that this capacity arises with the number of potential niches a planet contains, and that the complexity of life itself is reflected in the information density of planetary environments, which in turn can be computed from its niches.\nHarsh environmental conditions on Earth harboring life.\nIt is common knowledge that the conditions on other planets in the solar system, in addition to the many galaxies outside of the Milky Way galaxy, are very harsh and seem to be too extreme to harbor any life. The environmental conditions on these planets can have intense UV radiation paired with extreme temperatures, lack of water, and much more that can lead to conditions that don't seem to favor the creation or maintenance of extraterrestrial life. However, there has been much historical evidence that some of the earliest and most basic forms of life on Earth originated in some extreme environments that seem unlikely to have harbored life at least at one point in Earth's history. Fossil evidence as well as many historical theories backed up by years of research and studies have marked environments like hydrothermal vents or acidic hot springs as some of the first places that life could have originated on Earth. These environments can be considered extreme when compared to the typical ecosystems that the majority of life on Earth now inhabit, as hydrothermal vents are scorching hot due to the magma escaping from the Earth's mantle and meeting the much colder oceanic water. Even in today's world, there can be a diverse population of bacteria found inhabiting the area surrounding these hydrothermal vents which can suggest that some form of life can be supported even in the harshest of environments like the other planets in the solar system.\nThe aspects of these harsh environments that make them ideal for the origin of life on Earth, as well as the possibility of creation of life on other planets, is the chemical reactions forming spontaneously. For example, the hydrothermal vents found on the ocean floor are known to support many chemosynthetic processes which allow organisms to utilize energy through reduced chemical compounds that fix carbon. In return, these reactions will allow for organisms to live in relatively low oxygenated environments while maintaining enough energy to support themselves. The early Earth environment was reducing and therefore, these carbon fixing compounds were necessary for the survival and possible origin of life on Earth. With the little amount of information that scientists have found regarding the atmosphere on other planets in the Milky Way galaxy and beyond, the atmospheres are most likely reducing or with very low oxygen levels, especially when compared with Earth's atmosphere. If there were the necessary elements and ions on these planets, the same carbon fixing, reduced chemical compounds occurring around hydrothermal vents could also occur on these planets' surfaces and possibly result in the origin of extraterrestrial life.\nPlanetary habitability in the Solar System.\nThe Solar System has a wide variety of planets, dwarf planets, and moons, and each one is studied for its potential to host life. Each one has its own specific conditions that may benefit or harm life. So far, the only lifeforms found are those from Earth. No extraterrestrial intelligence other than humans exists or has ever existed within the Solar System. Astrobiologist Mary Voytek points out that it would be unlikely to find large ecosystems, as they would have already been detected by now.\nThe inner Solar System is likely devoid of life. However, Venus is still of interest to astrobiologists, as it is a terrestrial planet that was likely similar to Earth in its early stages and developed in a different way. There is a greenhouse effect, the surface is the hottest in the Solar System, sulfuric acid clouds, all surface liquid water is lost, and it has a thick carbon-dioxide atmosphere with huge pressure. Comparing both helps to understand the precise differences that lead to beneficial or harmful conditions for life. And despite the conditions against life on Venus, there are suspicions that microbial life-forms may still survive in high-altitude clouds.\nMars is a cold and almost airless desert, inhospitable to life. However, recent studies revealed that water on Mars used to be quite abundant, forming rivers, lakes, and perhaps even oceans. Mars may have been habitable back then, and life on Mars may have been possible. But when the planetary core ceased to generate a magnetic field, solar winds removed the atmosphere and the planet became vulnerable to solar radiation. Ancient life-forms may still have left fossilised remains, and microbes may still survive deep underground.\nAs mentioned, the gas giants and ice giants are unlikely to contain life. The most distant solar system bodies, found in the Kuiper Belt and outwards, are locked in permanent deep-freeze, but cannot be ruled out completely.\nAlthough the giant planets themselves are highly unlikely to have life, there is much hope to find it on moons orbiting these planets. Europa, from the Jovian system, has a subsurface ocean below a thick layer of ice. Ganymede and Callisto also have subsurface oceans, but life is less likely in them because water is sandwiched between layers of solid ice. Europa would have contact between the ocean and the rocky surface, which helps the chemical reactions. It may be difficult to dig so deep in order to study those oceans, though. Enceladus, a tiny moon of Saturn with another subsurface ocean, may not need to be dug, as it releases water to space in eruption columns. The space probe \"Cassini\" flew inside one of these, but could not make a full study because NASA did not expect this phenomenon and did not equip the probe to study ocean water. Still, \"Cassini\" detected complex organic molecules, salts, evidence of hydrothermal activity, hydrogen, and methane.\nTitan is the only celestial body in the Solar System besides Earth that has liquid bodies on the surface. It has rivers, lakes, and rain of hydrocarbons, methane, and ethane, and even a cycle similar to Earth's water cycle. This special context encourages speculations about lifeforms with different biochemistry, but the cold temperatures would make such chemistry take place at a very slow pace. Water is rock-solid on the surface, but Titan does have a subsurface water ocean like several other moons. However, it is of such a great depth that it would be very difficult to access it for study.\nScientific search.\nThe science that searches and studies life in the universe, both on Earth and elsewhere, is called astrobiology. With the study of Earth's life, the only known form of life, astrobiology seeks to study how life starts and evolves and the requirements for its continuous existence. This helps to determine what to look for when searching for life in other celestial bodies. This is a complex area of study, and uses the combined perspectives of several scientific disciplines, such as astronomy, biology, chemistry, geology, oceanography, and atmospheric sciences.\nThe scientific search for extraterrestrial life is being carried out both directly and indirectly. As of \u00a02017[ [update]], 3,667 exoplanets in 2,747 systems have been identified, and other planets and moons in the Solar System hold the potential for hosting primitive life such as microorganisms. As of 8 February 2021, an updated status of studies considering the possible detection of lifeforms on Venus (via phosphine) and Mars (via methane) was reported.\nSearch for basic life.\nScientists search for biosignatures within the Solar System by studying planetary surfaces and examining meteorites. Some claim to have identified evidence that microbial life has existed on Mars. In 1996, a controversial report stated that structures resembling nanobacteria were discovered in a meteorite, ALH84001, formed of rock ejected from Mars. Although all the unusual properties of the meteorite were eventually explained as the result of inorganic processes, the controversy over its discovery laid the groundwork for the development of astrobiology.\nAn experiment on the two Viking Mars landers reported gas emissions from heated Martian soil samples that some scientists argue are consistent with the presence of living microorganisms. Lack of corroborating evidence from other experiments on the same samples suggests that a non-biological reaction is a more likely hypothesis.\nIn February 2005 NASA scientists reported they may have found some evidence of extraterrestrial life on Mars. The two scientists, Carol Stoker and Larry Lemke of NASA's Ames Research Center, based their claim on methane signatures found in Mars's atmosphere resembling the methane production of some forms of primitive life on Earth, as well as on their own study of primitive life near the Rio Tinto river in Spain. NASA officials soon distanced NASA from the scientists' claims, and Stoker herself backed off from her initial assertions.\nIn November 2011, NASA launched the Mars Science Laboratory that landed the \"Curiosity\" rover on Mars. It is designed to assess the past and present habitability on Mars using a variety of scientific instruments. The rover landed on Mars at Gale Crater in August 2012.\nA group of scientists at Cornell University started a catalog of microorganisms, with the way each one reacts to sunlight. The goal is to help with the search for similar organisms in exoplanets, as the starlight reflected by planets rich in such organisms would have a specific spectrum, unlike that of starlight reflected from lifeless planets. If Earth was studied from afar with this system, it would reveal a shade of green, as a result of the abundance of plants with photosynthesis.\nIn August 2011, NASA studied meteorites found on Antarctica, finding adenine, guanine, hypoxanthine and xanthine. Adenine and guanine are components of DNA, and the others are used in other biological processes. The studies ruled out pollution of the meteorites on Earth, as those components would not be freely available the way they were found in the samples. This discovery suggests that several organic molecules that serve as building blocks of life may be generated within asteroids and comets. In October 2011, scientists reported that cosmic dust contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. It is still unclear if those compounds played a role in the creation of life on Earth, but Sun Kwok, of the University of Hong Kong, thinks so. \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\"\nIn August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary \"IRAS 16293-2422\", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.\nIn December 2023, astronomers reported the first time discovery, in the plumes of Enceladus, moon of the planet Saturn, of hydrogen cyanide, a possible chemical essential for life as we know it, as well as other organic molecules, some of which are yet to be better identified and understood. According to the researchers, \"these [newly discovered] compounds could potentially support extant microbial communities or drive complex organic synthesis leading to the origin of life.\"\nSearch for extraterrestrial intelligences.\nAlthough most searches are focused on the biology of extraterrestrial life, an extraterrestrial intelligence capable enough to develop a civilization may be detectable by other means as well. Technology may generate technosignatures, effects on the native planet that may not be caused by natural causes. There are three main types of techno-signatures considered: interstellar communications, effects on the atmosphere, and planetary-sized structures such as Dyson spheres.\nOrganizations such as the SETI Institute search the cosmos for potential forms of communication. They started with radio waves, and now search for laser pulses as well. The challenge for this search is that there are natural sources of such signals as well, such as gamma-ray bursts and supernovae, and the difference between a natural signal and an artificial one would be in its specific patterns. Astronomers intend to use artificial intelligence for this, as it can manage large amounts of data and is devoid of biases and preconceptions. Besides, even if there is an advanced extraterrestrial civilization, there is no guarantee that it is transmitting radio communications in the direction of Earth. The length of time required for a signal to travel across space means that a potential answer may arrive decades or centuries after the initial message.\nThe atmosphere of Earth is rich in nitrogen dioxide as a result of air pollution, which can be detectable. The natural abundance of carbon, which is also relatively reactive, makes it likely to be a basic component of the development of a potential extraterrestrial technological civilization, as it is on Earth. Fossil fuels may likely be generated and used on such worlds as well. The abundance of chlorofluorocarbons in the atmosphere can also be a clear technosignature, considering their role in ozone depletion. Light pollution may be another technosignature, as multiple lights on the night side of a rocky planet can be a sign of advanced technological development. However, modern telescopes are not strong enough to study exoplanets with the required level of detail to perceive it.\nThe Kardashev scale proposes that a civilization may eventually start consuming energy directly from its local star. This would require giant structures built next to it, called Dyson spheres. Those speculative structures would cause an excess infrared radiation, that telescopes may notice. The infrared radiation is typical of young stars, surrounded by dusty protoplanetary disks that will eventually form planets. An older star such as the Sun would have no natural reason to have excess infrared radiation. The presence of heavy elements in a star's light-spectrum is another potential biosignature; such elements would (in theory) be found if the star were being used as an incinerator/repository for nuclear waste products.\nExtrasolar planets.\nSome astronomers search for extrasolar planets that may be conducive to life, narrowing the search to terrestrial planets within the habitable zones of their stars. Since 1992, over four thousand exoplanets have been discovered (6,128 planets in 4,584 planetary systems including 1,017 multiple planetary systems as of none }}).\nThe extrasolar planets so far discovered range in size from that of terrestrial planets similar to Earth's size to that of gas giants larger than Jupiter. The number of observed exoplanets is expected to increase greatly in the coming years. The Kepler space telescope has also detected a few thousand candidate planets, of which about 11% may be false positives.\nThere is at least one planet on average per star. About 1 in 5 Sun-like stars have an \"Earth-sized\" planet in the habitable zone, with the nearest expected to be within 12 light-years distance from Earth. Assuming 200\u00a0billion stars in the Milky Way, that would be 11\u00a0billion potentially habitable Earth-sized planets in the Milky Way, rising to 40\u00a0billion if red dwarfs are included. The rogue planets in the Milky Way possibly number in the trillions.\nThe nearest known exoplanet is Proxima Centauri b, located from Earth in the southern constellation of Centaurus.\nAs of March 2014[ [update]], the least massive exoplanet known is PSR B1257+12 A, which is about twice the mass of the Moon. The most massive planet listed on the NASA Exoplanet Archive is DENIS-P J082303.1\u2212491201 b, about 29 times the mass of Jupiter, although according to most definitions of a planet, it is too massive to be a planet and may be a brown dwarf instead. Almost all of the planets detected so far are within the Milky Way, but there have also been a few possible detections of extragalactic planets. The study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.\nOne sign that a planet probably already contains life is the presence of an atmosphere with significant amounts of oxygen, since that gas is highly reactive and generally would not last long without constant replenishment. This replenishment occurs on Earth through photosynthetic organisms. One way to analyse the atmosphere of an exoplanet is through spectrography when it transits its star, though this might only be feasible with dim stars like white dwarfs.\nHistory and cultural impact.\nCosmic pluralism.\nThe modern concept of extraterrestrial life is based on assumptions that were not commonplace during the early days of astronomy. The first explanations for the celestial objects seen in the night sky were based on mythology. Scholars from Ancient Greece were the first to consider that the universe is inherently understandable and rejected explanations based on supernatural incomprehensible forces, such as the myth of the Sun being pulled across the sky in the chariot of Apollo. They had not developed the scientific method yet and based their ideas on pure thought and speculation, but they developed precursor ideas to it, such as that explanations had to be discarded if they contradict observable facts. The discussions of those Greek scholars established many of the pillars that would eventually lead to the idea of extraterrestrial life, such as Earth being round and not flat. The cosmos was first structured in a geocentric model that considered that the sun and all other celestial bodies revolve around Earth. However, they did not consider them as worlds. In Greek understanding, the world was composed by both Earth and the celestial objects with noticeable movements. Anaximander thought that the cosmos was made from \"apeiron\", a substance that created the world, and that the world would eventually return to the cosmos.\nEventually two groups emerged, the \"atomists\" that thought that matter at both Earth and the cosmos was equally made of small atoms of the classical elements (earth, water, fire and air), and the \"Aristotelians\" who thought that those elements were exclusive of Earth and that the cosmos was made of a fifth one, the \"aether\". Atomist Epicurus thought that the processes that created the world, its animals and plants should have created other worlds elsewhere, along with their own animals and plants. Aristotle thought instead that all the earth element naturally fell towards the center of the universe, and that would make it impossible for other planets to exist elsewhere. Under that reasoning, Earth was not only in the center, it was also the only planet in the universe.\nCosmic pluralism, the plurality of worlds, or simply pluralism, describes the philosophical belief in numerous \"worlds\" in addition to Earth, which might harbor extraterrestrial life. The earliest recorded assertion of extraterrestrial human life is found in ancient scriptures of Jainism. There are multiple \"worlds\" mentioned in Jain scriptures that support human life. These include, among others, \"Bharat Kshetra\", \"Mahavideh Kshetra\", \"Airavat Kshetra\", and \"Hari kshetra\". Medieval Muslim writers like Fakhr al-Din al-Razi and Muhammad al-Baqir supported cosmic pluralism on the basis of the Qur'an. Chaucer's poem \"The House of Fame\" engaged in medieval thought experiments that postulated the plurality of worlds. However, those ideas about other worlds were different from the current knowledge about the structure of the universe, and did not postulate the existence of planetary systems other than the Solar System. When those authors talk about other worlds, they talk about places located at the center of their own systems, and with their own stellar vaults and cosmos surrounding them.\nThe Greek ideas and the disputes between atomists and Aristotelians outlived the fall of the Greek empire. The Great Library of Alexandria compiled information about it, part of which was translated by Islamic scholars and thus survived the end of the Library. Baghdad combined the knowledge of the Greeks, the Indians, the Chinese and its own scholars, and the knowledge expanded through the Byzantine Empire. From there it eventually returned to Europe by the time of the Middle Ages. However, as the Greek atomist doctrine held that the world was created by random movements of atoms, with no need for a creator deity, it became associated with atheism, and the dispute intertwined with religious ones. Still, the Church did not react to those topics in a homogeneous way, and there were stricter and more permissive views within the church itself.\nThe first known mention of the term 'panspermia' was in the writings of the 5th-century BC Greek philosopher Anaxagoras. He proposed the idea that life exists everywhere.\nEarly modern period.\nBy the time of the late Middle Ages there were many known inaccuracies in the geocentric model, but it was kept in use because naked eye observations provided limited data. Nicolaus Copernicus started the Copernican Revolution by proposing that the planets revolve around the sun rather than Earth. His proposal had little acceptance at first because, as he kept the assumption that orbits were perfect circles, his model led to as many inaccuracies as the geocentric one. Tycho Brahe improved the available data with naked-eye observatories, which worked with highly complex sextants and quadrants. Tycho could not make sense of his observations, but Johannes Kepler did: orbits were not perfect circles, but ellipses. This knowledge benefited the Copernican model, which worked now almost perfectly. The invention of the telescope a short time later, perfected by Galileo Galilei, clarified the final doubts, and the paradigm shift was completed. Under this new understanding, the notion of extraterrestrial life became feasible: if Earth is but just a planet orbiting around a star, there may be planets similar to Earth elsewhere. The astronomical study of distant bodies also proved that physical laws are the same elsewhere in the universe as on Earth, with nothing making the planet truly special.\nThe new ideas were met with resistance from the Catholic church. Galileo was tried for the heliocentric model, which was considered heretical, and forced to recant it. The best-known early-modern proponent of ideas of extraterrestrial life was the Italian philosopher Giordano Bruno, who argued in the 16th century for an infinite universe in which every star is surrounded by its own planetary system. Bruno wrote that other worlds \"have no less virtue nor a nature different to that of our earth\" and, like Earth, \"contain animals and inhabitants\". Bruno's belief in the plurality of worlds was one of the charges leveled against him by the Venetian Holy Inquisition, which tried and executed him.\nThe heliocentric model was further strengthened by the postulation of the theory of gravity by Sir Isaac Newton. This theory provided the mathematics that explains the motions of all things in the universe, including planetary orbits. By this point, the geocentric model was definitely discarded. By this time, the use of the scientific method had become a standard, and new discoveries were expected to provide evidence and rigorous mathematical explanations. Science also took a deeper interest in the mechanics of natural phenomena, trying to explain not just the way nature works but also the reasons for working that way.\nThere was very little actual discussion about extraterrestrial life before this point, as the Aristotelian ideas remained influential while geocentrism was still accepted. When it was finally proved wrong, it not only meant that Earth was not the center of the universe, but also that the lights seen in the sky were not just lights, but physical objects. The notion that life may exist in them as well soon became an ongoing topic of discussion, although one with no practical ways to investigate.\nThe possibility of extraterrestrials remained a widespread speculation as scientific discovery accelerated. William Herschel, the discoverer of Uranus, was one of many 18th\u201319th-century astronomers who believed that the Solar System is populated by alien life. Other scholars of the period who championed \"cosmic pluralism\" included Immanuel Kant and Benjamin Franklin. At the height of the Enlightenment, even the Sun and Moon were considered candidates for extraterrestrial inhabitants.\n19th century.\nSpeculation about life on Mars increased in the late 19th century, following telescopic observation of apparent Martian canals \u2013 which soon, however, turned out to be optical illusions. Despite this, in 1895, American astronomer Percival Lowell published his book \"Mars,\" followed by \"Mars and its Canals\" in 1906, proposing that the canals were the work of a long-gone civilisation. \nSpectroscopic analysis of Mars's atmosphere began in earnest in 1894, when U.S. astronomer William Wallace Campbell showed that neither water nor oxygen was present in the Martian atmosphere. By 1909 better telescopes and the best perihelic opposition of Mars since 1877 conclusively put an end to the canal hypothesis.\nAs a consequence of the belief in the spontaneous generation there was little thought about the conditions of each celestial body: it was simply assumed that life would thrive anywhere. This theory was disproved by Louis Pasteur in the 19th century. Popular belief in thriving alien civilisations elsewhere in the solar system still remained strong until Mariner 4 and Mariner 9 provided close images of Mars, which debunked forever the idea of the existence of Martians and decreased the previous expectations of finding alien life in general. The end of the spontaneous generation belief forced investigation into the origin of life. Although abiogenesis is the more accepted theory, a number of authors reclaimed the term \"panspermia\" and proposed that life was brought to Earth from elsewhere. Some of those authors are J\u00f6ns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) and, somewhat later, by Svante Arrhenius (1903).\nThe science fiction genre, although not so named during the time, developed during the late 19th century. The expansion of the genre of extraterrestrials in fiction influenced the popular perception over the real-life topic, making people eager to jump to conclusions about the discovery of aliens. Science marched at a slower pace, some discoveries fueled expectations and others dashed excessive hopes. For example, with the advent of telescopes, most structures seen on the Moon or Mars were immediately attributed to Selenites or Martians, and later ones (such as more powerful telescopes) revealed that all such discoveries were natural features. A famous case is the Cydonia region of Mars, first imaged by the \"Viking 1\" orbiter. The low-resolution photos showed a rock formation that resembled a human face, but later spacecraft took photos in higher detail that showed that there was nothing special about the site.\nRecent history.\nThe search and study of extraterrestrial life became a science of its own, astrobiology. Also known as \"exobiology\", this discipline is studied by the NASA, the ESA, the INAF, and others. Astrobiology studies life from Earth as well, but with a cosmic perspective. For example, abiogenesis is of interest to astrobiology, not because of the origin of life on Earth, but for the chances of a similar process taking place in other celestial bodies. Many aspects of life, from its definition to its chemistry, are analyzed as either likely to be similar in all forms of life across the cosmos or only native to Earth. Astrobiology, however, remains constrained by the current lack of extraterrestrial life-forms to study, as all life on Earth comes from the same ancestor, and it is hard to infer general characteristics from a group with a single example to analyse.\nThe 20th century came with great technological advances, speculations about future hypothetical technologies, and an increased basic knowledge of science by the general population thanks to science divulgation through the mass media. The public interest in extraterrestrial life and the lack of discoveries by mainstream science led to the emergence of pseudosciences that provided affirmative, if questionable, answers to the existence of aliens. Ufology claims that many unidentified flying objects (UFOs) would be spaceships from alien species, and ancient astronauts hypothesis claim that aliens would have visited Earth in antiquity and prehistoric times but people would have failed to understand it by then. Most UFOs or UFO sightings can be readily explained as sightings of Earth-based aircraft (including top-secret aircraft), known astronomical objects or weather phenomenons, or as hoaxes.\nLooking beyond the pseudosciences, Lewis White Beck strove to elevate the level of public discourse on the topic of extraterrestrial life by tracing the evolution of philosophical thought over the centuries from ancient times into the modern era. His review of the contributions made by Lucretius, Plutarch, Aristotle, Copernicus, Immanuel Kant, John Wilkins, Charles Darwin and Karl Marx demonstrated that even in modern times, humanity could be profoundly influenced in its search for extraterrestrial life by subtle and comforting archetypal ideas which are largely derived from firmly held religious, philosophical and existential belief systems. On a positive note, however, Beck further argued that even if the search for extraterrestrial life proves to be unsuccessful, the endeavor itself could have beneficial consequences by assisting humanity in its attempt to actualize superior ways of living here on Earth.\nBy the 21st century, it was accepted that multicellular life in the Solar System can only exist on Earth, but the interest in extraterrestrial life increased regardless. This is a result of the advances in several sciences. The knowledge of planetary habitability allows to consider on scientific terms the likelihood of finding life at each specific celestial body, as it is known which features are beneficial and harmful for life. Astronomy and telescopes also improved to the point exoplanets can be confirmed and even studied, increasing the number of search places. Life may still exist elsewhere in the Solar System in unicellular form, but the advances in spacecraft allow to send robots to study samples in situ, with tools of growing complexity and reliability. Although no extraterrestrial life has been found and life may still be just a rarity from Earth, there are scientific reasons to suspect that it can exist elsewhere, and technological advances that may detect it if it does.\nMany scientists are optimistic about the chances of finding alien life. In the words of SETI's Frank Drake, \"All we know for sure is that the sky is not littered with powerful microwave transmitters\". Drake noted that it is entirely possible that advanced technology results in communication being carried out in some way other than conventional radio transmission. At the same time, the data returned by space probes, and giant strides in detection methods, have allowed science to begin delineating habitability criteria on other worlds, and to confirm that at least other planets are plentiful, though aliens remain a question mark. The Wow! signal, detected in 1977 by a SETI project, remains a subject of speculative debate.\nOn the other hand, other scientists are pessimistic. Jacques Monod wrote that \"Man knows at last that he is alone in the indifferent immensity of the universe, whence which he has emerged by chance\". In 2000, geologist and paleontologist Peter Ward and astrobiologist Donald Brownlee published a book entitled \"Rare Earth: Why Complex Life is Uncommon in the Universe\". In it, they discussed the Rare Earth hypothesis, in which they claim that Earth-like life is rare in the universe, whereas microbial life is common. Ward and Brownlee are open to the idea of evolution on other planets that is not based on essential Earth-like characteristics such as DNA and carbon.\nAs for the possible risks, theoretical physicist Stephen Hawking warned in 2010 that humans should not try to contact alien life forms. He warned that aliens might pillage Earth for resources. \"If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans\", he said. Jared Diamond had earlier expressed similar concerns. On 20 July 2015, Hawking and Russian billionaire Yuri Milner, along with the SETI Institute, announced a well-funded effort, called the Breakthrough Initiatives, to expand efforts to search for extraterrestrial life. The group contracted the services of the 100-meter Robert C. Byrd Green Bank Telescope in West Virginia in the United States and the 64-meter Parkes Telescope in New South Wales, Australia. On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a \"worldwide scientific, political and humanitarian discussion must occur before any message is sent\".\nGovernment responses.\nThe 1967 Outer Space Treaty and the 1979 Moon Agreement define rules of planetary protection against potentially hazardous extraterrestrial life. COSPAR also provides guidelines for planetary protection. A committee of the United Nations Office for Outer Space Affairs had in 1977 discussed for a year strategies for interacting with extraterrestrial life or intelligence. The discussion ended without any conclusions. As of 2010, the UN lacks response mechanisms for the case of an extraterrestrial contact.\nOne of the NASA divisions is the Office of Safety and Mission Assurance (OSMA), also known as the Planetary Protection Office. A part of its mission is to \"rigorously preclude backward contamination of Earth by extraterrestrial life.\"\nIn 2016, the Chinese Government released a white paper detailing its space program. According to the document, one of the research objectives of the program is the search for extraterrestrial life. It is also one of the objectives of the Chinese Five-hundred-meter Aperture Spherical Telescope (FAST) program.\nIn 2020, Dmitry Rogozin, the head of the Russian space agency, said the search for extraterrestrial life is one of the main goals of deep space research. \nHe also acknowledged the possibility of existence of primitive life on other planets of the Solar System.\nThe French space agency has an office for the study of \"non-identified aero spatial phenomena\". The agency is maintaining a publicly accessible database of such phenomena, with over 1600 detailed entries. According to the head of the office, the vast majority of entries have a mundane explanation; but for 25% of entries, their extraterrestrial origin can neither be confirmed nor denied.\nIn 2020, chairman of the Israel Space Agency Isaac Ben-Israel stated that the probability of detecting life in outer space is \"quite large\". But he disagrees with his former colleague Haim Eshed who stated that there are contacts between an advanced alien civilisation and some of Earth's governments.\nIn fiction.\nAlthough the idea of extraterrestrial peoples became feasible once astronomy developed enough to understand the nature of planets, they were not thought of as being any different from humans. Having no scientific explanation for the origin of mankind and its relation to other species, there was no reason to expect them to be any other way. This was changed by the 1859 book \"On the Origin of Species\" by Charles Darwin, which proposed the theory of evolution. Now with the notion that evolution on other planets may take other directions, science fiction authors created bizarre aliens, clearly distinct from humans. A usual way to do that was to add body features from other animals, such as insects or octopuses. Costuming and special effects feasibility alongside budget considerations forced films and TV series to tone down the fantasy, but these limitations lessened since the 1990s with the advent of computer-generated imagery (CGI), and later on as CGI became more effective and less expensive.\nReal-life events sometimes captivate people's imagination and this influences the works of fiction. For example, during the Barney and Betty Hill incident, the first recorded claim of an alien abduction, the couple reported that they were abducted and experimented on by aliens with oversized heads, big eyes, pale grey skin, and small noses, a description that eventually became the grey alien archetype once used in works of fiction.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9589", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=9589", "title": "European Strategic Programme on Research in Information Technology", "text": "Series of information technology R&amp;D programmes of the EC/EU\nEuropean Strategic Programme on Research in Information Technology (ESPRIT) was a series of integrated programmes of information technology research and development projects and industrial technology transfer measures. It was a European Union initiative managed by the Directorate General for Industry (DG III) of the European Commission.\nProgrammes.\nFive ESPRIT programmes (ESPRIT 0 to ESPRIT 4) ran consecutively from 1983 to 1998. ESPRIT 4 was succeeded by the Information Society Technologies (IST) programme in 1999.\nProjects.\nSome of the projects and products supported by ESPRIT were: \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9591", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=9591", "title": "E. E. Cummings", "text": "American author (1894\u20131962)\nEdward Estlin Cummings (October 14, 1894 \u2013 September 3, 1962), commonly known as e e cummings or E. E. Cummings, was an American poet, painter, essayist, author, and playwright. During World War I, he worked as an ambulance driver and was imprisoned in an internment camp, which provided the basis for his novel \"The Enormous Room\" (1922). The following year he published his first collection of poetry, \"Tulips and Chimneys\", which showed his early experiments with grammar and typography. He wrote four plays, the most successful of which were \"HIM\" (1927) and \"\" (1946). He wrote \"EIMI\" (1933), a travelog of the Soviet Union, and delivered the Charles Eliot Norton Lectures in poetry, published as \"i\u2014six nonlectures\" (1953). \"Fairy Tales\" (1965), a collection of short stories, was published posthumously. \nCummings wrote approximately 2,900 poems. He is often regarded as one of the most important American poets of the 20th century. He is associated with modernist free-form poetry, and much of his work uses idiosyncratic syntax and lower-case spellings for poetic expression. M. L. Rosenthal wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The chief effect of Cummings' jugglery with syntax, grammar, and diction was to blow open otherwise trite and bathetic motifs through a dynamic rediscovery of the energies sealed up in conventional usage... He succeeded masterfully in splitting the atom of the cute commonplace.\nFor Norman Friedman, Cummings's inventions \"are best understood as various ways of stripping the film of familiarity from language to strip the film of familiarity from the world. Transform the word, he seems to have felt, and you are on the way to transforming the world.\"\nThe poet Randall Jarrell said of Cummings, \"No one else has ever made avant-garde, experimental poems so attractive to the general and the special reader.\" James Dickey wrote, \"I think that Cummings is a daringly original poet, with more vitality and more sheer, uncompromising talent than any other living American writer.\" Dickey described himself as \"ashamed and even a little guilty in picking out flaws\" in Cummings's poetry, which he compared to noting \"the aesthetic defects in a rose. It is better to say what must finally be said about Cummings: that he has helped to give life to the language.\"\nLife.\nEarly years.\nEdward Estlin Cummings was born on October 14, 1894, in Cambridge, Massachusetts, to Edward Cummings and Rebecca Haswell (n\u00e9e\u00a0Clarke), a well-known Unitarian upper-class couple in the city. His father was a professor at Harvard University who later became nationally known as the minister of South Congregational Church (Unitarian) in Boston, Massachusetts. His mother, who loved to spend time with her children, played games with Edward and his sister, Elizabeth. From an early age, Cummings's parents supported his creative gifts. Cummings wrote poems and drew as a child, and he often played outdoors with the other children who lived in his neighborhood. He grew up in the company of family friends such as the philosophers William James and Josiah Royce. Many of Cummings's summers were spent on Silver Lake in Madison, New Hampshire, where his father had built two houses along the eastern shore. The family ultimately purchased the nearby Joy Farm where Cummings had his primary summer residence.\nHe expressed transcendental leanings his entire life. As he matured, Cummings moved to an \"I, Thou\" relationship with God. His journals are replete with references to \"le bon Dieu,\" as well as prayers for inspiration in his poetry and artwork (such as \"Bon Dieu! may i some day do something truly great. amen.\"). Cummings \"also prayed for strength to be his essential self ('may I be I is the only prayer\u2014not may I be great or good or beautiful or wise or strong'), and for relief of spirit in times of depression ('almighty God! I thank thee for my soul; &amp; may I never die spiritually into a mere mind through disease of loneliness')\".\nCummings wanted to be a poet from childhood and wrote poetry daily from age 8 to 22, exploring assorted forms. He studied Latin and Greek at Cambridge Latin High School. He attended Harvard University, graduating with a Bachelor of Arts degree magna cum laude and was elected to the Phi Beta Kappa society in 1915. The following year, he received a Master of Arts degree from the university. During his studies at Harvard, he developed an interest in modern poetry, which ignored conventional grammar and syntax and aimed for a dynamic use of language. His first published poems appeared in \"Eight Harvard Poets\" (1917). Upon graduating, he worked for a book dealer.\nWar years.\nIn 1917, with the First World War going on in Europe, Cummings enlisted in the Norton-Harjes Ambulance Corps. On the boat to France, he met William Slater Brown and they quickly became friends. Due to an administrative error, Cummings and Brown did not receive an assignment for five weeks, a period they spent exploring Paris. Cummings fell in love with the city, to which he would return throughout his life.\nDuring their service in the ambulance corps, the two young writers sent letters home that drew the attention of the military censors. They were known to prefer the company of French soldiers over fellow ambulance drivers. The two openly expressed anti-war views, Cummings spoke of his lack of hatred for the Germans. On September 21, 1917, five months after starting his belated assignment, Cummings and William Slater Brown were arrested by the French military on suspicion of espionage and undesirable activities. They were held for three and a half months in a military detention camp at the , in La Fert\u00e9-Mac\u00e9, Orne, Normandy.\nThey were imprisoned with other detainees in a large room. Cummings's father made strenuous efforts to obtain his son's release through diplomatic channels; although advised his son's release was approved, there were lengthy delays, with little explanation. In frustration, Cummings's father wrote a letter to President Woodrow Wilson in December 1917. Cummings was released on December 19, 1917, returning to his family in the U.S. by New Year's Day, 1918. Cummings, his father, and Brown's family continued to agitate for Brown's release. By mid-February, he, too, was America-bound. Cummings used his prison experience as the basis for his novel, \"The Enormous Room\" (1922), about which F. Scott Fitzgerald said, \"Of all the work by young men who have sprung up since 1920 one book survives\u2014\"The Enormous Room\" by E. E. Cummings\u00a0... Those few who cause books to live have not been able to endure the thought of its mortality.\" Later in 1918 he was drafted into the army. He served a training deployment in the 12th Division at Camp Devens, Massachusetts, until November 1918.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nBuffalo Bill's\ndefunct\n who used to\n ride a watersmooth-silver\n stallion\nand break onetwothreefourfive pigeonsjustlikethat\n Jesus\nhe was a handsome man\n and what i want to know is\nhow do you like your blueeyed boy\nMister Death\n&lt;/poem&gt;\n \"Buffalo Bill's\" (1920)\nPost-war years.\nCummings returned to Paris in 1921, and lived there for two years before returning to New York. His collection \"Tulips and Chimneys\" was published in 1923, and his inventive use of grammar and syntax is evident. The book was heavily cut by his editor. \"XLI Poems\" was published in 1925. With these collections, Cummings made his reputation as an avant-garde poet.\nDuring the rest of the 1920s and 1930s, Cummings returned to Paris a number of times, and traveled throughout Europe. In 1931 Cummings traveled to the Soviet Union, recounting his experiences in \"Eimi\", published two years later. During these years Cummings also traveled to Northern Africa and Mexico, and he worked as an essayist and portrait artist for \"Vanity Fair\" magazine (1924\u20131927).\nIn 1926, Cummings's parents were in a car crash; only his mother survived, although she was severely injured. Cummings later described the crash in the following passage from his \"i: six nonlectures\" series given at Harvard (as part of the Charles Eliot Norton Lectures) in 1952 and 1953:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A locomotive cut the car in half, killing my father instantly. When two brakemen jumped from the halted train, they saw a woman standing \u2013 dazed but erect \u2013 beside a mangled machine; with blood spouting (as the older said to me) out of her head. One of her hands (the younger added) kept feeling her dress, as if trying to discover why it was wet. These men took my sixty-six-year old mother by the arms and tried to lead her toward a nearby farmhouse; but she threw them off, strode straight to my father's body, and directed a group of scared spectators to cover him. When this had been done (and only then) she let them lead her away.\u2014\u200a\nHis father's death had a profound effect on Cummings, who entered a new period in his artistic life. He began to focus on more important aspects of life in his poetry. He started this new period by paying homage to his father in the poem \"my father moved through dooms of love\".\nIn the 1930s, Samuel Aiwaz Jacobs was Cummings's publisher; he had started the Golden Eagle Press after working as a typographer and publisher.\nFinal years.\nIn 1952, his alma mater, Harvard University, awarded Cummings an honorary seat as a guest professor. The Charles Eliot Norton Lectures he gave in 1952 and 1955 were later collected as \"i: six nonlectures\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nCummings spent the last decade of his life traveling, fulfilling speaking engagements, and spending time at his summer home, Joy Farm, in Silver Lake, New Hampshire. He died of a stroke on September 3, 1962, at the age of 67 at Memorial Hospital in North Conway, New Hampshire. Cummings was buried at Forest Hills Cemetery in Boston, Massachusetts. At the time of his death, Cummings was recognized as the \"second most widely read poet in the United States, after Robert Frost\".\nCummings's papers are held at the Houghton Library at Harvard University and the Harry Ransom Center at the University of Texas at Austin.\nPersonal life.\nMarriages.\nCummings was married twice: first to Elaine Orr Thayer in 1924, then to Anne Minnerly Barton in 1929. His longest relationship, with Marion Morehouse, began in 1934, and lasted until his death.\nIn 1917, before his first marriage, Cummings shared several passionate love letters with a Parisian prostitute, Marie Louise Lallemand. Despite Cummings's efforts, he was unable to find Lallemand upon his return to Paris after serving at the front.\nCummings's relationship with Elaine Orr began as a love affair in 1918, while she was still married to Scofield Thayer, one of Cummings' friends from Harvard. During this time, he wrote a large portion of his erotic poetry. The couple had a daughter while Orr was still married to Thayer. After Orr divorced Thayer, Cummings and Orr married on March 19, 1924. Thayer had been registered on the child's birth certificate as the father, but Cummings legally adopted her after his marriage to Orr. Although his relationship with Orr stretched back several years, the marriage was brief. On a trip to Paris, Orr met and fell in love with the Irish nobleman, future politician, author, journalist, and former banker Frank MacDermot. The couple separated after two months of marriage and divorced less than nine months later. \nCummings married his second wife, Anne Minnerly Barton, on May 1, 1929. They separated three years later in 1932. That same year, Minnerly obtained a Mexican divorce; it was not officially recognized in the United States until August 1934. Anne died in 1970 aged 72.\nIn 1934, Cummings met Marion Morehouse, a fashion model and photographer. It is not clear whether the two were ever formally married. Morehouse lived with Cummings until his death in 1962 after suffering a stroke at his home in New Hampshire. He never regained consciousness after the stroke and died at the age of 67. Morehouse died on May 18, 1969, while living at 4 Patchin Place, Greenwich Village, New York City, where Cummings had resided since September 1924.\nPolitical views.\nAccording to his testimony in \"EIMI\", Cummings had little interest in politics until his trip to the Soviet Union in 1931. He subsequently shifted rightward on many political and social issues. Despite his radical and bohemian public image, he was a Republican and later an ardent supporter of Joseph McCarthy.\nLiterary overview.\nPoetry.\nAs well as being influenced by notable modernists, including Gertrude Stein and Ezra Pound, Cummings was particularly drawn to early imagist experiments; later, his visits to Paris exposed him to Dada and Surrealism, which was reflected in his writing style. Cummings critic and biographer Norman Friedman remarks that in Cummings's later work the \"shift from simile to symbol\" created poetry that is \"frequently more lucid, more moving, and more profound than his earlier\".\nDespite Cummings's familiarity with avant-garde styles (likely affected by the calligrams of French poet Apollinaire, according to a contemporary observation), much of his work draws inspiration from traditional forms. For example, many of his poems are sonnets, albeit described by Richard D. Cureton as \"revisionary... with scrambled rhymes and rearranged, disproportioned structures; awkwardly unpredictable metrical variation; clashing, mawkish diction; complex, wandering syntax; etc.\" He occasionally drew from the blues form and used acrostics. Many of Cummings's poems are satirical and address social issues but have an equal or even stronger bias toward Romanticism: time and again his poems celebrate love, sex, and the season of rebirth.\nWhile his poetic forms and themes share an affinity with the Romantic tradition, critic Emily Essert asserts that Cummings's work is particularly modernist and frequently employs what linguist Irene Fairley calls \"syntactic deviance\". Some poems do not involve any typographical or punctuation innovations at all, but purely syntactic ones; many of the poems he is best known for, however, do possess a stylistic typography he made his own, particularly in his insistent use of the lower case 'i'.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\ni carry your heart with me(i carry it in\nmy heart)i am never without it(anywhere\ni go you go,my dear;and whatever is done\nby only me is your doing,my darling)\n i fear\nno fate(for you are my fate,my sweet)i want\nno world(for beautiful you are my world,my true)\nand it's you are whatever a moon has always meant\nand whatever a sun will always sing is you\nhere is the deepest secret nobody knows\n(here is the root of the root and the bud of the bud\nand the sky of the sky of a tree called life;which grows\nhigher than soul can hope or mind can hide)\nand this is the wonder that's keeping the stars apart\ni carry your heart(i carry it in my heart)\n&lt;/poem&gt;\nFrom \"i carry your heart with me(i carry it in\" (1952)\nWhile some of his poetry is free verse (and not beheld to rhyme or meter), Cureton has remarked that many of his sonnets follow an intricate rhyme scheme, and often employ pararhyme. A number of Cummings's poems feature his typographically exuberant style, with words, parts of words, or punctuation symbols scattered across the page, wherein Essert asserts \"feeling is first\" and the work begs to \"be re-read in order to be understood\"; Cummings, also a painter, created his texts not just as literature, but as \"visual objects\" on the page, and used typography to \"paint a picture\".\nThe seeds of Cummings's unconventional style appear well established even in his earliest work. At age six, he wrote to his father:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nFollowing his autobiographical novel, \"The Enormous Room\", Cummings's first published work was a collection of poems titled \"Tulips and Chimneys\" (1923). This early work already displayed Cummings's characteristically eccentric use of grammar and punctuation, although a fair number of the poems are written in conventional language.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nanyone lived in a pretty how town\nspring summer autumn winter\nhe sang his didn't he danced his did\nWomen and men (both little and small)\ncared for anyone not at all\nthey sowed their isn't they reaped their same\nsun moon stars rain\n&lt;/poem&gt;\nFrom \"anyone lived in a pretty how town\" (1940) \nCummings's works often do not follow the conventional rules that generate typical English sentences, or what Fairley identifies as \"ungrammar\". In addition, a number of Cummings's poems feature, in part or in whole, intentional misspellings, and several incorporate phonetic spellings intended to represent particular dialects. Cummings also employs what Fairley describes as \"morphological innovation\", wherein he frequently creates what critic Ian Landles calls: \"unusual compounds suggestive of 'a child's language'\" like \"'mud-luscious' and 'puddle-wonderful'\". Literary critic R. P. Blackmur has commented that this use of language is \"frequently unintelligible because [Cummings] disregards the historical accumulation of meaning in words in favor of merely private and personal associations\".\nFellow poet Edna St. Vincent Millay, in her equivocal letter recommending Cummings for the Guggenheim Fellowship he was awarded in 1934, expressed her frustration at his opaque symbolism. \"[I]f he prints and offers for sale poetry which he is quite content should be, after hours of sweating concentration, inexplicable from any point of view to a person as intelligent as myself, then he does so with a motive which is frivolous from the point of view of art, and should not be helped or encouraged by any serious person or group of persons... there is fine writing and powerful writing (as well as some of the most pompous nonsense I ever let slip to the floor with a wide yawn)... What I propose, then, is this: that you give Mr. Cummings enough rope. He may hang himself; or he may lasso a unicorn.\"\nCummings also wrote children's books and novels. A notable example of his versatility is an introduction he wrote for a collection of the comic strip \"Krazy Kat\".\nCummings included ethnic slurs in his writing, which proved controversial. In his 1950 collection \"Xaipe: Seventy-One Poems\", Cummings published two poems containing words that caused outrage in some quarters. Friedman considered these two poems to be \"condensed\" and \"cryptic\" parables, \"sparsely told\", in which setting the use of such \"inflammatory material\" was likely to meet with reader misapprehension. Poet William Carlos Williams spoke out in his defense.\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\u2014no. 24, from \"Xaipe\" (1950)\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\u2014no. 46, from \"Xaipe\" (1950)\nCummings biographer Catherine Reef notes of the controversy:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Friends begged Cummings to reconsider publishing these poems, and the book's editor pleaded with him to withdraw them, but he insisted that they stay. All the fuss perplexed him. The poems were commenting on prejudice, he pointed out, and not condoning it. He intended to show how derogatory words cause people to see others in terms of stereotypes rather than as individuals. \"America (which turns Hungarian into 'hunky' &amp; Irishman into 'mick' and Norwegian into 'square-head') is to blame for 'kike,'\" he said.\nPlays.\nDuring his lifetime, Cummings published four plays. \"HIM\", a three-act play, was first produced in 1928 by the Provincetown Players in New York City. The production was directed by James Light. The play's main characters are \"Him\", a playwright, portrayed by William Johnstone, and \"Me\", his girlfriend, portrayed by Erin O'Brien-Moore.\nCummings said of the unorthodox play:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Relax and give the play a chance to strut its stuff\u2014relax, stop wondering what it is all 'about'\u2014like many strange and familiar things, Life included, this play isn't 'about,' it simply is. ... Don't try to enjoy it, let it try to enjoy you. \"\n\"Anthropos, or the Future of Art\" is a short, one-act play that Cummings contributed to the anthology \"Whither, Whither or After Sex, What? A Symposium to End Symposium\". The play consists of dialogue between Man, the main character, and three \"infrahumans\", or inferior beings. The word \"anthropos\" is the Greek word for \"man\", in the sense of \"mankind\".\n\"Tom, A Ballet\" is a ballet based on \"Uncle Tom's Cabin\". The ballet is detailed in a \"synopsis\" as well as descriptions of four \"episodes\", which were published by Cummings in 1935. It remained unperformed until 2015.\n\"\" was probably Cummings's most successful play. It is an allegorical Christmas fantasy presented in one act of five scenes. The play was inspired by his daughter Nancy, with whom he was reunited in 1946. It was first published in the Harvard College magazine, \"Wake\". The play's main characters are Santa Claus, his family (Woman and Child), Death, and Mob. At the outset of the play, Santa Claus's family has disintegrated due to their lust for knowledge (Science). After a series of events, however, Santa Claus's faith in love and his rejection of the materialism and disappointment he associates with Science are reaffirmed, and he is reunited with Woman and Child.\nArt.\nCummings was an avid painter, referring to writing and painting as his &lt;q&gt;twin obsessions&lt;/q&gt; and to himself as a &lt;q&gt;poetandpainter.&lt;/q&gt; He painted &lt;q&gt;continuously, relentlessly, from childhood until his death, and left in his estate more than 1600 oils and watercolors (a figure that does not include the works he sold during his career) and over 9,000 drawings.&lt;/q&gt; In a self-interview from \"Foreword to an Exhibit: II\" (1945), the artist asked himself, &lt;q&gt;Tell me, doesn't your painting interfere with your writing?&lt;/q&gt; and answered, &lt;q&gt;Quite the contrary: they love each other dearly.&lt;/q&gt;\nCummings had more than 30 exhibits of his paintings in his lifetime. He received substantial acclaim as an American cubist and an abstract, avant garde painter between the World Wars, but with the publication of his books \"The Enormous Room\" and \"Tulips and Chimneys\" in the 1920s, his reputation as a poet eclipsed his success as a visual artist. In 1931, he published a limited edition volume of his artwork entitled \"CIOPW\", named for his media of charcoal, ink, oil, pencil, and watercolor. About this same time, he began to break from Modernist aesthetics and employ a more subjective and spontaneous style; his work became more representational: landscapes, nudes, still lifes, and portraits.\nName and capitalization.\nCummings's publishers and others have often echoed the unconventional orthography in his poetry by writing his name in lower case. Cummings himself used both the lowercase and capitalized versions, though he most often signed his name with capitals.\nThe use of lower case for his initials was popularized in part by the title of some books, particularly in the 1960s, printing his name in lower case on the cover and spine. In the preface to \"E. E. Cummings: The Growth of a Writer\" by Norman Friedman, critic Harry T. Moore notes Cummings \"had his name put legally into lower case, and in his later books the titles and his name were always in lower case\". According to Cummings's widow, however, this is incorrect. She wrote to Friedman: \"You should not have allowed H. Moore to make such a stupid &amp; childish statement about Cummings &amp; his signature.\" On February 27, 1951, Cummings wrote to his French translator D. Jon Grossman that he preferred the use of upper case for the particular edition they were working on. One Cummings scholar believes that on the rare occasions that Cummings signed his name in all lower case, he may have intended it as a gesture of humility, not as an indication that it was the preferred orthography for others to use. Additionally, \"The Chicago Manual of Style\", which prescribes favoring non-standard capitalization of names in accordance with the bearer's strongly stated preference, notes \"E. E. Cummings can be safely capitalized; it was one of his publishers, not he himself, who lowercased his name.\"\nAdaptations.\nIn 1943, modern dancer and choreographer, Jean Erdman presented \"The Transformations of Medusa, Forever and Sunsmell\" with a commissioned score by John Cage and a spoken text from the title poem by E. E. Cummings, sponsored by the Arts Club of Chicago. Erdman also choreographed \"Twenty Poems\" (1960), a cycle of E. E. Cummings's poems for eight dancers and one actor, with a commissioned score by Teiji Ito. It was performed in the round at the Circle in the Square Theatre in Greenwich Village.\nNumerous composers have set Cummings's poems to music:\nAwards.\nDuring his lifetime, Cummings received numerous awards in recognition of his work, including:\nReferences.\nPoems cited.\nFull text of poetry available at:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral and cited references.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9592", "revid": "6617018", "url": "https://en.wikipedia.org/wiki?curid=9592", "title": "East River", "text": "Navigable tidal strait in New York City\nThe East River is a saltwater tidal estuary or strait in New York City. The waterway, which is not a river despite its name, connects Upper New York Bay on its south end to Long Island Sound on its north end. It separates Long Island, with the boroughs of Brooklyn and Queens, from Manhattan Island, and from the Bronx on the North American mainland.\nBecause of its connection to Long Island Sound, the East River was once also known as the \"Sound River\". The tidal strait changes its direction of flow regularly, and is subject to strong fluctuations in its current, which are accentuated by its narrowness and variety of depths. The waterway is navigable for its entire length of , and was historically the center of maritime activities in the city.\nFormation and description.\nTechnically a drowned valley, like the other waterways around New York City, the strait was formed approximately 11,000 years ago at the end of the Wisconsin glaciation. The distinct change in the shape of the strait between the lower and upper portions is evidence of this glacial activity. The upper portion (from Long Island Sound to Hell Gate), running largely perpendicular to the glacial motion, is wide, meandering, and has deep narrow bays on both banks, scoured out by the glacier's movement. The lower portion (from Hell Gate to New York Bay) runs north\u2013south, parallel to the glacial motion. It is much narrower, with straight banks. The bays that exist, as well as those that used to exist before being filled in by human activity, are largely wide and shallow.\nThe section known as \"Hell Gate\"\u00a0\u2013 from the Dutch name meaning either \"bright strait\" or \"clear opening\", given to the entire river in 1614 by explorer Adriaen Block when he passed through it in his ship \"Tyger\"\u00a0\u2013 is a narrow, turbulent, and particularly treacherous stretch of the river. Tides from the Long Island Sound, New York Harbor and the Harlem River meet there, making it difficult to navigate, especially because of the number of rocky islets which once dotted it, with names such as \"Frying Pan\", \"Pot, Bread and Cheese\", \"Hen and Chicken\", \"Heel Top\"; \"Flood\"; and \"Gridiron\", roughly 12 islets and reefs in all, all of which led to a number of shipwrecks, including HMS \"Hussar\", a British frigate that sank in 1780 while supposedly carrying gold and silver intended to pay British troops. The stretch has since been cleared of rocks and widened. Washington Irving wrote of Hell Gate that the current sounded \"like a bull bellowing for more drink\" at half tide, while at full tide it slept \"as soundly as an alderman after dinner\". He said it was like \"a peaceable fellow enough when he has no liquor at all, or when he has a skinful, but who, when half-seas over, plays the very devil.\" The tidal regime is complex, with the two major tides\u00a0\u2013 from the Long Island Sound and from the Atlantic Ocean\u00a0\u2013 separated by about two hours; and this is without consideration of the tidal influence of the Harlem River, all of which creates a \"dangerous cataract\", as one ship's captain put it.\nThe river is navigable for its entire length of . In 1939 it was reported that the stretch from The Battery to the former Brooklyn Navy Yard near Wallabout Bay, a run of about , was deep, the long section from there, running to the west of Roosevelt Island, through Hell Gate and to Throg's Neck was at least deep, and then eastward from there the river was, at mean low tide, deep.\nThe broadness of the river's channel south of Roosevelt Island is caused by the dipping of the hardy Fordham gneiss underlying the island under the less strong Inwood marble which lies under the river bed. Why the river turns to the east as it approaches the three lower Manhattan bridges is geologically unknown.\nIslands.\nRoosevelt Island, a long () and narrow () landmass, lies in the stretch of the river between Manhattan Island and the borough of Queens, roughly from the latitude of Manhattan's East 46th to 86th Streets. The abrupt termination of the island on its north end is due to an extension of the 125th Street Fault. Politically, the island's constitute part of the borough of Manhattan. It is connected to Queens by the Roosevelt Island Bridge, to Manhattan by the Roosevelt Island Tramway, and to both boroughs by a subway station served by the F train. The Queensboro Bridge also runs across Roosevelt Island, and an elevator allowing both pedestrian and vehicular access to the island was added to the bridge in 1930, but elevator service was discontinued in 1955 following the opening of the Roosevelt Island Bridge, and the elevator was demolished in 1970. The island, which was formerly known as Blackwell's Island and Welfare Island before being renamed in honor of US President Franklin D. Roosevelt, historically served as the site of a penitentiary and a number of hospitals; today, it is dominated by residential neighborhoods consisting of large apartment buildings and parkland (much of which is dotted with the ruins of older structures).\nThe largest land mass in the River south of Roosevelt Island is U Thant Island, an artificial islet created during the construction of the Steinway Tunnel (which currently serves the subway's 7 and &lt;7&gt; lines). Officially named Belmont Island after one of the tunnel's financiers, the landmass owes its popular name (after Burmese diplomat U Thant, former Secretary-General of the United Nations) to the efforts of a group associated with the guru Sri Chinmoy that held mediation meetings on the island in the 1970s. Today, the island is owned by New York State and serves as a migratory bird sanctuary that is closed to visitors.\nProceeding north and east from Roosevelt Island, the River's principal islands include Manhattan's Mill Rock, an island located about 1000 feet from Manhattan's East 96th Street; Manhattan's 520-acre Randalls and Wards Islands, two formerly separate islands joined by landfill that are home to a large public park, a number of public institutions, and the supports for the Triborough and the Hell Gate Bridges; the Bronx's Rikers Island, once under but now over following extensive landfill expansion after the island's 1884 purchase by the city as a prison farm and still home to New York City's massive and controversial primary jail complex; and North and South Brother Islands, both of which also constitute part of the Bronx.\nTributaries.\nThe Bronx River, Pugsley Creek, and Westchester Creek drain into the northern bank of the East River in the northern section of the strait. The Flushing River, historically known as Flushing Creek, empties into the strait's southern bank near LaGuardia Airport via Flushing Bay. Further west, Luyster Creek drains into the East River in Astoria, Queens.\nNorth of Randalls Island, it is joined by the Bronx Kill. Along the east of Wards Island, at approximately the strait's midpoint, it narrows into a channel called Hell Gate, which is spanned by both the Robert F. Kennedy Bridge (formerly the Triborough), and the Hell Gate Bridge. On the south side of Wards Island, it is joined by the Harlem River.\nNewtown Creek on Long Island, which itself contained several tributaries, drains into the East River and forms part of the boundary between Queens and Brooklyn. Bushwick Inlet and Wallabout Bay on Long Island also drain into the strait on the Long Island side. The Gowanus Canal was built from Gowanus Creek, which emptied into the river.\nHistorically, there were other small streams which emptied into the river, though these and their associated wetlands have been filled in and built over. These small streams included the Harlem Creek, one of the most significant tributaries originating in Manhattan. Other streams that emptied into the East River included the Sawkill in Manhattan, Mill Brook in the Bronx, and Sunswick Creek in Queens.\nHistory.\nPrior to the arrival of Europeans, the land north of the East River was occupied by the Siwanoys, one of many groups of Algonquin-speaking Lenapes in the area. Those of the Lenapes who lived in the northern part of Manhattan Island in a campsite known as Konaande Kongh used a landing at around the current location of East 119th street to paddle into the river in canoes fashioned from tree trunks in order to fish.\nDutch settlement of what became New Amsterdam began in 1623. Some of the earliest of the small settlements in the area were along the west bank of the East River on sites that had previously been Native American settlements. As with the Native Americans, the river was central to their lives for transportation for trading and for fishing. They gathered marsh grass to feed their cattle, and the East River's tides helped to power mills which ground grain to flour. By 1642 there was a ferry running on the river between Manhattan Island and what is now Brooklyn, and the first pier on the river was built in 1647 at Pearl and Broad Streets. After the British took over the colony in 1664, which was renamed \"New York\", the development of the waterfront continued, and a shipbuilding industry grew up once New York started exporting flour. By the end of the 17th century, the Great Dock, located at Corlear's Hook on the East River, had been built.\nNarrowing the river.\nHistorically, the lower portion of the strait, which separates Manhattan from Brooklyn, was one of the busiest and most important channels in the world, particularly during the first three centuries of New York City's history. Because the water along the lower Manhattan shoreline was too shallow for large boats to tie up and unload their goods, from 1686 on\u00a0\u2013 after the signing of the Dongan Charter, which allowed intertidal land to be owned and sold\u00a0\u2013 the shoreline was \"wharfed out\" to the high-water mark by constructing retaining walls that were filled in with every conceivable kind of landfill: excrement, dead animals, ships deliberately sunk in place, ship ballast, and muck dredged from the bottom of the river. On the new land were built warehouses and other structures necessary for the burgeoning sea trade. Many of the \"water-lot\" grants went to the rich and powerful families of the merchant class, although some went to tradesmen. By 1700, the Manhattan bank of the river had been \"wharfed-out\" up to around Whitehall Street, narrowing the strait of the river.\nAfter the signing of the Montgomerie Charter in the late 1720s, another 127 acres of land along the Manhattan shore of the East River was authorized to be filled-in, this time to a point 400 feet beyond the low-water mark; the parts that had already been expanded to the low water mark\u00a0\u2013 much of which had been devastated by a coastal storm in the early 1720s and a nor'easter in 1723\u00a0\u2013 were also expanded, narrowing the channel even further. What had been quiet beach land was to become new streets and buildings, and the core of the city's sea-borne trade. This infilling went as far north as Corlear's Hook. In addition, the city was given control of the western shore of the river from Wallabout Bay south.\nAmerican Revolution.\nExpansion of the waterfront halted during the American Revolution, in which the East River played an important role early in the conflict. On August 28, 1776, while British and Hessian troops rested after besting the Americans at the Battle of Long Island, General George Washington was rounding up all the boats on the east shore of the river, in what is now Brooklyn, and used them to successfully move his troops across the river\u00a0\u2013 under cover of night, rain, and fog\u00a0\u2013 to Manhattan island, before the British could press their advantage. Thus, though the battle was a victory for the British, the failure of Sir William Howe to destroy the Continental Army when he had the opportunity allowed the Americans to continue fighting. Without the stealthy withdrawal across the East River, the American Revolution might have ended much earlier.\nWallabout Bay on the River was the site of most of the British prison ships\u00a0\u2013 most notoriously \u00a0\u2013 where thousands of American prisoners of war were held in terrible conditions. These prisoners had come into the hands of the British after the fall of New York City on September 15, 1776, after the American loss at the Battle of Long Island and the loss of Fort Washington on November 16. Prisoners began to be housed on the broken-down warships and transports in December; about 24 ships were used in total, but generally only 5 or 6 at a time. Almost twice as many Americans died from neglect in these ships than did from all the battles in the war: as many as 12,000 soldiers, sailors and civilians. The bodies were thrown overboard or were buried in shallow graves on the riverbanks, but their bones\u00a0\u2013 some of which were collected when they washed ashore\u00a0\u2013 were later relocated and are now inside the Prison Ship Martyrs' Monument in nearby Fort Greene Park. The existence of the ships and the conditions the men were held in was widely known at the time through letters, diaries and memoirs, and was a factor not only in the attitude of Americans toward the British, but in the negotiations to formally end the war.\nDevelopment begins again.\nAfter the war, East River waterfront development continued once more. New York State legislation, which in 1807 had authorized what would become the Commissioners Plan of 1811, authorized the creation of new land out to 400 feet from the low water mark into the river, and with the advent of gridded streets along the new waterline\u00a0\u2013 Joseph Mangin had laid out such a grid in 1803 in his \"A Plan and Regulation of the City of New York\", which was rejected by the city, but established the concept\u00a0\u2013 the coastline become regularized at the same time that the strait became even narrower.\nOne result of the narrowing of the East River along the shoreline of Manhattan and, later, Brooklyn\u00a0\u2013 which continued until the mid-19th century when the state put a stop to it\u00a0\u2013 was an increase in the speed of its current. Buttermilk Channel, the strait that divides Governors Island from Red Hook in Brooklyn, and which is located directly south of the \"mouth\" of the East River, was in the early 17th century a fordable waterway across which cattle could be driven. Further investigation by Colonel Jonathan Williams determined that the channel was by 1776 three fathoms deep (), five fathoms deep () in the same spot by 1798, and when surveyed by Williams in 1807 had deepened to 7 fathoms () at low tide. What had been almost a bridge between two landforms that were once connected had become a fully navigable channel, thanks to the constriction of the East River and the increased flow it caused. Soon, the current in the East River had become so strong that larger ships had to use auxiliary steam power in order to turn. The continued narrowing of the channel on both side may have been the reasoning behind the suggestion of one New York State Senator, who wanted to fill in the East River and annex Brooklyn, with the cost of doing so being covered by selling the newly made land. Others proposed a dam at Roosevelt Island (then Blackwell's Island) to create a wet basin for shipping.\nFilling in the river.\nFilling in part of the river was also proposed in 1867 by engineer James E. Serrell, later a city surveyor, but with emphasis on solving the problem of Hell Gate. Serrell proposed filling in Hell Gate and building a \"New East River\" through Queens with an extension to Westchester County. Serrell's plan\u00a0\u2013 which he publicized with maps, essay and lectures as well as presentations to the city, state and federal governments\u00a0\u2013 would have filled in the river from 14th Street to 125th Street. The New East River through Queens would be about three times the average width of the existing one at an even throughout, and would run as straight as an arrow for . The new land, and the portions of Queens which would become part of Manhattan, adding , would be covered with an extension of the existing street grid of Manhattan.\nVariations on Serrell's plan would be floated over the years. A pseudonymous \"Terra Firma\" brought up filling in the East River again in the \"Evening Post\" and \"Scientific American\" in 1904, and Thomas Alva Edison took it up in 1906. Then Thomas Kennard Thompson, a bridge and railway engineer, proposed in 1913 to fill in the river from Hell Gate to the tip of Manhattan and, as Serrell had suggested, make a new canalized East River, only this time from Flushing Bay to Jamaica Bay. He would also expand Brooklyn into the Upper Harbor, put up a dam from Brooklyn to Staten Island, and make extensive landfill in the Lower Bay. At around the same time, in the 1920s, John A. Harriss, New York City's chief traffic engineer, who had developed the first traffic signals in the city, also had plans for the river. Harriss wanted to dam the East River at Hell Gate and the Williamsburg Bridge, then remove the water, put a roof over it on stilts, and build boulevards and pedestrian lanes on the roof along with \"majestic structures\", with transportation services below. The East River's course would, once again, be shifted to run through Queens, and this time Brooklyn as well, to channel it to the Harbor.\nClearing Hell Gate.\nPeriodically, merchants and other interested parties would try to get something done about the difficulty of navigating through Hell Gate. In 1832, the New York State legislature was presented with a petition for a canal to be built through nearby Hallet's Point, thus avoiding Hell Gate altogether. Instead, the legislature responded by providing ships with pilots trained to navigate the shoals for the next 15 years.\nIn 1849, a French engineer whose specialty was underwater blasting, Benjamin Maillefert, had cleared some of the rocks which, along with the mix of tides, made the Hell Gate stretch of the river so dangerous to navigate. Ebenezer Meriam had organized a subscription to pay Maillefert $6,000 to, for instance, reduce \"Pot Rock\" to provide of depth at low-mean water. While ships continued to run aground (in the 1850s about 2% of ships did so) and petitions continued to call for action, the federal government undertook surveys of the area which ended in 1851 with a detailed and accurate map. By then Maillefert had cleared the rock \"Baldheaded Billy\", and it was reported that Pot Rock had been reduced to , which encouraged the United States Congress to appropriate $20,000 for further clearing of the strait. However, a more accurate survey showed that the depth of Pot Rock was actually a little more than , and eventually Congress withdrew its funding.\nWith the main shipping channels through The Narrows into the harbor silting up with sand due to littoral drift, thus providing ships with less depth, and a new generation of larger ships coming online\u00a0\u2013 epitomized by Isambard Kingdom Brunel's SS \"Great Eastern\", popularly known as \"Leviathan\"\u00a0\u2013 New York began to be concerned that it would start to lose its status as a great port if a \"back door\" entrance into the harbor was not created. In the 1850s the depth continued to lessen\u00a0\u2013 the harbor commission said in 1850 that the mean water low was and the extreme water low was \u00a0\u2013 while the draft required by the new ships continued to increase, meaning it was only safe for them to enter the harbor at high tide.\nThe U.S. Congress, realizing that the problem needed to be addressed, appropriated $20,000 for the Army Corps of Engineers to continue Maillefert's work. In 1851, the U.S. Army Corps of Engineers, \"under Lt. Bartlett of the Army Corps of Engineers\", began to do the job, in an operation which was to span 70 years. The appropriated money was soon spent without appreciable change in the hazards of navigating the strait. An advisory council recommended in 1856 that the strait be cleared of all obstacles, but nothing was done, and the Civil War soon broke out.\nAfter the Civil War.\nIn the late 1860s, after the Civil War, Congress realized the military importance of having easily navigable waterways, and charged the Army Corps of Engineers with clearing Hell Gate. Newton estimated that the operation would cost about half as much as the annual losses in shipping. The 2021 book by Thomas Barthel titled \"Opening the East River: John Newton and the Blasting of Hell Gate\" traces Newton's work on this project from 1866 to 1885. On September 24, 1876, the Corps used of explosives to blast the rocks, which was followed by further blasting. The process was started by excavating under Hallets reef from Astoria. Cornish miners, assisted by steam drills, dug galleries under the reef, which were then interconnected. They later drilled holes for explosives. A patent was issued for the detonating device. After the explosion, the rock debris was dredged and dropped into a deep part of the river. This was not repeated at the later Flood Rock explosion.\nOn October 10, 1885, the Corps carried out the largest explosion in this process, annihilating Flood Rock with of explosives. The blast was felt as far away as Princeton, New Jersey (50 miles). It sent a geyser of water in the air. The blast has been described as \"the largest planned explosion before testing began for the atomic bomb\", although the detonation at the Battle of Messines in 1917 was larger. Some of the rubble from the detonation was used in 1890 to fill the gap between Great Mill Rock and Little Mill Rock, merging the two islands into a single island, Mill Rock.\nAt the same time that Hell Gate was being cleared, the Harlem River Ship Canal was being planned. When it was completed in 1895, the \"back door\" to New York's center of ship-borne trade in the docks and warehouses of the East River was open from two directions, through the cleared East River, and from the Hudson River through the Harlem River to the East River. Ironically, though, while both forks of the northern shipping entrance to the city were now open, modern dredging techniques had cut through the sandbars of the Atlantic Ocean entrance, allowing new, even larger ships to use that traditional passage into New York's docks.\nAt the beginning of the 19th century, the East River was the center of New York's shipping industry, but by the end of the century, much of it had moved to the Hudson River, leaving the East River wharves and slips to begin a long process of decay, until the area was finally rehabilitated in the mid-1960s, and the South Street Seaport Museum was opened in 1967.\nA new seawall.\nBy 1870, the condition of the Port of New York along both the East and Hudson Rivers had so deteriorated that the New York State legislature created the Department of Docks to renovate the port and keep New York competitive with other ports on the American East Coast. The Department of Docks was given the task of creating the master plan for the waterfront, and General George B. McClellan was engaged to head the project. McClellan held public hearings and invited plans to be submitted, ultimately receiving 70 of them, although in the end he and his successors put his own plan into effect. That plan called for the building of a seawall around Manhattan island from West 61st Street on the Hudson, around The Battery, and up to East 51st Street on the East River. The area behind the masonry wall (mostly concrete but in some parts granite blocks) would be filled in with landfill, and wide streets would be laid down on the new land. In this way, a new edge for the island (or at least the part of it used as a commercial port) would be created.\nThe department had surveyed of shoreline by 1878, as well as documenting the currents and tides. By 1900, had been surveyed and core samples had been taken to inform the builders of how deep the bedrock was. The work was completed just as World War I began, allowing the Port of New York to be a major point of embarkation for troops and materiel.\nThe new seawall helps protect Manhattan island from storm surges, although it is only above the mean sea level, so that particularly dangerous storms, such as the nor'easter of 1992 and Hurricane Sandy in 2012, which hit the city in a way to create surges which are much higher, can still do significant damage. (The Hurricane of September 3, 1821, created the biggest storm surge on record in New York City: a rise of in one hour at the Battery, flooding all of lower Manhattan up to Canal Street.) Still, the new seawall begun in 1871 gave the island a firmer edge, improved the quality of the port, and continues to protect Manhattan from normal storm surges.\nBridges and tunnels.\nThe Brooklyn Bridge, completed in 1883, was the first bridge to span the East River, connecting the cities of New York and Brooklyn, and all but replacing the frequent ferry service between them, which did not return until the late 20th century. The bridge offered cable car service across the span. The Brooklyn Bridge was followed by the Williamsburg Bridge (1903), the Queensboro Bridge (1909), the Manhattan Bridge (1912) and the Hell Gate Railroad Bridge (1916). Later would come the Triborough Bridge (1936), the Bronx-Whitestone Bridge (1939), the Throgs Neck Bridge (1961) and the Rikers Island Bridge (1966). In addition, numerous rail tunnels pass under the East River\u00a0\u2013 most of them part of the New York City Subway system\u00a0\u2013 as does the Brooklyn-Battery Tunnel and the Queens-Midtown Tunnel. (See Crossings below for details.) Also under the river is Water Tunnel #1 of the New York City water supply system, built in 1917 to extend the Manhattan portion of the tunnel to Brooklyn, and via City Tunnel #2 (1936) to Queens; these boroughs became part of New York City after the city's consolidation in 1898. City Tunnel #3 will also run under the river, under the northern tip of Roosevelt Island, and is expected to not be completed until at least 2026; the Manhattan portion of the tunnel went into service in 2013.\n20th century.\nPhilanthropist John D. Rockefeller founded what is now Rockefeller University in 1901, between 63rd and 64th Streets on the river side of York Avenue, overlooking the river. The university is a research university for doctoral and post-doctoral scholars, primarily in the fields of medicine and biological science. North of it is one of the major medical centers in the city, NewYork Presbyterian / Weill Cornell Medical Center, which is associated with the medical schools of both Columbia University and Cornell University. Although it can trace its history back to 1771, the center on York Avenue, much of which overlooks the river, was built in 1932.\nThe East River was the site of one of the greatest disasters in the history of New York City when, in June 1904, the PS \"General Slocum\" sank near North Brother Island due to a fire. It was carrying 1,400 German-Americans to a picnic site on Long Island for an annual outing. There were only 321 survivors of the disaster, one of the worst losses of life in the city's long history, and a devastating blow to the Little Germany neighborhood on the Lower East Side. The captain of the ship and the managers of the company that owned it were indicted, but only the captain was convicted; he spent &lt;templatestyles src=\"Fraction/styles.css\" /&gt;3+1\u20442 years of his 10-year sentence at Sing Sing Prison before being released by a federal parole board, and then pardoned by President William Howard Taft.\nBeginning in 1934, and then again from 1948 to 1966, the Manhattan shore of the river became the location for the limited-access East River Drive, which was later renamed after Franklin Delano Roosevelt, and is universally known by New Yorkers as the \"FDR Drive\". The road is sometimes at grade, sometimes runs under locations such as the site of the Headquarters of the United Nations and Carl Schurz Park and Gracie Mansion\u00a0\u2013 the mayor's official residence, and is at time double-decked, because Hell Gate provides no room for more landfill. It begins at Battery Park, runs past the Brooklyn, Manhattan, Williamsburg and Queensboro Bridges, and the Ward's Island Footbridge, and terminates just before the Robert F. Kennedy Triboro Bridge when it connects to the Harlem River Drive. Between most of the FDR Drive and the River is the East River Greenway, part of the Manhattan Waterfront Greenway. The East River Greenway was primarily built in connection with the building of the FDR Drive, although some portions were built as recently as 2002, and other sections are still incomplete.\nIn 1963, Con Edison built the Ravenswood Generating Station on the Long Island City shore of the river, on land some of which was once stone quarries which provided granite and marble slabs for Manhattan's buildings. The plant has since been owned by KeySpan. National Grid and TransCanada, the result of deregulation of the electrical power industry. The station, which can generate about 20% of the electrical needs of New York City\u00a0\u2013 approximately 2,500 megawatts\u00a0\u2013 receives some of its fuel by oil barge.\nNorth of the power plant can be found Socrates Sculpture Park, an illegal dumpsite and abandoned landfill that in 1986 was turned into an outdoor museum, exhibition space for artists, and public park by sculptor Mark di Suvero and local activists. The area also contains Rainey Park, which honors Thomas C. Rainey, who attempted for 40 years to get a bridge built in that location from Manhattan to Queens. The Queensboro Bridge was eventually built south of this location.\n21st century.\nIn 2011, NY Waterway started operating its East River Ferry line. The route was a 7-stop East River service that runs in a loop between East 34th Street and Hunters Point, making two intermediate stops in Brooklyn and three in Queens. The ferry, an alternative to the New York City Subway, cost $4 per one-way ticket. It was instantly popular: from June to November 2011, the ferry saw 350,000 riders, over 250% of the initial ridership forecast of 134,000 riders. In December 2016, in preparation for the start of NYC Ferry service the next year, Hornblower Cruises purchased the rights to operate the East River Ferry. NYC Ferry started service on May 1, 2017, with the East River Ferry as part of the system.\nIn February 2012 the federal government announced an agreement with Verdant Power to install 30 tidal turbines in the channel of the East River. The turbines were projected to begin operations in 2015 and are supposed to produce 1.05 megawatts of power. The strength of the current foiled an earlier effort in 2007 to tap the river for tidal power.\nOn May 7, 2017, the catastrophic failure of a Con Edison substation in Brooklyn caused a spill into the river of over of dielectric fluid, a synthetic mineral oil used to cool electrical equipment and prevent electrical discharges. (See below.)\nAt the end of 2022, gold miner John Reeves claimed that up to 50 tons of ice age artifacts bound for the American Museum of Natural History, including mammoth remains, had been dumped into the East River near 65th Street. Although the museum denied that any fossils had been dumped into the river, Reeves's allegations prompted commercial divers to search the river for evidence of mammoth bones.\nEcosystem collapse, pollution and health.\nThroughout most of the history of New York City, and New Amsterdam before it, the East River has been the receptacle for the city's garbage and sewage. \"Night men\" who collected \"night soil\" from outdoor privies would dump their loads into the river, and even after the construction of the Croton Aqueduct (1842) and then the New Croton Aqueduct (1890) gave rise to indoor plumbing, the waste that was flushed away into the sewers, where it mixed with ground runoff, ran directly into the river, untreated. The sewers terminated at the slips where ships docked, until the waste began to build up, preventing dockage, after which the outfalls were moved to the end of the piers. The \"landfill\" which created new land along the shoreline when the river was \"wharfed out\" by the sale of \"water lots\" was largely garbage such as bones, offal, and even whole dead animals, along with excrement\u00a0\u2013 human and animal. The result was that by the 1850s, if not before, the East River, like the other waterways around the city, was undergoing the process of eutrophication where the increase in nitrogen from excrement and other sources led to a decrease in free oxygen, which in turn led to an increase in phytoplankton such as algae and a decrease in other life forms, breaking the area's established food chain. The East River became very polluted, and its animal life decreased drastically.\nIn an earlier time, one person had described the transparency of the water: \"I remember the time, gentlemen, when you could go in twelve feet of water and you could see the pebbles on the bottom of this river.\" As the water got more polluted, it darkened, underwater vegetation (such as photosynthesizing seagrass) began dying, and as the seagrass beds declined, the many associated species of their ecosystems declined as well, contributing to the decline of the river. Also harmful was the general destruction of the once plentiful oyster beds in the waters around the city, and the over-fishing of menhaden, or mossbunker, a small silvery fish which had been used since the time of the Native Americans for fertilizing crops\u00a0\u2013 however it took 8,000 of these schooling fish to fertilize a single acre, so mechanized fishing using the purse seine was developed, and eventually the menhaden population collapsed. Menhaden feed on phytoplankton, helping to keep them in check, and are also a vital step in the food chain, as bluefish, striped bass and other fish species which do not eat phytoplankton feed on the menhaden. The oyster is another filter feeder: oysters purify 10 to 100 gallons a day, while each menhaden filters four gallons in a minute, and their schools were immense: one report had a farmer collecting 20 oxcarts worth of menhaden using simple fishing nets deployed from the shore. The combination of more sewage, due to the availability of more potable water\u00a0\u2013 New York's water consumption \"per capita\" was twice that of Europe\u00a0\u2013 indoor plumbing, the destruction of filter feeders, and the collapse of the food chain, damaged the ecosystem of the waters around New York, including the East River, almost beyond repair.\nBecause of these changes to the ecosystem, by 1909, the level of dissolved-oxygen in the lower part of the river had declined to less than 65%, where 55% of saturation is the point at which the amount of fish and the number of their species begins to be affected. Only 17 years later, by 1926, the level of dissolved oxygen in the river had fallen to 13%, below the point at which most fish species can survive.\nDue to heavy pollution, the East River is dangerous to people who fall in or attempt to swim in it, although as of mid-2007 the water was cleaner than it had been in decades. As of 2010[ [update]], the New York City Department of Environmental Protection (DEP) categorizes the East River as Use Classification I, meaning it is safe for secondary contact activities such as boating and fishing. According to the marine sciences section of the DEP, the channel is swift, with water moving as fast as four knots, just as it does in the Hudson River on the other side of Manhattan. That speed can push casual swimmers out to sea. A few people drown in the waters around New York City each year.\nAs of 2013[ [update]], it was reported that the level of bacteria in the river was below federal guidelines for swimming on most days, although the readings may vary significantly, so that the outflow from Newtown Creek or the Gowanus Canal can be tens or hundreds of times higher than recommended, according to Riverkeeper, a non-profit environmentalist advocacy group. The counts are also higher along the shores of the strait than they are in the middle of its flow. Nevertheless, the \"Brooklyn Bridge Swim\" is an annual event where swimmers cross the channel from Brooklyn Bridge Park to Manhattan.\nThanks to reductions in pollution, cleanups, the restriction of development, and other environmental controls, the East River along Manhattan is one of the areas of New York's waterways\u00a0\u2013 including the Hudson-Raritan Estuary and both shores of Long Island\u00a0\u2013 which have shown signs of the return of biodiversity. On the other hand, the river is also under attack from hardy, competitive, alien species, such as the European green crab, which is considered to be one of the world's ten worst invasive species, and is present in the river.\n2017 oil spill.\nOn May 7, 2017, the catastrophic failure of Con Edison's Farragut Substation at 89 John Street in Dumbo, Brooklyn, caused a spill of dielectric fluid\u00a0\u2013 an insoluble synthetic mineral oil, considered non-toxic by New York state, used to cool electrical equipment and prevent electrical discharges\u00a0\u2013 into the East River from a tank. The National Response Center received a report of the spill at 1:30pm that day, although the public did not learn of the spill for two days, and then only from tweets from NYC Ferry. A \"safety zone\" was established, extending from a line drawn between Dupont Street in Greenpoint, Brooklyn, to East 25th Street in Kips Bay, Manhattan, south to Buttermilk Channel. Recreational and human-powered vehicles such as kayaks and paddleboards were banned from the zone while the oil was being cleaned up, and the speed of commercial vehicles restricted so as not to spread the oil in their wakes, causing delays in NYC Ferry service. The clean-up efforts were being undertaken by Con Edison personnel and private environmental contractors, the U.S. Coast Guard, and the New York State Department of Environmental Conservation, with the assistance of NYC Emergency Management.\nThe loss of the sub-station caused a voltage dip in the power provided by Con Ed to the Metropolitan Transportation Authority's New York City Subway system, which disrupted its signals.\nThe Coast Guard estimated that of oil spilled into the water, with the remainder soaking into the soil at the substation. In the past the Coast Guard has on average been able to recover about 10% of oil spilled, however the complex tides in the river make the recovery much more difficult, with the turbulent water caused by the river's change of tides pushing contaminated water over the containment booms, where it is then carried out to sea and cannot be recovered. By Friday May 12, officials from Con Edison reported that almost had been taken out of the water.\nEnvironmental damage to wildlife is expected to be less than if the spill was of petroleum-based oil, but the oil can still block the sunlight necessary for the river's fish and other organisms to live. Nesting birds are also in possible danger from the oil contaminating their nests and potentially poisoning the birds or their eggs. Water from the East River was reported to have tested positive for low levels of PCB, a known carcinogen.\nPutting the spill into perspective, John Lipscomb, the vice president of advocacy for Riverkeepers said that the chronic release after heavy rains of overflow from city's wastewater treatment system was \"a bigger problem for the harbor than this accident.\" The state Department of Environmental Conservation is investigating the spill. It was later reported that according to DEC data which dates back to 1978, the substation involved had spilled 179 times previously, more than any other Con Ed facility. The spills have included 8,400 gallons of dielectric oil, hydraulic oil, and antifreeze which leaked at various times into the soil around the substation, the sewers, and the East River.\nOn June 22, Con Edison used non-toxic green dye and divers in the river to find the source of the leak. As a result, a hole was plugged. The utility continued to believe that the bulk of the spill went into the ground around the substation, and excavated and removed several hundred cubic yards of soil from the area. They estimated that about went into the river, of which were recovered. Con Edison said that it installed a new transformer, and intended to add new barrier around the facility to help guard against future spills propagating into the river.\nReferences.\nhttps://www.nan.usace.army.mil/Portals/37/docs/history/hellgate.pdf.\nInformational notes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography"}
{"id": "9593", "revid": "36878010", "url": "https://en.wikipedia.org/wiki?curid=9593", "title": "Existentialism", "text": "Philosophy dealing with absurdity of existence\nExistentialism is a family of philosophical views and inquiry that explore the human individual's struggle to lead an authentic life despite the apparent absurdity or incomprehensibility of existence. In examining meaning, purpose, and value, existentialist thought often includes concepts such as existential crises, angst, courage, and freedom.\nExistentialism is associated with several 19th- and 20th-century European philosophers who shared an emphasis on the human subject, despite often profound differences in thought. Among the 19th-century figures now associated with existentialism are philosophers S\u00f8ren Kierkegaard and Friedrich Nietzsche, as well as novelist Fyodor Dostoevsky, all of whom critiqued rationalism and concerned themselves with the problem of meaning. The word \"existentialism\", however, was not coined until the mid 20th century, during which it became most associated with contemporaneous philosophers Jean-Paul Sartre, Martin Heidegger, Simone de Beauvoir, Karl Jaspers, Gabriel Marcel, Paul Tillich, and more controversially Albert Camus. \nMany existentialists considered traditional systematic or academic philosophies, in style and content, to be too abstract and removed from concrete human experience. A primary virtue in existentialist thought is authenticity. Existentialism would influence many disciplines outside of philosophy, including theology, drama, art, literature, and psychology.\nExistentialist philosophy encompasses a range of perspectives, but it shares certain underlying concepts. Among these, a central tenet of existentialism is that personal freedom, individual responsibility, and deliberate choice are essential to the pursuit of self-discovery and the determination of life's meaning.\nEtymology.\nThe term \"existentialism\" () was coined by the French Catholic philosopher Gabriel Marcel in the mid-1940s. When Marcel first applied the term to Jean-Paul Sartre, at a colloquium in 1945, Sartre rejected it. Sartre subsequently changed his mind and, on October 29, 1945, publicly adopted the existentialist label in a lecture to the Club Maintenant in Paris, published as (\"Existentialism Is a Humanism\"), a short book that helped popularize existentialist thought. Marcel later came to reject the label himself in favour of \"Neo-Socratic\", in honor of Kierkegaard's essay \"On the Concept of Irony\".\nSome scholars argue that the term should be used to refer only to the cultural movement in Europe in the 1940s and 1950s associated with the works of the philosophers Sartre, Simone de Beauvoir, Maurice Merleau-Ponty, and Albert Camus. Others extend the term to Kierkegaard, and yet others extend it as far back as Socrates. However, it is often identified with the philosophical views of Sartre.\nDefinitional issues and background.\nThe labels \"existentialism\" and \"existentialist\" are often seen as historical conveniences in as much as they were first applied to many philosophers long after they had died. While existentialism is generally considered to have originated with Kierkegaard, the first prominent existentialist philosopher to adopt the term as a self-description was Sartre. Sartre posits the idea that \"what all existentialists have in common is the fundamental doctrine that existence precedes essence\", as the philosopher Frederick Copleston explains. According to philosopher Steven Crowell, defining existentialism has been relatively difficult, and he argues that it is better understood as a general approach used to reject certain systematic philosophies rather than as a systematic philosophy itself. In a lecture delivered in 1945, Sartre described existentialism as \"the attempt to draw all the consequences from a position of consistent atheism\". For others, existentialism need not involve the rejection of God, but rather \"examines mortal man's search for meaning in a meaningless universe\", considering less \"What is the good life?\" (to feel, be, or do, good), instead asking \"What is life good for?\".\nAlthough many outside Scandinavia consider the term existentialism to have originated from Kierkegaard, it is more likely that Kierkegaard adopted this term (or at least the term \"existential\" as a description of his philosophy) from the Norwegian poet and literary critic Johan Sebastian Cammermeyer Welhaven. This assertion comes from two sources:\nConcepts.\nExistence precedes essence.\nSartre argued that a central proposition of existentialism is that existence precedes essence, which is to say that individuals shape themselves by existing and cannot be perceived through preconceived and \"a priori\" categories, an \"essence\". The actual life of the individual is what constitutes what could be called their \"true essence\" instead of an arbitrarily attributed essence others use to define them. Human beings, through their own consciousness, create their own values and determine a meaning to their life. This view is in contradiction to Aristotle and Aquinas, who taught that essence precedes individual existence. Although it was Sartre who explicitly coined the phrase, similar notions can be found in the thought of existentialist philosophers such as Heidegger, and Kierkegaard:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The subjective \"thinker's form\", the form of his communication, is his \"style\". His form must be just as manifold as are the opposites that he holds together. The systematic \"eins, zwei, drei\" is an abstract form that also must inevitably run into trouble whenever it is to be applied to the concrete. To the same degree as the subjective thinker is concrete, to that same degree his form must also be concretely dialectical. But just as he himself is not a poet, not an ethicist, not a dialectician, so also his form is none of these directly. His form must first and last be related to existence, and in this regard he must have at his disposal the poetic, the ethical, the dialectical, the religious. Subordinate character, setting, etc., which belong to the well-balanced character of the esthetic production, are in themselves breadth; the subjective thinker has only one setting\u2014existence\u2014and has nothing to do with localities and such things. The setting is not the fairyland of the imagination, where poetry produces consummation, nor is the setting laid in England, and historical accuracy is not a concern. The setting is inwardness in existing as a human being; the concretion is the relation of the existence-categories to one another. Historical accuracy and historical actuality are breadth.\u2014\u200a\nSome interpret the imperative to define oneself as meaning that anyone can wish to be anything. However, an existentialist philosopher would say such a wish constitutes an inauthentic existence \u2013 what Sartre would call \"bad faith\". Instead, the phrase should be taken to say that people are defined only insofar as they act and that they are responsible for their actions. Someone who acts cruelly towards other people is, by that act, defined as a cruel person. Such persons are themselves responsible for their new identity (cruel persons). This is opposed to their genes, or \"human nature\", bearing the blame.\nAs Sartre said in his lecture \"Existentialism is a Humanism\": \"Man first of all exists, encounters himself, surges up in the world\u2014and defines himself afterwards.\" The more positive, therapeutic aspect of this is also implied: a person can choose to act in a different way, and to be a good person instead of a cruel person.\nJonathan Webber interprets Sartre's usage of the term \"essence\" not in a modal fashion, i.e. as necessary features, but in a teleological fashion: \"an essence is the relational property of having a set of parts ordered in such a way as to collectively perform some activity\". For example, it belongs to the essence of a house to keep the bad weather out, which is why it has walls and a roof. Humans are different from houses because\u2014unlike houses\u2014they do not have an inbuilt purpose: they are free to \"choose\" their own purpose and thereby shape their essence; thus, \"their existence precedes their essence\".\nSartre is committed to a radical conception of freedom: nothing fixes our purpose but we ourselves, our projects have no weight or inertia except for our endorsement of them. Simone de Beauvoir, on the other hand, holds that there are various factors, grouped together under the term \"sedimentation\", that offer resistance to attempts to change our direction in life. \"Sedimentations\" are themselves products of past choices and can be changed by choosing differently in the present, but such changes happen slowly. They are a force of inertia that shapes the agent's evaluative outlook on the world until the transition is complete.\nSartre's definition of existentialism was based on Heidegger's magnum opus \"Being and Time\" (1927). In the correspondence with Jean Beaufret later published as the \"Letter on Humanism\", Heidegger implied that Sartre misunderstood him for his own purposes of subjectivism, and that he did not mean that actions take precedence over being so long as those actions were not reflected upon. Heidegger commented that \"the reversal of a metaphysical statement remains a metaphysical statement\", meaning that he thought Sartre had simply switched the roles traditionally attributed to essence and existence without interrogating these concepts and their history.\nThe absurd.\nThe notion of the absurd contains the idea that there is no meaning in the world beyond what meaning we give it. This meaninglessness also encompasses the amorality or \"unfairness\" of the world. This can be highlighted in the way it opposes the traditional Abrahamic religious perspective, which establishes that life's purpose is the fulfillment of God's commandments. To live the life of the absurd means rejecting a life that finds or pursues specific meaning for man's existence since there is nothing to be discovered. According to Albert Camus, the world or the human being is not in itself absurd. The concept only emerges through the juxtaposition of the two; life becomes absurd due to the incompatibility between human beings and the world they inhabit. This view constitutes one of the two interpretations of the absurd in existentialist literature. The second view, first elaborated by S\u00f8ren Kierkegaard, holds that absurdity is limited to actions and choices of human beings. These are considered absurd since they issue from human freedom, undermining their foundation outside of themselves.\nThe absurd contrasts with the claim that \"bad things don't happen to good people\"; to the world, metaphorically speaking, there is no such thing as a good person or a bad person; what happens happens, and it may just as well happen to a \"good\" person as to a \"bad\" person. Because of the world's absurdity, anything can happen to anyone at any time and a tragic event could plummet someone into direct confrontation with the absurd. Many of the literary works of Kierkegaard, Beckett, Kafka, Dostoevsky, Ionesco, Miguel de Unamuno, Luigi Pirandello, Sartre, Joseph Heller, and Camus contain descriptions of people who encounter the absurdity of the world.\nIt is because of the devastating awareness of meaninglessness that Camus claimed in \"The Myth of Sisyphus\" that \"There is only one truly serious philosophical problem, and that is suicide.\" Although \"prescriptions\" against the possible deleterious consequences of these kinds of encounters vary, from Kierkegaard's religious \"stage\" to Camus' insistence on persevering in spite of absurdity, the concern with helping people avoid living their lives in ways that put them in the perpetual danger of having everything meaningful break down is common to most existentialist philosophers. The possibility of having everything meaningful break down poses a threat of quietism, which is inherently against the existentialist philosophy. It has been said that the possibility of suicide makes all humans existentialists. The ultimate hero of absurdism lives without meaning and faces suicide without succumbing to it.\nFacticity.\nFacticity is defined by Sartre in \"Being and Nothingness\" (1943) as the \"in-itself\", which for humans takes the form of being and not being. It is the facts of one's personal life and as per Heidegger, it is \"the way in which we are thrown into the world.\" This can be more easily understood when considering facticity in relation to a person's past: one's past forms the person who exists in the present. However, to reduce a person to their past would ignore the change a person undergoes in the present and future, while saying that one's past is only what one was, would entirely detach it from the present self. A denial of one's concrete past constitutes an inauthentic lifestyle, and also applies to other kinds of facticity (having a human body with all its limitations, identity, values, etc.).\nFacticity is a limitation and a condition of freedom. It is a limitation in that a large part of a person's facticity consists of things they did not choose (birthplace, etc.), but a condition of freedom in the sense that one's values most likely depend on these factors. However, even though one's facticity is fixed, it cannot determine a person: they may choose to assign as much value to their facticity as they choose. As an example, consider two men, one of whom has no memory of his past and the other who remembers everything. Both have committed many crimes, but the first man, remembering nothing, leads a rather normal life while the second man, feeling trapped by his past, continues a life of crime, blaming his own past. There is nothing essential about his committing crimes, but he ascribes this meaning to his past.\nHowever, to disregard one's facticity during the evolution of one's sense of self would be a denial of the conditions shaping the present self and would be inauthentic. An example of the focus solely on possible projects without reflecting on one's current facticity would be continually thinking about future possibilities related to being rich (e.g. a better car, bigger house, better quality of life, etc.) without acknowledging the facticity of \"not currently having the financial means to do so\". In this example, considering both facticity and transcendence, an authentic mode of being would be considering future projects that might improve one's current finances (e.g. putting in extra hours, or investing savings) in order to arrive at a real future, or \"future-facticity\" of a modest pay rise, further leading to purchase of an affordable car.\nAnother aspect of facticity is that it entails angst. Freedom \"produces\" angst when limited by facticity and the lack of the possibility of having facticity to \"step in\" and take responsibility for something one has done also produces angst.\nAnother aspect of existential freedom is that one can change one's values. One is responsible for one's values, regardless of society's values. The focus on freedom in existentialism is related to the limits of responsibility one bears, as a result of one's freedom. The relationship between freedom and responsibility is one of interdependency and a clarification of freedom also clarifies that for which one is responsible.\nAuthenticity.\nMany noted existentialists consider the theme of authentic existence important. Authenticity involves the idea that one has to \"create oneself\" and live in accordance with this self. For an authentic existence, one should act as oneself, not as \"one's acts\" or as \"one's genes\" or as any other essence requires. The authentic act is one in accordance with one's freedom. A component of freedom is facticity, but not to the degree that this facticity determines one's transcendent choices (one could then blame one's background for making the choice one made [chosen project, from one's transcendence]). Facticity, in relation to authenticity, involves acting on one's actual values when making a choice (instead of, like Kierkegaard's Aesthete, \"choosing\" randomly), so that one takes responsibility for the act instead of choosing either-or without allowing the options to have different values.\nIn contrast, the inauthentic is the denial to live in accordance with one's freedom. This can take many forms, from pretending choices are meaningless or random, convincing oneself that some form of determinism is true, or \"mimicry\" where one acts as \"one should\".\nHow one \"should\" act is often determined by an image one has, of how one in such a role (bank manager, lion tamer, sex worker, etc.) acts. In \"Being and Nothingness\", Sartre uses the example of a waiter in \"bad faith\". He merely takes part in the \"act\" of being a typical waiter, albeit very convincingly. This image usually corresponds to a social norm, but this does not mean that all acting in accordance with social norms is inauthentic. The main point is the attitude one takes to one's own freedom and responsibility and the extent to which one acts in accordance with this freedom.\nThe Other and the Look.\nThe Other (written with a capital \"O\") is a concept more properly belonging to phenomenology and its account of intersubjectivity. However, it has seen widespread use in existentialist writings, and the conclusions drawn differ slightly from the phenomenological accounts. The Other is the experience of another free subject who inhabits the same world as a person does. In its most basic form, it is this experience of the Other that constitutes intersubjectivity and objectivity. To clarify, when one experiences someone else, and this Other person experiences the world (the same world that a person experiences)\u2014only from \"over there\"\u2014the world is constituted as objective in that it is something that is \"there\" as identical for both of the subjects; a person experiences the other person as experiencing the same things. This experience of the Other's look is what is termed the Look (sometimes the Gaze).\nWhile this experience, in its basic phenomenological sense, constitutes the world as objective and oneself as objectively existing subjectivity (one experiences oneself as seen in the Other's Look in precisely the same way that one experiences the Other as seen by him, as subjectivity), in existentialism, it also acts as a kind of limitation of freedom. This is because the Look tends to objectify what it sees. When one experiences oneself in the Look, one does not experience oneself as nothing (no thing), but as something (some thing). In Sartre's example of a man peeping at someone through a keyhole, the man is entirely caught up in the situation he is in. He is in a pre-reflexive state where his entire consciousness is directed at what goes on in the room. Suddenly, he hears a creaking floorboard behind him and he becomes aware of himself as seen by the Other. He is then filled with shame for he perceives himself as he would perceive someone else doing what he was doing\u2014as a Peeping Tom. For Sartre, this phenomenological experience of shame establishes proof for the existence of other minds and defeats the problem of solipsism. For the conscious state of shame to be experienced, one has to become aware of oneself as an object of another look, proving a priori, that other minds exist. The Look is then co-constitutive of one's facticity.\nAnother characteristic feature of the Look is that no Other really needs to have been there: It is possible that the creaking floorboard was simply the movement of an old house; the Look is not some kind of mystical telepathic experience of the actual way the Other sees one (there may have been someone there, but he could have not noticed that person). It is only one's perception of the way another might perceive him.\nAngst and dread.\n\"Existential angst\", sometimes called existential dread, anxiety, or anguish, is a term common to many existentialist thinkers. It is generally held to be a negative feeling arising from the experience of human freedom and responsibility. The archetypal example is the experience one has when standing on a cliff where one not only fears falling off it, but also dreads the possibility of throwing oneself off. In this experience that \"nothing is holding me back\", one senses the lack of anything that predetermines one to either throw oneself off or to stand still, and one experiences one's own freedom.\nIt can also be seen in relation to the previous point how angst is before nothing, and this is what sets it apart from fear that has an object. While one can take measures to remove an object of fear, for angst no such \"constructive\" measures are possible. The use of the word \"nothing\" in this context relates to the inherent insecurity about the consequences of one's actions and to the fact that, in experiencing freedom as angst, one also realizes that one is fully responsible for these consequences. There is nothing in people (genetically, for instance) that acts in their stead\u2014that they can blame if something goes wrong. Therefore, not every choice is perceived as having dreadful possible consequences (and, it can be claimed, human lives would be unbearable if every choice facilitated dread). However, this does not change the fact that freedom remains a condition of every action.\nDespair.\nDespair is generally defined as a loss of hope. In existentialism, it is more specifically a loss of hope in reaction to a breakdown in one or more of the defining qualities of one's self or identity. If a person is invested in being a particular thing, such as a bus driver or an upstanding citizen, and then finds their being-thing compromised, they would normally be found in a state of despair\u2014a hopeless state. For example, a singer who loses the ability to sing may despair if they have nothing else to fall back on\u2014nothing to rely on for their identity. They find themselves unable to be what defined their being.\nWhat sets the existentialist notion of despair apart from the conventional definition is that existentialist despair is a state one is in even when they are not overtly in despair. So long as a person's identity depends on qualities that can crumble, they are in perpetual despair\u2014and as there is, in Sartrean terms, no human essence found in conventional reality on which to constitute the individual's sense of identity, despair is a universal human condition. As Kierkegaard defines it in \"Either/Or\": \"Let each one learn what he can; both of us can learn that a person's unhappiness never lies in his lack of control over external conditions, since this would only make him completely unhappy.\" In \"Works of Love\", he says:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;When the God-forsaken worldliness of earthly life shuts itself in complacency, the confined air develops poison, the moment gets stuck and stands still, the prospect is lost, a need is felt for a refreshing, enlivening breeze to cleanse the air and dispel the poisonous vapors lest we suffocate in worldliness. ... Lovingly to hope all things is the opposite of despairingly to hope nothing at all. Love hopes all things\u2014yet is never put to shame. To relate oneself expectantly to the possibility of the good is to hope. To relate oneself expectantly to the possibility of evil is to fear. By the decision to choose hope one decides infinitely more than it seems, because it is an eternal decision.\u2014\u200a\nOpposition to positivism and rationalism.\nExistentialists oppose defining human beings as primarily rational, and, therefore, oppose both positivism and rationalism. Existentialism asserts that people make decisions based on subjective meaning rather than pure rationality. \nThe rejection of reason as the source of meaning is a common theme of existentialist thought, as is the focus on the anxiety and dread that we feel in the face of our own radical free will and our awareness of death. Kierkegaard advocated rationality as a means to interact with the objective world (e.g., in the natural sciences), but when it comes to existential problems, reason is insufficient: \"Human reason has boundaries\".\nLike Kierkegaard, Sartre saw problems with rationality, calling it a form of \"bad faith\", an attempt by the self to impose structure on a world of phenomena\u2014\"the Other\"\u2014that is fundamentally irrational and random. According to Sartre, rationality and other forms of bad faith hinder people from finding meaning in freedom. To try to suppress feelings of anxiety and dread, people confine themselves within everyday experience, Sartre asserted, thereby relinquishing their freedom and acquiescing to being possessed in one form or another by \"the Look\" of \"the Other\" (i.e., possessed by another person\u2014or at least one's idea of that other person).\nReligion.\nAn existentialist reading of the Bible would demand that the reader recognize that they are an existing subject studying the words more as a recollection of events. This is in contrast to looking at a collection of \"truths\" that are outside and unrelated to the reader, but may develop a sense of reality/God. Such a reader is not obligated to follow the commandments as if an external agent is forcing these commandments upon them, but as though they are inside them and guiding them from inside. This is the task Kierkegaard takes up when he asks: \"Who has the more difficult task: the teacher who lectures on earnest things a meteor's distance from everyday life\u2014or the learner who should put it to use?\" Philosophers such as Hans Jonas and Rudolph Bultmann introduced the concept of existentialist demythologization into the field of early Christianity and Christian theology, respectively.\nConfusion with nihilism.\nAlthough nihilism and existentialism are distinct philosophies, they are often confused with one another since both are rooted in the human experience of anguish and confusion that stems from the apparent meaninglessness of a world in which humans are compelled to find or create meaning. A primary cause of confusion is that Friedrich Nietzsche was an important philosopher in both fields.\nExistentialist philosophers often stress the importance of angst as signifying the absolute lack of any objective ground for action, a move that is often reduced to moral or existential nihilism. A pervasive theme in existentialist philosophy, however, is to persist through encounters with the absurd, as seen in Albert Camus's philosophical essay \"The Myth of Sisyphus\" (1942): \"One must imagine Sisyphus happy\". It is only very rarely that existentialist philosophers dismiss morality or one's self-created meaning: S\u00f8ren Kierkegaard regained a sort of morality in the religious (although he would not agree that it was ethical; the religious suspends the ethical), and Jean-Paul Sartre's final words in \"Being and Nothingness\" (1943): \"All these questions, which refer us to a pure and not an accessory (or impure) reflection, can find their reply only on the ethical plane. We shall devote to them a future work.\"\nHistory.\nPrecursors.\nSome have argued that existentialism has long been an element of European religious thought, even before the term came into use. William Barrett identified Blaise Pascal and S\u00f8ren Kierkegaard as two specific examples. Jean Wahl also identified William Shakespeare's Prince Hamlet (\"To be, or not to be\"), Jules Lequier, Thomas Carlyle, and William James as existentialists. According to Wahl, \"the origins of most great philosophies, like those of Plato, Descartes, and Kant, are to be found in existential reflections.\" Precursors to existentialism can also be identified in the works of Iranian Muslim philosopher Mulla Sadra (c. 1571\u20131635), who would posit that \"existence precedes essence\" becoming the principle expositor of the School of Isfahan, which is described as \"alive and active\".\n19th century.\nKierkegaard and Nietzsche.\nKierkegaard is generally considered to have been the first existentialist philosopher. He proposed that each individual\u2014not reason, society, or religious orthodoxy\u2014is solely tasked with giving meaning to life and living it sincerely, or \"authentically\".\nKierkegaard and Nietzsche were two of the first philosophers considered fundamental to the existentialist movement, though neither used the term \"existentialism\" and it is unclear whether they would have supported the existentialism of the 20th century. They focused on subjective human experience rather than the objective truths of mathematics and science, which they believed were too detached or observational to truly get at the human experience. Like Pascal, they were interested in people's quiet struggle with the apparent meaninglessness of life and the use of diversion to escape from boredom. Unlike Pascal, Kierkegaard and Nietzsche also considered the role of making free choices, particularly regarding fundamental values and beliefs, and how such choices change the nature and identity of the chooser. Kierkegaard's knight of faith and Nietzsche's \u00dcbermensch are representative of people who exhibit freedom, in that they define the nature of their own existence. Nietzsche's idealized individual invents his own values and creates the very terms they excel under. By contrast, Kierkegaard, opposed to the level of abstraction in Hegel, and not nearly as hostile (actually welcoming) to Christianity as Nietzsche, argues through a pseudonym that the objective certainty of religious truths (specifically Christian) is not only impossible, but even founded on logical paradoxes. Yet he continues to imply that a leap of faith is a possible means for an individual to reach a higher stage of existence that transcends and contains both an aesthetic and ethical value of life. Kierkegaard and Nietzsche were also precursors to other intellectual movements, including postmodernism, and various strands of psychotherapy. However, Kierkegaard believed that individuals should live in accordance with their thinking.\nIn \"Twilight of the Idols\", Nietzsche's sentiments resonate the idea of \"existence precedes essence.\" He writes, \"no one \"gives\" man his qualities-- neither God, nor society, nor his parents and ancestors, nor he himself...No one is responsible for man's being there at all, for his being such-and-such, or for his being in these circumstances or in this environment...Man is not the effect of some special purpose of a will, and end...\" Within this view, Nietzsche ties in his rejection of the existence of God, which he sees as a means to \"redeem the world.\" By rejecting the existence of God, Nietzsche also rejects beliefs that claim humans have a predestined purpose according to what God has instructed.\nDostoyevsky.\nThe first important literary author also important to existentialism was the Russian, Dostoyevsky. Dostoyevsky's \"Notes from Underground\" portrays a man unable to fit into society and unhappy with the identities he creates for himself. Sartre, in his book on existentialism \"Existentialism is a Humanism\", quoted Dostoyevsky's \"The Brothers Karamazov\" as an example of existential crisis. Other Dostoyevsky novels covered issues raised in existentialist philosophy while presenting story lines divergent from secular existentialism: for example, in \"Crime and Punishment\", the protagonist Raskolnikov experiences an existential crisis and then moves toward a Christian Orthodox worldview similar to that advocated by Dostoyevsky himself.\nEarly 20th century.\nIn the first decades of the 20th century, a number of philosophers and writers explored existentialist ideas. The Spanish philosopher Miguel de Unamuno y Jugo, in his 1913 book \"The Tragic Sense of Life in Men and Nations\", emphasized the life of \"flesh and bone\" as opposed to that of abstract rationalism. Unamuno rejected systematic philosophy in favor of the individual's quest for faith. He retained a sense of the tragic, even absurd nature of the quest, symbolized by his enduring interest in the eponymous character from the Miguel de Cervantes novel \"Don Quixote\". A novelist, poet and dramatist as well as philosophy professor at the University of Salamanca, Unamuno wrote a short story about a priest's crisis of faith, \"Saint Manuel the Good, Martyr\", which has been collected in anthologies of existentialist fiction. Another Spanish thinker, Jos\u00e9 Ortega y Gasset, writing in 1914, held that human existence must always be defined as the individual person combined with the concrete circumstances of his life: \"Yo soy yo y mi circunstancia\" (\"I am myself and my circumstances\"). Sartre likewise believed that human existence is not an abstract matter, but is always situated (\"en situation\").\nAlthough Martin Buber wrote his major philosophical works in German, and studied and taught at the Universities of Berlin and Frankfurt, he stands apart from the mainstream of German philosophy. Born into a Jewish family in Vienna in 1878, he was also a scholar of Jewish culture and involved at various times in Zionism and Hasidism. In 1938, he moved permanently to Jerusalem. His best-known philosophical work was the short book \"I and Thou\", published in 1922. For Buber, the fundamental fact of human existence, too readily overlooked by scientific rationalism and abstract philosophical thought, is \"man with man\", a dialogue that takes place in the so-called \"sphere of between\" (\"das Zwischenmenschliche\").\nTwo Russian philosophers, Lev Shestov and Nikolai Berdyaev, became well known as existentialist thinkers during their post-Revolutionary exiles in Paris. Shestov had launched an attack on rationalism and systematization in philosophy as early as 1905 in his book of aphorisms \"All Things Are Possible\". Berdyaev drew a radical distinction between the world of spirit and the everyday world of objects. Human freedom, for Berdyaev, is rooted in the realm of spirit, a realm independent of scientific notions of causation. To the extent the individual human being lives in the objective world, he is estranged from authentic spiritual freedom. \"Man\" is not to be interpreted naturalistically, but as a being created in God's image, an originator of free, creative acts. He published a major work on these themes, \"The Destiny of Man\", in 1931.\nGabriel Marcel, long before coining the term \"existentialism\", introduced important existentialist themes to a French audience in his early essay \"Existence and Objectivity\" (1925) and in his \"Metaphysical Journal\" (1927). A dramatist as well as a philosopher, Marcel found his philosophical starting point in a condition of metaphysical alienation: the human individual searching for harmony in a transient life. Harmony, for Marcel, was to be sought through \"secondary reflection\", a \"dialogical\" rather than \"dialectical\" approach to the world, characterized by \"wonder and astonishment\" and open to the \"presence\" of other people and of God rather than merely to \"information\" about them. For Marcel, such presence implied more than simply being there (as one thing might be in the presence of another thing); it connoted \"extravagant\" availability, and the willingness to put oneself at the disposal of the other.\nMarcel contrasted \"secondary reflection\" with abstract, scientific-technical \"primary reflection\", which he associated with the activity of the abstract Cartesian ego. For Marcel, philosophy was a concrete activity undertaken by a sensing, feeling human being incarnate\u2014embodied\u2014in a concrete world. Although Sartre adopted the term \"existentialism\" for his own philosophy in the 1940s, Marcel's thought has been described as \"almost diametrically opposed\" to that of Sartre. Unlike Sartre, Marcel was a Christian, and became a Catholic convert in 1929.\nIn Germany, the psychiatrist and philosopher Karl Jaspers\u2014who later described existentialism as a \"phantom\" created by the public\u2014called his own thought, heavily influenced by Kierkegaard and Nietzsche, \"Existenzphilosophie\". For Jaspers, \"\"Existenz\"-philosophy is the way of thought by means of which man seeks to become himself...This way of thought does not cognize objects, but elucidates and makes actual the being of the thinker\".\nJaspers, a professor at the university of Heidelberg, was acquainted with Heidegger, who held a professorship at Marburg before acceding to Husserl's chair at Freiburg in 1928. They held many philosophical discussions, but later became estranged over Heidegger's support of National Socialism. They shared an admiration for Kierkegaard, and in the 1930s, Heidegger lectured extensively on Nietzsche. Nevertheless, the extent to which Heidegger should be considered an existentialist is debatable. In \"Being and Time\" he presented a method of rooting philosophical explanations in human existence (\"Dasein\") to be analysed in terms of existential categories (\"existentiale\"); and this has led many commentators to treat him as an important figure in the existentialist movement.\nAfter the Second World War.\nFollowing the Second World War, existentialism became a well-known and significant philosophical and cultural movement, mainly through the public prominence of two French writers, Jean-Paul Sartre and Albert Camus, who wrote best-selling novels, plays and widely read journalism as well as theoretical texts. These years also saw the growing reputation of \"Being and Time\" outside Germany.\nSartre dealt with existentialist themes in his 1938 novel \"Nausea\" and the short stories in his 1939 collection \"The Wall\", and had published his treatise on existentialism, \"Being and Nothingness\", in 1943, but it was in the two years following the liberation of Paris from the German occupying forces that he and his close associates\u2014Camus, Simone de Beauvoir, Maurice Merleau-Ponty, and others\u2014became internationally famous as the leading figures of a movement known as existentialism. In a very short period of time, Camus and Sartre in particular became the leading public intellectuals of post-war France, achieving by the end of 1945 \"a fame that reached across all audiences.\" Camus was an editor of the most popular leftist (former French Resistance) newspaper \"Combat\"; Sartre launched his journal of leftist thought, \"Les Temps Modernes\", and two weeks later gave the widely reported lecture on existentialism and secular humanism to a packed meeting of the Club Maintenant. Beauvoir wrote that \"not a week passed without the newspapers discussing us\"; existentialism became \"the first media craze of the postwar era.\"\nBy the end of 1947, Camus' earlier fiction and plays had been reprinted, his new play \"Caligula\" had been performed and his novel \"The Plague\" published; the first two novels of Sartre's \"The Roads to Freedom\" trilogy had appeared, as had Beauvoir's novel \"The Blood of Others\". Works by Camus and Sartre were already appearing in foreign editions. The Paris-based existentialists had become famous.\nSartre had traveled to Germany in 1930 to study the phenomenology of Edmund Husserl and Martin Heidegger, and he included critical comments on their work in his major treatise \"Being and Nothingness\". Heidegger's thought had also become known in French philosophical circles through its use by Alexandre Koj\u00e8ve in explicating Hegel in a series of lectures given in Paris in the 1930s. The lectures were highly influential; members of the audience included not only Sartre and Merleau-Ponty, but Raymond Queneau, Georges Bataille, Louis Althusser, Andr\u00e9 Breton, and Jacques Lacan. A selection from \"Being and Time\" was published in French in 1938, and his essays began to appear in French philosophy journals.\nHeidegger read Sartre's work and was initially impressed, commenting: \"Here for the first time I encountered an independent thinker who, from the foundations up, has experienced the area out of which I think. Your work shows such an immediate comprehension of my philosophy as I have never before encountered.\" Later, however, in response to a question posed by his French follower Jean Beaufret, Heidegger distanced himself from Sartre's position and existentialism in general in his \"Letter on Humanism\". Heidegger's reputation continued to grow in France during the 1950s and 1960s. In the 1960s, Sartre attempted to reconcile existentialism and Marxism in his work \"Critique of Dialectical Reason\". A major theme throughout his writings was freedom and responsibility.\nCamus was a friend of Sartre, until their falling-out, and wrote several works with existential themes including \"The Rebel\", \"Summer in Algiers\", \"The Myth of Sisyphus\", and \"The Stranger\", the latter being \"considered\u2014to what would have been Camus's irritation\u2014the exemplary existentialist novel.\" Camus, like many others, rejected the existentialist label, and considered his works concerned with facing the absurd. In the titular book, Camus uses the analogy of the Greek myth of Sisyphus to demonstrate the futility of existence. In the myth, Sisyphus is condemned for eternity to roll a rock up a hill, but when he reaches the summit, the rock will roll to the bottom again. Camus believes that this existence is pointless but that Sisyphus ultimately finds meaning and purpose in his task, simply by continually applying himself to it. The first half of the book contains an extended rebuttal of what Camus took to be existentialist philosophy in the works of Kierkegaard, Shestov, Heidegger, and Jaspers.\nSimone de Beauvoir, an important existentialist who spent much of her life as Sartre's partner, wrote about feminist existentialist ethics in her works, including \"The Second Sex\" and \"The Ethics of Ambiguity\". Although often overlooked due to her relationship with Sartre, de Beauvoir integrated existentialism with other forms of thinking such as feminism, unheard of at the time, resulting in alienation from fellow writers such as Camus.\nPaul Tillich, an important existentialist theologian following Kierkegaard and Karl Barth, applied existentialist concepts to Christian theology, and helped introduce existential theology to the general public. His seminal work \"The Courage to Be\" follows Kierkegaard's analysis of anxiety and life's absurdity, but puts forward the thesis that modern humans must, via God, achieve selfhood in spite of life's absurdity. Rudolf Bultmann used Kierkegaard's and Heidegger's philosophy of existence to demythologize Christianity by interpreting Christian mythical concepts into existentialist concepts.\nMaurice Merleau-Ponty, an existential phenomenologist, was for a time a companion of Sartre. Merleau-Ponty's \"Phenomenology of Perception\" (1945) was recognized as a major statement of French existentialism. It has been said that Merleau-Ponty's work \"Humanism and Terror\" greatly influenced Sartre. However, in later years they were to disagree irreparably, dividing many existentialists such as de Beauvoir, who sided with Sartre.\nColin Wilson, an English writer, published his study \"The Outsider\" in 1956, initially to critical acclaim. In this book and others (e.g. \"Introduction to the New Existentialism\"), he attempted to reinvigorate what he perceived as a pessimistic philosophy and bring it to a wider audience. He was not, however, academically trained, and his work was attacked by professional philosophers for lack of rigor and critical standards.\nInfluence outside philosophy.\nArt.\nFilm and television.\nStanley Kubrick's 1957 anti-war film \"Paths of Glory\" \"illustrates, and even illuminates...existentialism\" by examining the \"necessary absurdity of the human condition\" and the \"horror of war\". The film tells the story of a fictional World War I French army regiment ordered to attack an impregnable German stronghold; when the attack fails, three soldiers are chosen at random, court-martialed by a \"kangaroo court\", and executed by firing squad. The film examines existentialist ethics, such as the issue of whether objectivity is possible and the \"problem of authenticity\". Orson Welles's 1962 film \"The Trial\", based upon Franz Kafka's book of the same name (\"Der Proze\u00df\"), is characteristic of both existentialist and absurdist themes in its depiction of a man (Joseph K.) arrested for a crime for which the charges are neither revealed to him nor to the reader.\n\"Neon Genesis Evangelion\" is a Japanese science fiction animation series created by the anime studio Gainax and was both directed and written by Hideaki Anno. Existential themes of individuality, consciousness, freedom, choice, and responsibility are heavily relied upon throughout the entire series, particularly through the philosophies of Jean-Paul Sartre and S\u00f8ren Kierkegaard. Episode 16's title, is a reference to Kierkegaard's book, \"The Sickness Unto Death\".\nSome contemporary films dealing with existentialist issues include \"Melancholia\", \"Fight Club\", \"I Heart Huckabees\", \"Waking Life\", \"The Matrix\", \"Ordinary People\", \"Life in a Day\", \"Barbie\", and \"Everything Everywhere All at Once\". Likewise, films throughout the 20th century such as \"The Seventh Seal\", \"Ikiru\", \"Taxi Driver\", the \"Toy Story\" films, \"\", \"The Great Silence\", \"Ghost in the Shell\", \"Harold and Maude\", \"High Noon\", \"Easy Rider\", \"One Flew Over the Cuckoo's Nest\", \"A Clockwork Orange\", \"Groundhog Day\", \"Apocalypse Now\", \"Badlands\", and \"Blade Runner\" also have existentialist qualities.\nNotable directors known for their existentialist films include Ingmar Bergman, Bela Tarr, Robert Bresson, Jean-Pierre Melville, Fran\u00e7ois Truffaut, Jean-Luc Godard, Michelangelo Antonioni, Akira Kurosawa, Terrence Malick, Stanley Kubrick, Andrei Tarkovsky, \u00c9ric Rohmer, Wes Anderson, Woody Allen, and Christopher Nolan. Charlie Kaufman's \"Synecdoche, New York\" focuses on the protagonist's desire to find existential meaning. Similarly, in Kurosawa's \"Red Beard\", the protagonist's experiences as an intern in a rural health clinic in Japan lead him to an existential crisis whereby he questions his reason for being. This, in turn, leads him to a better understanding of humanity. The French film, \"Mood Indigo\" (directed by Michel Gondry) embraced various elements of existentialism. The film \"The Shawshank Redemption\", released in 1994, depicts life in a prison in Maine, United States to explore several existentialist concepts.\nLiterature.\nExistential perspectives are also found in modern literature to varying degrees, especially since the 1920s. Louis-Ferdinand C\u00e9line's \"Journey to the End of the Night\" (\"Voyage au bout de la nuit\", 1932) celebrated by both Sartre and Beauvoir, contained many of the themes that would be found in later existential literature, and is in some ways, the proto-existential novel. Jean-Paul Sartre's 1938 novel \"Nausea\" was \"steeped in Existential ideas\", and is considered an accessible way of grasping his philosophical stance. Between 1900 and 1960, other authors such as Albert Camus, Franz Kafka, Rainer Maria Rilke, T. S. Eliot, Yukio Mishima, Hermann Hesse, Luigi Pirandello, Ralph Ellison, and Jack Kerouac composed literature or poetry that contained, to varying degrees, elements of existential or proto-existential thought. The philosophy's influence even reached pulp literature shortly after the turn of the 20th century, as seen in the existential disparity witnessed in Man's lack of control of his fate in the works of H. P. Lovecraft.\nTheatre.\nSartre wrote \"No Exit\" in 1944, an existentialist play originally published in French as \"Huis Clos\" (meaning \"In Camera\" or \"behind closed doors\"), which is the source of the popular quote, \"Hell is other people.\" (In French, \"L'enfer, c'est les autres\"). The play begins with a Valet leading a man into a room that the audience soon realizes is in hell. Eventually he is joined by two women. After their entry, the Valet leaves and the door is shut and locked. All three expect to be tortured, but no torturer arrives. Instead, they realize they are there to torture each other, which they do effectively by probing each other's sins, desires, and unpleasant memories.\nExistentialist themes are displayed in the Theatre of the Absurd, notably in Samuel Beckett's \"Waiting for Godot\", in which two men divert themselves while they wait expectantly for someone (or something) named Godot who never arrives. They claim Godot is an acquaintance, but in fact, hardly know him, admitting they would not recognize him if they saw him. Samuel Beckett, once asked who or what Godot is, replied, \"If I knew, I would have said so in the play.\" To occupy themselves, the men eat, sleep, talk, argue, sing, play games, exercise, swap hats, and contemplate suicide\u2014anything \"to hold the terrible silence at bay\". The play \"exploits several archetypal forms and situations, all of which lend themselves to both comedy and pathos.\" The play also illustrates an attitude toward human experience on earth: the poignancy, oppression, camaraderie, hope, corruption, and bewilderment of human experience that can be reconciled only in the mind and art of the absurdist. The play examines questions such as death, the meaning of human existence and the place of God in human existence.\nTom Stoppard's \"Rosencrantz &amp; Guildenstern Are Dead\" is an absurdist tragicomedy first staged at the Edinburgh Festival Fringe in 1966. The play expands upon the exploits of two minor characters from Shakespeare's \"Hamlet\". Comparisons have also been drawn to Samuel Beckett's \"Waiting for Godot\", for the presence of two central characters who appear almost as two halves of a single character. Many plot features are similar as well: the characters pass time by playing Questions, impersonating other characters, and interrupting each other or remaining silent for long periods of time. The two characters are portrayed as two clowns or fools in a world beyond their understanding. They stumble through philosophical arguments while not realizing the implications, and muse on the irrationality and randomness of the world.\nJean Anouilh's \"Antigone\" also presents arguments founded on existentialist ideas. It is a tragedy inspired by Greek mythology and the play of the same name (\"Antigone\", by Sophocles) from the fifth century BC. In English, it is often distinguished from its antecedent by being pronounced in its original French form, approximately \"Ante-G\u014cN.\" The play was first performed in Paris on 6 February 1944, during the Nazi occupation of France. Produced under Nazi censorship, the play is purposefully ambiguous with regards to the rejection of authority (represented by Antigone) and the acceptance of it (represented by Creon). The parallels to the French Resistance and the Nazi occupation have been drawn. Antigone rejects life as desperately meaningless but without affirmatively choosing a noble death. The crux of the play is the lengthy dialogue concerning the nature of power, fate, and choice, during which Antigone says that she is, \"...\u00a0disgusted with [the]...promise of a humdrum happiness.\" She states that she would rather die than live a mediocre existence.\nCritic Martin Esslin in his book \"Theatre of the Absurd\" pointed out how many contemporary playwrights such as Samuel Beckett, Eug\u00e8ne Ionesco, Jean Genet, and Arthur Adamov wove into their plays the existentialist belief that we are absurd beings loose in a universe empty of real meaning. Esslin noted that many of these playwrights demonstrated the philosophy better than did the plays by Sartre and Camus. Though most of such playwrights, subsequently labeled \"Absurdist\" (based on Esslin's book), denied affiliations with existentialism and were often staunchly anti-philosophical (for example Ionesco often claimed he identified more with 'Pataphysics or with Surrealism than with existentialism), the playwrights are often linked to existentialism based on Esslin's observation.\nActivism.\nBlack existentialism explores the existence and experiences of Black people in the world. Classical and contemporary thinkers include C.L.R James, Frederick Douglass, W.E.B DuBois, Frantz Fanon, Angela Davis, Cornel West, Naomi Zack, bell hooks, Stuart Hall, Lewis Gordon, and Audre Lorde.\nPsychoanalysis and psychotherapy.\nA major offshoot of existentialism as a philosophy is existentialist psychology and psychoanalysis, which first crystallized in the work of Otto Rank, Freud's closest associate for 20 years. Without awareness of the writings of Rank, Ludwig Binswanger was influenced by Freud, Edmund Husserl, Heidegger, and Sartre. A later figure was Viktor Frankl, who briefly met Freud as a young man. His logotherapy can be regarded as a form of existentialist therapy. The existentialists would also influence social psychology, antipositivist micro-sociology, symbolic interactionism, and post-structuralism, with the work of thinkers such as Georg Simmel and Michel Foucault. Foucault was a great reader of Kierkegaard even though he almost never refers to this author, who nonetheless had for him an importance as secret as it was decisive.\nAn early contributor to existentialist psychology in the United States was Rollo May, who was strongly influenced by Kierkegaard and Otto Rank. One of the most prolific writers on techniques and theory of existentialist psychology in the US is Irvin D. Yalom. Yalom states that\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Aside from their reaction against Freud's mechanistic, deterministic model of the mind and their assumption of a phenomenological approach in therapy, the existentialist analysts have little in common and have never been regarded as a cohesive ideological school. These thinkers\u2014who include Ludwig Binswanger, Medard Boss, Eug\u00e8ne Minkowski, V. E. Gebsattel, Roland Kuhn, G. Caruso, F. T. Buytendijk, G. Bally, and Victor Frankl\u2014were almost entirely unknown to the American psychotherapeutic community until Rollo May's highly influential 1958 book \"Existence\"\u2014and especially his introductory essay\u2014introduced their work into this country.\nA more recent contributor to the development of a European version of existentialist psychotherapy is the British-based Emmy van Deurzen.\nAnxiety's importance in existentialism makes it a popular topic in psychotherapy. Therapists often offer existentialist philosophy as an explanation for anxiety. The assertion is that anxiety is manifested of an individual's complete freedom to decide, and complete responsibility for the outcome of such decisions. Psychotherapists using an existentialist approach believe that a patient can harness his anxiety and use it constructively. Instead of suppressing anxiety, patients are advised to use it as grounds for change. By embracing anxiety as inevitable, a person can use it to achieve his full potential in life. Humanistic psychology also had major impetus from existentialist psychology and shares many of the fundamental tenets. Terror management theory, based on the writings of Ernest Becker and Otto Rank, is a developing area of study within the academic study of psychology. It looks at what researchers claim are implicit emotional reactions of people confronted with the knowledge that they will eventually die.\nAlso, Gerd B. Achenbach has refreshed the Socratic tradition with his own blend of philosophical counseling; as did Michel Weber with his Chromatiques Center in Belgium.\nCriticisms.\nGeneral criticisms.\nWalter Kaufmann criticized \"the profoundly unsound methods and the dangerous contempt for reason that have been so prominent in existentialism.\" Logical positivist philosophers, such as Rudolf Carnap and A. J. Ayer, assert that existentialists are often confused about the verb \"to be\" in their analyses of \"being\". Specifically, they argue that the verb \"is\" is transitive and pre-fixed to a predicate (e.g., an apple \"is red\") (without a predicate, the word \"is\" is meaningless), and that existentialists frequently misuse the term in this manner. Colin Wilson has stated in his book \"The Angry Years\" that existentialism has created many of its own difficulties: \"We can see how this question of freedom of the will has been vitiated by post-romantic philosophy, with its inbuilt tendency to laziness and boredom, we can also see how it came about that existentialism found itself in a hole of its own digging, and how the philosophical developments since then have amounted to walking in circles round that hole.\"\nSartre's philosophy.\nMany critics argue Sartre's philosophy is contradictory. Specifically, they argue that Sartre makes metaphysical arguments despite his claiming that his philosophical views ignore metaphysics. Herbert Marcuse criticized \"Being and Nothingness\" for projecting anxiety and meaninglessness onto the nature of existence itself: \"Insofar as Existentialism is a philosophical doctrine, it remains an idealistic doctrine: it hypostatizes specific historical conditions of human existence into ontological and metaphysical characteristics. Existentialism thus becomes part of the very ideology which it attacks, and its radicalism is illusory.\"\nIn \"Letter on Humanism\", Heidegger criticized Sartre's existentialism:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Existentialism says existence precedes essence. In this statement he is taking \"existentia\" and \"essentia\" according to their metaphysical meaning, which, from Plato's time on, has said that \"essentia\" precedes \"existentia\". Sartre reverses this statement. But the reversal of a metaphysical statement remains a metaphysical statement. With it, he stays with metaphysics, in oblivion of the truth of Being.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9596", "revid": "248739", "url": "https://en.wikipedia.org/wiki?curid=9596", "title": "Ellipsis", "text": "Triple-dot punctuation mark\nThe ellipsis (, plural ellipses; from , , lit.\u2009'leave out'), rendered ..., also known as suspension points dots, points periods of ellipsis, or ellipsis points, or colloquially, dot-dot-dot, is a punctuation mark consisting of a series of three dots. An ellipsis can be used in many ways, such as for intentional omission of text or numbers, to imply a concept without using words, or to mark a pause in speech. Style guides differ on how to render an ellipsis both digitally and in print. In some cases, an ellipsis may have four or more dots, spaced dots, or some incorporation with other punctuation.\nStyle.\nOpinions differ on how to render an ellipsis in printed material and are to some extent based on the technology used for rendering. According to \"The Chicago Manual of Style\", it should consist of three periods, each separated from its neighbor by a non-breaking space: .\u00a0.\u00a0.. According to the \"AP Stylebook\", the periods should be rendered with no space between them: ... A third option \u2013 available in electronic text \u2013 is to use the precomposed character HORIZONTAL ELLIPSIS.\nWhen text is omitted following a sentence, a period (full stop) terminates the sentence, and a subsequent ellipsis indicates one or more omitted sentences before continuing a longer quotation. \"Business Insider\" magazine suggests this style and it is also used in many academic journals. The \"Associated Press Stylebook\" favors this approach.\nWhen a sentence ends with ellipsis, some style guides indicate there should be four dots; three for ellipsis and a period. \"Chicago\" advises it, as does the \"Publication Manual of the American Psychological Association\" (APA style), while some other style guides do not; the \"Merriam-Webster Dictionary\" and related works treat this style as optional, saying that it \"may\" be used.\nIn writing.\nIn her book on the ellipsis, \"Ellipsis in English Literature: Signs of Omission\", Anne Toner suggests that the first use of the punctuation in the English language dates to a 1588 translation of Terence's \"Andria\", by Maurice Kyffin. In this case, however, the ellipsis consists not of dots but of short dashes. \"Subpuncting\" of medieval manuscripts also denotes omitted meaning and may be related.\nThe popularity of the ellipsis took off after Kyffin's usage; containing three examples in his 1588 translation of \"Andria\", by the 1627 translation of the same play there were 29 examples of its usage. They appear in William Shakespeare's plays in addition to Ben Jonson's. In 1634, John Barton, an English schoolmaster, wrote in \"The Art of Rhetorick\" that \"eclipsis\" is much used in playbooks \u201cwhere they are noted thus ---\u201d. In the first folio edition of Shakespeare\u2019s \"Henry IV, Part 1\", Toner writes, \"Hotspur dies on a dash\", with his last words cut short.\nDifferent types of ellipsis faced opposition. In the 18th-century, Jonathan Swift rhymed \"dash\" with \"printed trash\", while Henry Fielding chose the name 'Dash' for an unlikeable character in his 1730 play \"The Author's Farce\". It has also been championed by writers such as Percy Bysshe Shelley, Jane Austen and Virginia Woolf. According to Toner, an early example of the dot dot dot phrase is in Woolf's short story \"An Unwritten Novel\" (1920).\nOccasionally, it would be used in pulp fiction and other works of early 20th-century fiction to denote expletives that would otherwise have been censored.\nAn ellipsis may also imply an unstated alternative indicated by context. For example, \"I never drink wine\u00a0...\" implies that the speaker does drink something else\u2014such as vodka.\nIn reported speech, the ellipsis can be used to represent an intentional silence.\nIn poetry, an ellipsis is used as a thought-pause or line break at the caesura or this is used to highlight sarcasm or make the reader think about the last points in the poem.\nIn news reporting, often put inside square brackets, it is used to indicate that a quotation has been condensed for space, brevity or relevance, as in \"The President said that [...] he would not be satisfied\", where the exact quotation was \"The President said that, for as long as this situation continued, he would not be satisfied\".\nHerb Caen, Pulitzer-prize-winning columnist for the \"San Francisco Chronicle\", became famous for his \"three-dot journalism\".\nDepending on context, ellipsis can indicate an unfinished thought, a leading statement, a slight pause, an echoing voice, or a nervous or awkward silence. Aposiopesis is the use of an ellipsis to trail off into silence\u2014for example: \"But I thought he was...\" When placed at the end of a sentence, an ellipsis may be used to suggest melancholy or longing.\nIn newspaper and magazine columns, ellipses may separate items of a list instead of paragraph breaks.\nMerriam-Webster's \"Manual for Writers and Editors\" uses a line of ellipsis to indicate omission of whole lines in a quoted poem.\nIn different languages.\nIn English.\nAmerican English.\n\"The Chicago Manual of Style\" suggests the use of an ellipsis for any omitted word, phrase, line, or paragraph from within but not at the end of a quoted passage. There are two commonly used methods of using ellipses: one uses three dots for any omission, while the second one makes a distinction between omissions within a sentence (using three dots:\u00a0.\u00a0.\u00a0.) and omissions between sentences (using a period and a space followed by three dots:\u00a0.\u00a0...). The \"Chicago Style\" Q&amp;A recommends that writers avoid using the precomposed character in manuscripts and to place three periods plus two nonbreaking spaces (.\u00a0.\u00a0.) instead, leaving the editor, publisher, or typographer to replace them later.\nThe Modern Language Association (MLA) used to indicate that an ellipsis must include spaces before and after each dot in all uses. If an ellipsis is meant to represent an omission, square brackets must surround the ellipsis to make it clear that there was no pause in the original quote: [\u00a0.\u00a0.\u00a0.\u00a0]. Currently, the MLA has removed the requirement of brackets in its style handbooks. However, some maintain that the use of brackets is still correct because it clears confusion.\nThe MLA now indicates that a three-dot, spaced ellipsis \u00a0.\u00a0.\u00a0.\u00a0 should be used for removing material from within one sentence within a quote. When crossing sentences (when the omitted text contains a period, so that omitting the end of a sentence counts), a four-dot, spaced (except for before the first dot) ellipsis .\u00a0.\u00a0.\u00a0.\u00a0 should be used. When ellipsis points are used in the original text, ellipsis points that are not in the original text should be distinguished by enclosing them in square brackets (e.g. text [...] text).\nAccording to the Associated Press, the ellipsis should be used to condense quotations. It is less commonly used to indicate a pause in speech or an unfinished thought or to separate items in material such as show business gossip. The stylebook indicates that if the shortened sentence before the mark can stand as a sentence, it should do so, with an ellipsis placed after the period or other ending punctuation. When material is omitted at the end of a paragraph and also immediately following it, an ellipsis goes both at the end of that paragraph and at the beginning of the next, according to this style.\nAccording to Robert Bringhurst's \"Elements of Typographic Style\", the details of typesetting ellipses depend on the character and size of the font being set and the typographer's preference. Bringhurst writes that a full space between each dot is \"another Victorian eccentricity. In most contexts, the Chicago ellipsis is much too wide\"\u2014he recommends using flush dots (with a normal word space before and after), or \"thin\"-spaced dots (up to one-fifth of an em), or the prefabricated ellipsis character . Bringhurst suggests that normally an ellipsis should be spaced fore-and-aft to separate it from the text, but when it combines with other punctuation, the leading space disappears and the other punctuation follows. This is the usual practice in typesetting. He provides the following examples:\nIn legal writing in the United States, Rule 5.3 in the \"Bluebook\" citation guide governs the use of ellipses and requires a space before the first dot and between the two subsequent dots. If an ellipsis ends the sentence, then there are three dots, each separated by a space, followed by the final punctuation (e.g. Hah . . . ?). In some legal writing, an ellipsis is written as three asterisks, *** or *\u00a0*\u00a0*, to make it obvious that text has been omitted or to signal that the omitted text extends beyond the end of the paragraph.\nBritish English.\n\"The Oxford Style Guide\" recommends setting the ellipsis as a single character or as a series of three (narrow) spaced dots surrounded by spaces, thus: . If there is an ellipsis at the end of an incomplete sentence, the final full stop is omitted. However, it is retained if the following ellipsis represents an omission between two complete sentences.\n&lt;poem&gt;The ... fox jumps ...\nThe quick brown fox jumps over the lazy dog. ... And if they have not died, they are still alive today.\nIt is not cold ... it is freezing cold.&lt;/poem&gt;\nContrary to \"The Oxford Style Guide\", the \"University of Oxford Style Guide\" demands an ellipsis not to be surrounded by spaces, except when it stands for a pause; then, a space has to be set after the ellipsis (but not before), and it states that an ellipsis should never be preceded or followed by a full stop.\n&lt;poem&gt;The...fox jumps...\nThe quick brown fox jumps over the lazy dog...And if they have not died, they are still alive today.\nIt is not cold... it is freezing cold.&lt;/poem&gt;\nIn Polish.\nWhen applied in Polish syntax, the ellipsis is called , literally \"multidot\". The word \"wielokropek\" distinguishes the ellipsis of Polish syntax from that of mathematical notation, in which it is known as an . When an ellipsis replaces a fragment omitted from a quotation, the ellipsis is enclosed in parentheses or square brackets. An unbracketed ellipsis indicates an interruption or pause in speech. The syntactic rules for ellipses are standardized by the 1983 Polska Norma document PN-83/P-55366, (Rules for Setting Texts in Polish).\nIn Russian.\nThe combination \"ellipsis+period\" is replaced by the ellipsis. The combinations \"ellipsis+exclamation mark\" and \"ellipsis+question mark\" are written in this way: !.. ?..\nIn Japanese.\nThe most common character corresponding to an ellipsis is called \"3\"-ten r\u012bd\u0101 (\"\"3\"-dot leaders\", ). 2-ten r\u012bd\u0101 exists as a character, but it is used less commonly. In writing, the ellipsis consists usually of six dots (two \"3\"-ten r\u012bd\u0101 characters, ). Three dots (one \"3\"-ten r\u012bd\u0101 character) may be used where space is limited, such as in a header. However, variations in the number of dots exist. In horizontally written text the dots are commonly vertically centered within the text height (between the baseline and the ascent line), as in the standard Japanese Windows fonts; in vertically written text the dots are always centered horizontally. As the Japanese word for dot is pronounced \", the dots are colloquially called \" (, akin to the English \"dot dot dot\").\nIn text in Japanese media, such as in manga or video games, ellipses are much more frequent than in English, and are often changed to another punctuation sign in translation. The ellipsis by itself represents speechlessness, or a \"pregnant pause\". Depending on the context, this could be anything from an admission of guilt to an expression of being dumbfounded at another person's words or actions. As a device, the \"ten-ten-ten\" is intended to focus the reader on a character while allowing the character to not speak any dialogue. This conveys to the reader a focus of the narrative \"camera\" on the silent subject, implying an expectation of some motion or action. It is not unheard of to see inanimate objects \"speaking\" the ellipsis.\nIn Chinese.\nIn Chinese, the ellipsis is six dots (in two groups of three dots, occupying the same horizontal or vertical space as two characters). In horizontally written text the dots are commonly vertically centered along the midline (halfway between the Roman descent and Roman ascent, or equivalently halfway between the Roman baseline and the capital height, i.e. ). This is generally true of Traditional Chinese, while Simplified Chinese tends to have the ellipses aligned with the baseline; in vertically written text the dots are always centered horizontally (i.e. ). Also note that Taiwan and China have different punctuation standards.\nIn Spanish.\nIn Spanish, the ellipsis is commonly used as a substitute of \"et cetera\" at the end of unfinished lists. So it means \"and so forth\" or \"and other things\".\nOther use is the suspension of a part of a text, or a paragraph, or a phrase or a part of a word because it is obvious, or unnecessary, or implied. For instance, sometimes the ellipsis is used to avoid the complete use of expletives.\nWhen the ellipsis is placed alone into a parenthesis (...) or\u2014less often\u2014between brackets [...], which is what happens usually within a text transcription, it means the original text had more contents on the same position but are not useful to our target in the transcription. When the suppressed text is at the beginning or at the end of a text, the ellipsis does not need to be placed in a parenthesis.\nThe number of dots is three and only three. They should have no space in between them nor with the preceding word, but there should be a space with the following word (except if they are followed by a punctuation sign, such as a comma).\nIn French.\nIn French, the ellipsis is commonly used at the end of lists to represent . In French typography, the ellipsis is written immediately after the preceding word, but has a space after it, for example: . If, exceptionally, it begins a sentence, there is a space before and after, for example: . However, any omitted word, phrase or line at the end of a quoted passage would be indicated as follows: [...] (space before and after the square brackets but not inside), for example: .\nIn German.\nIn German, the ellipsis in general is surrounded by spaces, if it stands for one or more omitted words. On the other side there is no space between a letter or (part of) a word and an ellipsis, if it stands for one or more omitted letters, that should stick to the written letter or letters.\nExample for both cases, using German style: \"The first el...is stands for omitted letters, the second ... for an omitted word.\"\nIf the ellipsis is at the end of a sentence, the final full stop is omitted.\nExample: \"I think that ...\"\nIn Italian.\nThe suggests the use of an ellipsis () to indicate a pause longer than a period and, when placed between brackets, the omission of letters, words or phrases. &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;(Gabriele D'Annunzio, \"Il piacere\")\nIn mathematical notation.\nAn ellipsis is used in mathematics to mean \"and so forth\"; usually indicating the omission of terms that follow an obvious pattern as indicated by included terms.\nThe whole numbers from 1 to 100 can be shown as:\n formula_1\nThe positive whole numbers, an infinite list, can be shown as:\n formula_2\nTo indicate omitted terms in a repeated operation, an ellipsis is sometimes raised from the baseline, as:\n formula_3\nBut, this raised formatting is not standard. For example, Russian mathematical texts use the baseline format.\nThe ellipsis is not a formally defined mathematical symbol. Repeated summations or products may be more formally denoted using capital sigma and capital pi notation, respectively:\n formula_4 (see termial)\n formula_5 (see factorial)\nEllipsis is sometimes used where the pattern is not clear. For example, indicating the indefinite continuation of an irrational number such as:\n formula_6\nIt can be useful to display an expression compactly, for example:\n formula_7\nIn set notation, the ellipsis is used as horizontal, vertical and diagonal for indicating missing matrix terms, such as the size-\"n\" identity matrix:\n formula_8\nIn computer programming.\nSome programming languages use ellipsis to indicate a range or for a variable argument list.\nThe CSS codice_1 property can be set to codice_2, which cuts off text with an ellipsis when it overflows the content area.\nIn computer user interface.\nMore.\nAn ellipsis is sometimes used as the label for a button to access user interface that has been omitted \u2013 probably due to space limitations \u2013 particularly in mobile apps running on small screen devices. This may be described as a \"more button\".\nSimilar functionality may be accessible via a button with a hamburger icon (\u2261) or a narrow version called the kebab icon which is a vertical ellipsis (\u22ee).\nMore input will be needed.\nAccording to some style guides, a menu item or button labeled with a trailing ellipsis requests an operation that cannot be completed without additional information and selecting it will prompt the user for input. Without an ellipsis, selecting the item or button will perform an action without user input.\nFor example, the menu item \"Save\" overwrites an existing file whereas \"Save as...\" prompts the user for save options before saving.\nBusy/progress.\nEllipsis is commonly used to indicate that a longer-lasting operation is in progress like \"Loading...\", \"Saving...\".\nSometimes progress is animated with an ellipse-like construct of repeatedly adding dots to a label in a manner similar to a progress bar.\nIn texting.\nIn text-based communications, the ellipsis may indicate:\nAlthough an ellipsis is complete with three periods (...), an ellipsis-like construct with more dots is used to indicate \"trailing-off\" or \"silence\". The extent of repetition in itself might serve as an additional contextualization or paralinguistic cue; one paper wrote that they \"extend the lexical meaning of the words, add character to the sentences, and allow fine-tuning and personalisation of the message\".\nWhile composing a text message, some environments show others in the conversation a typing awareness indicator ellipsis to indicate remote activity.\nComputer representations.\nIn computing, several ellipsis characters have been codified.\nUnicode.\nUnicode defines the following ellipsis characters:\nUnicode recognizes a series of three period characters () as compatibility equivalent (though not canonical) to the horizontal ellipsis character.\nHTML.\nIn HTML, the horizontal ellipsis character may be represented by the entity reference codice_3 (since HTML 4.0), and the vertical ellipsis character by the entity reference codice_4 (since HTML 5.0). Alternatively, in HTML, XML, and SGML, a numeric character reference such as codice_5 or codice_6 can be used.\nTeX.\nIn the TeX typesetting system, the following types of ellipsis are available:\nIn LaTeX, the reverse orientation of codice_7 can be achieved with codice_8 provided by the codice_9 package: codice_10 yields .\nWith the codice_11 package from AMS-LaTeX, more specific ellipses are provided for math mode.\nOther.\nThe horizontal ellipsis character also appears in older character maps:\nNote that ISO/IEC 8859 encoding series provides no code point for ellipsis.\nAs with all characters, especially those outside the ASCII range, the author, sender and receiver of an encoded ellipsis must be in agreement upon what bytes are being used to represent the character. Naive text processing software may improperly assume that a particular encoding is being used, resulting in mojibake.\nInput.\nIn Windows using a suitable code page, can be inserted with , using the numeric keypad.\nIn macOS, it can be inserted with (on an English language keyboard).\nIn some Linux distributions, it can be inserted with (this produces an interpunct on other systems), or .\nIn Android, ellipsis is a long-press key. If Gboard is in alphanumeric layout, change to numeric and special characters layout by pressing from alphanumeric layout. Once in numeric and special characters layout, long press key to insert an ellipsis.\nIn Chinese and sometimes in Japanese, ellipsis characters are made by entering two consecutive \"horizontal ellipses\", each with Unicode code point U+2026. In vertical texts, the application should rotate the symbol accordingly.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9597", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=9597", "title": "Enola Gay", "text": "Airplane that dropped the first atomic bomb\nThe Enola Gay () is a Boeing B-29 Superfortress bomber, named after Enola Gay Tibbets, the mother of the pilot, Colonel Paul Tibbets. On 6 August 1945, during the final stages of World War II, it became the first aircraft to drop an atomic bomb in warfare. The bomb, code-named \"Little Boy\", was targeted at the city of Hiroshima, Japan, and destroyed about three-quarters of the city. \"Enola Gay\" participated in the second nuclear attack as the weather reconnaissance aircraft for the primary target of Kokura. Clouds and drifting smoke resulted in Nagasaki, a secondary target, being bombed instead.\nAfter the war, the \"Enola Gay\" returned to the United States, where it was operated from Roswell Army Air Field, New Mexico. In May 1946, it was flown to Kwajalein for the Operation Crossroads nuclear tests in the Pacific, but was not chosen to make the test drop at Bikini Atoll. Later that year, it was transferred to the Smithsonian Institution and spent many years parked at air bases exposed to the weather and souvenir hunters, before its 1961 disassembly and storage at a Smithsonian facility in Suitland, Maryland.\nIn the 1980s, veterans groups engaged in a call for the Smithsonian to put the aircraft on display, leading to an acrimonious debate about exhibiting the aircraft without a proper historical context. The cockpit and nose section of the aircraft were exhibited at the National Air and Space Museum (NASM) on the National Mall, for the bombing's 50th anniversary in 1995, amid controversy. Since 2003, the entire restored B-29 has been on display at NASM's Steven F. Udvar-Hazy Center. The last survivor of its crew, Theodore Van Kirk, died on 28 July 2014 at the age of 93.\nWorld War II.\nEarly history.\nThe \"Enola Gay\" (Model number B-29-45-MO, Serial number 44-86292, Victor number 82) was built by the Glenn L. Martin Company (later part of Lockheed Martin) at its bomber plant in Bellevue, Nebraska, located at Offutt Field, now Offutt Air Force Base. The bomber was one of the first fifteen B-29s built to the \"Silverplate\" specification\u2014 of 65 eventually completed during and after World War II\u2014giving them the primary ability to function as nuclear \"weapon delivery\" aircraft. These modifications included an extensively modified bomb bay with pneumatic doors and British bomb attachment and release systems, reversible pitch propellers that gave more braking power on landing, improved engines with fuel injection and better cooling, and the removal of protective armor and gun turrets.\n\"Enola Gay\" was personally selected by Colonel Paul W. Tibbets Jr., the commander of the 509th Composite Group, on 9 May 1945, while still on the assembly line. The aircraft was accepted by the United States Army Air Forces (USAAF) on 18 May 1945 and assigned to the 393d Bombardment Squadron, Heavy, 509th Composite Group. Crew B-9, commanded by Captain Robert A. Lewis, took delivery of the bomber and flew it from Omaha to the 509th base at Wendover Army Air Field, Utah, on 14 June 1945.\nThirteen days later, the aircraft left Wendover for Guam, where it received a bomb-bay modification, and flew to North Field, Tinian, on 6 July. It was initially given the Victor (squadron-assigned identification) number 12, but on 1 August, was given the circle R tail markings of the 6th Bombardment Group as a security measure and had its Victor number changed to 82 to avoid misidentification with actual 6th Bombardment Group aircraft. During July, the bomber made eight practice or training flights and flew two missions, on 24 and 26 July, to drop pumpkin bombs on industrial targets at Kobe and Nagoya. \"Enola Gay\" was used on 31 July on a rehearsal flight for the actual mission.\nThe partially assembled Little Boy gun-type fission weapon L-11, weighing , was contained inside a wooden crate that was secured to the deck of the . Unlike the six uranium-235 target discs, which were later flown to Tinian on three separate aircraft arriving 28 and 29 July, the assembled projectile with the nine uranium-235 rings installed was shipped in a single lead-lined steel container weighing that was locked to brackets welded to the deck of Captain Charles B. McVay III's quarters. Both the L-11 and projectile were dropped off at Tinian on 26 July 1945.\nHiroshima mission.\nOn 5 August 1945, during preparation for the first atomic mission, Tibbets assumed command of the aircraft and named it after his mother, Enola Gay Tibbets, who, in turn, had been named for the heroine of a novel. When it came to selecting a name for the plane, Tibbets later recalled that: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...\u00a0my thoughts turned at this point to my courageous red-haired mother, whose quiet confidence had been a source of strength to me since boyhood, and particularly during the soul-searching period when I decided to give up a medical career to become a military pilot. At a time when Dad had thought I had lost my marbles, she had taken my side and said, \"I know you will be all right, son.\"\nIn the early morning hours, just prior to the 6 August mission, Tibbets had a young Army Air Forces maintenance man, Private Nelson Miller, paint the name just under the pilot's window. Regularly assigned aircraft commander Robert A. Lewis was unhappy to be displaced by Tibbets for this important mission and became furious when he arrived at the aircraft on the morning of 6 August to see it painted with the now-famous nose art.\nHiroshima was the primary target of the first nuclear bombing mission on 6 August, with Kokura and Nagasaki as alternative targets. \"Enola Gay\", piloted by Tibbets, took off from North Field, in the Northern Mariana Islands, about six hours' flight time from Japan, accompanied by two other B-29s, \"The Great Artiste\", carrying instrumentation, and a then-nameless aircraft later called \"Necessary Evil\", commanded by Captain George Marquardt, to take photographs. The director of the Manhattan Project, Major General Leslie R. Groves Jr., wanted the event recorded for posterity, so the takeoff was illuminated by floodlights. When he wanted to taxi, Tibbets leaned out the window to direct the bystanders out of the way. On request, he gave a friendly wave for the cameras.\nAfter leaving Tinian, the three aircraft made their way separately to Iwo Jima, where they rendezvoused at and set course for Japan. The aircraft arrived over the target in clear visibility at . Navy Captain William S. \"Deak\" Parsons of Project Alberta, who was in command of the mission, armed the bomb during the flight to minimize the risks during takeoff. His assistant, Second Lieutenant Morris R. Jeppson, removed the safety devices 30 minutes before reaching the target area.\nThe release at 08:15 (Hiroshima time) went as planned, and the Little Boy took 53 seconds to fall from the aircraft flying at to the predetermined detonation height about above the city. \"Enola Gay\" traveled before it felt the shock waves from the blast. Although buffeted by the shock, neither \"Enola Gay\" nor \"The Great Artiste\" was damaged.\nThe detonation created a blast equivalent to . The U-235 weapon was considered very inefficient, with only 1.7% of its fissile material reacting. The radius of total destruction was about , with resulting fires across . Americans estimated that of the city were destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6\u20137% damaged. Some 70,000\u201380,000 people, 30% of the city's population, were killed by the blast and resultant firestorm, and another 70,000 injured. Out of those killed, 20,000 were soldiers and 20,000 were Korean slave laborers.\n\"Enola Gay\" returned safely to its base on Tinian to great fanfare, touching down at 2:58 pm, after 12 hours 13 minutes. \"The Great Artiste\" and \"Necessary Evil\" followed at short intervals. Several hundred people, including journalists and photographers, had gathered to watch the planes return. Tibbets was the first to disembark and was presented with the Distinguished Service Cross on the spot.\nNagasaki mission.\nThe Hiroshima mission was followed by another atomic strike. Originally scheduled for 11 August, it was brought forward by two days to 9 August owing to a forecast of bad weather. This time, a nuclear bomb code-named \"Fat Man\" was carried by B-29 \"Bockscar\", piloted by Major Charles W. Sweeney. \"Enola Gay\", flown by Captain George Marquardt's Crew B-10, was the weather reconnaissance aircraft for Kokura, the primary target. \"Enola Gay\" reported clear skies over Kokura, but by the time \"Bockscar\" arrived, the city was obscured by smoke from fires from the conventional bombing of Yahata by 224 B-29s the day before. After three unsuccessful passes, \"Bockscar\" diverted to its secondary target, Nagasaki, where it dropped its bomb. In contrast to the Hiroshima mission, the Nagasaki mission has been described as tactically botched, although the mission did meet its objectives. The crew encountered a number of problems in execution and had very little fuel by the time they landed at the emergency backup landing site Yontan Airfield on Okinawa.\nCrews.\nHiroshima mission.\n\"Enola Gay\"'s crew on 6 August 1945 consisted of 12 men. The crew was:\nAsterisks denote regular crewmen of the \"Enola Gay\".\nOf mission commander Parsons, it was said: \"There is no one more responsible for getting this bomb out of the laboratory and into some form useful for combat operations than Captain Parsons, by his plain genius in the ordnance business.\"\nNagasaki mission.\nFor the Nagasaki mission, \"Enola Gay\" was flown by Crew B-10, normally assigned to \"Up An' Atom\":\nSource: Campbell, 2005, pp.\u00a0134, 191\u2013192.\nSubsequent history.\nOn 6 November 1945, Lewis flew the \"Enola Gay\" back to the United States, arriving at the 509th's new base at Roswell Army Air Field, New Mexico, on 8 November. On 29 April 1946, \"Enola Gay\" left Roswell as part of the Operation Crossroads nuclear weapons tests in the Pacific. It flew to Kwajalein Atoll on 1 May. It was not chosen to make the test drop at Bikini Atoll and left Kwajalein on 1 July, the date of the test, reaching Fairfield-Suisun Army Air Field, California, the next day.\nThe decision was made to preserve the \"Enola Gay\", and on 24 July 1946, the aircraft was flown to Davis\u2013Monthan Air Force Base, Tucson, Arizona, in preparation for storage. On 30 August 1946, the title to the aircraft was transferred to the Smithsonian Institution and the \"Enola Gay\" was removed from the USAAF inventory. From 1946 to 1961, the \"Enola Gay\" was put into temporary storage at a number of locations. It was at Davis-Monthan from 1 September 1946 until 3 July 1949, when it was flown to Orchard Place Air Field, Park Ridge, Illinois, by Tibbets for acceptance by the Smithsonian. It was moved to Pyote Air Force Base, Texas, on 12 January 1952, and then to Andrews Air Force Base, Maryland, on 2 December 1953, because the Smithsonian had no storage space for the aircraft.\nIt was hoped that the Air Force would guard the plane, but, lacking hangar space, it was left outdoors on a remote part of the air base, exposed to the elements. Souvenir hunters broke in and removed parts. Insects and birds then gained access to the aircraft. Paul E. Garber of the Smithsonian Institution became concerned about the \"Enola Gay\"'s condition, and on 10 August 1960, Smithsonian staff began dismantling the aircraft. The components were transported to the Smithsonian storage facility at Suitland, Maryland, on 21 July 1961.\nThe \"Enola Gay\" remained at Suitland for many years. By the early 1980s, two veterans of the 509th, Don Rehl and his former navigator in the 509th, Frank B. Stewart, began lobbying for the aircraft to be restored and put on display. They enlisted Tibbets and Senator Barry Goldwater in their campaign. In 1983, Walter J. Boyne, a former B-52 pilot with the Strategic Air Command, became director of the National Air and Space Museum, and he made the \"Enola Gay\"'s restoration a priority. Looking at the aircraft, Tibbets recalled, was a \"sad meeting. [My] fond memories, and I don't mean the dropping of the bomb, were the numerous occasions I flew the airplane\u00a0... I pushed it very, very hard and it never failed me\u00a0... It was probably the most beautiful piece of machinery that any pilot ever flew.\"\nRestoration.\nRestoration of the bomber began on 5 December 1984, at the Paul E. Garber Preservation, Restoration, and Storage Facility in Suitland-Silver Hill, Maryland. The propellers that were used on the bombing mission were later shipped to Texas A&amp;M University. One of these propellers was trimmed to for use in the university's Oran W. Nicks Low Speed Wind Tunnel. The lightweight aluminum variable-pitch propeller is powered by a 1,250 kVA electric motor, providing a wind speed up to . Two engines were rebuilt at Garber and two at San Diego Air &amp; Space Museum. Some parts and instruments had been removed and could not be located. Replacements were found or fabricated, and marked so that future curators could distinguish them from the original components.\nExhibition controversy.\nThe \"Enola Gay\" became the center of a controversy at the Smithsonian Institution when the museum planned to put its fuselage on public display in 1995 as part of an exhibit commemorating the 50th anniversary of the atomic bombing of Hiroshima. The exhibit, \"The Crossroads: The End of World War II, the Atomic Bomb and the Cold War,\" was drafted by the Smithsonian's National Air and Space Museum staff, and arranged around the restored \"Enola Gay\".\nCritics of the planned exhibit, especially those of the American Legion and the Air Force Association, charged that the exhibit focused too much attention on the Japanese casualties inflicted by the nuclear bomb, rather than on the motives for the bombing or the discussion of the bomb's role in ending the conflict with Japan. The exhibit brought to national attention many long-standing academic and political issues related to retrospective views of the bombings. After attempts to revise the exhibit to meet the satisfaction of competing interest groups, the exhibit was canceled on 30 January 1995. Martin O. Harwit, Director of the National Air and Space Museum, was compelled to resign over the controversy. He later reflected that\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The dispute was not simply about the atomic bomb. Rather, the dispute was sometimes a symbolic issue in a \"culture war\" in which many Americans lumped together the seeming decline of American power, the difficulties of the domestic economy, the threats in world trade and especially Japan's successes, the loss of domestic jobs, and even changes in American gender roles and shifts in the American family. To a number of Americans, the very people responsible for the script were the people who were changing America. The bomb, representing the end of World War II and suggesting the height of American power was to be celebrated. It was, in this judgment, a crucial symbol of America's \"good war\", one fought justly for noble purposes at a time when America was united. Those who in any way questioned the bomb's use were, in this emotional framework, the enemies of America.\nThe forward fuselage went on display on 28 June 1995. On 2 July 1995, three people were arrested for throwing ash and human blood on the aircraft's fuselage, following an earlier incident in which a protester had thrown red paint over the gallery's carpeting. The exhibition closed on 18 May 1998 and the fuselage was returned to the Garber Facility for final restoration.\nComplete restoration and display.\nIts restoration work began in 1984, and eventually required 300,000 staff hours. While the fuselage was on display, from 1995 to 1998, work continued on the remaining unrestored components. The aircraft was shipped in pieces to the National Air and Space Museum's Steven F. Udvar-Hazy Center in Chantilly, Virginia from March\u2013June 2003, with the fuselage and wings reunited for the first time since 1960 on 10 April 2003 and assembly completed on 8 August 2003. The aircraft has been on display at the Udvar-Hazy Center since the museum annex opened on 15 December 2003. As a result of the earlier controversy, the signage around the aircraft provided only the same succinct technical data as is provided for other aircraft in the museum, without discussion of the controversial issues. It read:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nBoeing's B-29 Superfortress was the most sophisticated propeller-driven bomber of World War II, and the first bomber to house its crew in pressurized compartments. Although designed to fight in the European theater, the B-29 found its niche on the other side of the globe. In the Pacific, B-29s delivered a variety of aerial weapons: conventional bombs, incendiary bombs, mines, and two nuclear weapons.\nOn 6 August 1945, this Martin-built B-29-45-MO dropped the first atomic weapon used in combat on Hiroshima, Japan. Three days later, Bockscar (on display at the U.S. Air Force Museum near Dayton, Ohio) dropped a second atomic bomb on Nagasaki, Japan. \"Enola Gay\" flew as the advance weather reconnaissance aircraft that day. A third B-29, \"The Great Artiste\", flew as an observation aircraft on both missions.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Transferred from the U.S. Air Force\"\nWingspan: \nLength:\nHeight: \nWeight, empty: \nWeight, gross: \nTop speed: \nEngines: 4 Wright R-3350-57 Cyclone turbo-supercharged radials, 2,200 hp \nCrew: 12 (Hiroshima mission) \nArmament: two .50 caliber machine guns \nOrdnance: Little Boy atomic bomb \nManufacturer: Martin Co., Omaha, Nebraska, 1945 \nA19500100000\nThe display of the \"Enola Gay\" without reference to the historical context of World War II, the Cold War, or the development and deployment of nuclear weapons aroused controversy. A petition from a group calling themselves the Committee for a National Discussion of Nuclear History and Current Policy bemoaned the display of \"Enola Gay\" as a technological achievement, which it described as an \"extraordinary callousness toward the victims, indifference to the deep divisions among American citizens about the propriety of these actions, and disregard for the feelings of most of the world's peoples\". It attracted signatures from notable figures including historian Gar Alperovitz, social critic Noam Chomsky, whistle blower Daniel Ellsberg, physicist Joseph Rotblat, writer Kurt Vonnegut, producer Norman Lear, actor Martin Sheen and filmmaker Oliver Stone.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9598", "revid": "51011295", "url": "https://en.wikipedia.org/wiki?curid=9598", "title": "Electronvolt", "text": "Unit of energy\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nIn physics, an electronvolt (symbol eV), also written as electron-volt and electron volt, is a unit of measurement equivalent to the amount of kinetic energy gained by a single electron accelerating through an electric potential difference of one volt in a vacuum. When used as a unit of energy, the numerical value of 1 eV expressed in unit of joules (symbol J) is equal to the numerical value of the charge of an electron in coulombs (symbol C). Under the 2019 revision of the SI, this sets 1\u00a0eV equal to the exact value .\nHistorically, the electronvolt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences, because a particle with electric charge \"q\" gains an energy \"E\" = \"qV\" after passing through a voltage of \"V\".\nDefinition and use.\nAn electronvolt is the amount of energy gained or lost by a single electron when it moves through an electric potential difference of one volt. Hence, it has a value of one volt, which is , multiplied by the elementary charge \"e\"\u00a0=\u00a0. Therefore, one electronvolt is equal to .\nThe electronvolt (eV) is a unit of energy, but is not an SI unit. It is a commonly used unit of energy within physics, widely used in solid state, atomic, nuclear and particle physics, and high-energy astrophysics. It is commonly used with SI prefixes \"milli-\" (10\u22123), \"kilo-\" (103), \"mega-\" (106), \"giga-\" (109), \"tera-\" (1012), \"peta-\" (1015), \"exa-\" (1018), \"zetta-\" (1021), \"yotta-\" (1024), \"ronna-\" (1027), or \"quetta-\" (1030), the respective symbols being meV, keV, MeV, GeV, TeV, PeV, EeV, ZeV, YeV, ReV, and QeV. The SI unit of energy is the joule (J).\nIn some older documents, and in the name \"Bevatron\", the symbol \"BeV\" is used, where the \"B\" stands for \"billion\". The symbol \"BeV\" is therefore equivalent to \"GeV\", though neither is an SI unit.\nRelation to other physical properties and units.\nIn the fields of physics in which the electronvolt is used, other quantities are typically measured using units derived from it; products with fundamental constants of importance in the theory are often used.\nMass.\nBy mass\u2013energy equivalence, the electronvolt corresponds to a unit of mass. It is common in particle physics, where units of mass and energy are often interchanged, to express mass in units of eV/\"c\"2, where \"c\" is the speed of light in vacuum (from \"E\" = \"mc\"2). It is common to informally express mass in terms of eV as a unit of mass, effectively using a system of natural units with \"c\" set to 1. The kilogram equivalent of is:\nformula_1\nFor example, an electron and a positron, each with a mass of , can annihilate to yield of energy. A proton has a mass of . In general, the masses of all hadrons are of the order of , which makes the GeV/\"c\"2 a convenient unit of mass for particle physics:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;= .\nThe atomic mass constant (\"m\"u), one twelfth of the mass a carbon-12 atom, is close to the mass of a proton. To convert to electronvolt mass-equivalent, use the formula:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\"m\"u = 1 Da = = .\nMomentum.\nBy dividing a particle's kinetic energy in electronvolts by the fundamental constant \"c\" (the speed of light), one can describe the particle's momentum in units of eV/\"c\". In natural units in which the fundamental velocity constant \"c\" is numerically 1, the \"c\" may informally be omitted to express momentum using the unit electronvolt.\nThe energy\u2013momentum relation\nformula_2\nin natural units (with formula_3)\nformula_4\nis a Pythagorean equation. When a relatively high energy is applied to a particle with relatively low rest mass, it can be approximated as formula_5 in high-energy physics such that an applied energy with expressed in the unit eV conveniently results in a numerically approximately equivalent change of momentum when expressed with the unit\u00a0eV/\"c\".\nThe dimension of momentum is . The dimension of energy is . Dividing a unit of energy (such as eV) by a fundamental constant (such as the speed of light) that has the dimension of velocity () facilitates the required conversion for using a unit of energy to quantify momentum.\nFor example, if the momentum \"p\" of an electron is , then the conversion to MKS system of units can be achieved by:\nformula_6\nDistance.\nIn particle physics, a system of natural units in which the speed of light in vacuum \"c\" and the reduced Planck constant \"\u0127\" are dimensionless and equal to unity is widely used: \"c\" = \"\u0127\" = 1. In these units, both distances and times are expressed in inverse energy units (while energy and mass are expressed in the same units, see mass\u2013energy equivalence). In particular, particle scattering lengths are often presented using a unit of inverse particle mass.\nOutside this system of units, the conversion factors between electronvolt, second, and nanometer are the following:\nformula_7\nThe above relations also allow expressing the mean lifetime \"\u03c4\" of an unstable particle (in seconds) in terms of its decay width \u0393 (in eV) via \u0393 = \"\u0127\"/\"\u03c4\". For example, the meson has a lifetime of 1.530(9)\u00a0picoseconds, mean decay length is \"c\u03c4\" =, or a decay width of .\nConversely, the tiny meson mass differences responsible for meson oscillations are often expressed in the more convenient inverse picoseconds.\nEnergy in electronvolts is sometimes expressed through the wavelength of light with photons of the same energy:\nformula_8\nTemperature.\nIn certain fields, such as plasma physics, it is convenient to use the electronvolt to express temperature. The electronvolt is divided by the Boltzmann constant to convert to the Kelvin scale:\nformula_9\nwhere \"k\"B is the Boltzmann constant.\nThe \"k\"B is assumed when using the electronvolt to express temperature, for example, a typical magnetic confinement fusion plasma is (kiloelectronvolt), which corresponds to 174\u00a0MK (megakelvin).\nAs an approximation: at a temperature of \"T\" =, \"k\"B\"T\" is about (\u2248 ).\nWavelength.\nThe energy \"E\", frequency \"\u03bd\", and wavelength \"\u03bb\" of a photon are related by\nformula_10\nwhere \"h\" is the Planck constant, \"c\" is the speed of light. This reduces to\nformula_11\nA photon with a wavelength of (green light) would have an energy of approximately . Similarly, would correspond to an infrared photon of wavelength or frequency .\nScattering experiments.\nIn a low-energy nuclear scattering experiment, it is conventional to refer to the nuclear recoil energy in units of eVr, keVr, etc. This distinguishes the nuclear recoil energy from the \"electron equivalent\" recoil energy (eVee, keVee, etc.) measured by scintillation light. For example, the yield of a phototube is measured in phe/keVee (photoelectrons per keV electron-equivalent energy). The relationship between eV, eVr, and eVee depends on the medium the scattering takes place in, and must be established empirically for each material.\nEnergy comparisons.\nMolar energy.\nOne mole of particles given 1\u00a0eV of energy each has approximately 96.5\u00a0kJ of energy \u2013 this corresponds to the Faraday constant (\"F\" \u2248 ), where the energy in joules of \"n\" moles of particles each with energy \"E\"\u00a0eV is equal to \"E\"\u00b7\"F\"\u00b7\"n\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9599", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=9599", "title": "Elliptical Curve Cryptography", "text": ""}
{"id": "9600", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=9600", "title": "ElementalAllotropes", "text": ""}
{"id": "9601", "revid": "1319597231", "url": "https://en.wikipedia.org/wiki?curid=9601", "title": "Electrochemistry", "text": "Branch of physical chemistry\nElectrochemistry is the branch of physical chemistry concerned with the relationship between electrical potential difference and identifiable chemical change. These reactions involve electrons moving via an electronically conducting phase (typically an external electric circuit, but not necessarily, as in electroless plating) between electrodes separated by an ionically conducting and electronically insulating electrolyte (or ionic species in a solution).\nWhen a chemical reaction is driven by an electrical potential difference, as in electrolysis, or if a potential difference results from a chemical reaction as in an electric battery or fuel cell, it is called an \"electrochemical\" reaction. In electrochemical reactions, unlike in other chemical reactions, electrons are not transferred directly between atoms, ions, or molecules, but via the aforementioned electric circuit. This phenomenon is what distinguishes an electrochemical reaction from a conventional chemical reaction.\nHistory.\n16th\u201318th century.\nUnderstanding of electrical matters began in the sixteenth century. During this century, the English scientist William Gilbert spent 17 years experimenting with magnetism and, to a lesser extent, electricity. For his work on magnets, Gilbert became known as the \"Father of Magnetism.\" He discovered various methods for producing and strengthening magnets.\nIn 1663, the German physicist Otto von Guericke created the first electric generator, which produced static electricity by applying friction in the machine. The generator was made of a large sulfur ball cast inside a glass globe, mounted on a shaft. The ball was rotated by means of a crank and an electric spark was produced when a pad was rubbed against the ball as it rotated. The globe could be removed and used as source for experiments with electricity.\nBy the mid-18th century the French chemist Charles Fran\u00e7ois de Cisternay du Fay had discovered two types of static electricity, and that like charges repel each other whilst unlike charges attract. Du Fay announced that electricity consisted of two fluids: \"vitreous\" (from the Latin for \"glass\"), or positive, electricity; and \"resinous,\" or negative, electricity. This was the \"two-fluid theory\" of electricity, which was to be opposed by Benjamin Franklin's \"one-fluid theory\" later in the century.\nIn 1785, Charles-Augustin de Coulomb developed the law of electrostatic attraction as an outgrowth of his attempt to investigate the law of electrical repulsions as stated by Joseph Priestley in England.\nIn the late 18th century the Italian physician and anatomist Luigi Galvani marked the birth of electrochemistry by establishing a bridge between chemical reactions and electricity on his essay \"De Viribus Electricitatis in Motu Musculari Commentarius\" (Latin for Commentary on the Effect of Electricity on Muscular Motion) in 1791 where he proposed a \"nerveo-electrical substance\" on biological life forms.\nIn his essay Galvani concluded that animal tissue contained a here-to-fore neglected innate, vital force, which he termed \"animal electricity,\" which activated nerves and muscles spanned by metal probes. He believed that this new force was a form of electricity in addition to the \"natural\" form produced by lightning or by the electric eel and torpedo ray as well as the \"artificial\" form produced by friction (i.e., static electricity).\nGalvani's scientific colleagues generally accepted his views, but Alessandro Volta rejected the idea of an \"animal electric fluid,\" replying that the frog's legs responded to differences in metal temper, composition, and bulk. Galvani refuted this by obtaining muscular action with two pieces of the same material. Nevertheless, Volta's experimentation led him to develop the first practical battery, which took advantage of the relatively high energy (weak bonding) of zinc and could deliver an electrical current for much longer than any other device known at the time.\n19th century.\nIn 1800, William Nicholson and Johann Wilhelm Ritter succeeded in decomposing water into hydrogen and oxygen by electrolysis using Volta's battery. Soon thereafter Ritter discovered the process of electroplating. He also observed that the amount of metal deposited and the amount of oxygen produced during an electrolytic process depended on the distance between the electrodes. By 1801, Ritter observed thermoelectric currents and anticipated the discovery of thermoelectricity by Thomas Johann Seebeck.\nBy the 1810s, William Hyde Wollaston made improvements to the galvanic cell.\nSir Humphry Davy's work with electrolysis led to the conclusion that the production of electricity in simple electrolytic cells resulted from chemical action and that chemical combination occurred between substances of opposite charge. This work led directly to the isolation of metallic sodium and potassium by electrolysis of their molten salts, and of the alkaline earth metals from theirs, in 1808.\nHans Christian \u00d8rsted's discovery of the magnetic effect of electric currents in 1820 was immediately recognized as an epoch-making advance, although he left further work on electromagnetism to others. Andr\u00e9-Marie Amp\u00e8re quickly repeated \u00d8rsted's experiment, and formulated them mathematically.\nIn 1821, Estonian-German physicist Thomas Johann Seebeck demonstrated the electrical potential between the juncture points of two dissimilar metals when there is a temperature difference between the joints.\nIn 1827, the German scientist Georg Ohm expressed his law in this famous book \"Die galvanische Kette, mathematisch bearbeitet\" (The Galvanic Circuit Investigated Mathematically) in which he gave his complete theory of electricity.\nIn 1832, Michael Faraday's experiments led him to state his two laws of electrochemistry. In 1836, John Daniell invented a primary cell which solved the problem of polarization by introducing copper ions into the solution near the positive electrode and thus eliminating hydrogen gas generation. Later results revealed that at the other electrode, amalgamated zinc (i.e., zinc alloyed with mercury) would produce a higher voltage.\nWilliam Grove produced the first fuel cell in 1839. In 1846, Wilhelm Weber developed the electrodynamometer. In 1868, Georges Leclanch\u00e9 patented a new cell which eventually became the forerunner to the world's first widely used battery, the zinc\u2013carbon cell.\nSvante Arrhenius published his thesis in 1884 on \"Recherches sur la conductibilit\u00e9 galvanique des \u00e9lectrolytes\" (Investigations on the galvanic conductivity of electrolytes). From his results the author concluded that electrolytes, when dissolved in water, become to varying degrees split or dissociated into electrically opposite positive and negative ions.\nIn 1886, Paul H\u00e9roult and Charles M. Hall developed an efficient method (the Hall\u2013H\u00e9roult process) to obtain aluminium using electrolysis of molten alumina.\nIn 1894, Friedrich Ostwald concluded important studies of the conductivity and electrolytic dissociation of organic acids.\nWalther Hermann Nernst developed the theory of the electromotive force of the voltaic cell in 1888. In 1889, he showed how the characteristics of the voltage produced could be used to calculate the free energy change in the chemical reaction producing the voltage. He constructed an equation, known as Nernst equation, which related the voltage of a cell to its properties.\nIn 1898, Fritz Haber showed that definite reduction products can result from electrolytic processes if the potential at the cathode is kept constant. In 1898, he explained the reduction of nitrobenzene in stages at the cathode and this became the model for other similar reduction processes.\n20th century.\nIn 1902, The Electrochemical Society (ECS) was founded.\nIn 1909, Robert Andrews Millikan began a series of experiments (see oil drop experiment) to determine the electric charge carried by a single electron.\nIn 1911, Harvey Fletcher, working with Millikan, was successful in measuring the charge on the electron, by replacing the water droplets used by Millikan, which quickly evaporated, with oil droplets. Within one day Fletcher measured the charge of an electron within several decimal places.\nIn 1923, Johannes Nicolaus Br\u00f8nsted and Martin Lowry published essentially the same theory about how acids and bases behave, using an electrochemical basis.\nIn 1937, Arne Tiselius developed the first sophisticated electrophoretic apparatus. Some years later, he was awarded the 1948 Nobel Prize for his work in protein electrophoresis.\nA year later, in 1949, the International Society of Electrochemistry (ISE) was founded.\nBy the 1960s\u20131970s quantum electrochemistry was developed by Revaz Dogonadze and his students.\nPrinciples.\nOxidation and reduction.\nThe term \"redox\" stands for reduction-oxidation. It refers to electrochemical processes involving electron transfer to or from a molecule or ion, changing its oxidation state. This reaction can occur through the application of an external voltage or through the release of chemical energy. Oxidation and reduction describe the change of oxidation state that takes place in the atoms, ions or molecules involved in an electrochemical reaction. Formally, oxidation state is the hypothetical charge that an atom would have if all bonds to atoms of different elements were 100% ionic. An atom or ion that gives up an electron to another atom or ion has its oxidation state increase, and the recipient of the negatively charged electron has its oxidation state decrease.\nFor example, when atomic sodium reacts with atomic chlorine, sodium donates one electron and attains an oxidation state of +1. Chlorine accepts the electron and its oxidation state is reduced to \u22121. The sign of the oxidation state (positive/negative) actually corresponds to the value of each ion's electronic charge. The attraction of the differently charged sodium and chlorine ions is the reason they then form an ionic bond.\nThe loss of electrons from an atom or molecule is called oxidation, and the gain of electrons is reduction. This can be easily remembered through the use of mnemonic devices. Two of the most popular are \"OIL RIG\" (Oxidation Is Loss, Reduction Is Gain) and \"LEO\" the lion says \"GER\" (Lose Electrons: Oxidation, Gain Electrons: Reduction). Oxidation and reduction always occur in a paired fashion such that one species is oxidized when another is reduced. For cases where electrons are shared (covalent bonds) between atoms with large differences in electronegativity, the electron is assigned to the atom with the largest electronegativity in determining the oxidation state.\nThe atom or molecule which loses electrons is known as the \"reducing agent\", or \"reductant\", and the substance which accepts the electrons is called the \"oxidizing agent\", or \"oxidant\". Thus, the oxidizing agent is always being reduced in a reaction; the reducing agent is always being oxidized. Oxygen is a common oxidizing agent, but not the only one. Despite the name, an oxidation reaction does not necessarily need to involve oxygen. In fact, a fire can be fed by an oxidant other than oxygen; fluorine fires are often unquenchable, as fluorine is an even stronger oxidant (it has a weaker bond and higher electronegativity, and thus accepts electrons even better) than oxygen.\nFor reactions involving oxygen, the gain of oxygen implies the oxidation of the atom or molecule to which the oxygen is added (and the oxygen is reduced). In organic compounds, such as butane or ethanol, the loss of hydrogen implies oxidation of the molecule from which it is lost (and the hydrogen is reduced). This follows because the hydrogen donates its electron in covalent bonds with non-metals but it takes the electron along when it is lost. Conversely, loss of oxygen or gain of hydrogen implies reduction.\nBalancing redox reactions.\nElectrochemical reactions in water are better analyzed by using the ion-electron method, where H+, OH\u2212 ion, H2O and electrons (to compensate the oxidation changes) are added to the cell's half-reactions for oxidation and reduction.\nAcidic medium.\nIn acidic medium, H+ ions and water are added to balance each half-reaction.\nFor example, when manganese reacts with sodium bismuthate.\n\"Unbalanced reaction\": Mn2+(aq) + NaBiO3(s) \u2192 Bi3+(aq) + MnO4\u2212(aq)\n\"Oxidation\": 4 H2O(l) + Mn2+(aq) \u2192 MnO4\u2212(aq) + 8 H+(aq) + 5 e\u2212\n\"Reduction\": 2 e\u2212 + 6 H+(aq) + BiO3\u2212(s) \u2192 Bi3+(aq) + 3 H2O(l)\nFinally, the reaction is balanced by multiplying the stoichiometric coefficients so the numbers of electrons in both half reactions match\n8 H2O(l) + 2 Mn2+(aq) \u2192 2 MnO4\u2212(aq) + 16 H+(aq) + 10 e\u2212\n10 e\u2212 + 30 H+(aq) + 5 BiO3\u2212(s) \u2192 5 Bi3+(aq) + 15 H2O(l)\nand adding the resulting half reactions to give the balanced reaction:\n14 H+(aq) + 2 Mn2+(aq) + 5 NaBiO3(s) \u2192 7 H2O(l) + 2 MnO4\u2212(aq) + 5 Bi3+(aq) + 5 Na+(aq)\nBasic medium.\nIn basic medium, OH\u2212 ions and water are added to balance each half-reaction. For example, in a reaction between potassium permanganate and sodium sulfite:\n\"Unbalanced reaction\": KMnO4 + Na2SO3 + H2O \u2192 MnO2 + Na2SO4 + KOH\n\"Reduction\": 3 e\u2212 + 2 H2O + MnO4\u2212 \u2192 MnO2 + 4 OH\u2212\n\"Oxidation\": 2 OH\u2212 + SO32\u2212 \u2192 SO42\u2212 + H2O + 2 e\u2212\nHere, 'spectator ions' (K+, Na+) were omitted from the half-reactions. By multiplying the stoichiometric coefficients so the numbers of electrons in both half reaction match:\n6 e\u2212 + 4 H2O + 2 MnO4\u2212 \u2192 2 MnO2 + 8 OH\u2212\n6 OH\u2212 + 3 SO32\u2212 \u2192 3 SO42\u2212 + 3 H2O + 6 e\u2212\nthe balanced overall reaction is obtained:\n2 KMnO4 + 3 Na2SO3 + H2O \u2192 2 MnO2 + 3 Na2SO4 + 2 KOH\nNeutral medium.\nThe same procedure as used in acidic medium can be applied, for example, to balance the complete combustion of propane:\n\"Unbalanced reaction\": C3H8 + O2 \u2192 CO2 + H2O\n\"Reduction\": 4 H+ + O2 + 4 e\u2212 \u2192 2 H2O\n\"Oxidation\": 6 H2O + C3H8 \u2192 3 CO2 + 20 e\u2212 + 20 H+\nBy multiplying the stoichiometric coefficients so the numbers of electrons in both half reaction match:\n20 H+ + 5 O2 + 20 e\u2212 \u2192 10 H2O\n6 H2O + C3H8 \u2192 3 CO2 + 20 e\u2212 + 20 H+\nthe balanced equation is obtained:\nC3H8 + 5 O2 \u2192 3 CO2 + 4 H2O\nElectrochemical cells.\nAn electrochemical cell is a device that produces an electric current from energy released by a spontaneous redox reaction. This kind of cell includes the Galvanic cell or Voltaic cell, named after Luigi Galvani and Alessandro Volta, both scientists who conducted experiments on chemical reactions and electric current during the late 18th century.\nElectrochemical cells have two conductive electrodes (the anode and the cathode). The anode is defined as the electrode where oxidation occurs and the cathode is the electrode where the reduction takes place. Electrodes can be made from any sufficiently conductive materials, such as metals, semiconductors, graphite, and even conductive polymers. In between these electrodes is the electrolyte, which contains ions that can freely move.\nThe galvanic cell uses two different metal electrodes, each in an electrolyte where the positively charged ions are the oxidized form of the electrode metal. One electrode will undergo oxidation (the anode) and the other will undergo reduction (the cathode). The metal of the anode will oxidize, going from an oxidation state of 0 (in the solid form) to a positive oxidation state and become an ion. At the cathode, the metal ion in solution will accept one or more electrons from the cathode and the ion's oxidation state is reduced to 0. This forms a solid metal that electrodeposits on the cathode. The two electrodes must be electrically connected to each other, allowing for a flow of electrons that leave the metal of the anode and flow through this connection to the ions at the surface of the cathode. This flow of electrons is an electric current that can be used to do work, such as turn a motor or power a light.\nA galvanic cell whose electrodes are zinc and copper submerged in zinc sulfate and copper sulfate, respectively, is known as a Daniell cell.\nThe half reactions in a Daniell cell are as follows:\nZinc electrode (anode): Zn(s) \u2192 Zn2+(aq) + 2 e\u2212\nCopper electrode (cathode): Cu2+(aq) + 2 e\u2212 \u2192 Cu(s)\nIn this example, the anode is the zinc metal which is oxidized (loses electrons) to form zinc ions in solution, and copper ions accept electrons from the copper metal electrode and the ions deposit at the copper cathode as an electrodeposit. This cell forms a simple battery as it will spontaneously generate a flow of electric current from the anode to the cathode through the external connection. This reaction can be driven in reverse by applying a voltage, resulting in the deposition of zinc metal at the anode and formation of copper ions at the cathode.\nTo provide a complete electric circuit, there must also be an ionic conduction path between the anode and cathode electrolytes in addition to the electron conduction path. The simplest ionic conduction path is to provide a liquid junction. To avoid mixing between the two electrolytes, the liquid junction can be provided through a porous plug that allows ion flow while minimizing electrolyte mixing. To further minimize mixing of the electrolytes, a salt bridge can be used which consists of an electrolyte saturated gel in an inverted U-tube. As the negatively charged electrons flow in one direction around this circuit, the positively charged metal ions flow in the opposite direction in the electrolyte.\nA voltmeter is capable of measuring the change of electrical potential between the anode and the cathode.\nThe electrochemical cell voltage is also referred to as electromotive force or emf.\nA cell diagram can be used to trace the path of the electrons in the electrochemical cell. For example, here is a cell diagram of a Daniell cell:\nZn(s) | Zn2+ (1 M) || Cu2+ (1 M) | Cu(s)\nFirst, the reduced form of the metal to be oxidized at the anode (Zn) is written. This is separated from its oxidized form by a vertical line, which represents the limit between the phases (oxidation changes). The double vertical lines represent the saline bridge on the cell. Finally, the oxidized form of the metal to be reduced at the cathode, is written, separated from its reduced form by the vertical line. The electrolyte concentration is given as it is an important variable in determining the exact cell potential.\nStandard electrode potential.\nTo allow prediction of the cell potential, tabulations of standard electrode potential are available. Such tabulations are referenced to the standard hydrogen electrode (SHE). The standard hydrogen electrode undergoes the reaction\n2 H+(aq) + 2 e\u2212 \u2192 H2\nwhich is shown as a reduction but, in fact, the SHE can act as either the anode or the cathode, depending on the relative oxidation/reduction potential of the other electrode/electrolyte combination. The term standard in SHE requires a supply of hydrogen gas bubbled through the electrolyte at a pressure of 1 atm and an acidic electrolyte with H+ activity equal to 1 (usually assumed to be [H+] = 1\u00a0mol/liter, i.e. pH = 0).\nThe SHE electrode can be connected to any other electrode by a salt bridge and an external circuit to form a cell. If the second electrode is also at standard conditions, then the measured cell potential is called the standard electrode potential for the electrode. The standard electrode potential for the SHE is zero, by definition. The polarity of the standard electrode potential provides information about the relative reduction potential of the electrode compared to the SHE. If the electrode has a positive potential with respect to the SHE, then that means it is a strongly reducing electrode which forces the SHE to be the anode (an example is Cu in aqueous CuSO4 with a standard electrode potential of 0.337 V). Conversely, if the measured potential is negative, the electrode is more oxidizing than the SHE (such as Zn in ZnSO4 where the standard electrode potential is \u22120.76 V).\nStandard electrode potentials are usually tabulated as reduction potentials. However, the reactions are reversible and the role of a particular electrode in a cell depends on the relative oxidation/reduction potential of both electrodes. The oxidation potential for a particular electrode is just the negative of the reduction potential. A standard cell potential can be determined by looking up the standard electrode potentials for both electrodes (sometimes called half cell potentials). The one that is smaller will be the anode and will undergo oxidation. The cell potential is then calculated as the sum of the reduction potential for the cathode and the oxidation potential for the anode.\n\"E\"\u00b0cell = \"E\"\u00b0red (cathode) \u2013 \"E\"\u00b0red (anode) = \"E\"\u00b0red (cathode) + \"E\"\u00b0oxi (anode)\nFor example, the standard electrode potential for a copper electrode is:\n\"Cell diagram\"\nPt(s) | H2 (1 atm) | H+ (1 M) || Cu2+ (1 M) | Cu(s)\n\"E\"\u00b0cell = \"E\"\u00b0red (cathode) \u2013 \"E\"\u00b0red (anode)\nAt standard temperature, pressure and concentration conditions, the cell's emf (measured by a multimeter) is 0.34 V. By definition, the electrode potential for the SHE is zero. Thus, the Cu is the cathode and the SHE is the anode giving\n\"E\"cell = \"E\"\u00b0(Cu2+/Cu) \u2013 \"E\"\u00b0(H+/H2)\nOr,\n\"E\"\u00b0(Cu2+/Cu) = 0.34 V\nChanges in the stoichiometric coefficients of a balanced cell equation will not change the \"E\"\u00b0red value because the standard electrode potential is an intensive property.\nSpontaneity of redox reaction.\nDuring operation of an electrochemical cell, chemical energy is transformed into electrical energy. This can be expressed mathematically as the product of the cell's emf \"E\"cell measured in volts (V) and the electric charge formula_1 transferred through the external circuit.\nElectrical energy = formula_2\nformula_3 is the cell current integrated over time and measured in coulombs (C); it can also be determined by multiplying the total number \"n\"e of electrons transferred (measured in moles) times Faraday's constant (\"F\").\nThe emf of the cell at zero current is the maximum possible emf. It can be used to calculate the maximum possible electrical energy that could be obtained from a chemical reaction. This energy is referred to as electrical work and is expressed by the following equation:\nformula_4,\nwhere work is defined as positive when it increases the energy of the system.\nSince the free energy is the maximum amount of work that can be extracted from a system, one can write:\nformula_5\nA positive cell potential gives a negative change in Gibbs free energy. This is consistent with the cell production of an electric current from the cathode to the anode through the external circuit. If the current is driven in the opposite direction by imposing an external potential, then work is done on the cell to drive electrolysis.\nA spontaneous electrochemical reaction (change in Gibbs free energy less than zero) can be used to generate an electric current in electrochemical cells. This is the basis of all batteries and fuel cells. For example, gaseous oxygen (O2) and\nhydrogen (H2) can be combined in a fuel cell to form water and energy, typically a combination of heat and electrical energy.\nConversely, non-spontaneous electrochemical reactions can be driven forward by the application of a current at sufficient voltage. The electrolysis of water into gaseous oxygen and hydrogen is a typical example.\nThe relation between the equilibrium constant, \"K\", and the Gibbs free energy for an electrochemical cell is expressed as follows:\nformula_6.\nRearranging to express the relation between standard potential and equilibrium constant yields\nformula_7.\nAt \"T\" = 298 K, the previous equation can be rewritten using the Briggsian logarithm as follows:\nformula_8\nCell EMF dependency on changes in concentration.\nNernst equation.\nThe standard potential of an electrochemical cell requires standard conditions (\u0394\"G\"\u00b0) for all of the reactants. When reactant concentrations differ from standard conditions, the cell potential will deviate from the standard potential. In the 20th century German chemist Walther Nernst proposed a mathematical model to determine the effect of reactant concentration on electrochemical cell potential.\nIn the late 19th century, Josiah Willard Gibbs had formulated a theory to predict whether a chemical reaction is spontaneous based on the free energy\nformula_9\nHere \u0394\"G\" is change in Gibbs free energy, \u0394\"G\"\u00b0 is the cell potential when \"Q\" is equal to 1, \"T\" is absolute temperature (Kelvin), \"R\" is the gas constant and \"Q\" is the reaction quotient, which can be calculated by dividing concentrations of products by those of reactants, each raised to the power of its stoichiometric coefficient, using only those products and reactants that are aqueous or gaseous.\nGibbs' key contribution was to formalize the understanding of the effect of reactant concentration on spontaneity.\nBased on Gibbs' work, Nernst extended the theory to include the contribution from electric potential on charged species. As shown in the previous section, the change in Gibbs free energy for an electrochemical cell can be related to the cell potential. Thus, Gibbs' theory becomes\nformula_10\nHere \"ne\" is the number of electrons (in moles), \"F\" is the Faraday constant (in coulombs/mole), and \u0394\"E\" is the cell potential (in volts).\nFinally, Nernst divided through by the amount of charge transferred to arrive at a new equation which now bears his name:\nformula_11\nAssuming standard conditions (\"T\" = 298 K or 25\u00a0\u00b0C) and \"R\" = 8.3145 J/(K\u00b7mol), the equation above can be expressed on base-10 logarithm as shown below:\nformula_12\nNote that \"\" is also known as the thermal voltage \"V\"T and is found in the study of plasmas and semiconductors as well. The value 0.05916\u00a0V in the above equation is just the thermal voltage at standard temperature multiplied by the natural logarithm of 10.\nConcentration cells.\nA concentration cell is an electrochemical cell where the two electrodes are the same material, the electrolytes on the two half-cells involve the same ions, but the electrolyte concentration differs between the two half-cells.\nAn example is an electrochemical cell, where two copper electrodes are submerged in two copper(II) sulfate solutions, whose concentrations are 0.05 M and 2.0 M, connected through a salt bridge. This type of cell will generate a potential that can be predicted by the Nernst equation. Both can undergo the same chemistry (although the reaction proceeds in reverse at the anode)\nCu2+(aq) + 2 e\u2212 \u2192 Cu(s)\nLe Chatelier's principle indicates that the reaction is more favorable to reduction as the concentration of Cu2+ ions increases. Reduction will take place in the cell's compartment where the concentration is higher and oxidation will occur on the more dilute side.\nThe following cell diagram describes the concentration cell mentioned above:\nCu(s) | Cu2+ (0.05 M) || Cu2+ (2.0 M) | Cu(s)\nwhere the half cell reactions for oxidation and reduction are:\nOxidation: Cu(s) \u2192 Cu2+ (0.05 M) + 2 e\u2212\nReduction: Cu2+ (2.0 M) + 2 e\u2212 \u2192 Cu(s)\nOverall reaction: Cu2+ (2.0 M) \u2192 Cu2+ (0.05 M)\nThe cell's emf is calculated through the Nernst equation as follows:\nformula_13\nThe value of \"E\"\u00b0 in this kind of cell is zero, as electrodes and ions are the same in both half-cells.\nAfter replacing values from the case mentioned, it is possible to calculate cell's potential:\nformula_14\nor by:\nformula_15\nHowever, this value is only approximate, as reaction quotient is defined in terms of ion activities which can be approximated with the concentrations as calculated here.\nThe Nernst equation plays an important role in understanding electrical effects in cells and organelles. Such effects include nerve synapses and cardiac beat as well as the resting potential of a somatic cell.\nBattery.\nMany types of battery have been commercialized and represent an important practical application of electrochemistry. Early wet cells powered the first telegraph and telephone systems, and were the source of current for electroplating. The zinc-manganese dioxide dry cell was the first portable, non-spillable battery type that made flashlights and other portable devices practical. The mercury battery using zinc and mercuric oxide provided higher levels of power and capacity than the original dry cell for early electronic devices, but has been phased out of common use due to the danger of mercury pollution from discarded cells.\nThe lead\u2013acid battery was the first practical secondary (rechargeable) battery that could have its capacity replenished from an external source. The electrochemical reaction that produced current was (to a useful degree) reversible, allowing electrical energy and chemical energy to be interchanged as needed. Common lead acid batteries contain a mixture of sulfuric acid and water, as well as lead plates. The most common mixture used today is 30% acid. One problem, however, is if left uncharged acid will crystallize within the lead plates of the battery rendering it useless. These batteries last an average of 3 years with daily use but it is not unheard of for a lead acid battery to still be functional after 7\u201310 years. Lead-acid cells continue to be widely used in automobiles.\nAll the preceding types have water-based electrolytes, which limits the maximum voltage per cell. The freezing of water limits low temperature performance. The lithium metal battery, which does not (and cannot) use water in the electrolyte, provides improved performance over other types; a rechargeable lithium-ion battery is an essential part of many mobile devices.\nThe flow battery, an experimental type, offers the option of vastly larger energy capacity because its reactants can be replenished from external reservoirs. The fuel cell can turn the chemical energy bound in hydrocarbon gases or hydrogen and oxygen directly into electrical energy with a much higher efficiency than any combustion process; such devices have powered many spacecraft and are being applied to grid energy storage for the public power system.\nCorrosion.\nCorrosion is an electrochemical process, which reveals itself as rust or tarnish on metals like iron or copper and their respective alloys, steel and brass.\nIron corrosion.\nFor iron rust to occur the metal has to be in contact with oxygen and water. The chemical reactions for this process are relatively complex and not all of them are completely understood. It is believed the causes are the following:\nElectron transfer (reduction-oxidation)\nOne area on the surface of the metal acts as the anode, which is where the oxidation (corrosion) occurs. At the anode, the metal gives up electrons.\nFe(s) \u2192 Fe2+(aq) + 2 e\u2212\nElectrons are transferred from iron, reducing oxygen in the atmosphere into water on the cathode, which is placed in another region of the metal.\nO2(g) + 4 H+(aq) + 4 e\u2212 \u2192 2 H2O(l)\nGlobal reaction for the process:\n2 Fe(s) + O2(g) + 4 H+(aq) \u2192 2 Fe2+(aq) + 2 H2O(l)\nStandard emf for iron rusting:\n\"E\"\u00b0 = \"E\"\u00b0 (cathode) \u2212 \"E\"\u00b0 (anode)\n\"E\"\u00b0 = 1.23V \u2212 (\u22120.44 V) = 1.67 V\nIron corrosion takes place in an acid medium; H+ ions come from reaction between carbon dioxide in the atmosphere and water, forming carbonic acid. Fe2+ ions oxidize further, following this equation:\n 4 Fe2+(aq) + O2(g) + (4+2x) H2O(l) \u2192 2 Fe2O3\u00b7xH2O + 8 H+(aq)\nIron(III) oxide hydrate is known as rust. The concentration of water associated with iron oxide varies, thus the chemical formula is represented by Fe2O3\u00b7xH2O.\nAn electric circuit is formed as passage of electrons and ions occurs; thus if an electrolyte is present it will facilitate oxidation, explaining why rusting is quicker in salt water.\nCorrosion of common metals.\nCoinage metals, such as copper and silver, slowly corrode through use.\nA patina of green-blue copper carbonate forms on the surface of copper with exposure to the water and carbon dioxide in the air. Silver coins or cutlery that are exposed to high sulfur foods such as eggs or the low levels of sulfur species in the air develop a layer of black silver sulfide.\nGold and platinum are extremely difficult to oxidize under normal circumstances, and require exposure to a powerful chemical oxidizing agent such as aqua regia.\nSome common metals oxidize extremely rapidly in air. Titanium and aluminium oxidize instantaneously in contact with the oxygen in the air. These metals form an extremely thin layer of oxidized metal on the surface, which bonds with the underlying metal. This thin oxide layer protects the underlying bulk of the metal from the air preventing the entire metal from oxidizing. These metals are used in applications where corrosion resistance is important. Iron, in contrast, has an oxide that forms in air and water, called rust, that does not bond with the iron and therefore does not stop the further oxidation of the iron. Thus iron left exposed to air and water will continue to rust until all of the iron is oxidized.\nPrevention of corrosion.\nAttempts to save a metal from becoming anodic are of two general types. Anodic regions dissolve and destroy the structural integrity of the metal.\nWhile it is almost impossible to prevent anode/cathode formation, if a non-conducting material covers the metal, contact with the electrolyte is not possible and corrosion will not occur.\nCoating.\nMetals can be coated with paint or other less conductive metals (\"passivation\"). This prevents the metal surface from being exposed to electrolytes. Scratches exposing the metal substrate will result in corrosion. The region under the coating adjacent to the scratch acts as the anode of the reaction.\nSacrificial anodes.\nA method commonly used to protect a structural metal is to attach a metal which is more anodic than the metal to be protected. This forces the structural metal to be cathodic, thus spared corrosion. It is called \"sacrificial\" because the anode dissolves and has to be replaced periodically.\nZinc bars are attached to various locations on steel ship hulls to render the ship hull cathodic. The zinc bars are replaced periodically. Other metals, such as magnesium, would work very well but zinc is the least expensive useful metal.\nTo protect pipelines, an ingot of buried or exposed magnesium (or zinc) is buried beside the pipeline and is connected electrically to the pipe above ground. The pipeline is forced to be a cathode and is protected from being oxidized and rusting. The magnesium anode is sacrificed. At intervals new ingots are buried to replace those dissolved.\nElectrolysis.\nThe spontaneous redox reactions of a conventional battery produce electricity through the different reduction potentials of the cathode and anode in the electrolyte. However, electrolysis requires an external source of electrical energy to induce a chemical reaction, and this process takes place in a compartment called an electrolytic cell.\nElectrolysis of molten sodium chloride.\nWhen molten, the salt sodium chloride can be electrolyzed to yield metallic sodium and gaseous chlorine. Industrially this process takes place in a special cell named Downs cell. The cell is connected to an electrical power supply, allowing electrons to migrate from the power supply to the electrolytic cell.\nReactions that take place in a Downs cell are the following:\nAnode (oxidation): 2 Cl\u2212(l) \u2192 Cl2(g) + 2 e\u2212\nCathode (reduction): 2 Na+(l) + 2 e\u2212 \u2192 2 Na(l)\nOverall reaction: 2 Na+(l) + 2 Cl\u2212(l) \u2192 2 Na(l) + Cl2(g)\nThis process can yield large amounts of metallic sodium and gaseous chlorine, and is widely used in mineral dressing and metallurgy industries.\nThe emf for this process is approximately \u22124\u00a0V indicating a (very) non-spontaneous process. In order for this reaction to occur the power supply should provide at least a potential difference of 4\u00a0V. However, larger voltages must be used for this reaction to occur at a high rate.\nElectrolysis of water.\nWater can be converted to its component elemental gases, H2 and O2, through the application of an external voltage. Water does not decompose into hydrogen and oxygen spontaneously as the Gibbs free energy change for the process at standard conditions is very positive, about 474.4 kJ. The decomposition of water into hydrogen and oxygen can be performed in an electrolytic cell. In it, a pair of inert electrodes usually made of platinum immersed in water act as anode and cathode in the electrolytic process. The electrolysis starts with the application of an external voltage between the electrodes. This process will not occur except at extremely high voltages without an electrolyte such as sodium chloride or sulfuric acid (most used 0.1 M).\nBubbles from the gases will be seen near both electrodes. The following half reactions describe the process mentioned above:\nAnode (oxidation): 2 H2O(l) \u2192 O2(g) + 4 H+(aq) + 4 e\u2212\nCathode (reduction): 2 H2O(g) + 2 e\u2212 \u2192 H2(g) + 2 OH\u2212(aq)\nOverall reaction: 2 H2O(l) \u2192 2 H2(g) + O2(g)\nAlthough strong acids may be used in the apparatus, the reaction will not net consume the acid. While this reaction will work at any conductive electrode at a sufficiently large potential, platinum catalyzes both hydrogen and oxygen formation, allowing for relatively low voltages (~2 V depending on the pH).\nElectrolysis of aqueous solutions.\nElectrolysis in an aqueous solution is a similar process as mentioned in electrolysis of water. However, it is considered to be a complex process because the contents in solution have to be analyzed in half reactions, whether reduced or oxidized.\nElectrolysis of a solution of sodium chloride.\nThe presence of water in a solution of sodium chloride must be examined in respect to its reduction and oxidation in both electrodes. Usually, water is electrolysed as mentioned above in electrolysis of water yielding \"gaseous oxygen in the anode\" and gaseous hydrogen in the cathode. On the other hand, sodium chloride in water dissociates in Na+ and Cl\u2212 ions. The cation, which is the positive ion, will be attracted to the cathode (\u2212), thus reducing the sodium ion. The chloride anion will then be attracted to the anode (+), where it is oxidized to chlorine gas.\nThe following half reactions should be considered in the process mentioned:\nReaction 1 is discarded as it has the most negative value on standard reduction potential thus making it less thermodynamically favorable in the process.\nWhen comparing the reduction potentials in reactions 2 and 4, the oxidation of chloride ion is favored over oxidation of water, thus chlorine gas is produced at the anode and not oxygen gas.\nAlthough the initial analysis is correct, there is another effect, known as the overvoltage effect. Additional voltage is sometimes required, beyond the voltage predicted by the \"E\"\u00b0cell. This may be due to kinetic rather than thermodynamic considerations. In fact, it has been proven that the activation energy for the chloride ion is very low, hence favorable in kinetic terms. In other words, although the voltage applied is thermodynamically sufficient to drive electrolysis, the rate is so slow that to make the process proceed in a reasonable time frame, the voltage of the external source has to be increased (hence, overvoltage).\nThe overall reaction for the process according to the analysis is the following:\nAnode (oxidation): 2 Cl\u2212(aq) \u2192 Cl2(g) + 2 e\u2212\nCathode (reduction): 2 H2O(l) + 2 e\u2212 \u2192 H2(g) + 2 OH\u2212(aq)\nOverall reaction: 2 H2O + 2 Cl\u2212(aq) \u2192 H2(g) + Cl2(g) + 2 OH\u2212(aq)\nAs the overall reaction indicates, the concentration of chloride ions is reduced in comparison to OH\u2212 ions (whose concentration increases). The reaction also shows the production of gaseous hydrogen, chlorine and aqueous sodium hydroxide.\nQuantitative electrolysis and Faraday's laws.\nQuantitative aspects of electrolysis were originally developed by Michael Faraday in 1834. Faraday is also credited to have coined the terms \"electrolyte\", electrolysis, among many others while he studied quantitative analysis of electrochemical reactions. Also he was an advocate of the law of conservation of energy.\nFirst law.\nFaraday concluded after several experiments on electric current in a non-spontaneous process that the mass of the products yielded on the electrodes was proportional to the value of current supplied to the cell, the length of time the current existed, and the molar mass of the substance analyzed. In other words, the amount of a substance deposited on each electrode of an electrolytic cell is directly proportional to the quantity of electricity passed through the cell.\nBelow is a simplified equation of Faraday's first law:\nformula_16\nwhere\n\"m\" is the mass of the substance produced at the electrode (in grams),\n\"Q\" is the total electric charge that passed through the solution (in coulombs),\n\"n\" is the valence number of the substance as an ion in solution (electrons per ion),\n\"M\" is the molar mass of the substance (in grams per mole),\n\"F\" is Faraday's constant (96485 coulombs per mole).\nSecond law.\nFaraday devised the laws of chemical electrodeposition of metals from solutions in 1857. He formulated the second law of electrolysis stating \"the amounts of bodies which are equivalent to each other in their ordinary chemical action have equal quantities of electricity naturally associated with them.\" In other words, the quantities of different elements deposited by a given amount of electricity are in the ratio of their chemical equivalent weights.\nAn important aspect of the second law of electrolysis is electroplating, which together with the first law of electrolysis has a significant number of applications in industry, as when used to protectively coat metals to avoid corrosion.\nApplications.\nThere are various important electrochemical processes in both nature and industry, like the coating of objects with metals or metal oxides through electrodeposition, the addition (electroplating) or removal (electropolishing) of thin layers of metal from an object's surface, and the detection of alcohol in drunk drivers through the redox reaction of ethanol. The generation of chemical energy through photosynthesis is inherently an electrochemical process, as is production of metals like aluminum and titanium from their ores. Certain diabetes blood sugar meters measure the amount of glucose in the blood through its redox potential. In addition to established electrochemical technologies (like deep cycle lead acid batteries) there is also a wide range of new emerging technologies such as fuel cells, large format lithium-ion batteries, electrochemical reactors and super-capacitors that are becoming increasingly commercial. Electrochemical or coulometric titrations were introduced for quantitative analysis of minute quantities in 1938 by the Hungarian chemists L\u00e1szl\u00f3 Szebell\u00e9dy and Zoltan Somogyi. Electrochemistry also has important applications in the food industry, like the assessment of food/package interactions, the analysis of milk composition, the characterization and the determination of the freezing end-point of ice-cream mixes, or the determination of free acidity in olive oil.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9602", "revid": "45205944", "url": "https://en.wikipedia.org/wiki?curid=9602", "title": "Edinburgh", "text": "Capital city of Scotland\nEdinburgh is the capital city of Scotland and one of its 32 council areas. It is located in southeast Scotland and is bounded to the north by the Firth of Forth and to the south by the Pentland Hills. Edinburgh had a population of\n in 2020, making it the second-most-populous city in Scotland and the seventh-most-populous in the United Kingdom. The wider metropolitan area had a population of 912,490 in the same year.\nRecognised as the capital of Scotland since at least the 15th century, Edinburgh is the seat of the Scottish Government, the Scottish Parliament, the highest courts in Scotland, and the Palace of Holyroodhouse, the official residence of the British monarch in Scotland. It is also the annual venue of the General Assembly of the Church of Scotland. The city has long been a centre of education, particularly in the fields of medicine, Scottish law, literature, philosophy, the sciences and engineering. The University of Edinburgh was founded in 1582 and is now one of three universities in the city. The financial centre of Scotland, Edinburgh was in 2020 ranked the second-largest financial centre in the United Kingdom, the fourth-largest in Europe, and the thirteenth-largest in the world in the Global Financial Centres Index.\nThe city is a cultural centre, and is the home of institutions including the National Museum of Scotland, the National Library of Scotland, and the Scottish National Gallery. The city is also known for the Edinburgh International Festival and the Fringe, the latter being the world's largest annual international arts festival. Historic sites in Edinburgh include Edinburgh Castle, the Palace of Holyroodhouse, St Giles' Cathedral, Greyfriars Kirk, Canongate Kirk and the extensive Georgian New Town built in the 18th and 19th centuries. The Old Town and the New Town are together listed as a World Heritage Site by UNESCO, and the site has been managed by Edinburgh World Heritage since 1999. The city's historical and cultural attractions have made it Britain's second-most-visited tourist destination, attracting 5.3 million visits, including 2.4 million from overseas, in 2023.\nEdinburgh is governed by the City of Edinburgh Council, a unitary authority. The City of Edinburgh council area had an estimated population of 530,680 in 2024, and includes outlying towns and villages. The city is in the Lothian region and was historically part of the shire of Midlothian (also called Edinburghshire).\nEtymology.\n\"Edin\", the root of the city's name, derives from \"\", the name for the region in Cumbric, the Brittonic Celtic language formerly spoken there. The name's meaning is unknown. The district of Eidyn was centred on the stronghold of Din Eidyn, the dun or hillfort of Eidyn. This stronghold is believed to have been located at Castle Rock, now the site of Edinburgh Castle. A siege of Din Eidyn by Oswald, king of the Angles of Northumbria in 638 marked the beginning of three centuries of Germanic influence in south east Scotland that laid the foundations for the development of Scots, before the town was ultimately subsumed in 954 by the kingdom known to the English as Scotland. As the language shifted from Cumbric to Northumbrian Old English and then Scots, the Brittonic \"din\" in Din Eidyn was replaced by \"burh\", producing \"Edinburgh\". In Scottish Gaelic \"din\" becomes \"d\u00f9n\", producing modern \"D\u00f9n \u00c8ideann\".\nNicknames.\nThe city is nicknamed \"Auld Reekie\", Scots for \"Old Smoky\", for the views from the country of the smoke-covered Old Town. A note in a collection of the works of the poet Allan Ramsay explains, \"Auld Reeky...A name the country people give Edinburgh, from the cloud of smoke or reek that is always impending over it.\" In Walter Scott's 1820 novel \"The Abbot\", a character observes that \"yonder stands Auld Reekie\u2014you may see the smoke hover over her at twenty miles' distance\". Thomas Carlyle comments on the phenomenon: \"Smoke cloud hangs over old Edinburgh, for, ever since Aeneas Silvius's time and earlier, the people have the art, very strange to Aeneas, of burning a certain sort of black stones, and Edinburgh with its chimneys is called 'Auld Reekie' by the country people\". The 19th-century historian Robert Chambers asserted that the sobriquet could not be traced before the reign of King Charles II in the late 17th century. He attributed the name to a Fife laird, Durham of Largo, who regulated the bedtime of his children by the smoke rising above Edinburgh from the fires of the tenements. \"It's time now bairns, to tak' the beuks, and gang to our beds, for yonder's Auld Reekie, I see, putting on her nicht-cap!\".\nEdinburgh has been popularly called the \"Athens of the North\" since the early 19th century. References to Athens, such as \"Athens of Britain\" and \"Modern Athens\", had been made as early as the 1760s. The similarities were seen to be topographical but also intellectual. Edinburgh's Castle Rock reminded returning grand tourists of the Athenian Acropolis, as did aspects of the neoclassical architecture and layout of New Town. In 1818, naturalist Edward Daniel Clarke called Edinburgh \"a very correct model of a Grecian city\", pointing out perceived similarities between both cities and their ports (respectively, Leith and Piraeus). Intellectually, the Scottish Enlightenment, with its humanist and rationalist outlook, was influenced by Ancient Greek philosophy. In 1822 the Scottish landscape painter Hugh William Williams organised an exhibition that showed his paintings of Athens alongside views of Edinburgh, and the idea of a direct parallel between both cities quickly caught the popular imagination. When plans were drawn up in the early 19th century to architecturally develop Calton Hill, the design of the National Monument directly copied Athens' Parthenon. Tom Stoppard's character Archie of \"Jumpers\" said, perhaps playing on Reykjav\u00edk meaning \"smoky bay\", that the \"Reykjav\u00edk of the South\" would be more appropriate.\nThe city has also been known by several Latin names, such as \"Edinburgum\", while the adjectival forms \"Edinburgensis\" and \"Edinensis\" are used in educational and scientific contexts.\n\"Edina\" is a late 18th-century poetical form used by the Scottish poets Robert Fergusson and Robert Burns. \"Embra\" or \"Embro\" are colloquialisms from the same time, as in Robert Garioch's \"Embro to the Ploy\".\nBen Jonson described it as \"Britaine's other eye\", and Sir Walter Scott referred to it as \"yon Empress of the North\". Robert Louis Stevenson, also a son of the city, wrote that Edinburgh \"is what Paris ought to be\".\nHistory.\nEarly history.\nThe earliest known human habitation in the Edinburgh area was at Cramond, where evidence was found of a Mesolithic camp site dated to c. 8500 BC. Traces of later Bronze Age and Iron Age settlements have been found on Castle Rock, Arthur's Seat, Craiglockhart Hill, and the Pentland Hills.\nWhen the Romans arrived in Lothian at the end of the 1st century AD, they found a Brittonic Celtic tribe whose name they recorded as the Votadini. The Votadini transitioned into the Gododdin kingdom in the Early Middle Ages, with Eidyn serving as one of the kingdom's districts. During this period, the Castle Rock site, thought to have been the stronghold of Din Eidyn, emerged as the kingdom's major centre. The medieval Welsh-language poem \"Y Gododdin\" describes a war band from across the Brittonic world who gathered in Eidyn before a fateful raid; this may describe a historical event around AD 600.\nIn 638 the Gododdin stronghold was besieged by forces loyal to King Oswald of Northumbria, and around this time, control of Lothian passed to the Angles. Their influence continued for the next three centuries until around 950, when, during the reign of Indulf, son of Constantine II, the \"burh\" (fortress), named in the 10th-century \"Pictish Chronicle\" as \"oppidum Eden\", was abandoned to the Scots. It thenceforth remained, for the most part, under their jurisdiction.\nThe royal burgh was founded by King David I in the early 12th century on land belonging to the Crown, though the date of its charter is unknown. The first documentary evidence of the medieval burgh is a royal charter, c.\u20091124\u20131127, by King David I granting a toft in to the Priory of Dunfermline. The shire of Edinburgh seems also to have been created during David's reign, possibly covering all of Lothian at first, but by 1305 the eastern and western parts of Lothian had become Haddingtonshire and Linlithgowshire, leaving Edinburgh as the county town of a shire covering the central part of Lothian, which was called Edinburghshire or Midlothian (the latter name being an informal, but commonly used, alternative until the county's name was legally changed in 1947).\nEdinburgh was largely under English control from 1291 to 1314 and from 1333 to 1341, during the Wars of Scottish Independence. When the English invaded Scotland in 1298, Edward I of England chose not to enter Edinburgh but passed by it with his army.\nIn the middle of the 14th century the French chronicler Jean Froissart described it as the capital of Scotland (c. 1365), and James III (1451\u20131488) referred to it in the 15th century as \"the principal burgh of our kingdom\". In 1482 James III \"granted and perpetually confirmed to the said Provost, Bailies, Clerk, Council, and Community, and their successors, the office of Sheriff within the Burgh for ever, to be exercised by the Provost for the time as Sheriff, and by the Bailies for the time as Sheriffsdepute conjunctly and severally; with full power to hold Courts, to punish transgressors not only by banishment but by death, to appoint officers of Court, and to do everything else appertaining to the office of Sheriff; as also to apply to their own proper use the fines and escheats arising out of the exercise of the said office.\" Despite being burnt by the English in 1544, Edinburgh continued to develop and grow, and was at the centre of events in the 16th-century Scottish Reformation and 17th-century Wars of the Covenant. In 1582 Edinburgh's town council was given a royal charter by King James VI and I permitting the establishment of a university; founded as \"Tounis College\" (Town's College), the institution developed into the University of Edinburgh, which contributed to Edinburgh's central intellectual role in subsequent centuries.\n17th century.\nIn 1603 King James VI of Scotland succeeded to the English throne, uniting the crowns of Scotland and England, an event known as the Union of the Crowns, though the two kingdoms remained separate realms governed in personal union. In 1638 King Charles I's attempt to introduce Anglican church forms in Scotland encountered stiff Presbyterian opposition, culminating in the conflicts of the Wars of the Three Kingdoms. Subsequent Scottish support for Charles II's restoration to the throne of England resulted in Edinburgh's occupation by Oliver Cromwell's Commonwealth of England forces \u2013 the New Model Army \u2013 in 1650.\nIn the 17th century Edinburgh's boundaries were still defined by the city's defensive town walls. As a result, the city's growing population was accommodated by increasing the height of the houses. Buildings of 11 storeys or more were common, and have been described as forerunners of the modern-day skyscraper. Most of these old structures were replaced by the predominantly Victorian buildings seen in today's Old Town. In 1611 an act of parliament created the High Constables of Edinburgh to keep order in the city, thought to be the oldest statutory police force in the world.\n18th century.\nFollowing the Treaty of Union in 1706, the Parliaments of England and Scotland passed Acts of Union in 1706 and 1707 respectively, uniting the two kingdoms in the Kingdom of Great Britain effective from 1 May 1707. As a consequence, the Parliament of Scotland merged with the Parliament of England to form the Parliament of Great Britain, which sat at Westminster in London. The Union was opposed by many Scots, resulting in riots in the city.\nBy the first half of the 18th century, Edinburgh was described as one of Europe's most densely populated, overcrowded, and unsanitary towns. Visitors were struck by the fact that the social classes shared the same urban space, even inhabiting the same tenement buildings; although here a form of social segregation did prevail, whereby shopkeepers and tradesmen tended to occupy the cheaper-to-rent cellars and garrets, while the more well-to-do professional classes occupied the more expensive middle storeys.\nDuring the Jacobite rising of 1745, Edinburgh was briefly occupied by the Jacobite \"Highland Army\" before its march into England. After its eventual defeat at Culloden, there followed a period of reprisals and pacification, largely directed at the rebellious clans. In Edinburgh, the Town Council, keen to emulate London by initiating city improvements and expansion to the north of the castle, reaffirmed its belief in the Union and loyalty to the Hanoverian monarch George III by its choice of names for the streets of the New Town: for example, Rose Street and Thistle Street; and for the royal family, George Street, Queen Street, Hanover Street, Frederick Street and Princes Street (in honour of George's two sons). The consistently geometric layout of the plan for the extension of Edinburgh was the result of a major competition in urban planning staged by the Town Council in 1766.\nIn the second half of the century, the city was at the heart of the Scottish Enlightenment, when thinkers like David Hume, Adam Smith, James Hutton and Joseph Black were familiar figures in its streets. Edinburgh became a major intellectual centre, earning it the nickname \"Athens of the North\" because of its many neo-classical buildings and reputation for learning, recalling ancient Athens. In the 18th-century novel \"The Expedition of Humphry Clinker\" by Tobias Smollett one character describes Edinburgh as a \"hotbed of genius\". Edinburgh was also a major centre for the Scottish book trade. The highly successful London bookseller Andrew Millar was apprenticed there to James McEuen.\nFrom the 1770s onwards, the professional and business classes gradually deserted the Old Town in favour of the more elegant \"one-family\" residences of the New Town, a migration that changed the city's social character. According to the foremost historian of this development, \"Unity of social feeling was one of the most valuable heritages of old Edinburgh, and its disappearance was widely and properly lamented.\"\n19th and 20th centuries.\nDespite an enduring myth to the contrary, Edinburgh became an industrial centre with its traditional industries of printing, brewing and distilling continuing to grow in the 19th century and joined by new industries such as rubber works, engineering works and others. By 1821 Edinburgh had been overtaken by Glasgow as Scotland's largest city. The city centre between Princes Street and George Street became a major commercial and shopping district, a development partly stimulated by the arrival of railways in the 1840s. The Old Town became an increasingly dilapidated, overcrowded slum with high mortality rates. Improvements carried out under Lord Provost William Chambers in the 1860s began the transformation of the area into the predominantly Victorian Old Town seen today. More improvements followed in the early 20th century as a result of the work of Patrick Geddes, but relative economic stagnation during the two world wars and beyond saw the Old Town deteriorate further before major slum clearance in the 1960s and 1970s began to reverse the process. New construction, such as of Argyle House near the castle and the University building developments, which transformed the George Square and Potterrow areas, proved highly controversial.\nSince the 1990s a new \"financial district\", including the Edinburgh International Conference Centre, has grown mainly on demolished railway property to the west of the castle, stretching into Fountainbridge, a run-down 19th-century industrial suburb which has undergone radical change since the 1980s with the demise of industrial and brewery premises. This ongoing development has enabled Edinburgh to maintain its place as the United Kingdom's second largest financial and administrative centre after London. Financial services now account for a third of all commercial office space in the city. The development of Edinburgh Park, a new business and technology park covering , west of the city centre, has also contributed to the District Council's strategy for the city's major economic regeneration.\nIn 1998 the Scotland Act, which came into force the following year, established a devolved Scottish Parliament and Scottish Executive (renamed the Scottish Government since September 2007). Both based in Edinburgh, they are responsible for governing Scotland while reserved matters such as defence, foreign affairs, and some elements of income tax remain the responsibility of the Parliament of the United Kingdom in London.\n21st century.\nIn 2022 Edinburgh was affected by the 2022 Scotland bin strikes. In 2023 Edinburgh became the first capital city in Europe to sign the global Plant Based Treaty, which was introduced at COP26 in 2021 in Glasgow. The Scottish Greens councillor Steve Burgess introduced the treaty. The Scottish Countryside Alliance and other farming groups called the treaty \"anti-farming\".\nGeography.\nLocation.\nSituated in Scotland's Central Belt, Edinburgh lies on the southern shore of the Firth of Forth. The city centre is southwest of the shoreline of Leith and inland, as the crow flies, from the east coast of Scotland and the North Sea at Dunbar. While the early burgh grew up near the prominent Castle Rock, the modern city is often said to be built on seven hills, namely Calton Hill, Corstorphine Hill, Craiglockhart Hill, Braid Hill, Blackford Hill, Arthur's Seat and the Castle Rock, giving rise to allusions to the seven hills of Rome.\nCityscape.\nOccupying a narrow gap between the Firth of Forth to the north and the Pentland Hills and their outrunners to the south, the city sprawls over a landscape which is the product of early volcanic activity and later periods of intensive glaciation.\n Igneous activity between 350 and 400 million years ago, coupled with faulting, led to the creation of tough basalt volcanic plugs, which predominate over much of the area. One such example is the Castle Rock which forced the advancing ice sheet to divide, sheltering the softer rock and forming a tail of material to the east, thus creating a distinctive crag and tail formation. Glacial erosion on the north side of the crag gouged a deep valley later filled by the now drained Nor Loch. These features, along with another hollow on the rock's south side, formed an ideal natural strongpoint upon which Edinburgh Castle was built. Similarly, Arthur's Seat is the remains of a volcano dating from the Carboniferous period, which was eroded by a glacier moving west to east during the ice age. Erosive action such as plucking and abrasion exposed the rocky crags to the west before leaving a tail of deposited glacial material swept to the east. This process formed the distinctive Salisbury Crags, a series of teschenite cliffs between Arthur's Seat and the location of the early burgh. The residential areas of Marchmont and Bruntsfield are built along a series of drumlin ridges south of the city centre, which were deposited as the glacier receded.\nOther prominent landforms, such as Calton Hill and Corstorphine Hill, are also products of glacial erosion. The Braid Hills and Blackford Hill are a series of small summits to the south of the city centre that command expansive views looking northwards over the urban area to the Firth of Forth.\nEdinburgh is drained by the river named the Water of Leith, which rises at the Colzium Springs in the Pentland Hills and runs for through the south and west of the city, emptying into the Firth of Forth at Leith. The nearest the river gets to the city centre is at Dean Village on the north-western edge of the New Town, where a deep gorge is spanned by Thomas Telford's Dean Bridge, built in 1832 for the road to Queensferry. The Water of Leith Walkway is a mixed-use trail that follows the course of the river for from Balerno to Leith.\nExcepting the shoreline of the Firth of Forth, Edinburgh is encircled by a green belt, designated in 1957, which stretches from Dalmeny in the west to Prestongrange in the east. With an average width of the principal objectives of the green belt were to contain the outward expansion of the city and to prevent the agglomeration of urban areas. Expansion affecting the green belt is strictly controlled but developments such as Edinburgh Airport and the Royal Highland Showground at Ingliston lie within the zone. Similarly, suburbs such as Juniper Green and Balerno are situated on green belt land. One feature of the Edinburgh green belt is the inclusion of parcels of land within the city which are designated green belt, even though they do not connect with the peripheral ring. Examples of these independent wedges of green belt include Holyrood Park and Corstorphine Hill.\nAreas.\nEarly settlements.\nEdinburgh includes former towns and villages that retain much of their original character as settlements in existence before they were absorbed into the expanding city of the nineteenth and twentieth centuries. Many areas, such as Dalry, contain residences that are multi-occupancy buildings known as tenements, although the more southern and western parts of the city have traditionally been less built-up with a greater number of detached and semi-detached villas.\nThe historic centre of Edinburgh is divided into two by the broad green swathe of Princes Street Gardens. To the south, the view is dominated by Edinburgh Castle, built high on Castle Rock, and the long sweep of the Old Town descending towards Holyrood Palace. To the north lie Princes Street and the New Town.\nThe West End includes the financial district, with insurance and banking offices as well as the Edinburgh International Conference Centre.\nOld and New Towns.\nEdinburgh's Old and New Towns were listed as a UNESCO World Heritage Site in 1995 in recognition of the unique character of the Old Town with its medieval street layout and the planned Georgian New Town, including the adjoining Dean Village and Calton Hill areas. There are over 4,500 listed buildings within the city, a higher proportion relative to area than any other city in the United Kingdom.\nThe castle is perched on top of a rocky crag (the remnant of an extinct volcano), and the Royal Mile runs down the crest of a ridge from it, terminating at Holyrood Palace. Minor streets (called closes or wynds) lie on either side of the main spine, forming a herringbone pattern. Due to space restrictions imposed by the narrowness of this landform, the Old Town became home to some of the earliest \"high rise\" residential buildings. Multi-storey dwellings known as \"lands\" were the norm from the 16th century onwards, with ten and eleven storeys being typical, and one even reaching fourteen or fifteen storeys. Vaults below street level were inhabited to accommodate the influx of incomers, particularly Irish immigrants, during the Industrial Revolution. The street has several fine public buildings such as St Giles' Cathedral, the City Chambers and the Law Courts. Other places of historical interest nearby are Greyfriars Kirkyard and Mary King's Close. The Grassmarket, running deep below the castle, is connected by the steep double terraced Victoria Street. The street layout is typical of the old quarters of many Northern European cities.\nThe New Town was an 18th-century solution to the problem of an increasingly crowded city, which had been confined to the ridge sloping down from the castle. In 1766 a competition to design a \"New Town\" was won by James Craig, a 27-year-old architect. The plan was a rigid, ordered grid, which fitted in well with Enlightenment ideas of rationality. The principal street was to be George Street, running along the natural ridge to the north of what became known as the \"Old Town\". To either side of it are two other main streets: Princes Street and Queen Street. Princes Street has become Edinburgh's main shopping street and now has few of its Georgian buildings in their original state. The three main streets are connected by a series of streets running perpendicular to them. The east and west ends of George Street are terminated by St Andrew Square and Charlotte Square respectively. The latter, designed by Robert Adam, influenced the architectural style of the New Town into the early 19th century. Bute House, the official residence of the First Minister of Scotland, is on the north side of Charlotte Square.\nThe hollow between the Old and New Towns was formerly the Nor Loch, which was created for the town's defence but came to be used by the inhabitants for dumping their sewage. It was drained by the 1820s as part of the city's northward expansion. Craig's original plan included an ornamental canal on the site of the loch, but this idea was abandoned. Soil excavated while laying the foundations of buildings in the New Town was dumped on the site of the loch to create the slope connecting the Old and New Towns known as The Mound.\nIn the middle of the 19th century the National Gallery of Scotland and the Royal Scottish Academy Building were built on The Mound, and tunnels for the railway line between Haymarket and Waverley stations were driven through it.\nSouthside.\nThe Southside is a residential part of the city, which includes the districts of St Leonards, Marchmont, Morningside, Newington, Sciennes, the Grange and Blackford. The Southside is broadly analogous to the area covered formerly by the Burgh Muir, and was developed as a residential area after the opening of the South Bridge in the 1780s. The Southside is particularly popular with families (many state and private schools are here), young professionals and students (the central University of Edinburgh campus is based around George Square just north of Marchmont and the Meadows), and Napier University (with major campuses around Merchiston and Morningside). The area is also well provided with hotels and \"bed and breakfast\" accommodation for visiting festival-goers. These districts often feature in works of fiction. For example, Church Hill in Morningside, was the home of Muriel Spark's Miss Jean Brodie, and Ian Rankin's Inspector Rebus lives in Marchmont and works in St Leonards.\nLeith.\nLeith was historically the port of Edinburgh, an arrangement of unknown date that was confirmed by the royal charter Robert the Bruce granted to the city in 1329. The port developed a separate identity from Edinburgh, which to some extent it still retains, and it was a matter of great resentment when the two burghs merged in 1920 into the City of Edinburgh. Even today, the parliamentary seat is known as \"Edinburgh North and Leith\". The loss of traditional industries and commerce (the last shipyard closed in 1983) resulted in economic decline. The Edinburgh Waterfront development has transformed old dockland areas from Leith to Granton into residential areas with shopping and leisure facilities and helped rejuvenate the area. With the redevelopment, Edinburgh has gained the business of cruise liner companies, which now provide cruises to Norway, Sweden, Denmark, Germany, and the Netherlands.\nThe coastal suburb of Portobello is characterised by Georgian villas, Victorian tenements, a beach and promenade, and caf\u00e9s, bars, restaurants and independent shops. There are rowing and sailing clubs, a restored Victorian swimming pool, and Victorian Turkish baths.\nUrban area.\nThe urban area of Edinburgh is almost entirely within the City of Edinburgh Council boundary, merging with Musselburgh in East Lothian. Towns within easy reach of the city boundary include Inverkeithing, Haddington, Tranent, Prestonpans, Dalkeith, Bonnyrigg, Loanhead, Penicuik, Broxburn, Livingston and Dunfermline. Edinburgh lies at the heart of the Edinburgh &amp; South East Scotland City region, with a population in 2014 of 1,339,380.\nClimate.\nLike most of Scotland, Edinburgh has a cool temperate maritime climate (\"Cfb\") which, despite its northerly latitude, is milder than places which lie at similar latitudes such as Moscow and Labrador. The city's proximity to the sea mitigates any large variations in temperature or extremes of climate. Winter daytime temperatures rarely fall below freezing while summer temperatures are moderate, rarely exceeding . The highest temperature recorded in the city was on 25 July 2019 at Gogarbank, beating the previous record of on 4 August 1975 at Edinburgh Airport. The lowest temperature recorded in recent years was during December 2010 at Gogarbank.\nGiven Edinburgh's position between the coast and hills, it is renowned as \"the windy city\", with the prevailing wind direction coming from the south-west, which is often associated with warm, unstable air from the North Atlantic Current that can give rise to rainfall \u2013 although considerably less than cities to the west, such as Glasgow. Rainfall is distributed fairly evenly throughout the year. Winds from an easterly direction are usually drier but considerably colder, and may be accompanied by haar, a persistent coastal fog. Vigorous Atlantic depressions, known as European windstorms, can affect the city between October and April.\nLocated slightly north of the city centre, the weather station at the Royal Botanic Garden Edinburgh (RBGE) has been an official weather station for the Met Office since 1956.\nThe Met Office operates its own weather station at Gogarbank on the city's western outskirts, near Edinburgh Airport. This slightly inland station has a slightly wider temperature span between seasons, is cloudier and somewhat wetter, but differences are minor.\nTemperature and rainfall records have been kept at the Royal Observatory since 1764.\nDemography.\nCurrent.\nThe most recent official population estimates (2020) are\n for the locality (includes Currie), and\n for the Edinburgh settlement (includes Musselburgh).\nEdinburgh has a high proportion of young adults, with 19.5% of the population in their 20s (exceeded only by Aberdeen) and 15.2% in their 30s which is the highest in Scotland. The proportion of Edinburgh's population born in the UK fell from 92% to 84% between 2001 and 2011, while the proportion of White Scottish-born fell from 78% to 70%. Of those Edinburgh residents born in the UK, 335,000 or 83% were born in Scotland, with 58,000 or 14% being born in England.\nSome 16,000 people or 3.2% of the city's population are of Polish descent. 77,800 people or 15.1% of Edinburgh's population class themselves as Non-White, which is an increase from 8.2% in 2011 and 4% in 2001. Of the Non-White population, the largest group by far is Asian, totalling about 44 thousand people. Within the Asian population, people of Chinese descent are now the largest sub-group, with 15,076 people, amounting to about 2.9% of the city's total population. The city's population of Indian descent amounts to 12,414 (2.4% of the total population), while there are some 7,454 of Pakistani descent (1.5% of the total population). Although they account for only 2,685 people or 0.5% of the city's population, Edinburgh has the highest number and proportion of people of Bangladeshi descent in Scotland. Close to 12,000 people were born in African countries (2.3% of the total population) and over 13,000 in the Americas. With the notable exception of Inner London, Edinburgh has a higher number of people born in the United States (over 6,500) than any other city in the UK.\nThe proportion of people residing in Edinburgh born outside the UK was 23.5% in 2022, compared with 15.9% in 2011 and 8.3% in 2001. Below are the largest overseas-born groups in Edinburgh according to the 2022 census, alongside the two previous censuses.\nHistorical.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nA census by the Edinburgh presbytery in 1592 recorded a population of 8,003 adults spread equally north and south of the High Street which runs along the spine of the ridge sloping down from the Castle. In the 18th and 19th centuries, the population expanded rapidly, rising from 49,000 in 1751 to 136,000 in 1831, primarily due to migration from rural areas. As the population grew, problems of overcrowding in the Old Town, particularly in the cramped tenements that lined the present-day Royal Mile and the Cowgate, were exacerbated. Poor sanitary arrangements resulted in a high incidence of disease, with outbreaks of cholera occurring in 1832, 1848, and 1866.\nThe construction of the New Town from 1767 onwards witnessed the migration of the professional and business classes from the difficult living conditions in the Old Town to the lower-density, higher quality surroundings taking shape on land to the north.\n Expansion southwards from the Old Town saw more tenements being built in the 19th century, giving rise to Victorian suburbs such as Dalry, Newington, Marchmont and Bruntsfield.\nEarly 20th-century population growth coincided with lower-density suburban development. As the city expanded to the south and west, detached and semi-detached villas with large gardens replaced tenements as the predominant building style. Nonetheless, the 2001 census revealed that over 55% of Edinburgh's population were still living in tenements or blocks of flats, a figure in line with other Scottish cities, but much higher than other British cities, and even central London.\nFrom the early to mid 20th century, the growth in population, together with slum clearance in the Old Town and other areas, such as Dumbiedykes, Leith, and Fountainbridge, led to the creation of new estates such as Stenhouse and Saughton, Craigmillar and Niddrie, Pilton and Muirhouse, Piershill, and Sighthill.\nReligion.\nAs per the 2022 census, 13% of the population belonged to the Church of Scotland and 10% to the Catholic Church.\nSaint Giles is historically the patron saint of Edinburgh. St Cuthbert's, situated at the west end of Princes Street Gardens in the shadow of Edinburgh Castle and St Giles' can lay claim to being the oldest Christian sites in the city, though the present St Cuthbert's, designed by Hippolyte Blanc, was dedicated in 1894.\nOther Church of Scotland churches include Greyfriars Kirk, the Canongate Kirk, and the Barclay Church. The Church of Scotland Offices are in Edinburgh, as is the Assembly Hall where the annual General Assembly is held.\nThe Roman Catholic Archdiocese of St Andrews and Edinburgh has 27 parishes across the city. The Archbishop of St Andrews and Edinburgh has his official residence in Greenhill, the diocesan offices are in nearby Marchmont, and its cathedral is St Mary's Cathedral, Edinburgh. The Diocese of Edinburgh of the Scottish Episcopal Church has over 50 churches, half of them in the city. Its centre is the late 19th-century Gothic style St Mary's Cathedral in the West End's Palmerston Place. Orthodox Christianity is represented by Pan, Romanian and Russian Orthodox churches, including St Andrew's Orthodox Church, part of the Greek Orthodox Archdiocese of Thyateira and Great Britain. There are several independent churches in the city, both Catholic and Protestant, including Charlotte Chapel, Carrubbers Christian Centre, Bellevue Chapel and Sacred Heart. There are also churches belonging to Quakers, Christadelphians, Seventh-day Adventists, Church of Christ, Scientist, The Church of Jesus Christ of Latter-day Saints (LDS Church) and Elim Pentecostal Church.\nMuslims have several places of worship across the city. Edinburgh Central Mosque, the largest Islamic place of worship, is located in Potterrow on the city's Southside, near Bristo Square. Construction was largely financed by a gift from King Fahd of Saudi Arabia and was completed in 1998. There is also an Ahmadiyya Muslim community. The first recorded presence of a Jewish community in Edinburgh dates back to the late 18th century. Edinburgh's Orthodox synagogue, opened in 1932, is in Salisbury Road and can accommodate a congregation of 2000. A Liberal Jewish congregation also meets in the city. A Sikh gurdwara and a Hindu mandir are located in Leith. The city also has a Brahma Kumaris centre in the Polwarth area.\nThe Edinburgh Buddhist Centre, run by the Triratna Buddhist Community, formerly situated in Melville Terrace, now runs sessions at the Healthy Life Centre, Bread Street. Other Buddhist traditions are represented by groups which meet in the capital: the Community of Interbeing (followers of Thich Nhat Hanh), Rigpa, Samye Dzong, Theravadin, Pure Land and Shambala. There is a S\u014dt\u014d Zen Priory in Portobello and a Theravadin Thai Buddhist Monastery in Slateford Road. Edinburgh is home to a Bah\u00e1\u02bc\u00ed community, and a Theosophical Society meets in Great King Street. Edinburgh has an Inter-Faith Association.\nEdinburgh has over 39 graveyards and cemeteries, many of which are listed and of historical character, including several former church burial grounds. Examples include Old Calton Burial Ground, Greyfriars Kirkyard and Dean Cemetery.\nEconomy.\nEdinburgh has the second-strongest economy of any city in the United Kingdom behind London and the highest percentage of professionals in the UK with 43% of the population holding a degree-level or professional qualification. According to the Centre for International Competitiveness, it is the most competitive large city in the United Kingdom. In 2023, its gross domestic product per capita of \u00a369,809 surpassed London's for the first time. It also had the highest gross value added per employee of any city in the UK outside London, measuring \u00a357,594 in 2010. It was named European Best Large City of the Future for Foreign Direct Investment and Best Large City for Foreign Direct Investment Strategy in the \"Financial Times\" magazine in 2012.\nAs the centre of Scotland's government and legal system, the public sector plays a central role in Edinburgh's economy. Many departments of the Scottish Government are in the city, including the headquarters of the government at St Andrew's House, the official residence of the First Minister at Bute House and Scottish Government offices at Victoria Quay. Other major sectors across the city include administrative and support services, the education sector, public administration and defence, the health and social care sector, scientific and technical services, and construction and manufacturing. When the \u00a31.3bn Edinburgh &amp; South East Scotland City Region Deal was signed in 2018, the region's Gross Value Added (GVA) contribution to the Scottish economy was cited as \u00a333bn, or 33% of the country's output. The City Region Deal funds a range of \"Data Driven Innovation\" hubs which are using data to innovate in the region, recognising the region's strengths in technology and data science, the growing importance of the data economy, and the need to tackle the digital skills gap, as a route to social and economic prosperity.\nTourism is also an important element in the city's economy. As a World Heritage Site, tourists visit historical sites such as Edinburgh Castle, the Palace of Holyroodhouse, and the Old and New Towns. Their numbers are augmented in August each year during the Edinburgh Festivals, which attract over 4\u00a0million visitors, and generate over \u00a3400M for the local economy. In May 2024, unemployment in Edinburgh was at 3.5%, in line with the Scottish average of 3.7%. In 2022 Edinburgh was the second most visited city in the United Kingdom, behind London, by overseas visitors.\nCulture.\nFestivals and celebrations.\nEdinburgh festivals.\nThe city hosts a series of festivals that run between the end of July and early September each year. The best known of these events are the Edinburgh Festival Fringe, the Edinburgh International Festival, the Edinburgh Military Tattoo, the Edinburgh Art Festival, the Edinburgh International Book Festival and Edinburgh International Film Festival.\nThe longest established of these festivals is the Edinburgh International Festival (EIF) and Edinburgh International Film Festival (EIFF). Both festivals were first held in 1947. EIF consists mainly of a programme of high-profile theatre productions and classical music performances, featuring international directors, conductors, theatre companies and orchestras. Edinburgh International Film Festival is the world's oldest continually running film festival. EIFF is globally renowned for world, international and UK premieres, high-profile film talent, significant filmmaking competitions and prominent industry guests.\nThe Edinburgh Fringe, which began as a programme of marginal acts alongside the \"official\" Festival, has become the world's largest performing arts festival. In 2023, over 3700 different shows were staged in 300 venues across the city. Comedy has become one of the mainstays of the Fringe, with many comedians getting their first 'break' there, often by being chosen to receive the Edinburgh Comedy Award. The Edinburgh Military Tattoo occupies the Castle Esplanade every night for three weeks each August, with massed pipe bands and military bands drawn from around the world. Performances end with a short fireworks display.\nAs well as the summer festivals, many other festivals are held during the rest of the year, including Edinburgh International Science Festival. The summer of 2020 was the first time in its 70-year history that the Edinburgh festival was not run, being cancelled due to the COVID-19 pandemic. This affected many of the tourist-focused businesses in Edinburgh which depend on the various festivals over summer to return an annual profit.\nEdinburgh's Hogmanay.\nThe annual Edinburgh Hogmanay celebration was originally an informal street party focused on the Tron Kirk in the Old Town's High Street. Since 1993, it has been officially organised with the focus moved to Princes Street. In 1996, over 300,000 people attended, leading to ticketing of the main street party in later years up to a limit of 100,000 tickets. Hogmanay now covers four days of processions, concerts and fireworks, with the street party beginning on Hogmanay. Alternative tickets are available for entrance into the Princes Street Gardens concert and C\u00e8ilidh, where well-known artists perform and ticket holders can participate in traditional Scottish c\u00e8ilidh dancing. The event attracts thousands of people from all over the world.\nBeltane and other festivals.\nOn the night of 30 April the Beltane Fire Festival takes place on Calton Hill, involving a procession followed by scenes inspired by pagan old spring fertility celebrations. At the beginning of October each year the Dussehra Hindu Festival is also held on Calton Hill.\nMusic, theatre and film.\nOutside the Festival season, Edinburgh supports several theatres and production companies. The Royal Lyceum Theatre has its own company, while the King's Theatre, Edinburgh Festival Theatre and Edinburgh Playhouse stage large touring shows. The Traverse Theatre presents a more contemporary repertoire. Amateur theatre companies productions are staged at the Bedlam Theatre, Church Hill Theatre and King's Theatre among others. The Usher Hall is Edinburgh's premier venue for classical music, as well as occasional popular music concerts. It was the venue for the Eurovision Song Contest 1972. Other halls staging music and theatre include The Hub, the Assembly Rooms and the Queen's Hall. The Scottish Chamber Orchestra is based in Edinburgh.\nEdinburgh has two repertory cinemas, The Cameo and the Edinburgh Filmhouse, as well as the independent Dominion Cinema and a range of multiplexes. Large concerts are occasionally staged at Murrayfield Stadium and Meadowbank Stadium, while mid-sized events take place at smaller venues such as O2 Academy Edinburgh. In 2010, PRS for Music listed Edinburgh among the UK's top ten 'most musical' cities. Several city pubs are well known for their live performances of folk music.\nNightclub venues within the city host electronic dance music events.\nMedia.\nThe main local newspaper is the \"Edinburgh Evening News\". It is owned and published alongside its sister titles \"The Scotsman\" and \"Scotland on Sunday\" by JPIMedia. Student newspapers include, \"The Journal\" Scotland wide Universities, and \"The Student\" University of Edinburgh which was founded in 1887. Community newspapers include \"The Spurtle\" from Broughton, \"Spokes Bulletin\", and \"The Edinburgh Reporter\".\nThe city has many commercial radio stations including Forth 1, a station which broadcasts mainstream chart music, Greatest Hits Edinburgh on DAB which plays classic hits and Edge Radio. Capital Scotland and Heart Scotland also have transmitters covering Edinburgh. Along with the UK national radio stations, BBC Radio Scotland and the Gaelic language service BBC Radio nan G\u00e0idheal are also broadcast. DAB digital radio is broadcast over two local multiplexes. BFBS Radio broadcasts from studios on the base at Dreghorn Barracks across the city on 98.5FM as part of its UK Bases network. Small-scale DAB started in October 2022 with community stations on board.\nTelevision, along with most radio services, is broadcast to the city from the Craigkelly transmitting station situated in Fife on the opposite side of the Firth of Forth and the Black Hill transmitting station in North Lanarkshire to the west. There are no television stations based in the city. Edinburgh Television existed in the late 1990s to early 2003 and STV Edinburgh existed from 2015 to 2018.\nMuseums, libraries, and galleries.\nEdinburgh has many museums and libraries. These include the National Museum of Scotland, the National Library of Scotland, National War Museum, the Museum of Edinburgh, Surgeons' Hall Museum, the Writers' Museum, the Museum of Childhood and Dynamic Earth. The Museum on The Mound has exhibits on money and banking. Edinburgh Zoo, covering on Corstorphine Hill, is the second most visited paid tourist attraction in Scotland, and was previously home to two giant pandas, Tian Tian and Yang Guang, on loan from the People's Republic of China. Edinburgh is also home to The Royal Yacht Britannia, decommissioned in 1997 and now a five-star visitor attraction and evening events venue permanently berthed at Ocean Terminal.\nEdinburgh contains Scotland's three National Galleries of Art as well as smaller art galleries. The national collection is housed in the Scottish National Gallery, located on The Mound, comprising the linked National Gallery of Scotland building and the Royal Scottish Academy building. Contemporary collections are shown in the Scottish National Gallery of Modern Art, which occupies a split site at Belford. The Scottish National Portrait Gallery on Queen Street focuses on portraits and photography. The council-owned City Art Centre in Market Street mounts regular art exhibitions. Across the road, The Fruitmarket Gallery offers world-class exhibitions of contemporary art, featuring work by British and international artists with both emerging and established international reputations.\nThe city hosts several of Scotland's galleries and organisations dedicated to contemporary visual art. Significant strands of this infrastructure include Creative Scotland, Edinburgh College of Art, Talbot Rice Gallery (University of Edinburgh), Collective Gallery (based at the City Observatory) and the Edinburgh Annuale. Many small private shops/galleries provide space to showcase works from local artists.\nShopping.\nThe locale around Princes Street is the main shopping area in the city centre, with souvenir shops, chain stores such as Boots the Chemist, Edinburgh Woollen Mill, and H&amp;M. George Street, north of Princes Street, has several upmarket shops and independent stores. At the east end of Princes Street, the redeveloped St James Quarter opened its doors in June 2021, while next to the Balmoral Hotel and Waverley Station is Waverley Market. Multrees Walk is a pedestrian shopping district, dominated by the presence of Harvey Nichols, and other names including Louis Vuitton, Mulberry and Michael Kors.\nEdinburgh also has substantial retail parks outside the city centre. These include The Gyle Shopping Centre and Hermiston Gait in the west of the city, Cameron Toll Shopping Centre, Straiton Retail Park (actually just outside the city, in Midlothian) and Fort Kinnaird in the south and east, and Ocean Terminal in the north on the Leith waterfront.\nGovernment and politics.\nGovernment.\nFollowing local government reorganisation in 1996, the City of Edinburgh Council constitutes one of the 32 council areas of Scotland. Like all other local authorities of Scotland, the council has powers over most matters of local administration such as housing, planning, local transport, parks, economic development and regeneration. The council comprises 63 elected councillors, returned from 17 multi-member electoral wards in the city. Following the 2007 City of Edinburgh Council election the incumbent Labour Party lost majority control of the council after 23 years to a Liberal Democrat/SNP coalition.\nAfter the 2017 election, the SNP and Labour formed a coalition administration, which lasted until the next election in 2022. The 2022 City of Edinburgh Council election resulted in the most politically balanced council in the UK, with 19 SNP, 13 Labour, 12 Liberal Democrat, 10 Green, and 9 Conservative councillors. A minority Labour administration was formed, being voted in by Scottish Conservative and Scottish Liberal Democrat councillors. The SNP and Greens presented a coalition agreement, but could not command a majority in the council. This caused controversy amongst the Scottish Labour Party group for forming an administration supported by Conservatives, and led to the suspension of two Labour councillors on the council for abstaining on the vote to approve the new administration. The city's coat of arms was registered by the Lord Lyon King of Arms in 1732.\nPolitics.\nEdinburgh, like all of Scotland, is represented in the Scottish Parliament, situated in the Holyrood area of the city. For electoral purposes, the city is divided into six constituencies, which, along with 3 seats outside of the city, form part of the Lothian region. Each constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system of election, and the region elects seven additional MSPs to produce a result based on a form of proportional representation.\nAs of the 2021 election, the Scottish National Party have four MSPs: Ash Denham for Edinburgh Eastern, Ben Macpherson for Edinburgh Northern and Leith and Gordon MacDonald for Edinburgh Pentlands and Angus Robertson for Edinburgh Central constituencies. Alex Cole-Hamilton, the Leader of the Scottish Liberal Democrats represents Edinburgh Western and Daniel Johnson of the Scottish Labour Party represents Edinburgh Southern constituency. In addition, the city is also represented by seven regional MSPs representing the Lothian electoral region: The Conservatives have three regional MSPs: Jeremy Balfour, Miles Briggs and Sue Webber, Labour have two regional MSPs: Sarah Boyack and Foysol Choudhury; two Scottish Green regional MSPs were elected: Green's Co-Leader Lorna Slater and Alison Johnstone.\nEdinburgh is also represented in the House of Commons of the United Kingdom by five Members of Parliament. The city is divided into Edinburgh North and Leith, Edinburgh East and Musselburgh, Edinburgh South, Edinburgh South West, and Edinburgh West, each constituency electing one member by the first past the post system. Since the 2024 UK General election, Edinburgh is represented by four Labour MPs (Tracy Gilbert in Edinburgh North and Leith, Chris Murray in Edinburgh East and Musselburgh, Ian Murray in Edinburgh South, and Scott Arthur in Edinburgh South West), and one Liberal Democrat MP in Edinburgh West (Christine Jardine).\nTransport.\nAir.\nEdinburgh Airport is Scotland's busiest airport and the principal international gateway to the capital, handling over 14.7\u00a0million passengers; it was also the sixth-busiest airport in the United Kingdom by total passengers in 2019. In anticipation of rising passenger numbers, the former operator of the airport BAA outlined a draft masterplan in 2011 to provide for the expansion of the airfield and the terminal building. In June 2012, Global Infrastructure Partners purchased the airport for \u00a3807 million. The possibility of building a second runway to cope with an increased number of aircraft movements has also been mooted.\nBuses.\nTravel in Edinburgh is undertaken predominantly by bus. Lothian Buses, the successor company to Edinburgh Corporation Transport Department, operates the majority of city bus services within the city and to surrounding towns and villages, with most routes running via Princes Street. Services further afield operate from the Edinburgh Bus Station off St Andrew Square and Waterloo Place and are operated mainly by Stagecoach East Scotland, Scottish Citylink, National Express Coaches and Borders Buses.\nLothian Buses and McGill's Scotland East operate the city's branded public tour buses. The night bus service and airport buses are mainly operated by Lothian Buses. In 2019, Lothian Buses recorded 124.2 million passenger journeys.\nTo tackle traffic congestion, Edinburgh is now served by six park &amp; ride sites on the periphery of the city at Sheriffhall (in Midlothian), Ingliston, Riccarton, Inverkeithing (in Fife), Newcraighall and Straiton (in Midlothian). A referendum of Edinburgh residents in February 2005 rejected a proposal to introduce congestion charging in the city.\nRailway.\nEdinburgh Waverley is the second-busiest railway station in Scotland, with only Glasgow Central handling more passengers. On the evidence of passenger entries and exits between April 2015 and March 2016, Edinburgh Waverley is the fifth-busiest station outside London; it is also the UK's second biggest station in terms of the number of platforms and area size. Waverley is the terminus for most trains arriving from London King's Cross and the departure point for many rail services within Scotland operated by ScotRail.\nTo the west of the city centre lies Haymarket station, which is an important commuter stop. Opened in 2003, Edinburgh Park station serves the Gyle business park in the west of the city and the nearby Gogarburn headquarters of the Royal Bank of Scotland. The Edinburgh Crossrail route connects Edinburgh Park with Haymarket, Edinburgh Waverley and the suburban stations of Brunstane and Newcraighall in the east of the city. There are also commuter lines to Edinburgh Gateway, South Gyle and Dalmeny, the latter serving South Queensferry by the Forth Bridges, and to Wester Hailes and Curriehill in the south-west of the city.\nTrams.\nEdinburgh Trams became operational on 31 May 2014. The city had been without a tram system since Edinburgh Corporation Tramways ceased on 16 November 1956. Following parliamentary approval in 2007, construction began in early 2008. The first stage of the project was expected to be completed by July 2011 but, following delays caused by extra utility work and a long-running contractual dispute between the council and the main contractor, Bilfinger SE, the project was rescheduled. The line opened in 2014 but had been cut short to in length, running from Edinburgh Airport To York Place in the east end of the city.\nThe line was later extended north onto Leith and Newhaven, opening a further eight stops to passengers in June 2023. The York Place stop was replaced by a new island stop at Picardy Place. The original plan would have seen a second line run from Haymarket through Ravelston and Craigleith to Granton Square on the Waterfront Edinburgh. This was shelved in 2011 but is now once again under consideration, as is another line potentially linking the south of the city and the Bioquarter. There were also long-term plans for lines running west from the airport to Ratho and Newbridge and another connecting Granton to Newhaven via Lower Granton Road. Lothian Buses and Edinburgh Trams are both owned and operated by Transport for Edinburgh.\nDespite its modern transport links, in January 2021, Edinburgh was named the most congested city in the UK for the fourth year running, though it has since fallen to 7th place in 2022.\nEducation.\nSchools.\nThere are 18 nursery, 94 primary and 23 secondary schools administered by the City of Edinburgh Council.\nEdinburgh is home to The Royal High School, one of the oldest schools in the country and the world. The city also has several independent, fee-paying schools including Edinburgh Academy, Fettes College, George Heriot's School, George Watson's College, Merchiston Castle School, Stewart's Melville College and The Mary Erskine School. In 2009, the proportion of pupils attending independent schools was 24.2%, far above the Scottish national average of just over 7% and higher than in any other region of Scotland. In August 2013, the City of Edinburgh Council opened the city's first stand-alone Gaelic primary school, Bun-sgoil Taobh na P\u00e0irce.\nCollege and university.\nThere are three universities in Edinburgh: the University of Edinburgh, Heriot-Watt University, and Edinburgh Napier University. Established by royal charter in 1583, the University of Edinburgh is one of Scotland's ancient universities and is the fourth oldest in the country after St Andrews, Glasgow and Aberdeen. Originally centred on Old College the university expanded to premises on The Mound, the Royal Mile and George Square. Today, the King's Buildings in the south of the city contain most of the schools within the College of Science and Engineering. In 2002, the medical school moved to purpose-built accommodation adjacent to the new Royal Infirmary of Edinburgh at Little France. The university is placed 16th in the QS World University Rankings for 2022.\nHeriot-Watt University is based at the Riccarton campus in the west of Edinburgh. Originally established in 1821 as the world's first mechanics' institute, it was granted university status by royal charter in 1966. It has other campuses in the Scottish Borders, Orkney, the United Arab Emirates and Putrajaya in Malaysia. It takes the name \"Heriot-Watt\" from Scottish inventor James Watt and Scottish philanthropist and goldsmith George Heriot. Heriot-Watt University has been named International University of the Year by \"The Times and Sunday Times Good University Guide 2018\". In the latest Research Excellence Framework, it was ranked overall in the Top 25% of UK universities and 1st in Scotland for research impact.\nEdinburgh Napier University was originally founded as Napier College, which was renamed Napier Polytechnic in 1986 and gained university status in 1992. Edinburgh Napier University has campuses in the south and west of the city, including the former Merchiston Tower and Craiglockhart Hydropathic. It is home to the Screen Academy Scotland. Queen Margaret University was located in Edinburgh before it moved outside the city boundary to a new campus in the county of East Lothian on the outskirts of Musselburgh in 2008. Until 2012, further education colleges in the city included Jewel and Esk College (incorporating Leith Nautical College founded in 1903), Telford College, opened in 1968, and Stevenson College, opened in 1970. These have now been amalgamated to form Edinburgh College. Scotland's Rural College also has a campus in South Edinburgh. Other institutions include the Royal College of Surgeons of Edinburgh and the Royal College of Physicians of Edinburgh, which were established by royal charter in 1506 and 1681, respectively. The Trustees' Academy of Edinburgh, founded in 1760, became the Edinburgh College of Art in 1907.\nHealthcare.\nThe main NHS Lothian hospitals serving the Edinburgh area are the Royal Infirmary of Edinburgh, which includes the University of Edinburgh Medical School, and the Western General Hospital, which has a large cancer treatment centre and nurse-led Minor Injuries Clinic. The Royal Edinburgh Hospital in Morningside specialises in mental health. The Royal Hospital for Children and Young People, colloquially referred to as \"the Sick Kids\", is a specialist paediatrics hospital.\nThere are two private hospitals: Murrayfield Hospital in the west of the city and Shawfair Hospital in the south; both are owned by Spire Healthcare.\nSport.\nFootball.\nEdinburgh has four football clubs that play in the Scottish Professional Football League (SPFL): Heart of Midlothian, founded in 1874, Hibernian, founded in 1875, Edinburgh City F.C., founded in 1966 and Spartans, founded in 1951. Heart of Midlothian and Hibernian are known locally as \"Hearts\" and \"Hibs\", respectively. Both play in the Scottish Premiership. They are the oldest city rivals in Scotland and the Edinburgh derby is one of the oldest derby matches in world football. Both clubs have won the Scottish league championship four times. Hearts have won the Scottish Cup eight times and the Scottish League Cup four times. Hibs have won the Scottish Cup and the Scottish League Cup three times each. Edinburgh City were promoted to Scottish League Two in the 2015\u201316 season, becoming the first club to win promotion to the SPFL via the pyramid system playoffs.\nEdinburgh was also home to four other former Scottish Football League clubs: the original Edinburgh City (founded in 1928), Leith Athletic, Meadowbank Thistle and St Bernard's. Meadowbank Thistle played at Meadowbank Stadium until 1995, when the club moved to Livingston and became Livingston F.C. The Scottish national team has very occasionally played at Easter Road and Tynecastle, although its normal home stadium is Hampden Park in Glasgow. St Bernard's New Logie Green was used to host the 1896 Scottish Cup Final, the only time the match has been played outside Glasgow. The city also plays host to Lowland Football League clubs Civil Service Strollers, Edinburgh University and Spartans, as well as East of Scotland League clubs Craigroyston, Edinburgh United, Heriot-Watt University, Leith Athletic, Lothian Thistle Hutchison Vale, and Tynecastle.\nIn women's football, Hearts, Hibs and Spartans play in the SWPL 1. Hutchison Vale and Boroughmuir Thistle play in the SWPL 2.\nRugby.\nThe Scotland national rugby union team play at Murrayfield Stadium, and the professional Edinburgh Rugby team play at the nextdoor Edinburgh Rugby Stadium; both are owned by the Scottish Rugby Union and are also used for other events, including music concerts. Murrayfield is the largest capacity stadium in Scotland, seating 67,144 spectators. Edinburgh is also home to Scottish Premiership teams Boroughmuir RFC, Currie RFC, the Edinburgh Academicals, Heriot's Rugby Club and Watsonians RFC.\nThe Edinburgh Academicals ground at Raeburn Place was the location of the world's first international rugby game on 27 March 1871, between Scotland and England. Rugby league is represented by the Edinburgh Eagles who play in the Rugby League Conference Scotland Division. Murrayfield Stadium has hosted the Magic Weekend where all Super League matches are played in the stadium over one weekend.\nOther sports.\nThe Scottish cricket team, which represents Scotland internationally, plays its home matches at the Grange cricket club. The Edinburgh Capitals are the latest of a succession of ice hockey clubs in the Scottish capital. Previously, Edinburgh was represented by the Murrayfield Racers (2018), the original Murrayfield Racers \"(who folded in 1996)\", and the Edinburgh Racers. The club plays their home games at the Murrayfield Ice Rink and have competed in the eleven-team professional Scottish National League (SNL) since the 2018\u201319 season.\nCaledonia Pride is the only women's professional basketball team in Scotland. Established in 2016, the team competes in the UK-wide Women's British Basketball League and plays their home matches at the Oriam National Performance Centre. Edinburgh also has several men's basketball teams within the Scottish National League. Boroughmuir Blaze, City of Edinburgh Kings, and Edinburgh Lions all compete in Division 1 of the National League, and Pleasance B.C. compete in Division 2. The Edinburgh Diamond Devils is a baseball club that won its first Scottish Championship in 1991 as the \"Reivers\". 1992 saw the team repeat the achievement, becoming the first team to do so in league history. The same year saw the start of their first youth team, the Blue Jays. The club adopted its present name in 1999.\nEdinburgh has also hosted national and international sports events including the World Student Games, the 1970 British Commonwealth Games, the 1986 Commonwealth Games and the inaugural 2000 Commonwealth Youth Games. For the 1970 Games the city built Olympic standard venues and facilities including Meadowbank Stadium and the Royal Commonwealth Pool. The Pool underwent refurbishment in 2012 and hosted the Diving competition in the 2014 Commonwealth Games, which were held in Glasgow.\nIn American football, the Scottish Claymores played WLAF/NFL Europe games at Murrayfield, including their World Bowl 96 victory. From 1995 to 1997, they played all their games there, from 1998 to 2000 they split their home matches between Murrayfield and Glasgow's Hampden Park, then moved to Glasgow full-time, with one final Murrayfield appearance in 2002. The city's most successful non-professional team are the Edinburgh Wolves who play at Meadowbank Stadium. The Edinburgh Marathon has been held annually in the city since 2003 with more than 16,000 runners taking part on each occasion. Its organisers have called it \"the fastest marathon in the UK\" due to the elevation drop of . The city also organises a half-marathon, as well as 10\u00a0km () and 5\u00a0km () races, including a race on 1 January each year.\nEdinburgh has a speedway team, the Edinburgh Monarchs, which, since the loss of its stadium in the city, has raced at the Lothian Arena in Armadale, West Lothian. The Monarchs have won the Premier League championship five times in their history, in 2003 and again in 2008, 2010, 2014 and 2015. Edinburgh also has Scotland's first onshore artificial open air surfing pool, located at former Craigpark quarry in Ratho.\nPeople.\nEdinburgh has a literary tradition, which became especially evident during the Scottish Enlightenment. This heritage and the city's literary life in the present led to it being declared the first UNESCO City of Literature in 2004. Authors who have lived in Edinburgh include the economist Adam Smith, born in Kirkcaldy and author of \"The Wealth of Nations\", James Boswell, biographer of Samuel Johnson; Sir Walter Scott, creator of the historical novel and author of works such as \"Rob Roy\", \"Ivanhoe\", and \"Heart of Midlothian\"; James Hogg, author of \"The Private Memoirs and Confessions of a Justified Sinner\"; Robert Louis Stevenson, creator of \"Treasure Island\", \"Kidnapped\", and \"Strange Case of Dr Jekyll and Mr Hyde\"; Sir Arthur Conan Doyle, the creator of Sherlock Holmes; Muriel Spark, author of \"The Prime of Miss Jean Brodie\"; Irvine Welsh, author of \"Trainspotting\", whose novels are mostly set in the city and often written in colloquial Scots;\n Ian Rankin, author of the Inspector Rebus series of crime thrillers, Alexander McCall Smith, author of the No. 1 Ladies' Detective Agency series, and J. K. Rowling, author of Harry Potter, who moved to the city in 1993 and wrote much of her first book in Edinburgh coffee shops.\nEdinburgh produced figures in science and engineering. John Napier, inventor of logarithms, was born in Merchiston Tower and lived and died in the city. His house now forms part of the original campus of Napier University which was named in his honour. He lies buried under St. Cuthbert's Church. James Clerk Maxwell, founder of the modern theory of electromagnetism, was born at 14 India Street (now the home of the James Clerk Maxwell Foundation) and educated at the Edinburgh Academy and the University of Edinburgh, as was the engineer and telephone pioneer Alexander Graham Bell. James Braidwood, who organised Britain's first municipal fire brigade, was also born in the city and began his career there.\nOther names connected with the city include physicist Max Born, a principle founder of Quantum mechanics and Nobel laureate; Charles Darwin, the biologist who propounded the theory of natural selection; David Hume, philosopher, economist and historian; James Hutton, regarded as the \"Father of Geology\"; Joseph Black, the chemist who discovered magnesium and carbon dioxide, and one of the founders of Thermodynamics; pioneering medical researchers Joseph Lister and James Young Simpson; chemist and discoverer of the element nitrogen Daniel Rutherford; Colin Maclaurin, mathematician and developer of the Maclaurin series, and Ian Wilmut, the geneticist involved in the cloning of Dolly the sheep just outside Edinburgh, at the Roslin Institute. The stuffed carcass of Dolly the sheep is now on display in the National Museum of Scotland. The latest in a long line of science celebrities associated with the city is theoretical physicist, Nobel laureate and professor emeritus at the University of Edinburgh Peter Higgs, born in Newcastle but resident in Edinburgh for most of his academic career, after whom the Higgs boson particle has been named.\nEdinburgh has been the birthplace of actors like Alastair Sim and Sir Sean Connery, the first cinematic James Bond, the comedian and actor Ronnie Corbett, one of The Two Ronnies, and the impressionist Rory Bremner. Artists from the city include the portrait painters Sir Henry Raeburn, Sir David Wilkie, and Allan Ramsay. The city has produced or been home to musicians Ian Anderson, front man of the band Jethro Tull, The Incredible String Band, the folk duo The Corries, Wattie Buchan, lead singer and founding member of punk band The Exploited, Shirley Manson, lead singer of the band Garbage, the Bay City Rollers, The Proclaimers, Swim School, Boards of Canada and Idlewild. Edinburgh is the birthplace of former British Prime Minister Tony Blair who attended the city's Fettes College.\nCriminals from Edinburgh's past include Deacon Brodie, head of a trades guild and Edinburgh city councillor by day but a burglar by night, who is said to have been the inspiration for Robert Louis Stevenson's story, the \"Strange Case of Dr Jekyll and Mr Hyde\", and murderers Burke and Hare who delivered fresh corpses for dissection to the famous anatomist Robert Knox.\nAnother Edinburgh resident was Greyfriars Bobby. The small Skye Terrier reputedly kept vigil over his dead master's grave in Greyfriars Kirkyard for 14 years in the 1860s and 1870s, giving rise to a story of canine devotion which plays a part in attracting visitors to the city.\nInternational relations.\nTwin towns and sister cities.\nThe City of Edinburgh has entered into 14 international twinning arrangements since 1954. Most of the arrangements are styled as \"Twin Cities\" but the agreement with Krak\u00f3w is designated as a \"Partner City\", and the agreement with Kyoto Prefecture is officially styled as a \"Friendship Link\", reflecting its status as the only region to be twinned with Edinburgh.\nIn June 2024, the City of Edinburgh Council shelved plans for a friendship arrangement with Kaohsiung, Taiwan, after a report raised concerns that the agreement could heighten cyber attacks. A few weeks before the decision, the Chinese Consul General met with Scottish government minister Angus Robertson to protest against the potential agreement. In a letter to the city council, the Chinese representative said signing a sister city agreement \"will hurt the feeling of the Chinese people and bring about serious consequences to \u2026 bilateral relations\".\nFor a list of consulates in Edinburgh, see List of diplomatic missions in Scotland.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9603", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=9603", "title": "Ernest Rutherford", "text": "New Zealand physicist and chemist (1871\u20131937)\nErnest Rutherford, Baron Rutherford of Nelson (30 August 1871 \u2013 19 October 1937) was a New Zealand physicist and chemist who was a pioneering researcher in both atomic and nuclear physics. He has been described as \"the father of nuclear physics\" and \"the greatest experimentalist since Michael Faraday.\" In 1908, he was awarded the Nobel Prize in Chemistry \"for his investigations into the disintegration of the elements, and the chemistry of radioactive substances.\" He was the first Oceanian Nobel laureate, and the first to perform Nobel-awarded work in Canada.\nRutherford's discoveries include the concept of radioactive half-life, the radioactive element radon, and the differentiation and naming of alpha and beta radiation. Together with Thomas Royds, Rutherford is credited with proving that alpha radiation is composed of helium nuclei. In 1911, he theorised that atoms have their charge concentrated in a very small nucleus. He arrived at this theory through his discovery and interpretation of Rutherford scattering during the gold foil experiment performed by Hans Geiger and Ernest Marsden. In 1912, he invited Niels Bohr to join his lab, leading to the Bohr model of the atom. In 1917, he performed the first artificially induced nuclear reaction by conducting experiments in which nitrogen nuclei were bombarded with alpha particles. These experiments led him to discover the emission of a subatomic particle that he initially called the \"hydrogen atom\", but later (more precisely) renamed the proton. He is also credited with developing the atomic numbering system alongside Henry Moseley. His other achievements include advancing the fields of radio communications and ultrasound technology.\nRutherford became Director of the Cavendish Laboratory at the University of Cambridge in 1919. Under his leadership, the neutron was discovered by James Chadwick in 1932. In the same year, the first controlled experiment to split the nucleus was performed by John Cockcroft and Ernest Walton, working under his direction. In honour of his scientific advancements, Rutherford was recognised as a baron of the United Kingdom. After his death in 1937, he was buried in Westminster Abbey near Charles Darwin and Isaac Newton. The chemical element rutherfordium (104Rf) was named after him in 1997.\nEarly life and education.\nErnest Rutherford was born on 30 August 1871 in Brightwater, New Zealand, the fourth of twelve children of James Rutherford, an immigrant farmer and mechanic from Perth, Scotland, and Martha Thompson, a schoolteacher from Hornchurch, England. Rutherford's birth certificate was mistakenly written as 'Earnest'. He was known by his family as Ern. \nWhen Rutherford was age 5, he moved to Foxhill, New Zealand, and attended Foxhill School. At 11 in 1883, the Rutherford family moved to Havelock, a town in the Marlborough Sounds. The move was made to be closer to the flax mill Rutherford's father developed. Ernest studied at Havelock School.\nIn 1887, on his second attempt, he won a scholarship to study at Nelson College. On his first examination attempt, he had the highest mark of anyone from Nelson. When he was awarded the scholarship, he had received 580 out of 600 possible marks. After being awarded the scholarship, Havelock School presented him with a five-volume set of books titled \"The Peoples of the World\". He studied at Nelson College between 1887 and 1889, and was head boy in 1889. He also played in the school's rugby team. He was offered a cadetship in government service, but he declined as he still had 15 months of college remaining.\nIn 1889, after his second attempt, he won a scholarship to study at Canterbury College, University of New Zealand, between 1890 and 1894. He participated in its debating society and the Science Society. At Canterbury, he was awarded a complex B.A. in Latin, English and Maths in 1892, a M.A. in Mathematics and Physical Science in 1893, and a B.Sc. in Chemistry and Geology in 1894.\nThereafter, Rutherford invented a new form of a radio receiver, and in 1895 he was awarded an 1851 Research Fellowship from the Royal Commission for the Exhibition of 1851, to travel to England for postgraduate study in the Cavendish Laboratory at the University of Cambridge. In 1897, he was awarded a B.A. Research Degree and the Coutts-Trotter Studentship from Trinity College, Cambridge.\nCareer and research.\nWhen Rutherford began his studies at Cambridge, he was among the first 'aliens' (those without a Cambridge degree) allowed to do research at the university, and was additionally honoured to study under J. J. Thomson.\nWith Thomson's encouragement, Rutherford detected radio waves at , and briefly held the world record for the distance over which electromagnetic waves could be detected, although when he presented his results at the British Association meeting in 1896, he discovered he had been outdone by Guglielmo Marconi, whose radio waves had sent a message across nearly .\nRadioactivity.\nAgain under Thomson's leadership, Rutherford worked on the conductive effects of X-rays on gases, which led to the discovery of the electron, the results first presented by Thomson in 1897. Hearing of Henri Becquerel's experience with uranium, Rutherford started to explore its radioactivity, discovering two types that differed from X-rays in their penetrating power. Continuing his research in Canada, in 1899 he coined the terms \"alpha ray\" and \"beta ray\" to describe these two distinct types of radiation.\nIn 1898, Rutherford accepted the Macdonald Chair of Physics at McGill University in Montreal, Canada, on Thomson's recommendation. From 1900 to 1903, he was joined at McGill by the young chemist Frederick Soddy (Nobel Prize in Chemistry, 1921) for whom he set the problem of identifying the noble gas emitted by the radioactive element thorium, a substance which was itself radioactive and would coat other substances. Once he had eliminated all the normal chemical reactions, Soddy suggested that it must be one of the inert gases, which they named thoron. This substance was later found to be 220Rn, an isotope of radon. They also found another substance they called Thorium X, later identified as 224Rn, and continued to find traces of helium. They also worked with samples of \"Uranium X\" (protactinium), from William Crookes, and radium, from Marie Curie. Rutherford further investigated thoron in conjunction with R.B. Owens and found that a sample of radioactive material of any size invariably took the same amount of time for half the sample to decay (in this case, 11&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 minutes), a phenomenon for which he coined the term \"half-life\". Rutherford and Soddy published their paper \"Law of Radioactive Change\" to account for all their experiments. Until then, atoms were assumed to be the indestructible basis of all matter; and although Curie had suggested that radioactivity was an atomic phenomenon, the idea of the atoms of radioactive substances breaking up was a radically new idea. Rutherford and Soddy demonstrated that radioactivity involved the spontaneous disintegration of atoms into other, as yet, unidentified matter.\nIn 1903, Rutherford considered a type of radiation, discovered (but not named) by French chemist Paul Villard in 1900, as an emission from radium, and realised that this observation must represent something different from his own alpha and beta rays, due to its very much greater penetrating power. Rutherford therefore gave this third type of radiation the name of gamma ray. All three of Rutherford's terms are in standard use today \u2013 other types of radioactive decay have since been discovered, but Rutherford's three types are among the most common. In 1904, Rutherford suggested that radioactivity provides a source of energy sufficient to explain the existence of the Sun for the many millions of years required for the slow biological evolution on Earth proposed by biologists such as Charles Darwin. The physicist Lord Kelvin had argued earlier for a much younger Earth, based on the insufficiency of known energy sources, but Rutherford pointed out, at a lecture attended by Kelvin, that radioactivity could solve this problem. In 1907, he returned to Britain to take the Langworthy Professorship at the Victoria University of Manchester.\nIn Manchester, Rutherford continued his work with alpha radiation. In conjunction with Hans Geiger, he developed zinc sulfide scintillation screens and ionisation chambers to count alpha particles. By dividing the total charge accumulated on the screen by the number counted, Rutherford determined that the charge on the alpha particle was two. In late 1907, Ernest Rutherford and Thomas Royds allowed alphas to penetrate a very thin window into an evacuated tube. As they sparked the tube into discharge, the spectrum obtained from it changed, as the alphas accumulated in the tube. Eventually, the clear spectrum of helium gas appeared, proving that alphas were at least ionised helium atoms, and probably helium nuclei. \nIn 1910 Rutherford, with Geiger and mathematician Harry Bateman published\ntheir classic paper describing the first analysis of the distribution in time of radioactive emission, a distribution now called the Poisson distribution.\nIn 1908, Rutherford was awarded the Nobel Prize in Chemistry \"for his investigations into the disintegration of the elements, and the chemistry of radioactive substances.\"\nModel of the atom.\nRutherford continued to make ground-breaking discoveries long after receiving the Nobel prize in 1908. Under his direction in 1909, Hans Geiger and Ernest Marsden performed the Geiger\u2013Marsden experiment, which demonstrated the nuclear nature of atoms by measuring the deflection of alpha particles passing through a thin gold foil. Rutherford was inspired to ask Geiger and Marsden in this experiment to look for alpha particles with very high deflection angles, which was not expected according to any theory of matter at that time. Such deflection angles, although rare, were found. Reflecting on these results in one of his last lectures, Rutherford was quoted as saying: \"It was quite the most incredible event that has ever happened to me in my life. It was almost as incredible as if you fired a 15-inch shell at a piece of tissue paper and it came back and hit you.\" It was Rutherford's interpretation of this data that led him to propose the nucleus, a very small, charged region containing much of the atom's mass.\nIn 1912, Rutherford was joined by Niels Bohr (who postulated that electrons moved in specific orbits about the compact nucleus). Bohr adapted Rutherford's nuclear structure to be consistent with Max Planck's quantum hypothesis. The resulting Bohr model was the basis for quantum mechanical atomic physics of Heisenberg which remains valid today.\nPiezoelectricity.\nDuring World War I, Rutherford worked on a top-secret project to solve the practical problems of submarine detection. Both Rutherford and Paul Langevin suggested the use of piezoelectricity, and Rutherford successfully developed a device which measured its output. The use of piezoelectricity then became essential to the development of ultrasound as it is known today. The claim that Rutherford developed sonar, however, is a misconception, as subaquatic detection technologies utilise Langevin's transducer.\nDiscovery of the proton.\nTogether with H.G. Moseley, Rutherford developed the atomic numbering system in 1913. Rutherford and Moseley's experiments used cathode rays to bombard various elements with streams of electrons and observed that each element responded in a consistent and distinct manner. Their research was the first to assert that each element could be defined by the properties of its inner structures \u2013 an observation that later led to the discovery of the atomic nucleus. This research led Rutherford to theorise that the hydrogen atom (at the time the least massive entity known to bear a positive charge) was a sort of \"positive electron\" \u2013 a component of every atomic element.\nIt was not until 1919 that Rutherford expanded upon his theory of the \"positive electron\" with a series of experiments beginning shortly before the end of his time at Manchester. He found that nitrogen, and other light elements, ejected a proton, which he called a \"hydrogen atom,\" when hit with \u03b1 (alpha) particles. In particular, he showed that particles ejected by alpha particles colliding with hydrogen have unit charge and 1/4 the momentum of alpha particles.\nRutherford returned to the Cavendish Laboratory in 1919, succeeding J. J. Thomson as Cavendish Professor of Physics, a position he held until his death in 1937. During his tenure, Nobel prizes were awarded to James Chadwick for discovering the neutron (in 1932), John Cockcroft and Ernest Walton for an experiment that was to be known as \"splitting the atom\" using a particle accelerator, and Edward Appleton for demonstrating the existence of the ionosphere.\nDevelopment of proton and neutron theory.\nIn 1919\u20131920, Rutherford continued his research on the \"hydrogen atom\" to confirm that alpha particles break down nitrogen nuclei and to affirm the nature of the products. This result showed Rutherford that hydrogen nuclei were a part of nitrogen nuclei (and by inference, probably other nuclei as well). Such a construction had been suspected for many years, on the basis of atomic weights that were integral multiples of that of hydrogen; see Prout's hypothesis. Hydrogen was known to be the lightest element, and its nuclei presumably the lightest nuclei. Now, because of all these considerations, Rutherford decided that a hydrogen nucleus was possibly a fundamental building block of all nuclei, and also possibly a new fundamental particle as well, since nothing was known to be lighter than that nucleus. Thus, confirming and extending the work of Wilhelm Wien, who in 1898 discovered the proton in streams of ionised gas, in 1920 Rutherford postulated the hydrogen nucleus to be a new particle, which he dubbed the \"proton\".\nIn 1921, while working with Niels Bohr, Rutherford theorised about the existence of neutrons, (which he had christened in his 1920 Bakerian Lecture), which could somehow compensate for the repelling effect of the positive charges of protons by causing an attractive nuclear force and thus keep the nuclei from flying apart, due to the repulsion between protons. The only alternative to neutrons was the existence of \"nuclear electrons\", which would counteract some of the proton charges in the nucleus, since by then it was known that nuclei had about twice the mass that could be accounted for if they were simply assembled from hydrogen nuclei (protons). But how these nuclear electrons could be trapped in the nucleus, was a mystery.\nIn 1932, Rutherford's theory of neutrons was proved by his associate James Chadwick, who recognised neutrons immediately when they were produced by other scientists and later himself, in bombarding beryllium with alpha particles. In 1935, Chadwick was awarded the Nobel Prize in Physics for this discovery.\nInduced nuclear reaction and probing the nucleus.\nIn Rutherford's four-part article on the \"Collision of \u03b1-particles with light atoms\" he reported two additional fundamental and far reaching discoveries. First, he showed that at high angles the scattering of alpha particles from hydrogen differed from the theoretical results he himself published in 1911. These were the first results to probe the interactions that hold a nucleus together. Second, he showed that \u03b1-particles colliding with nitrogen nuclei would react rather than simply bounce off. One product of the reaction was the proton; the other product was shown by Patrick Blackett, Rutherford's colleague and former student, to be oxygen:\n14N + \u03b1 \u2192 17O + p. \nRutherford therefore recognised \"that the nucleus may increase rather than diminish in mass as the result of collisions in which the proton is expelled\".\nBlackett was awarded the Nobel prize in 1948 for his work in perfecting the high-speed cloud chamber apparatus used to make that discovery and many others.\nPersonal life and death.\nIn the late 1880s Rutherford made his grandmother a wooden potato masher, which is now in the collection of the Royal Society.\nIn 1900, at St Paul's Anglican Church, Papanui in Christchurch, Rutherford married Mary Georgina Newton (1876\u20131954), to whom he had been engaged before leaving New Zealand. They had one daughter, Eileen Mary (1901\u20131930); she married the physicist Ralph Fowler, and died during the birth of her fourth child. Rutherford's hobbies included golf and motoring.\nFor some time before his death, Rutherford had a small hernia, which he neglected to have repaired, and it eventually became strangulated, rendering him violently ill. He had an emergency operation in London, but died in Cambridge four days later, on 19 October 1937, at the age of 66, of what physicians termed \"intestinal paralysis.\" After cremation at Golders Green Crematorium, he was given the high honour of burial in Westminster Abbey, near Isaac Newton, Charles Darwin, and other illustrious British scientists.\nLegacy.\nAt the opening session of the 1938 Indian Science Congress, which Rutherford had been expected to preside over before his death, astrophysicist James Jeans spoke in his place and deemed him \"one of the greatest scientists of all time\", saying:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In his flair for the right line of approach to a problem, as well as in the simple directness of his methods of attack, [Rutherford] often reminds us of Faraday, but he had two great advantages which Faraday did not possess, first, exuberant bodily health and energy, and second, the opportunity and capacity to direct a band of enthusiastic co-workers. Great though Faraday's output of work was, it seems to me that to match Rutherford's work in quantity as well as in quality, we must go back to Newton. In some respects he was more fortunate than Newton. Rutherford was ever the happy warrior \u2013 happy in his work, happy in its outcome, and happy in its human contacts.\nNuclear physics.\nRutherford is known as \"the father of nuclear physics\" because his research, and work done under him as laboratory director, established the nuclear structure of the atom and the essential nature of radioactive decay as a nuclear process. Patrick Blackett, a research fellow working under Rutherford, using natural alpha particles, demonstrated \"induced\" nuclear transmutation. Later, Rutherford's team, using protons from an accelerator, demonstrated \"artificially-induced\" nuclear reactions and transmutation.\nRutherford died too early to see Le\u00f3 Szil\u00e1rd's idea of controlled nuclear chain reactions come into being. However, a speech of Rutherford's about his artificially-induced transmutation in lithium, printed in the 12 September 1933 issue of \"The Times\", was reported by Szil\u00e1rd to have been his inspiration for thinking of the possibility of a controlled energy-producing nuclear chain reaction.\nRutherford's speech touched on the 1932 work of his students John Cockcroft and Ernest Walton in \"splitting\" lithium into alpha particles by bombardment with protons from a particle accelerator they had constructed. Rutherford realised that the energy released from the split lithium atoms was enormous, but he also realised that the energy needed for the accelerator, and its essential inefficiency in splitting atoms in this fashion, made the project an impossibility as a practical source of energy (accelerator-induced fission of light elements remains too inefficient to be used in this way, even today). Rutherford's speech in part, read:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We might in these processes obtain very much more energy than the proton supplied, but on the average we could not expect to obtain energy in this way. It was a very poor and inefficient way of producing energy, and anyone who looked for a source of power in the transformation of the atoms was talking moonshine. But the subject was scientifically interesting because it gave insight into the atoms.\nThe element rutherfordium, Rf, Z=104, was named in honour of Rutherford in 1997.\nIn popular culture.\nAndrew Hodwitz portrays Rutherford in episode 11 of season 13 \"Staring Blindly into the Future\" (January 13, 2020) of the Canadian television period detective series \"Murdoch Mysteries\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9604", "revid": "14650386", "url": "https://en.wikipedia.org/wiki?curid=9604", "title": "Many-worlds interpretation", "text": "Interpretation of quantum mechanics\nThe many-worlds interpretation (MWI) is an interpretation of quantum mechanics that asserts that the universal wavefunction is objectively real, and that there is no wave function collapse. This implies that all possible outcomes of quantum measurements are physically realized in different \"worlds\". The evolution of reality as a whole in MWI is rigidly deterministic and local. Many-worlds is also called the relative state formulation or the Everett interpretation, after physicist Hugh Everett, who first proposed it in 1957. Bryce DeWitt popularized the formulation and named it \"many-worlds\" in the 1970s.\nIn modern versions of many-worlds, the subjective appearance of wave function collapse is explained by the mechanism of quantum decoherence. Decoherence approaches to interpreting quantum theory have been widely explored and developed since the 1970s. MWI is considered a mainstream interpretation of quantum mechanics, along with the other decoherence interpretations, the Copenhagen interpretation, and hidden variable theories such as Bohmian mechanics.\nThe many-worlds interpretation implies that there are many parallel, non-interacting worlds. It is one of a number of multiverse hypotheses in physics and philosophy. MWI views time as a many-branched tree, wherein every possible quantum outcome is realized. This is intended to resolve the measurement problem and thus some paradoxes of quantum theory, such as Wigner's friend, the EPR paradox and Schr\u00f6dinger's cat, since every possible outcome of a quantum event exists in its own world.\nOverview of the interpretation.\nThe many-worlds interpretation's key idea is that the linear and unitary dynamics of quantum mechanics applies everywhere and at all times and so describes the whole universe. In particular, it models a measurement as a unitary transformation, a correlation-inducing interaction, between observer and object, without using a collapse postulate, and models observers as ordinary quantum-mechanical systems. This stands in contrast to the Copenhagen interpretation, in which a measurement is a \"primitive\" concept, not describable by unitary quantum mechanics; using the Copenhagen interpretation the universe is divided into a quantum and a classical domain, and the collapse postulate is central. In MWI, there is no division between classical and quantum: everything is quantum and there is no collapse. MWI's main conclusion is that the universe (or multiverse in this context) is composed of a quantum superposition of an uncountable or undefinable amount or number of increasingly divergent, non-communicating parallel universes or quantum worlds. Sometimes dubbed Everett worlds, each is an internally consistent and actualized alternative history or timeline.\nThe many-worlds interpretation uses decoherence to explain the measurement process and the emergence of a quasi-classical world. Wojciech H. Zurek, one of decoherence theory's pioneers, said: \"Under scrutiny of the environment, only pointer states remain unchanged. Other states decohere into mixtures of stable pointer states that can persist, and, in this sense, exist: They are einselected.\" Zurek emphasizes that his work does not depend on a particular interpretation.\nThe many-worlds interpretation shares many similarities with the decoherent histories interpretation, which also uses decoherence to explain the process of measurement or wave function collapse. MWI treats the other histories or worlds as real, since it regards the universal wave function as the \"basic physical entity\" or \"the fundamental entity, obeying at all times a deterministic wave equation\". The decoherent histories interpretation, on the other hand, needs only one of the histories (or worlds) to be real.\nSeveral authors, including Everett, John Archibald Wheeler and David Deutsch, call many-worlds a theory or metatheory, rather than just an interpretation. Everett argued that it was the \"only completely coherent approach to explaining both the contents of quantum mechanics and the appearance of the world.\" Deutsch dismissed the idea that many-worlds is an \"interpretation\", saying that to call it an interpretation \"is like talking about dinosaurs as an 'interpretation' of fossil records\".\nFormulation.\nIn his 1957 doctoral dissertation, Everett proposed that, rather than relying on external observation for analysis of isolated quantum systems, one could mathematically model an object, as well as its observers, as purely physical systems within the mathematical framework developed by Paul Dirac, John von Neumann, and others, discarding altogether the \"ad hoc\" mechanism of wave function collapse.\nRelative state.\nEverett's original work introduced the concept of a \"relative state\". Two (or more) subsystems, after a general interaction, become \"correlated\", or as is now said, entangled. Everett noted that such entangled systems can be expressed as the sum of products of states, where the two or more subsystems are each in a state relative to each other. After a measurement or observation one of the pair (or triple, etc.) is the measured, object or observed system, and one other member is the measuring apparatus (which may include an observer) having recorded the state of the measured system. Each product of subsystem states in the overall superposition evolves over time independently of other products. Once the subsystems interact, their states have become correlated or entangled and can no longer be considered independent. In Everett's terminology, each subsystem state was now \"correlated\" with its \"relative state\", since each subsystem must now be considered relative to the other subsystems with which it has interacted.\nIn the example of Schr\u00f6dinger's cat, after the box is opened, the entangled system is the cat, the poison vial and the observer. \"One\" relative triple of states would be the alive cat, the unbroken vial and the observer seeing an alive cat. \"Another\" relative triple of states would be the dead cat, the broken vial and the observer seeing a dead cat.\nIn the example of a measurement of a continuous variable (e.g., position \"q\"), the object-observer system decomposes into a continuum of pairs of relative states: the object system's relative state becomes a Dirac delta function each centered on a particular value of \"q\" and the corresponding observer relative state representing an observer having recorded the value of \"q\". The states of the pairs of relative states are, post measurement, \"correlated\" with each other.\nIn Everett's scheme, there is no collapse; instead, the Schr\u00f6dinger equation, or its quantum field theory, relativistic analog, holds all the time, everywhere. An observation or measurement is modeled by applying the wave equation to the entire system, comprising the object being observed \"and\" the observer. One consequence is that every observation causes the combined observer\u2013object's wavefunction to change into a quantum superposition of two or more non-interacting branches.\nThus the process of measurement or observation, or any correlation-inducing interaction, splits the system into sets of relative states, where each set of relative states, forming a branch of the universal wave function, is consistent within itself, and all future measurements (including by multiple observers) will confirm this consistency.\nRenamed many-worlds.\nEverett had referred to the combined observer\u2013object system as split by an observation, each split corresponding to the different or multiple possible outcomes of an observation. These splits generate a branching tree, where each branch is a set of all the states relative to each other. Bryce DeWitt popularized Everett's work with a series of publications calling it the Many Worlds Interpretation. Focusing on the splitting process, DeWitt introduced the term \"world\" to describe a single branch of that tree, which is a consistent history. All observations or measurements within any branch are consistent within themselves.\nSince many observation-like events have happened and are constantly happening, Everett's model implies that there are an enormous and growing number of simultaneously existing states or \"worlds\".\nProperties.\nMWI removes the observer-dependent role in the quantum measurement process by replacing wave function collapse with the established mechanism of quantum decoherence. As the observer's role lies at the heart of all \"quantum paradoxes\" such as the EPR paradox and von Neumann's \"boundary problem\", this provides a clearer and easier approach to their resolution.\nSince the Copenhagen interpretation requires the existence of a classical domain beyond the one described by quantum mechanics, it has been criticized as inadequate for the study of cosmology. While there is no evidence that Everett was inspired by issues of cosmology, he developed his theory with the explicit goal of allowing quantum mechanics to be applied to the universe as a whole, hoping to stimulate the discovery of new phenomena. This hope has been realized in the later development of quantum cosmology.\nMWI is a realist, deterministic and local theory. It achieves this by removing wave function collapse, which is indeterministic and nonlocal, from the deterministic and local equations of quantum theory.\nMWI (like other, broader multiverse theories) provides a context for the anthropic principle, which may provide an explanation for the fine-tuned universe.\nMWI depends crucially on the linearity of quantum mechanics, which underpins the superposition principle. If the final theory of everything is non-linear with respect to wavefunctions, then many-worlds is invalid. All quantum field theories are linear and compatible with the MWI, a point Everett emphasized as a motivation for the MWI. While quantum gravity or string theory may be non-linear in this respect, there is as yet no evidence of this.\nWeingarten and Taylor &amp; McCulloch have made separate proposals for how to define wavefunction branches in terms of quantum circuit complexity.\nAlternative to wavefunction collapse.\nAs with the other interpretations of quantum mechanics, the many-worlds interpretation is motivated by behavior that can be illustrated by the double-slit experiment. When particles of light (or anything else) pass through the double slit, a calculation assuming wavelike behavior of light can be used to identify where the particles are likely to be observed. Yet when the particles are observed in this experiment, they appear as particles (i.e., at definite places) and not as non-localized waves.\nSome versions of the Copenhagen interpretation of quantum mechanics proposed a process of \"collapse\" in which an indeterminate quantum system would probabilistically collapse onto, or select, just one determinate outcome to \"explain\" this phenomenon of observation. Wave function collapse was widely regarded as artificial and \"ad hoc\", so an alternative interpretation in which the behavior of measurement could be understood from more fundamental physical principles was considered desirable.\nEverett's PhD work provided such an interpretation. He argued that for a composite system\u2014such as a subject (the \"observer\" or measuring apparatus) observing an object (the \"observed\" system, such as a particle)\u2014the claim that either the observer or the observed has a well-defined state is meaningless; in modern parlance, the observer and the observed have become entangled: we can only specify the state of one \"relative\" to the other, i.e., the state of the observer and the observed are correlated \"after\" the observation is made. This led Everett to derive from the unitary, deterministic dynamics alone (i.e., without assuming wave function collapse) the notion of a \"relativity of states\".\nEverett noticed that the unitary, deterministic dynamics alone entailed that after an observation is made each element of the quantum superposition of the combined subject\u2013object wave function contains two \"relative states\": a \"collapsed\" object state and an associated observer who has observed the same collapsed outcome; what the observer sees and the state of the object have become correlated by the act of measurement or observation. The subsequent evolution of each pair of relative subject\u2013object states proceeds with complete indifference as to the presence or absence of the other elements, \"as if\" wave function collapse has occurred, which has the consequence that later observations are always consistent with the earlier observations. Thus the \"appearance\" of the object's wave function's collapse has emerged from the unitary, deterministic theory itself. (This answered Einstein's early criticism of quantum theory: that the theory should define what is observed, not for the observables to define the theory.) Since the wave function \"appears\" to have collapsed then, Everett reasoned, there was no need to actually assume that it \"had\" collapsed. And so, invoking Occam's razor, he removed the postulate of wave function collapse from the theory.\nTestability.\nIn 1985, David Deutsch proposed a variant of the Wigner's friend thought experiment as a test of many-worlds versus the Copenhagen interpretation. It consists of an experimenter (Wigner's friend) making a measurement on a quantum system in an isolated laboratory, and another experimenter (Wigner) who would make a measurement on the first one. According to the many-worlds theory, the first experimenter would end up in a macroscopic superposition of seeing one result of the measurement in one branch, and another result in another branch. The second experimenter could then interfere these two branches in order to test whether it is in fact in a macroscopic superposition or has collapsed into a single branch, as predicted by the Copenhagen interpretation. Since then Lockwood, Vaidman, and others have made similar proposals, which require placing macroscopic objects in a coherent superposition and interfering them, a task currently beyond experimental capability.\nProbability and the Born rule.\nSince the many-worlds interpretation's inception, physicists have been puzzled about the role of probability in it. As put by Wallace, there are two facets to the question: the \"incoherence problem\", which asks why we should assign probabilities at all to outcomes that are certain to occur in some worlds, and the \"quantitative problem\", which asks why the probabilities should be given by the Born rule.\nEverett tried to answer these questions in the paper that introduced many-worlds. To address the incoherence problem, he argued that an observer who makes a sequence of measurements on a quantum system will in general have an apparently random sequence of results in their memory, which justifies the use of probabilities to describe the measurement process. To address the quantitative problem, Everett proposed a derivation of the Born rule based on the properties that a measure on the branches of the wave function should have. His derivation has been criticized as relying on unmotivated assumptions. Since then several other derivations of the Born rule in the many-worlds framework have been proposed. There is no consensus on whether this has been successful.\nFrequentism.\nDeWitt and Graham and Farhi et al., among others, have proposed derivations of the Born rule based on a frequentist interpretation of probability. They try to show that in the limit of uncountably many measurements, no worlds would have relative frequencies that didn't match the probabilities given by the Born rule, but these derivations have been shown to be mathematically incorrect.\nDecision theory.\nA decision-theoretic derivation of the Born rule was produced by David Deutsch (1999) and refined by Wallace and Saunders. They consider an agent who takes part in a quantum gamble: the agent makes a measurement on a quantum system, branches as a consequence, and each of the agent's future selves receives a reward that depends on the measurement result. The agent uses decision theory to evaluate the price they would pay to take part in such a gamble, and concludes that the price is given by the utility of the rewards weighted according to the Born rule. Some reviews have been positive, although these arguments remain highly controversial; some theoretical physicists have taken them as supporting the case for parallel universes. For example, a \"New Scientist\" story on a 2007 conference about Everettian interpretations quoted physicist Andy Albrecht as saying, \"This work will go down as one of the most important developments in the history of science.\" In contrast, the philosopher Huw Price, also attending the conference, found the Deutsch\u2013Wallace\u2013Saunders approach fundamentally flawed.\nSymmetries and invariance.\nIn 2005, Zurek produced a derivation of the Born rule based on the symmetries of entangled states; Schlosshauer and Fine argue that Zurek's derivation is not rigorous, as it does not define what probability is and has several unstated assumptions about how it should behave.\nIn 2016, Charles Sebens and Sean M. Carroll, building on work by Lev Vaidman, proposed a similar approach based on self-locating uncertainty. In this approach, decoherence creates multiple identical copies of observers, who can assign credences to being on different branches using the Born rule. The Sebens\u2013Carroll approach has been criticized by Adrian Kent, and Vaidman does not find it satisfactory.\nBranch counting.\nIn 2021, Simon Saunders produced a branch counting derivation of the Born rule. The crucial feature of this approach is to define the branches so that they all have the same magnitude or 2-norm. The ratios of the numbers of branches thus defined give the probabilities of the various outcomes of a measurement, in accordance with the Born rule.\nPreferred basis problem.\nAs originally formulated by Everett and DeWitt, the many-worlds interpretation had a privileged role for measurements: they determined which basis of a quantum system would give rise to the eponymous worlds. Without this the theory was ambiguous, as a quantum state can equally well be described (e.g.) as having a well-defined position or as being a superposition of two delocalized states. The assumption is that the preferred basis to use is the one which assigns a unique measurement outcome to each world. This special role for measurements is problematic for the theory, as it contradicts Everett and DeWitt's goal of having a reductionist theory and undermines their criticism of the ill-defined measurement postulate of the Copenhagen interpretation. This is known today as the \"preferred basis problem\".\nThe preferred basis problem has been solved, according to Saunders and Wallace, among others, by incorporating decoherence into the many-worlds theory. In this approach, the preferred basis does not have to be postulated, but rather is identified as the basis stable under environmental decoherence. In this way measurements no longer play a special role; rather, any interaction that causes decoherence causes the world to split. Since decoherence is never complete, there will always remain some infinitesimal overlap between two worlds, making it arbitrary whether a pair of worlds has split or not. Wallace argues that this is not problematic: it only shows that worlds are not a part of the fundamental ontology, but rather of the \"emergent\" ontology, where these approximate, effective descriptions are routine in the physical sciences. Since in this approach the worlds are derived, it follows that they must be present in any other interpretation of quantum mechanics that does not have a collapse mechanism, such as Bohmian mechanics.\nThis approach to deriving the preferred basis has been criticized as creating circularity with derivations of probability in the many-worlds interpretation, as decoherence theory depends on probability and probability depends on the ontology derived from decoherence. Wallace contends that decoherence theory depends not on probability but only on the notion that one is allowed to do approximations in physics.\nHistory.\nMWI originated in Everett's Princeton University PhD thesis \"The Theory of the Universal Wave Function\", developed under his thesis advisor John Archibald Wheeler, a shorter summary of which was published in 1957 under the title \"Relative State Formulation of Quantum Mechanics\" (Wheeler contributed the title \"relative state\"; Everett originally called his approach the \"Correlation Interpretation\", where \"correlation\" refers to quantum entanglement). The phrase \"many-worlds\" is due to Bryce DeWitt, who was responsible for the wider popularization of Everett's theory, which had been largely ignored for a decade after publication in 1957.\nEverett's proposal was not without precedent. In 1952, Erwin Schr\u00f6dinger gave a lecture in Dublin in which at one point he jocularly warned his audience that what he was about to say might \"seem lunatic\". He went on to assert that while the Schr\u00f6dinger equation seemed to be describing several different histories, they were \"not alternatives but all really happen simultaneously\". According to David Deutsch, this is the earliest known reference to many-worlds; Jeffrey A. Barrett describes it as indicating the similarity of \"general views\" between Everett and Schr\u00f6dinger. Schr\u00f6dinger's writings from the period also contain elements resembling the modal interpretation originated by Bas van Fraassen. Because Schr\u00f6dinger subscribed to a kind of post-Machian neutral monism, in which \"matter\" and \"mind\" are only different aspects or arrangements of the same common elements, treating the wave function as physical and treating it as information became interchangeable.\nLeon Cooper and Deborah Van Vechten developed a very similar approach before reading Everett's work. Zeh also came to the same conclusions as Everett before reading his work, then built a new theory of quantum decoherence based on these ideas.\nAccording to people who knew him, Everett believed in the literal reality of the other quantum worlds. His son and wife reported that he \"never wavered in his belief over his many-worlds theory\". In their detailed review of Everett's work, Osnaghi, Freitas, and Freire Jr. note that Everett consistently used quotes around \"real\" to indicate a meaning within scientific practice.\nReception.\nMWI's initial reception was overwhelmingly negative, in the sense that it was ignored, with the notable exception of DeWitt. Wheeler made considerable efforts to formulate the theory in a way that would be palatable to Bohr, visited Copenhagen in 1956 to discuss it with him, and convinced Everett to visit as well, which happened in 1959. Nevertheless, Bohr and his collaborators completely rejected the theory. Everett had already left academia in 1957, never to return, and in 1980, Wheeler disavowed the theory.\nSupport.\nOne of the strongest longtime advocates of MWI is David Deutsch. According to him, the single photon interference pattern observed in the double slit experiment can be explained by interference of photons in multiple universes. Viewed this way, the single photon interference experiment is indistinguishable from the multiple photon interference experiment. In a more practical vein, in one of the earliest papers on quantum computing, Deutsch suggested that parallelism that results from MWI could lead to \"a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it\". He also proposed that MWI will be testable (at least against \"naive\" Copenhagenism) when reversible computers become conscious via the reversible observation of spin.\nEquivocal.\nPhilosophers of science James Ladyman and Don Ross say that MWI could be true, but do not embrace it. They note that no quantum theory is yet empirically adequate for describing all of reality, given its lack of unification with general relativity, and so do not see a reason to regard any interpretation of quantum mechanics as the final word in metaphysics. They also suggest that the multiple branches may be an artifact of incomplete descriptions and of using quantum mechanics to represent the states of macroscopic objects. They argue that macroscopic objects are significantly different from microscopic objects in not being isolated from the environment, and that using quantum formalism to describe them lacks explanatory and descriptive power and accuracy.\nRejection.\nSome scientists consider some aspects of MWI to be unfalsifiable and hence unscientific because the multiple parallel universes are non-communicating, in the sense that no information can be passed between them.\nVictor J. Stenger remarked that Murray Gell-Mann's published work explicitly rejects the existence of simultaneous parallel universes. Collaborating with James Hartle, Gell-Mann worked toward the development of a more \"palatable\" \"post-Everett quantum mechanics\". Stenger thought it fair to say that most physicists find MWI too extreme, though it \"has merit in finding a place for the observer inside the system being analyzed and doing away with the troublesome notion of wave function collapse\".\nRoger Penrose argues that the idea is flawed because it is based on an oversimplified version of quantum mechanics that does not account for gravity. In his view, applying conventional quantum mechanics to the universe implies the MWI, but the lack of a successful theory of quantum gravity negates the claimed universality of conventional quantum mechanics. According to Penrose, \"the rules must change when gravity is involved\". He further asserts that gravity helps anchor reality and \"blurry\" events have only one allowable outcome: \"electrons, atoms, molecules, etc., are so minute that they require almost no amount of energy to maintain their gravity, and therefore their overlapping states. They can stay in that state forever, as described in standard quantum theory\". On the other hand, \"in the case of large objects, the duplicate states disappear in an instant due to the fact that these objects create a large gravitational field\".\nPhilosopher of science Robert P. Crease says that MWI is \"one of the most implausible and unrealistic ideas in the history of science\" because it means that everything conceivable happens. Science writer Philip Ball calls MWI's implications fantasies, since \"beneath their apparel of scientific equations or symbolic logic, they are acts of imagination, of 'just supposing'\".\nTheoretical physicist Gerard 't Hooft also dismisses the idea: \"I do not believe that we have to live with the many-worlds interpretation. Indeed, it would be a stupendous number of parallel worlds, which are only there because physicists couldn't decide which of them is real.\"\nAsher Peres was an outspoken critic of MWI. A section of his 1993 textbook had the title \"Everett's interpretation and other bizarre theories\". Peres argued that the various many-worlds interpretations merely shift the arbitrariness or vagueness of the collapse postulate to the question of when \"worlds\" can be regarded as separate, and that no objective criterion for that separation can actually be formulated.\nPolls.\nA poll of 72 \"leading quantum cosmologists and other quantum field theorists\" conducted before 1991 by L. David Raub showed 58% agreement with \"Yes, I think MWI is true\".\nMax Tegmark reports the result of a \"highly unscientific\" poll taken at a 1997 quantum mechanics workshop. According to Tegmark, \"The many worlds interpretation (MWI) scored second, comfortably ahead of the consistent histories and Bohm interpretations.\"\nIn response to Sean M. Carroll's statement \"As crazy as it sounds, most working physicists buy into the many-worlds theory\", Michael Nielsen counters: \"at a quantum computing conference at Cambridge in 1998, a many-worlder surveyed the audience of approximately 200 people\u00a0... Many-worlds did just fine, garnering support on a level comparable to, but somewhat below, Copenhagen and decoherence.\" But Nielsen notes that it seemed most attendees found it to be a waste of time: Peres \"got a huge and sustained round of applause\u2026when he got up at the end of the polling and asked 'And who here believes the laws of physics are decided by a democratic vote?'\"\nA 2005 poll of fewer than 40 students and researchers taken after a course on the Interpretation of Quantum Mechanics at the Institute for Quantum Computing University of Waterloo found \"Many Worlds (and decoherence)\" to be the least favored.\nA 2011 poll of 33 participants at an Austrian conference on quantum foundations found 6 endorsed MWI, 8 \"Information-based/information-theoretical\", and 14 Copenhagen; the authors remark that MWI received a similar percentage of votes as in Tegmark's 1997 poll.\nSpeculative implications.\nDeWitt has said that Everett, Wheeler, and Graham \"do not in the end exclude any element of the superposition. All the worlds are there, even those in which everything goes wrong and all the statistical laws break down.\" Tegmark affirmed that absurd or highly unlikely events are rare but inevitable under MWI: \"Things inconsistent with the laws of physics will never happen\u2014everything else will\u00a0... it's important to keep track of the statistics, since even if everything conceivable happens somewhere, really freak events happen only exponentially rarely.\" David Deutsch speculates in his book \"The Beginning of Infinity\" that some fiction, such as alternate history, could occur somewhere in the multiverse, as long as it is consistent with the laws of physics.\nAccording to Ladyman and Ross, many seemingly physically plausible but unrealized possibilities, such as those discussed in other scientific fields, generally have no counterparts in other branches, because they are in fact incompatible with the universal wave function. According to Carroll, human decision-making, contrary to common misconceptions, is best thought of as a classical process, not a quantum one, because it works on the level of neurochemistry rather than fundamental particles. Human decisions do not cause the world to branch into equally realized outcomes; even for subjectively difficult decisions, the \"weight\" of realized outcomes is almost entirely concentrated in a single branch.\n\"Quantum suicide\" is a thought experiment in quantum mechanics and the philosophy of physics that can purportedly distinguish between the Copenhagen interpretation of quantum mechanics and the many-worlds interpretation by a variation of the Schr\u00f6dinger's cat thought experiment, from the cat's point of view. \"Quantum immortality\" refers to the subjective experience of surviving quantum suicide. Most experts believe the experiment would not work in the real world, because the world with the surviving experimenter has a lower \"measure\" than the world before the experiment, making it less likely that the experimenter will experience their survival.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9606", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=9606", "title": "Human factors and ergonomics", "text": "Designing systems to suit their users"}
{"id": "9607", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=9607", "title": "Electromagnectic radiation", "text": ""}
{"id": "9611", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=9611", "title": "E-commerce", "text": "Type of business industry usually conducted over the internet\nE-commerce (electronic commerce) refers to commercial activities including the electronic buying or selling products and services which are conducted on online platforms or over the Internet. E-commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. E-commerce is a part of the retail, the largest segment of the electronics industry and is in turn driven by the technological advances of the semiconductor industry.\nDefining e-commerce.\nThe term was coined and first employed by Robert Jacobson, Principal Consultant to the California State Assembly's Utilities &amp; Commerce Committee, in the title and text of California's Electronic Commerce Act, carried by the late Committee Chairwoman Gwen Moore (D-L.A.) and enacted in 1984.\nE-commerce typically uses the web for at least a part of a transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of products (such as books from Amazon) or services (such as music downloads in the form of digital distribution such as the iTunes Store). There are three areas of e-commerce: online retailing, electronic markets, and online auctions. E-commerce is supported by electronic business. The existence value of e-commerce is to allow consumers to shop online and pay online through the Internet, saving the time and space of customers and enterprises, greatly improving transaction efficiency, especially for busy office workers, and also saving a lot of valuable time.\nE-commerce businesses may also employ some or all of the following:\nThere are five essential categories of E-commerce:\nForms.\nContemporary electronic commerce can be classified into two categories. The first category is business based on types of goods sold (involves everything from ordering \"digital\" content for immediate online consumption, to ordering conventional goods and services, to \"meta\" services to facilitate other types of electronic commerce). The second category is based on the nature of the participant (B2B, B2C, C2B and C2C).\nOn the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.\nAside from traditional e-commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce have also been used.\nGovernmental regulation.\nIn the United States, California's Electronic Commerce Act (1984), enacted by the Legislature, the more recent California Privacy Rights Act (2020), enacted through a popular election proposition and to control specifically how electronic commerce may be conducted in California. In the US in its entirety, electronic commerce activities are regulated more broadly by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive. Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information. As a result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.\nThe Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.\nConflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996).\nInternationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.\nThere is also Asia Pacific Economic Cooperation. APEC was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.\nIn Australia, trade is covered under Australian Treasury Guidelines for electronic commerce and the Australian Competition &amp; Consumer Commission regulates and offers advice on how to deal with businesses online, and offers specific advice on what happens if things go wrong.\nThe European Union undertook an extensive enquiry into e-commerce in 2015\u201316 which observed significant growth in the development of e-commerce, along with some developments which raised concerns, such as increased use of selective distribution systems, which allow manufacturers to control routes to market, and \"increased use of contractual restrictions to better control product distribution\". The European Commission felt that some emerging practices might be justified if they could improve the quality of product distribution, but \"others may unduly prevent consumers from benefiting from greater product choice and lower prices in e-commerce and therefore warrant Commission action\" in order to promote compliance with EU competition rules.\nIn the United Kingdom, the Financial Services Authority (FSA) was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority. The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD required the European Commission to report on the implementation and impact of the PSD by 1 November 2012.\nIn India, the Information Technology Act 2000 governs the basic applicability of e-commerce.\nIn China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce. On the same day, the Administrative Measures on Internet Information Services were released, the first administrative regulations to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China. On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted an Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China's e-commerce legislation. It was a milestone in the course of improving China's electronic commerce legislation, and also marks the entering of China's rapid development stage for electronic commerce legislation.\nGlobal trends.\nE-commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.\nCross-border e-Commerce is also an essential field for e-Commerce businesses. \u00a0It has responded to the trend of globalization. It shows that numerous firms have opened up new businesses, expanded new markets, and overcome trade barriers; more and more enterprises have started exploring the cross-border cooperation field. In addition, compared with traditional cross-border trade, the information on cross-border e-commerce is more concealed. In the era of globalization, cross-border e-commerce for inter-firm companies means the activities, interactions, or social relations of two or more e-commerce enterprises. However, the success of cross-border e-commerce promotes the development of small and medium-sized firms, and it has finally become a new transaction mode. It has helped the companies solve financial problems and realize the reasonable allocation of resources field. SMEs ( small and medium enterprises) can also precisely match the demand and supply in the market, having the industrial chain majorization and creating more revenues for companies.\nIn 2012, e-commerce sales topped $1 trillion for the first time in history.\nMobile devices are playing an increasing role in the mix of e-commerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.\nFor traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested an enormous volume of investment in mobile applications. The DeLone and McLean Model stated that three perspectives contribute to a successful e-business: information system quality, service quality and users' satisfaction. There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in the company.\nModern 3D graphics technologies, such as Facebook 3D Posts, are considered by some social media marketers and advertisers as a preferable way to promote consumer goods than static photos, and some brands like Sony are already paving the way for augmented reality commerce. Wayfair now lets you inspect a 3D version of its furniture in a home setting before buying.\nChina.\nAmong emerging economies, China's e-commerce presence continued to expand every year. With 668 million Internet users as of 2014, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in that period. The Chinese retailers have been able to help consumers feel more comfortable shopping online. e-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade. In 2013, Alibaba had an e-commerce market share of 80% in China. In 2014, Alibaba still dominated the B2B marketplace in China with a market share of 44.82%, followed by several other companies including Made-in-China.com at 3.21%, and GlobalSources.com at 2.98%, with the total transaction value of China's B2B market exceeding 4.5 billion yuan.\nChina was also the largest e-commerce market in the world by value of sales, with an estimated US$ in 2016. It accounted for 42.4% of worldwide retail e-commerce in that year, the most of any country.110 Research shows that Chinese consumer motivations are different enough from Western audiences to require unique e-commerce app designs instead of simply porting Western apps into the Chinese market.\nThe expansion of e-commerce in China has resulted in the development of Taobao villages, clusters of e-commerce businesses operating in rural areas.112 Because Taobao villages have increased the incomes or rural people and entrepreneurship in rural China, Taobao villages have become a component of rural revitalization strategies.278\nIn 2015, the State Council promoted the Internet Plus initiative, a five-year plan to integrate traditional manufacturing and service industries with big data, cloud computing, and Internet of things technology.44 The State Council provided support for Internet Plus through policy support in area including cross-border e-commerce and rural e-commerce.44\nIn 2019, the city of Hangzhou established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to e-commerce and internet-related intellectual property claims.124\nEurope.\nIn 2010, the United Kingdom had the highest per capita e-commerce spending in the world. As of 2013, the Czech Republic was the European country where e-commerce delivers the biggest contribution to the enterprises' total revenue. Almost a quarter (24%) of the country's total turnover is generated via the online channel.\nArab states.\nThe rate of growth of the number of internet users in the Arab countries has been rapid \u2013 13.1% in 2015. A significant portion of the e-commerce market in the Middle East comprises people in the 30\u201334 year age group. Egypt has the largest number of internet users in the region, followed by Saudi Arabia and Morocco; these constitute 3/4th of the region's share. Yet, internet penetration is low: 35% in Egypt and 65% in Saudi Arabia.\nThe Gulf Cooperation Council countries have a rapidly growing market and are characterized by a population that becomes wealthier (Yuldashev). As such, retailers have launched Arabic-language websites as a means to target this population. Secondly, there are predictions of increased mobile purchases and an expanding internet audience (Yuldashev). The growth and development of the two aspects make the GCC countries become larger players in the electronic commerce market with time progress. Specifically, research shows that the e-commerce market was expected to grow to over $20 billion by 2020 among these GCC countries (Yuldashev). The e-commerce market has also gained much popularity among western countries, and in particular Europe and the U.S. These countries have been highly characterized by consumer-packaged goods (CPG) (Geisler, 34). However, trends show that there are future signs of a reverse. Similar to the GCC countries, there has been increased purchase of goods and services in online channels rather than offline channels. Activist investors are trying hard to consolidate and slash their overall cost and the governments in western countries continue to impose more regulation on CPG manufacturers (Geisler, 36). In these senses, CPG investors are being forced to adapt to e-commerce as it is effective as well as a means for them to thrive.\nThe future trends in the GCC countries will be similar to that of the western countries. Despite the forces that push business to adapt e-commerce as a means to sell goods and products, the manner in which customers make purchases is similar in countries from these two regions. For instance, there has been an increased usage of smartphones which comes in conjunction with an increase in the overall internet audience from the regions. Yuldashev writes that consumers are scaling up to more modern technology that allows for mobile marketing.\nHowever, the percentage of smartphone and internet users who make online purchases is expected to vary in the first few years. It will be independent on the willingness of the people to adopt this new trend (The Statistics Portal). For example, UAE has the greatest smartphone penetration of 73.8 per cent and has 91.9 per cent of its population has access to the internet. On the other hand, smartphone penetration in Europe has been reported to be at 64.7 per cent (The Statistics Portal). Regardless, the disparity in percentage between these regions is expected to level out in future because e-commerce technology is expected to grow to allow for more users.\nThe e-commerce business within these two regions will result in competition. Government bodies at the country level will enhance their measures and strategies to ensure sustainability and consumer protection (Krings, et al.). These increased measures will raise the environmental and social standards in the countries, factors that will determine the success of the e-commerce market in these countries. For example, an adoption of tough sanctions will make it difficult for companies to enter the e-commerce market while lenient sanctions will allow ease of companies. As such, the future trends between GCC countries and the Western countries will be independent of these sanctions (Krings, et al.). These countries need to make rational conclusions in coming up with effective sanctions.\nIndia.\nIndia had an Internet user base of about 460 million as of December 2017. Despite being the third largest user base in the world, the penetration of the Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around six million new entrants every month. In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities. The India retail market was expected to rise from 2.5% in 2016 to 5% in 2020.\nBrazil.\nIn 2013, Brazil's e-commerce was growing quickly with retail e-commerce sales expected to grow at a double-digit pace through 2014. By 2016, eMarketer expected retail e-commerce sales in Brazil to reach $17.3 billion.\nLogistics.\nLogistics in e-commerce mainly concerns fulfillment. Online markets and retailers have to find the best possible way to fill orders and deliver products. Small companies usually control their own logistic operation because they do not have the ability to hire an outside company. Most large companies hire a fulfillment service that takes care of a company's logistic needs. The optimization of logistics processes that contains long-term investment in an efficient storage infrastructure system and adoption of inventory management strategies is crucial to prioritize customer satisfaction throughout the entire process, from order placement to final delivery.\nImpacts.\nImpact on markets and retailers.\nE-commerce markets grew at noticeable rates. The online market was expected to grow by 56% in 2015\u20132020. In 2017, retail e-commerce sales worldwide amounted to 2.3 trillion US dollars and e-retail revenues were projected to grow to 4.891 trillion US dollars in 2021. Traditional markets are only expected 2% growth during the same time. Brick and mortar retailers are struggling because of online retailer's ability to offer lower prices and higher efficiency. Many larger retailers are able to maintain a presence offline and online by linking physical and online offerings.\nE-commerce allows customers to overcome geographical barriers and allows them to purchase products anytime and from anywhere. Online and traditional markets have different strategies for conducting business. Traditional retailers offer fewer assortment of products because of shelf space where, online retailers often hold no inventory but send customer orders directly to the manufacturer. The pricing strategies are also different for traditional and online retailers. Traditional retailers base their prices on store traffic and the cost to keep inventory. Online retailers base prices on the speed of delivery.\nThere are two ways for marketers to conduct business through e-commerce: fully online or online along with a brick and mortar store. Online marketers can offer lower prices, greater product selection, and high efficiency rates. Many customers prefer online markets if the products can be delivered quickly at relatively low price. However, online retailers cannot offer the physical experience that traditional retailers can. It can be difficult to judge the quality of a product without the physical experience, which may cause customers to experience product or seller uncertainty. Another issue regarding the online market is concerns about the security of online transactions. Many customers remain loyal to well-known retailers because of this issue.\nSecurity is a primary problem for e-commerce in developed and developing countries. E-commerce security is protecting businesses' websites and customers from unauthorized access, use, alteration, or destruction. The type of threats include: malicious codes, unwanted programs (ad ware, spyware), phishing, hacking, and cyber vandalism. E-commerce websites use different tools to avert security threats. These tools include firewalls, encryption software, digital certificates, and passwords.\nImpact on supply chain management.\nFor a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.\nE-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimized the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.\nIn addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems, like SAP ERP, Xero, or Megaventory, have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.\nImpact on employment.\nE-commerce helps create new job opportunities due to information related services, software app and digital products. It also causes job losses. The areas with the greatest predicted job-loss are retail, postal, and travel agencies. The development of e-commerce will create jobs that require highly skilled workers to manage large amounts of information, customer demands, and production processes. In contrast, people with poor technical skills cannot enjoy the wages welfare. On the other hand, because e-commerce requires sufficient stocks that could be delivered to customers in time, the warehouse becomes an important element. Warehouse needs more staff to manage, supervise and organize, thus the condition of warehouse environment will be concerned by employees.\nImpact on customers.\nE-commerce brings convenience for customers as they do not have to leave home and only need to browse websites online, especially for buying products which are not sold in nearby shops. It could help customers buy a wider range of products and save customers' time. Consumers also gain power through online shopping. They are able to research products and compare prices among retailers. Thanks to the practice of user-generated ratings and reviews from companies like Bazaarvoice, Trustpilot, and Yelp, customers can also see what other people think of a product, and decide before buying if they want to spend money on it. Also, online shopping often provides sales promotion or discounts code, thus it is more price effective for customers. Moreover, e-commerce provides products' detailed information; even the in-store staff cannot offer such detailed explanation. Customers can also review and track the order history online.\nE-commerce technologies cut transaction costs by allowing both manufactures and consumers to skip through the intermediaries. This is achieved through by extending the search area best price deals and by group purchase. The success of e-commerce in urban and regional levels depend on how the local firms and consumers have adopted to e-commerce.\nHowever, e-commerce lacks human interaction for customers, especially who prefer face-to-face connection. Customers are also concerned with the security of online transactions and tend to remain loyal to well-known retailers. In recent years, clothing retailers such as Tommy Hilfiger have started adding Virtual Fit platforms to their e-commerce sites to reduce the risk of customers buying the wrong sized clothes, although these vary greatly in their fit for purpose. When the customer regret the purchase of a product, it involves returning goods and refunding process. This process is inconvenient as customers need to pack and post the goods. If the products are expensive, large or fragile, it refers to safety issues.\nImpact on the environment.\nIn 2018, E-commerce generated of container cardboard in North America, an increase from ) in 2017. Only 35 percent of North American cardboard manufacturing capacity was from recycled content. The recycling rate in Europe was 80 percent and Asia was 93 percent. Amazon, the largest user of boxes, had a strategy to cut back on packing material and reduced packaging material used by 19 percent by weight since 2016. Amazon is requiring retailers to manufacture their product packaging in a way that does not require additional shipping packaging. Amazon also has an 85-person team researching ways to reduce and improve their packaging and shipping materials.\nAccelerated movement of packages around the world includes accelerated movement of living things, such as invasive species. Weeds, pests, and diseases all sometimes travel in packages of seeds. Some of these packages are part of brushing manipulation of e-commerce reviews.\nImpact on traditional retail.\nE-commerce has been cited as a major force for the failure of major U.S. retailers in a trend frequently referred to as a \"retail apocalypse.\" The rise of e-commerce outlets like Amazon has made it harder for traditional retailers to attract customers to their stores and forced companies to change their sales strategies. Many companies have turned to sales promotions and increased digital efforts to lure shoppers while shutting down brick-and-mortar locations. The trend has forced some traditional retailers to shutter its brick and mortar operations.\nE-commerce during COVID-19.\nIn March 2020, global retail website traffic hit 14.3 billion visits signifying an unprecedented growth of e-commerce during the lockdown of 2020. Later studies show that online sales increased by 25% and online grocery shopping increased by over 100% during the crisis in the United States. Meanwhile, as many as 29% of surveyed shoppers state that they will never go back to shopping in person again; in the UK, 43% of consumers state that they expect to keep on shopping the same way even after the lockdown is over.\nRetail sales of e-commerce shows that COVID-19 has a significant impact on e-commerce and its sales were expected to reach $6.5 trillion by 2023.\nBusiness application.\nSome common applications related to electronic commerce are:\nTimeline.\nA timeline for the development of e-commerce:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9613", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=9613", "title": "Euler's formula", "text": "Complex exponential in terms of sine and cosine\nEuler's formula, named after Leonhard Euler, is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that, for any real number\u00a0x, one has\nformula_1\nwhere e is the base of the natural logarithm, i is the imaginary unit, and cos and sin are the trigonometric functions cosine and sine respectively. This complex exponential function is sometimes denoted cis \"x\" (\"cosine plus \"i\" sine\"). The formula is still valid if x is a complex number, and is also called \"Euler's formula\" in this more general case.\nEuler's formula is ubiquitous in mathematics, physics, chemistry, and engineering. The physicist Richard Feynman called the equation \"our jewel\" and \"the most remarkable formula in mathematics\".\nWhen \"x\" = \"\u03c0\", Euler's formula may be rewritten as \"ei\u03c0\" + 1 = 0 or \"ei\u03c0\" = \u22121, which is known as Euler's identity.\nHistory.\nIn 1714, the English mathematician Roger Cotes presented a geometrical argument that can be interpreted (after correcting a misplaced factor of formula_2) as:\nformula_3\nExponentiating this equation yields Euler's formula. Note that the logarithmic statement is not universally correct for complex numbers, since a complex logarithm can have infinitely many values, differing by multiples of 2\"\u03c0i\".\nAround 1740 Leonhard Euler turned his attention to the exponential function and derived the equation named after him by comparing the series expansions of the exponential and trigonometric expressions. The formula was first published in 1748 in his foundational work \"Introductio in analysin infinitorum\".\nJohann Bernoulli had found that\nformula_4\nAnd since\nformula_5\nthe above equation tells us something about complex logarithms by relating natural logarithms to imaginary (complex) numbers. Bernoulli, however, did not evaluate the integral.\nBernoulli's correspondence with Euler (who also knew the above equation) shows that Bernoulli did not fully understand complex logarithms. Euler also suggested that complex logarithms can have infinitely many values.\nThe view of complex numbers as points in the complex plane was described about 50 years later by Caspar Wessel.\nDefinitions of complex exponentiation.\nThe exponential function \"ex\" for real values of x may be defined in a few different equivalent ways (see Characterizations of the exponential function). Several of these methods may be directly extended to give definitions of \"ez\" for complex values of z simply by substituting z in place of x and using the complex algebraic operations. In particular, we may use any of the three following definitions, which are equivalent. From a more advanced perspective, each of these definitions may be interpreted as giving the unique analytic continuation of \"ex\" to the complex plane.\nDifferential equation definition.\nThe exponential function formula_6 is the unique differentiable function of a complex variable for which the derivative equals the function formula_7 and formula_8\nPower series definition.\nFor complex z\nformula_9\nUsing the ratio test, it is possible to show that this power series has an infinite radius of convergence and so defines \"ez\" for all complex z.\nLimit definition.\nFor complex z\nformula_10\nHere, n is restricted to positive integers, so there is no question about what the power with exponent n means.\nProofs.\nVarious proofs of the formula are possible.\nUsing differentiation.\nThis proof shows that the quotient of the trigonometric and exponential expressions is the constant function one, so they must be equal (the exponential function is never zero, so this is permitted).\nConsider the function \"f\"(\"\u03b8\") \nformula_11\nfor real \u03b8. Differentiating gives by the product rule\nformula_12\nThus, \"f\"(\"\u03b8\") is a constant. Since the exponential function is 1 for \"\u03b8\" = 0, by definition, and the complex trig function also evaluates to 1 there, \"f\"(0) = 1/1 = 1, then \"f\"(\"\u03b8\") = 1 for all real \u03b8, and thus \nformula_13\nUsing power series.\nHere is a proof of Euler's formula using power-series expansions, as well as basic facts about the powers of i:\nformula_14\nUsing now the power-series definition from above, we see that for real values of x\nformula_15\nwhere in the last step we recognize the two terms are the Maclaurin series for cos \"x\" and sin \"x\". The rearrangement of terms is justified because each series is absolutely convergent.\nUsing polar coordinates.\nAnother proof is based on the fact that all complex numbers can be expressed in polar coordinates and on the assumption that formula_16 can be likewise represented; this will be the case if we find a solution. Therefore, for some r and \u03b8 depending on x,\nformula_17\nNo assumptions are being made about r and \u03b8; they will be determined in the course of the proof. From any of the definitions of the exponential function it can be shown that the derivative of \"e\"\"ix\" is \"ie\"\"ix\". Therefore, differentiating both sides gives\nformula_18\nSubstituting \"r\"(cos \"\u03b8\" + \"i\" sin \"\u03b8\") for \"eix\" and equating real and imaginary parts in this formula gives ' = 0 and ' = 1. Thus, r is a constant, and \u03b8 is \"x\" + \"C\" for some constant C. We now have\nformula_19\nknowing that \"e\"\"i\"0 = 1, for \"\u03b8\" = 0, this becomes\nformula_20\ngiving us the constant formula_21 and proving the formula\nformula_22\nApplications.\nApplications in complex number theory.\nInterpretation of the formula.\nThis formula can be interpreted as saying that the function \"e\"\"i\u03c6\" is a unit complex number, i.e., it traces out the unit circle in the complex plane as \u03c6 ranges through the real numbers. Here \u03c6 is the angle that a line connecting the origin with a point on the unit circle makes with the positive real axis, measured counterclockwise and in radians.\nThe original proof is based on the Taylor series expansions of the exponential function \"e\"\"z\" (where z is a complex number) and of sin \"x\" and cos \"x\" for real numbers x (see above). In fact, the same proof shows that Euler's formula is even valid for all \"complex\" numbers\u00a0x.\nA point in the complex plane can be represented by a complex number written in cartesian coordinates. Euler's formula provides a means of conversion between cartesian coordinates and polar coordinates. The polar form simplifies the mathematics when used in multiplication or powers of complex numbers. Any complex number \"z\" = \"x\" + \"iy\", and its complex conjugate, \"z\" = \"x\" \u2212 \"iy\", can be written as\nformula_23\nwhere\n\u03c6 is the argument of z, i.e., the angle between the \"x\" axis and the vector \"z\" measured counterclockwise in radians, which is defined up to addition of 2\"\u03c0\". Many texts write \"\u03c6\" = tan\u22121 \"\" instead of \"\u03c6\" = atan2(\"y\", \"x\"), but the first equation needs adjustment when \"x\" \u2264 0. This is because for any real x and y, not both zero, the angles of the vectors (\"x\", \"y\") and (\u2212\"x\", \u2212\"y\") differ by \u03c0 radians, but have the identical value of tan \"\u03c6\" =.\nUse of the formula to define the logarithm of complex numbers.\nNow, taking this derived formula, we can use Euler's formula to define the logarithm of a complex number. To do this, we also use the definition of the logarithm (as the inverse operator of exponentiation):\nformula_25\nand that\nformula_26\nboth valid for any complex numbers a and b. Therefore, one can write:\nformula_27\nfor any \"z\" \u2260 0. Taking the logarithm of both sides shows that\nformula_28\nand in fact, this can be used as the definition for the complex logarithm. The logarithm of a complex number is thus a multi-valued function, because \u03c6 is multi-valued.\nFinally, the other exponential law\nformula_29\nwhich can be seen to hold for all integers k, together with Euler's formula, implies several trigonometric identities, as well as de Moivre's formula.\nRelationship to trigonometry.\nEuler's formula, the definitions of the trigonometric functions and the standard identities for exponentials are sufficient to easily derive most trigonometric identities. It provides a powerful connection between analysis and trigonometry, and provides an interpretation of the sine and cosine functions as weighted sums of the exponential function:\nformula_30\nThe two equations above can be derived by adding or subtracting Euler's formulas:\nformula_31\nand solving for either cosine or sine.\nThese formulas can even serve as the definition of the trigonometric functions for complex arguments x. For example, letting \"x\" = \"iy\", we have:\nformula_32\nIn addition\nformula_33\nComplex exponentials can simplify trigonometry, because they are mathematically easier to manipulate than their sine and cosine components. One technique is simply to convert sines and cosines into equivalent expressions in terms of exponentials sometimes called \"complex sinusoids\". After the manipulations, the simplified result is still real-valued. For example:\nformula_34\nAnother technique is to represent sines and cosines in terms of the real part of a complex expression and perform the manipulations on the complex expression. For example:\nformula_35\nThis formula is used for recursive generation of cos \"nx\" for integer values of n and arbitrary x (in radians).\nConsidering cos \"x\" a parameter in equation above yields recursive formula for Chebyshev polynomials of the first kind.\nTopological interpretation.\nIn the language of topology, Euler's formula states that the imaginary exponential function formula_36 is a (surjective) morphism of topological groups from the real line formula_37 to the unit circle formula_38. In fact, this exhibits formula_37 as a covering space of formula_38. Similarly, Euler's identity says that the kernel of this map is formula_41, where formula_42. These observations may be combined and summarized in the commutative diagram below:\nOther applications.\nIn differential equations, the function \"eix\" is often used to simplify solutions, even if the final answer is a real function involving sine and cosine. The reason for this is that the exponential function is the eigenfunction of the operation of differentiation.\nIn electrical engineering, signal processing, and similar fields, signals that vary periodically over time are often described as a combination of sinusoidal functions (see Fourier analysis), and these are more conveniently expressed as the sum of exponential functions with imaginary exponents, using Euler's formula. Also, phasor analysis of circuits can include Euler's formula to represent the impedance of a capacitor or an inductor.\nIn the four-dimensional space of quaternions, there is a sphere of imaginary units. For any point r on this sphere, and x a real number, Euler's formula applies:\nformula_43\nand the element is called a versor in quaternions. The set of all versors forms a 3-sphere in the 4-space.\nOther special cases.\nThe special cases that evaluate to units illustrate rotation around the complex unit circle:\nThe special case at \"x\" = \"\u03c4\" (where \"\u03c4\" = 2\"\u03c0\", one turn) yields \"ei\u03c4\" = 1 + 0. This is also argued to link five fundamental constants with three basic arithmetic operations, but, unlike Euler's identity, without rearranging the addends from the general case:\nformula_44\nAn interpretation of the simplified form \"ei\u03c4\" = 1 is that rotating by a full turn is an identity function."}
{"id": "9614", "revid": "146242", "url": "https://en.wikipedia.org/wiki?curid=9614", "title": "Eductor-jet pump", "text": ""}
{"id": "9615", "revid": "32589484", "url": "https://en.wikipedia.org/wiki?curid=9615", "title": "\u00c9douard Manet", "text": "French painter (1832\u20131883)\n\u00c9douard Manet (, ; ; 23 January 1832 \u2013 30 April 1883) was a French modernist painter. He was one of the first 19th-century artists to paint modern life, as well as a pivotal figure in the transition from Realism to Impressionism.\nBorn into an upper-class household with strong political connections, Manet rejected the naval career originally envisioned for him; he became engrossed in the world of painting. His early masterworks, \"The Luncheon on the Grass\" (\"Le d\u00e9jeuner sur l'herbe\") and \"Olympia\", premiering in 1863 and '65, respectively, caused great controversy with both critics and the Academy of Fine Arts, but soon were praised by progressive artists as the breakthrough acts to the new style, Impressionism. These works, along with others, are considered watershed paintings that mark the start of modern art. The last 20 years of Manet's life saw him form bonds with other great artists of the time; he developed his own simple and direct style that would be heralded as innovative and serve as a major influence for future painters.\nEarly life.\n\u00c9douard Manet was born in Paris on 23 January 1832, in the ancestral h\u00f4tel particulier (mansion) on the Rue des Petits Augustins (now Rue Bonaparte) to an affluent and well-connected family. He had two younger brothers, Eug\u00e8ne (born 1833) and Gustave (born 1835). His mother, Eug\u00e9nie-Desir\u00e9e Fournier, was the daughter of a diplomat and goddaughter of the Swedish crown prince Charles Bernadotte, from whom the Swedish monarchs are descended. His father, Auguste Manet, was a French judge who expected \u00c9douard to pursue a career in law. His uncle, Edmond Fournier, encouraged him to pursue painting and took young Manet to the Louvre. In 1844, he enrolled at secondary school, the Coll\u00e8ge Rollin, where he boarded until 1848. He showed little academic talent and was generally unhappy at the school. In 1845, at the advice of his uncle, Manet enrolled in a special course of drawing where he met Antonin Proust, future Minister of Fine Arts and his lifelong friend.\nAt his father's suggestion, in 1848 he sailed on a training vessel to Rio de Janeiro. After he twice failed the examination to join the Navy, his father relented to his wishes to pursue an art education. From 1850 to 1856, Manet studied under the academic painter Thomas Couture. Couture encouraged his students to paint contemporary life, though he would eventually be horrified by Manet's choice of lower-class and \"degenerate\" subjects such as \"The Absinthe Drinker\". In his spare time, Manet copied Old Masters such as Diego Vel\u00e1zquez and Titian in the Louvre.\nFrom 1853 to 1856, Manet made brief visits to Germany, Italy, and the Netherlands, during which time he was influenced by the Dutch painter Frans Hals and the Spanish artists Vel\u00e1zquez and Francisco Jos\u00e9 de Goya.\nCareer.\nIn 1856, Manet opened a studio. His style in this period was characterized by loose brush strokes, simplification of details, and the suppression of transitional tones. Adopting the current style of realism initiated by Gustave Courbet, he painted \"The Absinthe Drinker\" (1858\u201359) and other contemporary subjects such as beggars, singers, Romani, people in caf\u00e9s, and bullfights. After his early career, he rarely painted religious, mythological, or historical subjects; religious paintings from 1864 include his \"\" and \"The Dead Christ with Angels\".\nManet had two canvases accepted at the Salon in 1861. A portrait of his mother and father (\"Portrait of Monsieur and Madame Manet\"), the latter of whom at the time was paralysed by a stroke or advanced syphilis, was ill-received by critics. The other, \"The Spanish Singer\", was admired by Th\u00e9ophile Gautier, and placed in a more conspicuous location as a result of its popularity with Salon-goers. Manet's work, which appeared \"slightly slapdash\" when compared with the meticulous style of so many other Salon paintings, intrigued some young artists and brought new business to his studio. According to one contemporary source, \"The Spanish Singer\", painted in a \"strange new fashion[,] caused many painters' eyes to open and their jaws to drop.\"\n\"Music in the Tuileries\".\nIn 1862, Manet exhibited \"Music in the Tuileries\" (probably painted in 1860), one of his first masterpieces. With its portrayal of a crowd of subjects at the Jardin des Tuileries, the painting shows the outdoor leisure of contemporary Paris, which would be a lifelong subject of Manet's. Among the figures in the gardens are the poet Charles Baudelaire, the musician Jacques Offenbach, and others of Manet's family and friends, including a self-portrait of the artist.\n\"Music in the Tuileries\" received substantial critical and public attention, most of it negative. In the words of one Manet biographer, \"it is difficult for us to imagine the kind of fury \"Music in the Tuileries\" provoked when it was exhibited\". By portraying Manet's social circle instead of classical heroes, historical icons, or gods, the painting could be interpreted as challenging the value of those subjects or as an attempt to elevate his contemporaries to the same level. The public, accustomed to the finely detailed brushwork of historical painters such as Ernest Meissonier, thought Manet's thick brushstrokes looked crude and unfinished. Angered by the subject matter and technique, several visitors even threatened to destroy the painting. One of Manet's idols, Eug\u00e8ne Delacroix, was one of the painting's few defenders. Despite the largely negative reaction, the controversy made Manet a well-known name in Paris.\n\"Luncheon on the Grass \"(\"Le D\u00e9jeuner sur l'herbe\").\nAnother major early work is \"The Luncheon on the Grass (Le D\u00e9jeuner sur l'herbe)\", originally \"Le Bain\". The Paris Salon rejected it for exhibition in 1863, but Manet agreed to exhibit it at the Salon des Refus\u00e9s (Salon of the Rejected). This parallel salon was initiated by Emperor Napoleon III as a solution to the public outcry after the official salon's Selection Committee only accepted 2,217 paintings out of more than 5,000 submissions. It gave rejected artists the opportunity to display their paintings if they chose.\nThe painting's juxtaposition of fully dressed men and a nude woman was controversial, as was its abbreviated, sketch-like handling, an innovation that distinguished Manet from Courbet. One critic stated that the brushwork appeared to have been done with a \"floor mop\". However, others such as his friend Antonin Proust celebrated the painting, and novelist \u00c9mile Zola was so affected by the experience of viewing it that he later based the title painting in his novel \"L'\u0152uvre\" (\"The Work of Art\") on \"Le D\u00e9jeuner sur l'herbe\".\nAt the same time, Manet's composition reveals his study of the old masters, as the disposition of the main figures is derived from Marcantonio Raimondi's engraving of the \"Judgement of Paris\" (c.\u20091515) based on a drawing by Raphael. Two additional works cited by scholars as important precedents for are \"Pastoral Concert\" (c.\u20091510) and \"The Tempest\", both of which are attributed variously to Italian Renaissance masters Giorgione or Titian.\n\"Le D\u00e9jeuner\" and James McNeill Whistler's \"\" were the two most discussed works of the Salon des Refus\u00e9s, which itself would become one of the most famous art exhibitions of all time. Following the Salon, Manet became yet more notorious and widely discussed. However, \"Le D\u00e9jeuner sur l'herbe\" and Manet's other paintings still failed to sell, and Manet continued living off of his inheritance from his recently deceased father.\n\"Olympia\".\nAs he had in \"Luncheon on the Grass\", Manet again paraphrased a respected work by a Renaissance artist in the painting \"Olympia\" (1863), a nude portrayed in a style reminiscent of early studio photographs, but whose pose was based on Titian's \"Venus of Urbino\" (1538). The painting is also reminiscent of Francisco Goya's painting \"The Nude Maja\" (1800).\nManet embarked on the canvas after being challenged to give the Salon a nude painting to display. His uniquely frank depiction of a self-assured prostitute was accepted by the Paris Salon in 1865, where it created a scandal. According to Antonin Proust, \"only the precautions taken by the administration prevented the painting being punctured and torn\" by offended viewers. The painting was controversial partly because the nude is wearing some small items of clothing such as an orchid in her hair, a bracelet, a ribbon around her neck, and mule slippers, all of which accentuated her nakedness, sexuality, and comfortable courtesan lifestyle. The orchid, upswept hair, black cat, and bouquet of flowers were all recognized symbols of sexuality at the time. This modern Venus' body is thin, counter to prevailing standards; the painting's lack of idealism rankled viewers. The painting's flatness, inspired by Japanese wood block art, serves to make the nude more human and less voluptuous. A fully dressed black servant is featured, exploiting the then-current theory that black people were hyper-sexed. That she is wearing the clothing of a servant to a courtesan here furthers the sexual tension of the piece.\nOlympia's body as well as her gaze is unabashedly confrontational. She defiantly looks out as her servant offers flowers from one of her male suitors. Although her hand rests on her leg, hiding her pubic area, the reference to traditional female virtue is ironic; a notion of modesty is notoriously absent in this work. A contemporary critic denounced Olympia's \"shamelessly flexed\" left hand, which seemed to him a mockery of the relaxed, shielding hand of Titian's Venus. Likewise, the alert black cat at the foot of the bed strikes a sexually rebellious note in contrast to that of the sleeping dog in Titian's portrayal of the goddess in his \"Venus of Urbino\".\n\"Olympia\" was the subject of caricatures in the popular press, but was championed by the French avant-garde community, and the painting's significance was appreciated by artists such as Gustave Courbet, Paul C\u00e9zanne, Claude Monet, and later Paul Gauguin.\nAs with \"Luncheon on the Grass\", the painting raised the issue of prostitution within contemporary France and the roles of women within society.\nLife and times.\nAfter the death of his father in 1862, Manet married Suzanne Leenhoff in 1863 at a Protestant church. Leenhoff was a Dutch-born piano teacher two years Manet's senior with whom he had been romantically involved for approximately ten years. Leenhoff initially had been employed by Manet's father, Auguste, to teach Manet and his younger brother piano. She also may have been Auguste's mistress. In 1852, Leenhoff gave birth, out of wedlock, to a son, Leon Koella Leenhoff.\nManet painted his wife in \"The Reading\", among other paintings. Her son, Leon Leenhoff, whose father may have been either of the Manets, posed often for Manet. Most famously, he is the subject of the \"Boy Carrying a Sword\" of 1861 (Metropolitan Museum of Art, New York). He also appears as the boy carrying a tray in the background of \"The Balcony\" (1868\u201369).\nManet became friends with the Impressionists Edgar Degas, Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, Paul C\u00e9zanne, and Camille Pissarro through another painter, Berthe Morisot, who was a member of the group and drew him into their activities. They later became widely known as the Batignolles group (Le groupe des Batignolles).\nThe supposed grand-niece of the painter Jean-Honor\u00e9 Fragonard, Morisot had her first painting accepted in the Salon de Paris in 1864, and she continued to show in the salon for the next ten years.\nManet became the friend and colleague of Morisot in 1868. She is credited with convincing Manet to attempt plein air painting, which she had been practicing since she was introduced to it by another friend of hers, Camille Corot. They had a reciprocating relationship and Manet incorporated some of her techniques into his paintings. In 1874, she became his sister-in-law when she married his brother, Eug\u00e8ne. It has been speculated that there was a repressed love between Manet and Morisot, exemplified by the numerous portraits he did of her before she married his brother.\nUnlike the core Impressionist group, Manet maintained that modern artists should seek to exhibit at the Paris Salon rather than abandon it in favor of independent exhibitions. Nevertheless, when Manet was excluded from the International Exhibition of 1867, he set up his own exhibition. His mother worried that he would waste all his inheritance on this project, which was enormously expensive. While the exhibition earned poor reviews from the major critics, it also provided his first contacts with several future Impressionist painters, including Degas.\nAlthough his own work influenced and anticipated the Impressionist style, Manet resisted involvement in Impressionist exhibitions, partly because he did not wish to be seen as the representative of a group identity, and partly because he preferred to exhibit at the Salon. Eva Gonzal\u00e8s, a daughter of the novelist Emmanuel Gonzal\u00e8s, was his only formal student.\nHe was influenced by the Impressionists, especially Monet and Morisot. Their influence is seen in Manet's use of lighter colors: after the early 1870s he made less use of dark backgrounds but retained his distinctive use of black, uncharacteristic of Impressionist painting. He painted many outdoor (plein air) pieces, but always returned to what he considered the serious work of the studio.\nManet enjoyed a close friendship with composer Emmanuel Chabrier, painting two portraits of him; the musician owned 14 of Manet's paintings and dedicated his \"Impromptu\" to Manet's wife.\nOne of Manet's frequent models at the beginning of the 1880s was the \"semimondaine\" M\u00e9ry Laurent, who posed for seven portraits in pastel. Laurent's salons hosted many French (and even American) writers and painters of her time; Manet had connections and influence through such events.\nThroughout his life, although resisted by art critics, Manet could number as his champions \u00c9mile Zola, who supported him publicly in the press, St\u00e9phane Mallarm\u00e9, and Charles Baudelaire, who challenged him to depict life as it was. Manet, in turn, drew or painted each of them.\nCaf\u00e9 scenes.\nManet's paintings of caf\u00e9 scenes are observations of social life in 19th-century Paris. People are depicted drinking beer, listening to music, flirting, reading, or waiting. Many of these paintings were based on sketches executed on the spot. Manet often visited the Brasserie Reichshoffen on boulevard de Rochechourt, upon which he based \"At the Cafe\" in 1878. Several people are at the bar, and one woman confronts the viewer while others wait to be served. Such depictions represent the painted journal of a fl\u00e2neur. These are painted in a style which is loose, referencing Hals and Vel\u00e1zquez, yet they capture the mood and feeling of Parisian night life. They are painted snapshots of bohemianism, urban working people, as well as some of the bourgeoisie.\nIn \"Corner of a Caf\u00e9-Concert\", a man smokes while behind him a waitress serves drinks. In \"The Beer Drinkers\" a woman enjoys her beer in the company of a friend. In \"The Caf\u00e9-Concert\", shown at right, a sophisticated gentleman sits at a bar while a waitress stands resolutely in the background, sipping her drink. In \"The Waitress\", a serving woman pauses for a moment behind a seated customer smoking a pipe, while a ballet dancer, with arms extended as she is about to turn, is on stage in the background.\nManet also sat at the restaurant on the Avenue de Clichy called Pere Lathuille's, which had a garden in addition to the dining area. One of the paintings he produced here was \"Chez le p\u00e8re Lathuille\" (At Pere Lathuille's), in which a man displays an unrequited interest in a woman dining near him.\nIn \"Le Bon Bock\" (1873), a large, cheerful, bearded man sits with a pipe in one hand and a glass of beer in the other, looking straight at the viewer.\nPaintings of social activities.\nManet painted the upper class enjoying more formal social activities. In \"Masked Ball at the Opera\", Manet shows a lively crowd of people enjoying a party. Men stand with top hats and long black suits while talking to women with masks and costumes. He included portraits of his friends in this picture.\nHis 1868 painting \"The Luncheon\" was posed in the dining room of the Manet house.\nManet depicted other popular activities in his work. In \"The Races at Longchamp\", an unusual perspective is employed to underscore the furious energy of racehorses as they rush toward the viewer. In \"Skating\", Manet shows a well dressed woman in the foreground, while others skate behind her. Always there is the sense of active urban life continuing behind the subject, extending outside the frame of the canvas.\nIn \"View of the International Exhibition\", soldiers relax, seated and standing, prosperous couples are talking. There is a gardener, a boy with a dog, a woman on horseback\u2014in short, a sample of the classes and ages of the people of Paris.\nWar.\nManet's response to modern life included works devoted to war, in subjects that may be seen as updated interpretations of the genre of \"history painting\". The first such work was \"The Battle of the Kearsarge and the Alabama\" (1864), a sea skirmish known as the \"Battle of Cherbourg\" from the American Civil War which took place off the French coast, and may have been witnessed by the artist.\nOf interest next was the French intervention in Mexico; from 1867 to 1869 Manet painted three versions of the \"Execution of Emperor Maximilian\", an event which raised concerns regarding French foreign and domestic policy. The several versions of the \"Execution\" are among Manet's largest paintings, which suggests that the theme was one which the painter regarded as most important. Its subject is the execution by Mexican firing squad of a Habsburg emperor who had been installed by Napoleon III. Neither the paintings nor a lithograph of the subject were permitted to be shown in France. As an indictment of formalized slaughter, the paintings look back to Goya, and anticipate Picasso's \"Guernica\".\nDuring the Franco-Prussian War, Manet served in the National Guard to help defend the city during the siege of Paris, along with Degas.&lt;ref name=\"Manet/Degas\"&gt;&lt;/ref&gt; In January 1871, he traveled to Oloron-Sainte-Marie in the Pyrenees. In his absence his friends added his name to the \"F\u00e9d\u00e9ration des artistes\" (see: Courbet) of the Paris Commune. Manet stayed away from Paris, perhaps, until after the \"semaine sanglante\": in a letter to Berthe Morisot at Cherbourg (10 June 1871) he writes, \"We came back to Paris a few days ago...\" (the semaine sanglante ended on 28 May).\nThe prints and drawings collection of the Museum of Fine Arts (Budapest) has a watercolour/gouache by Manet, \"The Barricade\", depicting a summary execution of Communards by Versailles troops based on a lithograph of the execution of Maximilian. A similar piece, \"The Barricade\" (oil on plywood), is held by a private collector.\nOn 18 March 1871, he wrote to his (confederate) friend F\u00e9lix Bracquemond in Paris about his visit to Bordeaux, the provisional seat of the French National Assembly of the Third French Republic where \u00c9mile Zola introduced him to the sites: \"I never imagined that France could be represented by such doddering old fools, not excepting that little twit Thiers...\" If this could be interpreted as support of the Commune, a following letter to Bracquemond (21 March 1871) expressed his idea more clearly: \"Only party hacks and the ambitious, the Henrys of this world following on the heels of the Milli\u00e9res, the grotesque imitators of the Commune of 1793\". He knew the communard Lucien Henry to have been a former painter's model and Milli\u00e8re, an insurance agent. \"What an encouragement all these bloodthirsty caperings are for the arts! But there is at least one consolation in our misfortunes: that we're not politicians and have no desire to be elected as deputies\".\nThe public figure Manet admired most was the republican L\u00e9on Gambetta. In the heat of the \"seize mai\" coup in 1877, Manet opened up his atelier to a republican electoral meeting chaired by Gambetta's friend Eug\u00e8ne Spuller.\nParis.\nManet depicted many scenes of the streets of Paris in his works. \"The Rue Mosnier Decked with Flags\" depicts red, white, and blue pennants covering buildings on either side of the street; another painting of the same title features a one-legged man walking with crutches. Again depicting the same street, but this time in a different context, is \"Rue Mosnier with Pavers\", in which men repair the roadway while people and horses move past.\n\"The Railway\", widely known as \"The Gare Saint-Lazare\", was painted in 1873. The setting is the urban landscape of Paris in the late 19th century. Using his favorite model in his last painting of her, a fellow painter, Victorine Meurent, also the model for \"Olympia\" and the \"Luncheon on the Grass\", sits before an iron fence holding a sleeping puppy and an open book in her lap. Next to her is a little girl with her back to the painter, watching a train pass beneath them.\nInstead of choosing the traditional natural view as background for an outdoor scene, Manet opts for the iron grating which \"boldly stretches across the canvas\". The only evidence of the train is its white cloud of steam. In the distance, modern apartment buildings are seen. This arrangement compresses the foreground into a narrow focus. The traditional convention of deep space is ignored.\nHistorian Isabelle Dervaux has described the reception this painting received when it was first exhibited at the official Paris Salon of 1874: \"Visitors and critics found its subject baffling, its composition incoherent, and its execution sketchy. Caricaturists ridiculed Manet's picture, in which only a few recognized the symbol of modernity that it has become today\". The painting is currently in the National Gallery of Art in Washington, D.C.\nManet painted several boating subjects in 1874. \"Boating\", now in the Metropolitan Museum of Art, exemplifies in its conciseness the lessons Manet learned from Japanese prints, and the abrupt cropping by the frame of the boat and sail adds to the immediacy of the image.\nIn 1875, a book-length French edition of Edgar Allan Poe's \"The Raven\" included lithographs by Manet and translation by Mallarm\u00e9.\nIn 1881, with pressure from his friend Antonin Proust, the French government awarded Manet the L\u00e9gion d'honneur.\nLate works.\nIn his mid-forties Manet's health deteriorated, and he developed severe pain and partial paralysis in his legs. In 1879 he began receiving hydrotherapy treatments at a spa near Meudon intended to improve what he believed was a circulatory problem, but in reality he was suffering from locomotor ataxia, a known side-effect of syphilis. In 1880, he painted a portrait there of the opera singer \u00c9milie Ambre as Carmen. Ambre and her lover Gaston de Beauplan had an estate in Meudon and had organized the first exhibition of Manet's \"The Execution of Emperor Maximilian\" in New York in December 1879.\nIn his last years Manet painted many small-scale still lifes of fruits and vegetables, such as \"A\" \"Bunch of Asparagus\" and \"The Lemon\" (both 1880). He completed his last major work, \"A Bar at the Folies-Berg\u00e8re (Un Bar aux Folies-Berg\u00e8re)\", in 1882, and it hung in the Salon that year. Afterwards, he limited himself to small formats.\nManet's last paintings were of flowers in glass vases. There are 20 such paintings known, with the last one painted in March 1883, barely two months before his death. Quoted in Venice thirteen years later, Manet is credited with stating that an artist can say everything he has to say with \"flowers, fruit, and clouds.\" His last flower paintings are a demonstration of that belief.\nIn 2023, the Metropolitan Museum of Art in New York City exhibited a two-person exhibition of Manet with Degas.\nDeath.\nIn April 1883, his left foot was amputated because of gangrene caused by complications from syphilis and rheumatism. He died eleven days later on 30 April in Paris. He is buried in the Passy Cemetery in the city.\nLegacy.\nManet's public career lasted from 1861, the year of his first participation in the Salon, until his death in 1883. His known extant works, as catalogued in 1975 by Denis Rouart and Daniel Wildenstein, comprise 430 oil paintings, 89 pastels, and more than 400 works on paper.\nAlthough harshly condemned by critics who decried its lack of conventional finish, Manet's work had admirers from the beginning. One was \u00c9mile Zola, who wrote in 1867: \"We are not accustomed to seeing such simple and direct translations of reality. Then, as I said, there is such a surprisingly elegant awkwardness ... it is a truly charming experience to contemplate this luminous and serious painting which interprets nature with a gentle brutality.\"\nThe roughly painted style and photographic lighting in Manet's paintings was seen as specifically modern, and as a challenge to the Renaissance works he copied or used as source material. He rejected the technique he had learned in the studio of Thomas Couture \u2013 in which a painting was constructed using successive layers of paint on a dark-toned ground \u2013 in favor of a direct, \"alla prima\" method using opaque paint on a light ground. Novel at the time, this method made possible the completion of a painting in a single sitting. It was adopted by the Impressionists, and became the prevalent method of painting in oils for generations that followed. Manet's work is considered \"early modern\", partially because of the opaque flatness of his surfaces, the frequent sketch-like passages, and the black outlining of figures, all of which draw attention to the surface of the picture plane and the material quality of paint.\nThe art historian Beatrice Farwell says Manet \"has been universally regarded as the Father of Modernism. With Courbet he was among the first to take serious risks with the public whose favour he sought, the first to make \"alla prima\" painting the standard technique for oil painting and one of the first to take liberties with Renaissance perspective and to offer 'pure painting' as a source of aesthetic pleasure. He was a pioneer, again with Courbet, in the rejection of humanistic and historical subject-matter, and shared with Degas the establishment of modern urban life as acceptable material for high art.\"\nArt market.\nThe late Manet painting, \"Le Printemps\" (1881), sold to the J. Paul Getty Museum for $65.1 million, setting a new auction record for Manet, exceeding its pre-sale estimate of $25\u201335 million at Christie's on 5 November 2014. The previous auction record was held by \"Self-Portrait With Palette\" which sold for $33.2 million at Sotheby's on 22 June 2010.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9616", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=9616", "title": "Evolutionarily stable strategy", "text": "Solution concept in game theory\nAn evolutionarily stable strategy (ESS) is a strategy (or set of strategies) that is \"impermeable\" when adopted by a population in adaptation to a specific environment, that is to say it cannot be displaced by an alternative strategy (or set of strategies) which may be novel or initially rare. Introduced by John Maynard Smith and George R. Price in 1972/3, it is an important concept in behavioural ecology, evolutionary psychology, mathematical game theory and economics, with applications in other fields such as anthropology, philosophy and political science.\nIn game-theoretical terms, an ESS is an equilibrium refinement of the Nash equilibrium, being a Nash equilibrium that is also \"evolutionarily stable.\" Thus, once fixed in a population, natural selection alone is sufficient to prevent alternative (mutant) strategies from replacing it (although this does not preclude the possibility that a better strategy, or set of strategies, will emerge in response to selective pressures resulting from environmental change).\nHistory.\nEvolutionarily stable strategies were defined and introduced by John Maynard Smith and George R. Price in a 1973 \"Nature\" paper. Such was the time taken in peer-reviewing the paper for \"Nature\" that this was preceded by a 1972 essay by Maynard Smith in a book of essays titled \"On Evolution\". The 1972 essay is sometimes cited instead of the 1973 paper, but university libraries are much more likely to have copies of \"Nature\". Papers in \"Nature\" are usually short; in 1974, Maynard Smith published a longer paper in the \"Journal of Theoretical Biology\". Maynard Smith explains further in his 1982 book \"Evolution and the Theory of Games\". Sometimes these are cited instead. In fact, the ESS has become so central to game theory that often no citation is given, as the reader is assumed to be familiar with it.\nMaynard Smith mathematically formalised a verbal argument made by Price, which he read while peer-reviewing Price's paper. When Maynard Smith realized that the somewhat disorganised Price was not ready to revise his article for publication, he offered to add Price as co-author.\nMaynard Smith on p.\u00a0174 of the Postscript in his, 1982 work \"Evolution and the Theory of Games\" argues that the concept of an ESS is itself polyphyletic (i.e. derives from more than one common evolutionary ancestral group or ancestor). In this he not only mentions Price but also list various other sources for the concept. These include the following:\nBut the concept proper was derived from R. H. MacArthur and W. D. Hamilton's work on sex ratios, derived from Fisher's principle, especially Hamilton's (1967) concept of an unbeatable strategy. Maynard Smith was jointly awarded the 1999 Crafoord Prize for his development of the concept of evolutionarily stable strategies and the application of game theory to the evolution of behaviour.\nUses of ESS:\nMotivation.\nThe Nash equilibrium is the traditional solution concept in game theory. It depends on the cognitive abilities of the players. It is assumed that players are aware of the structure of the game and consciously try to predict the moves of their opponents and to maximize their own payoffs. In addition, it is presumed that all the players know this (see common knowledge). These assumptions are then used to explain why players choose Nash equilibrium strategies.\nEvolutionarily stable strategies are motivated entirely differently. Here, it is presumed that the players' strategies are biologically encoded and heritable. Individuals have no control over their strategy and need not be aware of the game. They reproduce and are subject to the forces of natural selection, with the payoffs of the game representing reproductive success (biological fitness). It is imagined that alternative strategies of the game occasionally occur, via a process like mutation. To be an ESS, a strategy must be resistant to these alternatives.\nGiven the radically different motivating assumptions, it may come as a surprise that ESSes and Nash equilibria often coincide. In fact, every ESS corresponds to a Nash equilibrium, but some Nash equilibria are not ESSes.\nNash equilibrium.\nAn ESS is a refined or modified form of a Nash equilibrium. (See the next section for examples which contrast the two.) In a Nash equilibrium, if all players adopt their respective parts, no player can \"benefit\" by switching to any alternative strategy. In a two player game, it is a strategy pair. Let E(\"S\",\"T\") represent the payoff for playing strategy \"S\" against strategy \"T\". The strategy pair (\"S\", \"S\") is a Nash equilibrium in a two player game if and only if for both players, for any strategy \"T\":\nE(\"S\",\"S\") \u2265 E(\"T\",\"S\")\nIn this definition, a strategy \"T\"\u2260\"S\" can be a neutral alternative to \"S\" (scoring equally well, but not better). \nA Nash equilibrium is presumed to be stable even if \"T\" scores equally, on the assumption that there is no long-term incentive for players to adopt \"T\" instead of \"S\". This fact represents the point of departure of the ESS.\nMaynard Smith and Price specify two conditions for a strategy \"S\" to be an ESS. For all \"T\"\u2260\"S\", either\nThe first condition is sometimes called a \"strict\" Nash equilibrium. The second is sometimes called \"Maynard Smith's second condition\". The second condition means that although strategy \"T\" is neutral with respect to the payoff against strategy \"S\", the population of players who continue to play strategy \"S\" has an advantage when playing against \"T\".\nThere is also an alternative, stronger definition of ESS, due to Thomas. This places a different emphasis on the role of the Nash equilibrium concept in the ESS concept. Following the terminology given in the first definition above, this definition requires that for all \"T\"\u2260\"S\"\nIn this formulation, the first condition specifies that the strategy is a Nash equilibrium, and the second specifies that Maynard Smith's second condition is met. Note that the two definitions are not precisely equivalent: for example, each pure strategy in the coordination game below is an ESS by the first definition but not the second.\nIn words, this definition looks like this: The payoff of the first player when both players play strategy S is higher than (or equal to) the payoff of the first player when he changes to another strategy T and the second player keeps his strategy S \"and\" the payoff of the first player when only his opponent changes his strategy to T is higher than his payoff in case that both of players change their strategies to T.\nThis formulation more clearly highlights the role of the Nash equilibrium condition in the ESS. It also allows for a natural definition of related concepts such as a weak ESS or an evolutionarily stable set.\nExamples of differences between Nash equilibria and ESSes.\nIn most simple games, the ESSes and Nash equilibria coincide perfectly. For instance, in the prisoner's dilemma there is only one Nash equilibrium, and its strategy (\"Defect\") is also an ESS.\nSome games may have Nash equilibria that are not ESSes. For example, in harm thy neighbor (whose payoff matrix is shown here) both (\"A\", \"A\") and (\"B\", \"B\") are Nash equilibria, since players cannot do better by switching away from either. However, only \"B\" is an ESS (and a strong Nash). \"A\" is not an ESS, so \"B\" can neutrally invade a population of \"A\" strategists and predominate, because \"B\" scores higher against \"B\" than \"A\" does against \"B\". This dynamic is captured by Maynard Smith's second condition, since E(\"A\", \"A\") = E(\"B\", \"A\"), but it is not the case that E(\"A\",\"B\") &gt; E(\"B\",\"B\").\nNash equilibria with equally scoring alternatives can be ESSes. For example, in the game \"Harm everyone\", \"C\" is an ESS because it satisfies Maynard Smith's second condition. \"D\" strategists may temporarily invade a population of \"C\" strategists by scoring equally well against \"C\", but they pay a price when they begin to play against each other; \"C\" scores better against \"D\" than does \"D\". So here although E(\"C\", \"C\") = E(\"D\", \"C\"), it is also the case that E(\"C\",\"D\") &gt; E(\"D\",\"D\"). As a result, \"C\" is an ESS.\nEven if a game has pure strategy Nash equilibria, it might be that none of those pure strategies are ESS. Consider the Game of chicken. There are two pure strategy Nash equilibria in this game (\"Swerve\", \"Stay\") and (\"Stay\", \"Swerve\"). However, in the absence of an uncorrelated asymmetry, neither \"Swerve\" nor \"Stay\" are ESSes. There is a third Nash equilibrium, a mixed strategy which is an ESS for this game (see Hawk-dove game and Best response for explanation).\nThis last example points to an important difference between Nash equilibria and ESS. Nash equilibria are defined on \"strategy sets\" (a specification of a strategy for each player), while ESS are defined in terms of strategies themselves. The equilibria defined by ESS must always be symmetric, and thus have fewer equilibrium points.\nVs. evolutionarily stable state.\nIn population biology, the two concepts of an \"evolutionarily stable strategy\" (ESS) and an \"evolutionarily stable state\" are closely linked but describe different situations.\nIn an evolutionarily stable \"strategy,\" if all the members of a population adopt it, no mutant strategy can invade. Once virtually all members of the population use this strategy, there is no 'rational' alternative. ESS is part of classical game theory.\nIn an evolutionarily stable \"state,\" a population's genetic composition is restored by selection after a disturbance, if the disturbance is not too large. An evolutionarily stable state is a dynamic property of a population that returns to using a strategy, or mix of strategies, if it is perturbed from that initial state. It is part of population genetics, dynamical system, or evolutionary game theory. This is now called convergent stability.\nB. Thomas (1984) applies the term ESS to an individual strategy which may be mixed, and evolutionarily stable population state to a population mixture of pure strategies which may be formally equivalent to the mixed ESS.\nWhether a population is evolutionarily stable does not relate to its genetic diversity: it can be genetically monomorphic or polymorphic.\nStochastic ESS.\nIn the classic definition of an ESS, no mutant strategy can invade. In finite populations, any mutant could in principle invade, albeit at low probability, implying that no ESS can exist. In an infinite population, an ESS can instead be defined as a strategy which, should it become invaded by a new mutant strategy with probability p, would be able to counterinvade from a single starting individual with probability &gt;p, as illustrated by the evolution of bet-hedging.\nPrisoner's dilemma.\nA common model of altruism and social cooperation is the Prisoner's dilemma. Here a group of players would collectively be better off if they could play \"Cooperate\", but since \"Defect\" fares better each individual player has an incentive to play \"Defect\". One solution to this problem is to introduce the possibility of retaliation by having individuals play the game repeatedly against the same player. In the so-called \"iterated\" Prisoner's dilemma, the same two individuals play the prisoner's dilemma over and over. While the Prisoner's dilemma has only two strategies (\"Cooperate\" and \"Defect\"), the iterated Prisoner's dilemma has a huge number of possible strategies. Since an individual can have different contingency plan for each history and the game may be repeated an indefinite number of times, there may in fact be an infinite number of such contingency plans.\nThree simple contingency plans which have received substantial attention are \"Always Defect\", \"Always Cooperate\", and \"Tit for Tat\". The first two strategies do the same thing regardless of the other player's actions, while the latter responds on the next round by doing what was done to it on the previous round\u2014it responds to \"Cooperate\" with \"Cooperate\" and \"Defect\" with \"Defect\".\nIf the entire population plays \"Tit-for-Tat\" and a mutant arises who plays \"Always Defect\", \"Tit-for-Tat\" will outperform \"Always Defect\". If the population of the mutant becomes too large \u2014 the percentage of the mutant will be kept small. \"Tit for Tat\" is therefore an ESS, \"with respect to only these two strategies\". On the other hand, an island of \"Always Defect\" players will be stable against the invasion of a few \"Tit-for-Tat\" players, but not against a large number of them. If we introduce \"Always Cooperate\", a population of \"Tit-for-Tat\" is no longer an ESS. Since a population of \"Tit-for-Tat\" players always cooperates, the strategy \"Always Cooperate\" behaves identically in this population. As a result, a mutant who plays \"Always Cooperate\" will not be eliminated. However, even though a population of \"Always Cooperate\" and \"Tit-for-Tat\" can coexist, if there is a small percentage of the population that is \"Always Defect\", the selective pressure is against \"Always Cooperate\", and in favour of \"Tit-for-Tat\". This is due to the lower payoffs of cooperating than those of defecting in case the opponent defects.\nThis demonstrates the difficulties in applying the formal definition of an ESS to games with large strategy spaces, and has motivated some to consider alternatives.\nHuman behavior.\nThe fields of sociobiology and evolutionary psychology attempt to explain animal and human behavior and social structures, largely in terms of evolutionarily stable strategies. Sociopathy (chronic antisocial or criminal behavior) may be a result of a combination of two such strategies.\nEvolutionarily stable strategies were originally considered for biological evolution, but they can apply to other contexts. In fact, there are stable states for a large class of adaptive dynamics. As a result, they can be used to explain human behaviours that lack any genetic influences.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9617", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=9617", "title": "Element", "text": "Element or elements may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "9618", "revid": "644038553", "url": "https://en.wikipedia.org/wiki?curid=9618", "title": "Emission line", "text": ""}
{"id": "9619", "revid": "940017", "url": "https://en.wikipedia.org/wiki?curid=9619", "title": "Extremophile", "text": "Organisms capable of living in extreme environments\nAn extremophile (from la\u00a0'extreme' and grc \" \"\" ()\"\u00a0'love') is an organism that is able to live (or in some cases thrive) in extreme environments, i.e., environments with conditions approaching or stretching the limits of what known life can adapt to, such as extreme temperature, pressure, radiation, salinity, or pH level.\nSince the definition of an extreme environment is relative to an arbitrarily defined standard, often an anthropocentric one, these organisms can be considered ecologically dominant in the evolutionary history of the planet. Extremophiles have continued to thrive in the most extreme conditions, making them one of the most abundant lifeforms. The study of extremophiles has expanded human knowledge of the limits of life, and informs speculation about extraterrestrial life. Extremophiles are also of interest because of their potential for bioremediation of environments made hazardous to humans due to pollution or contamination.\nCharacteristics.\nIn the 1980s and 1990s, biologists found that microbial life can survive in extreme environments\u2014niches that are acidic, extraordinarily hot, or with irregular air pressure for example\u2014that would be inhospitable to complex organisms. Some scientists even concluded that life may have begun on Earth in hydrothermal vents far beneath the ocean's surface.\nAccording to astrophysicist Steinn Sigurdsson, \"There are viable bacterial spores that have been found that are 40 million years old on Earth\u2014and we know they're very hardened to radiation.\" Some bacteria were found living in the cold and dark in a lake buried a half-mile deep under the ice in Antarctica, and in the Mariana Trench, the deepest place in Earth's oceans. Expeditions of the International Ocean Discovery Program found microorganisms in sediment that is below seafloor in the Nankai Trough subduction zone. Some microorganisms have been found thriving inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere\u2014they're extremely adaptable to conditions, and survive wherever they are.\" A key to extremophile adaptation is their amino acid composition, affecting their protein folding ability under particular conditions. Studying extreme environments on Earth can help researchers understand the limits of habitability on other worlds.\nTom Gheysens from Ghent University in Belgium and colleagues showed that endospores from a species of \"Bacillus\" bacteria were viable after being heated to temperatures of .\nClassification.\nPolyextremophiles.\nThere are many classes of extremophiles that range all around the globe; each corresponding to the way its environmental niche differs from mesophilic conditions. These classifications are not exclusive. Many extremophiles fall under multiple categories and are classified as polyextremophiles. For example, organisms living inside hot rocks deep under Earth's surface are thermophilic and piezophilic such as \"Thermococcus barophilus\". A polyextremophile living at the summit of a mountain in the Atacama Desert might be a radioresistant xerophile, a psychrophile, and an oligotroph. Polyextremophiles are well known for their ability to tolerate both high and low pH levels. Note that \"tolerant\" or \"resistant\" organisms are not necessarily extremophiles: tolerant or resistant organisms may \"survive despite harsh conditions\" instead of \"thriving in harsh conditions\". For example, the tardigrade (\"Tardigrada\" spp.), despite being highly resistant to many stresses, is not an extremophile properly speaking.\nIn astrobiology.\nAstrobiology is the multidisciplinary field that investigates how life arises, distributes, and evolves in the universe. Astrobiology makes use of physics, chemistry, astronomy, solar physics, biology, molecular biology, ecology, planetary science, geography, and geology to investigate the possibility of life on other worlds and recognize biospheres that might be different from that on Earth. Astrobiologists are interested in extremophiles, as it allows them to map what is known about the limits of life on Earth to potential extraterrestrial environments For example, analogous deserts of Antarctica are exposed to harmful UV radiation, low temperature, high salt concentration and low mineral concentration. These conditions are similar to those on Mars. Therefore, finding viable microbes in the subsurface of Antarctica suggests that there may be microbes surviving in endolithic communities and living under the Martian surface. Research indicates it is unlikely that Martian microbes exist on the surface or at shallow depths, but may be found at subsurface depths of around 100 meters.\nRecent research carried out on extremophiles in Japan involved a variety of bacteria including \"Escherichia coli\" and \"Paracoccus denitrificans\" being subject to conditions of extreme gravity. The bacteria were cultivated while being rotated in an ultracentrifuge at high speeds corresponding to 403,627 g (i.e. 403,627 times the gravity experienced on Earth). \"P.\u00a0denitrificans\" was one of the bacteria which displayed not only survival but also robust cellular growth under these conditions of hyperacceleration which are usually found only in cosmic environments, such as on very massive stars or in the shock waves of supernovas. Analysis showed that the small size of prokaryotic cells is essential for successful growth under hypergravity. The research has implications on the feasibility of panspermia.\nOn 26 April 2012, scientists reported that lichen survived and showed remarkable results on the adaptation capacity of photosynthetic activity within the simulation time of 34 days under some conditions similar to those on Mars in the Mars Simulation Laboratory (MSL) maintained by the German Aerospace Center (DLR).\nOn 29 April 2013, scientists at Rensselaer Polytechnic Institute, funded by NASA, reported that, during spaceflight on the International Space Station, microbes seem to adapt to the space environment in ways \"not observed on Earth\" and in ways that \"can lead to increases in growth and virulence\".\nOn 19 May 2014, scientists announced that some microbes, like \"Tersicoccus phoenicis\", may be resistant to methods usually used in spacecraft assembly clean rooms, giving rise to speculation that such microbes could have withstood space travel and are present on the \"Curiosity\" rover now on the planet Mars.\nOn 20 August 2014, scientists confirmed the existence of microorganisms living half a mile below the ice of Antarctica.\nIn September 2015, scientists from https:// of Italy reported that \"S.\u00a0soflataricus\" survived under Martian radiation at a wavelength that was considered lethal to most bacteria. This discovery is significant because it indicates that not only bacterial spores, but also growing cells can resist to strong UV radiation.\nIn June 2016, scientists from Brigham Young University reported that endospores of \"Bacillus subtilis\" were able to survive high speed impacts up to 299\u00b128\u00a0m/s, extreme shock, and extreme deceleration. They pointed out that this feature might allow endospores to survive and to be transferred between planets by traveling within meteorites or by experiencing atmosphere disruption. Moreover, they suggested that the landing of spacecraft may also result in interplanetary spore transfer, given that spores can survive high-velocity impact while ejected from the spacecraft onto the planet surface. This is the first study which reported that bacteria can survive in such high-velocity impact. However, the lethal impact speed is unknown, and further experiments should be done by introducing higher-velocity impact to bacterial endospores.\nIn August 2020 scientists reported that bacteria that feed on air discovered 2017 in Antarctica are likely not limited to Antarctica after discovering the two genes previously linked to their \"atmospheric chemosynthesis\" in soil of two other similar cold desert sites, which provides further information on this carbon sink and further strengthens the extremophile evidence that supports the potential existence of microbial life on alien planets.\nThe same month, scientists reported that bacteria from Earth, particularly \"Deinococcus radiodurans\", were found to survive for three years in outer space, based on studies on the International Space Station. These findings support the notion of panspermia.\nBioremediation.\nExtremophiles can also be useful players in the bioremediation of contaminated sites as some species are capable of biodegradation under conditions too extreme for classic bioremediation candidate species. Anthropogenic activity causes the release of pollutants that may potentially settle in extreme environments as is the case with tailings and sediment released from deep-sea mining activity. While most bacteria would be crushed by the pressure in these environments, piezophiles can tolerate these depths and can metabolize pollutants of concern if they possess bioremediation potential.\nHydrocarbons.\nThere are multiple potential destinations for hydrocarbons after an oil spill has settled and currents routinely deposit them in extreme environments. Methane bubbles resulting from the Deepwater Horizon oil spill were found 1.1 kilometers below water surface level and at concentrations as high as 183 \"\u03bc\"mol per kilogram. The combination of low temperatures and high pressures in this environment result in low microbial activity. However, bacteria that are present including species of \"Pseudomonas\", \"Aeromonas\" and \"Vibrio\" were found to be capable of bioremediation, albeit at a tenth of the speed they would perform at sea level pressure. Polycyclic aromatic hydrocarbons increase in solubility and bioavailability with increasing temperature. Thermophilic \"Thermus\" and \"Bacillus\" species have demonstrated higher gene expression for the alkane mono-oxygenase \"alkB\" at temperatures exceeding . The expression of this gene is a crucial precursor to the bioremediation process. Fungi that have been genetically modified with cold-adapted enzymes to tolerate differing pH levels and temperatures have been shown to be effective at remediating hydrocarbon contamination in freezing conditions in the Antarctic.\nMetals.\n\"Acidithiobacillus ferrooxidans\" has been shown to be effective in remediating mercury in acidic soil due to its \"merA\" gene making it mercury resistant. Industrial effluent contain high levels of metals that can be detrimental to both human and ecosystem health. In extreme heat environments the extremophile \"Geobacillus thermodenitrificans\" has been shown to effectively manage the concentration of these metals within twelve hours of introduction. Some acidophilic microorganisms are effective at metal remediation in acidic environments due to proteins found in their periplasm, not present in any mesophilic organisms, allowing them to protect themselves from high proton concentrations. Rice paddies are highly oxidative environments that can produce high levels of lead or cadmium. \"Deinococcus radiodurans\" are resistant to the harsh conditions of the environment and are therefore candidate species for limiting the extent of contamination of these metals.\nSome bacteria are known to also use rare earth elements on their biological processes. For example, \"Methylacidiphilum fumariolicum\", \"Methylorubrum extorquens,\" and \"Methylobacterium radiotolerans\" are known to be able to use lanthanides as cofactors to increase their methanol dehydrogenase activity.\nAcid mine drainage.\nAcid mine drainage is a major environmental concern associated with many metal mines. This is due to the fact that this highly acidic water can mix with groundwater, streams, and lakes. The drainage turns the pH in these water sources from a more neutral pH to a pH lower than 4. This is close to the acidity levels of battery acid or stomach acid. Exposure to the polluted water can greatly affect the health of plants, humans, and animals. However, a productive method of remediation is to introduce the extremophile, \"Thiobacillus\" \"ferrooxidans\". This extremophile is useful for its bioleaching property. It helps to break down minerals in the waste water created by the mine. By breaking down the minerals \"Thiobacillus ferrooxidans\" start to help neutralize the acidity of the waste water. This is a way to reduce the environmental impact and help remediate the damage caused by the acid mine drainage leaks.\nOil-based, hazardous pollutants in Arctic regions.\nPsychrophilic microbes metabolize hydrocarbons which assists in the remediation of hazardous, oil-based pollutants in the Arctic and Antarctic regions. These specific microbes are used in this region due to their ability to perform their functions at extremely cold temperatures.\nRadioactive materials.\nAny bacteria capable of inhabiting radioactive mediums can be classified as an extremophile. Radioresistant organisms are therefore critical in the bioremediation of radionuclides. Uranium is particularly challenging to contain when released into an environment and very harmful to both human and ecosystem health. The NANOBINDERS project is equipping bacteria that can survive in uranium rich environments with gene sequences that enable proteins to bind to uranium in mining effluent, making it more convenient to collect and dispose of. Some examples are \"Shewanella putrefaciens\", \"Geobacter metallireducens\" and some strains of \"Burkholderia fungorum.\"\nRadiotrophic fungi, which use radiation as an energy source, have been found inside and around the Chernobyl Nuclear Power Plant.\nRadioresistance has also been observed in certain species of macroscopic lifeforms. The lethal dose required to kill up to 50% of a tortoise population is 40,000 roentgens, compared to only 800 roentgens needed to kill 50% of a human population. In experiments exposing lepidopteran insects to gamma radiation, significant DNA damage was detected only at 20\u00a0Gy and higher doses, in contrast with human cells that showed similar damage at only 2\u00a0Gy.\nExamples and recent findings.\nNew sub-types of extremophiles are identified frequently and the sub-category list for extremophiles is always growing. For example, microbial life lives in the liquid asphalt lake, Pitch Lake. Research indicates that extremophiles inhabit the asphalt lake in populations ranging between 106 and 107 cells/gram. Likewise, until recently, boron tolerance was unknown, but a strong borophile was discovered in bacteria. With the recent isolation of \"Bacillus boroniphilus\", borophiles came into discussion. Studying these borophiles may help illuminate the mechanisms of both boron toxicity and boron deficiency.\nIn July 2019, a scientific study of Kidd Mine in Canada discovered sulfur-breathing organisms which live below the surface, and which breathe sulfur in order to survive. These organisms are also remarkable due to eating rocks such as pyrite as their regular food source.\nBiotechnology.\nThe thermoalkaliphilic catalase, which initiates the breakdown of hydrogen peroxide into oxygen and water, was isolated from an organism, \"Thermus brockianus\", found in Yellowstone National Park by Idaho National Laboratory researchers. The catalase operates over a temperature range from 30\u00a0\u00b0C to over 94\u00a0\u00b0C and a pH range from 6\u201310. This catalase is extremely stable compared to other catalases at high temperatures and pH. In a comparative study, the \"T. brockianus\" catalase exhibited a half-life of 15 days at 80\u00a0\u00b0C and pH 10 while a catalase derived from \"Aspergillus niger\" had a half-life of 15 seconds under the same conditions. The catalase will have applications for removal of hydrogen peroxide in industrial processes such as pulp and paper bleaching, textile bleaching, food pasteurization, and surface decontamination of food packaging.\nDNA modifying enzymes such as \"Taq\" DNA polymerase and some \"Bacillus\" enzymes used in clinical diagnostics and starch liquefaction are produced commercially by several biotechnology companies.\nDNA transfer.\nOver 65 prokaryotic species are known to be naturally competent for genetic transformation, the ability to transfer DNA from one cell to another cell followed by integration of the donor DNA into the recipient cell's chromosome. Several extremophiles are able to carry out species-specific DNA transfer, as described below. However, it is not yet clear how common such a capability is among extremophiles.\nThe bacterium \"Deinococcus radiodurans\" is one of the most radioresistant organisms known. This bacterium can also survive cold, dehydration, vacuum and acid and is thus known as a polyextremophile. \"D.\u00a0radiodurans\" is competent to perform genetic transformation. Recipient cells are able to repair DNA damage in donor transforming DNA that had been UV irradiated as efficiently as they repair cellular DNA when the cells themselves are irradiated. The extreme thermophilic bacterium \"Thermus thermophilus\" and other related \"Thermus\" species are also capable of genetic transformation.\n\"Halobacterium volcanii\", an extreme halophilic (saline tolerant) archaeon, is capable of natural genetic transformation. Cytoplasmic bridges are formed between cells that appear to be used for DNA transfer from one cell to another in either direction.\n\"Sulfolobus solfataricus\" and \"Sulfolobus acidocaldarius\" are hyperthermophilic archaea. Exposure of these organisms to the DNA damaging agents UV irradiation, bleomycin or mitomycin C induces species-specific cellular aggregation. UV-induced cellular aggregation of \"S. acidocaldarius\" mediates chromosomal marker exchange with high frequency. Recombination rates exceed those of uninduced cultures by up to three orders of magnitude. Frols et al. and Ajon et al. hypothesized that cellular aggregation enhances species-specific DNA transfer between \"Sulfolobus\" cells in order to repair damaged DNA by means of homologous recombination. Van Wolferen et al. noted that this DNA exchange process may be crucial under DNA damaging conditions such as high temperatures. It has also been suggested that DNA transfer in \"Sulfolobus\" may be an early form of sexual interaction similar to the more well-studied bacterial transformation systems that involve species-specific DNA transfer leading to homologous recombinational repair of DNA damage (and see Transformation (genetics)).\nExtracellular membrane vesicles (MVs) might be involved in DNA transfer between different hyperthermophilic archaeal species. It has been shown that both plasmids and viral genomes can be transferred via MVs. Notably, a horizontal plasmid transfer has been documented between hyperthermophilic \"Thermococcus\" and \"Methanocaldococcus\" species, respectively belonging to the orders \"Thermococcales\" and \"Methanococcales\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9620", "revid": "34204912", "url": "https://en.wikipedia.org/wiki?curid=9620", "title": "Education reform", "text": "Changing how people are taught, especially on a mass scale\nEducation reform is the goal of changing public education. The meaning and educational methods have changed through debates over what content or experiences result in an educated individual or an educated society. Historically, the motivations for reform have not reflected the current needs of society. A consistent theme of reform includes the idea that large systematic changes to educational standards will produce social returns in citizens' health, wealth, and well-being.\nAs part of the broader social and political processes, the term education reform refers to the chronology of significant, systematic revisions made to amend the educational legislation, standards, methodology, and policy affecting a nation's public school system to reflect the needs and values of contemporary society. In the 18th century, classical education instruction from an in-home personal tutor, hired at the family's expense, was primarily a privilege for children from wealthy families. Innovations such as encyclopedias, public libraries, and grammar schools all aimed to relieve some of the financial burden associated with the expenses of the classical education model. Motivations during the Victorian era emphasized the importance of self-improvement. Victorian education focused on teaching commercially valuable topics, such as modern languages and mathematics, rather than classical liberal arts subjects, such as Latin, art, and history.\nMotivations for education reformists like Horace Mann and his proponents focused on making schooling more accessible and developing a robust state-supported common school system. John Dewey, an early 20th-century reformer, focused on improving society by advocating for a scientific, pragmatic, or democratic principle-based curriculum. Whereas Maria Montessori incorporated humanistic motivations to \"meet the needs of the child\". In historic Prussia, a motivation to foster national unity led to formal education concentrated on teaching national language literacy to young children, resulting in Kindergarten.\nThe history of educational pedagogy in the United States has ranged from teaching literacy and proficiency of religious doctrine to establishing cultural literacy, assimilating immigrants into a democratic society, producing a skilled labor force for the industrialized workplace, preparing students for careers, and competing in a global marketplace. Educational inequality is also a motivation for education reform, seeking to address problems of a community.\nMotivations for education reform.\nEducation reform, in general, implies a continual effort to modify and improve the institution of education. Over time, as the needs and values of society change, attitudes towards public education also change. As a social institution, education plays an integral role in the process of socialization. \"Socialization is broadly composed of distinct inter- and intra-generational processes. Both involve the harmonization of an individual's attitudes and behaviors with that of their socio-cultural milieu.\" Educational matrices aim to reinforce those socially acceptable informal and formal norms, values, and beliefs that individuals need to learn in order to be accepted as good, functioning, and productive members of their society. Education reform is the process of constantly renegotiating and restructuring the educational standards to reflect the ever-evolving contemporary ideals of social, economic, and political culture. Reforms can be based on bringing education into alignment with a society's core values. Reforms that attempt to change a society's core values can connect alternative education initiatives with a network of other alternative institutions.\nEducation reform has been pursued for a variety of specific reasons, but generally most reforms aim at redressing some societal ills, such as poverty-, gender-, or class-based inequities, or perceived ineffectiveness. Current education trends in the United States represent multiple achievement gaps across ethnicities, income levels, and geographies. As McKinsey and Company reported in a 2009 analysis, \"These educational gaps impose on the United States the economic equivalent of a permanent national recession.\" Reforms are usually proposed by thinkers who aim to redress societal ills or institute societal changes, most often through a change in the education of the members of a class of people\u2014the preparation of a ruling class to rule or a working class to work, the social hygiene of a lower or immigrant class, the preparation of citizens in a democracy or republic, etc. The idea that all children should be provided with a high level of education is a relatively recent idea, and has arisen largely in the context of Western democracy in the 20th century.\nThe \"beliefs\" of school districts are optimistic that quite literally \"all students will succeed\", which in the context of high school graduation examination in the United States, all students in all groups, regardless of heritage or income will pass tests typically fall beyond the ability of all but the top 20 to 30 percent of students. The claims clearly renounce historical research that shows that all ethnic and income groups score differently on all standardized tests and standards based assessments and that students will achieve on a bell curve. Instead, education officials across the world believe that by setting clear, achievable, higher standards, aligning the curriculum, and assessing outcomes, learning can be increased for all students, and more students can succeed than the 50 percent who are defined to be above or below grade level by norm referenced standards.\nStates have tried to use state schools to increase state power, especially to make better soldiers and workers, and increase nationalism. This strategy was first adopted to unify related linguistic groups in Europe, including France, Germany and Italy. Exact mechanisms are unclear, but it often fails in areas where populations are culturally segregated, as when the U.S. Indian school service failed to suppress Lakota and Navaho, or when a culture has widely respected autonomous cultural institutions, as when the Spanish failed to suppress Catalan.\nAdvocates of democracy suggest that improving public education leads to better government. They reason that democracies rely on citizens who can think clearly and make informed decisions, and education helps people develop those abilities.\nPolitically motivated educational reforms of the democratic type are recorded as far back as Plato in \"The Republic\". In the United States, this lineage of democratic education reform was continued by Thomas Jefferson, who advocated ambitious reforms partly along Platonic lines for public schooling in Virginia.\nAnother motivation for reform is the desire to address socio-economic problems, which many people see as having significant roots in lack of education. Starting in the 20th century, people have attempted to argue that small improvements in education can have large returns in such areas as health, wealth and well-being. For example, in Kerala, India in the 1950s, increases in women's health were correlated with increases in female literacy rates. In Iran, increased primary education was correlated with increased farming efficiencies and income. In both cases some researchers have concluded these correlations as representing an underlying causal relationship: education causes socio-economic benefits. In the case of Iran, researchers concluded that the improvements were due to farmers gaining reliable access to national crop prices and scientific farming information.\nHistory.\nClassical education.\nAs taught from the 18th to the 19th century, Western classical education curriculums focused on concrete details like \"Who?\", \"What?\", \"When?\", \"Where?\". Unless carefully taught, large group instruction naturally neglects asking the theoretical \"Why?\" and \"Which?\" questions that can be discussed in smaller groups.\nClassical education in this period also did not teach local (vernacular) languages and culture. Instead, it taught high-status ancient languages (Greek and Latin) and their cultures. This produced odd social effects in which an intellectual class might be more loyal to ancient cultures and institutions than to their native vernacular languages and their actual governing authorities.\n18th century.\nChild-study.\nJean-Jacques Rousseau, father of the Child Study Movement, centered the child as an object of study.\nIn \"\", Rousseau's principal work on education lays out an educational program for a hypothetical newborn's education through adulthood.\nRousseau provided a dual critique of the educational vision outlined in Plato's Republic and that of his society in contemporary Europe. He regarded the educational methods contributing to the child's development; he held that a person could either be a man or a citizen. While Plato's plan could have brought the latter at the expense of the former, contemporary education failed at both tasks. He advocated a radical withdrawal of the child from society and an educational process that utilized the child's natural potential and curiosity, teaching the child by confronting them with simulated real-life obstacles and conditioning the child through experience rather intellectual instruction.\nRousseau ideas were rarely implemented directly, but influenced later thinkers, particularly Johann Heinrich Pestalozzi and Friedrich Wilhelm August Fr\u00f6bel, the inventor of the kindergarten.\nNational identity.\nEuropean and Asian nations regard education as essential to maintaining national, cultural, and linguistic unity. In the late 18th century (~1779), Prussia instituted primary school reforms expressly to teach a unified version of the national language, \"Hochdeutsch\".\nOne significant reform was kindergarten whose purpose was to have the children participate in supervised activities taught by instructors who spoke the national language. The concept embraced the idea that children absorb new language skills more easily and quickly when they are young\nThe current model of kindergarten is reflective of the Prussian model.\nIn other countries, such as the Soviet Union, France, Spain, and Germany, the Prussian model has dramatically improved reading and math test scores for linguistic minorities.\n19th century England.\nIn the 19th century, before the advent of government-funded public schools, Protestant organizations established Charity Schools to educate the lower social classes. The Roman Catholic Church and governments later adopted the model.\nDesigned to be inexpensive, Charity schools operated on minimal budgets and strived to serve as many needy children as possible. This led to the development of grammar schools, which primarily focused on teaching literacy, grammar, and bookkeeping skills so that the students could use books as an inexpensive resource to continue their education. \"Grammar\" was the first third of the then-prevalent system of classical education.\nEducators Joseph Lancaster and Andrew Bell developed the monitorial system, also known as \"mutual instruction\" or the \"Bell\u2013Lancaster method\". Their contemporary, educationalist and writer Elizabeth Hamilton, suggested that in some important aspects the method had been \"anticipated\" by the Belfast schoolmaster David Manson. In the 1760s Manson had developed a peer-teaching and monitoring system within the context of what he called a \"play school\" that dispensed with \"the discipline of the rod\". (More radically, Manson proposed the \"liberty of each [child] to take the quantity [of lessons] agreeable to his inclination\").\nLancaster, an impoverished Quaker during the early 19th century in London and Bell at the Madras School of India developed this model independent of one another. However, by design, their model utilizes more advanced students as a resource to teach the less advanced students; achieving student-teacher ratios as small as 1:2 and educating more than 1000 students per adult. The lack of adult supervision at the Lancaster school resulted in the older children acting as disciplinary monitors and taskmasters.\nTo provide order and promote discipline the school implemented a unique internal economic system, inventing a currency called a \"Scrip.\" Although the currency was worthless in the outside world, it was created at a fixed exchange rate from a student's tuition and student's could use scrip to buy food, school supplies, books, and other items from the school store. Students could earn scrip through tutoring. To promote discipline, the school adopted a work-study model. Every job of the school was bid-for by students, with the largest bid winning. However, any student tutor could auction positions in his or her classes to earn scrip. The bids for student jobs paid for the adult supervision.\nLancaster promoted his system in a piece called Improvements in Education that spread widely throughout the English-speaking world. Lancaster schools provided a grammar-school education with fully developed internal economies for a cost per student near $40 per year in 2010 U.S. dollars. To reduce cost and motivated to save up scrip, Lancaster students rented individual pages of textbooks from the school library instead of purchasing the textbook. Student's would read aloud their pages to groups. Students commonly exchanged tutoring and paid for items and services with receipts from \"down tutoring\".\nThe schools did not teach submission to orthodox Christian beliefs or government authorities. As a result, most English-speaking countries developed mandatory publicly paid education explicitly to keep public education in \"responsible\" hands. These elites said that Lancaster schools might become dishonest, provide poor education, and were not accountable to established authorities. Lancaster's supporters responded that any child could cheat given the opportunity, and that the government was not paying for the education and thus deserved no say in their composition.\nThough motivated by charity, Lancaster claimed in his pamphlets to be surprised to find that he lived well on the income of his school, even while the low costs made it available to the most impoverished street children. Ironically, Lancaster lived on the charity of friends in his later life.\nModern reformist.\nAlthough educational reform occurred on a local level at various points throughout history, the modern notion of education reform is tied with the spread of compulsory education. Economic growth and the spread of democracy raised the value of education and increased the importance of ensuring that all children and adults have access to free, high-quality, effective education. Modern education reforms are increasingly driven by a growing understanding of what works in education and how to go about successfully improving teaching and learning in schools. However, in some cases, the reformers' goals of \"high-quality education\" has meant \"high-intensity education\", with a narrow emphasis on teaching individual, test-friendly subskills quickly, regardless of long-term outcomes, developmental appropriateness, or broader educational goals.\nHorace Mann.\nIn the United States, Horace Mann (1796 \u2013 1859) of Massachusetts used his political base and role as Secretary of the Massachusetts State Board of Education to promote public education in his home state and nationwide. Advocating a substantial public investment be made in education, Mann and his proponents developed a strong system of state supported common schools..\nHis crusading style attracted wide middle class support. Historian Ellwood P. Cubberley asserts:\n No one did more than he to establish in the minds of the American people the conception that education should be universal, non-sectarian, free, and that its aims should be social efficiency, civic virtue, and character, rather than mere learning or the advancement of sectarian ends.\nIn 1852, Massachusetts passed a law making education mandatory. This model of free, accessible education spread throughout the country and in 1917 Mississippi was the final state to adopt the law.\nJohn Dewey.\nJohn Dewey, a philosopher and educator based in Chicago and New York, helped conceptualize the role of American and international education during the first four decades of the 20th century. An important member of the American Pragmatist movement, he carried the subordination of knowledge to action into the educational world by arguing for experiential education that would enable children to learn theory and practice simultaneously; a well-known example is the practice of teaching elementary physics and biology to students while preparing a meal. He was a harsh critic of \"dead\" knowledge disconnected from practical human life.\nDewey criticized the rigidity and volume of humanistic education, and the emotional idealizations of education based on the child-study movement that had been inspired by Rousseau and those who followed him. Dewey understood that children are naturally active and curious and learn by doing. Dewey's understanding of logic is presented in his work \"Logic, the Theory of Inquiry\" (1938). His educational philosophies were presented in \"My Pedagogic Creed\", \"The School and Society\", \"The Child and Curriculum\", and \"Democracy and Education\" (1916). Bertrand Russell criticized Dewey's conception of logic, saying \"What he calls \"logic\" does not seem to me to be part of logic at all; I should call it part of psychology.\"\nDewey left the University of Chicago in 1904 over issues relating to the Dewey School.\nDewey's influence began to decline in the time after the Second World War and particularly in the Cold War era, as more conservative educational policies came to the fore.\nAdministrative progressives.\nThe form of educational progressivism which was most successful in having its policies implemented has been dubbed \"administrative progressivism\" by historians. This began to be implemented in the early 20th century. While influenced particularly in its rhetoric by Dewey and even more by his popularizers, administrative progressivism was in its practice much more influenced by the Industrial Revolution and the concept economies of scale.\nThe administrative progressives are responsible for many features of modern American education, especially American high schools: counseling programs, the move from many small local high schools to large centralized high schools, curricular differentiation in the form of electives and tracking, curricular, professional, and other forms of standardization, and an increase in state and federal regulation and bureaucracy, with a corresponding reduction of local control at the school board level. (Cf. \"State, federal, and local control of education in the United States\", below) \nPublic school reform in the United States.\nIn the United States, public education is characterized as \"any federally funded primary or secondary school, administered to some extent by the government, and charged with educating all citizens. Although there is typically a cost to attend some public higher education institutions, they are still considered part of public education.\"\nColonial America.\nIn what would become the United States, the first public school was established in Boston, Massachusetts, on April 23, 1635. Puritan schoolmaster Philemon Pormont led instruction at the Boston Latin School. During this time, post-secondary education was a commonly utilized tool to distinguish one's social class and social status. Access to education was the \"privilege of white, upper-class, Christian male children\" in preparation for university education in ministry.\nIn colonial America, to maintain Puritan religious traditions, formal and informal education instruction focused on teaching literacy. All colonists needed to understand the written language on some fundamental level in order to read the Bible and the colony's written secular laws. Religious leaders recognized that each person should be \"educated enough to meet the individual needs of their station in life and social harmony.\" The first compulsory education laws were passed in Massachusetts between 1642 and 1648 when religious leaders noticed not all parents were providing their children with \"proper\" education. These laws stated that all towns with 50 or more families were obligated to hire a schoolmaster to teach children reading, writing, and basic arithmetic.\"In 1642 the General Court passed a law that required heads of households to teach all their dependents \u2014 apprentices and servants as well as their own children \u2014 to read English or face a fine. Parents could provide the instruction themselves or hire someone else to do it. Selectmen were to keep 'a vigilant eye over their brethren and neighbors,' young people whose education was neglected could be removed from their parents or masters.\"The 1647 law eventually led to establishing publicly funded district schools in all Massachusetts towns, although, despite the threat of fines, compliance and quality of public schools were less than satisfactory.\"Many towns were 'shamefully neglectful' of children's education. In 1718 '...by sad experience, it is found that many towns that not only are obliged by law, but are very able to support a grammar school, yet choose rather to incur and pay the fine or penalty than maintain a grammar school.\"When John Adams drafted the Massachusetts Constitution in 1780, he included provisions for a comprehensive education law that guaranteed public education to \"all\" citizens. However, access to formal education in secondary schools and colleges was reserved for free, white males. During the 17th and 18th centuries, females received little or no formal education except for home learning or attending Dame Schools. Likewise, many educational institutions maintained a policy of refusing to admit Black applicants. The Virginia Code of 1819 outlawed teaching enslaved people to read or write.\nPost-revolution.\nSoon after the American Revolution, early leaders, like Thomas Jefferson and John Adams, proposed the creation of a more \"formal and unified system of publicly funded schools\" to satiate the need to \"build and maintain commerce, agriculture and shipping interests\". Their concept of free public education was not well received and did not begin to take hold on until the 1830s. However, in 1790, evolving socio-cultural ideals in the Commonwealth of Pennsylvania led to the first significant and systematic reform in education legislation that mandated economic conditions would not inhibit a child's access to education:\"https://\"\nReconstruction and the American Industrial Revolution.\nDuring Reconstruction, from 1865 to 1877, African Americans worked to encourage public education in the South. With the U.S. Supreme Court decision in Plessy v. Ferguson, which held that \"segregated public facilities were constitutional so long as the black and white facilities were equal to each other\", this meant that African American children were legally allowed to attend public schools, although these schools were still segregated based on race. However, by the mid-twentieth century, civil rights groups would challenge racial segregation.\nDuring the second half of the nineteenth century (1870 and 1914), America's Industrial Revolution refocused the nation's attention on the need for a universally accessible public school system. Inventions, innovations, and improved production methods were critical to the continued growth of American manufacturing. To compete in the global economy, an overwhelming demand for literate workers that possessed practical training emerged. Citizens argued, \"educating children of the poor and middle classes would prepare them to obtain good jobs, thereby strengthen the nation's economic position.\" Institutions became an essential tool in yielding ideal factory workers with sought-after attitudes and desired traits such as dependability, obedience, and punctuality. Vocationally oriented schools offered practical subjects like shop classes for students who were not planning to attend college for financial or other reasons. Not until the latter part of the 19th century did public elementary schools become available throughout the country. Although, it would be longer for children of color, girls, and children with special needs to attain access free public education.\nMid 20th and early 21st century (United states).\nCivil rights reform.\nSystemic bias remained a formidable barrier. From the 1950s to the 1970s, many of the proposed and implemented reforms in U.S. education stemmed from the civil rights movement and related trends; examples include ending racial segregation, and busing for the purpose of desegregation, affirmative action, and banning of school prayer.\nIn the early 1950s, most U.S. public schools operated under a legally sanctioned racial segregation system. Civil Rights reform movements sought to address the biases that ensure unequal distribution of academic resources such as school funding, qualified and experienced teachers, and learning materials to those socially excluded communities. In the early 1950s, the NAACP lawyers brought class-action lawsuits on behalf of black schoolchildren and their families in Kansas, South Carolina, Virginia, and Delaware, petitioning court orders to compel school districts to let black students attend white public schools. Finally, in 1954, the U.S. Supreme Court rejected that framework with Brown v. Board of Education and declared state-sponsored segregation of public schools unconstitutional.\nIn 1964, Title VI of the Civil Rights Act \"prohibited discrimination on the basis of race, color, and national origin in programs and activities receiving federal financial assistance.\" Educational institutions could now utilize public funds to implement in-service training programs to assist teachers and administrators in establishing desegregation plans.\nIn 1965, the Higher Education Act (HEA) authorizes federal aid for postsecondary students.\nThe Elementary and Secondary Education Act of 1965 (ESEA) represents the federal government's commitment to providing equal access to quality education; including those children from low-income families, limited English proficiency, and other minority groups. This legislation had positive retroactive implications for Historically Black Colleges and Universities, more commonly known as HBCUs.\"The Higher Education Act of 1965, as amended, defines an HBCU as: \"\u2026any historically black college or university that was established prior to 1964, whose principal mission was, and is, the education of black Americans, and that is accredited by a nationally recognized accrediting agency or association determined by the Secretary [of Education] to be a reliable authority as to the quality of training offered or is, according to such an agency or association, making reasonable progress toward accreditation.\"Known as the Bilingual Education Act, Title VII of ESEA, offered federal aid to school districts to provide bilingual instruction for students with limited English speaking ability.\nThe Education Amendments of 1972 (Public Law 92-318, 86 Stat. 327) establishes the Education Division in the U.S. Department of Health, Education, and Welfare and the National Institute of Education. Title IX of the Education Amendments of 1972 states, \"No person in the United States shall, on the basis of sex, be excluded from participation in, be denied the benefits of, or be subjected to discrimination under any education program or activity receiving Federal financial assistance.\"\nEqual Educational Opportunities Act of 1974 - Civil Rights Amendments to the Elementary and Secondary Education Act of 1965:\"Title I: Bilingual Education Act - Authorizes appropriations for carrying out the provisions of this Act. Establishes, in the Office of Education, an Office of Bilingual Education through which the Commissioner of Education shall carry out his functions relating to bilingual education. Authorizes appropriations for school nutrition and health services, correction education services, and ethnic heritage studies centers.\nTitle II: Equal Educational Opportunities and the Transportation of Students: Equal Educational Opportunities Act - Provides that no state shall deny equal educational opportunity to an individual on account of his or her race, color, sex, or national origin by means of specified practices...\nTitle IV: Consolidation of Certain Education Programs: Authorizes appropriations for use in various education programs including libraries and learning resources, education for use of the metric system of measurement, gifted and talented children programs, community schools, career education, consumers' education, women's equity in education programs, and arts in education programs.\nCommunity Schools Act - Authorizes the Commissioner to make grants to local educational agencies to assist in planning, establishing, expanding, and operating community education programs\nWomen's Educational Equity Act - Establishes the Advisory Council on Women's Educational Programs and sets forth the composition of such Council. Authorizes the Commissioner of Education to make grants to, and enter into contracts with, public agencies, private nonprofit organizations, and individuals for activities designed to provide educational equity for women in the United States.\nTitle V: Education Administration: Family Educational Rights and Privacy Act (FERPA)- Provides that no funds shall be made available under the General Education Provisions Act to any State or local educational agency or educational institution which denies or prevents the parents of students to inspect and review all records and files regarding their children.\nTitle VII: National Reading Improvement Program: Authorizes the Commissioner to contract with State or local educational agencies for the carrying out by such agencies, in schools having large numbers of children with reading deficiencies, of demonstration projects involving the use of innovative methods, systems, materials, or programs which show promise of overcoming such reading deficiencies.\"In 1975, The Education for All Handicapped Children Act (Public Law 94-142) ensured that all handicapped children (age 3-21) receive a \"free, appropriate public education\" designed to meet their special needs.\n1980\u20131989: A Nation at Risk.\nDuring the 1980s, some of the momentum of education reform moved from the left to the right, with the release of \"A Nation at Risk\", Ronald Reagan's efforts to reduce or eliminate the United States Department of Education. \"[T]he federal government and virtually all state governments, teacher training institutions, teachers' unions, major foundations, and the mass media have all pushed strenuously for higher standards, greater accountability, more \"time on task,\" and more impressive academic results\".\nPer the shift in educational motivation, families sought institutional alternatives, including \"charter schools, progressive schools, Montessori schools, Waldorf schools, Afrocentric schools, religious schools - or home school instruction in their communities.\"\nIn 1984 President Reagan enacted the Education for Economic Security Act\nIn 1989, the https:// authorized funds for Head Start Programs to include child care services.\nIn the latter half of the decade, E. D. Hirsch put forth an influential attack on one or more versions of progressive education. Advocating an emphasis on \"cultural literacy\"\u2014the facts, phrases, and texts.\nSee also Uncommon Schools.\n1990-1999: standards-based education model.\nIn 1994, the land grant system was expanded via the Elementary and Secondary Education Act to include tribal colleges.\nMost states and districts in the 1990s adopted outcome-based education (OBE) in some form or another. A state would create a committee to adopt standards, and choose a quantitative instrument to assess whether the students knew the required content or could perform the required tasks.\nIn 1992 The National Commission on Time and Learning, Extension revise funding for civic education programs and those educationally disadvantaged children.\"\nIn 1994 the Improving America's Schools Act (IASA) reauthorized the Elementary and Secondary Education Act of 1965; amended as The Eisenhower Professional Development Program; IASA designated Title I funds for low income and otherwise marginalized groups; i.e., females, minorities, individuals with disabilities, individuals with limited English proficiency (LEP). By tethering federal funding distributions to student achievement, IASA meant use high stakes testing and curriculum standards to hold schools accountable for their results at the same level as other students. The Act significantly increased impact aid for the establishment of the Charter School Program, drug awareness campaigns, bilingual education, and technology.\nIn 1998 The Charter School Expansion Act amended the Charter School Program, enacted in 1994.\n2000\u20132015: No Child Left Behind.\nhttps:// appropriated funding to repair educational institution's buildings as well as repair and renovate charter school facilities, reauthorized the Even Start program, and enacted the Children's Internet Protection Act.\nThe standards-based National Education Goals 2000, set by the U.S. Congress in the 1990s, were based on the principles of outcomes-based education. In 2002, the standards-based reform movement culminated as the No Child left Behind Act of 2001 where achievement standard were set by each individual state. This federal policy was active until 2015 in the United States .\nAn article released by CBNC.com said a principal Senate Committee will take into account legislation that reauthorizes and modernizes the Carl D. Perkins Act. President George Bush approved this statute in 2006 on August 12, 2006. This new bill will emphasize the importance of federal funding for various Career and Technical (CTE) programs that will better provide learners with in-demand skills. Pell Grants are specific amount of money is given by the government every school year for disadvantaged students who need to pay tuition fees in college.\nAt present, there are many initiatives aimed at dealing with these concerns like innovative cooperation between federal and state governments, educators, and the business sector. One of these efforts is the Pathways to Technology Early College High School (P-TECH). This six-year program was launched in cooperation with IBM, educators from three cities in New York, Chicago, and Connecticut, and over 400 businesses. The program offers students in high school and associate programs focusing on the STEM curriculum. The High School Involvement Partnership, private and public venture, was established through the help of Northrop Grumman, a global security firm. It has given assistance to some 7,000 high school students (juniors and seniors) since 1971 by means of one-on-one coaching as well as exposure to STEM areas and careers.\nThe Teacher Incentive Fund (TIF), established in 2006 by the U.S. Department of Education, was a federal grant program designed to strengthen educational outcomes by supporting performance-based compensation and professional development systems for teachers and principals in high-need schools. TIF aimed to recruit, retain, and reward effective educators through financial incentives linked to student achievement growth and other measures of educator effectiveness. The program reflected broader federal efforts during the 2000s and 2010s to promote accountability and improve teacher quality as part of comprehensive educational reform initiatives. Between 2006 and 2016, TIF awarded over $2 billion in grants to school districts, charter schools, and state education agencies, contributing to the development of performance-based compensation models across the country before being replaced by the Teacher and School Leader (TSL) Incentive Program under the Every Student Succeeds Act (ESSA).\n2016\u20132021: Every Student Succeeds Act.\nThe American Reinvestment and Recovery Act, enacted in 2009, reserved more than $85 billion in public funds to be used for education.\nThe 2009 Council of Chief State School Officers and the National Governors Association launch the Common Core State Standards Initiative.\nIn 2012 the Obama administration launched the Race to the Top competition aimed at spurring K\u201312 education reform through higher standards.\"The Race to the Top \u2013 District competition will encourage transformative change within schools, targeted toward leveraging, enhancing, and improving classroom practices and resources.\nThe four key areas of reform include:\nIn 2015, under the Obama administration, many of the more restrictive elements that were enacted under No Child Left Behind (NCLB, 2001), were removed in the Every Student Succeeds Act (ESSA, 2015) which limits the role of the federal government in school liability. Every Student Succeeds Act reformed educational standards by \"moving away from such high stakes and assessment based accountability models\" and focused on assessing student achievement from a holistic approach by utilizing qualitative measures. Some argue that giving states more authority can help prevent considerable discrepancies in educational performance across different states. ESSA was approved by former President Obama in 2015 which amended and empowered the Elementary and Secondary Education Act of 1965. The Department of Education has the choice to carry out measures in drawing attention to said differences by pinpointing lowest-performing state governments and supplying information on the condition and progress of each state on different educational parameters. It can also provide reasonable funding along with technical aid to help states with similar demographics collaborate in improving their public education programs.\nSocial and emotional learning: strengths-based education model.\nThis uses a methodology that values purposeful engagement in activities that turn students into self-reliant and efficient learners. Holding on to the view that everyone possesses natural gifts that are unique to one's personality (e.g. computational aptitude, musical talent, visual arts abilities), it likewise upholds the idea that children, despite their inexperience and tender age, are capable of coping with anguish, able to survive hardships, and can rise above difficult times.\nTrump administration.\nIn 2017, Betsy DeVos was instated as the 11th Secretary of Education. A strong proponent of school choice, school voucher programs, and charter schools, DeVos was a much-contested choice as her own education and career had little to do with formal experience in the US education system. In a Republican-dominated senate, she received a 50\u201350 vote - a tie that was broken by Vice President Mike Pence. Prior to her appointment, DeVos received a BA degree in business economics from Calvin College in Grand Rapids, Michigan and she served as chairman of an investment management firm, The Windquest Group. She supported the idea of leaving education to state governments under the new K-12 legislation. DeVos cited the interventionist approach of the federal government to education policy following the signing of the ESSA. The primary approach to that rule has not changed significantly. Her opinion was that the education movement populist politics or populism encouraged reformers to commit promises which were not very realistic and therefore difficult to deliver.\nOn July 31, 2018, President Donald Trump signed the Strengthening Career and Technical Education for the 21st Century Act (HR 2353) The Act reauthorized the Carl D. Perkins Career and Technical Education Act, a $1.2 billion program modified by the United States Congress in 2006. A move to change the Higher Education Act was also deferred.\nThe legislation enacted on July 1, 2019, replaced the Carl D. Perkins Career and Technical Education (Perkins IV) Act of 2006. Stipulations in Perkins V enables school districts to make use of federal subsidies for all students' career search and development activities in the middle grades as well as comprehensive guidance and academic mentoring in the upper grades. At the same time, this law revised the meaning of \"special populations\" to include homeless persons, foster youth, those who left the foster care system, and children with parents on active duty in the United States armed forces.\nBarriers to reform.\nEducation inequalities facing students of color.\nAnother factor to consider in education reform is that of equity and access. Contemporary issues in the United States regarding education faces a history of inequalities that come with consequences for education attainment across different social groups. For example, students of color often attend underfunded schools, have less access to advanced classes, and face higher suspension rates, which all impact their chances of graduating and going to college.\nRacial and socioeconomic class segregation.\nA history of racial, and subsequently class, segregation in the U.S. resulted from practices of law. Residential segregation is a direct result of twentieth century policies that separated by race using zoning and redlining practices, in addition to other housing policies, whose effects continue to endure in the United States. These neighborhoods that have been segregated de jure\u2014by force of purposeful public policy at the federal, state, and local levels\u2014disadvantage people of color as students must attend school near their homes.\nWith the inception of the New Deal between 1933 and 1939, and during and following World War II, federally funded public housing was explicitly racially segregated by the local government in conjunction with federal policies through projects that were designated for Whites or Black Americans in the South, Northeast, Midwest, and West. Following an ease on the housing shortage post-World War II, the federal government subsidized the relocation of Whites to suburbs. The Federal Housing and Veterans Administration constructed such developments on the East Coast in towns like Levittown on Long Island, New Jersey, Pennsylvania, and Delaware. On the West Coast, there was Panorama City, Lakewood, Westlake, and Seattle suburbs developed by Bertha and William Boeing. As White families left for the suburbs, Black families remained in public housing and were explicitly placed in Black neighborhoods. Policies such as public housing director, Harold Ickes', \"neighborhood composition rule\" maintained this segregation by establishing that public housing must not interfere with pre-existing racial compositions of neighborhoods. Federal loan guarantees were given to builders who adhered to the condition that no sales were made to Black families and each deed prohibited re-sales to Black families, what the Federal Housing Administration (FHA) described as an \"incompatible racial element\". In addition, banks and savings intuitions refused loans to Black families in White suburbs and Black families in Black neighborhoods. In the mid-twentieth century, urban renewal programs forced low-income black residents to reside in places farther from universities, hospitals, or business districts and relocation options consisted of public housing high-rises and ghettos.\nThis history of de jure segregation has impacted resource allocation for public education in the United States, with schools continuing to be segregated by race and class. Low-income White students are more likely than Black students to be integrated into middle-class neighborhoods and less likely to attend schools with other predominantly disadvantaged students. Students of color disproportionately attend underfunded schools and Title I schools in environments entrenched in environmental pollution and stagnant economic mobility with limited access to college readiness resources. According to research, schools attended by primarily Hispanic or African American students often have high turnover of teaching staff and are labeled high-poverty schools, in addition to having limited educational specialists, less available extracurricular opportunities, greater numbers of provisionally licensed teachers, little access to technology, and buildings that are not well maintained. With this segregation, more local property tax is allocated to wealthier communities and public schools' dependence on local property taxes has led to large disparities in funding between neighboring districts. The top 10% of wealthiest school districts spend approximately ten times more per student than the poorest 10% of school districts.\nRacial wealth gap.\nThis history of racial and socioeconomic class segregation in the U.S. has manifested into a racial wealth divide. With this history of geographic and economic segregation, trends illustrate a racial wealth gap that has impacted educational outcomes and its concomitant economic gains for minorities. Wealth or net worth\u2014the difference between gross assets and debt\u2014is a stock of financial resources and a significant indicator of financial security that offers a more complete measure of household capability and functioning than income. Within the same income bracket, the chance of completing college differs for White and Black students. Nationally, White students are at least 11% more likely to complete college across all four income groups. Intergenerational wealth is another result of this history, with White college-educated families three times as likely as Black families to get an inheritance of $10,000 or more. 10.6% of White children from low-income backgrounds and 2.5% of Black children from low-income backgrounds reach the top 20% of income distribution as adults. Less than 10% of Black children from low-income backgrounds reach the top 40%.\nAccess to early childhood education.\nThese disadvantages facing students of color are apparent early on in early childhood education. By the age of five, children of color are impacted by opportunity gaps indicated by poverty, school readiness gap, segregated low-income neighborhoods, implicit bias, and inequalities within the justice system as Hispanic and African American boys account for as much as 60% of total prisoners within the incarceration population. These populations are also more likely to experience adverse childhood experiences (ACEs).\nHigh-quality early care and education are less accessible to children of color, particularly African American preschoolers as findings from the National Center for Education Statistics show that in 2013, 40% of Hispanic and 36% White children were enrolled in learning center-based classrooms rated as high, while 25% of African American children were enrolled in these programs. 15% of African American children attended low ranking center-based classrooms. In home-based settings, 30% of White children and over 50% of Hispanic and African American children attended low rated programs.\nContemporary issues (United States).\nOverview.\nIn the first decade of the 21st century, several issues are salient in debates over further education reform:\nPrivate interest in American charter schools.\nCharter schools public independent institutions in which both the cost and risk are fully funded by the taxpayers. Some charter schools are nonprofit in name only and are structured in ways that individuals and private enterprises connected to them can make money. Other charter schools are for-profit. In many cases, the public is largely unaware of this rapidly changing educational landscape, the debate between public and private/market approaches, and the decisions that are being made that affect their children and communities. Critics have accused for-profit entities, (education management organizations, EMOs) and private foundations such as the Bill and Melinda Gates Foundation, the Eli and Edythe Broad Foundation, and the Walton Family Foundation of funding Charter school initiatives to undermine public education and turn education into a \"Business Model\" which can make a profit. In some cases a school's charter is held by a non-profit that chooses to contract all of the school's operations to a third party, often a for-profit, CMO. This arrangement is defined as a \"vendor-operated school\", (\"VOS\").\nSchool choice.\nEconomists, such as the late Nobel laureate Milton Friedman, advocate for school choice to promote excellence in education through competition and choice. Proponents claim that a competitive market for schooling provides a workable method of accountability for results. Public education vouchers permit guardians to select and pay any school, public or private, with public funds that were formerly allocated directly to local public schools. The theory is that children's guardians will naturally shop for the best schools for their children, much as is already done at college level.\nMany reforms based on school choice have led to slight to moderate improvements. Some teachers' union members see those improvements as insufficient to offset the decreased teacher pay and job security. For instance, New Zealand's landmark reform in 1989, during which schools were granted substantial autonomy, funding was devolved to schools, and parents were given a free choice of which school their children would attend, led to moderate improvements in most schools. It was argued that the associated increases in inequity and greater racial stratification in schools nullified the educational gains. Others, however, argued that the original system created more inequity, due to lower income students being required to attend poorer performing inner city schools and not being allowed school choice or better educations that are available to higher income inhabitants of suburbs. Thus, it was argued that school choice promoted social mobility and increased test scores, especially in the cases of low income students. Similar results have been found in other jurisdictions. The small improvements produced by some school choice policies seem to reflect weaknesses in the ways that choice is implemented, rather than a failure of the basic principle itself.\nTeacher tenure.\nCritics of teacher tenure claim that the laws protect ineffective teachers from being fired, which can be detrimental to student success. Tenure laws vary from state to state, but generally they set a probationary period during which the teacher proves themselves worthy of the lifelong position. Probationary periods range from one to three years. Advocates for tenure reform often consider these periods too short to make such an important decision; especially when that decision is exceptionally hard to revoke. Due process restriction protect tenured teachers from being wrongfully fired; however these restrictions can also prevent administrators from removing ineffective or inappropriate teachers. A 2008 survey conducted by the US Department of Education found that, on average, only 2.1% of teachers are dismissed each year for poor performance.\nIn October 2010 Apple Inc. CEO Steve Jobs had a consequential meeting with U.S. President Barack Obama to discuss U.S. competitiveness and the nation's education system. During the meeting Jobs recommended pursuing policies that would make it easier for school principals to hire and fire teachers based on merit.\nIn 2012 tenure for school teachers was challenged in a California lawsuit called \"Vergara v. California\". The primary issue in the case was the impact of tenure on student outcomes and on equity in education. On June 10, 2014, the trial judge ruled that California's teacher tenure statute produced disparities that \"shock the conscience\" and violate the equal protection clause of the California Constitution.\nFunding levels.\nAccording to a 2005 report from the OECD, the United States is tied for first place with Switzerland when it comes to annual spending per student on its public schools, with each of those two countries spending more than $11,000 (in U.S.\u00a0currency).\nDespite this high level of funding, U.S.\u00a0public schools lag behind the schools of other rich countries in the areas of reading, math, and science. A further analysis of developed countries shows no correlation between per student spending and student performance, suggesting that there are other factors influencing education. Top performers include Singapore, Finland and Korea, all with relatively low spending on education, while high spenders including Norway and Luxembourg have relatively low performance. One possible factor is the distribution of the funding.\nIn the US, schools in wealthy areas tend to be over-funded while schools in poorer areas tend to be underfunded. These differences in spending between schools or districts may accentuate inequalities, if they result in the best teachers moving to teach in the most wealthy areas. The inequality between districts and schools led to 23 states instituting school finance reform based on adequacy standards that aim to increase funding to low-income districts. A 2018 study found that between 1990 and 2012, these finance reforms led to an increase in funding and test scores in the low income districts; which suggests finance reform is effective at bridging inter-district performance inequalities. It has also been shown that the socioeconomic situation of the students family has the most influence in determining success; suggesting that even if increased funds in a low income area increase performance, they may still perform worse than their peers from wealthier districts.\nStarting in the early 1980s, a series of analyses by Eric Hanushek indicated that the amount spent on schools bore little relationship to student learning. This controversial argument, which focused attention on how money was spent instead of how much was spent, led to lengthy scholarly exchanges. In part the arguments fed into the class size debates and other discussions of \"input policies.\" It also moved reform efforts towards issues of school accountability (including No Child Left Behind) and the use of merit pay and other incentives.\nThere have been studies that show smaller class sizes and newer buildings (both of which require higher funding to implement) lead to academic improvements. It should also be noted that many of the reform ideas that stray from the traditional format require greater funding.\nAccording to a 1999 article, William J. Bennett, former U.S.\u00a0Secretary of Education, argued that increased levels of spending on public education have not made the schools better, citing the following statistics:\nInternationally.\nEducation for All.\nThe Education for All (EFA) Assessment 2000 was launched in July 1998 with an aim to help countries to identify both problems and prospects for further progress of EFA, and to strengthen their capacity to improve and monitor the provision and outcomes of basic education. Some 179 countries set up National Assessment Groups which collected quantitative data focusing on eighteen core indicators and carried out case-studies to collect qualitative information.\nEducation 2030 Agenda refers to the global commitment of the Education for All movement to ensure access to basic education for all. It is an essential part of the 2030 Agenda for Sustainable Development. The roadmap to achieve the Agenda is the Education 2030 Incheon Declaration and Framework for Action, which outlines how countries, working with UNESCO and global partners, can translate commitments into action.\nThe United Nations, over 70 ministers, representatives of member-countries, bilateral and multilateral agencies, regional organizations, academic institutions, teachers, civil society, and the youth supported the Framework for Action of the Education 2030 platform. The Framework was described as the outcome of continuing consultation to provide guidance for countries in implementing this Agenda. At the same time, it mobilizes various stakeholders in the new education objectives, coordination, implementation process, funding, and review of Education 2030.\nThailand.\nIn 1995, the Minister of Education, Sukavich Rangsitpol, launched education reforms for all aimed at realizing the potential of Thai people to develop themselves for a better quality of life and to develop the nation for peaceful coexistence in the global community.\nSince December 1995, activities have been conducted in four main areas:\nSchool-based management (SBM) in Thailand was implemented in 1997 as part of a reform aimed at overcoming a profound crisis in the education system.\nThe 1995 Education Reform resulted in 40,000 schools being required to improve their school environment and encourage local community involvement in school administration and management.\nThese schools later accepted 4.35 million students aged between 3\u201317 years old from poor families in remote areas. Thereafter, Thailand successfully established Education For All (EFA).\nAs a result, Thailand received the 1997 ACEID award for excellence in education from UNESCO.\nAccording to UNESCO, Thailand's education reform led to the following results:\nThis program was later added to the 1997 Constitution of Thailand, granting access to all citizens.\nA World Bank report indicated that after the 1997 Asian financial crisis, income in the northeast, the poorest part of Thailand, rose by 46 percent from 1994 to 2000. Nationwide poverty fell from 21.3% to 11.3%.\nLearning crisis.\nThe learning crisis is the reality that while the majority of children around the world attend school, a large proportion of them are not learning. A World Bank study found that \"53 percent of children in low- and middle-income countries cannot read and understand a simple story by the end of primary school.\" While schooling has increased rapidly over the last few decades, learning has not followed suit. Many practitioners and academics call for education system reform in order to address the learning needs of all children.\nDigital education.\nThe movement to use computers more in education naturally includes many unrelated ideas, methods, and pedagogies since there are many uses for digital computers. For example, the fact that computers are naturally good at math leads to the question of the use of calculators in math education. The Internet's communication capabilities make it potentially useful for collaboration, and foreign language learning. The computer's ability to simulate physical systems makes it potentially useful in teaching science. More often, however, debate of digital education reform centers around more general applications of computers to education, such as electronic test-taking and online classes.\nAnother viable addition to digital education has been blended learning. In 2009, over 3 million K-12 students took an online course, compared to 2000 when 45,000 took an online course. Blended learning examples include pure online, blended, and traditional education. Research results show that the most effective learning takes place in a blended format. This allows children to view the lecture ahead of time and then spend class time practicing, refining, and applying what they have previously learned.\nThe idea of creating artificial intelligence led some computer scientists to believe that teachers could be replaced by computers, through something like an expert system; however, attempts to accomplish this have predictably proved inflexible. The computer is now more understood to be a tool or assistant for the teacher and students.\nHarnessing the richness of the Internet is another goal. In some cases classrooms have been moved entirely online, while in other instances the goal is more to learn how the Internet can be more than a classroom.\nWeb-based international educational software is under development by students at New York University, based on the belief that current educational institutions are too rigid: effective teaching is not routine, students are not passive, and questions of practice are not predictable or standardized. The software allows for courses tailored to an individual's abilities through frequent and automatic multiple intelligences assessments. Ultimate goals include assisting students to be intrinsically motivated to educate themselves, and aiding the student in self-actualization. Courses typically taught only in college are being reformatted so that they can be taught to any level of student, whereby elementary school students may learn the foundations of any topic they desire. Such a program has the potential to remove the bureaucratic inefficiencies of education in modern countries, and with the decreasing digital divide, help developing nations rapidly achieve a similar quality of education. With an open format similar to Wikipedia, any teacher may upload their courses online and a feedback system will help students choose relevant courses of the highest quality. Teachers can provide links in their digital courses to webcast videos of their lectures. Students will have personal academic profiles and a forum will allow students to pose complex questions, while simpler questions will be automatically answered by the software, which will bring you to a solution by searching through the knowledge database, which includes all available courses and topics.\nThe 21st century ushered in the acceptance and encouragement of internet research conducted on college and university campuses, in homes, and even in gathering areas of shopping centers. Addition of cyber cafes on campuses and coffee shops, loaning of communication devices from libraries, and availability of more portable technology devices, opened up a world of educational resources. Availability of knowledge to the elite had always been obvious, yet provision of networking devices, even wireless gadget sign-outs from libraries, made availability of information an expectation of most persons. Cassandra B. Whyte researched the future of computer use on higher education campuses focusing on student affairs. Though at first seen as a data collection and outcome reporting tool, the use of computer technology in the classrooms, meeting areas, and homes continued to unfold. The sole dependence on paper resources for subject information diminished and e-books and articles, as well as online courses, were anticipated to become increasingly staple and affordable choices provided by higher education institutions according to Whyte in a 2002 presentation.\nDigitally \"flipping\" classrooms is a trend in digital education that has gained significant momentum. Will Richardson, author and visionary for the digital education realm, points to the not-so-distant future and the seemingly infinite possibilities for digital communication linked to improved education. Education on the whole, as a stand-alone entity, has been slow to embrace these changes. The use of web tools such as wikis, blogs, and social networking sites is tied to increasing overall effectiveness of digital education in schools. Examples exist of teacher and student success stories where learning has transcended the classroom and has reached far out into society.\nThe media has been instrumental in pushing formal educational institutions to become savvier in their methods. Additionally, advertising has been (and continues to be) a vital force in shaping students and parents thought patterns.\nTechnology is a dynamic entity that is constantly in flux. As time presses on, new technologies will continue to break paradigms that will reshape human thinking regarding technological innovation. This concept stresses a certain disconnect between teachers and learners and the growing chasm that started some time ago. Richardson asserts that traditional classroom's will essentially enter entropy unless teachers increase their comfort and proficiency with technology.\nAdministrators are not exempt from the technological disconnect. They must recognize the existence of a younger generation of teachers who were born during the Digital Age and are very comfortable with technology. However, when old meets new, especially in a mentoring situation, conflict seems inevitable. Ironically, the answer to the outdated mentor may be digital collaboration with worldwide mentor webs; composed of individuals with creative ideas for the classroom.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n\u00a0This article incorporates text from a free content work. Licensed under CC-BY-SA IGO 3.0 (http://). Text taken from http://, 6, 8-9, UNESCO, UNESCO. UNESCO. \nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9621", "revid": "1320339706", "url": "https://en.wikipedia.org/wiki?curid=9621", "title": "Ellensburg, Washington", "text": "City in Washington, United States\nEllensburg is a city in and the county seat of Kittitas County, Washington, United States. It is located just east of the Cascade Range near the junction of Interstate 90 and Interstate 82. The population was 18,666 at the 2020 census. and was estimated to be 20,996 in 2024.\nThe city is located along the Yakima River in the Kittitas Valley, an agricultural region that extends east towards the Columbia River. The valley is a major producer of timothy hay, which is processed and shipped internationally. Ellensburg is also the home of Central Washington University (CWU).\nEllensburg, originally named Ellensburgh for the wife of town founder John Alden Shoudy, was founded in 1871 and grew rapidly in the 1880s following the arrival of the Northern Pacific Railway. The city was once a leading candidate to become the state capital of Washington, but its campaign was scuppered by a major fire in 1889.\nHistory.\nJohn Alden Shoudy arrived in the Kittitas Valley in 1871 and purchased a small trading post from Andrew Jackson \"A.J.\" Splawn, called \"Robber's Roost\". Robber's Roost was the first business in the valley, aside from the early trading that occurred among Native Americans, cattle drivers, trappers, and miners. A small stone monument to Robber's Roost with a placard can be found at its original location, present-day 3rd Avenue, just west of Main Street near the alley.\nShoudy named the new town after his wife, Mary Ellen, thus officially starting the city of Ellensburgh around 1872. Shoudy had not been the first settler nor the first business person in the Kittitas Valley, but he was responsible for platting the city of Ellensburgh in the 1870s and also named the streets in the downtown district. Ellensburgh was officially incorporated on November 26, 1883. In 1894, the final -\"h\" was dropped under standardization pressure from the United States Postal Service and Board of Geography Names. Ellensburg was an early center of commerce in Washington and was among the first cities in the state to have electrical service.\nThe city launched a bid to become Washington state's capital in 1889, preparing a site in the Capital Hill neighborhood for government offices. On July 4 that year, however, a major fire destroyed much of the downtown area and stalled the campaign, which resumed with a series of referendums, in which Washington voters chose Olympia. The state legislature selected Ellensburg as the location for the State Normal School (now Central Washington University).\nThere were several early newspapers in Ellensburg. \"The Daily Record\", which started in 1909, is the publication which serves the city and county today. Concerns over the state of Ellensburg's historic downtown led to the formation of the Ellensburg Downtown Association to work on revitalizing the area.\nGeography.\nEllensburg lies in the Kittitas Valley east of the Cascade Mountains and at the edge of the Columbia Plateau. The Yakima River runs through the area and continues south into the Yakima River Canyon, which cuts through the Manastash Ridge. According to the United States Census Bureau, the city has a total area of , of which is land and is water.\nClimate.\nOwing to the strong Cascade rain shadow, Ellensburg experiences a typical Intermountain cool semi-arid climate (K\u00f6ppen \"BSk\"). The hottest temperature recorded in Ellensburg was on July 26, 1928, while the coldest temperature recorded was on December 12, 1919.\nEconomy.\nIn 2024, WinCo Foods, a regional supermarket chain, began construction of a distribution center and cold storage facility in Ellensburg. It is scheduled to be fully completed in 2031 and encompass .\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\n2020 census.\nAs of the 2020 census, there were 18,666 people and 8,110 households, and 3,541 families residing in the city.\n2010 census.\nAs of the 2010 census, there were 18,174 people, 7,301 households, and 2,889 families living in the city. The population density was . There were 7,867 housing units at an average density of . The racial makeup of the city was 85.7% White, 1.5% African American, 1.0% Native American, 3.2% Asian, 0.2% Pacific Islander, 4.6% from other races, and 3.7% from two or more races. Hispanic or Latino of any race were 9.7% of the population.\nThere were 7,301 households, of which 19.3% had children under the age of 18 living with them, 28.2% were married couples living together, 8.2% had a female householder with no husband present, 3.1% had a male householder with no wife present, and 60.4% were non-families. 35.1% of all households were made up of individuals, and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.16 and the average family size was 2.86.\nThe median age in the city was 23.5 years. 14.2% of residents were under the age of 18; 41.2% were between the ages of 18 and 24; 21.8% were from 25 to 44; 13.9% were from 45 to 64; and 8.9% were 65 years of age or older. The gender makeup of the city was 50.1% male and 49.9% female.\n2000 census.\nAs of the 2000 census, there were 15,414 people, 6,249 households, and 2,649 families living in the city. The population density was . There were 6,732 housing units at an average density of . The racial makeup of the city was 88.07% White, 1.17% Black or African American, 0.95% Native American, 4.09% Asian, 0.16% Pacific Islander, 2.86% from other races, and 2.69% from two or more races. 6.33% of the population were Hispanic or Latino of any race.\nThere were 6,249 households, of which 20.8% had children under the age of 18 living with them, 31.4% were married couples living together, 8.1% had a female householder with no husband present, and 57.6% were non-families. 35.5% of all households were made up of individuals, and 9.1% had someone living alone who was 65 years of age or older. The average household size was 2.12 and the average family size was 2.84.\nIn the city, the population was spread out, with 15.8% under the age of 18, 39.3% from 18 to 24, 22.7% from 25 to 44, 12.8% from 45 to 64, and 9.4% who were 65 years of age or older. The median age was 24 years. For every 100 females, there were 95.0 males. For every 100 females age 18 and over, there were 93.1 males.\nThe median income for a household in the city was $20,034, and the median income for a family was $37,625. Males had a median income of $31,022 versus $22,829 for females. The per capita income for the city was $13,662. About 18.8% of families and 34.3% of the population were below the poverty line, including 29.0% of those under age 18 and 11.2% of those age 65 or over.\nArts and culture.\nThe City of Ellensburg has several local art museums and galleries: \nGovernment and politics.\nThe City of Ellensburg uses the council\u2013manager form of government with a city manager hired by the city council. The seven-member city council is elected at-large and serve four-year terms. The City Council elects a Mayor and Deputy Mayor from the council to serve 2-year terms.\nThe city lies within the 13th legislative district, which elects one senator and two representatives to serve in the Washington State Legislature. At the congressional level, Ellensburg is within the 8th district, which includes all of Kittitas and Chelan counties, along with the eastern portions of the Seattle metropolitan area.\nMedia.\nKittitas County is served by the \"Daily Record\", a newspaper published in Ellensburg five days a week.\nThe city maintains its own public library, which opened on January 20, 1910, using funds donated by Andrew Carnegie.\nEducation.\nHigher education.\nThe main campus of Central Washington University in Ellensburg covers over and is the only four-year university in the region. It was established in 1891 as the Washington State Normal School, a teachers' college, and later renamed as it expanded to offer bachelor's and master's degrees in various programs. Central Washington had 8,509 total enrolled student in 2024, ranking sixth among post-secondary institutions in Washington state.\nPublic schools.\nPublic schools are operated by Ellensburg School District 401. The district includes one high school (Ellensburg High School), one middle school, and four elementary schools.\nInfrastructure.\nTransportation.\nEllensburg lies at the intersection of several major highways that cross Washington state. Interstate 82 connects the city to the Yakima Valley and crosses over the Manastash Ridge. Interstate 90 carries east\u2013west traffic from Seattle to Spokane and crosses the Cascades at Snoqualmie Pass. U.S. Route 97 travels north\u2013south from Yakima to the Wenatchee Valley with onward connections to the British Columbia Interior in the north and Central Oregon to the south.\nThe city government operates Central Transit, which has five bus routes, paratransit, and on-demand medical transport. The system is fare-free and primarily funded by a sales tax within the designated public transportation benefit area around cities in Kittitas County. Central Transit also partners with HopeSource, a non-profit organization, to operate the Kittitas County Connector, which launched in 2019 and connects Ellensburg to outlying communities in the county. Several intercity bus operators also serve stops in Ellensburg, including FlixBus and the Travel Washington Apple Line. Yakima Transit also provides intercity commuter service from Ellensburg to Yakima with a state grant.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9623", "revid": "50241433", "url": "https://en.wikipedia.org/wiki?curid=9623", "title": "Eugene, Oregon", "text": "City in Oregon, United States\nEugene ( ) is a city in and the county seat of Lane County, Oregon, United States. It is located at the southern end of the Willamette Valley, near the confluence of the McKenzie and Willamette rivers, about east of the Oregon Coast and approximately 110 miles south of Portland.\nThe second-most populous city in Oregon, Eugene had a population of 176,654 as of the 2020 United States census and it covers city area of . The Eugene-Springfield metropolitan statistical area is the second largest in Oregon after Portland. In 2022, Eugene's population was estimated to have reached 179,887.\nEugene is home to the University of Oregon, Bushnell University, and Lane Community College. The city is noted for its natural environment, recreational opportunities (especially bicycling, hiking trails, running/jogging, rafting, and kayaking), and focus on the arts, along with its history of civil unrest, riots, and green activism. Eugene's official motto is \"A Great City for the Arts and Outdoors\". It is also referred to as the \"Emerald City\" and as \"Track Town, USA\". The Nike Corporation had its beginnings in Eugene. In July 2022, the city hosted the 18th World Athletics Championship.\nHistory.\nIndigenous history.\nThe first people to settle in the Eugene area were the Chifin band of the Kalapuyans, also written Calapooia or Calapooya. They made \"seasonal rounds,\" moving around the countryside to collect and preserve local foods, including acorns, the bulbs of the wapato and camas plants, and berries. They stored these foods in their permanent winter village. When crop activities waned, they returned to their winter villages and took up hunting, fishing, and trading. They were known as the Chifin Kalapuyans and called the Eugene area where they lived \"Chifin\", sometimes recorded as \"Chafin\" or \"Chiffin\".\nOther Kalapuyan tribes occupied villages that are also now within Eugene city limits. Pee-you or Mohawk Calapooians, Winefelly or Pleasant Hill Calapooians, and the Lungtum or Long Tom. They were close-neighbors to the Chifin, intermarried, and were political allies. Some authorities suggest the Brownsville Kalapuyans (Calapooia Kalapuyans) were related to the Pee-you. It is likely that since the Santiam had an alliance with the Brownsville Kalapuyans that the Santiam influence also went as far at Eugene.\nAccording to archeological evidence, the ancestors of the Kalapuyans may have been in Eugene for as long as 10,000 years. In the 1800s their traditional way of life faced significant changes due to devastating epidemics and settlement, first by French fur traders and later by an overwhelming number of European settlers.Some sources also stated that their number started to decline in the nineteenth century dues to diseases such as Small pox and Malaria.\nSettlement and impact.\nFrench fur traders had settled seasonally in the Willamette Valley by the beginning of the 19th century. Their settlements were concentrated in the \"French Prairie\" community in Northern Marion County but may have extended south to the Eugene area. Having already developed relationships with Native communities through intermarriage and trade, they negotiated for land from the Kalapuyans. By 1828 to 1830 they and their Native wives began year-round occupation of the land, raising crops and tending animals. In this process, the mixed race families began to impact Native access to land, food supply, and traditional materials for trade and religious practices.\nIn July 1830, \"intermittent fever\" struck the lower Columbia region and a year later, the Willamette Valley. Natives traced the arrival of the disease, then new to the Pacific Northwest, to the \"USS Owyhee\", captained by John Dominis. \"Intermittent fever\" is thought by researchers now to be malaria. According to Robert T. Boyd, an anthropologist at Portland State University, the first three years of the epidemic, \"probably constitute the single most important epidemiological event in the recorded history of what would eventually become the state of Oregon\". In his book \"The Coming of the Spirit Pestilence\" Boyd reports there was a 92% population loss for the Kalapuyans between 1830 and 1841. This catastrophic event shattered the social fabric of Kalapuyan society and altered the demographic balance in the Valley. This balance was further altered over the next few years by the arrival of Anglo-American settlers, beginning in 1840 with 13 people and growing steadily each year until within 20 years more than 11,000 European American settlers, including Eugene Skinner, had arrived.\nAs the demographic pressure from the settlers grew, the remaining Kalapuyans were forcibly removed to Indian reservations. Though some Natives avoided transfer into the reservation, most were moved to the Grand Ronde reservation in 1856. Strict racial segregation was enforced and mixed race people, known as M\u00e9tis in French, had to make a choice between the reservation and Anglo-American society. Native Americans could not leave the reservation without traveling papers and white people could not enter the reservation.\nEugene Franklin Skinner, after whom Eugene is named, arrived in the Willamette Valley in 1846 with 1,200 other settlers that year. Advised by the Kalapuyans to build on high ground to avoid flooding, he erected the first pioneer cabin on south or west slope of what the Kalapuyans called Ya-po-ah. The \"isolated hill\" is now known as Skinner's Butte. The cabin was used as a trading post and was registered as an official post office on January 8, 1850.\nAt this time the settlement was known by settlers as Skinner's Mudhole. It was relocated in 1853 and named Eugene City in 1853. Formally incorporated as a city in 1862, it was renamed to Eugene in 1889. Skinner ran a ferry service across the Willamette River where the Ferry Street Bridge now stands.\nEducational institutions.\nThe first major educational institution in the area was Columbia College, founded a few years earlier than the University of Oregon. It fell victim to two major fires in four years, and after the second fire, the college decided not to rebuild again. The part of south Eugene known as College Hill was the former location of Columbia College. There is no college there today.\nThe town raised the initial funding to start a public university, which later became the University of Oregon, with the hope of turning the small town into a center of learning. In 1872, the Legislative Assembly passed a bill creating the University of Oregon as a state institution. Eugene bested the nearby town of Albany in the competition for the state university. In 1873, community member J.H.D. Henderson donated the hilltop land for the campus, overlooking the city. The university first opened in 1876 with the regents electing the first faculty and naming John Wesley Johnson as president. The first students registered on October 16, 1876. The first building was completed in 1877; it was named Deady Hall in honor of the first Board of Regents President and community leader Judge Matthew P. Deady.\nOther universities in Eugene include Bushnell University and New Hope Christian College.\nTwentieth century.\nEugene grew rapidly throughout most of the twentieth century, with the exception being the early 1980s when a downturn in the timber industry caused high unemployment. By 1985, the industry had recovered and Eugene began to attract more high-tech industries, earning it the moniker the \"Emerald Shire\". In 2012, Eugene and the surrounding metro area was dubbed the Silicon shire.\nThe first Nike shoe was used in 1972 during the US Olympic trials held in Eugene.\nActivism.\nThe 1970s saw an increase in community activism. Local activists stopped a proposed freeway and lobbied for the construction of the Washington Jefferson Park beneath the Washington-Jefferson Street Bridge. Community Councils soon began to form as a result of these efforts. A notable impact of the turn to community-organized politics came with Eugene Local Measure 51, a ballot measure in 1978 that repealed a gay rights ordinance approved by the Eugene City Council in 1977 that prohibited discrimination by sexual orientation. Eugene is also home to Beyond Toxics, a nonprofit environmental justice organization founded in 2000.\nOne hotspot for protest activity since the 1990s has been the Whiteaker district, located in the northwest of downtown Eugene. The Whiteaker is primarily a working-class neighborhood that has become a cultural hub, center of community and activism and home to alternative artists. It saw an increase of activity in the 1990s after many young people drawn to Eugene's political climate relocated there. Animal rights groups have had a heavy presence in the Whiteaker, and several vegan restaurants are located there. According to David Samuels, the Animal Liberation Front and the Earth Liberation Front have had an underground presence in the neighborhood. The neighborhood is home to a number of communal apartment buildings, which are often organized by anarchist or environmentalist groups. Local activists have also produced independent films and started art galleries, community gardens, and independent media outlets. Copwatch, Food Not Bombs, and Critical Mass are also active in the neighborhood.\nThe 21st century has seen continued environmental and social justice activism. In 2011, the Occupy Eugene protests from October to December occurred in connection with the Occupy Wall Street movement, and these local protests included around 2000 participants. In 2020, George Floyd protests occurred in May and June, including peaceful demonstrations and riots. These protests in connection with the nationwide 2020 George Floyd protests resulted in increased consciousness towards Eugene's black history and race issues and resulted in the renaming of University Hall and the toppling of the Pioneer and Pioneer mother statues at the University of Oregon. In 2023 and 2024, activist groups held several demonstrations related to the Israel-Palestine conflict across the city, including marches with hundreds of participants, the April 15, 2024 I-5 protest which blocked southbound traffic for hours and led to the largest mass arrests in Eugene activist history, and the 2024 University of Oregon pro-Palestinian campus occupation as part of the 2024 pro-Palestinian protests on college campuses.\nGeography.\nAccording to the United States Census Bureau, the city has a total area of , of which is land and is water. Eugene is at an elevation of .\nTo the north of downtown are Gillespie Butte and Skinner Butte. Northeast of the city are the Coburg Hills. Spencer Butte is a prominent landmark south of the city. Mount Pisgah is southeast of Eugene and includes the Mount Pisgah Arboretum and the Howard Buford Recreation Area, a Lane County Park. Eugene is surrounded by foothills and forests to the south, east, and west, while to the north the land levels out into the Willamette Valley and consists of mostly farmland.\nThe Willamette and McKenzie Rivers run through Eugene and its neighboring city, Springfield. Another important stream is Amazon Creek, whose headwaters are near Spencer Butte. The creek discharges into the Long Tom River north Fern Ridge Reservoir, maintained for winter flood control by the Army Corps of Engineers. The Eugene Yacht Club hosts a sailing school and sailing regattas at Fern Ridge during summer months.\nNeighborhoods.\nEugene has 23 neighborhoods:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThe River Road and Santa Clara communities, which make up the northwestern part of the city, are neighborhoods within the urban growth boundary of Eugene. However, they are mostly outside of the city limits.\nClimate.\nLike the rest of the Willamette Valley, Eugene lies in the Marine West Coast climate zone, with Mediterranean characteristics. Under the K\u00f6ppen climate classification scheme, Eugene has a warm-summer Mediterranean climate (K\u00f6ppen: \"Csb\"). Temperatures can vary from cool to warm, with warm, dry summers and cool, wet winters. Spring and fall are also moist seasons, with light rain falling for long periods. The average rainfall is , with the wettest \"rain year\" being from July 1973 to June 1974 with and the driest from July 2000 to June 2001 with . Measurements taken by NOAA over the past four decades have indicated a significant decline in average annual precipitation. From 1981 to 2010 inclusive, the reported annual average precipitation was , but for the thirty-year period ending in 2020, the annual average had declined , to . The figures from the second half of that period, or 2006 \u2013 2020 inclusive, pointed to a further decline of more than , down to an annual average of .\nWinter snowfall does occur, but it is sporadic and rarely accumulates in large amounts: the normal seasonal amount is , but the median is zero. The record snowfall was of accumulation due to a pineapple express on January 25\u201329, 1969. Ice storms, like snowfall, are rare, but occur sporadically.\nThe hottest months are July and August, with a normal monthly mean temperature of , with an average of 16\u00a0days per year reaching . The coolest month is December, with a mean temperature of , and there are 52 mornings per year with a low at or below freezing, and 2 afternoons with highs not exceeding the freezing mark. The coldest daytime high of the year averages , reaching the freezing point.\nEugene's average annual temperature is , and annual precipitation at . Eugene is slightly cooler on average than Portland. Despite being located about south and at an only slightly higher elevation, Eugene has a more continental climate than Portland, less subject to the maritime air that blows inland from the Pacific Ocean via the Columbia River. Eugene's normal annual mean minimum is , compared to in Portland; in August, the gap in the normal mean minimum widens to for Eugene and Portland, respectively. Eugene's warmest night annually averages a modest . Average winter temperatures (and summer high temperatures) are similar for the two cities.\nExtreme temperatures range from , recorded on December 8, 1972, to on June 27, 2021; the record cold daily maximum is , recorded on December 13, 1919, while, conversely, the record warm daily minimum is on July 22, 2006.\nAir quality and allergies.\nEugene is downwind of Willamette Valley grass seed farms. The combination of summer grass pollen and the confining shape of the hills around Eugene make it \"the area of the highest grass pollen counts in the USA (&gt;1,500 pollen grains/m3 of air).\" These high pollen counts have led to difficulties for some track athletes who compete in Eugene. In the Olympic trials in 1972, \"Jim Ryun won the 1,500 after being flown in by helicopter because he was allergic to Eugene's grass seed pollen.\" Further, six-time Olympian Maria Mutola abandoned Eugene as a training area \"in part to avoid allergies\".\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\n2010 census.\nAccording to the 2010 census, Eugene's population was 156,185. The population density was 3,572.2 people per square mile. There were 69,951 housing units at an average density of 1,600 per square mile. Those age 18 and over accounted for 81.8% of the total population.\nThe racial makeup of the city was 85.8% White, 4.0% Asian, 1.4% Black or African American, 1.0% Native American, 0.2% Pacific Islander, and 4.7% from other races.\nHispanics and Latinos of any race accounted for 7.8% of the total population.Of the non-Hispanics, 82% were White, 1.3% Black or African American, 0.8% Native American, 4% Asian, 0.2% Pacific Islander, 0.2% some other race alone, and 3.4% were of two or more races.\nFemales represented 51.1% of the total population, and males represented 48.9%. The median age in the city was 33.8 years.\n2000 census.\nThe census of 2000 showed there were 137,893 people, 58,110 households, and 31,321 families residing in the city of Eugene. The population density was . There were 61,444 housing units at an average density of . The racial makeup of the city was 88.15% White, down from 99.5% in 1950, 3.57% Asian, 1.25% Black or African American, 0.93% Native American, 0.21% Pacific Islander, 2.18% from other races, and 3.72% from two or more races. 4.96% of the population were Hispanic or Latino of any race.\nThere were 58,110 households, of which 25.8% had children under the age of 18 living with them, 40.6% were married couples living together, 9.7% had a female householder with no husband present, and 46.1% were non-families. 31.7% of all households were made up of individuals, and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.27 and the average family size was 2.87. In the city, the population was 20.3% under the age of 18, 17.3% from 18 to 24, 28.5% from 25 to 44, 21.8% from 45 to 64, and 12.1% who were 65 years of age or older. The median age was 33 years. For every 100 females, there were 96.0 males. For every 100 females age 18 and over, there were 94.0 males. The median income for a household in the city was $35,850, and the median income for a family was $48,527. Males had a median income of $35,549 versus $26,721 for females. The per capita income for the city was $21,315. About 8.7% of families and 17.1% of the population were below the poverty line, including 14.8% of those under age 18 and 7.1% of those age 65 or over.\nEconomy.\nEugene's largest employers are PeaceHealth Medical Group, the University of Oregon, and the Eugene School District. Eugene's largest industries are wood products manufacturing and recreational vehicle manufacturing.\nCorporate headquarters for the employee-owned Bi-Mart corporation and family-owned supermarket Market of Choice remain in Eugene.\nMany multinational businesses were launched in Eugene. Some of the most famous include Nike, Taco Time, and Broderbund Software.\nThe footwear repair product Shoe Goo is manufactured by Eclectic Products, based in Eugene.\nRun Gum, an energy gum created for runners, also began its life in Eugene. Run Gum was created by track athlete Nick Symmonds and track and field coach Sam Lapray in 2014.\nBurley Design LLC produces bicycle trailers and was founded in Eugene by Alan Scholz out of a Saturday Market business in 1978. Eugene is also the birthplace and home of Bike Friday bicycle manufacturer Green Gear Cycling.\nOrganically Grown Company, the largest distributor of organic fruits and vegetables in the northwest, started in Eugene in 1978 as a non-profit co-op for organic farmers. Notable local food processors, many of whom manufacture certified organic products, include Golden Temple (Yogi Tea), Merry Hempsters, Springfield Creamery (Nancy's Yogurt), and Mountain Rose Herbs.\nUntil July 2008, Hynix Semiconductor America had operated a large semiconductor plant in west Eugene. In late September 2009, Uni-Chem of South Korea announced its intention to purchase the Hynix site for solar cell manufacturing. However, this deal fell through and as of late 2012, is no longer planned. In 2015, semiconductor manufacturer Broadcom purchased the plant with plans to upgrade and reopen it. The company abandoned these plans and put it up for sale in November 2016.\nLuckey's Club Cigar Store is one of the oldest bars in Oregon. Tad Luckey Sr. purchased it in 1911, making it one of the oldest businesses in Eugene. The \"Club Cigar\", as it was called in the late 19th century, was for many years a men-only salon. It survived both the Great Depression and Prohibition, partly because Eugene was a \"dry town\" before the end of Prohibition.\nThe city has over 25 breweries, offers a variety of dining options with a local focus; the city is surrounded by wineries. The most notable fungi here is the truffle; Eugene hosts the annual Oregon Truffle Festival in January.\nTop employers.\nAccording to Eugene's 2017 Comprehensive Annual Financial Report, the city's top employers are:\nHomelessness.\nEugene has a growing problem with homelessness. The problem has been referenced in popular culture, including in the episode The 30% Iron Chef in Futurama. During the COVID-19 pandemic, the city experienced a controversy over its continuing policy of homeless removal, despite CDC guidelines to not engage in homeless removal.\nArts and culture.\nEugene has a significant population of people in pursuit of alternative ideas and a large original hippie population. Beginning in the 1960s, the countercultural ideas and viewpoints espoused by area native Ken Kesey became established as the seminal elements of the vibrant social tapestry that continue to define Eugene. The Merry Prankster, as Kesey was known, has arguably left the most indelible imprint of any cultural icon in his hometown. He is best known as the author of \"One Flew Over the Cuckoo's Nest\" and as the male protagonist in Tom Wolfe's \"The Electric Kool-Aid Acid Test\".\nIn 2005, the city council unanimously approved a new slogan for the city: \"World's Greatest City for the Arts &amp; Outdoors\". While Eugene has a vibrant arts community for a city its size, and is well situated near many outdoor opportunities, this slogan was frequently criticized by locals as embarrassing and ludicrous. In early 2010, the slogan was changed to \"A Great City for the Arts &amp; Outdoors.\"\nEugene's Saturday Market, open every Saturday from April through November, was founded in 1970 as the first \"Saturday Market\" in the United States. It is adjacent to the Lane County Farmer's Market in downtown Eugene. All vendors must create or grow all their own products. The market reappears as the \"Holiday Market\" between Thanksgiving and New Year's in the Lane County Events Center at the fairgrounds.\nCommunity.\nEugene is noted for its \"community inventiveness.\" Many U.S. trends in community development originated in Eugene. The University of Oregon's participatory planning process, known as The Oregon Experiment, was the result of student protests in the early 1970s. The book of the same name is a major document in modern enlightenment thinking in planning and architectural circles. The process, still used by the university in modified form, was created by Christopher Alexander, whose works also directly inspired the creation of the Wiki. Some research for the book \"A Pattern Language\", which inspired the Design Patterns movement and Extreme Programming, was done by Alexander in Eugene. Not coincidentally, those engineering movements also had origins here. Decades after its publication, \"A Pattern Language\" is still one of the best-selling books on urban design.\nIn the 1970s, Eugene was packed with cooperative and community projects. It still has small natural food stores in many neighborhoods, some of the oldest student cooperatives in the country, and alternative schools have been part of the school district since 1971. The old Grower's Market, downtown near the Amtrak depot, is the only food cooperative in the U.S. with no employees. It is possible to see Eugene's trend-setting non-profit tendencies in much newer projects, such as Square One Villages and the Center for Appropriate Transport. In 2006, an initiative began to create a tenant-run development process for downtown Eugene.\nIn the fall of 2003, neighbors noticed \"an unassuming two-acre remnant orchard tucked into the Friendly Area Neighborhood\" had been put up for sale by its owner, a resident of New York City. Learning a prospective buyer had plans to build several houses on the property, they formed a nonprofit organization called Madison Meadow in June 2004 in order to buy the property and \"preserve it as undeveloped space in perpetuity.\" In 2007 their effort was named Third Best Community Effort by the \"Eugene Weekly\", and by the end of 2008 they had raised enough money to purchase the property.\nThe City of Eugene has an active Neighborhood Program. Several neighborhoods are known for their green activism. Friendly Neighborhood has a highly popular neighborhood garden established on the right of way of a street never built. There are a number of community gardens on public property. Amazon Neighborhood has a former church turned into a community center. Whiteaker hosts a housing co-op that dates from the early 1970s that has re-purposed both their parking lots into food production and play space. An unusual eco-village with natural building techniques and large shared garden can be found in Jefferson Westside neighborhood. A several block area in the River Road Neighborhood is known as a permaculture hotspot with an increasing number of suburban homes trading grass for garden, installing rain water catchment systems, food producing landscapes and solar retrofits. Several sites have planted gardens by removing driveways. Citizen volunteers are working with the City of Eugene to restore a 65-tree filbert grove on public property. There are deepening social and economic networks in the neighborhood.\nMuseums.\nEugene museums include the University of Oregon's Jordan Schnitzer Museum of Art and Museum of Natural and Cultural History, the Oregon Air and Space Museum, Lane County History Museum, Maude Kerns Art Center, Shelton McMurphey Johnson House, and the Eugene Science Center.\nPerforming arts.\nEugene is home to numerous cultural organizations, including the Eugene Symphony (whose previous music directors include Marin Alsop, Giancarlo Guerrero, and Miguel Harth-Bedoya); the Eugene Ballet, a professional full-time touring company; the Eugene Opera, the Eugene Concert Choir, the Bushnell University Community Choir, the Oregon Mozart Players, the Oregon Bach Festival, the Oregon Children's Choir, the Eugene-Springfield Youth Orchestras, Ballet Fantastique and Oregon Festival of American Music. Principal performing arts venues include the Hult Center for the Performing Arts, The John G. Shedd Institute for the Arts (\"The Shedd\"), the McDonald Theatre, and W.O.W. Hall.\nThe University of Oregon School of Music and Dance also attracts world class performers and teaching artists throughout the year, many of whom perform at Beall Concert Hall. The university campus also frequently hosts performances at Matthew Knight Arena and the Erb Memorial Union ballroom.\nA number of live theater groups are based in Eugene, including Free Shakespeare in the Park, Oregon Contemporary Theatre, The Very Little Theatre, Actors Cabaret, LCC Theatre, Rose Children's Theatre, and University Theatre. Each has its own performance venue.\nMusic.\nBecause of its status as a college town, Eugene has been home to many music genres, musicians and bands, ranging from electronic dance music such as dubstep and drum and bass to garage rock, hip hop, folk and heavy metal. Eugene also has growing reggae and street-performing bluegrass and jug band scenes. Multi-genre act the Cherry Poppin' Daddies became a prominent figure in Eugene's music scene and became the house band at Eugene's W.O.W. Hall. In the late 1990s, their contributions to the swing revival movement propelled them to national stardom. Rock band Floater originated in Eugene as did the Robert Cray blues band. Doom metal band YOB is among the leaders of the Eugene heavy music scene.\nEugene is home to \"Classical Gas\" Composer and two-time Grammy award winner Mason Williams who spent his years as a youth living between his parents in Oakridge, Oregon and Oklahoma. Mason Williams puts on a yearly Christmas show at the Hult center for performing arts with a full orchestra produced by author, audio engineer and University of Oregon professor Don Latarski.\nDick Hyman, noted jazz pianist and musical director for many of Woody Allen's films, designs and hosts the annual Now Hear This! jazz festival at the Oregon Festival of American Music (OFAM). OFAM and the Hult Center routinely draw major jazz talent for concerts.\nEugene is also home to a large Zimbabwean music community, home to the Kutsinhira Cultural Arts Center \"dedicated to the music and people of Zimbabwe\". It was founded in 1990.\nThe city of Eugene is mentioned in the Johnny Cash song \"Lumberjack\".\nVisual arts.\nEugene's visual arts community is supported by over 20 private art galleries and several organizations, including Maude Kerns Art Center, Lane Arts Council, DIVA (the Downtown Initiative for the Visual Arts) and the Eugene Glass School.\nIn 2015 installations from a group of Eugene-based artists known as Light At Play were showcased in several events around the world as part of the International Year of Light, including displays at the Smithsonian and the National Academy of Sciences.\nFilm.\nThe Eugene area has been used as a filming location for several Hollywood films, most famously for 1978's \"National Lampoon's Animal House\", which was also filmed in nearby Cottage Grove. John Belushi had the idea for the film \"The Blues Brothers\" during filming of \"Animal House\" when he happened to meet Curtis Salgado at what was then the Eugene Hotel.\n\"Getting Straight\", starring Elliott Gould and Candice Bergen, was filmed at Lane Community College in 1969. As the campus was still under construction at the time, the \"occupation scenes\" were easier to shoot.\nThe \"Chicken Salad on Toast\" scene in the 1970 Jack Nicholson movie \"Five Easy Pieces\" was filmed at the Denny's restaurant at the southern I-5 freeway interchange near Glenwood. Nicholson directed the 1971 film \"Drive, He Said\" in Eugene.\n\"How to Beat the High Cost of Living\", starring Jane Curtin, Jessica Lange and Susan St. James, was filmed in Eugene in the fall of 1979. Locations visible in the film include Valley River Center (which is a driving force in the plot), Skinner Butte and Ya-Po-Ah Terrace, the Willamette River and River Road Hardware.\nSeveral track and field movies have used Eugene as a setting and/or a filming location. \"Personal Best\", starring Mariel Hemingway, was filmed in Eugene in 1982. The film centered on a group of women who are trying to qualify for the Olympic track and field team. Two track and field movies about the life of Steve Prefontaine, \"Prefontaine\" and \"Without Limits\", were released within a year of each other in 1997\u20131998. Kenny Moore, Eugene-trained Olympic runner and co-star in \"Prefontaine\", co-wrote the screenplay for \"Without Limits\". \"Prefontaine\" was filmed in Washington because the \"Without Limits\" production bought out Hayward Field for the summer to prevent its competition from shooting there. Kenny Moore also wrote a biography of Bill Bowerman, played in \"Without Limits\" by Donald Sutherland back in Eugene 20 years after he had appeared in \"Animal House\". Moore had also had a role in \"Personal Best\".\n\"Stealing Time\", a 2003 independent film, was partially filmed in Eugene. When the film premiered in June 2001 at the Seattle International Film Festival, it was titled \"Rennie's Landing\" after a popular bar near the University of Oregon campus. The title was changed for its DVD release.\nThe 2006 film \"Zerophilia\" was partly filmed in Eugene.\nThe 2016 film \"Tracktown\" was about a distance runner training for the Olympics was filmed in Eugene.\nReligion.\nReligious institutions of higher learning in Eugene include Bushnell University and New Hope Christian College. Bushnell University (formerly Northwest Christian University), founded in 1895, has ties with the Christian Church (Disciples of Christ). New Hope Christian College (formerly Eugene Bible College) originated with the Bible Standard Conference in 1915, which joined with Open Bible Evangelistic Association to create Open Bible Standard Churches in 1932. Eugene Bible College was started from this movement by Fred Hornshuh in 1925.\nThere are two Eastern Orthodox Church parishes in Eugene: St John the Wonderworker Orthodox Christian Church in the Historic Whiteaker Neighborhood and Saint George Greek Orthodox Church.\nThere are six Roman Catholic parishes in Eugene as well: St. Mary Catholic Church, St. Jude Catholic Church, St. Mark Catholic Church, St. Peter Catholic Church, St. Paul Catholic Church, and St. Thomas More Catholic Church.\nEugene also has a Ukrainian Catholic Church named Nativity of the Mother of God.\nThere is a mainline Protestant contingency in the city as well\u2014such as the largest of the Lutheran Churches, Central Lutheran near the U of O Campus and the Episcopal Church of the Resurrection.\nThe Eugene area has a sizeable LDS Church presence, with three stakes, consisting of 23 congregations (wards and branches). The Church of Jesus Christ announced plans in April 2020 to build a temple in Eugene.\nThe greater Eugene-Springfield area also has a Jehovah's Witnesses presence with five Kingdom Halls, several having multiple congregations in one Kingdom Hall.\nThe Reconstructionist Temple Beth Israel is Eugene's largest Jewish congregation. It was also, for many decades, Eugene's only synagogue, until Orthodox members broke away in 1992 and formed \"Congregation Ahavas Torah\".\nEugene has a community of some 140 Sikhs, who have established a Sikh temple.\nThe 340-member congregation of the Unitarian Universalist Church in Eugene (UUCE) purchased the former Eugene Scottish Rite Temple in May 2010, renovated it, and began services there in September 2012.\nSaraha Nyingma Buddhist Temple in Eugene opened in 2012 in the former site of the Unitarian Universalist Church.\nThe First Congregational Church, UCC is a large progressive Christian Church with a long history of justice focused ministries and a very active membership. Three years ago, the congregation coordinated with the Connections Program of the St Vincent DePaul organization to provide transitional homes for two unhoused families on the church's property. Through life - skills support and training and a more stable housing situation these families are then able to make their way into independent living.\nSports.\nEugene markets itself as \"Track Town USA\". There are close links between the University of Oregon's successful track &amp; field program, the Oregon Track Club, and Nike, Inc, who were founded by University of Oregon track athlete Phil Knight and his coach, Bill Bowerman.\nEugene's miles of running trails, through its unusually large park system, are among the most extensive in the U.S. Notable trails include Pre's Trail in Alton Baker Park, Rexius Trail, the Adidas Oregon Trail, and the Ridgeline Trail. There is also an extensive network of trails along the Willamette River that reaches into neighboring Springfield, as well as along Amazon Creek in the southern and western parts of town.\nJogging was introduced to the U.S. through Eugene, brought from New Zealand by Bill Bowerman, who wrote the best-selling book \"Jogging\", and coached the champion University of Oregon track and cross country teams. During Bowerman's tenure, his \"Men of Oregon\" won 24 individual NCAA titles, including titles in 15 out of the 19 events contested. During Bowerman's 24 years at Oregon, his track teams finished in the top ten at the NCAA championships 16 times, including four team titles (1962, '64, '65, '70), and two second-place trophies. His teams also posted a dual meet record of 114\u201320.\nBowerman also invented the waffle sole for running shoes in Eugene, and with Oregon alumnus Phil Knight founded shoe giant Nike. The city has dozens of running clubs. The climate is cool and temperate, good both for jogging and record-setting. Eugene is home to the University of Oregon's Hayward Field track, which hosts numerous collegiate and amateur track and field meets throughout the year, most notably the Prefontaine Classic. Hayward Field was host to the 2004 AAU Junior Olympic Games, the 1989 World Masters Athletics Championships, the track and field events of the 1998 World Masters Games, the 2006 Pacific-10 track and field championships, the 1971, 1975, 1986, 1993, 1999, 2001, 2009, and 2011 USA Track &amp; Field Outdoor Championships and the 1972, 1976, 1980, 2008, 2012, and 2016 U.S. Olympic trials. Eugene was the host of the delayed 2021 World Athletics Championships. The city bid for the 2019 event but lost narrowly to Doha, Qatar.\nEugene's Oregon Ducks are part of the Big Ten Conference. American football is especially popular, with intense rivalries between the Ducks and both the Oregon State University Beavers and the University of Washington Huskies. Autzen Stadium is home to Duck football, with a seating capacity of 54,000 but has had over 60,000 with standing room only. The basketball arena, McArthur Court, was built in 1926. The arena was replaced by the Matthew Knight Arena in late 2010.\nThe Nationwide Tour's golfing event Oregon Classic takes place at Shadow Hills Country Club, just north of Eugene. The event has been played every year since 1998, except in 2001 when it was slated to begin the day after the 9/11 terrorist attacks. The top 20 players from the Nationwide Tour are promoted to the PGA Tour for the following year.\nEugene is also home to the Eugene Emeralds, a short-season Class A minor-league baseball team. The \"Ems\" play their home games in PK Park, also the home of the University of Oregon baseball team. The Eugene Jr. Generals, a Tier III Junior \"A\" ice hockey team belonging to the Northern Pacific Hockey League (NPHL) consisting of 8 teams throughout Oregon and Washington, plays at the Lane County Ice Center. Lane United FC, a soccer club that participates in the Northwest Division of USL League Two, was founded in 2013 and plays its home games at Civic Park.\nThe following table lists some sports clubs in Eugene and their usual home venue:\nParks and recreation.\nSpencer Butte Park at the southern edge of town provides access to Spencer Butte, a dominant feature of Eugene's skyline. Hendricks Park, situated on a knoll to the east of downtown, is known for its rhododendron garden and nearby memorial to Steve Prefontaine, known as Pre's Rock, where the legendary University of Oregon runner was killed in an auto accident. Alton Baker Park, next to the Willamette River, contains Pre's Trail. Also next to the Willamette are Skinner Butte Park and the Owen Memorial Rose Garden, which contains more than 4,500 roses of over 400 varieties, as well as the 150-year-old Black Tartarian Cherry tree, an Oregon Heritage Tree.\nThe city of Eugene maintains an urban forest. The University of Oregon campus is an arboretum, with over 500 species of trees. The city operates and maintains scenic hiking trails that pass through and across the ridges of a cluster of hills in the southern portion of the city, on the fringe of residential neighborhoods. Some trails allow biking, and others are for hikers and runners only.\nThe nearest ski resort, Willamette Pass, is one hour from Eugene by car. On the way, along Oregon Route 58, are several reservoirs and lakes, the Oakridge mountain bike trails, hot springs, and waterfalls within Willamette National Forest. Eugene residents also frequent the Hoodoo and Mount Bachelor ski resorts. The Three Sisters Wilderness, the Oregon Dunes National Recreation Area, and Smith Rock are just a short drive away.\nGovernment.\nIn 1944, Eugene adopted a council\u2013manager form of government, replacing the day-to-day management of city affairs by the part-time mayor and volunteer city council with a full-time professional city manager. The subsequent history of Eugene city government has largely been one of the dynamics\u2014often contentious\u2014between the city manager, the mayor and city council.\nAccording to statute, all Eugene and Lane County elections are officially non-partisan, with a primary containing all candidates in May. If a candidate gets more than 50% of the vote in the primary, they win the election outright, otherwise the top two candidates face off in a November runoff. This allows candidates to win seats during the lower-turnout primary election.\nThe mayor of Eugene is Kaarin Knudson, who took office in January 2025. Recent mayors include Edwin Cone (1958\u201369), Les Anderson (1969\u201377) Gus Keller (1977\u201384), Brian Obie (1985\u201388), Jeff Miller (1989\u201392), Ruth Bascom (1993\u201396), Jim Torrey (1997\u20132004), Kitty Piercy (2005\u20132017), and Lucy Vinis (2017\u201325).\nEugene City Council.\nMayor: Kaarin Knudson\nPublic safety.\nThe Eugene Police Department is the city's law enforcement and public safety agency. The Lane County Sheriff's Office also has its headquarters in Eugene.\nThe University of Oregon is served by the University of Oregon Police Department, and Eugene Police Department also has a police station in the West University District near campus. Lane Community College is served by the Lane Community College Public Safety Department. The Oregon State Police have a presence in the rural areas and highways around the Eugene metro area. The LTD downtown station, and the EmX lines are patrolled by LTD Transit Officers. Since 1989 the mental health crisis intervention non-governmental agency CAHOOTS has responded to Eugene's mental health 911 calls.\nEugene-Springfield Fire Department is the agency responsible for emergency medical services, fire suppression, HAZMAT operations and water/Confined spaces rescues in the combined Eugene-Springfield metropolitan area.\nEugene used to have an ordinance which prohibited car horn usage for non-driving purposes. After several residents were cited for this offense during the anti-Gulf War demonstrations in January 1991, the city was taken to court and in 1992 the Oregon Court of Appeals overturned the ordinance, finding it unconstitutionally vague. Eugene City Hall was abandoned in 2012 for reasons of structural integrity, energy efficiency, and obsolete size. Various offices of city government became tenants in eight other buildings.\nPolitics.\nBeing the largest city by far in Lane County, Eugene's voters almost always decide the county's partisan tilt. While Eugene has historically been a counter-culture-heavy and left-leaning college town, the county's partisan leanings have intensified in recent decades, mirroring the general polarization of Oregon voters along urban (pro-Democratic) and rural (pro-Republican) lines.\nLane County voted for Bernie Sanders over eventual 2016 nominee Hillary Clinton by 60.6-38.1%, and Eugene offered Sanders an even larger share of its vote.\nEducation.\nTertiary education.\nEugene is home to the University of Oregon. Other institutions of higher learning include Bushnell University, Lane Community College, New Hope Christian College, Gutenberg College, and Pacific University's Eugene campus.\nAll of Lane County is in the Lane Community College district.\nSchools.\nThe Eugene School District covers around 85% of the Eugene city limits. The remainder of Eugene's northwestern neighborhoods are in the Bethel School District.\nThe Eugene School District includes four full-service high schools (Churchill, North Eugene, Sheldon, and South Eugene) and many alternative education programs, such as international schools and charter schools. Foreign language immersion programs in the district are available in Spanish, French, Chinese, and Japanese.\nThe Bethel School District serves children in the Bethel neighborhood on the northwest edge of Eugene. The district is home to the traditional Willamette High School and the alternative Kalapuya High School. There are 11 schools in this district.\nEugene also has several private schools, including the Eugene Waldorf School, the Outdoor High School, Eugene Montessori, Far Horizon Montessori, Eugene Sudbury School, Wellsprings Friends School, Oak Hill School, and The Little French School.\nParochial schools in Eugene include Marist Catholic High School, O'Hara Catholic Elementary School, Eugene Christian School, and St. Paul Parish School.\nLibraries.\nThe largest library in Oregon is the University of Oregon's Knight Library, with collections totaling more than 3 million volumes and over 100,000 audio and video items. The Eugene Public Library moved into a new, larger building downtown in 2002. The four-story library is an increase from . There are also two branches of the Eugene Public Library, the Sheldon Branch Library in the neighborhood of Cal Young/Sheldon, and the Bethel Branch Library, in the neighborhood of Bethel. Eugene also has the Lane County Law Library.\nMedia.\nPrint.\nThe largest newspaper serving the area is \"The Register-Guard\", a daily newspaper with a circulation of about 70,000, published independently by the Baker family of Eugene until 2018, before being acquired by GateHouse Media, (now owned by Gannett Company). Other newspapers serving the area include the \"Eugene Weekly\", the \"Emerald\", the student-run independent newspaper at the University of Oregon, now published on Mondays and Thursdays;\"The Torch\", the student-run newspaper at Lane Community College, the \"Ignite\", the newspaper at New Hope Christian College and \"The Beacon Bolt,\" the student-run newspaper at Bushnell University. \"Eugene Magazine\", \"Lifestyle Quarterly\", \"Eugene Living\", and \"Sustainable Home and Garden\" magazines also serve the area. \"Adelante Latino\" is a Spanish language newspaper in Eugene that serves all of Lane County.\nTelevision.\nLocal television stations include KMTR (NBC/The CW), KVAL (CBS), KLSR-TV (Fox), KEVU-CD, KEZI (ABC), KEPB (PBS), and KTVC (independent).\nRadio.\nThe local NPR affiliates are KOPB and KLCC. Radio station KRVM-AM is an affiliate of Jefferson Public Radio, based at Southern Oregon University. The Pacifica Radio affiliate is the University of Oregon student-run radio station, KWVA. Additionally, the community supports two other radio stations: KWAX (classical) and KRVM-FM (alternative).\nAM stations\nFM stations\nInfrastructure.\nTransportation.\nBus.\nLane Transit District (LTD), a public transportation agency formed in 1970, covers of Lane County, including Creswell, Cottage Grove, Junction City, Veneta, and Blue River. Operating more than 90 buses during peak hours, LTD carries riders on 3.7 million trips every year. LTD also operates a bus rapid transit line that runs between Eugene and Springfield\u2014Emerald Express (EmX)\u2014much of which runs in its own lane, with stations providing for off-board fare payment. LTD's main terminus in Eugene is at the Eugene Station. LTD also offers paratransit.\nGreyhound Lines provides service between Los Angeles and Portland on the I-5 corridor.\nCycling.\nCycling is popular in Eugene and many people commute via bicycle. Summertime events and festivals frequently have valet bicycle parking corrals that are often filled to capacity by three hundred or more bikes. Many people commute to work by bicycle every month of the year. PeaceHealth Rides, a bike share system formerly operated by Uber subsidiary JUMP, and currently operated by non-profit Cascadia Mobility, offers 300 city-owned bicycles available to the public for a small fee. Bike trails take commuting and recreational bikers along the Willamette River past a scenic rose garden, along Amazon Creek, through the downtown, and through the University of Oregon campus. Eugene is close to many popular mountain bike trails, and Disciples of Dirt is the local mountain bike club that organizes group rides and promotes trail stewardship.\nIn 2009, the League of American Bicyclists cited Eugene as 1 of 10 \"Gold-level\" cities in the U.S. because of its \"remarkable commitments to bicycling.\" In 2010, \"Bicycling\" magazine named Eugene the 5th most bike-friendly city in America. The U.S. Census Bureau's annual American Community Survey reported that Eugene had a bicycle commuting mode share of 7.3% in 2011, the fifth highest percentage nationwide among U.S. cities with 65,000 people or more, and 13 times higher than the national average of 0.56%.\nRail.\nThe 1908 Amtrak depot downtown was restored in 2004; it is the southern terminus for two daily runs of the Amtrak \"Cascades\", and a stop along the route in each direction for the daily \"Coast Starlight\".\nAir travel.\nAir travel is served by the Eugene Airport, also known as Mahlon Sweet Field, which is the fifth largest airport in the Northwest and second largest airport in Oregon. The Eugene Metro area also has numerous private airports. The Eugene Metro area also has several heliports, such as the Sacred Heart Medical Center Heliport and Mahlon Sweet Field Heliport, and many single helipads.\nHighways.\nHighways traveling within and through Eugene include:\nUtilities.\nEugene is the home of Oregon's largest publicly owned water and power utility, the Eugene Water &amp; Electric Board (EWEB). EWEB got its start in the first decade of the 20th century, after an epidemic of typhoid found in the groundwater supply. The City of Eugene condemned Eugene's private water utility and began treating river water (first the Willamette; later the McKenzie) for domestic use. EWEB got into the electric business when power was needed for the water pumps. Excess electricity generated by the EWEB's hydropower plants was used for street lighting.\nNatural gas service is provided by NW Natural.\nWastewater treatment services are provided by the Metropolitan Wastewater Management Commission, a partnership between the Cities of Eugene and Springfield and Lane County.\nHealthcare.\nTwo hospitals serve the Eugene-Springfield area. McKenzie-Willamette Medical Center and Sacred Heart Medical Center at RiverBend are in Springfield. Oregon Medical Group, a primary care based multi-specialty group, operates several clinics in Eugene, as does PeaceHealth Medical Group. White Bird Clinic provides a broad range of health and human services, including low-cost clinics. The Volunteers in Medicine &amp; Occupy Medical clinics provide free medical and mental care to low-income adults without health insurance.\nEugene is one of the few municipalities in the US that does not fluoridate its water supply.\nSister cities.\nEugene has four sister cities:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9624", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=9624", "title": "Early Music", "text": ""}
{"id": "9625", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=9625", "title": "Eigenstate", "text": ""}
{"id": "9627", "revid": "31831", "url": "https://en.wikipedia.org/wiki?curid=9627", "title": "Elizabeth Barrett Browning", "text": "English poet (1806\u20131861)\nElizabeth Barrett Browning (n\u00e9e Moulton-Barrett; 6 March 1806 \u2013 29 June 1861) was an English poet of the Victorian era, popular in Britain and the United States during her lifetime and frequently anthologised after her death. Her work received renewed attention following the feminist scholarship of the 1970s and 1980s, and greater recognition of women writers in English. Born in County Durham, the eldest of 12 children, Elizabeth Barrett wrote poetry from the age of eleven. Her mother's collection of her poems forms one of the largest extant collections of juvenilia by any English writer. At 15, she became ill, suffering intense head and spinal pain for the rest of her life. Later in life, she also developed lung problems, possibly tuberculosis. She took laudanum for the pain from an early age, which is likely to have contributed to her frail health.\nIn the 1840s, Elizabeth was introduced to literary society through her distant cousin and patron John Kenyon. Her first adult collection of poems was published in 1838, and she wrote prolifically from 1841 to 1844, producing poetry, translation, and prose. She campaigned for the abolition of slavery, and her work helped influence reform in child labour legislation. Her prolific output made her a rival to Tennyson as a candidate for poet laureate on the death of Wordsworth. Elizabeth's volume \"Poems\" (1844) brought her great success, attracting the admiration of the writer Robert Browning. Their correspondence, courtship, and marriage were carried out in secret, for fear of her father's disapproval. Following the wedding, she was indeed disinherited by her father. In 1846, the couple moved to Italy, where she lived for the rest of her life. Elizabeth died in Florence in 1861. A collection of her later poems was published by her husband shortly after her death.\nThey had a son, known as \"Pen\" (Robert Barrett, 1849\u20131912). Pen devoted himself to painting until his eyesight began to fail later in life. He also built a large collection of manuscripts and memorabilia of his parents, but because he died intestate, it was sold by public auction to various bidders and then scattered upon his death. The Armstrong Browning Library has recovered some of his collection, and it houses the world's largest collection of Browning memorabilia. Elizabeth's work had a major influence on prominent writers of the day, including the American poets Edgar Allan Poe and Emily Dickinson. She is remembered for such poems as \"How Do I Love Thee?\" (Sonnet 43, 1845) and \"Aurora Leigh \"(1856).\nLife and career.\nFamily background.\nElizabeth Barrett had both maternal and paternal family who profited from slavery. Her father's family had lived in the colony of Jamaica since 1655, though her father chose to raise his family in England, while his business enterprises remained in Jamaica. Their wealth derived primarily from the ownership of slave plantations in the British West Indies. Edward Barrett owned of land in the estates of Cinnamon Hill, Cornwall, Cambridge, and Oxford in northern Jamaica. \nElizabeth's maternal grandfather owned sugar plantations, sugar cane mills, glassworks and merchant ships, which traded between Jamaica and Newcastle upon Tyne.\nThe family wished to hand down their name, stipulating that Barrett always should be held as a surname. In some cases, inheritance was given on condition that the name was used by the beneficiary; the British upper class had long encouraged this sort of name changing. Given this strong tradition, Elizabeth used \"Elizabeth Barrett Moulton Barrett\" on legal documents, and before she was married, she often signed herself \"Elizabeth Barrett Barrett\" or \"EBB\" (initials which she was able to keep after her wedding).\nEarly life.\nElizabeth Barrett Moulton-Barrett was born on (it is supposed) 6 March 1806 in Coxhoe Hall, between the villages of Coxhoe and Kelloe in County Durham, England. Her parents were Edward Barrett Moulton-Barrett and Mary Graham Clarke. However, biographers have suggested that, when she was christened on 9 March, she was already three or four months old, and that this was concealed because her parents had married only on 14 May 1805. Although she had already been baptised by a family friend in that first week of her life, she was baptised again, more publicly, on 10 February 1808 at Kelloe parish church, at the same time as her younger brother, Edward (known as Bro). He had been born in June 1807, 15 months after Elizabeth's stated date of birth. A private christening might seem unlikely for a family of standing, and while Bro's birth was celebrated with a holiday on the family's Caribbean plantations, Elizabeth's was not.\nElizabeth was the eldest of 12 children (eight boys and four girls). Eleven lived to adulthood; one daughter died at the age of 3, when Elizabeth was 8. The children all had nicknames: Elizabeth was Ba. She rode her pony, went for family walks and picnics, socialised with other county families, and participated in home theatrical productions. Unlike her siblings, she immersed herself in books as often as she could get away from the social rituals of her family.\nIn 1809, the family moved to Hope End, a estate near the Malvern Hills in Ledbury, Herefordshire. Her father converted the Georgian house into stables and built a mansion of opulent Turkish design, which his wife described as something from the \"Arabian Nights' Entertainments\".\nThe interior's brass balustrades, mahogany doors inlaid with mother-of-pearl, and finely carved fireplaces were eventually complemented by lavish landscaping: ponds, grottos, kiosks, an ice house, a hothouse, and a subterranean passage from house to gardens. Her time at Hope End inspired her in later life to write \"Aurora Leigh\" (1856), her most ambitious work, which went through more than 20 editions by 1900, but none from 1905 to 1978.\nShe was educated at home and tutored by Daniel McSwiney with her oldest brother. She began writing verses at the age of four. During the Hope End period, she was an intensely studious, precocious child. She claimed that she was reading novels at age 6, having been entranced by Pope's translations of Homer at age 8, studying Greek at age 10, and writing her own Homeric epic \"\" at age 11.\nIn 1820, Mr Barrett privately published \"The Battle of Marathon\", an epic-style poem, but all copies remained within the family. Her mother compiled the child's poetry into collections of \"Poems by Elizabeth B. Barrett\". Her father called her the \"Poet Laureate of Hope End\" and encouraged her work. The result is one of the larger collections of juvenilia of any English writer. Mary Russell Mitford described the young Elizabeth at this time as having \"a slight, delicate figure, with a shower of dark curls falling on each side of a most expressive face; large, tender eyes, richly fringed by dark eyelashes, and a smile like a sunbeam.\"\nAt about this time, Elizabeth began to battle an illness, which the medical science of the time was unable to diagnose. All three sisters came down with the syndrome, but it lasted only with Elizabeth. She had intense head and spinal pain with loss of mobility. Various biographies link this to a riding accident at the time (she fell while trying to dismount a horse), but there is no evidence to support the link. Sent to recover at the Gloucester spa, she was treated \u2013 in the absence of symptoms supporting another diagnosis \u2013 for a spinal problem. This illness continued for the rest of her life, and it is believed to be unrelated to the lung disease that she developed in 1837.\nShe began to take opiates for the pain, laudanum (an opium concoction) followed by morphine, then commonly prescribed. She became dependent on them for much of her adulthood; the use from an early age may well have contributed to her frail health. Biographers such as Alethea Hayter have suggested this dependency contributed to the wild vividness of her imagination and the poetry that it produced.\nBy 1821, she had read Mary Wollstonecraft's \"A Vindication of the Rights of Woman\" (1792), and she become a passionate supporter of Wollstonecraft's political ideas. The child's intellectual fascination with the classics and metaphysics was reflected in a religious intensity that she later described as \"not the deep persuasion of the mild Christian but the wild visions of an enthusiast.\" The Barretts attended services at the nearest Dissenting chapel, and Edward was active in Bible and missionary societies.\nElizabeth's mother died in 1828, and is buried at St Michael's Church, Ledbury, next to her daughter Mary. Sarah Graham-Clarke, Elizabeth's aunt, helped to care for the children, and she had clashes with Elizabeth's strong will. In 1831, Elizabeth's grandmother, Elizabeth Moulton, died. Following lawsuits and the abolition of slavery, Mr Barrett incurred great financial and investment losses that forced him to sell Hope End. Although the family was never poor, the place was seized and sold to satisfy creditors. Always secretive in his financial dealings, he would not discuss his situation, and the family was haunted by the idea that they might have to move to Jamaica.\nFrom 1833 to 1835, she was living with her family at Belle Vue in Sidmouth. The site has been renamed Cedar Shade and redeveloped. A blue plaque at the entrance to the site attests to its previous existence. In 1838, some years after the sale of Hope End, the family settled at 50 Wimpole Street, Marylebone, London.\nDuring 1837\u20131838, the poet was struck with illness again, with symptoms suggesting tuberculous ulceration of the lungs. The same year, at her physician's insistence, she moved from London to Torquay on the Devonshire coast. Her former home forms part of the Regina Hotel. Two tragedies then struck. In February 1840, her brother Samuel died of a fever in Jamaica, then her favourite brother Edward (Bro) was drowned in a sailing accident in Torquay in July. These events had a serious effect on her already fragile health. She felt guilty as her father had disapproved of Edward's trip to Torquay. She wrote to Mitford: \"That was a very near escape from madness, absolute hopeless madness\". The family returned to Wimpole Street in 1841.\nSuccess.\nAt Wimpole Street, Elizabeth spent most of her time in her upstairs room. Her health began to improve, but she saw few people other than her immediate family. One of those was John Kenyon, a wealthy friend and distant cousin of the family and patron of the arts. She received comfort from a spaniel named Flush, a gift from Mary Mitford. (Virginia Woolf later fictionalised the life of the dog, making him the protagonist of her 1933 novel \"\").\nFrom 1841 to 1844, Elizabeth was prolific in poetry, translation, and prose. The poem \"The Cry of the Children\", published in 1843 in \"Blackwood's\", condemned child labour and helped bring about child-labour reforms by raising support for Lord Shaftesbury's Ten Hours Bill (1844). About the same time, she contributed critical prose pieces to Richard Henry Horne's \"A New Spirit of the Age\", including a laudatory essay on Thomas Carlyle.\nIn 1844, she published the two-volume \"Poems\", which included \"A Drama of Exile\", \"A Vision of Poets\", and \"Lady Geraldine's Courtship\", and two substantial critical essays for 1842 issues of \"The Athenaeum\". A self-proclaimed \"adorer of Carlyle\", she sent a copy to him as \"a tribute of admiration &amp; respect\", which began a correspondence between them. \"Since she was not burdened with any domestic duties expected of her sisters, Barrett Browning could now devote herself entirely to the life of the mind, cultivating an enormous correspondence, reading widely\". Her prolific output made her a rival to Tennyson as a candidate for poet laureate in 1850 on the death of Wordsworth.\nA Royal Society of Arts blue plaque commemorates Elizabeth at 50 Wimpole Street.\nRobert Browning and Italy.\nHer 1844 volume \"Poems\" made her one of the more popular writers in the country and inspired Robert Browning to write to her. He wrote \"I love your verses with all my heart, dear Miss Barrett,\" praising their \"fresh strange music, the affluent language, the exquisite pathos and true new brave thought.\"\nKenyon arranged for Browning to meet Elizabeth on 20 May 1845, in her rooms, and so began one of the most famous courtships in literature. Elizabeth had produced a large amount of work, but Browning had a great influence on her subsequent writing as did she on his: Two of Barrett's most famous pieces were written after she met Browning, \"Sonnets from the Portuguese\" and \"Aurora Leigh\". Robert's \"Men and Women\" is also a product of that time.\nSome critics state that her activity was, in some ways, in decay before she met Browning: \"Until her relationship with Robert Browning began in 1845, Barrett's willingness to engage in public discourse about social issues and about aesthetic issues in poetry, which had been so strong in her youth, gradually diminished, as did her physical health. As an intellectual presence and a physical being, she was becoming a shadow of herself.\"\nThe courtship and marriage between Robert Browning and Elizabeth were made secretly as she knew her father would disapprove. After a private marriage at St Marylebone Parish Church, they honeymooned in Paris and then moved to Italy in September 1846, which became their home almost continuously until her death. Elizabeth's loyal lady's maid Elizabeth Wilson witnessed the marriage and accompanied the couple to Italy.\nMr Barrett disinherited Elizabeth as he did each of his children who married. Elizabeth had foreseen her father's anger but had not anticipated her brothers' rejection. As Elizabeth had some money of her own, the couple were reasonably comfortable in Italy. The Brownings were well respected and even famous. Elizabeth grew stronger, and in 1849, at the age of 43, between four miscarriages, she gave birth to a son, Robert Wiedeman Barrett Browning, whom they called Pen. Their son later married, but had no legitimate children.\nAt her husband's insistence, Elizabeth's second edition of \"Poems\" included her love sonnets; as a result, her popularity increased (as did critical regard), and her artistic position was confirmed. During the years of her marriage, her literary reputation far surpassed that of her poet-husband; when visitors came to their home in Florence, she was invariably the greater attraction.\nThe couple came to know a wide circle of artists and writers, including William Makepeace Thackeray, sculptor Harriet Hosmer (who, she wrote, seemed to be the \"perfectly emancipated female\") and Harriet Beecher Stowe. In 1849, she met Margaret Fuller; Carlyle in 1851; French novelist George Sand in 1852, whom she had long admired. Among her intimate friends in Florence was the writer Isa Blagden, whom she encouraged to write novels. They met Alfred Tennyson in Paris, and John Forster, Samuel Rogers and the Carlyles in London, later befriending Charles Kingsley and John Ruskin.\nDecline and death.\nAfter the death of an old friend, G. B. Hunter, and then of her father, Barrett Browning's health started to deteriorate. The Brownings moved from Florence to Siena, residing at the \"Villa Alberti\". Engrossed in Italian politics, she issued a small volume of political poems titled \"Poems before Congress\" (1860) \"most of which were written to express her sympathy with the Italian cause after the outbreak of fighting in 1859\". They caused a furore in Britain, and the conservative magazines \"Blackwood's\" and the \"Saturday Review\" labelled her a fanatic. She dedicated this book to her husband. Her last work was \"A Musical Instrument\", published posthumously.\nBarrett Browning's sister Henrietta died in November 1860. The couple spent the winter of 1860\u20131861 in Rome where Barrett Browning's health deteriorated, and they returned to Florence in early June 1861. She became gradually weaker, using morphine to ease her pain. She died on 29 June 1861 in her husband's arms. Browning said that she died \"smilingly, happily, and with a face like a girl's...Her last word was...'Beautiful' \". She was buried in the Protestant English Cemetery of Florence. \"On Monday July 1 the shops in the area around Casa Guidi were closed, while Elizabeth was mourned with unusual demonstrations.\" The nature of her illness is still unclear. Some modern scientists speculate her illness may have been hypokalemic periodic paralysis, a genetic disorder that causes weakness and many of the other symptoms she described.\nPublications.\nBarrett Browning's first known poem \"On the Cruelty of Forcement to Man\" was written at the age of 6 or 8. The manuscript, which protests against impressment, is in the Berg Collection of the New York Public Library; the exact date is controversial because the \"2\" in the date 1812 is written over something else that is scratched out.\nHer first independent publication was \"Stanzas Excited by Reflections on the Present State of Greece\" in \"The New Monthly Magazine\" of May 1821; followed two months later by \"Thoughts Awakened by Contemplating a Piece of the Palm which Grows on the Summit of the Acropolis at Athens\".\nHer first collection of poems, \"An Essay on Mind, with Other Poems,\" was published in 1826 and reflected her passion for Byron and Greek politics. Its publication drew the attention of Hugh Stuart Boyd, a blind scholar of the Greek language, and of Uvedale Price, another Greek scholar, with whom she maintained sustained correspondence. Among other neighbours was Mrs James Martin from Colwall, with whom she corresponded throughout her life. Later, at Boyd's suggestion, she translated Aeschylus' \"Prometheus Bound\" (published in 1833; retranslated in 1850). During their friendship, Barrett studied Greek literature, including Homer, Pindar and Aristophanes.\nElizabeth opposed slavery and published two poems highlighting the barbarity of the institution and her support for the abolitionist cause: \"The Runaway Slave at Pilgrim's Point\" and \"A Curse for a Nation\". The first depicts an enslaved woman whipped, raped, and made pregnant cursing her enslavers. Elizabeth declared herself glad that the slaves were \"virtually free\" when the Slavery Abolition Act passed in the British Parliament despite the fact that her father believed that abolition would ruin his business.\nThe date of publication of these poems is in dispute, but her position on slavery in the poems is clear and may have led to a rift between Elizabeth and her father. She wrote to John Ruskin in 1855 \"I belong to a family of West Indian slaveholders, and if I believed in curses, I should be afraid\". Her father and uncle were unaffected by the Baptist War (1831\u20131832) and continued to own slaves until passage of the Slavery Abolition Act.\nIn London, John Kenyon introduced Elizabeth to literary figures including William Wordsworth, Mary Russell Mitford, Samuel Taylor Coleridge, Alfred Tennyson and Thomas Carlyle. Elizabeth continued to write, contributing \"The Romaunt of Margaret\", \"The Romaunt of the Page\", \"The Poet's Vow\" and other pieces to various periodicals. She corresponded with other writers, including Mary Russell Mitford, who became a close friend and who supported Elizabeth's literary ambitions.\nIn 1838 \"The Seraphim and Other Poems\" appeared, the first volume of Elizabeth's mature poetry to appear under her own name.\n\"Sonnets from the Portuguese\" was published in 1850. There is debate about the origin of the title. Some say it refers to the series of sonnets of the 16th-century Portuguese poet Lu\u00eds de Cam\u00f5es. However, \"my little Portuguese\" was a pet name that Browning had adopted for Elizabeth and this may have some connection.\nThe verse-novel \"Aurora Leigh\", her most ambitious and perhaps the most popular of her longer poems, appeared in 1856. It is the story of a female writer making her way in life, balancing work and love, and based on Elizabeth's own experiences. \"Aurora Leigh\" was an important influence on Susan B. Anthony's thinking about the traditional roles of women, with regard to marriage versus independent individuality. The \"North American Review\" praised Elizabeth's poem: \"Mrs. Browning's poems are, in all respects, the utterance of a woman \u2014 of a woman of great learning, rich experience, and powerful genius, uniting to her woman's nature the strength which is sometimes thought peculiar to a man.\"\nSpiritual influence.\nMuch of Barrett Browning's work carries a religious theme. She had read and studied such works as Milton's \"Paradise Lost\" and Dante's \"Inferno\". She says in her writing, \"We want the sense of the saturation of Christ's blood upon the souls of our poets, that it may cry through them in answer to the ceaseless wail of the Sphinx of our humanity, expounding agony into renovation. Something of this has been perceived in art when its glory was at the fullest. Something of a yearning after this may be seen among the Greek Christian poets, something which would have been much with a stronger faculty\". She believed that \"Christ's religion is essentially poetry \u2013 poetry glorified\". She explored the religious aspect in many of her poems, especially in her early work, such as the sonnets.\nShe was interested in theological debate, had learned Hebrew and read the Hebrew Bible. Her seminal \"Aurora Leigh\", for example, features religious imagery and allusion to the apocalypse. The critic Cynthia Scheinberg notes that female characters in \"Aurora Leigh\" and her earlier work \"The Virgin Mary to the Child Jesus\" allude to Miriam, sister and caregiver to Moses. These allusions to Miriam in both poems mirror the way in which Barrett Browning herself drew from Jewish history, while distancing herself from it, in order to maintain the cultural norms of a Christian woman poet of the Victorian Age.\nIn the correspondence Barrett Browning kept with the Reverend William Merry from 1843 to 1844 on predestination and salvation by works, she identifies herself as a Congregationalist: \"I am not a Baptist \u2014 but a Congregational Christian, \u2014 in the holding of my private opinions.\"\nBarrett Browning Institute.\nIn 1892, Ledbury, Herefordshire, held a design competition to build an Institute in honour of Barrett Browning. Brightwen Binyon beat 44 other designs. It was based on the timber-framed Market House, which was opposite the site, and was completed in 1896. However, Nikolaus Pevsner was not impressed by its style. It was used as a public library from 1938 to 2021, when new library facilities were provided for the town, and it became the headquarters of the Ledbury Poetry Festival. It has been Grade II-listed since 2007.\nCritical reception.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nHow Do I Love Thee?\nHow do I love thee? Let me count the ways.\nI love thee to the depth and breadth and height\nMy soul can reach, when feeling out of sight\nFor the ends of being and ideal grace.\nI love thee to the level of every day's\nMost quiet need, by sun and candle-light.\nI love thee freely, as men strive for right.\nI love thee purely, as they turn from praise.\nI love thee with the passion put to use\nIn my old griefs, and with my childhood's faith.\nI love thee with a love I seemed to lose\nWith my lost saints. I love thee with the breath,\nSmiles, tears, of all my life; and, if God choose,\nI shall but love thee better after death.\n&lt;/poem&gt;\n Sonnet XLIII from \"Sonnets from the Portuguese\", 1845 (published 1850)\nBarrett Browning was widely popular in the United Kingdom and the United States during her lifetime. Edgar Allan Poe was inspired by her poem \"Lady Geraldine's Courtship\" and specifically borrowed the poem's metre for his poem \"The Raven\". Poe had reviewed Barrett Browning's work in the January 1845 issue of the \"Broadway Journal\", writing that \"her poetic inspiration is the highest \u2013 we can conceive of nothing more august. Her sense of Art is pure in itself.\" In return, she praised \"The Raven\", and Poe dedicated his 1845 collection \"The Raven and Other Poems\" to her, referring to her as \"the noblest of her sex\".\nBarrett Browning's poetry greatly influenced Emily Dickinson, who admired her as a woman of achievement. Her popularity in the United States and Britain was advanced by her stands against social injustice, including slavery in the United States, injustice toward Italians from their foreign rulers, and child labour.\nLilian Whiting published a biography of Barrett Browning (1899), which describes her as \"the most philosophical poet\" and depicts her life as \"a Gospel of applied Christianity\". To Whiting, the term \"art for art's sake\" did not apply to Barrett Browning's work, as each poem, distinctively purposeful, was borne of a more \"honest vision\". In this critical analysis, Whiting portrays Barrett Browning as a poet who uses knowledge of Classical literature with an \"intuitive gift of spiritual divination\". In \"Elizabeth Barrett Browning\", Angela Leighton suggests that the portrayal of Barrett Browning as the \"pious iconography of womanhood\" has distracted us from her poetic achievements. Leighton cites the 1931 play by Rudolf Besier \"The Barretts of Wimpole Street\" as evidence that 20th-century literary criticism of Barrett Browning's work has suffered more as a result of her popularity than poetic ineptitude. The play was popularized by actress Katharine Cornell, for whom it became a signature role. It was an enormous success, both artistically and commercially, and was revived several times and adapted twice into movies. Sampson, however, considers the play to have been the most damaging cause of false myths about Elizabeth, and particularly the relationship with her, allegedly 'tyrannical', father.\nThroughout the 20th century, literary criticism of Barrett Browning's poetry remained sparse until her poems were discovered by the women's movement. She once described herself as being inclined to reject several women's rights principles, suggesting in letters to Mary Russell Mitford and her husband that she believed that there was an inferiority of intellect in women. In \"Aurora Leigh\", however, she created a strong and independent woman who embraces both work and love. Leighton writes that because Elizabeth participates in the literary world, where voice and diction are dominated by perceived masculine superiority, she \"is defined only in mysterious opposition to everything that distinguishes the male subject who writes...\" A five-volume scholarly edition of her works was published in 2010, the first in over a century.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9628", "revid": "11308236", "url": "https://en.wikipedia.org/wiki?curid=9628", "title": "Enlil", "text": "Ancient Mesopotamian god\nEnlil, later known as Elil and Ellil, is an ancient Mesopotamian god associated with wind, air, earth, and storms. He is first attested as the chief deity of the Sumerian pantheon, but he was later worshipped by the Akkadians, Babylonians, Assyrians, and Hurrians. Enlil's primary center of worship was the Ekur temple in the city of Nippur, which was believed to have been built by Enlil himself and was regarded as the \"mooring-rope\" of heaven and earth. He is also sometimes referred to in Sumerian texts as Nunamnir. According to one Sumerian hymn, Enlil himself was so holy that not even the other gods could look upon him. Enlil rose to prominence during the twenty-fourth century BC with the rise of Nippur. His cult fell into decline after Nippur was sacked by the Elamites in 1230 BC and he was eventually supplanted as the chief god of the Mesopotamian pantheon by the Babylonian national god Marduk.\nEnlil plays a vital role in the ancient near eastern cosmology; he separates An (heaven) from Ki (earth), thus making the world habitable for humans. In the Sumerian flood myth Eridu Genesis, Enlil rewards Ziusudra with immortality for having survived the flood and, in the Babylonian flood myth, Enlil is the cause of the flood himself, having sent the flood to exterminate the human race, who made too much noise and prevented him from sleeping; the cuneiform tablets of Atra-Hasis report on this connections in a comparatively well-preserved state. The myth of \"Enlil and Ninlil\" is about Enlil's serial seduction of the goddess Ninlil in various guises, resulting in the conception of the moon-god Nanna and the Underworld deities Nergal, Ninazu, and Enbilulu. Enlil was regarded as the inventor of the mattock and the patron of agriculture. Enlil also features prominently in several myths involving his son Ninurta, including \"Anz\u00fb and the Tablet of Destinies\" and \"Lugale\".\nEtymology.\nEnlil's name comes from ancient Sumerian EN (\ud808\udc97), meaning \"lord\" and L\u00cdL (\ud808\udda4), the meaning of which is contentious, and which has sometimes been interpreted as meaning winds as a weather phenomenon (making Enlil a weather and sky god, \"Lord Wind\" or \"Lord Storm\"), or alternatively as signifying a spirit or phantom whose presence may be felt as stirring of the air, or possibly as representing a partial Semitic loanword rather than a Sumerian word at all. Enlil's name is not a genitive construction, suggesting that Enlil was seen as the personification of L\u00cdL rather than merely the cause of L\u00cdL.\nPiotr Steinkeller has written that the meaning of L\u00cdL may not actually be a clue to a specific divine domain of Enlil's, whether storms, spirits, or otherwise, since Enlil may have been \"a typical universal god [...] without any specific domain.\"\nPiotr Steinkeller and Piotr Michalowski have doubts about the Sumerian origin of Enlil. They have questioned the true meaning of the name, and identified Enlil with the Eblaite word \"I-li-lu\". As noted by Manfred Krebernik and M. P. Streck; Enlil being referred to as \"Kur-gal\" (the Great Mountain) in Sumerian texts suggests he might have originated in eastern Mesopotamia.\nWorship.\n&lt;templatestyles src=\"Rquote/styles.css\"/&gt;{ class=\"rquote pullquote floatright\" role=\"presentation\" style=\"display:table; border-collapse:collapse; border-style:none; float:right; margin:0.5em 0.75em; width:33%; \"\nEnlil was the patron god of the Sumerian city-state of Nippur and his main center of worship was the Ekur temple located there. The name of the temple literally means \"Mountain House\" in ancient Sumerian. The Ekur was believed to have been built and established by Enlil himself. It was believed to be the \"mooring-rope\" of heaven and earth, meaning that it was seen as \"a channel of communication between earth and heaven\". A hymn written during the reign of Ur-Nammu, the founder of the Third Dynasty of Ur, describes the E-kur in great detail, stating that its gates were carved with scenes of Imdugud, a lesser deity sometimes shown as a giant bird, slaying a lion and an eagle snatching up a sinner.\nThe Sumerians believed that the sole purpose of humanity's existence was to serve the gods. They thought that a god's statue was a physical embodiment of the god himself. As such, cult statues were given constant care and attention and a set of priests was assigned to tend to them. People worshipped Enlil by offering food and other human necessities to him. The food, which was ritually laid out before the god's cult statue in the form of a feast, was believed to be Enlil's daily meal, but, after the ritual, it would be distributed among his priests. These priests were also responsible for changing the cult statue's clothing.\nThe Sumerians envisioned Enlil as a benevolent, fatherly deity, who watches over humanity and cares for their well-being. One Sumerian hymn describes Enlil as so glorious that even the other gods could not look upon him. The same hymn also states that, without Enlil, civilization could not exist. Enlil's epithets include titles such as \"the Great Mountain\" and \"King of the Foreign Lands\". Enlil is also sometimes described as a \"raging storm\", a \"wild bull\", and a \"merchant\". The Mesopotamians envisioned him as a creator, a father, a king, and the supreme lord of the universe. He was also known as \"Nunamnir\" and is referred to in at least one text as the \"East Wind and North Wind\".\nKings regarded Enlil as a model ruler and sought to emulate his example. Enlil was said to be supremely just and intolerant towards evil. Rulers from all over Sumer would travel to Enlil's temple in Nippur to be legitimized. They would return Enlil's favor by devoting lands and precious objects to his temple as offerings. Nippur was the only Sumerian city-state that never built a palace; this was intended to symbolize the city's importance as the center of the cult of Enlil by showing that Enlil himself was the city's king. Even during the Babylonian Period, when Marduk had superseded Enlil as the supreme god, Babylonian kings still traveled to the holy city of Nippur to seek recognition of their right to rule.\nEnlil first rose to prominence during the twenty-fourth century BC, when the importance of the god An began to wane. During this time period, Enlil and An are frequently invoked together in inscriptions. Enlil remained the supreme god in Mesopotamia throughout the Amorite Period, with Amorite monarchs proclaiming Enlil as the source of their legitimacy. Enlil's importance began to wane after the Babylonian king Hammurabi conquered Sumer. The Babylonians worshipped Enlil under the name \"Elil\" and the Hurrians syncretized him with their own god Kumarbi. In one Hurrian ritual, Enlil and Apantu are invoked as \"the father and mother of I\u0161\u1e2bara\". Enlil is also invoked alongside Ninlil as a member of \"the mighty and firmly established gods\".\nDuring the Kassite Period (c. 1592\u20131155 BC), Nippur briefly managed to regain influence in the region and Enlil rose to prominence once again. From around 1300 BC onwards, Enlil was syncretized with the Assyrian national god A\u0161\u0161ur, who was the most important deity in the Assyrian pantheon. Then, in 1230 BC, the Elamites attacked Nippur and the city fell into decline, taking the cult of Enlil along with it. Approximately one hundred years later, Enlil's role as the head of the pantheon was given to Marduk, the national god of the Babylonians.\nIconography.\nEnlil was represented by the symbol of a horned cap, which consisted of up to seven superimposed pairs of ox-horns. Such crowns were an important symbol of divinity; gods had been shown wearing them ever since the third millennium BC. The horned cap remained consistent in form and meaning from the earliest days of Sumerian prehistory up until the time of the Persian conquest and beyond.\nThe Sumerians had a complex numerological system, in which certain numbers were believed to hold special ritual significance. Within this system, Enlil was associated with the number fifty, which was considered sacred to him. Enlil was part of a triad of deities, which also included An and Enki. These three deities together were the embodiment of all the fixed stars in the night sky. An was identified with all the stars of the equatorial sky, Enlil with those of the northern sky, and Enki with those of the southern sky. The path of Enlil's celestial orbit was a continuous, symmetrical circle around the north celestial pole, but those of An and Enki were believed to intersect at various points. Enlil was associated with the constellation Bo\u00f6tes.\nMythology.\nOrigins myths.\nThe main source of information about Sumerian creation mythology is the prologue to the epic poem \"Gilgamesh, Enkidu, and the Netherworld\" (ETCSL http://), which briefly describes the process of creation: originally, there was only Nammu, the primeval sea. Then, Nammu gave birth to An, the sky, and Ki, the earth. An and Ki mated with each other, causing Ki to give birth to Enlil. Enlil separated An from Ki and carried off the earth as his domain, while An carried off the sky. Enlil marries his mother, Ki, and from this union all the plant and animal life on earth is produced.\n\"Enlil and Ninlil\" (ETCSL http://) is a nearly complete 152-line Sumerian poem describing the affair between Enlil and the goddess Ninlil. First, Ninlil's mother Nunbarshegunu instructs Ninlil to go bathe in the river. Ninlil goes to the river, where Enlil seduces her and impregnates her with their son, the moon-god Nanna. Because of this, Enlil is banished to Kur, the Sumerian underworld. Ninlil follows Enlil to the underworld, where he impersonates the \"man of the gate\". Ninlil demands to know where Enlil has gone, but Enlil, still impersonating the gatekeeper, refuses to answer. He then seduces Ninlil and impregnates her with Nergal, the god of death. The same scenario repeats, only this time Enlil instead impersonates the \"man of the river of the nether world, the man-devouring river\"; once again, he seduces Ninlil and impregnates her with the god Ninazu. Finally, Enlil impersonates the \"man of the boat\"; once again, he seduces Ninlil and impregnates her with Enbilulu, the \"inspector of the canals\".\nThe story of Enlil's courtship with Ninlil is primarily a genealogical myth invented to explain the origins of the moon-god Nanna, as well as the various gods of the Underworld, but it is also, to some extent, a coming-of-age story describing Enlil and Ninlil's emergence from adolescence into adulthood. The story also explains Ninlil's role as Enlil's consort; in the poem, Ninlil declares, \"As Enlil is your master, so am I also your mistress!\" The story is also historically significant because, if the current interpretation of it is correct, it is the oldest known myth in which a god changes shape.\nFlood myth.\nIn the Sumerian version of the flood story (ETCSL http://), the causes of the flood are unclear because the portion of the tablet recording the beginning of the story has been destroyed. Somehow, a mortal known as Ziusudra manages to survive the flood, likely through the help of the god Enki. The tablet begins in the middle of the description of the flood. The flood lasts for seven days and seven nights before it subsides. Then, Utu, the god of the Sun, emerges. Ziusudra opens a window in the side of the boat and falls down prostrate before the god. Next, he sacrifices an ox and a sheep in honor of Utu. At this point, the text breaks off again. When it picks back up, Enlil and An are in the midst of declaring Ziusudra immortal as an honor for having managed to survive the flood. The remaining portion of the tablet after this point is destroyed.\nIn the later Akkadian version of the flood story, recorded in the \"Epic of Gilgamesh\", Enlil actually causes the flood, seeking to annihilate every living thing on earth because the humans, who are vastly overpopulated, make too much noise and prevent him from sleeping. In this version of the story, the hero is Utnapishtim, who is warned ahead of time by Ea, the Babylonian equivalent of Enki, that the flood is coming. The flood lasts for seven days; when it ends, Ishtar, who had mourned the destruction of humanity, promises Utnapishtim that Enlil will never cause a flood again. When Enlil sees that Utnapishtim and his family have survived, he is outraged, but his son Ninurta speaks up in favor of humanity, arguing that, instead of causing floods, Enlil should simply ensure that humans never become overpopulated by reducing their numbers using wild animals and famines. Enlil goes into the boat; Utnapishtim and his wife bow before him. Enlil, now appeased, grants Utnapishtim immortality as a reward for his loyalty to the gods.\nChief god and arbitrator.\n&lt;templatestyles src=\"Rquote/styles.css\"/&gt;{ class=\"rquote pullquote floatright\" role=\"presentation\" style=\"display:table; border-collapse:collapse; border-style:none; float:right; margin:0.5em 0.75em; width:33%; \"\nA nearly complete 108-line poem from the Early Dynastic Period (c. 2900\u20132350 BC) describes Enlil's invention of the mattock, a key agricultural pick, hoe, ax, or digging tool of the Sumerians. In the poem, Enlil conjures the mattock into existence and decrees its fate. The mattock is described as gloriously beautiful; it is made of pure gold and its head is carved from lapis lazuli. Enlil gives the tool over to the humans, who use it to build cities, subjugate their people, and pull up weeds. Enlil was believed to aid in the growth of plants.\nThe Sumerian poem \"Enlil Chooses the Farmer\u2013God\" (ETCSL http://) describes how Enlil, hoping \"to establish abundance and prosperity\", creates two gods Emesh and Enten, a shepherd and a farmer, respectively. The two gods argue and Emesh lays claim to Enten's position. They take the dispute before Enlil, who rules in favor of Enten; the two gods rejoice and reconcile.\nNinurta myths.\nIn the Sumerian poem \"Lugale\" (ETCSL http://), Enlil gives advice to his son, the god Ninurta, advising him on a strategy to slay the demon Asag. This advice is relayed to Ninurta by way of Sharur, his enchanted talking mace, which had been sent by Ninurta to the realm of the gods to seek counsel from Enlil directly.\nIn the Old, Middle, and Late Babylonian myth of \"Anz\u00fb and the Tablet of Destinies\", the Anz\u00fb, a giant, monstrous bird, betrays Enlil and steals the Tablet of Destinies, a sacred clay tablet belonging to Enlil that grants him his authority, while Enlil is preparing for a bath. The rivers dry up and the gods are stripped of their powers. The gods send Adad, Girra, and Shara to defeat the Anz\u00fb, but all of them fail. Finally, Ea proposes that the gods should send Ninurta, Enlil's son. Ninurta successfully defeats the Anz\u00fb and returns the Tablet of Destinies to his father. As a reward, Ninurta is granted a prominent seat on the council of the gods.\nWar of the gods.\nA badly damaged text from the Neo-Assyrian Period (911\u2013612 BC) describes Marduk leading his army of Anunnaki into the sacred city of Nippur and causing a disturbance. The disturbance causes a flood, which forces the resident gods of Nippur under the leadership of Enlil to take shelter in the Eshumesha temple to Ninurta. Enlil is enraged at Marduk's transgression and orders the gods of Eshumesha to take Marduk and the other Anunnaki as prisoners. The Anunnaki are captured, but Marduk appoints his front-runner Mushteshirhablim to lead a revolt against the gods of Eshumesha and sends his messenger Neretagmil to alert Nabu, the god of literacy. When the Eshumesha gods hear Nabu speak, they come out of their temple to search for him. Marduk defeats the Eshumesha gods and takes 360 of them as prisoners of war, including Enlil himself. Enlil protests that the Eshumesha gods are innocent, so Marduk puts them on trial before the Anunnaki. The text ends with a warning from Damkianna (another name for Ninhursag) to the gods and to humanity, pleading them not to repeat the war between the Anunnaki and the gods of Eshumesha.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9630", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=9630", "title": "Ecology", "text": "Study of organisms and their environment\nEcology (from grc \" ' ()\"\u00a0'house' and \" ' ()\"\u00a0'study of')[A] is the natural science of the relationships among living organisms and their environment. Ecology considers organisms at the individual, population, community, ecosystem, and biosphere levels. Ecology overlaps with the closely related sciences of biogeography, evolutionary biology, genetics, ethology, and natural history.\nEcology is a branch of biology, and is the study of abundance, biomass, and distribution of organisms in the context of the environment. It encompasses life processes, interactions, and adaptations; movement of materials and energy through living communities; successional development of ecosystems; cooperation, competition, and predation within and between species; and patterns of biodiversity and its effect on ecosystem processes.\nEcology has practical applications in fields such as conservation biology, wetland management, natural resource management, and human ecology.\nThe term \"ecology\" () was coined in 1866 by the German scientist Ernst Haeckel. The science of ecology as we know it today began with a group of American botanists in the 1890s. Evolutionary concepts relating to adaptation and natural selection are cornerstones of modern ecological theory.\nEcosystems are dynamically interacting systems of organisms, the communities they make up, and the non-living (abiotic) components of their environment. Ecosystem processes, such as primary production, nutrient cycling, and niche construction, regulate the flux of energy and matter through an environment. Ecosystems have biophysical feedback mechanisms that moderate processes acting on living (biotic) and abiotic components of the planet. Ecosystems sustain life-supporting functions and provide ecosystem services like biomass production (food, fuel, fiber, and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection, and many other natural features of scientific, historical, economic, or intrinsic value.\nLevels, scope, and scale of organization.\nEcosystems vary from tiny to vast. A single tree is of little consequence to the classification of a forest ecosystem, but is critically relevant to organisms living in and on it. Several generations of an aphid population can exist over the lifespan of a single leaf. Each of those aphids, in turn, supports diverse bacterial communities. The nature of connections in ecological communities cannot be explained by knowing the details of each species in isolation, because the emergent pattern is neither revealed nor predicted until the ecosystem is studied as an integrated whole.\nThe main subdisciplines of ecology, population (or community) ecology and ecosystem ecology, differ in their contrasting paradigms. The former focuses on organisms' distribution and abundance, while the latter focuses on materials and energy fluxes.\nHierarchy.\nTo structure the study of ecology into a conceptually manageable framework, the biological world is organized into a hierarchy, ranging in scale from (as far as ecology is concerned) organisms, to populations, to guilds, to communities, to ecosystems, to biomes, and up to the level of the biosphere. This framework forms a panarchy and exhibits non-linear behaviors; this means that \"effect and cause are disproportionate, so that small changes to critical variables, such as the number of nitrogen fixers, can lead to disproportionate, perhaps irreversible, changes in the system properties.\"\nBiodiversity.\nBiodiversity (an abbreviation of \"biological diversity\") describes the diversity of life from genes to ecosystems and spans every level of biological organization. The term has several interpretations, and there are many ways to index, measure, characterize, and represent its complex organization. Biodiversity includes species diversity, ecosystem diversity, and genetic diversity and scientists are interested in the way that this diversity affects the complex ecological processes operating at and among these respective levels. \nBiodiversity plays an important role in ecosystem services which by definition maintain and improve human quality of life. It delivers ecosystem services across heterogeneous real-world landscapes, influenced by human management and environmental conditions.\nConservation priorities and management techniques require different approaches and considerations to address the full ecological scope of biodiversity. Natural capital that supports populations is critical for maintaining ecosystem services and species migration (e.g., riverine fish runs and avian insect control) has been implicated as one mechanism by which those service losses are experienced. An understanding of biodiversity has practical applications for species and ecosystem-level conservation planners as they make management recommendations to consulting firms, governments, and industry.\nHabitat.\nThe habitat of a species describes the environment over which it occurs and the type of community that is formed. More specifically, \"habitats can be defined as regions in environmental space that are composed of multiple dimensions, each representing a biotic or abiotic environmental variable; that is, any component or characteristic of the environment related directly (e.g. forage biomass and quality) or indirectly (e.g. elevation) to the use of a location by the animal.\"\nNiche.\nDefinitions of niche date back to 1917. In 1957, G. Evelyn Hutchinson introduced \"the set of biotic and abiotic conditions in which a species is able to persist and maintain stable population sizes.\" The niche is a central concept in the ecology of organisms and is sub-divided into \"fundamental\" and \"realized\" niches. The fundamental niche is the set of environmental conditions under which a species is able to persist. The realized niche is the set of environmental plus ecological conditions under which a species persists. The Hutchinsonian niche is defined more technically as a \"Euclidean hyperspace whose \"dimensions\" are defined as environmental variables and whose \"size\" is a function of the number of values that the environmental values may assume for which an organism has \"positive fitness\".\"\nNiche construction.\nOrganisms are subject to environmental pressures, but they also modify their habitats. The regulatory feedback between organisms and their environment can affect conditions from local (e.g., a beaver pond) to global scales, over time and even after death, such as decaying logs or silica skeleton deposits from marine organisms. Ecosystem engineering is related to niche construction, but the former relates only to the physical modifications of the habitat whereas the latter also considers the evolutionary implications of physical changes to the environment and feedback on natural selection. Ecosystem engineers are defined as: \"organisms that directly or indirectly modulate the availability of resources to other species, by causing physical state changes in biotic or abiotic materials. In so doing they modify, maintain and create habitats.\"\nBiome.\nBiomes are larger units of organization that categorize regions of the Earth's ecosystems, mainly according to the structure and composition of vegetation. There are different methods to define the continental boundaries of biomes dominated by different functional types of vegetative communities that are limited in distribution by climate, precipitation, weather, and other environmental variables. Biomes include tropical rainforest, temperate broadleaf and mixed forest, temperate deciduous forest, taiga, tundra, hot desert, and polar desert. Other researchers have recently categorized other biomes, such as the human and oceanic microbiomes. To a microbe, the human body is a habitat and a landscape. Microbiomes were discovered largely through advances in molecular genetics, which have revealed a hidden richness of microbial diversity on the planet. The oceanic microbiome plays a significant role in the ecological biogeochemistry of the planet's oceans.\nBiosphere.\nThe largest scale of ecological organization is the biosphere: the total sum of ecosystems on the planet. Ecological relationships regulate the flux of energy, nutrients, and climate all the way up to the planetary scale. For example, the dynamic history of the planetary atmosphere's CO2 and O2 composition has been affected by the biogenic flux of gases coming from respiration and photosynthesis, with levels fluctuating over time in relation to the ecology and evolution of plants and animals. Ecological theory has also been used to explain self-emergent regulatory phenomena at the planetary scale: for example, the Gaia hypothesis is an example of holism applied in ecological theory. The Gaia hypothesis states that there is an emergent feedback loop generated by the metabolism of living organisms that maintains the core temperature of the Earth and atmospheric conditions within a narrow self-regulating range of tolerance.\nPopulation ecology.\nPopulation ecology studies the dynamics of species populations and how these populations interact with the wider environment. A population consists of individuals of the same species that live, interact, and migrate through the same niche and habitat.\nA primary law of population ecology is the Malthusian growth model which states, \"a population will grow (or decline) exponentially as long as the environment experienced by all individuals in the population remains constant.\" Simplified population models usually starts with four variables: death, birth, immigration, and emigration.\nAn example of an introductory population model describes a closed population, such as on an island, where immigration and emigration does not take place. Hypotheses are evaluated with reference to a null hypothesis which states that random processes create the observed data. In these island models, the rate of population change is described by:\n formula_1\nwhere \"N\" is the total number of individuals in the population, \"b\" and \"d\" are the per capita rates of birth and death respectively, and \"r\" is the per capita rate of population change.\nUsing these modeling techniques, Malthus' population principle of growth was later transformed into a model known as the logistic equation by Pierre Verhulst:\n formula_2\nwhere \"N(t)\" is the number of individuals measured as biomass density as a function of time, \"t\", \"r\" is the maximum per-capita rate of change commonly known as the intrinsic rate of growth, and formula_3 is the crowding coefficient, which represents the reduction in population growth rate per individual added. The formula states that the rate of change in population size (formula_4) will grow to approach equilibrium, where (formula_5), when the rates of increase and crowding are balanced, formula_6. A common, analogous model fixes the equilibrium, formula_6 as \"K\", which is known as the \"carrying capacity.\"\nPopulation ecology builds upon these introductory models to further understand demographic processes in real study populations. Commonly used types of data include life history, fecundity, and survivorship, and these are analyzed using mathematical techniques such as matrix algebra. The information is used for managing wildlife stocks and setting harvest quotas. In cases where basic models are insufficient, ecologists may adopt different kinds of statistical methods, such as the Akaike information criterion, or use models that can become mathematically complex as \"several competing hypotheses are simultaneously confronted with the data.\"\nMetapopulations and migration.\nThe concept of metapopulations was defined in 1969 as \"a population of populations which go extinct locally and recolonize\". Metapopulation ecology is another statistical approach that is often used in conservation research. Metapopulation models simplify the landscape into patches of varying levels of quality, and metapopulations are linked by the migratory behaviours of organisms. Animal migration is set apart from other kinds of movement because it involves the seasonal departure and return of individuals from a habitat. Migration is also a population-level phenomenon, as with the migration routes followed by plants as they occupied northern post-glacial environments. Plant ecologists use pollen records that accumulate and stratify in wetlands to reconstruct the timing of plant migration and dispersal relative to historic and contemporary climates. These migration routes involved an expansion of the range as plant populations expanded from one area to another. There is a larger taxonomy of movement, such as commuting, foraging, territorial behavior, stasis, and ranging. Dispersal is usually distinguished from migration because it involves the one-way permanent movement of individuals from their birth population into another population.\nCommunity ecology.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nCommunity ecology examines how interactions among species and their environment affect the abundance, distribution and diversity of species within communities.\nJohnson &amp; Stinchcomb (2007)\nCommunity ecology is the study of the interactions among a collection of species that inhabit the same geographic area. Community ecologists study the determinants of patterns and processes for two or more interacting species. Research in community ecology might measure species diversity in grasslands in relation to soil fertility. It might also include the analysis of predator-prey dynamics, competition among similar plant species, or mutualistic interactions between crabs and corals.\nEcosystem ecology.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThese ecosystems, as we may call them, are of the most various kinds and sizes. They form one category of the multitudinous physical systems of the universe, which range from the universe as a whole down to the atom.\nTansley (1935)\nThe underlying concept of an ecosystem can be traced back to 1864 in the published work of George Perkins Marsh (\"Man and Nature\"). Ecosystems may be habitats within biomes that form an integrated whole and a dynamically responsive system having both physical and biological complexes. Ecosystem ecology is the science of determining the fluxes of materials (e.g. carbon, phosphorus) between different pools (e.g., tree biomass, soil organic material). Ecosystem ecologists attempt to determine the underlying causes of these fluxes. Research in ecosystem ecology might measure primary production (g C/m^2) in a wetland in relation to decomposition and consumption rates (g C/m^2/y). This requires an understanding of the community connections between plants (i.e., primary producers) and the decomposers (e.g., fungi and bacteria).\nFood webs.\nA food web is the archetypal ecological network. Plants capture solar energy and use it to synthesize simple sugars during photosynthesis. As plants grow, they accumulate nutrients and are eaten by grazing herbivores, and the energy is transferred through a chain of organisms by consumption. The simplified linear feeding pathways that move from a basal trophic species to a top consumer is called the food chain. Food chains in an ecological community create a complex food web. Food webs are a type of concept map used to illustrate and study pathways of energy and material flows.\nTrophic levels.\nA trophic level (from Greek \"troph\", \u03c4\u03c1\u03bf\u03c6\u03ae, troph\u0113, meaning \"food\" or \"feeding\") is \"a group of organisms acquiring a considerable majority of its energy from the lower adjacent level (according to ecological pyramids) nearer the abiotic source.\" Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.\nSpecies are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores), and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores, they are relatively inefficient at grazing.\nTrophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to \"reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction.\" Nonetheless, recent studies have shown that real trophic levels do exist, but \"above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores.\"\nKeystone species.\nA keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects (termed \"trophic cascades\") that alters trophic dynamics, other food web connections, and can cause the extinction of other species. The term keystone species was coined by Robert Paine in 1969 and is a reference to the keystone architectural feature as the removal of a keystone species can result in a community collapse just as the removal of the keystone in an arch can result in the arch's loss of stability.\nSea otters (\"Enhydra lutris\") are commonly cited as an example because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear, and this has a dramatic effect on community structure.\nComplexity.\nComplexity is understood as a large computational effort needed to assemble numerous interacting parts. Global patterns of biological diversity are complex. This biocomplexity stems from the interplay among ecological processes that influence patterns at different scales, such as transitional areas or ecotones spanning landscapes. Complexity stems from the interplay among levels of biological organization as energy, and matter is integrated into larger units that superimpose onto the smaller parts. Small scale patterns do not necessarily explain larger ones, as in Aristotle's expression \"the sum is greater than the parts\".[E] \"Complexity in ecology is of at least six distinct types: spatial, temporal, structural, process, behavioral, and geometric.\" From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level.\nHolism.\nHolism is a critical part of the theory of ecology. Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to non-reducible properties. This means that higher-order patterns of a whole functional system, such as an ecosystem, cannot be predicted or understood by a simple summation of the parts. \"New properties emerge because the components interact, not because the basic nature of the components is changed.\"\nRelation to evolution.\nEcology and evolutionary biology are sister disciplines. Natural selection, life history, development, adaptation, populations, and inheritance thread equally into both. In this framework, the analytical tools of ecologists and evolutionists overlap as they study life through phylogenetics or Linnaean taxonomy. There is no sharp boundary separating ecology from evolution, and they differ more in their areas of applied focus. Both explain properties and processes across different spatial or temporal scales of organization. Ecologists study the abiotic and biotic factors that influence evolutionary processes, and evolution can be rapid, occurring on ecological timescales as short as one generation.\nBehavioural ecology.\nAll organisms have behaviours. Even plants express complex behaviour, including memory and communication. Behavioural ecology is the study of an organism's behaviour in its environment and its ecological and evolutionary implications. Ethology is the study of observable movement or behaviour in animals. This could include investigations of motile sperm of plants, mobile phytoplankton, zooplankton swimming toward the female egg, the cultivation of fungi by weevils, the mating dance of a salamander, or social gatherings of amoeba.\nAdaptation is the central unifying concept in behavioural ecology. Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can. Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness.\nCognitive ecology.\nCognitive ecology integrates theory and observations from evolutionary ecology and cognitive science, to understand the effect of animal interaction with their habitat on their cognitive systems. \"Until recently, however, cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings. With consideration of the selection pressure on cognition, cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition.\"\nSocial ecology.\nSocial-ecological behaviours are notable in the social insects, slime moulds, social spiders, human society, and naked mole-rats where eusocialism has evolved. Social behaviours include reciprocally beneficial behaviours among kin and nest mates and evolve from kin and group selection. Kin selection explains altruism through genetic relationships, whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives. The social insects, including ants, bees, and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony. In contrast, group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group; whereby, it becomes selectively advantageous for groups if their members express altruistic behaviours to one another. Groups with predominantly altruistic members survive better than groups with predominantly selfish members.\nCoevolution.\nEcological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships between species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.\nBiogeography.\nBiogeography is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time. The \"Journal of Biogeography\" was established in 1974. Biogeography and ecology share many of their disciplinary roots. \"Island biogeography\", published by Robert MacArthur and Edward O. Wilson in 1967, is one of the fundamentals of ecological theory. Biogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals. Ecology and evolution provide the explanatory context for biogeographical studies. Biogeographical patterns result from ecological processes that influence range distributions, such as migration and dispersal. and from historical processes that split populations or species into different areas. The biogeographic processes that result in the natural splitting of species explain much of the modern distribution of the Earth's biota. The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography. There are also practical applications in the field of biogeography concerning ecological systems and processes. For example, the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming.\nr/K selection theory.\nr/K selection theory[D] is one of the first predictive models in ecology used to explain life-history evolution. Its premise is that natural selection varies with population density. For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience \"density-independent\" forces of natural selection, which is called \"r\"-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called \"K\"-selection. In the \"r/K\"-selection model, the first variable \"r\" is the intrinsic rate of natural increase in population size and the second variable \"K\" is the carrying capacity of a population.\nMolecular ecology.\nThe relationship between ecology and genetic inheritance predates modern techniques for molecular analysis. Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies, such as the polymerase chain reaction (PCR). The rise of molecular technologies and the influx of research questions into this new field resulted in the publication \"Molecular Ecology\" in 1992. Molecular ecology uses analytical techniques to study genes in an evolutionary and ecological context. In 1994, John Avise played a leading role in this area of science with the publication of his book, \"Molecular Markers, Natural History and Evolution\".\nHuman ecology.\nA dual discipline.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nOnly within the moment of time represented by the present century has one species man acquired significant power to alter the nature of his world.\nRachel Carson, \"Silent Spring\"\nEcology is both a biological science and a human science. Human ecology is an interdisciplinary investigation into the ecology of our species. \"Human ecology may be defined: (1) from a bioecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems; (2) from a bioecological standpoint as simply another animal affecting and being affected by his physical environment; and (3) as a human being, somehow different from animal life in general, interacting with physical and modified environments in a distinctive and creative way. A truly interdisciplinary human ecology will most likely address itself to all three.\" The term was formally introduced in 1921, but many sociologists, geographers, psychologists, and other disciplines were interested in human relations to natural systems centuries prior, especially in the late 19th century.\nRestoration Ecology.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nEcosystem management is not just about science nor is it simply an extension of traditional resource management; it offers a fundamental reframing of how humans may work with nature.\nGrumbine (1994)\nEcology is an employed science of restoration, repairing disturbed sites through human intervention, in natural resource management, and in environmental impact assessments. Edward O. Wilson predicted in 1992 that the 21st century \"will be the era of restoration in ecology\".\nRelation to the environment.\nThe environment of ecosystems includes both physical parameters and biotic attributes. It is dynamically interlinked and contains resources for organisms at any time throughout their life cycle. Like ecology, the term environment has different conceptual meanings and overlaps with the concept of nature. Environment \"includes the physical world, the social world of human relations and the built world of human creation.\" The physical environment is external to the level of biological organization under investigation, including abiotic factors such as temperature, radiation, light, chemistry, climate and geology. The biotic environment includes genes, cells, organisms, members of the same species (conspecifics) and other species that share a habitat.\nDisturbance and resilience.\nA disturbance is any process that changes or removes biomass from a community, such as a fire, flood, drought, or predation. Disturbances are both the cause and product of natural fluctuations within an ecological community. Biodiversity can protect ecosystems from disturbances.\nMetabolism and the early atmosphere.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nMetabolism \u2013 the rate at which energy and material resources are taken up from the environment, transformed within an organism, and allocated to maintenance, growth and reproduction \u2013 is a fundamental physiological trait.\nErnest et al.\nThe Earth was formed approximately 4.5\u00a0billion years ago. As it cooled and a crust and oceans formed, its atmosphere transformed from being dominated by hydrogen to one composed mostly of methane and ammonia. Over the next billion years, the metabolic activity of life transformed the atmosphere into a mixture of carbon dioxide, nitrogen, and water vapor. These gases changed the way that light from the sun hit the Earth's surface and greenhouse effects trapped heat. There were untapped sources of free energy within the mixture of reducing and oxidizing gasses that set the stage for primitive ecosystems to evolve and, in turn, the atmosphere also evolved.\nThroughout history, the Earth's atmosphere and biogeochemical cycles have been in a dynamic equilibrium with planetary ecosystems. The history is characterized by periods of significant transformation followed by millions of years of stability. The evolution of the earliest organisms, likely anaerobic methanogen microbes, started the process by converting atmospheric hydrogen into methane (4H2 + CO2 \u2192 CH4 + 2H2O). Anoxygenic photosynthesis reduced hydrogen concentrations and increased atmospheric methane, by converting hydrogen sulfide into water or other sulfur compounds (for example, 2H2S + CO2 + h\"v\" \u2192 CH2O + H2O + 2S). Early forms of fermentation also increased levels of atmospheric methane. The transition to an oxygen-dominant atmosphere (the \"Great Oxidation\") did not begin until approximately 2.4\u20132.3\u00a0billion years ago, but photosynthetic processes started 0.3\u20131\u00a0billion years prior.\nRadiation: heat, temperature and light.\nThe biology of life operates within a certain range of temperatures. Heat is a form of energy that regulates temperature. Heat affects growth rates, activity, behaviour, and primary production. Temperature is largely dependent on the incidence of solar radiation. The latitudinal and longitudinal spatial variation of temperature greatly affects climates and consequently the distribution of biodiversity and levels of primary production in different ecosystems or biomes across the planet. Heat and temperature relate importantly to metabolic activity. Poikilotherms have a body temperature largely dependent on the temperature of the external environment. In contrast, homeotherms regulate their internal body temperature by expending metabolic energy. There is a relationship between light, primary production, and ecological energy budgets. Sunlight is the primary input of energy into the planet's ecosystems. Light is composed of electromagnetic energy of different wavelengths. Radiant energy from the sun generates heat, provides photons of light measured as active energy in the chemical reactions of life, and also acts as a catalyst for genetic mutation. Plants, algae, and some bacteria absorb light and assimilate the energy through photosynthesis. Organisms capable of assimilating energy by photosynthesis or through inorganic fixation of H2S are autotrophs. Autotrophs\u2014responsible for primary production\u2014assimilate light energy which becomes metabolically stored as potential energy in the form of biochemical enthalpic bonds.\nPhysical environments.\nWater.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nWetland conditions such as shallow water, high plant productivity, and anaerobic substrates provide a suitable environment for important physical, biological, and chemical processes. Because of these processes, wetlands play a vital role in global nutrient and element cycles.\nCronk &amp; Fennessy (2001)\nDiffusion of carbon dioxide and oxygen is approximately 10,000 times slower in water than in air. When soils are flooded, they quickly lose oxygen, becoming hypoxic (an environment with O2 concentration below 2\u00a0mg/liter) and eventually completely anoxic where anaerobic bacteria thrive among the roots. Water influences the intensity and spectral composition of light as it reflects off the water surface and submerged particles. Salt water plants (halophytes) have additional specialized adaptations, such as the development of special organs for shedding salt and osmoregulating their internal salt (NaCl) concentrations, to live in estuarine, brackish, or oceanic environments. The physiology of fish is adapted to compensate for environmental salt levels through osmoregulation. Their gills form electrochemical gradients that mediate salt excretion in salt water and uptake in fresh water.\nGravity.\nThe shape and energy of the land are significantly affected by gravitational forces. These govern many of the geophysical properties and distributions of biomes across the Earth. On the organismal scale, gravitational forces provide directional cues for plant and fungal growth (gravitropism), orientation cues for animal migrations, and influence the biomechanics and size of animals. Ecological traits, such as allocation of biomass in trees during growth are subject to mechanical failure as gravitational forces influence the position and structure of branches and leaves. The cardiovascular systems of animals are functionally adapted to overcome the pressure and gravitational forces that change according to the features of organisms (e.g., height, size, shape), their behaviour (e.g., diving, running, flying), and the habitat occupied (e.g., water, hot deserts, cold tundra).\nPressure.\nClimatic and osmotic pressure places physiological constraints on organisms, especially those that fly and respire at high altitudes, or dive to deep ocean depths. These constraints influence vertical limits of ecosystems in the biosphere, as organisms are physiologically sensitive and adapted to atmospheric and osmotic water pressure differences. For example, oxygen levels decrease with decreasing pressure and are a limiting factor for life at higher altitudes. Water transportation by plants is affected by osmotic pressure gradients. Water pressure in the depths of oceans requires that organisms adapt to these conditions. For example, diving animals such as whales, dolphins, and seals are adapted to deal with changes in sound due to water pressure differences. \nWind and turbulence.\nTurbulent forces in air and water affect the environment and ecosystem distribution, form, and dynamics. On a planetary scale, ecosystems are affected by circulation patterns in the global trade winds. Wind power and the turbulent forces it creates can influence heat, nutrient, and biochemical profiles of ecosystems. For example, wind running over the surface of a lake creates turbulence, mixing the water column and influencing the environmental profile to create thermally layered zones, affecting how fish, algae, and other parts of the aquatic ecosystem are structured. \nWind speed and turbulence influence evapotranspiration rates and energy budgets in plants and animals. Wind speed, temperature and moisture content vary as winds travel across different land features and elevations. For example, the westerlies come into contact with the coastal and interior mountains of western North America to produce a rain shadow on the leeward side of the mountain. The air expands and moisture condenses as the winds increase in elevation; this is called orographic lift and can cause precipitation. This environmental process produces spatial divisions in biodiversity, as species adapted to wetter conditions are range-restricted to the coastal mountain valleys and unable to migrate across the xeric ecosystems to intermix with sister lineages that are segregated to the interior mountain systems.\nFire.\nPlants convert carbon dioxide into biomass and emit oxygen into the atmosphere. By approximately 350 million years ago (the end of the Devonian period), photosynthesis had brought the concentration of atmospheric oxygen above 17%, which allowed combustion to occur. Fire releases CO2 and converts fuel into ash and tar. Fire is a significant ecological parameter that raises many issues pertaining to its control and suppression. While the issue of fire in relation to ecology and plants has been recognized for a long time, Charles Cooper brought attention to the issue of forest fires in relation to the ecology of forest fire suppression and management in the 1960s.\nSoils.\nSoil is the living top layer of mineral and organic dirt that covers the surface of the planet. It is the chief organizing centre of most ecosystem functions, and it is of critical importance in agricultural science and ecology. The decomposition of dead organic matter (for example, leaves on the forest floor), results in soils containing minerals and nutrients that feed into plant production. The whole of the planet's soil ecosystems is called the pedosphere where a large biomass of the Earth's biodiversity organizes into trophic levels. Invertebrates that feed and shred larger leaves, for example, create smaller bits for smaller organisms in the feeding chain. Collectively, these organisms are the detritivores that regulate soil formation. Soils form composite phenotypes where inorganic matter is enveloped into the physiology of a whole community. As organisms feed and migrate through soils they physically displace materials, an ecological process called bioturbation. This aerates soils and stimulates heterotrophic growth and production. Soil microorganisms are influenced by and are fed back into the trophic dynamics of the ecosystem. \nBiogeochemistry and climate.\nEcologists study nutrient budgets to understand how these materials are regulated, flow, and recycled through the environment. This research has led to an understanding that there is global feedback between ecosystems and the physical parameters of this planet, including minerals, soil, pH, ions, water, and atmospheric gases. Six major elements (hydrogen, carbon, nitrogen, oxygen, sulfur, and phosphorus; H, C, N, O, S, and P) form the constitution of all biological macromolecules and feed into the Earth's geochemical processes. From the smallest scale of biology, the combined effect of billions of ecological processes amplify and regulate the biogeochemical cycles of the Earth.\nHistory.\nEarly beginnings.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nBy ecology, we mean the whole science of the relations of the organism to the environment including, in the broad sense, all the \"conditions of existence\". Thus, the theory of evolution explains the housekeeping relations of organisms mechanistically as the necessary consequences of effectual causes; and so forms the monistic groundwork of ecology.\nErnst Haeckel (1866) [B]\nEcology has a complex origin. Ancient Greek philosophers such as Hippocrates and Aristotle recorded observations on natural history. However, they saw species as unchanging, while varieties were seen as aberrations. Modern ecology sees varieties as the real phenomena, leading to adaptation by natural selection. Ecological concepts such as a balance and regulation in nature can be traced to Herodotus (died \"c\". 425 BC), who described mutualism in his observation of \"natural dentistry\". Basking Nile crocodiles, he noted, opened their mouths to give sandpipers safe access to pluck leeches out, giving nutrition to the sandpiper and oral hygiene for the crocodile. Aristotle and his student Theophrastus observed plant and animal migrations, biogeography, physiology, and their behavior, giving an early analogue to the concept of an ecological niche.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nNowhere can one see more clearly illustrated what may be called the sensibility of such an organic complex, \u2013 expressed by the fact that whatever affects any species belonging to it, must speedily have its influence of some sort upon the whole assemblage. He will thus be made to see the impossibility of studying any form completely, out of relation to the other forms, \u2013 the necessity for taking a comprehensive survey of the whole as a condition to a satisfactory understanding of any part.\nStephen Forbes (1887)\n Ernst Haeckel (left) and Eugenius Warming (right), two founders of ecology\nEcological concepts such as food chains, population regulation, and productivity were developed in the 1700s, through the works of microscopist Antonie van Leeuwenhoek (1632\u20131723) and botanist Richard Bradley (1688?\u20131732). Biogeographer Alexander von Humboldt (1769\u20131859) recognized ecological gradients, where species are replaced or altered in form along environmental gradients. Humboldt drew inspiration from Isaac Newton, as he developed a form of \"terrestrial physics\". Natural historians, such as Humboldt, James Hutton, and Jean-Baptiste Lamarck laid the foundations of ecology. The term \"ecology\" () was coined by Ernst Haeckel in his book \"Generelle Morphologie der Organismen\" (1866). Haeckel was a zoologist, artist, writer, and later in life a professor of comparative anatomy.\nLinnaeus founded an early branch of ecology that he called the economy of nature. He influenced Charles Darwin, who adopted Linnaeus' phrase in \"The Origin of Species\". Linnaeus was the first to frame the balance of nature as a testable hypothesis.\nSince 1900.\nModern ecology first attracted substantial scientific attention toward the end of the 19th century. Ellen Swallow Richards adopted the term \"oekology\" in the U.S. as early as 1892. In the early 20th century, ecology transitioned from description to a more analytical form of \"scientific natural history\". Frederic Clements published the first American ecology book, \"Research Methods in Ecology\" in 1905, presenting the idea of plant communities as a superorganism. This launched a debate between ecological holism and individualism that lasted until the 1970s.\nIn 1942, Raymond Lindeman wrote a landmark paper on the trophic dynamics of ecology. Trophic dynamics became the foundation for much work on energy and material flow through ecosystems. Robert MacArthur advanced mathematical theory, predictions, and tests in ecology in the 1950s. \n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThis whole chain of poisoning, then, seems to rest on a base of minute plants which must have been the original concentrators. But what of the opposite end of the food chain\u2014the human being who, in probable ignorance of all this sequence of events, has rigged his fishing tackle, caught a string of fish from the waters of Clear Lake, and taken them home to fry for his supper?\nRachel Carson (1962)\nEcology surged in popular and scientific interest during the 1960\u20131970s environmental movement. In 1962, marine biologist and ecologist Rachel Carson's book \"Silent Spring\" helped to mobilize the environmental movement by alerting the public to toxic pesticides, such as DDT (C14H9Cl5), bioaccumulating in the environment. Since then, ecologists have worked to bridge their understanding of the degradation of the planet's ecosystems with environmental politics, law, restoration, and natural resources management.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9631", "revid": "84591", "url": "https://en.wikipedia.org/wiki?curid=9631", "title": "Glossary of country dance terms", "text": "An alphabetic list of modern country dance terminology:\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\n&lt;dl class=\"glossary \" &gt;"}
{"id": "9632", "revid": "14650386", "url": "https://en.wikipedia.org/wiki?curid=9632", "title": "Ecosystem", "text": "Community of living organisms together with the nonliving components of their environment\nAn ecosystem (or ecological system) is a system formed by organisms in interaction with their environment. The biotic and abiotic components are linked together through nutrient cycles and energy flows.\nEcosystems are controlled by external and internal factors. External factors\u2014including climate\u2014control the ecosystem's structure, but are not influenced by it. By contrast, internal factors control and are controlled by ecosystem processes; these include decomposition, the types of species present, root competition, shading, disturbance, and succession. While external factors generally determine which resource inputs an ecosystem has, their availability within the ecosystem is controlled by internal factors. Ecosystems are dynamic, subject to periodic disturbances and always in the process of recovering from past disturbances. The tendency of an ecosystem to remain close to its equilibrium state, is termed its resistance. Its capacity to absorb disturbance and reorganize, while undergoing change so as to retain essentially the same function, structure, identity, is termed its ecological resilience. \nEcosystems can be studied through a variety of approaches\u2014theoretical studies, studies monitoring specific ecosystems over long periods of time, those that look at differences between ecosystems to elucidate how they work and direct manipulative experimentation. Biomes are general classes or categories of ecosystems. However, there is no clear distinction between biomes and ecosystems. Ecosystem classifications are specific kinds of ecological classifications that consider all four elements of the definition of ecosystems: a biotic component, an abiotic complex, the interactions between and within them, and the physical space they occupy. Biotic factors are living things; such as plants, while abiotic are non-living components; such as soil. Plants allow energy to enter the system through photosynthesis, building up plant tissue. Animals play an important role in the movement of matter and energy through the system, by feeding on plants and one another. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and microbes.\nEcosystems provide a variety of goods and services upon which people depend, and may be part of. Ecosystem goods include the \"tangible, material products\" of ecosystem processes such as water, food, fuel, construction material, and medicinal plants. Ecosystem services, on the other hand, are generally \"improvements in the condition or location of things of value\". These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. Many ecosystems become degraded through human impacts, such as soil loss, air and water pollution, habitat fragmentation, water diversion, fire suppression, and introduced species and invasive species. These threats can lead to abrupt transformation of the ecosystem or to gradual disruption of biotic processes and degradation of abiotic conditions of the ecosystem. Once the original ecosystem has lost its defining features, it is considered \"collapsed\". Ecosystem restoration can contribute to achieving the Sustainable Development Goals.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDefinition.\nAn ecosystem (or ecological system) consists of all the organisms and the abiotic pools (or physical environment) with which they interact. The biotic and abiotic components are linked together through nutrient cycles and energy flows.\n\"Ecosystem processes\" are the transfers of energy and materials from one pool to another. Ecosystem processes are known to \"take place at a wide range of scales\". Therefore, the correct scale of study depends on the question asked.\nOrigin and development of the term.\nThe term \"ecosystem\" was first used in 1935 in a publication by British ecologist Arthur Tansley. The term was coined by Arthur Roy Clapham, who came up with the word at Tansley's request. Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment. He later refined the term, describing it as \"The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment\". Tansley regarded ecosystems not simply as natural units, but as \"mental isolates\". Tansley later defined the spatial extent of ecosystems using the term \"ecotope\".\nG. Evelyn Hutchinson, a limnologist who was a contemporary of Tansley's, combined Charles Elton's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky. As a result, he suggested that mineral nutrient availability in a lake limited algal production. This would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum, further developed a \"systems approach\" to the study of ecosystems. This allowed them to study the flow of energy and material through ecological systems.\nProcesses.\nExternal and internal factors.\nEcosystems are controlled by both external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. On broad geographic scales, climate is the factor that \"most strongly determines ecosystem processes and structure\". Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and seasonal temperatures influence photosynthesis and thereby determine the amount of energy available to the ecosystem.\nParent material determines the nature of the soil in an ecosystem, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate, soil development and the movement of water through a system. For example, ecosystems can be quite different if situated in a small depression on the landscape, versus one present on an adjacent steep hillside.\nOther external factors that play an important role in ecosystem functioning include time and potential biota, the organisms that are present in a region and could potentially occupy a particular site. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present. The introduction of non-native species can cause substantial shifts in ecosystem function.\nUnlike external factors, internal factors in ecosystems not only control ecosystem processes but are also controlled by them. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other factors like disturbance, succession or the types of species present are also internal factors.\nPrimary production.\nPrimary production is the production of organic matter from inorganic carbon sources. This mainly occurs through photosynthesis. The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels. It also drives the carbon cycle, which influences global climate via the greenhouse effect.\nThrough the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen. The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP). About half of the gross GPP is respired by plants in order to provide the energy that supports their growth and maintenance. The remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP). Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), the rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.\nEnergy flow.\nEnergy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration. The carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus. In terrestrial ecosystems, the vast majority of the net primary production ends up being broken down by decomposers. The remainder is consumed by animals while still alive and enters the plant-based trophic system. After plants and animals die, the organic matter contained in them enters the detritus-based trophic system.\nEcosystem respiration is the sum of respiration by all living organisms (plants, animals, and decomposers) in the ecosystem. Net ecosystem production is the difference between gross primary production (GPP) and ecosystem respiration. In the absence of disturbance, net ecosystem production is equivalent to the net carbon accumulation in the ecosystem.\nEnergy can also be released from an ecosystem through disturbances such as wildfire or transferred to other ecosystems (e.g., from a forest to a stream to a lake) by erosion.\nIn aquatic systems, the proportion of plant biomass that gets consumed by herbivores is much higher than in terrestrial systems. In trophic systems, photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers\u2014herbivores. Organisms which feed on microbes (bacteria and fungi) are termed microbivores. Animals that feed on primary consumers\u2014carnivores\u2014are secondary consumers. Each of these constitutes a trophic level.\nThe sequence of consumption\u2014from plant to herbivore, to carnivore\u2014forms a food chain. Real systems are much more complex than this\u2014organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey that is part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains which present a number of common, non random properties in the topology of their network.\nDecomposition.\nThe carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, the dead organic matter would accumulate in an ecosystem, and nutrients and atmospheric carbon dioxide would be depleted.\nDecomposition processes can be separated into three categories\u2014leaching, fragmentation and chemical alteration of dead material. As water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered lost to it). Newly shed leaves and newly dead animals have high concentrations of water-soluble components and include sugars, amino acids and mineral nutrients. Leaching is more important in wet environments and less important in dry ones.\nFragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark, and cell contents are protected by a cell wall. Newly dead animals may be covered by an exoskeleton. Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition. Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material.\nThe chemical alteration of the dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes that can break through the tough outer structures surrounding dead plant material. They also produce enzymes that break down lignin, which allows them access to both cell contents and the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources.\nDecomposition rates.\nDecomposition rates vary among ecosystems. The rate of decomposition is governed by three sets of factors\u2014the physical environment (temperature, moisture, and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself. Temperature controls the rate of microbial respiration; the higher the temperature, the faster the microbial decomposition occurs. Temperature also affects soil moisture, which affects decomposition. Freeze-thaw cycles also affect decomposition\u2014freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the spring, creating a pulse of nutrients that become available.\nDecomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth.\nDynamics and resilience.\nEcosystems are dynamic entities. They are subject to periodic disturbances and are always in the process of recovering from past disturbances. When a perturbation occurs, an ecosystem responds by moving away from its initial state. The tendency of an ecosystem to remain close to its equilibrium state, despite that disturbance, is termed its resistance. The capacity of a system to absorb disturbance and reorganize while undergoing change so as to retain essentially the same function, structure, identity, and feedbacks is termed its ecological resilience. Resilience thinking also includes humanity as an integral part of the biosphere where we are dependent on ecosystem services for our survival and must build and maintain their natural capacities to withstand shocks and disturbances. Time plays a central role over a wide range, for example, in the slow development of soil from bare rock and the faster recovery of a community from disturbance.\nDisturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as \"a relatively discrete event in time that removes plant biomass\". This can range from herbivore outbreaks, treefalls, fires, hurricanes, floods, glacial advances, to volcanic eruptions. Such disturbances can cause large changes in plant, animal and microbe populations, as well as soil organic matter content. Disturbance is followed by succession, a \"directional change in ecosystem structure and functioning resulting from biotically driven changes in resource supply.\"\nThe frequency and severity of disturbance determine the way it affects ecosystem function. A major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience such disturbances undergo primary succession. A less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession and a faster recovery. More severe and more frequent disturbance result in longer recovery times.\nFrom one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought, a colder than usual winter, and a pest outbreak all are short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. Longer-term changes also shape ecosystem processes. For example, the forests of eastern North America still show legacies of cultivation which ceased in 1850 when large areas were reverted to forests. Another example is the methane production in eastern Siberian lakes that is controlled by organic matter which accumulated during the Pleistocene.\nNutrient cycling.\nEcosystems continually exchange energy and carbon with the wider environment. Mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation, is deposited through precipitation, dust, gases or is applied as fertilizer. Most terrestrial ecosystems are nitrogen-limited in the short term making nitrogen cycling an important control on ecosystem production. Over the long term, phosphorus availability can also be critical.\nMacronutrients which are required by all plants in large quantities include the primary nutrients (which are most limiting as they are used in largest amounts): Nitrogen, phosphorus, potassium. Secondary major nutrients (less often limiting) include: Calcium, magnesium, sulfur. Micronutrients required by all plants in small quantities include boron, chloride, copper, iron, manganese, molybdenum, zinc. Finally, there are also beneficial nutrients which may be required by certain plants or by plants under specific environmental conditions: aluminum, cobalt, iodine, nickel, selenium, silicon, sodium, vanadium.\nUntil modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen-fixing bacteria either live symbiotically with plants or live freely in the soil. The energetic cost is high for plants that support nitrogen-fixing symbionts\u2014as much as 25% of gross primary production when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs, which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants. Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust. Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems.\nWhen plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi, and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization. Others convert ammonium to nitrite and nitrate ions, a process known as nitrification. Nitric oxide and nitrous oxide are also produced during nitrification. Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas, a process known as denitrification.\nMycorrhizal fungi which are symbiotic with plant roots, use carbohydrates supplied by the plants and in return transfer phosphorus and nitrogen compounds back to the plant roots. This is an important pathway of organic nitrogen transfer from dead organic matter to plants. This mechanism may contribute to more than 70 Tg of annually assimilated plant nitrogen, thereby playing a critical role in global nutrient cycling and ecosystem function.\nPhosphorus enters ecosystems through weathering. As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics). Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter.\nFunction and biodiversity.\nBiodiversity plays an important role in ecosystem functioning. Ecosystem processes are driven by the species in an ecosystem, the nature of the individual species, and the relative abundance of organisms among these species. Ecosystem processes are the net effect of the actions of individual organisms as they interact with their environment. Ecological theory suggests that in order to coexist, species must have some level of limiting similarity\u2014they must be different from one another in some fundamental way, otherwise, one species would competitively exclude the other. Despite this, the cumulative effect of additional species in an ecosystem is not linear: additional species may enhance nitrogen retention, for example. However, beyond some level of species richness, additional species may have little additive effect unless they differ substantially from species already present. This is the case for example for exotic species.\nThe addition (or loss) of species that are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large effect on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem.\nAn ecosystem engineer is any organism that creates, significantly modifies, maintains or destroys a habitat.\nStudy approaches.\nEcosystem ecology.\nEcosystem ecology is the \"study of the interactions between organisms and their environment as an integrated system\". The size of ecosystems can range up to ten orders of magnitude, from the surface layers of rocks to the surface of the planet.\nThe Hubbard Brook Ecosystem Study started in 1963 to study the White Mountains in New Hampshire. It was the first successful attempt to study an entire watershed as an ecosystem. The study used stream chemistry as a means of monitoring ecosystem properties, and developed a detailed biogeochemical model of the ecosystem. Long-term research at the site led to the discovery of acid rain in North America in 1972. Researchers documented the depletion of soil cations (especially calcium) over the next several decades.\nEcosystems can be studied through a variety of approaches\u2014theoretical studies, studies monitoring specific ecosystems over long periods of time, those that look at differences between ecosystems to elucidate how they work and direct manipulative experimentation. Studies can be carried out at a variety of scales, ranging from whole-ecosystem studies to studying or mesocosms (simplified representations of ecosystems). American ecologist Stephen R. Carpenter has argued that microcosm experiments can be \"irrelevant and diversionary\" if they are not carried out in conjunction with field studies done at the ecosystem scale. In such cases, microcosm experiments may fail to accurately predict ecosystem-level dynamics.\nClassifications.\nBiomes are general classes or categories of ecosystems. However, there is no clear distinction between biomes and ecosystems. Biomes are always defined at a very general level. Ecosystems can be described at levels that range from very general (in which case the names are sometimes the same as those of biomes) to very specific, such as \"wet coastal needle-leafed forests\".\nBiomes vary due to global variations in climate. Biomes are often defined by their structure: at a general level, for example, tropical forests, temperate grasslands, and arctic tundra. There can be any degree of subcategories among ecosystem types that comprise a biome, e.g., needle-leafed boreal forests or wet tropical forests. Although ecosystems are most commonly categorized by their structure and geography, there are also other ways to categorize and classify ecosystems such as by their level of human impact (see anthropogenic biome), or by their integration with social processes or technological processes or their novelty (e.g. novel ecosystem). Each of these taxonomies of ecosystems tends to emphasize different structural or functional properties. None of these is the \"best\" classification.\nEcosystem classifications are specific kinds of ecological classifications that consider all four elements of the definition of ecosystems: a biotic component, an abiotic complex, the interactions between and within them, and the physical space they occupy. Different approaches to ecological classifications have been developed in terrestrial, freshwater and marine disciplines, and a function-based typology has been proposed to leverage the strengths of these different approaches into a unified system.\nHuman interactions with ecosystems.\nHuman activities are important in almost all ecosystems. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.\nEcosystem goods and services.\nEcosystems provide a variety of goods and services upon which people depend. Ecosystem goods include the \"tangible, material products\" of ecosystem processes such as water, food, fuel, construction material, and medicinal plants. They also include less tangible items like tourism and recreation, and genes from wild plants and animals that can be used to improve domestic species.\nEcosystem services, on the other hand, are generally \"improvements in the condition or location of things of value\". These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. While material from the ecosystem had traditionally been recognized as being the basis for things of economic value, ecosystem services tend to be taken for granted.\nThe \"Millennium Ecosystem Assessment\" is an international synthesis by over 1000 of the world's leading biological scientists that analyzes the state of the Earth's ecosystems and provides summaries and guidelines for decision-makers. The report identified four major categories of ecosystem services: provisioning, regulating, cultural and supporting services. It concludes that human activity is having a significant and escalating impact on the biodiversity of the world ecosystems, reducing both their resilience and biocapacity. The report refers to natural systems as humanity's \"life-support system\", providing essential ecosystem services. The assessment measures 24 ecosystem services and concludes that only four have shown improvement over the last 50 years, 15 are in serious decline, and five are in a precarious condition.\nThe Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) is an intergovernmental organization established to improve the interface between science and policy on issues of biodiversity and ecosystem services. It is intended to serve a similar role to the Intergovernmental Panel on Climate Change. \nEcosystem services are limited and also threatened by human activities. To help inform decision-makers, many ecosystem services are being assigned economic values, often based on the cost of replacement with anthropogenic alternatives. The ongoing challenge of prescribing economic value to nature, for example through biodiversity banking, is prompting transdisciplinary shifts in how we recognize and manage the environment, social responsibility, business opportunities, and our future as a species.\nDegradation and decline.\nAs human population and per capita consumption grow, so do the resource demands imposed on ecosystems and the effects of the human ecological footprint. Natural resources are vulnerable and limited. The environmental impacts of anthropogenic actions are becoming more apparent. Problems for all ecosystems include: environmental pollution, climate change and biodiversity loss. For terrestrial ecosystems further threats include air pollution, soil degradation, and deforestation. For aquatic ecosystems threats also include unsustainable exploitation of marine resources (for example overfishing), marine pollution, microplastics pollution, the effects of climate change on oceans (e.g. warming and acidification), and building on coastal areas.\nMany ecosystems become degraded through human impacts, such as soil loss, air and water pollution, habitat fragmentation, water diversion, fire suppression, and introduced species and invasive species.\nThese threats can lead to abrupt transformation of the ecosystem or to gradual disruption of biotic processes and degradation of abiotic conditions of the ecosystem. Once the original ecosystem has lost its defining features, it is considered \"collapsed\" (see also IUCN Red List of Ecosystems). Ecosystem collapse could be reversible and in this way differs from species extinction. Quantitative assessments of the risk of collapse are used as measures of conservation status and trends.\nManagement.\nWhen natural resource management is applied to whole ecosystems, rather than single species, it is termed ecosystem management. Although definitions of ecosystem management abound, there is a common set of principles which underlie these definitions: A fundamental principle is the long-term sustainability of the production of goods and services by the ecosystem; \"intergenerational sustainability [is] a precondition for management, not an afterthought\". While ecosystem management can be used as part of a plan for wilderness conservation, it can also be used in intensively managed ecosystems (see, for example, agroecosystem and close to nature forestry).\nRestoration and sustainable development.\nIntegrated conservation and development projects (ICDPs) aim to address conservation and human livelihood (sustainable development) concerns in developing countries together, rather than separately as was often done in the past.\nSee also.\nTypes.\nThe following articles are types of ecosystems for particular types of regions or zones:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nInstances.\nEcosystem instances in specific regions of the world:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9633", "revid": "46264695", "url": "https://en.wikipedia.org/wiki?curid=9633", "title": "E (mathematical constant)", "text": "2.71828..., base of natural logarithms\nConstant value used in mathematics\nThe number e is a mathematical constant, approximately equal to 2.71828, that is the base of the natural logarithm and exponential function. It is sometimes called Euler's number, after the Swiss mathematician Leonhard Euler, though this can invite confusion with Euler numbers, or with Euler's constant, a different constant typically denoted formula_1. Alternatively, e can be called Napier's constant after John Napier. The Swiss mathematician Jacob Bernoulli discovered the constant while studying compound interest.\nThe number e is of great importance in mathematics, alongside 0, 1, \u03c0, and i. All five appear in one formulation of Euler's identity formula_2 and play important and recurring roles across mathematics. Like the constant \u03c0, e is irrational, meaning that it cannot be represented as a ratio of integers. Moreover, it is transcendental, meaning that it is not a root of any non-zero polynomial with rational coefficients. To 30 decimal places, the value of e is:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nDefinitions.\nThe number e is the limit \nformula_3\nan expression that arises in the computation of compound interest.\nIt is the sum of the infinite series\nformula_4\nIt is the unique positive number a such that the graph of the function \"y\" = \"a\"\"x\" has a slope of 1 at \"x\" = 0.\nOne has formula_5 where formula_6 is the (natural) exponential function, the unique function that equals its own derivative and satisfies the equation formula_7 Therefore, e is also the base of the natural logarithm, the inverse of the natural exponential function.\nThe number e can also be characterized in terms of an integral:\nformula_8\nFor other characterizations, see .\nHistory.\nThe first references to this constant were published in 1618 in the table of an appendix of a work on logarithms by John Napier. However, this did not contain the constant itself, but simply a list of logarithms to the base formula_9. It is assumed that the table was written by William Oughtred. In 1661, Christiaan Huygens studied how to compute logarithms by geometrical methods and calculated a quantity that, in retrospect, is the base-10 logarithm of e, but he did not recognize e itself as a quantity of interest.\nThe constant itself was introduced by Jacob Bernoulli in 1683, for solving the problem of continuous compounding of interest.\nIn his solution, the constant e occurs as the limit\nformula_10\nwhere n represents the number of intervals in a year on which the compound interest is evaluated (for example, formula_11 for monthly compounding).\nThe first symbol used for this constant was the letter b by Gottfried Leibniz in letters to Christiaan Huygens in 1690 and 1691.\nLeonhard Euler started to use the letter e for the constant in 1727 or 1728, in an unpublished paper on explosive forces in cannons, and in a letter to Christian Goldbach on 25 November 1731. The first appearance of e in a printed publication was in Euler's \"Mechanica\" (1736). It is unknown why Euler chose the letter e. Although some researchers used the letter c in the subsequent years, the letter e was more common and eventually became standard.\nEuler proved that e is the sum of the infinite series\nformula_12\nwhere \"n\"! is the factorial of n. The equivalence of the two characterizations using the limit and the infinite series can be proved via the binomial theorem.\nApplications.\nCompound interest.\nJacob Bernoulli discovered this constant in 1683, while studying a question about compound interest:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;An account starts with $1.00 and pays 100 percent interest per year. If the interest is credited once, at the end of the year, the value of the account at year-end will be $2.00. What happens if the interest is computed and credited more frequently during the year?\nIf the interest is credited twice in the year, the interest rate for each 6 months will be 50%, so the initial $1 is multiplied by 1.5 twice, yielding $1.00 \u00d7 1.52 = $2.25 at the end of the year. Compounding quarterly yields $1.00 \u00d7 1.254 = $2.44140625, and compounding monthly yields $1.00 \u00d7 (1 + 1/12)12 = $2.613035... If there are n compounding intervals, the interest for each interval will be 100%/\"n\" and the value at the end of the year will be $1.00\u00a0\u00d7\u00a0(1 + 1/\"n\")\"n\".\nBernoulli noticed that this sequence approaches a limit (the force of interest) with larger n and, thus, smaller compounding intervals. Compounding weekly (\"n\" = 52) yields $2.692596..., while compounding daily (\"n\" = 365) yields $2.714567... (approximately two cents more). The limit as n grows large is the number that came to be known as e. That is, with \"continuous\" compounding, the account value will reach $2.718281828... More generally, an account that starts at $1 and offers an annual interest rate of R will, after t years, yield \"e\"\"Rt\" dollars with continuous compounding. Here, R is the decimal equivalent of the rate of interest expressed as a \"percentage\", so for 5% interest, \"R\" = 5/100 = 0.05.\nBernoulli trials.\nThe number e itself also has applications in probability theory, in a way that is not obviously related to exponential growth. Suppose that a gambler plays a slot machine that pays out with a probability of one in n and plays it n times. As n increases, the probability that gambler will lose all n bets approaches 1/\"e\", which is approximately 36.79%. For \"n\" = 20, this is already 1/2.789509... (approximately 35.85%).\nThis is an example of a Bernoulli trial process. Each time the gambler plays the slots, there is a one in n chance of winning. Playing n times is modeled by the binomial distribution, which is closely related to the binomial theorem and Pascal's triangle. The probability of winning k times out of n trials is:\nformula_13\nIn particular, the probability of winning zero times (\"k\" = 0) is\nformula_14\nThe limit of the above expression, as n tends to infinity, is precisely 1/\"e\".\nExponential growth and decay.\nExponential growth is a process that increases quantity over time at an ever-increasing rate. It occurs when the instantaneous rate of change (that is, the derivative) of a quantity with respect to time is proportional to the quantity itself. Described as a function, a quantity undergoing exponential growth is an exponential function of time, that is, the variable representing time is the exponent (in contrast to other types of growth, such as quadratic growth). If the constant of proportionality is negative, then the quantity decreases over time, and is said to be undergoing exponential decay instead. The law of exponential growth can be written in different but mathematically equivalent forms, by using a different base, for which the number e is a common and convenient choice:\nformula_15\nHere, formula_16 denotes the initial value of the quantity x, k is the growth constant, and formula_17 is the time it takes the quantity to grow by a factor of e.\nStandard normal distribution.\nThe normal distribution with zero mean and unit standard deviation is known as the \"standard normal distribution\", given by the probability density function\nformula_18\nThe constraint of unit standard deviation (and thus also unit variance) results in the in the exponent, and the constraint of unit total area under the curve formula_19 results in the factor formula_20. This function is symmetric around \"x\" = 0, where it attains its maximum value formula_20, and has inflection points at \"x\" = \u00b11.\nDerangements.\nAnother application of e, also discovered in part by Jacob Bernoulli along with Pierre Remond de Montmort, is in the problem of derangements, also known as the \"hat check problem\": n guests are invited to a party and, at the door, the guests all check their hats with the butler, who in turn places the hats into n boxes, each labelled with the name of one guest. But the butler has not asked the identities of the guests, and so puts the hats into boxes selected at random. The problem of de Montmort is to find the probability that \"none\" of the hats gets put into the right box. This probability, denoted by formula_22, is:\nformula_23\nAs n tends to infinity, \"p\"\"n\" approaches 1/\"e\". Furthermore, the number of ways the hats can be placed into the boxes so that none of the hats are in the right box is \"n\"!/\"e\", rounded to the nearest integer, for every positive\u00a0n.\nOptimal planning problems.\nThe maximum value of formula_24 occurs at formula_25. Equivalently, for any value of the base \"b\" &gt; 1, it is the case that the maximum value of formula_26 occurs at formula_25 (Steiner's problem, discussed below).\nThis is useful in the problem of a stick of length L that is broken into n equal parts. The value of n that maximizes the product of the lengths is then either\nformula_28 or formula_29\nThe quantity formula_26 is also a measure of information gleaned from an event occurring with probability formula_31 (approximately formula_32 when formula_33), so that essentially the same optimal division appears in optimal planning problems like the secretary problem.\nAsymptotics.\nThe number e occurs naturally in connection with many problems involving asymptotics. An example is Stirling's formula for the asymptotics of the factorial function, in which both the numbers e and \u03c0 appear:\nformula_34\nAs a consequence,\nformula_35\nProperties.\nCalculus.\nThe principal motivation for introducing the number e, particularly in calculus, is to perform differential and integral calculus with exponential functions and logarithms. A general exponential has a derivative, given by a limit:\nformula_36\nThe parenthesized limit on the right is independent of the Its value turns out to be the logarithm of a to base e. Thus, when the value of a is set this limit is equal and so one arrives at the following simple identity:\nformula_37\nConsequently, the exponential function with base e is particularly suited to doing calculus. (as opposed to some other number) as the base of the exponential function makes calculations involving the derivatives much simpler.\nAnother motivation comes from considering the derivative of the base-a logarithm (i.e., log\"a\" \"x\"), for\u00a0\"x\" &gt; 0:\nformula_38\nwhere the substitution \"u\" \n \"h\"/\"x\" was made. The base-a logarithm of e is 1, if a equals e. So symbolically,\nformula_39\nThe logarithm with this special base is called the natural logarithm, and is usually denoted as ln; it behaves well under differentiation since there is no undetermined limit to carry through the calculations.\nThus, there are two ways of selecting such special numbers a. One way is to set the derivative of the exponential function \"a\"\"x\" equal to \"a\"\"x\", and solve for a. The other way is to set the derivative of the base a logarithm to 1/\"x\" and solve for a. In each case, one arrives at a convenient choice of base for doing calculus. It turns out that these two solutions for a are actually \"the same\": the number e.\nThe Taylor series for the exponential function can be deduced from the facts that the exponential function is its own derivative and that it equals 1 when evaluated at 0:\nformula_40\nSetting formula_41 recovers the definition of e as the sum of an infinite series.\nThe natural logarithm function can be defined as the integral from 1 to formula_42 of formula_43, and the exponential function can then be defined as the inverse function of the natural logarithm. The number e is the value of the exponential function evaluated at formula_41, or equivalently, the number whose natural logarithm is 1. It follows that e is the unique positive real number such that\nformula_45\nBecause \"e\"\"x\" is the unique function (up to multiplication by a constant K) that is equal to its own derivative,\nformula_46\nit is therefore its own antiderivative as well:\nformula_47\nEquivalently, the family of functions\nformula_48\nwhere K is any real or complex number, is the full solution to the differential equation\nformula_49\nInequalities.\nThe number e is the unique real number such that\nformula_50\nfor all positive x.\nAlso, we have the inequality\nformula_51\nfor all real x, with equality if and only if \"x\" \n 0. Furthermore, e is the unique base of the exponential for which the inequality \"a\"\"x\" \u2265 \"x\" + 1 holds for all x. This is a limiting case of Bernoulli's inequality.\nExponential-like functions.\nSteiner's problem asks to find the global maximum for the function\nformula_52\nThis maximum occurs precisely at \"x\" \n \"e\". (One can check that the derivative of ln \"f\"(\"x\") is zero only for this value of\u00a0x.)\nSimilarly, \"x\" \n 1/\"e\" is where the global minimum occurs for the function\nformula_53\nThe infinite tetration\nformula_54 or formula_55\nconverges if and only if \"x\" \u2208 [(1/\"e\")\"e\", \"e\"1/\"e\"] \u2248 [0.06599, 1.4447], shown by a theorem of Leonhard Euler.\nNumber theory.\nThe real number e is irrational. Euler proved this by showing that its simple continued fraction expansion does not terminate. (See also Fourier's proof that e is irrational.)\nFurthermore, by the Lindemann\u2013Weierstrass theorem, e is transcendental, meaning that it is not a solution of any non-zero polynomial equation with rational coefficients. It was the first number to be proved transcendental without having been specifically constructed for this purpose (compare with Liouville number); the proof was given by Charles Hermite in 1873. The number e is one of only a few transcendental numbers for which the exact irrationality exponent is known (given by formula_56).\nAn unsolved problem thus far is the question of whether or not the numbers e and \u03c0 are algebraically independent. This would be resolved by Schanuel's conjecture \u2013 a currently unproven generalization of the Lindemann\u2013Weierstrass theorem.\nIt is conjectured that e is normal, meaning that when e is expressed in any base the possible digits in that base are uniformly distributed (occur with equal probability in any sequence of given length).\nIn algebraic geometry, a \"period\" is a number that can be expressed as an integral of an algebraic function over an algebraic domain. The constant \u03c0 is a period, but it is conjectured that e is not.\nComplex numbers.\nThe exponential function \"e\"\"x\" may be written as a Taylor series\nformula_57\nBecause this series is convergent for every complex value of x, it is commonly used to extend the definition of \"e\"\"x\" to the complex numbers. This, with the Taylor series for sin and cos \"x\", allows one to derive Euler's formula:\nformula_58\nwhich holds for every complex x. The special case with \"x\" \n \u03c0 is Euler's identity:\nformula_59\nwhich is considered to be an exemplar of mathematical beauty as it shows a profound connection between the most fundamental numbers in mathematics. In addition, it is directly used in a proof that \u03c0 is transcendental, which implies the impossibility of squaring the circle. Moreover, the identity implies that, in the principal branch of the logarithm,\nformula_60\nFurthermore, using the laws for exponentiation,\nformula_61\nfor any integer n, which is de Moivre's formula.\nThe expressions of formula_62 and formula_63 in terms of the exponential function can be deduced from the Taylor series:\nformula_64\nThe expression\nformula_65\nis sometimes abbreviated as formula_66.\nEntropy.\nThe constant formula_9 plays a distinguished role in the theory of entropy in probability theory and ergodic theory. The basic idea is to consider a partition of a probability space into a finite number of measurable sets, formula_68, the entropy of which is the expected information gained regarding the probability distribution by performing a random sample (or \"experiment\"). The entropy of the partition is\nformula_69\nThe function formula_70 is thus of fundamental importance, representing the amount of entropy contributed by a particular element of the partition, formula_71. This function is maximized when formula_72. What this means, concretely, is that the entropy contribution of the particular event formula_73 is maximized when formula_74; outcomes that are either too likely or too rare contribute less to the total entropy.\nRepresentations.\nThe number e can be represented in a variety of ways: as an infinite series, an infinite product, a continued fraction, or a limit of a sequence. In addition to the limit and the series given above, there is also the simple continued fraction\nformula_75\nwhich written out looks like\nformula_76\nThe following infinite product evaluates to e:\nformula_77\nMany other series, sequence, continued fraction, and infinite product representations of e have been proved.\nStochastic representations.\nIn addition to exact analytical expressions for representation of e, there are stochastic techniques for estimating e. One such approach begins with an infinite sequence of independent random variables \"X\"1, \"X\"2..., drawn from the uniform distribution on [0, 1]. Let V be the least number n such that the sum of the first n observations exceeds 1:\nformula_78\nThen the expected value of V is e: E(\"V\") \n \"e\".\nKnown digits.\nThe number of known digits of e has increased substantially since the introduction of the computer, due both to increasing performance of computers and to algorithmic improvements.\nSince around 2010, the proliferation of modern high-speed desktop computers has made it feasible for amateurs to compute trillions of digits of e within acceptable amounts of time. On December 24, 2023, a record-setting calculation was made by Jordan Ranous, giving e to 35,000,000,000,000 digits.\nComputing the digits.\nOne way to compute the digits of e is with the series\nformula_79\nA faster method involves two recursive functions formula_80 and formula_81. The functions are defined as formula_82\nThe expression formula_83 produces the nth partial sum of the series above. This method uses binary splitting to compute e with fewer single-digit arithmetic operations and thus reduced bit complexity. Combining this with fast Fourier transform-based methods of multiplying integers makes computing the digits very fast.\nIn computer culture.\nDuring the emergence of internet culture, individuals and organizations sometimes paid homage to the number e.\nIn an early example, the computer scientist Donald Knuth let the version numbers of his program Metafont approach e. The versions are 2, 2.7, 2.71, 2.718, and so forth.\nIn another instance, the IPO filing for Google in 2004, rather than a typical round-number amount of money, the company announced its intention to raise 2,718,281,828 USD, which is e billion dollars rounded to the nearest dollar.\nGoogle was also responsible for a billboard\nthat appeared in the heart of Silicon Valley, and later in Cambridge, Massachusetts; Seattle, Washington; and Austin, Texas. It read \"{first 10-digit prime found in consecutive digits of e}.com\". The first 10-digit prime in e is 7427466391, which starts at the 99th digit. Solving this problem and visiting the advertised (now defunct) website led to an even more difficult problem to solve, which consisted of finding the fifth term in the sequence 7182818284, 8182845904, 8747135266, 7427466391. It turned out that the sequence consisted of 10-digit numbers found in consecutive digits of e whose digits summed to 49. The fifth term in the sequence is 5966290435, which starts at the 127th digit.\nSolving this second problem finally led to a Google Labs webpage where the visitor was invited to submit a r\u00e9sum\u00e9.\nThe last release of the official Python 2 interpreter has version number 2.7.18, a reference to \"e\".\nIn computing.\nIn scientific computing, the constant formula_9 is often hard-coded. For example, the Python standard library includes codice_1, a floating-point approximation of formula_9. Despite this, it is generally more numerically stable and efficient to use the built-in exponential function\u2014such as codice_2 in Python\u2014rather than computing formula_86 via codice_3, even when formula_42 is an integer.\nMost implementations of the exponential function use range reduction, lookup tables, and polynomial or rational approximations (such as Pad\u00e9 approximants or Taylor expansions) to achieve accurate results across a wide range of inputs. In contrast, general-purpose exponentiation functions\u2014like codice_4\u2014may involve additional intermediate computations, such as logarithms and multiplications, and may accumulate more rounding error, particularly when formula_9 is used in floating-point form.\nAt very high precision, methods based on elliptic functions and fast convergence of the AGM and Newton's method can be used to compute the exponential function. The digit expansion of formula_9 can then be obtained as formula_90 Although this is asymptotically faster than other known methods for computing the exponential function, it is impractical because of high overhead cost.\nTools such as y-cruncher are optimized for computing many digits of individual constants like formula_9, and use the Taylor series for formula_9 because it converges very rapidly, especially when combined with various optimizations. In particular, the method of binary splitting applies to computing the series for formula_9, as opposed to the series for formula_94, because the summands in the former series are simple rational numbers. This allows the complexity of computing formula_95 digits of formula_9 to be reduced to formula_97, asymptotically the same as AGM methods, but much cheaper in practice.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9635", "revid": "4626", "url": "https://en.wikipedia.org/wiki?curid=9635", "title": "Euler - Maclaurin formula", "text": ""}
{"id": "9637", "revid": "19655214", "url": "https://en.wikipedia.org/wiki?curid=9637", "title": "Euler\u2013Maclaurin formula", "text": "Summation formula\nIn mathematics, the Euler\u2013Maclaurin formula is a formula for the difference between an integral and a closely related sum. It can be used to approximate integrals by finite sums, or conversely to evaluate finite sums and infinite series using integrals and the machinery of calculus. For example, many asymptotic expansions are derived from the formula, and Faulhaber's formula for the sum of powers is an immediate consequence.\nThe formula was discovered independently by Leonhard Euler and Colin Maclaurin around 1735. Euler needed it to compute slowly converging infinite series while Maclaurin used it to calculate integrals. It was later generalized to Darboux's formula.\nThe formula.\nIf m and n are natural numbers and \"f\"(\"x\") is a real or complex valued continuous function for real numbers x in the interval [\"m\",\"n\"], then the integral\nformula_1\ncan be approximated by the sum (or vice versa)\nformula_2\n(see rectangle method). The Euler\u2013Maclaurin formula provides expressions for the difference between the sum and the integral in terms of the higher derivatives \"f\"(\"k\")(\"x\") evaluated at the endpoints of the interval, that is to say \"x\" \n \"m\" and \"x\" \n \"n\".\nExplicitly, for p a positive integer and a function \"f\"(\"x\") that is p times continuously differentiable on the interval [\"m\",\"n\"], we have\nformula_3\nwhere Bk is the kth Bernoulli number (with \"B\"1 \n ) and Rp is an error term which depends on n, m, p, and f and is usually small for suitable values of p.\nThe formula is often written with the subscript taking only even values, since the odd Bernoulli numbers are zero except for \"B\"1. In this case we have\nformula_4\nor alternatively\nformula_5\nThe remainder term.\nThe remainder term arises because the integral is usually not exactly equal to the sum. The formula may be derived by applying repeated integration by parts to successive intervals [\"r\", \"r\" + 1] for \"r\" \n \"m\", \"m\" + 1, \u2026, \"n\" \u2212 1. The boundary terms in these integrations lead to the main terms of the formula, and the leftover integrals form the remainder term.\nThe remainder term has an exact expression in terms of the periodized Bernoulli functions \"Pk\"(\"x\"). The Bernoulli polynomials may be defined recursively by \"B\"0(\"x\") \n 1 and, for \"k\" \u2265 1,\nformula_6\nThe periodized Bernoulli functions are defined as\nformula_7\nwhere \u230a\"x\"\u230b denotes the largest integer less than or equal to x, so that \"x\" \u2212 \u230a\"x\"\u230b always lies in the interval [0,1).\nWith this notation, the remainder term Rp equals\nformula_8\nWhen \"k\" &gt; 0, it can be shown that for 0 \u2264 \"x\" \u2264 1,\nformula_9\nwhere \u03b6 denotes the Riemann zeta function; one approach to prove this inequality is to obtain the Fourier series for the polynomials \"Bk\"(\"x\"). The bound is achieved for even k when x is zero. The term \"\u03b6\"(\"k\") may be omitted for odd k but the proof in this case is more complex (see Lehmer). Using this inequality, the size of the remainder term can be estimated as\nformula_10\nLow-order cases.\nThe Bernoulli numbers from \"B\"1 to \"B\"7 are , , 0, \u2212, 0, , 0. Therefore, the low-order cases of the Euler\u2013Maclaurin formula are:\nformula_11\nApplications.\nThe Basel problem.\nThe Basel problem is to determine the sum\nformula_12\nEuler computed this sum to 20 decimal places with only a few terms of the Euler\u2013Maclaurin formula in 1735. This probably convinced him that the sum equals , which he proved in the same year.\nSums involving a polynomial.\nIf f is a polynomial and p is big enough, then the remainder term vanishes. For instance, if \"f\"(\"x\") \n \"x\"3, we can choose \"p\" \n 2 to obtain, after simplification,\nformula_13\nApproximation of integrals.\nThe formula provides a means of approximating a finite integral. Let \"a\" &lt; \"b\" be the endpoints of the interval of integration. Fix N, the number of points to use in the approximation, and denote the corresponding step size by \"h\" \n . Set \"xi\" \n \"a\" + (\"i\" \u2212 1)\"h\", so that \"x\"1 \n \"a\" and \"xN\" \n \"b\". Then:\nformula_14\nThis may be viewed as an extension of the trapezoid rule by the inclusion of correction terms. Note that this asymptotic expansion is usually not convergent; there is some p, depending upon f and h, such that the terms past order p increase rapidly. Thus, the remainder term generally demands close attention.\nThe Euler\u2013Maclaurin formula is also used for detailed error analysis in numerical quadrature. It explains the superior performance of the trapezoidal rule on smooth periodic functions and is used in certain extrapolation methods. Clenshaw\u2013Curtis quadrature is essentially a change of variables to cast an arbitrary integral in terms of integrals of periodic functions where the Euler\u2013Maclaurin approach is very accurate (in that particular case the Euler\u2013Maclaurin formula takes the form of a discrete cosine transform). This technique is known as a periodizing transformation.\nAsymptotic expansion of sums.\nIn the context of computing asymptotic expansions of sums and series, usually the most useful form of the Euler\u2013Maclaurin formula is\nformula_15\nwhere a and b are integers. Often the expansion remains valid even after taking the limits \"a\" \u2192 \u2212\u221e or \"b\" \u2192 +\u221e or both. In many cases the integral on the right-hand side can be evaluated in closed form in terms of elementary functions even though the sum on the left-hand side cannot. Then all the terms in the asymptotic series can be expressed in terms of elementary functions. For example,\nformula_16\nHere the left-hand side is equal to \"\u03c8\"(1)(\"z\"), namely the first-order polygamma function defined by\nformula_17\nthe gamma function \u0393(\"z\") is equal to (\"z\" \u2212 1)! when z is a positive integer. This results in an asymptotic expansion for \"\u03c8\"(1)(\"z\"). That expansion, in turn, serves as the starting point for one of the derivations of precise error estimates for Stirling's approximation of the factorial function.\nExamples.\nIf s is an integer greater than 1 then we have the following asymptotic expansions as formula_18, using the Riemann zeta function formula_19:\nformula_20\nFor s equal to 2 this simplifies to\nformula_21\nor\nformula_22\nThis is related to the trigamma function:\nformula_23\nWhen \"s\" \n 1, the sum and the integral go to infinity but the difference between them goes to a limit, the Euler\u2013Mascheroni constant ,\"\u03b3\" \u2248 0.5772... This gives:\nformula_24\nfrom which we have the asymptotic expansion:\nformula_25\nThese harmonic numbers are related to the digamma function:\nformula_26\nMore generally,\nformula_27\nProofs.\nDerivation by mathematical induction.\nWe outline the argument given in Apostol.\nThe Bernoulli polynomials \"Bn\"(\"x\") and the periodic Bernoulli functions \"Pn\"(\"x\") for \"n\" \n 0, 1, 2, ... were introduced above.\nThe first several Bernoulli polynomials are\nformula_28\nThe values \"Bn\"(1) are the Bernoulli numbers \"B\"\"n\". Notice that for \"n\" \u2260 1 we have\nformula_29\nand for \"n\" \n 1,\nformula_30\nThe functions \"P\"\"n\" agree with the Bernoulli polynomials on the interval [0,\u00a01] and are periodic with period 1. Furthermore, except when \"n\" \n 1, they are also continuous. Thus,\nformula_31\nLet \"k\" be an integer, and consider the integral\nformula_32\nwhere\nformula_33\nIntegrating by parts, we get\nformula_34\nUsing \"B\"1(0) \n \u2212, \"B\"1(1) \n , and summing the above from \"k\" \n 0 to \"k\" \n \"n\" \u2212 1, we get\nformula_35\nAdding to both sides and rearranging, we have\nformula_36\nThis is the \"p\" \n 1 case of the summation formula. To continue the induction, we apply integration by parts to the error term:\nformula_37\nwhere\nformula_38\nThe result of integrating by parts is\nformula_39\nSumming from \"k\" \n 0 to \"k\" \n \"n\" \u2212 1 and substituting this for the lower order error term results in the \"p\" \n 2 case of the formula,\nformula_40\nThis process can be iterated. In this way we get a proof of the Euler\u2013Maclaurin summation formula which can be formalized by mathematical induction, in which the induction step relies on integration by parts and on identities for periodic Bernoulli functions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9638", "revid": "85121", "url": "https://en.wikipedia.org/wiki?curid=9638", "title": "Epimenides paradox", "text": "Paradox revealing a problem with self-reference in logic\nThe Epimenides paradox reveals a problem with self-reference in logic. It is named after the Cretan philosopher Epimenides of Knossos (alive circa 600 BC) who is credited with the original statement. A typical description of the problem is given in the book \"G\u00f6del, Escher, Bach\", by Douglas Hofstadter:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Epimenides was a Cretan who made the immortal statement: \"All Cretans are liars.\"\nA paradox of self-reference arises when one considers whether it is possible for Epimenides to have spoken the truth.\nMythology of lying Cretans.\nAccording to Ptolemaeus Chennus, Thetis and Medea had once argued in Thessaly over which was the most beautiful; they appointed the Cretan Idomeneus as the judge, who gave the victory to Thetis. In her anger, Medea called all Cretans liars, and cursed them to never say the truth.\nLogical paradox.\nThomas Fowler (1869) states the paradox as follows: \"Epimenides the Cretan says, 'that all the Cretans are liars,' but Epimenides is himself a Cretan; therefore he is himself a liar. But if he is a liar, what he says is untrue, and consequently, the Cretans are veracious; but Epimenides is a Cretan, and therefore what he says is true; saying the Cretans are liars, Epimenides is himself a liar, and what he says is untrue. Thus we may go on alternately proving that Epimenides and the Cretans are truthful and untruthful.\"\nIf we assume the statement is false and that Epimenides is lying about all Cretans being liars, then there must exist at least one Cretan who is honest. This does not lead to a contradiction since it is not required that this Cretan be Epimenides. This means that Epimenides can say the false statement that all Cretans are liars while knowing at least one honest Cretan and lying about this particular Cretan. Hence, from the assumption that the statement is false, it does not follow that the statement is true. So we can avoid a paradox as seeing the statement \"all Cretans are liars\" as a false statement, which is made by a lying Cretan, Epimenides. The mistake made by Thomas Fowler (and many other people) above is to think that the negation of \"all Cretans are liars\" is \"all Cretans are honest\" (a paradox) when in fact the negation is \"there exists a Cretan who is honest\", or \"not all Cretans are liars\". The Epimenides paradox can be slightly modified as to not allow the kind of solution described above, as it was in the first paradox of Eubulides but instead leading to a non-avoidable self-contradiction. Paradoxical versions of the Epimenides problem are closely related to a class of more difficult logical problems, including the liar paradox, Socratic paradox and the Burali-Forti paradox, all of which have self-reference in common with Epimenides. The Epimenides paradox is usually classified as a variation on the liar paradox, and sometimes the two are not distinguished. The study of self-reference led to important developments in logic and mathematics in the twentieth century.\nIn other words, it is not a paradox once one realizes \"All Cretans are liars\" being untrue only means \"Not all Cretans are liars\" instead of the assumption that \"All Cretans are honest\".\nPerhaps better put, for \"All Cretans are liars\" to be a true statement, it does not mean that all Cretans must lie all the time. In fact, Cretans could tell the truth quite often, but still all be liars in the sense that liars are people prone to deception for dishonest gain. Considering that \"All Cretans are liars\" has been seen as a paradox only since the 19th century, this seems to resolve the alleged paradox. If 'all Cretans are continuous liars' is actually true, then asking a Cretan if they are honest would always elicit the dishonest answer 'yes'. So arguably the original proposition is not so much paradoxical as invalid.\nA contextual reading of the contradiction may also provide an answer to the paradox. The original phrase, \"The Cretans, always liars, evil beasts, idle bellies!\" asserts not an intrinsic paradox, but rather an opinion of the Cretans from Epimenides. A stereotyping of his people not intended to be an absolute statement about the people as a whole. Rather it is a claim made about their position regarding their religious beliefs and socio-cultural attitudes. Within the context of his poem the phrase is specific to a certain belief, a context that Callimachus repeats in his poem regarding Zeus. Further, a more poignant answer to the paradox is simply that to be a \"liar\" is to state falsehoods, nothing in the statement asserts everything said is false, but rather they're \"always\" lying. This is not an absolute statement of fact and thus we cannot conclude there's a true contradiction made by Epimenides with this statement.\nOrigin of the phrase.\nEpimenides was a 6th-century BC philosopher and religious prophet who, against the general sentiment of Crete, proposed that Zeus was immortal, as in the following poem:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;They fashioned a tomb for thee, O holy and high one&lt;br&gt;The Cretans, always liars, evil beasts, idle bellies!&lt;br&gt;But thou art not dead: thou livest and abidest forever,&lt;br&gt;For in thee we live and move and have our being.\u2014\u200a\nDenying the immortality of Zeus, then, was the lie of the Cretans.\nThe phrase \"Cretans, always liars\" was quoted by the poet Callimachus in his \"Hymn to Zeus\", with the same theological intent as Epimenides:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;O Zeus, some say that thou wert born on the hills of Ida;&lt;br&gt; Others, O Zeus, say in Arcadia;&lt;br&gt; Did these or those, O Father lie? -- \"Cretans are ever liars\". &lt;br&gt;Yea, a tomb, O Lord, for thee the Cretans builded;&lt;br&gt; But thou didst not die, for thou art for ever.\u2014\u200a\nEmergence as a logical contradiction.\nThe logical inconsistency of a Cretan asserting all Cretans are always liars may not have occurred to Epimenides, nor to Callimachus, who both used the phrase to emphasize their point, without irony, perhaps meaning that all Cretans lie routinely, but not exclusively.\nIn the 1st century AD, the quote is mentioned by the author of the Epistle to Titus as having been spoken truly by \"one of their own prophets.\"\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"One of Crete's own prophets has said it: 'Cretans are always liars, evil brutes, idle bellies'.&lt;br&gt;He has surely told the truth. For this reason correct them sternly, that they may be sound in faith instead of paying attention to Jewish fables and to commandments of people who turn their backs on the truth.\"\u2014\u200a\nClement of Alexandria, in the late 2nd century AD, fails to indicate that the concept of logical paradox is an issue:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In his epistle to Titus, Apostle Paul wants to warn Titus that Cretans don't believe in the one truth of Christianity, because \"Cretans are always liars\". To justify his claim, Apostle Paul cites Epimenides. \u2014\u200a\nDuring the early 4th century, Saint Augustine restates the closely related liar paradox in \"Against the Academicians\" (III.13.29), but without mentioning Epimenides.\nIn the Middle Ages, many forms of the liar paradox were studied under the heading of insolubilia, but these were not explicitly associated with Epimenides.\nFinally, in 1740, the second volume of Pierre Bayle's \"Dictionnaire Historique et Critique\" explicitly connects Epimenides with the paradox, though Bayle labels the paradox a \"sophisme\".\nReferences by other authors.\nAll of the works of Epimenides are now lost, and known only through quotations by other authors. The quotation from the \"Cretica\" of Epimenides is given by R.N. Longenecker, \"Acts of the Apostles\", in volume 9 of \"The Expositor's Bible Commentary\", Frank E. Gaebelein, editor (Grand Rapids, Michigan: Zondervan Corporation, 1976\u20131984), page 476. Longenecker in turn cites M.D. Gibson, \"Horae Semiticae X\" (Cambridge: Cambridge University Press, 1913), page 40, \"in Syriac\". Longenecker states the following in a footnote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Syr. version of the quatrain comes to us from the Syr. church father Isho'dad of Merv (probably based on the work of Theodore of Mopsuestia), which J.R. Harris translated back into Gr. in Exp [\"The Expositor\"] 7 (1907), p 336.\nAn oblique reference to Epimenides in the context of logic appears in \"The Logical Calculus\" by W. E. Johnson, \"Mind\" (New Series), volume 1, number 2 (April, 1892), pages 235\u2013250. Johnson writes in a footnote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Compare, for example, such occasions for fallacy as are supplied by \"Epimenides is a liar\" or \"That surface is red,\" which may be resolved into \"All or some statements of Epimenides are false,\" \"All or some of the surface is red.\"\nThe Epimenides paradox appears explicitly in \"Mathematical Logic as Based on the Theory of Types\", by Bertrand Russell, in the \"American Journal of Mathematics\", volume 30, number 3 (July, 1908), pages 222\u2013262, which opens with the following:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The oldest contradiction of the kind in question is the Epimenides. Epimenides the Cretan said that all Cretans were liars, and all other statements made by Cretans were certainly lies. Was this a lie?\nIn that article, Russell uses the Epimenides paradox as the point of departure for discussions of other problems, including the Burali-Forti paradox and the paradox now called Russell's paradox. Since Russell, the Epimenides paradox has been referenced repeatedly in logic. Typical of these references is \"G\u00f6del, Escher, Bach\" by Douglas Hofstadter, which accords the paradox a prominent place in a discussion of self-reference.\nIt is also believed that the \"Cretan tales\" told by Odysseus in \"The Odyssey\" by Homer are a reference to this paradox.\nIn \"The Second Sex\" (1949) Simone de Beauvoir writes \"I think certain women are still best suited to elucidate the situation of women. It is a sophism to claim that Epimenides should be enclosed within the concept of Cretan and all Cretans within the concept of liar: it is not a mysterious essence that dictates good or bad faith to men and women\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9640", "revid": "21479757", "url": "https://en.wikipedia.org/wiki?curid=9640", "title": "Engine", "text": "Machine that converts one or more forms of energy into mechanical energy (of motion)\nAn engine or motor is a machine designed to convert one or more forms of energy into mechanical energy.\nAvailable energy sources include potential energy (e.g. energy of the Earth's gravitational field as exploited in hydroelectric power generation), heat energy (e.g. geothermal), chemical energy, electric potential and nuclear energy (from nuclear fission or nuclear fusion). Many of these processes generate heat as an intermediate energy form; thus heat engines have special importance. Some natural processes, such as atmospheric convection cells convert environmental heat into motion (e.g. in the form of rising air currents). Mechanical energy is of particular importance in transportation, but also plays a role in many industrial processes such as cutting, grinding, crushing, and mixing.\nMechanical heat engines convert heat into work via various thermodynamic processes. The internal combustion engine is perhaps the most common example of a mechanical heat engine in which heat from the combustion of a fuel causes rapid pressurisation of the gaseous combustion products in the combustion chamber, causing them to expand and drive a piston, which turns a crankshaft. Unlike internal combustion engines, a reaction engine (such as a jet engine) produces thrust by expelling reaction mass, in accordance with Newton's third law of motion.\nApart from heat engines, electric motors convert electrical energy into mechanical motion, pneumatic motors use compressed air, and clockwork motors in wind-up toys use elastic energy. In biological systems, molecular motors, like myosins in muscles, use chemical energy to create forces and ultimately motion (a chemical engine, but not a heat engine).\nChemical heat engines which employ air (ambient atmospheric gas) as a part of the fuel reaction are regarded as airbreathing engines. Chemical heat engines designed to operate outside of Earth's atmosphere (e.g. rockets, deeply submerged submarines) need to carry an additional fuel component called the oxidizer (although there exist super-oxidizers suitable for use in rockets, such as fluorine, a more powerful oxidant than oxygen itself); or the application needs to obtain heat by non-chemical means, such as by means of nuclear reactions.\nEmission/Byproducts.\nAll chemically fueled heat engines emit exhaust gases. The cleanest engines emit water only. Strict zero-emissions generally means zero emissions other than water and water vapour. Only heat engines which combust pure hydrogen (fuel) and pure oxygen (oxidizer) achieve zero-emission by a strict definition (in practice, one type of rocket engine). If hydrogen is burnt in combination with air (all airbreathing engines), a side reaction occurs between atmospheric oxygen and atmospheric nitrogen resulting in small emissions of . If a hydrocarbon (such as alcohol or gasoline) is burnt as fuel, CO2, a greenhouse gas, is emitted. Hydrogen and oxygen from air can be reacted into water by a fuel cell without side production of , but this is an electrochemical engine not a heat engine.\nTerminology.\nThe word \"engine\" derives from Old French , from the Latin \u2013the root of the word . Pre-industrial weapons of war, such as catapults, trebuchets and battering rams, were called \"siege engines\", and knowledge of how to construct them was often treated as a military secret. The word \"gin\", as in \"cotton gin\", is short for \"engine\". Most mechanical devices invented during the Industrial Revolution were described as engines\u2014the steam engine being a notable example. However, the original steam engines, such as those by Thomas Savery, were not mechanical engines but pumps. In this manner, a fire engine in its original form was merely a water pump, with the engine being transported to the fire by horses.\nIn modern usage, the term \"engine\" typically describes devices, like steam engines and internal combustion engines, that burn or otherwise consume fuel to perform mechanical work by exerting a torque or linear force (usually in the form of thrust). Devices converting heat energy into motion are commonly referred to simply as \"engines\". Examples of engines which exert a torque include the familiar automobile gasoline and diesel engines, as well as turboshafts. Examples of engines which produce thrust include turbofans and rockets.\nWhen the internal combustion engine was invented, the term \"motor\" was initially used to distinguish it from the steam engine\u2014which was in wide use at the time, powering locomotives and other vehicles such as steam rollers. The term \"motor\" derives from the Latin verb which means 'to set in motion', or 'maintain motion'. Thus a motor is a device that imparts motion.\n\"Motor\" and \"engine\" are interchangeable in standard English. In some engineering jargons, the two words have different meanings, in which \"engine\" is a device that burns or otherwise consumes fuel, changing its chemical composition, and a motor is a device driven by electricity, air, or hydraulic pressure, which does not change the chemical composition of its energy source. However, rocketry uses the term rocket motor, even though they consume fuel.\nA heat engine may also serve as a \"prime mover\"\u2014a component that transforms the flow or changes in pressure of a fluid into mechanical energy. An automobile powered by an internal combustion engine may make use of various motors and pumps, but ultimately all such devices derive their power from the engine. Another way of looking at it is that a motor receives power from an external source, and then converts it into mechanical energy, while an engine creates power from pressure (derived directly from the explosive force of combustion or other chemical reaction, or secondarily from the action of some such force on other substances such as air, water, or steam).\nHistory.\nAntiquity.\nSimple machines, such as the club and oar (examples of the lever), are prehistoric. More complex engines using human power, animal power, water power, wind power and even steam power date back to antiquity. Human power was focused by the use of simple engines, such as the capstan, windlass or treadmill, and with ropes, pulleys, and block and tackle arrangements; this power was transmitted usually with the forces multiplied and the speed reduced. These were used in cranes and aboard ships in Ancient Greece, as well as in mines, water pumps and siege engines in Ancient Rome. The writers of those times, including Vitruvius, Frontinus and Pliny the Elder, treat these engines as commonplace, so their invention may be more ancient. By the 1st century AD, cattle and horses were used in mills, driving machines similar to those powered by humans in earlier times.\nAccording to Strabo, a water-powered mill was built in Kaberia of the kingdom of Mithridates during the 1st century BC. Use of water wheels in mills spread throughout the Roman Empire over the next few centuries. Some were quite complex, with aqueducts, dams, and sluices to maintain and channel the water, along with systems of gears, or toothed-wheels made of wood and metal to regulate the speed of rotation. More sophisticated small devices, such as the Antikythera Mechanism used complex trains of gears and dials to act as calendars or predict astronomical events. In a poem by Ausonius in the 4th century AD, he mentions a stone-cutting saw powered by water. Hero of Alexandria is credited with many such wind and steam powered machines in the 1st century AD, including the Aeolipile and the vending machine, often these machines were associated with worship, such as animated altars and automated temple doors.\nMedieval.\nMedieval Muslim engineers employed gears in mills and water-raising machines, and used dams as a source of water power to provide additional power to watermills and water-raising machines. In the medieval Islamic world, such advances made it possible to mechanize many industrial tasks previously carried out by manual labour.\nIn 1206, al-Jazari employed a crank-conrod system for two of his water-raising machines. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629.\nIn the 13th century, the solid rocket motor was invented in China. Driven by gunpowder, this simplest form of internal combustion engine was unable to deliver sustained power, but was useful for propelling weaponry at high speeds towards enemies in battle and for fireworks. After invention, this innovation spread throughout Europe.\nIndustrial Revolution.\nThe Watt steam engine was the first type of steam engine to make use of steam at a pressure just above atmospheric to drive the piston helped by a partial vacuum. Improving on the design of the 1712 Newcomen steam engine, the Watt steam engine, developed sporadically from 1763 to 1775, was a great step in the development of the steam engine. Offering a dramatic increase in fuel efficiency, James Watt's design became synonymous with steam engines, due in no small part to his business partner, Matthew Boulton. It enabled rapid development of efficient semi-automated factories on a previously unimaginable scale in places where waterpower was not available. Later development led to steam locomotives and great expansion of railway transportation.\nAs for internal combustion piston engines, these were tested in France in 1807 by de Rivaz and independently, by the Ni\u00e9pce brothers. They were theoretically advanced by Carnot in 1824. In 1853\u201357 Eugenio Barsanti and Felice Matteucci invented and patented an engine using the free-piston principle that was possibly the first 4-cycle engine.\nThe invention of an internal combustion engine which was later commercially successful was made during 1860 by Etienne Lenoir.\nIn 1877, the Otto cycle was capable of giving a far higher power-to-weight ratio than steam engines and worked much better for many transportation applications such as cars and aircraft.\nAutomobiles.\nThe first commercially successful automobile, created by Karl Benz, added to the interest in light and powerful engines. The lightweight gasoline internal combustion engine, operating on a four-stroke Otto cycle, has been the most successful for light automobiles, while the thermally more-efficient Diesel engine is used for trucks and buses. However, in recent years, turbocharged Diesel engines have become increasingly popular in automobiles, especially outside of the United States, even for quite small cars.\nHorizontally-opposed pistons.\nIn 1896, Karl Benz was granted a patent for his design of the first engine with horizontally opposed pistons. His design created an engine in which the corresponding pistons move in horizontal cylinders and reach top dead center simultaneously, thus automatically balancing each other with respect to their individual momentum. Engines of this design are often referred to as \u201cflat\u201d or \u201cboxer\u201d engines due to their shape and low profile. They were used in the Volkswagen Beetle, the Citro\u00ebn 2CV, some Porsche and Subaru cars, many BMW and Honda motorcycles. Opposed four- and six-cylinder engines continue to be used as a power source in small, propeller-driven aircraft.\nAdvancement.\nThe continued use of internal combustion engines in automobiles is partly due to the improvement of engine control systems, such as on-board computers providing engine management processes, and electronically controlled fuel injection. Forced air induction by turbocharging and supercharging have increased the power output of smaller displacement engines that are lighter in weight and more fuel-efficient at normal cruise power. Similar changes have been applied to smaller Diesel engines, giving them almost the same performance characteristics as gasoline engines. This is especially evident with the popularity of smaller diesel engine-propelled cars in Europe. Diesel engines produce lower hydrocarbon and CO2 emissions, but greater particulate and pollution, than gasoline engines. Diesel engines are also 40% more fuel efficient than comparable gasoline engines.\nIncreasing power.\nIn the first half of the 20th century, a trend of increasing engine power occurred, particularly in the U.S. models. Design changes incorporated all known methods of increasing engine capacity, including increasing the pressure in the cylinders to improve efficiency, increasing the size of the engine, and increasing the rate at which the engine produces work. The higher forces and pressures created by these changes created engine vibration and size problems that led to stiffer, more compact engines with V and opposed cylinder layouts replacing longer straight-line arrangements.\nCombustion efficiency.\nOptimal combustion efficiency in passenger vehicles is reached with a coolant temperature of around .\nEngine configuration.\nEarlier automobile engine development produced a much larger range of engines than is in common use today. Engines have ranged from 1- to 16-cylinder designs with corresponding differences in overall size, weight, engine displacement, and cylinder bores. Four cylinders and power ratings from 19 to 120\u00a0hp (14 to 90\u00a0kW) were followed in a majority of the models. Several three-cylinder, two-stroke-cycle models were built while most engines had straight or in-line cylinders. There were several V-type models and horizontally opposed two- and four-cylinder makes too. Overhead camshafts were frequently employed. The smaller engines were commonly air-cooled and located at the rear of the vehicle; compression ratios were relatively low. The 1970s and 1980s saw an increased interest in improved fuel economy, which caused a return to smaller V-6 and four-cylinder layouts, with as many as five valves per cylinder to improve efficiency. The Bugatti Veyron 16.4 operates with a W16 engine, meaning that two V8 cylinder layouts are positioned next to each other to create the W\u00a0shape sharing the same crankshaft.\nThe largest internal combustion engine ever built is the W\u00e4rtsil\u00e4-Sulzer RTA96-C, a 14-cylinder, 2-stroke turbocharged diesel engine that was designed to power the \"Emma M\u00e6rsk\", the largest container ship in the world when launched in 2006. This engine has a mass of 2,300 tonnes, and when running at 102\u00a0rpm (1.7\u00a0Hz) produces over 80 MW, and can use up to 250 tonnes of fuel per day.\nTypes.\nAn engine can be put into a category according to two criteria: the form of energy it accepts in order to create motion, and the type of motion it outputs.\nHeat engine.\nCombustion engine.\nCombustion engines are heat engines driven by the heat of a combustion process.\nInternal combustion engine.\nThe \"internal combustion engine\" is an engine in which the combustion of a fuel (generally, fossil fuel) occurs with an oxidizer (usually air) in a combustion chamber. In an internal combustion engine the expansion of the high temperature and high pressure gases, which are produced by the combustion, directly applies force to components of the engine, such as the pistons or turbine blades or a nozzle, and by moving it over a distance, generates mechanical work.\nExternal combustion engine.\nAn \"external combustion engine\" (EC engine) is a heat engine where an internal working fluid is heated by combustion of an external source, through the engine wall or a heat exchanger. The fluid then, by expanding and acting on the mechanism of the engine produces motion and usable work. The fluid is then cooled, compressed and reused (closed cycle), or (less commonly) dumped, and cool fluid pulled in (open cycle air engine).\n\"Combustion\" refers to burning fuel with an oxidizer, to supply the heat. Engines of similar (or even identical) configuration and operation may use a supply of heat from other sources such as nuclear, solar, geothermal or exothermic reactions not involving combustion; but are not then strictly classed as external combustion engines, but as external thermal engines.\nThe working fluid can be a gas as in a Stirling engine, or steam as in a steam engine or an organic liquid such as n-pentane in an Organic Rankine cycle. The fluid can be of any composition; gas is by far the most common, although even single-phase liquid is sometimes used. In the case of the steam engine, the fluid changes phases between liquid and gas.\nAir-breathing combustion engines.\n\"Air-breathing combustion engines\" are combustion engines that use the oxygen in atmospheric air to oxidise ('burn') the fuel, rather than carrying an oxidiser, as in a rocket. Theoretically, this should result in a better specific impulse than for rocket engines.\nA continuous stream of air flows through the air-breathing engine. This air is compressed, mixed with fuel, ignited and expelled as the exhaust gas. In reaction engines, the majority of the combustion energy (heat) exits the engine as exhaust gas, which provides thrust directly.\nTypical air-breathing engines include:\nEnvironmental effects.\nThe operation of engines typically has a negative impact upon air quality and ambient sound levels. There has been a growing emphasis on the pollution producing features of automotive power systems. This has created new interest in alternate power sources and internal-combustion engine refinements. Though a few limited-production battery-powered electric vehicles have appeared, they have not proved competitive owing to costs and operating characteristics. In the 21st century the diesel engine has been increasing in popularity with automobile owners. However, the gasoline engine and the Diesel engine, with their new emission-control devices to improve emission performance, have not yet been significantly challenged. A number of manufacturers have introduced hybrid engines, mainly involving a small gasoline engine coupled with an electric motor and with a large battery bank, these are starting to become a popular option because of their environment awareness.\nAir quality.\nExhaust gas from a spark ignition engine consists of the following: nitrogen 70 to 75% (by volume), water vapor 10 to 12%, carbon dioxide 10 to 13.5%, hydrogen 0.5 to 2%, oxygen 0.2 to 2%, carbon monoxide: 0.1 to 6%, unburnt hydrocarbons and partial oxidation products (e.g. aldehydes) 0.5 to 1%, nitrogen monoxide 0.01 to 0.4%, nitrous oxide &lt;100 ppm, sulfur dioxide 15 to 60 ppm, traces of other compounds such as fuel additives and lubricants, also halogen and metallic compounds, and other particles. Carbon monoxide is highly toxic, and can cause carbon monoxide poisoning, so it is important to avoid any build-up of the gas in a confined space. Catalytic converters can reduce toxic emissions, but not eliminate them. Also, resulting greenhouse gas emissions, chiefly carbon dioxide, from the widespread use of engines in the modern industrialized world is contributing to the global greenhouse effect \u2013 a primary concern regarding global warming.\nNon-combusting heat engines.\nSome engines convert heat from noncombustive processes into mechanical work, for example a nuclear power plant uses the heat from the nuclear reaction to produce steam and drive a steam engine, or a gas turbine in a rocket engine may be driven by decomposing hydrogen peroxide. Apart from the different energy source, the engine is often engineered much the same as an internal or external combustion engine.\nAnother group of noncombustive engines includes thermoacoustic heat engines (sometimes called \"TA engines\") which are thermoacoustic devices that use high-amplitude sound waves to pump heat from one place to another, or conversely use a heat difference to induce high-amplitude sound waves. In general, thermoacoustic engines can be divided into standing wave and travelling wave devices.\nStirling engines can be another form of non-combustive heat engine. They use the Stirling thermodynamic cycle to convert heat into work. An example is the alpha type Stirling engine, whereby gas flows, via a recuperator, between a hot cylinder and a cold cylinder, which are attached to reciprocating pistons 90\u00b0 out of phase. The gas receives heat at the hot cylinder and expands, driving the piston that turns the crankshaft. After expanding and flowing through the recuperator, the gas rejects heat at the cold cylinder and the ensuing pressure drop leads to its compression by the other (displacement) piston, which forces it back to the hot cylinder.\nNon-thermal chemically powered motor.\nNon-thermal motors usually are powered by a chemical reaction, but are not heat engines. Examples include:\nElectric motor.\nAn \"electric motor\" uses electrical energy to produce mechanical energy, usually through the interaction of magnetic fields and current-carrying conductors. The reverse process, producing electrical energy from mechanical energy, is accomplished by a generator or dynamo. Traction motors used on vehicles often perform both tasks. Electric motors can be run as generators and vice versa, although this is not always practical.\nElectric motors are ubiquitous, being found in applications as diverse as industrial fans, blowers and pumps, machine tools, household appliances, power tools, and disk drives. They may be powered by direct current (for example a battery powered portable device or motor vehicle), or by alternating current from a central electrical distribution grid. The smallest motors may be found in electric wristwatches. Medium-size motors of highly standardized dimensions and characteristics provide convenient mechanical power for industrial uses. The very largest electric motors are used for propulsion of large ships, and for such purposes as pipeline compressors, with ratings in the thousands of kilowatts. Electric motors may be classified by the source of electric power, by their internal construction, and by their application.\nThe physical principle of production of mechanical force by the interactions of an electric current and a magnetic field was known as early as 1821. Electric motors of increasing efficiency were constructed throughout the 19th century, but commercial exploitation of electric motors on a large scale required efficient electrical generators and electrical distribution networks.\nTo reduce the electric energy consumption from motors and their associated carbon footprints, various regulatory authorities in many countries have introduced and implemented legislation to encourage the manufacture and use of higher efficiency electric motors. A well-designed motor can convert over 90% of its input energy into useful power for decades. When the efficiency of a motor is raised by even a few percentage points, the savings, in kilowatt hours (and therefore in cost), are enormous. The electrical energy efficiency of a typical industrial induction motor can be improved by: 1) reducing the electrical losses in the stator windings (e.g., by increasing the cross-sectional area of the conductor, improving the winding technique, and using materials with higher electrical conductivities, such as copper), 2) reducing the electrical losses in the rotor coil or casting (e.g., by using materials with higher electrical conductivities, such as copper), 3) reducing magnetic losses by using better quality magnetic steel, 4) improving the aerodynamics of motors to reduce mechanical windage losses, 5) improving bearings to reduce friction losses, and 6) minimizing manufacturing tolerances. \"For further discussion on this subject, see Premium efficiency).\"\nBy convention, \"electric engine\" refers to a railroad electric locomotive, rather than an electric motor.\nPhysically powered motor.\nSome motors are powered by potential or kinetic energy, for example some funiculars, gravity plane and ropeway conveyors have used the energy from moving water or rocks, and some clocks have a weight that falls under gravity. Other forms of potential energy include compressed gases (such as pneumatic motors), springs (clockwork motors) and elastic bands.\nHistoric military siege engines included large catapults, trebuchets, and (to some extent) battering rams were powered by potential energy.\nPneumatic motor.\nA \"pneumatic motor\" is a machine that converts potential energy in the form of compressed air into mechanical work. Pneumatic motors generally convert the compressed air to mechanical work through either linear or rotary motion. Linear motion can come from either a diaphragm or a piston actuator, while rotary motion is supplied by either a vane type air motor or piston air motor. Pneumatic motors have found widespread success in the hand-held tool industry and continual attempts are being made to expand their use to the transportation industry. However, pneumatic motors must overcome efficiency deficiencies before being seen as a viable option in the transportation industry.\nHydraulic motor.\nA \"hydraulic motor\" derives its power from a pressurized liquid. This type of engine is used to move heavy loads and drive machinery.\nHybrid.\nSome motor units can have multiple sources of energy. For example, a plug-in hybrid electric vehicle's electric motor could source electricity from either a battery or from fossil fuels inputs via an internal combustion engine and a generator.\nPerformance.\nThe following are used in the assessment of the performance of an engine.\nSpeed.\nSpeed refers to crankshaft rotation in piston engines and the speed of compressor/turbine rotors and electric motor rotors. It is typically measured in revolutions per minute (rpm).\nThrust.\nThrust is the force exerted on an airplane as a consequence of its propeller or jet engine accelerating the air passing through it. It is also the force exerted on a ship as a consequence of its propeller accelerating the water passing through it.\nTorque.\nTorque is a turning moment on a shaft and is calculated by multiplying the force causing the moment by its distance from the shaft.\nPower.\nPower is the measure of how fast work is done.\nEfficiency.\nEfficiency is a proportion of useful energy output compared to total input.\nSound levels.\nVehicle noise is predominantly from the engine at low vehicle speeds and from tires and the air flowing past the vehicle at higher speeds. Electric motors are quieter than internal combustion engines. Thrust-producing engines, such as turbofans, turbojets and rockets emit the greatest amount of noise due to the way their thrust-producing, high-velocity exhaust streams interact with the surrounding stationary air.\nNoise reduction technology includes intake and exhaust system mufflers (silencers) on gasoline and diesel engines and noise attenuation liners in turbofan inlets.\nEngines by use.\nParticularly notable kinds of engines include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9642", "revid": "66", "url": "https://en.wikipedia.org/wiki?curid=9642", "title": "Extropian", "text": ""}
{"id": "9643", "revid": "44383382", "url": "https://en.wikipedia.org/wiki?curid=9643", "title": "Economic and monetary union", "text": "Trade bloc with a common tariff and currency\nAn economic and monetary union (EMU) is a type of trade bloc that features a combination of a common market, customs union, and monetary union. Established via a trade pact, an EMU constitutes the sixth of seven stages in the process of economic integration. An EMU agreement usually combines a customs union with a common market. A typical EMU establishes free trade and a common external tariff throughout its jurisdiction. It is also designed to protect freedom in the movement of goods, services, and people. This arrangement is distinct from a monetary union (e.g., the Latin Monetary Union), which does not usually involve a common market. As with the economic and monetary union established among the 27 member states of the European Union (EU), an EMU may affect different parts of its jurisdiction in different ways. Some areas are subject to separate customs regulations from other areas subject to the EMU. These various arrangements may be established in a formal agreement, or they may exist on a \"de facto\" basis. For example, not all EU member states use the Euro established by its currency union, and not all EU member states are part of the Schengen Area. Some EU members participate in both unions, and some in neither.\nTerritories of the United States, Australian External Territories and New Zealand territories each share a currency and, for the most part, the market of their respective mainland states. However, they are generally not part of the same customs territories.\nHistory.\nSeveral countries initially attempted to form an EMU at the Hague Summit in 1969. Afterward, a \"draft plan\" was announced. During this time, the main member presiding over this decision was Pierre Werner, Prime Minister of Luxembourg. The decision to form the Economic and Monetary Union of the European Union (EMU) was accepted in December 1991, which later became part of the Maastricht Treaty (the Treaty on European Union).\nProcesses in the European EMU.\nThe EMU involves four main activities.\nThe first responsibility is to be in charge of implementing effective monetary policy for the euro area with price stability. There is a group of economists whose only role is studying how to improve the monetary policy while maintaining price stability. They conduct research, and their results are presented to the leaders of the EMU. Thereafter, the role of the leaders is to find a suitable way to implement the economists' work into their country's policies. Maintaining price stability is a long-term goal for all states in the EU, due to the effects it might have on the Euro as a currency.\nSecondly, the EMU must coordinate economic and fiscal policies in EU countries. They must find an equilibrium between the implementation of monetary and fiscal policies. They will advise countries to have greater coordination, even if that means having countries tightly coupled with looser monetary and tighter fiscal policy. Not coordinating the monetary market could result in risking an unpredictable situation. The EMU also deliberates on a mixed policy option, which has been shown to be beneficial in some empirical studies.\nThirdly, the EMU ensures that the single market runs smoothly. The member countries respect the decisions made by the EMU and ensure that their actions will be in favor of a stable market.\nFinally, regulations of the EMU aid in supervising and monitoring financial institutions. There is an imperative need for all members of the EMU to act in unison. Therefore, the EMU has to have institutions supervising all the member states to protect the main aim of the EMU.\nRoles of national governments.\nThe economic roles of nations within the EMU are to:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9644", "revid": "41373646", "url": "https://en.wikipedia.org/wiki?curid=9644", "title": "European Environment Agency", "text": "Agency of the European Union\nThe European Environment Agency (EEA) is the agency of the European Union (EU) which provides independent information on the environment in climate policy, biodiversity conservation and other environmental goals set by the country.\nDefinition.\nThe European Environment Agency (EEA) is the agency of the European Union (EU) which provides independent information on the environment. \nIts goal is to help those involved in developing, implementing and evaluating environmental policy, and to inform the general public.\nOrganization.\nThe EEA was established by the European Economic Community (EEC) Regulation 1210/1990 (amended by EEC Regulation 933/1999 and EC Regulation 401/2009) and became operational in 1994, headquartered in Copenhagen, Denmark.\nThe agency is governed by a management board composed of representatives of the governments of its 32 member states, a European Commission representative and two scientists appointed by the European Parliament, assisted by its Scientific Committee.\nThe current Executive Director of the agency is Leena Yl\u00e4-Mononen, who has been appointed for a five-year term, starting on 1 June 2023. Ms Yl\u00e4-Mononen is the successor of professor Hans Bruyninckx.\nMember countries.\nThe member states of the European Union are members; however other states may become members of it by means of agreements concluded between them and the EU.\nIt was the first EU body to open its membership to the 13 candidate countries (pre-2004 enlargement).\nThe EEA has 32 member countries and six cooperating countries. The members are the 27 European Union member states together with Iceland, Liechtenstein, Norway, Switzerland and Turkey.\nSince Brexit in 2020, the UK is not a member of the EU anymore and therefore not a member state of the EEA.\nThe six Western Balkan countries are cooperating countries: Albania, Bosnia and Herzegovina, Montenegro, North Macedonia, Serbia as well as Kosovo under the UN Security Council Resolution 1244/99. These cooperation activities are integrated into Eionet and are supported by the EU under the \"Instrument for Pre-Accession Assistance\".\nThe EEA is an active member of the EPA Network.\nReports, data and knowledge.\nThe European Environment Agency (EEA) produces assessments based on quality-assured data on a wide range of issues from biodiversity, air quality, transport to climate change. These assessments are closely linked to the European Union's environment policies and legislation and help monitor progress in some areas and indicate areas where additional efforts are needed.\nAs required in its founding regulation, the EEA publishes its flagship report the https:// (SOER), which is an integrated assessment, analysing trends, progress to targets as well as outlook for the mid- to long-term. The agency publishes annually a report on Europe's most polluted provinces for air quality, detailing fine particulate matter PM 2.5.\nThe EEA shares this information, including the datasets used in its assessments, through its https:// and a number of thematic information platforms such as https:// (BISE), https:// (WISE) and https://. The Climate-ADAPT knowledge platform presents information and data on expected climatic changes, the vulnerability of regions and sectors, adaptation case studies, and adaptation options, adaptation planning tools, and EU policy.\nEuropean Nature Information System.\nThe European Nature Information System (EUNIS) provides access to the publicly available data in the EUNIS database for species, habitat types and protected sites across Europe. It is part of the European Biodiversity data centre (BDC), and is maintained by the EEA.\nThe database contains data\nEuropean environment information and observation network.\nThe European Environment Information and Observation Network (Eionet) is a collaboration network between EEA member countries and non-member, cooperating nations. Cooperation is facilitated through different national environmental agencies, ministries, or offices. Eionet encourages the sharing of data and highlights specific topics for discussion and cooperation among participating countries.\nEionet currently includes covers seven https:// (ETCs):\nThe European Environment Agency (EEA) implements the \"Shared Environmental Information System\" principles and best practices via projects such as the \"ENI SEIS II EAST PROJECT\" &amp; the \"ENI SEIS II SOUTH PROJECT\" to support environmental protection within the six eastern partnership countries (ENP) &amp; to contribute to the reduction in marine pollution in the Mediterranean through the shared availability and access to relevant environmental information.\nBudget management and discharge.\nAs for every EU body and institution, the EEA's budget is subject to a discharge process, consisting of external examination of its budget execution and financial management, to ensure sound financial management of its budget. Since its establishment, the EEA has been granted discharge for its budget without exception. The EEA provides full access to its administrative and budgetary documents in its https://.\nThe discharge process for the 2010 budget required additional clarifications. In February 2012, the European Parliament's Committee on Budgetary Control published a draft report, identifying areas of concern in the use of funds and its influence for the 2010 budget such as a 26% budget increase from 2009 to 2010 to \u20ac50 600 000. and questioned that maximum competition and value-for-money principles were honored in hiring, also possible fictitious employees.\nThe EEA's Executive Director refuted allegations of irregularities in a public hearing. On 27 March 2012 Members of the European Parliament (MEPs) voted on the report and commended the cooperation between the Agency and NGOs working in the environmental area. On 23 October 2012, the European Parliament voted and granted the discharge to the European Environment Agency for its 2010 budget.\nInternational cooperation.\nIn addition to its 32 members and six Balkan cooperating countries, the EEA also cooperates and fosters partnerships with its neighbours and other countries and regions, mostly in the context of the European Neighbourhood Policy:\nAdditionally the EEA cooperates with multiple international organizations and the corresponding agencies of the following countries:\nOfficial languages.\nThe 26 official languages used by the EEA are: Bulgarian, Czech, Croatian, Danish, German, Greek, English, Spanish, Estonian, Finnish, French, Hungarian, Icelandic, Italian, Lithuanian, Latvian, Malti, Dutch, Norwegian, Polish, Portuguese, Romanian, Slovak, Slovene, Swedish and Turkish.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9645", "revid": "1299970410", "url": "https://en.wikipedia.org/wiki?curid=9645", "title": "EV", "text": "Ev or EV may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "9646", "revid": "49984288", "url": "https://en.wikipedia.org/wiki?curid=9646", "title": "Erlang (programming language)", "text": "Programming language\nErlang ( ) is a general-purpose, concurrent, functional high-level programming language, and a garbage-collected runtime system. The term Erlang is used interchangeably with Erlang/OTP, or Open Telecom Platform (OTP), which consists of the Erlang runtime system, several ready-to-use components (OTP) mainly written in Erlang, and a set of design principles for Erlang programs.\nThe Erlang runtime system is designed for systems with these traits:\nThe Erlang programming language has data, pattern matching, and functional programming. The sequential subset of the Erlang language supports eager evaluation, single assignment, and dynamic typing.\nA normal Erlang application is built out of hundreds of small Erlang processes.\nIt was originally proprietary software within Ericsson, developed by Joe Armstrong, Robert Virding, and Mike Williams in 1986, but was released as free and open-source software in 1998. Erlang/OTP is supported and maintained by the Open Telecom Platform (OTP) product unit at Ericsson.\nHistory.\nThe name \"Erlang\", attributed to Bjarne D\u00e4cker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang and a syllabic abbreviation of \"Ericsson Language\". Erlang was designed with the aim of improving the development of telephony applications. The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster to be suitable for production use. In 1992, work began on the BEAM virtual machine (VM), which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space. According to co-inventor Joe Armstrong, the language went from laboratory product to real applications following the collapse of the next-generation AXE telephone exchange named in 1995. As a result, Erlang was chosen for the next Asynchronous Transfer Mode (ATM) exchange \"AXD\".\nIn February 1998, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages. The ban caused Armstrong and others to make plans to leave Ericsson. In March 1998 Ericsson announced the AXD301 switch, containing over a million lines of Erlang and reported to achieve a high availability of nine \"9\"s. In December 1998, the implementation of Erlang was open-sourced and most of the Erlang team resigned to form a new company, Bluetail AB. Ericsson eventually relaxed the ban and re-hired Armstrong in 2004.\nIn 2006, native symmetric multiprocessing support was added to the runtime system and VM.\nProcesses.\nErlang applications are built of very lightweight Erlang processes in the Erlang runtime system. The Erlang runtime system provides strict process isolation between Erlang processes (this includes data and garbage collection, separated individually by each Erlang process) and transparent communication between processes (see Location transparency) on different Erlang nodes (on different hosts).\nJoe Armstrong, co-inventor of Erlang, summarized the principles of processes in his PhD thesis:\nJoe Armstrong remarked in an interview with Rackspace in 2013: \"If Java is 'write once, run anywhere', then Erlang is 'write once, run forever'.\"\nUsage.\nIn 2014, Ericsson reported Erlang was being used in its support nodes, and in GPRS, 3G and LTE mobile networks worldwide and also by Nortel and Deutsche Telekom.\nErlang is used in RabbitMQ. As Tim Bray, director of Web Technologies at Sun Microsystems, expressed in his keynote at O'Reilly Open Source Convention (OSCON) in July 2008:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;If somebody came to me and wanted to pay me a lot of money to build a large scale message handling system that really had to be up all the time, could never afford to go down for years at a time, I would unhesitatingly choose Erlang to build it in.\nErlang is the programming language used to code WhatsApp.\nIt is also the language of choice for Ejabberd \u2013 an XMPP messaging server.\nElixir is a programming language that compiles into BEAM byte code (via Erlang Abstract Format).\nSince being released as open source, Erlang has been spreading beyond telecoms, establishing itself in other vertical markets such as FinTech, gaming, healthcare, automotive, Internet of Things and blockchain. Apart from WhatsApp, there are other companies listed as Erlang's success stories, including Vocalink (a MasterCard company), Goldman Sachs, Nintendo, AdRoll, Grindr, BT Mobile, Samsung, OpenX, and SITA.\nFunctional programming examples.\nFactorial.\nA factorial algorithm implemented in Erlang:\n-module(fact). % This is the file 'fact.erl', the module and the filename must match\n-export([fac/1]). % This exports the function 'fac' of arity 1 (1 parameter, no type, no name)\nfac(0) -&gt; 1; % If 0, then return 1, otherwise (note the semicolon ; meaning 'else')\nfac(N) when N &gt; 0, is_integer(N) -&gt; N * fac(N-1).\n% Recursively determine, then return the result\n% (note the period . meaning 'endif' or 'function end')\n%% This function will crash if anything other than a nonnegative integer is given.\n%% It illustrates the \"Let it crash\" philosophy of Erlang.\nFibonacci sequence.\nA tail recursive algorithm that produces the Fibonacci sequence:\n%% The module declaration must match the file name \"series.erl\" \n-module(series).\n%% The export statement contains a list of all those functions that form\n%% the module's public API. In this case, this module exposes a single\n%% function called fib that takes 1 argument (I.E. has an arity of 1)\n%% The general syntax for -export is a list containing the name and\n%% arity of each public function\n-export([fib/1]).\n%% Public API\n%% Handle cases in which fib/1 receives specific values\n%% The order in which these function signatures are declared is a vital\n%% part of this module's functionality\n%% If fib/1 receives a negative number, then return the atom err_neg_val\n%% Normally, such defensive coding is discouraged due to Erlang's 'Let\n%% it Crash' philosophy, but here the result would be an infinite loop.\nfib(N) when N &lt; 0 -&gt; err_neg_val;\n%% If fib/1 is passed precisely the integer 0, then return 0\nfib(0) -&gt; 0;\n%% For all other values, call the private function fib_int/3 to perform\n%% the calculation\nfib(N) -&gt; fib_int(N-1, 0, 1).\n%% Private API\n%% If fib_int/3 receives 0 as its first argument, then we're done, so\n%% return the value in argument B. The second argument is denoted _ to\n%% disregard its value.\nfib_int(0, _, B) -&gt; B;\n%% For all other argument combinations, recursively call fib_int/3\n%% where each call does the following:\n%% - decrement counter N\n%% - pass the third argument as the new second argument\n%% - pass the sum of the second and third arguments as the new\n%% third argument\nfib_int(N, A, B) -&gt; fib_int(N-1, B, A+B).\nOmitting the comments gives a much shorter program.\n-module(series).\n-export([fib/1]).\nfib(N) when N &lt; 0 -&gt; err_neg_val;\nfib(0) -&gt; 0;\nfib(N) -&gt; fib_int(N-1, 0, 1).\nfib_int(0, _, B) -&gt; B;\nfib_int(N, A, B) -&gt; fib_int(N-1, B, A+B).\nQuicksort.\nQuicksort in Erlang, using list comprehension:\n%% qsort:qsort(List)\n%% Sort a list of items\n-module(qsort). % This is the file 'qsort.erl'\n-export([qsort/1]). % A function 'qsort' with 1 parameter is exported (no type, no name)\nqsort([]) -&gt; []; % If the list [] is empty, return an empty list (nothing to sort)\nqsort([Pivot|Rest]) -&gt;\n % Compose recursively a list with 'Front' for all elements that should be before 'Pivot'\n % then 'Pivot' then 'Back' for all elements that should be after 'Pivot'\n qsort([Front || Front &lt;- Rest, Front &lt; Pivot]) ++ \n [Pivot] ++\n qsort([Back || Back &lt;- Rest, Back &gt;= Pivot]).\nThe above example recursively invokes the function codice_1 until nothing remains to be sorted. The expression codice_2 is a list comprehension, meaning \"Construct a list of elements codice_3 such that codice_3 is a member of codice_5, and codice_3 is less than codice_7.\" codice_8 is the list concatenation operator.\nA comparison function can be used for more complicated structures for the sake of readability.\nThe following code would sort lists according to length:\n% This is file 'listsort.erl' (the compiler is made this way)\n-module(listsort).\n% Export 'by_length' with 1 parameter (don't care about the type and name)\n-export([by_length/1]).\nby_length(Lists) -&gt; % Use 'qsort/2' and provides an anonymous function as a parameter\n qsort(Lists, fun(A,B) -&gt; length(A) &lt; length(B) end).\nqsort([], _)-&gt; []; % If list is empty, return an empty list (ignore the second parameter)\nqsort([Pivot|Rest], Smaller) -&gt;\n % Partition list with 'Smaller' elements in front of 'Pivot' and not-'Smaller' elements\n % after 'Pivot' and sort the sublists.\n qsort([X || X &lt;- Rest, Smaller(X,Pivot)], Smaller)\n ++ [Pivot] ++\n qsort([Y || Y &lt;- Rest, not(Smaller(Y, Pivot))], Smaller).\nA codice_7 is taken from the first parameter given to codice_10 and the rest of codice_11 is named codice_5. Note that the expression\n[X || X &lt;- Rest, Smaller(X,Pivot)]\nis no different in form from\n[Front || Front &lt;- Rest, Front &lt; Pivot]\n(in the previous example) except for the use of a comparison function in the last part, saying \"Construct a list of elements codice_13 such that codice_13 is a member of codice_5, and codice_16 is true\", with codice_16 being defined earlier as\nfun(A,B) -&gt; length(A) &lt; length(B) end\nThe anonymous function is named codice_16 in the parameter list of the second definition of codice_1 so that it can be referenced by that name within that function. It is not named in the first definition of codice_1, which deals with the base case of an empty list and thus has no need of this function, let alone a name for it.\nData types.\nErlang has eight primitive data types:\nAnd three compound data types:\nTwo forms of syntactic sugar are provided:\n\"Let it crash\" coding style.\nErlang is designed with a mechanism that makes it easy for external processes to monitor for crashes (or hardware failures), rather than an in-process mechanism like exception handling used in many other programming languages. Crashes are reported like other messages, which is the only way processes can communicate with each other, and subprocesses can be spawned cheaply (see below). The \"let it crash\" philosophy prefers that a process be completely restarted rather than trying to recover from a serious failure. Though it still requires handling of errors, this philosophy results in less code devoted to defensive programming where error-handling code is highly contextual and specific.\nSupervisor trees.\nA typical Erlang application is written in the form of a supervisor tree. This architecture is based on a hierarchy of processes in which the top level process is known as a \"supervisor\". The supervisor then spawns multiple child processes that act either as workers or more, lower level supervisors. Such hierarchies can exist to arbitrary depths and have proven to provide a highly scalable and fault-tolerant environment within which application functionality can be implemented.\nWithin a supervisor tree, all supervisor processes are responsible for managing the lifecycle of their child processes, and this includes handling situations in which those child processes crash. Any process can become a supervisor by first spawning a child process, then calling codice_35 on that process. If the monitored process then crashes, the supervisor will receive a message containing a tuple whose first member is the atom codice_36. The supervisor is responsible firstly for listening for such messages and for taking the appropriate action to correct the error condition.\nConcurrency and distribution orientation.\nErlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Erlang is conceptually similar to the language occam, though it recasts the ideas of communicating sequential processes (CSP) in a functional framework and uses asynchronous message passing. Processes are the primary means to structure an Erlang application. They are neither operating system processes nor threads, but lightweight processes that are scheduled by BEAM. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words. Thus, many processes can be created without degrading performance. In 2005, a benchmark with 20 million processes was successfully performed with 64-bit Erlang on a machine with 16 GB random-access memory (RAM; total 800 bytes/process). Erlang has supported symmetric multiprocessing since release R11B of May 2006.\nWhile threads need external library support in most languages, Erlang provides language-level features to create and manage processes with the goal of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM).\nInter-process communication works via a shared-nothing asynchronous message passing system: every process has a \"mailbox\", a queue of messages that have been sent by other processes and not yet consumed. A process uses the codice_37 primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.\nThe code example below shows the built-in support for distributed processes:\n % Create a process and invoke the function web:start_server(Port, MaxConnections)\n ServerProcess = spawn(web, start_server, [Port, MaxConnections]),\n % Create a remote process and invoke the function\n % web:start_server(Port, MaxConnections) on machine RemoteNode\n RemoteProcess = spawn(RemoteNode, web, start_server, [Port, MaxConnections]),\n % Send a message to ServerProcess (asynchronously). The message consists of a tuple\n % with the atom \"pause\" and the number \"10\".\n ServerProcess ! {pause, 10},\n % Receive messages sent to this process\n receive\n a_message -&gt; do_something;\n {data, DataContent} -&gt; handle(DataContent);\n {hello, Text} -&gt; io:format(\"Got hello message: ~s\", [Text]);\n {goodbye, Text} -&gt; io:format(\"Got goodbye message: ~s\", [Text])\n end.\nAs the example shows, processes may be created on remote nodes, and communication with them is transparent in the sense that communication with remote processes works exactly as communication with local processes.\nConcurrency supports the primary method of error-handling in Erlang. When a process crashes, it neatly exits and sends a message to the controlling process which can then take action, such as starting a new process that takes over the old process's task.\nImplementation.\nThe official reference implementation of Erlang uses BEAM. BEAM is included in the official distribution of Erlang, called Erlang/OTP. BEAM executes bytecode which is converted to threaded code at load time. It also includes a native code compiler on most platforms, developed by the High Performance Erlang Project (HiPE) at Uppsala University. Since October 2001 the HiPE system is fully integrated in Ericsson's Open Source Erlang/OTP system. It also supports interpreting, directly from source code via abstract syntax tree, via script as of R11B-5 release of Erlang.\nHot code loading and modules.\nErlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as \"module\" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the \"new\" and the \"old\" version. A process will not move into the new version until it makes an external call to its module.\nAn example of the mechanism of hot code loading:\n %% A process whose only job is to keep a counter.\n %% First version\n -module(counter).\n -export([start/0, codeswitch/1]).\n start() -&gt; loop(0).\n loop(Sum) -&gt;\n receive\n {increment, Count} -&gt;\n loop(Sum+Count);\n {counter, Pid} -&gt;\n Pid ! {counter, Sum},\n loop(Sum);\n code_switch -&gt;\n ?MODULE:codeswitch(Sum)\n % Force the use of 'codeswitch/1' from the latest MODULE version\n end.\n codeswitch(Sum) -&gt; loop(Sum).\nFor the second version, we add the possibility to reset the count to zero.\n %% Second version\n -module(counter).\n -export([start/0, codeswitch/1]).\n start() -&gt; loop(0).\n loop(Sum) -&gt;\n receive\n {increment, Count} -&gt;\n loop(Sum+Count);\n reset -&gt;\n loop(0);\n {counter, Pid} -&gt;\n Pid ! {counter, Sum},\n loop(Sum);\n code_switch -&gt;\n ?MODULE:codeswitch(Sum)\n end.\n codeswitch(Sum) -&gt; loop(Sum).\nOnly when receiving a message consisting of the atom codice_38 will the loop execute an external call to codeswitch/1 (codice_39 is a preprocessor macro for the current module). If there is a new version of the \"counter\" module in memory, then its codeswitch/1 function will be called. The practice of having a specific entry-point into a new version allows the programmer to transform state to what is needed in the newer version. In the example, the state is kept as an integer.\nIn practice, systems are built up using design principles from the Open Telecom Platform, which leads to more code upgradable designs. Successful hot code loading is exacting. Code must be written with care to make use of Erlang's facilities.\nDistribution.\nIn 1998, Ericsson released Erlang as free and open-source software to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the OTP collection of libraries. Ericsson and a few other companies support Erlang commercially.\nSince the open source release, Erlang has been used by several firms worldwide, including Nortel and Deutsche Telekom. Although Erlang was designed to fill a niche and has remained an obscure language for most of its existence, its popularity is growing due to demand for concurrent services.\nErlang has found some use in fielding massively multiplayer online role-playing game (MMORPG) servers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9647", "revid": "30394555", "url": "https://en.wikipedia.org/wiki?curid=9647", "title": "Euphoria (programming language)", "text": "Euphoria is a programming language created by Robert Craig of Rapid Deployment Software in Toronto, Ontario, Canada. Initially developed (though not publicly released) on the Atari ST, the first commercial release was for MS-DOS as proprietary software. In 2006, with the release of version 3, Euphoria became open-source software. The openEuphoria Group continues to administer and develop the project. In December 2010, the openEuphoria Group released version 4 of openEuphoria along with a new identity and mascot for the project. OpenEuphoria is currently available for Windows, Linux, macOS and three flavors of *BSD.\nEuphoria is a general-purpose high-level imperative-procedural interpreted language. A translator generates C source code and the GNU compiler collection (GCC) and Open Watcom compilers are supported. Alternatively, Euphoria programs may be bound with the interpreter to create stand-alone executables. A number of graphical user interface (GUI) libraries are supported including Win32lib and wrappers for wxWidgets, GTK+ and IUP. Euphoria has a simple built-in database and wrappers for a variety of other databases.\nOverview.\nThe Euphoria language is a general purpose procedural language that focuses on simplicity, legibility, rapid development and performance via several means.\nHistory.\nDeveloped as a personal project to invent a programming language from scratch, Euphoria was created by Robert Craig on an Atari Mega-ST. Many design ideas for the language came from Craig's Master's thesis in computer science at the University of Toronto. Craig's thesis was heavily influenced by the work of John Backus on functional programming (FP) languages.\nCraig ported his original Atari implementation to MS-DOS and Euphoria was first released, version 1.0, in July 1993 under a proprietary licence. The original Atari implementation is described by Craig as \"primitive\" and has not been publicly released. Euphoria continued to be developed and released by Craig via his company Rapid Deployment Software (RDS) and website rapideuphoria.com. In October 2006 RDS released version 3 of Euphoria and announced that henceforth Euphoria would be freely distributed under an open-source software licence.\nRDS continued to develop Euphoria, culminating with the release of version 3.1.1 in August, 2007. Subsequently, RDS ceased unilateral development of Euphoria and the openEuphoria Group took over ongoing development. The openEuphoria Group released version 4 in December, 2010 along with a new logo and mascot for the openEuphoria project.\nVersion 3.1.1 is the last version of Euphoria which supports MS-DOS.\nEuphoria is an acronym for \"End-User Programming with Hierarchical Objects for Robust Interpreted Applications\" although there is some suspicion that this is a backronym.\nThe Euphoria interpreter was originally written in C. With the release of version 2.5 in November 2004 the Euphoria interpreter was split into two parts: a front-end parser, and a back-end interpreter. The front-end is now written in Euphoria (and used with the Euphoria-to-C translator and the Binder). The main back-end and run time library are written in C.\nFeatures.\nEuphoria was conceived and developed with the following design goals and features:\nUse.\nEuphoria is designed handle dynamic sets of data of varying types. It is particularly useful for string and image processing. Euphoria has been used in artificial intelligence experiments, the study of mathematics, for teaching programming, and to implement fonts involving thousands of characters. A large part of the Euphoria interpreter is written in Euphoria.\nData types.\nEuphoria has two basic data types:\nEuphoria has two additional data types predefined:\nThere is no character string data type. Strings are represented by a \"sequence\" of \"integer\" values. However, because literal strings are so commonly used in programming, Euphoria interprets double-quote enclosed characters as a sequence of integers. Thus\n \"ABC\"\nis seen as if the coder had written:\nwhich is the same as:\nExamples.\nProgram comments start with a double hyphen codice_1 and go through the end of line.\nThe following code looks for an old item in a group of items. If found, it removes it by concatenating all the elements before it with all the elements after it. Note that the first element in a sequence has the index one [1] and that $ refers to the length (i.e., total number of elements) of the sequence.\n global function delete_item( object old, sequence group )\n integer pos\n -- Code begins --\n pos = find( old, group )\n if pos &gt; 0 then\n group = group[1 .. pos-1] &amp; group[pos+1 .. $]\n end if\n return group\n end function\nThe following modification to the above example replaces an old item with a new item. As the variables \"old\" and \"new\" have been defined as objects, they could be \"atoms\" or \"sequences\". Type checking is not needed as the function will work with any sequence of data of any type and needs no external libraries.\n global function replace_item( object old, object new, sequence group )\n integer pos\n -- Code begins --\n pos = find( old, group )\n if pos &gt; 0 then\n group[pos] = new\n end if\n return group\n end function\nFurthermore, no pointers are involved and subscripts are automatically checked. Thus the function cannot access memory out-of-bounds. There is no need to allocate or deallocate memory explicitly and no chance of a memory leak.\nThe line\n group = group[1 .. pos-1] &amp; group[pos+1 .. $]\nshows some of the \"sequence\" handling facilities. A \"sequence\" may contain a set of any types, and this can be sliced (to take a subset of the data in a \"sequence\") and concatenated in expressions with no need for special functions.\nParameter passing.\nArguments to routines are always passed by value; there is no pass-by-reference facility. However, parameters are allowed to be modified \"locally\" (i.e., within the callee) which is implemented very efficiently as sequences have automatic copy-on-write semantics. In other words, when you pass a sequence to a routine, initially only a reference to it is passed, but at the point the routine modifies this sequence parameter the sequence is copied and the routine updates only a copy of the original.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nFree downloads of Euphoria for the various platforms, packages, Windows IDE, Windows API libraries, a cross-platform GTK3 wrapper for Linux and Windows, graphics libraries (DOS, OpenGL, etc.)."}
{"id": "9649", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=9649", "title": "Energy", "text": "Physical quantity\nEnergy (from grc \" \"\" ()\"\u00a0'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J).\nForms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive.\nAll living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.\nForms.\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the object's components \u2013 while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself.\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.\nHistory.\nThe word \"energy\" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\nIn the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total \"vis viva\" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from \"vis viva\" only by a factor of two. Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's \"Principia Mathematica\", which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of \"vis viva\", in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, Walther Nernst, and others. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jo\u017eef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\nAlbert Einstein's 1905 theory of special relativity showed that rest mass corresponds to an equivalent amount of \"rest energy\". This means that \"rest mass\" can be converted to or from equivalent amounts of (non-material) forms of energy, for example, kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, rest mass is not conserved, unlike the \"total\" mass or \"total\" energy. All forms of energy contribute to the total mass and total energy. Thus, conservation of energy (\"total\", including material or \"rest\" energy) and conservation of mass (\"total\", not just \"rest\") are one (equivalent) law. In the 18th century, these had appeared as two seemingly-distinct laws.\nThe first evidence of quantization in atoms was the observation of spectral lines in light from the sun in the early 1800s by Joseph von Fraunhofer and William Hyde Wollaston. The notion of quantized energy levels was proposed in 1913 by Danish physicist Niels Bohr in the Bohr theory of the atom. The modern quantum mechanical theory giving an explanation of these energy levels in terms of the Schr\u00f6dinger equation was advanced by Erwin Schr\u00f6dinger and Werner Heisenberg in 1926. Noether's theorem shows that the symmetry of this equation is equivalent to a \"conservation of probability\". At the quantum level, mass-energy interactions are all subject to this principle. During wave function collapse, the conservation of energy does not hold at the local level, although statistically the principle holds on average for sufficiently large numbers of collapses. Conservation of energy does apply during wave function collapse in H. Everett's many-worlds interpretation of quantum mechanics.\nUnits of measure.\nIn dimensional analysis, the base units of energy are given by: Work = Force \u00d7 Distance = M L2 T\u22122, with the fundamental dimensions of Mass M, Length L, and time T. In the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre.\nThe SI unit of power, defined as energy per unit of time, is the watt, which is one joule per second. Thus, a kilowatt-hour (kWh), which can be realized as the energy delivered by one kilowatt of power for an hour, is equal to 3.6 million joules. The CGS energy unit is the erg and the imperial and US customary unit is the foot-pound.\nOther energy units such as the electronvolt, food calorie, thermodynamic kilocalorie and BTU are used in specific areas of science and commerce.\nScientific use.\nClassical mechanics.\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\nWork, a function of energy, is force times distance.\n formula_1\nThis says that the work (formula_2) is equal to the line integral of the force F along a path \"C\"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have direct analogs in nonrelativistic quantum mechanics.\nAnother energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy \"minus\" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\nChemistry.\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse.\nChemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at a given temperature\u00a0\"T\") is related to the activation energy\u00a0\"E\" by the Boltzmann population factor\u00a0e\u2212\"E\"/\"kT\"; that is, the probability of a molecule to have energy greater than or equal to\u00a0\"E\" at a given temperature\u00a0\"T\". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.\nBiology.\nIn biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. It enables the growth, development, and functioning of a biological cell or organelle in an organism. All living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. Energy provided through cellular respiration is stored in nutrients such as carbohydrates (including sugars), lipids, and proteins by cells.\nSunlight's radiant energy is captured by plants as \"chemical potential energy\" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins, and oxygen. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.\nHumans.\nThe basal metabolism rate measures the food energy expenditure per unit time by endothermic animals at rest. In other words it is the energy required by body organs to perform normally. For humans, metabolic equivalent of task (MET) compares the energy expenditure per unit mass while performing a physical activity, relative to a baseline. By convention, this baseline is 3.5\u00a0mL of oxygen consumed per kg per minute, which is the energy consumed by a typical individual when sitting quietly.\n For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.\nThe daily recommended for a human adult are taken as food molecules, mostly carbohydrates and fats. Only a tiny fraction of the original chemical energy is used for work:\n gain in kinetic energy of a sprinter during a 100\u00a0m race: 4\u00a0kJ\n gain in gravitational potential energy of a 150\u00a0kg weight lifted through 2\u00a0metres: 3\u00a0kJ\n daily food intake of a normal adult: 6\u20138\u00a0MJ\nIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies.\nIn growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism's tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7\u00a0Pg/a of carbon that is fixed by photosynthesis, 64.3\u00a0Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\nCell metabolism.\nMulticellular organisms such as humans have cell forms that are classified as Eukaryote. These cells include an organelle called the mitochondria that generates chemical energy for the rest of the hosting cell. Ninety percent of the oxygen intake by humans is utilized by the mitochondria, especially for nutrient processing. The molecule adenosine triphosphate (ATP) is the primary energy transporter in living cells, providing an energy source for cellular processes. It is continually being broken down and synthesized as a component of cellular respiration.\nTwo examples of nutrients consumed by animals are glucose (C6H12O6) and stearin (C57H110O6). These food molecules are oxidized to carbon dioxide and water in the mitochondria:\n&lt;chem display=\"block\"&gt;C6H12O6 + 6O2 -&gt; 6CO2 + 6H2O&lt;/chem&gt;\n&lt;chem display=\"block\"&gt;C57H110O6 + (81 1/2) O2 -&gt; 57CO2 + 55H2O&lt;/chem&gt;\nand some of the energy is used to convert ADP into ATP:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;ADP + HPO42\u2212 \u2192 ATP + H2O\nThe rest of the chemical energy of the nutrients are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat).\nEarth sciences.\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes, and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy.\nSunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability, after accounting for interaction with the atmosphere. Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat, which supplies more than half of the planet's internal heat budget. In the present day, this radiogenic heat production was primarily driven by the decay of Uranium-235, Potassium-40, and Thorium-232 some time in the past. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).\nEarly in a planet's history, the accretion process provides impact energy that can partially or completely melt the body. This allows a planet to become differentiated by chemical element. Chemical phase changes of minerals during formation provide additional internal heating. Over time the internal heat is brought to the surface then radiated away into space, cooling the body. Accreted radiogenic heat sources settle toward the core, providing thermal energy to the planet on a geologic time scale. Ongoing sedimentation provides a persistent internal energy source for gas giant planets like Jupiter and Saturn.\nCosmology.\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars, and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen).\nThe nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\nThe accretion of matter onto a compact object is a very efficient means of generating energy from gravitational potential. This behavior is responsible for some of the universe's brightest persistent energy sources. The Penrose process is a theoretical method by which energy could be extracted from a rotating black hole. Hawking radiation is the emission of black-body radiation from a black hole, which results in a steady loss of mass and rotational energy. As the object evaporates, the temperature of this radiation is predicted to increase, speeding up the process.\nQuantum mechanics.\nIn quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.\nIn the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by the Planck relation: formula_3, where formula_4 is the Planck constant and formula_5 the frequency. In the case of an electromagnetic wave these energy states are called quanta of light or photons. For matter waves, the de Broglie relation yields formula_6, where formula_7 is the momentum.\nRelativity.\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\nformula_8\nwhere\nFor example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of the particles is created from a sufficiently energetic photon near a nucleus.\nIn general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.\nEnergy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts).\nTransformation.\nEnergy may be transformed between different forms at various efficiencies. Devices that usefully transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to the kinetic energy of water spinning the blades of a turbine, and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a sufficiently small scale, but certain larger transformations are highly improbable because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\nEnergy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.\nYet another example of energy transformation is that of a simple gravity pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever. Energy is transferred from potential energy (formula_10) to kinetic energy (formula_11) and then back to potential energy constantly. This is referred to as conservation of energy.\nIn this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nThe equation can then be simplified further since formula_12 (mass times acceleration due to gravity times the height) and formula_13 (half\u00a0mass times velocity squared). Then the total amount of energy can be found by adding formula_14.\nConservation of energy and mass in transformation.\nWithin a gravitational field, both mass and energy give rise to a measureable weight when trapped in a system with zero momentum. The formula \"E\"\u00a0=\u00a0\"mc\"2, derived by Albert Einstein (1905) quantifies this mass\u2013energy equivalence between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904), and others (see Mass\u2013energy equivalence#History for further information).\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since formula_15 is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy, as can be seen in nuclear reactors and nuclear weapons. For example, 1\u00a0kg of rest mass equals , equivalent to 21.5 megatonnes of TNT.\nConversely, the mass equivalent of an everyday amount energy is minuscule. Examples of large-scale transformations between the rest energy of matter and other forms of energy are found in nuclear physics and particle physics. The complete conversion of matter, such as atoms, to non-matter, such as photons, occurs during interaction with antimatter.\nReversible and non-reversible transformations.\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above.\nAt the atomic scale, thermal energy is present in the form of motion and vibrations of individual atoms and molecules. When heat is generated, radiation excites lower energy states of these atoms and their surrounding fields. This heating process acts as a reservoir for part of the applied energy, from which it cannot be converted with 100% efficiency into other forms of energy. According to the second law of thermodynamics, this heat can only be completely recovered as usable energy at the price of an increase in some other kind of heat-like disorder in quantum states.\nAs the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.\nConservation of energy.\nThe fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.\nWhile heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\nRichard Feynman said during a 1961 lecture:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There is a fact, or if you wish, a \"law\", governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the \"conservation of energy\". It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.\u2014\u200a\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.\nThis law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured.\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scale, the uncertainty in the energy is given by\n formula_16\nwhich is similar in form to the Heisenberg Uncertainty Principle, but not really mathematically equivalent thereto, since \"E\" and \"t\" are not dynamically conjugate variables, neither in classical nor in quantum mechanics.\nIn particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions).101 Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law),336 for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force, and some other observable phenomena.\nEnergy transfer.\nClosed systems.\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, tidal interactions, and the conductive transfer of thermal energy.\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nwhere formula_17 is the amount of energy transferred, formula_2\u00a0 represents the work done on or by the system, and formula_19 represents the heat flow into or out of the system. As a simplification, the heat term, formula_19, can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes,\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nThis simplified equation is the one used to define the joule, for example.\nOpen systems.\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by formula_21, one may write:\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nThermodynamics.\nInternal energy.\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\nFirst law of thermodynamics.\nThe first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a \"gain\" in energy signified by a positive quantity) is given as:\n formula_22\nwhere the first term on the right is the heat transferred into the system, expressed in terms of temperature \"T\" and entropy \"S\" (in which entropy increases and its change d\"S\" is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is \"P\" and volume \"V\" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d\"V\", is negative when work is done on the system).\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and \"PV\"-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a \"closed\" system is expressed in a general form by:\n formula_23\nwhere formula_24 is the heat supplied to the system and formula_25 is the work applied to the system.\nEquipartition of energy.\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.\nThis principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way as to maximize their entropy production.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9653", "revid": "29330520", "url": "https://en.wikipedia.org/wiki?curid=9653", "title": "Expected value", "text": "Average value of a random variable\nIn probability theory, the expected value (also called expectation, expectancy, expectation operator, mathematical expectation, mean, expectation value, or first moment) is a generalization of the weighted average.\nThe expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes. In the case of a continuum of possible outcomes, the expectation is defined by integration. In the axiomatic foundation for probability provided by measure theory, the expectation is given by Lebesgue integration.\nThe expected value of a random variable X is often denoted by formula_1, formula_2, or formula_3, with E also often stylized as formula_4 or \"E\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nThe idea of the expected value originated in the middle of the 17th century from the study of the so-called problem of points, which seeks to divide the stakes \"in a fair way\" between two players, who have to end their game before it is properly finished. This problem had been debated for centuries. Many conflicting proposals and solutions had been suggested over the years when it was posed to Blaise Pascal by French writer and amateur mathematician Chevalier de M\u00e9r\u00e9 in 1654. M\u00e9r\u00e9 claimed that this problem could not be solved and that it showed just how flawed mathematics was when it came to its application to the real world. Pascal, being a mathematician, decided to work on a solution to the problem.\nHe began to discuss the problem in the famous series of letters to Pierre de Fermat. Soon enough, they both independently came up with a solution. They solved the problem in different computational ways, but their results were identical because their computations were based on the same fundamental principle. The principle is that the value of a future gain should be directly proportional to the chance of getting it. This principle seemed to have come naturally to both of them. They were very pleased by the fact that they had found essentially the same solution, and this in turn made them absolutely convinced that they had solved the problem conclusively; however, they did not publish their findings. They only informed a small circle of mutual scientific friends in Paris about it.\nIn Dutch mathematician Christiaan Huygens' book, he considered the problem of points, and presented a solution based on the same principle as the solutions of Pascal and Fermat. Huygens published his treatise in 1657, (see Huygens (1657)) \"De ratiociniis in ludo ale\u00e6\" on probability theory just after visiting Paris. The book extended the concept of expectation by adding rules for how to calculate expectations in more complicated situations than the original problem (e.g., for three or more players), and can be seen as the first successful attempt at laying down the foundations of the theory of probability.\nIn the foreword to his treatise, Huygens wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It should be said, also, that for some time some of the best mathematicians of France have occupied themselves with this kind of calculus so that no one should attribute to me the honour of the first invention. This does not belong to me. But these savants, although they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods. I have had therefore to examine and go deeply for myself into this matter by beginning with the elements, and it is impossible for me for this reason to affirm that I have even started from the same principle. But finally I have found that my answers in many cases do not differ from theirs.\u2014\u200a\nIn the mid-nineteenth century, Pafnuty Chebyshev became the first person to think systematically in terms of the expectations of random variables.\nEtymology.\nNeither Pascal nor Huygens used the term \"expectation\" in its modern sense. In particular, Huygens writes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;That any one Chance or Expectation to win any thing is worth just such a Sum, as wou'd procure in the same Chance and Expectation at a fair Lay. ... If I expect a or b, and have an equal chance of gaining them, my Expectation is worth (a+b)/2.\nMore than a hundred years later, in 1814, Pierre-Simon Laplace published his tract \"Th\u00e9orie analytique des probabilit\u00e9s\", where the concept of expected value was defined explicitly:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;... this advantage in the theory of chance is the product of the sum hoped for by the probability of obtaining it; it is the partial sum which ought to result when we do not wish to run the risks of the event in supposing that the division is made proportional to the probabilities. This division is the only equitable one when all strange circumstances are eliminated; because an equal degree of probability gives an equal right for the sum hoped for. We will call this advantage \"mathematical hope.\"\nNotations.\nThe use of the letter E to denote \"expected value\" goes back to W. A. Whitworth in 1901. The symbol has since become popular for English writers. In German, E stands for \"Erwartungswert\", in Spanish for \"esperanza matem\u00e1tica\", and in French for \"esp\u00e9rance math\u00e9matique.\"\nWhen \"E\" is used to denote \"expected value\", authors use a variety of stylizations: the expectation operator can be stylized as E (upright), E (italic), or formula_4 (in blackboard bold), while a variety of bracket notations (such as E(\"X\"), E[\"X\"], and E\"X\") are all used.\nAnother popular notation is \u03bc\"X\". \u27e8\"X\"\u27e9, \u27e8\"X\"\u27e9av, and formula_6 are commonly used in physics. M(\"X\") is used in Russian-language literature.\nDefinition.\nAs discussed above, there are several context-dependent ways of defining the expected value. The simplest and original definition deals with the case of finitely many possible outcomes, such as in the flip of a coin. With the theory of infinite series, this can be extended to the case of countably many possible outcomes. It is also very common to consider the distinct case of random variables dictated by (piecewise-)continuous probability density functions, as these arise in many natural contexts. All of these specific definitions may be viewed as special cases of the general definition based upon the mathematical tools of measure theory and Lebesgue integration, which provide these different contexts with an axiomatic foundation and common language.\nAny definition of expected value may be extended to define an expected value of a multidimensional random variable, i.e. a random vector X. It is defined component by component, as E[\"X\"]\"i\" \n E[\"X\"\"i\"]. Similarly, one may define the expected value of a random matrix X with components \"X\"\"ij\" by E[\"X\"]\"ij\" \n E[\"X\"\"ij\"].\nRandom variables with finitely many outcomes.\nConsider a random variable X with a \"finite\" list \"x\"1, ..., \"x\"\"k\" of possible outcomes, each of which (respectively) has probability \"p\"1, ..., \"p\"\"k\" of occurring. The expectation of X is defined as\nformula_7\nSince the probabilities must satisfy \"p\"1 + \u22c5\u22c5\u22c5 + \"p\"\"k\" = 1, it is natural to interpret E[\"X\"] as a weighted average of the \"x\"\"i\" values, with weights given by their probabilities \"p\"\"i\".\nIn the special case that all possible outcomes are equiprobable (that is, \"p\"1 = \u22c5\u22c5\u22c5 = \"p\"\"k\"), the weighted average is given by the standard average. In the general case, the expected value takes into account the fact that some outcomes are more likely than others.\nRandom variables with countably infinitely many outcomes.\nInformally, the expectation of a random variable with a countably infinite set of possible outcomes is defined analogously as the weighted average of all possible outcomes, where the weights are given by the probabilities of realizing each given value. This is to say that\nformula_17\nwhere \"x\"1, \"x\"2, ... are the possible outcomes of the random variable X and \"p\"1, \"p\"2, ... are their corresponding probabilities. In many non-mathematical textbooks, this is presented as the full definition of expected values in this context.\nHowever, there are some subtleties with infinite summation, so the above formula is not suitable as a mathematical definition. In particular, the Riemann series theorem of mathematical analysis illustrates that the value of certain infinite sums involving positive and negative summands depends on the order in which the summands are given. Since the outcomes of a random variable have no naturally given order, this creates a difficulty in defining expected value precisely.\nFor this reason, many mathematical textbooks only consider the case that the infinite sum given above converges absolutely, which implies that the infinite sum is a finite number independent of the ordering of summands. In the alternative case that the infinite sum does not converge absolutely, one says the random variable \"does not have finite expectation.\"\nRandom variables with density.\nNow consider a random variable X which has a probability density function given by a function f on the real number line. This means that the probability of X taking on a value in any given open interval is given by the integral of f over that interval. The expectation of X is then given by the integral\nformula_23\nA general and mathematically precise formulation of this definition uses measure theory and Lebesgue integration, and the corresponding theory of \"absolutely continuous random variables\" is described in the next section. The density functions of many common distributions are piecewise continuous, and as such the theory is often developed in this restricted setting. For such functions, it is sufficient to only consider the standard Riemann integration. Sometimes \"continuous random variables\" are defined as those corresponding to this special class of densities, although the term is used differently by various authors.\nAnalogously to the countably-infinite case above, there are subtleties with this expression due to the infinite region of integration. Such subtleties can be seen concretely if the distribution of X is given by the Cauchy distribution Cauchy(0, \u03c0), so that \"f\"(\"x\") \n (\"x\"2 + \u03c02)\u22121. It is straightforward to compute in this case that\nformula_24\nThe limit of this expression as \"a\" \u2192 \u2212\u221e and \"b\" \u2192 \u221e does not exist: if the limits are taken so that \"a\" \n \u2212\"b\", then the limit is zero, while if the constraint 2\"a\" \n \u2212\"b\" is taken, then the limit is ln(2).\nTo avoid such ambiguities, in mathematical textbooks it is common to require that the given integral converges absolutely, with E[\"X\"] left undefined otherwise. However, measure-theoretic notions as given below can be used to give a systematic definition of E[\"X\"] for more general random variables X.\nArbitrary real-valued random variables.\nAll definitions of the expected value may be expressed in the language of measure theory. In general, if X is a real-valued random variable defined on a probability space (\u03a9, \u03a3, P), then the expected value of X, denoted by E[\"X\"], is defined as the Lebesgue integral\nformula_25\nDespite the newly abstract situation, this definition is extremely similar in nature to the very simplest definition of expected values, given above, as certain weighted averages. This is because, in measure theory, the value of the Lebesgue integral of X is defined via weighted averages of \"approximations\" of X which take on finitely many values. Moreover, if given a random variable with finitely or countably many possible values, the Lebesgue theory of expectation is identical to the summation formulas given above. However, the Lebesgue theory clarifies the scope of the theory of probability density functions. A random variable X is said to be \"absolutely continuous\" if any of the following conditions are satisfied:\nThese conditions are all equivalent, although this is nontrivial to establish. In this definition, f is called the \"probability density function\" of X (relative to Lebesgue measure). According to the change-of-variables formula for Lebesgue integration, combined with the law of the unconscious statistician, it follows that\nformula_27\nfor any absolutely continuous random variable X. The above discussion of continuous random variables is thus a special case of the general Lebesgue theory, due to the fact that every piecewise-continuous function is measurable.\nThe expected value of any real-valued random variable formula_8 can also be defined on the graph of its cumulative distribution function formula_29 by a nearby equality of areas. In fact, formula_30 with a real number formula_31 if and only if the two surfaces in the formula_32-formula_33-plane, described by\nformula_34\nrespectively, have the same finite area, i.e. if\nformula_35\nand both improper Riemann integrals converge. Finally, this is equivalent to the representation\nformula_36\nalso with convergent integrals.\nInfinite expected values.\nExpected values as defined above are automatically finite numbers. However, in many cases it is fundamental to be able to consider expected values of \u00b1\u221e. This is intuitive, for example, in the case of the St. Petersburg paradox, in which one considers a random variable with possible outcomes \"x\"\"i\" \n 2\"i\", with associated probabilities \"p\"\"i\" \n 2\u2212\"i\", for i ranging over all positive integers. According to the summation formula in the case of random variables with countably many outcomes, one has\nformula_37\nIt is natural to say that the expected value equals +\u221e.\nThere is a rigorous mathematical theory underlying such ideas, which is often taken as part of the definition of the Lebesgue integral. The first fundamental observation is that, whichever of the above definitions are followed, any \"nonnegative\" random variable whatsoever can be given an unambiguous expected value; whenever absolute convergence fails, then the expected value can be defined as +\u221e. The second fundamental observation is that any random variable can be written as the difference of two nonnegative random variables. Given a random variable X, one defines the positive and negative parts by \"X\" + \n max(\"X\", 0) and \"X\" \u2212 \n \u2212min(\"X\", 0). These are nonnegative random variables, and it can be directly checked that \"X\" \n \"X\" + \u2212 \"X\" \u2212. Since E[\"X\" +] and E[\"X\" \u2212] are both then defined as either nonnegative numbers or +\u221e, it is then natural to define:\nformula_38\nAccording to this definition, E[\"X\"] exists and is finite if and only if E[\"X\" +] and E[\"X\" \u2212] are both finite. Due to the formula |\"X\"| \n \"X\" + + \"X\" \u2212, this is the case if and only if E|\"X\"| is finite, and this is equivalent to the absolute convergence conditions in the definitions above. As such, the present considerations do not define finite expected values in any cases not previously considered; they are only useful for infinite expectations.\n 0 and so E[\"X\"] \n +\u221e as desired.\n \u221e and E[\"X\" \u2212] \n \u221e (see Harmonic series). Hence, in this case the expectation of X is undefined.\nTail-sum formula.\nIn the case of a non-negative integer-valued random variable X, the expected value can also be expressed in terms of its \"tail probabilities\" (sometimes called the tail-sum formula):\nformula_39\nA more general version holds for any non-negative random variable (discrete or continuous):\nformula_40\nwhere the integrand is the survival function of X.\nExpected values of common distributions.\nThe following table gives the expected values of some commonly occurring probability distributions. The third column gives the expected values both in the form immediately given by the definition, as well as in the simplified form obtained by computation therefrom. The details of these computations, which are not always straightforward, can be found in the indicated references.\nProperties.\nThe basic properties below (and their names in bold) replicate or follow immediately from those of Lebesgue integral. Note that the letters \"a.s.\" stand for \"almost surely\"\u2014a central property of the Lebesgue integral. Basically, one says that an inequality like formula_41 is true almost surely, when the probability measure attributes zero-mass to the complementary event formula_42\nInequalities.\nConcentration inequalities control the likelihood of a random variable taking on large values. Markov's inequality is among the best-known and simplest to prove: for a \"nonnegative\" random variable X and any positive number a, it states that formula_89\nIf X is any random variable with finite expectation, then Markov's inequality may be applied to the random variable |\"X\"\u2212E[\"X\"]|2 to obtain Chebyshev's inequality formula_90\nwhere Var is the variance. These inequalities are significant for their nearly complete lack of conditional assumptions. For example, for any random variable with finite expectation, the Chebyshev inequality implies that there is at least a 75% probability of an outcome being within two standard deviations of the expected value. However, in special cases the Markov and Chebyshev inequalities often give much weaker information than is otherwise available. For example, in the case of an unweighted dice, Chebyshev's inequality says that odds of rolling between 1 and 6 is at least 53%; in reality, the odds are of course 100%. The Kolmogorov inequality extends the Chebyshev inequality to the context of sums of random variables.\nThe following three inequalities are of fundamental importance in the field of mathematical analysis and its applications to probability theory.\n 1, then formula_93 for any random variables X and Y. The special case of \"p\" \n \"q\" \n 2 is called the Cauchy\u2013Schwarz inequality, and is particularly well-known.\nThe H\u00f6lder and Minkowski inequalities can be extended to general measure spaces, and are often given in that context. By contrast, the Jensen inequality is special to the case of probability spaces.\nExpectations under convergence of random variables.\nIn general, it is not the case that formula_95 even if formula_96 pointwise. Thus, one cannot interchange limits and expectation, without additional conditions on the random variables. To see this, let formula_97 be a random variable distributed uniformly on formula_98 For formula_99 define a sequence of random variables\nformula_100\nwith formula_101 being the indicator function of the event formula_102 Then, it follows that formula_103 pointwise. But, formula_104 for each formula_105 Hence, formula_106\nAnalogously, for general sequence of random variables formula_107 the expected value operator is not formula_108-additive, i.e.\nformula_109\nAn example is easily obtained by setting formula_110 and formula_111 for formula_112 where formula_113 is as in the previous example.\nA number of convergence results specify exact conditions which allow one to interchange limits and expectations, as specified below.\nRelationship with characteristic function.\nThe probability density function formula_138 of a scalar random variable formula_8 is related to its characteristic function formula_140 by the inversion formula:\nformula_141\nFor the expected value of formula_142 (where formula_143 is a Borel function), we can use this inversion formula to obtain\nformula_144\nIf formula_145 is finite, changing the order of integration, we get, in accordance with Fubini\u2013Tonelli theorem,\nformula_146\nwhere\nformula_147\nis the Fourier transform of formula_148 The expression for formula_145 also follows directly from the Plancherel theorem.\nUses and applications.\nThe expectation of a random variable plays an important role in a variety of contexts.\nIn statistics, where one seeks estimates for unknown parameters based on available data gained from samples, the sample mean serves as an estimate for the expectation, and is itself a random variable. In such settings, the sample mean is considered to meet the desirable criterion for a \"good\" estimator in being unbiased; that is, the expected value of the estimate is equal to the true value of the underlying parameter. \nFor a different example, in decision theory, an agent making an optimal choice in the context of incomplete information is often assumed to maximize the expected value of their utility function.\nIt is possible to construct an expected value equal to the probability of an event by taking the expectation of an indicator function that is one if the event has occurred and zero otherwise. This relationship can be used to translate properties of expected values into properties of probabilities, e.g. using the law of large numbers to justify estimating probabilities by frequencies.\nThe expected values of the powers of \"X\" are called the moments of \"X\"; the moments about the mean of \"X\" are expected values of powers of \"X\" \u2212 E[\"X\"]. The moments of some random variables can be used to specify their distributions, via their moment generating functions.\nTo empirically estimate the expected value of a random variable, one repeatedly measures observations of the variable and computes the arithmetic mean of the results. If the expected value exists, this procedure estimates the true expected value in an unbiased manner and has the property of minimizing the sum of the squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.\nThis property is often exploited in a wide variety of applications, including general problems of statistical estimation and machine learning, to estimate (probabilistic) quantities of interest via Monte Carlo methods, since most quantities of interest can be written in terms of expectation, e.g. formula_150 where formula_151 is the indicator function of the set formula_152\nIn classical mechanics, the center of mass is an analogous concept to expectation. For example, suppose \"X\" is a discrete random variable with values \"xi\" and corresponding probabilities \"pi.\" Now consider a weightless rod on which are placed weights, at locations \"xi\" along the rod and having masses \"pi\" (whose sum is one). The point at which the rod balances is E[\"X\"].\nExpected values can also be used to compute the variance, by means of the computational formula for the variance\nformula_153\nA very important application of the expectation value is in the field of quantum mechanics. The expectation value of a quantum mechanical operator formula_154 operating on a quantum state vector formula_155 is written as formula_156 The uncertainty in formula_154 can be calculated by the formula formula_158.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9656", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=9656", "title": "Electric light", "text": "Device for producing light from electricity\nAn electric light, lamp, or light bulb is an electrical device that produces light from electricity. It is the most common form of artificial lighting. Lamps usually have a base made of ceramic, metal, glass, or plastic that secures them in the socket of a light fixture, which is also commonly referred to as a 'lamp.' The electrical connection to the socket may be made with a screw-thread base, two metal pins, two metal caps or a bayonet mount.\nThe three main categories of electric lights are incandescent lamps, which produce light by a filament heated white-hot by electric current, gas-discharge lamps, which produce light by means of an electric arc through a gas, such as fluorescent lamps, and LED lamps, which produce light by a flow of electrons across a band gap in a semiconductor.\nThe energy efficiency of electric lighting has significantly improved since the first demonstrations of arc lamps and incandescent light bulbs in the 19th century. Modern electric light sources come in a profusion of types and sizes adapted to many applications. Most modern electric lighting is powered by centrally generated electric power, but lighting may also be powered by mobile or standby electric generators or battery systems. Battery-powered light is often reserved for when and where stationary lights fail, often in the form of flashlights or electric lanterns, as well as in vehicles.\nHistory.\nBefore electric lighting became common in the early 20th century, people used candles, gas lights, oil lamps, and fires. In 1799\u20131800, Alessandro Volta created the voltaic pile, the first electric battery. Current from these batteries could heat copper wire to incandescence. Vasily Vladimirovich Petrov developed the first persistent electric arc in 1802, and English chemist Humphry Davy gave a practical demonstration of an arc light in 1806. \nIt took more than a century of continuous and incremental improvement, including numerous designs, patents, and resulting intellectual property disputes, to get from these early experiments to commercially produced incandescent light bulbs in the 1920s.\nIn 1840, Warren de la Rue enclosed a platinum coil in a vacuum tube and passed an electric current through it, thus creating one of the world's first electric light bulbs. The design was based on the concept that the high melting point of platinum would allow it to operate at high temperatures and that the evacuated chamber would contain fewer gas molecules to react with the platinum, improving its longevity. Although it was an efficient design, the cost of the platinum made it impractical for commercial use.\nWilliam Greener, an English inventor, made significant contributions to early electric lighting with his lamp in 1846 (patent specification 11076), laying the groundwork for future innovations such as those by Thomas Edison.\nThe late 1870s and 1880s were marked by intense competition and innovation, with inventors like Joseph Swan in the UK and Thomas Edison in the US independently developing functional incandescent lamps. Swan's bulbs, based on designs by William Staite, were successful, but the filaments were too thick. Edison worked to create bulbs with thinner filaments and better vacuum, producing a more commercially viable light bulb. The rivalry between Swan and Edison eventually led to a merger, forming the Edison and Swan Electric Light Company which sold lamps with a new filament designed by Swan. By the early twentieth century these had completely replaced arc lamps.\nThe turn of the century saw further improvements in bulb longevity and efficiency, notably with the introduction of the tungsten filament by William D. Coolidge, who applied for a patent in 1912. This innovation became a standard for incandescent bulbs for many years.\nIn 1910, Georges Claude introduced the first neon light, paving the way for neon signs which would become ubiquitous in advertising.\nIn 1934, Arthur Compton, a renowned physicist and GE consultant, reported to the GE lamp department on successful experiments with fluorescent lighting at General Electric Co., Ltd. in Great Britain (unrelated to General Electric in the United States). Stimulated by this report, and with all of the key elements available, a team led by George E. Inman built a prototype fluorescent lamp in 1934 at General Electric's Nela Park (Ohio) engineering laboratory. This was not a trivial exercise; as noted by Arthur A. Bright, \"A great deal of experimentation had to be done on lamp sizes and shapes, cathode construction, gas pressures of both argon and mercury vapor, colors of fluorescent powders, methods of attaching them to the inside of the tube, and other details of the lamp and its auxiliaries before the new device was ready for the public.\"\nThe first practical LED arrived in 1962. These early LEDs were inefficient and could only display deep red colors, making them unsuitable for general lighting and restricting their usage to numeric displays and indicator lights.\nThe first high-brightness blue LED was demonstrated by Shuji Nakamura of Nichia Corporation in 1994. The existence of blue LEDs led to the development of the first 'white LED', which employed a phosphor coating to partially convert the emitted blue light to lower frequencies, creating white light. By the start of the 21st century LED lamps suitable for general lighting were entering the market, and in 2009 Phillips introduced the first lamps designed to replace standard 60\u00a0W \"Edison screw fixture\" light bulbs.\nA phase-out of incandescent light bulbs took place worldwide in the first few decades of the 21st century, driven by a combination of government regulation and consumer preference for higher energy efficiency and longer-lived bulbs. By 2019 electricity usage in the United States had decreased for at least five straight years, due in part to U.S. electricity consumers replacing incandescent light bulbs with LEDs.\nTypes.\nIncandescent.\nIn its modern form, the incandescent light bulb consists of a coiled filament of tungsten sealed in a globular glass chamber, either a vacuum or full of an inert gas such as argon. When an electric current is connected, the tungsten is heated to and glows, emitting light that approximates a continuous spectrum.\nIncandescent bulbs are highly inefficient, in that just 2\u20135% of the energy consumed is emitted as visible, usable light. The remaining 95% is lost as heat. In warmer climates, the emitted heat must then be removed, putting additional pressure on ventilation or air conditioning systems. In colder weather, the heat byproduct has some value, and has been successfully harnessed for warming in devices such as heat lamps. Incandescent bulbs are nonetheless being phased out in favor of technologies like CFLs and LED bulbs in many countries due to their low energy efficiency. The European Commission estimated in 2012 that a complete ban on incandescent bulbs would contribute 5 to 10 billion euros to the economy and save 15 billion metric tonnes of carbon dioxide emissions.\nHalogen.\nHalogen lamps are usually much smaller than standard incandescent lamps, because for successful operation a bulb temperature over 200\u00a0\u00b0C is generally necessary. For this reason, most have a bulb of fused silica (quartz) or aluminosilicate glass. This is often sealed inside an additional layer of glass. The outer glass is a safety precaution, to reduce ultraviolet emission and to contain hot glass shards should the inner envelope explode during operation. Oily residue from fingerprints may cause a hot quartz envelope to shatter due to excessive heat buildup at the contamination site. The risk of burns or fire is also greater with bare bulbs, leading to their prohibition in some places, unless enclosed by the luminaire.\nThose designed for 12- or 24-volt operation have compact filaments, useful for good optical control. Also, they have higher efficacies (lumens per watt) and longer lives than non-halogen types. The light output remains almost constant throughout their life.\nFluorescent.\nFluorescent lamps consist of a glass tube that contains mercury vapour or argon under low pressure. Electricity flowing through the tube causes the gases to give off ultraviolet energy. The inside of the tubes are coated with phosphors that give off visible light when struck by ultraviolet photons. They have much higher efficiency than incandescent lamps. For the same amount of light generated, they typically use around one-quarter to one-third the power of an incandescent. The typical luminous efficacy of fluorescent lighting systems is 50\u2013100 lumens per watt, several times the efficacy of incandescent bulbs with comparable light output. Fluorescent lamp fixtures are more costly than incandescent lamps, because they require a ballast to regulate the current through the lamp, but the lower energy cost typically offsets the higher initial cost. Compact fluorescent lamps are available in the same popular sizes as incandescent lamps and are used as an energy-saving alternative in homes. Because they contain mercury, many fluorescent lamps are classified as hazardous waste. The United States Environmental Protection Agency recommends that fluorescent lamps be segregated from general waste for recycling or safe disposal, and some jurisdictions require recycling of them.\nLED.\nThe solid-state light-emitting diode (LED) has been popular as an indicator light in consumer electronics and professional audio gear since the 1970s. In the 2000s, efficacy and output have risen to the point where LEDs are now being used in lighting applications such as car headlights and brake lights, in flashlights and bicycle lights, as well as in decorative applications, such as holiday lighting. Indicator LEDs are known for their extremely long life, up to 100,000 hours, but lighting LEDs are operated much less conservatively, and consequently have shorter lives. LED technology is useful for lighting designers, because of its low power consumption, low heat generation, instantaneous on/off control, and in the case of single color LEDs, continuity of color throughout the life of the diode and relatively low cost of manufacture. LED lifetime depends strongly on the temperature of the diode. Operating an LED lamp in conditions that increase the internal temperature can greatly shorten the lamp's life. Some lasers have been adapted as an alternative to LEDs to provide highly focused illumination.\nCarbon arc.\nCarbon arc lamps consist of two carbon rod electrodes in open air, supplied by a current-limiting ballast. The electric arc is struck by touching the rod tips then separating them. The ensuing arc produces a white-hot plasma between the rod tips. These lamps have higher efficacy than filament lamps, but the carbon rods are short-lived and require constant adjustment in use, as the intense heat of the arc erodes them. The lamps produce significant ultraviolet output, they require ventilation when used indoors, and due to their intensity they need protection from direct sight.\nInvented by Humphry Davy around 1805, the carbon arc was the first practical electric light. It was used commercially beginning in the 1870s for large building and street lighting until it was superseded in the early 20th century by the incandescent light. Carbon arc lamps operate at high power and produce high intensity white light. They also are a point source of light. They remained in use in limited applications that required these properties, such as movie projectors, stage lighting, and searchlights, until after World War II.\nDischarge.\nA discharge lamp has a glass or silica envelope containing two metal electrodes separated by a gas. Gases used include, neon, argon, xenon, sodium, metal halides, and mercury. The core operating principle is much the same as the carbon arc lamp, but the term \"arc lamp\" normally refers to carbon arc lamps, with more modern types of gas discharge lamp normally called discharge lamps. With some discharge lamps, very high voltage is used to strike the arc. This requires an electrical circuit called an igniter, which is part of the electrical ballast circuitry. After the arc is struck, the internal resistance of the lamp drops to a low level, and the ballast limits the current to the operating current. Without a ballast, excess current would flow, causing rapid destruction of the lamp.\nSome lamp types contain a small amount of neon, which permits striking at normal running voltage with no external ignition circuitry. Low-pressure sodium lamps operate this way. The simplest ballasts are just an inductor, and are chosen where cost is the deciding factor, such as street lighting. More advanced electronic ballasts may be designed to maintain constant light output over the life of the lamp, may drive the lamp with a square wave to maintain completely flicker-free output, and shut down in the event of certain faults.\nThe most efficient source of electric light is the low-pressure sodium lamp. It produces, for all practical purposes, a monochromatic orange-yellow light, which gives a similarly monochromatic perception of any illuminated scene. For this reason, it is generally reserved for outdoor public lighting applications. Low-pressure sodium lights are favoured for public lighting by astronomers, since the light pollution that they generate can be easily filtered, contrary to broadband or continuous spectra.\nCharacteristics.\nForm factor.\nMany lamp units, or light bulbs, are specified in standardized shape codes and socket names. Incandescent bulbs and their retrofit replacements are often specified as \"A19/A60 E26/E27\", a common size for those kinds of light bulbs. In this example, the \"A\" parameters describe the bulb size and shape within the A-series light bulb while the \"E\" parameters describe the Edison screw base size and thread characteristics.\nComparison parameters.\nCommon comparison parameters include:\nLess common parameters include color rendering index (CRI).\nLife expectancy.\nLife expectancy for many types of lamp is defined as the number of hours of operation at which 50% of them fail, that is the median life of the lamps. Production tolerances as low as 1% can create a variance of 25% in lamp life, so in general some lamps will fail well before the rated life expectancy, and some will last much longer. For LEDs, lamp life is defined as the operation time at which 50% of lamps have experienced a 70% decrease in light output. In the 1900s the Phoebus cartel formed in an attempt to reduce the life of electric light bulbs, an example of planned obsolescence.\nSome types of lamp are also sensitive to switching cycles. Rooms with frequent switching, such as bathrooms, can expect much shorter lamp life than what is printed on the box. Compact fluorescent lamps are particularly sensitive to switching cycles.\nUses.\nThe total amount of artificial light (especially from street lights) is sufficient for cities to be easily visible at night from the air, and from space. External lighting grew at a rate of 3\u20136 percent for the later half of the 20th century and is the major source of light pollution that burdens astronomers and others with 80% of the world's population living in areas with night time light pollution. Light pollution has been shown to have a negative effect on some wildlife.\nElectric lamps can be used as heat sources, for example in incubators, as infrared lamps in fast food restaurants and toys such as the Kenner Easy-Bake Oven.\nLamps can also be used for light therapy to deal with such issues as vitamin D deficiency, skin conditions such as acne and dermatitis, skin cancers, and seasonal affective disorder. Lamps which emit a specific frequency of blue light are also used to treat neonatal jaundice with the treatment which was initially undertaken in hospitals being able to be conducted at home.\nElectric lamps can also be used as a grow light to aid in plant growth especially in indoor hydroponics and aquatic plants with recent research into the most effective types of light for plant growth.\nDue to their nonlinear resistance characteristics, tungsten filament lamps have long been used as fast-acting thermistors in electronic circuits. Popular uses have included:\nCultural symbolism.\nIn Western culture, a lightbulb \u2014 in particular, the appearance of an illuminated lightbulb above a person's head \u2014 signifies sudden inspiration.\nA stylized depiction of a light bulb features as the logo of the Turkish AK Party.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9657", "revid": "2723875", "url": "https://en.wikipedia.org/wiki?curid=9657", "title": "Edgar Rice Burroughs", "text": "American writer (1875\u20131950)\nEdgar Rice Burroughs (September 1, 1875\u00a0\u2013 March 19, 1950) was an American writer, recognized for his prolific output in the adventure, science fiction, and fantasy genres. Best known for creating the characters Tarzan (who appeared in a series of twenty-four books by him) and John Carter (who was a recurring character in a series of eleven books), he also wrote the \"Pellucidar\" series, the \"Amtor\" series, and the \"Caspak\" trilogy.\nTarzan was immediately popular, and Burroughs capitalized on it in every possible way, including a syndicated Tarzan comic strip, films, and merchandise. Tarzan remains one of the most successful fictional characters to this day and is a cultural icon. Burroughs's California ranch is now the center of the Tarzana neighborhood in Los Angeles, named after the character. Burroughs was an explicit supporter of eugenics and scientific racism in both his fiction and nonfiction; Tarzan was meant to reflect these concepts.\nBiography.\nEarly life and family.\nBurroughs was born on September 1, 1875, in Chicago, Illinois, the fourth son of Major George Tyler Burroughs, a businessman and Civil War veteran, and his wife, Mary Evaline (Zieger) Burroughs. Edgar's middle name is from his paternal grandmother, Mary Coleman Rice Burroughs.\nBurroughs was of English and Pennsylvania Dutch ancestry, with a family line that had been in North America since the Colonial era. Through his Rice grandmother, Burroughs was descended from settler Edmund Rice, one of the English Puritans who moved to Massachusetts Bay Colony in the early 17th century. He once remarked: \"I can trace my ancestry back to Deacon Edmund Rice.\" The Burroughs side of the family was also of English origin, having emigrated to Massachusetts around the same time. Many of his ancestors fought in the American Revolution. Some of his ancestors settled in Virginia during the colonial period, and Burroughs often emphasized his connection with that side of his family, seeing it as romantic and warlike.\nBurroughs was educated at a number of local schools then at Phillips Academy in Andover, Massachusetts, and then the Michigan Military Academy. He graduated in 1895, but he failed the entrance exam for the United States Military Academy at West Point, so instead he enlisted with the 7th U.S. Cavalry in Fort Grant, Arizona Territory. However, he was diagnosed with a heart problem and thus ineligible to serve, so he was discharged in 1897.\nAfter his discharge, Burroughs worked at a number of different jobs. During the Chicago influenza epidemic of 1891, he spent half a year at his brother's ranch on the Raft River in Idaho as a cowboy. He drifted afterward, then worked at his father's Chicago battery factory in 1899. He married his childhood sweetheart, Emma Hulbert (1876\u20131944), in January 1900.\nIn 1903, Burroughs joined his brothers, Yale graduates George and Harry, who were by then prominent Pocatello area ranchers in southern Idaho and partners in the Sweetser-Burroughs Mining Company, where he took on managing their ill-fated Snake River gold dredge, a classic bucket-line dredge. The Burroughs brothers were also the sixth cousins once removed of famed miner Kate Rice who in 1914 became the first female prospector in the Canadian North. Journalist and publisher C. Allen Thorndike Rice was also his third cousin.\nWhen the new mine proved unsuccessful, the brothers secured for Burroughs a position with the Oregon Short Line Railroad in Salt Lake City. Burroughs resigned from the railroad in October 1904.\nLater life.\nBy 1911, around age 36, after seven years of low wages as a pencil-sharpener wholesaler, Burroughs began to write fiction. By this time, Emma and he had two children, Joan (1908\u20131972), and Hulbert (1909\u20131991). During this period, he had copious spare time and began reading pulp-fiction magazines. In 1929, he recalled thinking that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"[...] if people were paid for writing rot such as I read in some of those magazines, that I could write stories just as rotten. As a matter of fact, although I had never written a story, I knew absolutely that I could write stories just as entertaining and probably a whole lot more so than any I chanced to read in those magazines.\"\nIn 1913, Burroughs and Emma had their third and last child, John Coleman Burroughs (1913\u20131979), later known for his illustrations of his father's books.\nIn the 1920s, Burroughs became a pilot, purchased a Security Airster S-1, and encouraged his family to learn to fly.\nHis daughter Joan married \"Tarzan\" film actor James Pierce. She starred with her husband as the voice of \"Jane\" during 1932\u20131934 for the \"Tarzan\" radio series.\nBurroughs divorced Emma in 1934 and in 1935 married the former actress Florence Gilbert Dearholt, who was the former wife of his friend Ashton Dearholt, with whom he had co-founded Burroughs-Tarzan Enterprises while filming \"The New Adventures of Tarzan\". Burroughs adopted the Dearholts' two children. He and Florence divorced in 1942.\nBurroughs was in his late 60s and was in Honolulu at the time of the Japanese attack on Pearl Harbor. Despite his age, he applied for and received permission to become a war correspondent, becoming one of the oldest U.S. war correspondents during World War II. This period of his life is mentioned in William Brinkley's bestselling novel \"Don't Go Near the Water\".\nDeath.\nAfter the war ended, Burroughs moved back to Encino, California, where after many health problems, he died of a heart attack on March 19, 1950, having written almost 80 novels. He is buried in Tarzana, California, US.\nAt the time of his death he was believed to have been the writer who had made the most from films, earning over US$2\u00a0million in royalties from 27 Tarzan pictures.\nThe Science Fiction Hall of Fame inducted Burroughs in 2003.\nLiterary career.\nAiming his work at the pulps\u2014under the name \"Norman Bean\" to protect his reputation\u2014Burroughs had his first story, \"Under the Moons of Mars\", serialized by Frank Munsey in the February to July 1912 issues of \"The All-Story\". \"Under the Moons of Mars\" inaugurated the \"Barsoom\" series, introduced John Carter, and earned Burroughs US$400 ($11,922 today). It was first published as a book by A. C. McClurg of Chicago in 1917, entitled \"A Princess of Mars\", after three Barsoom sequels had appeared as serials and McClurg had published the first four serial Tarzan novels as books.\nBurroughs soon took up writing full-time, and by the time the run of \"Under the Moons of Mars\" had finished, he had completed two novels, including \"Tarzan of the Apes\", published from October 1912 and one of his most successful series.\nBurroughs also wrote popular science fiction and fantasy stories involving adventurers from Earth transported to various planets (notably Barsoom, Burroughs's fictional name for Mars, and Amtor, his fictional name for Venus), lost islands (Caspak), and into the interior of the Hollow Earth in his \"Pellucidar\" stories. He also wrote Westerns and historical romances. Besides those published in \"All-Story\", many of his stories were published in \"The Argosy\" magazine.\nTarzan was a cultural sensation when introduced. Burroughs was determined to capitalize on Tarzan's popularity in every way possible. He planned to exploit Tarzan through several different media including a syndicated Tarzan comic strip, movies, and merchandise. Experts in the field advised against this course of action, stating that the different media would just end up competing against each other. Burroughs went ahead, however, and proved the experts wrong \u2013 the public wanted Tarzan in whatever fashion he was offered. Tarzan remains one of the most successful fictional characters to this day and is a cultural icon.\nIn either 1915 or 1919, Burroughs purchased a large ranch north of Los Angeles, California, which he named \"Tarzana\". The citizens of the community that sprang up around the ranch voted to adopt that name when their community, Tarzana, California, was formed in 1927. Also, the unincorporated community of Tarzan, Texas, was formally named in 1927 when the US Postal Service accepted the name, reputedly coming from the popularity of the first (silent) \"Tarzan of the Apes\" film, starring Elmo Lincoln, and an early \"Tarzan\" comic strip.\nIn 1923, Burroughs set up his own company, Edgar Rice Burroughs, Inc., and began printing his own books through the 1930s.\nReception.\nBecause of the part Burroughs's science fiction played in inspiring real exploration of Mars, an impact crater on Mars was named in his honor after his death. In a \"Paris Review\" interview, Ray Bradbury said of Burroughs: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Edgar Rice Burroughs never would have looked upon himself as a social mover and shaker with social obligations. But as it turns out \u2013 and I love to say it because it upsets everyone terribly \u2013 Burroughs is probably the most influential writer in the entire history of the world. By giving romance and adventure to a whole generation of boys, Burroughs caused them to go out and decide to become special.\"\nIn \"Something of Myself\" (published posthumously in 1937) Rudyard Kipling wrote: \"My \"Jungle Books\" begat Zoos of [imitators]. But the genius of all the genii was one who wrote a series called \"Tarzan of the Apes\". I read it, but regret I never saw it on the films, where it rages most successfully. He had 'jazzed' the motif of the \"Jungle Books\" and, I imagine, had thoroughly enjoyed himself. He was reported to have said that he wanted to find out how bad a book he could write and 'get away with', which is a legitimate ambition.\"\nBy 1963, Floyd C. Gale of \"Galaxy Science Fiction\" wrote when discussing reprints of several Burroughs novels by Ace Books, \"an entire generation has grown up inexplicably Burroughs-less\". He stated that most of the author's books had been out of print for years and that only the \"occasional laughable Tarzan film\" reminded the public of his fiction. Gale reported his surprise that after two decades his books were again available, with Canaveral Press, Dover Publications, and Ballantine Books also reprinting them.\nFew critical books have been written about Burroughs. From an academic standpoint, the most helpful are Erling Holtsmark's two books: \"Tarzan and Tradition\" and \"Edgar Rice Burroughs\"; Stan Galloway's \"The Teenage Tarzan: A Literary Analysis of Edgar Rice Burroughs' \"Jungle Tales of Tarzan; and Richard Lupoff's two books: \"Master of Adventure: Edgar Rice Burroughs\" and \"Barsoom: Edgar Rice Burroughs and the Martian Vision\". Galloway was identified by James Edwin Gunn as \"one of the half-dozen finest Burroughs scholars in the world\"; Galloway called Holtsmark his \"most important predecessor\".\nBurroughs strongly supported eugenics and scientific racism. His views held that English nobles made up a particular heritable elite among Anglo-Saxons. Tarzan was meant to reflect this, with him being born to English nobles and then adopted by talking apes (the Mangani). They express eugenicist views themselves, but Tarzan is permitted to live despite being deemed \"unfit\" in comparison and grows up to surpass not only them but black Africans, whom Burroughs clearly presents as inherently inferior. In one Tarzan story, he finds an ancient civilization where eugenics has been practiced for over 2,000 years, with the result that it is free of all crime. Criminal behavior is held to be entirely hereditary, with the solution having been to kill not only criminals but also their families. \"Lost on Venus\", a later novel, presents a similar utopia where forced sterilization is practiced and the \"unfit\" are killed. Burroughs explicitly supported such ideas in his unpublished nonfiction essay \"I See A New Race\". Additionally, his \"Pirate Blood\", which is not speculative fiction and remained unpublished after his death, portrayed the characters as victims of their hereditary criminal traits (one a descendant of the corsair Jean Lafitte, another from the Jukes family). These views have been compared with Nazi eugenics (though noting that they were popular and common at the time), with his \"Lost on Venus\" being released the same year the Nazis took power (in 1933).\nIn 2003, Burroughs was inducted into the Science Fiction and Fantasy Hall of Fame.\nAs of 2025, there exists a significant special collection of Edgar Rice Burroughs' various works at the Oak Park Public Library. Consisting of many rare books of his Tarzan, Mucker, Barsoom, Pellucidar, Venus, Caspak, and Moon series, the collection was developed due to Burroughs' own connection to the city, being where he wrote several of his first works, those being the Tarzan and Martian stories. Beyond the rare editions, the collection also holds a number of newspaper clippings, ephemera, correspondence between Burroughs and others, as well as various old Tarzan films. Much of the initial collection was gathered during a block party held in 1975 by a group called CHEETAH (Citizens Holding Exercises Extolling Tarzan's Anniversary Here) and compiled by Florence Moyer.\nIn popular culture.\nAdam Butcher portrays Burroughs (known in the show as \"Norman Bean\") in episode 3 of season 16 \"The Write Stuff\" (September 26, 2022) of the Canadian television period detective series Murdoch Mysteries.\nSelected works.\n\"Moon\" series.\nThese three texts have been published by various houses in one or two volumes. Adding to the confusion, some editions have the original (significantly longer) introduction to Part I from the first publication as a magazine serial, and others have the shorter version from the first book publication, which included all three parts under the title \"The Moon Maid\".\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9658", "revid": "40803200", "url": "https://en.wikipedia.org/wiki?curid=9658", "title": "Eug\u00e8ne Viollet-le-Duc", "text": "French architect and author (1814-1879)\nEug\u00e8ne Emmanuel Viollet-le-Duc (; 27 January 1814\u00a0\u2013 17 September 1879) was a French architect and author, famous for his restoration of the most prominent medieval landmarks in France. His major restoration projects included Notre-Dame de Paris, the Basilica of Saint Denis, Mont Saint-Michel, Sainte-Chapelle, the medieval walls of the city of Carcassonne, and Ch\u00e2teau de Roquetaillade in the Bordeaux region.\nHis writings on decoration and on the relationship between form and function in architecture had a fundamental influence on a whole new generation of architects, including all the major Art Nouveau artists: Antoni Gaud\u00ed, Victor Horta, Hector Guimard, Henry van de Velde, Henri Sauvage and the \u00c9cole de Nancy, Paul Hankar, Otto Wagner, Eug\u00e8ne Grasset, \u00c9mile Gall\u00e9, and Hendrik Petrus Berlage. He also influenced the first modern architects, Frank Lloyd Wright, Mies van der Rohe, Auguste Perret, Louis Sullivan, and Le Corbusier, who considered Viollet-le-Duc as the father of modern architecture. English architect William Burges claimed that \"We all cribbed on Viollet-le-Duc even though no one could read French\".\nHis writings also influenced John Ruskin, William Morris, and the Arts and Crafts movement. At the International Exhibition of 1862 in London, the aesthetic works of Edward Burne-Jones, Christina Rossetti, Philip Webb, William Morris, Simeon Solomon, and Edward Poynter were directly influenced from drawings in Viollet-le-Duc's Dictionary.\nYouth and education.\nViollet-le-Duc was born in Paris in 1814. His grandfather was an architect, and his father was a high-ranking civil servant, who in 1816 became the overseer of the royal residences of Louis XVIII. His uncle \u00c9tienne-Jean Del\u00e9cluze was a painter, a former student of Jacques-Louis David, an art critic and hosted a literary salon, which was attended by Stendhal and Sainte-Beuve. His mother hosted her own salon, which women could attend as well as men. There, in 1822 or 1823, Eug\u00e8ne met Prosper M\u00e9rim\u00e9e, a writer who would play a decisive role in his career.\nIn 1825 he began his education at the Pension Moran, in Fontenay-aux-Roses. He returned to Paris in 1829 as a student at the college de Bourbon (now the Lyc\u00e9e Condorcet). He passed his baccalaureate examination in 1830. His uncle urged him to enter the \u00c9cole des Beaux-Arts, which had been created in 1806, but the \"\u00c9cole\" had an extremely rigid system, based entirely on copying classical models, and Eug\u00e8ne was not interested. Instead he decided to get practical experience in the architectural offices of Jacques-Marie Huv\u00e9 and Achille Lecl\u00e8re, while devoting much of his time to drawing medieval churches and monuments around Paris.\nAt sixteen he participated in the July 1830 revolution which overthrew Charles X, building a barricade. Following the revolution, which brought Louis Philippe I to power, his father became chief of the bureau of royal residences. The new government created, for the first time, the position of Inspector General of Historic Monuments. Eug\u00e8ne's uncle Del\u00e9cluze agreed to take Eug\u00e8ne on a long tour of France to see monuments. They travelled from July to October 1831 throughout the south of France, and he returned with a large collection of detailed paintings and watercolours of churches and monuments.\nOn his return to Paris, he moved with his family into the Tuileries Palace, where his father was now governor of royal residences. His family again urged him to attend the \u00c9cole des Beaux-Arts, but he still refused. He wrote in his journal in December 1831, \"the \"\u00c9cole\" is just a mould for architects. they all come out practically identical.\" He was a talented and meticulous artist; he travelled around France to visit monuments, cathedrals, and other medieval architecture, made detailed drawings and watercolours. In 1834, at the age of twenty, he married \u00c9lisabeth Templier, and in the same year he was named an associate professor of ornamental decoration at the Royal School of Decorative Arts, which gave him a more regular income. His first pupils there included L\u00e9on Gaucherel.\nWith the money from the sale of his drawings and paintings, Viollet-le-Duc set off on a long tour of the monuments of Italy, visiting Rome, Venice, Florence and other sites, drawing and painting. In 1838, he presented several of his drawings at the Paris Salon, and began making a travel book, \"Picturesque and romantic images of the old France\", for which, between 1838 and 1844, he made nearly three hundred engravings.\nFirst architectural restorations.\nIn October 1838, with the recommendation of Achille Lecl\u00e8re, the architect with whom he had trained, he was named deputy inspector of the enlargement of the H\u00f4tel Soubise, the new home of the French National Archives. His uncle, Del\u00e9cluze, then recommended him to the new Commission of Historic Monuments of France, led by Prosper M\u00e9rim\u00e9e, who had just published a book on medieval French monuments. Though he was just twenty-four years old and had no degree in architecture, he was asked to go to Narbonne to propose a plan for the completion of the cathedral there. The project was rejected by the local authorities as too ambitious and too expensive.\nHis first real project was a restoration of the V\u00e9zelay Abbey, which many considered as impossible. The church had been sacked by the Huguenots in 1569, and during the French Revolution, the facade and statuary on the facade were destroyed. The vaults of the roof were weakened, and many of the stones had been carried off for other projects. When M\u00e9rim\u00e9e visited to inspect the structure he heard stones falling around him. In February 1840 he gave Viollet-le-Duc the mission of restoring and reconstructing the church so it would not collapse, while \"respecting exactly in his project of restoration all the ancient dispositions of the church\".\nThe task was all the more difficult because up until that time no scientific studies had been made of medieval building techniques, and there were no schools of restoration. He had no plans for the original building to work from. Viollet-le-Duc had to discover the flaws of construction that had caused the building to start to collapse in the first place and to construct a more solid and stable structure. He lightened the roof and built new arches to stabilize the structure, and slightly changed the shape of the vaults and arches. He was criticized for these modifications in the 1960s, though, as his defenders pointed out, without them the roof would have collapsed under its own weight. M\u00e9rim\u00e9e's deputy, Lenormant, inspected the construction and reported to M\u00e9rim\u00e9e: \"The young Leduc seems entirely worthy of your confidence. He needed a magnificent audacity to take charge of such a desperate enterprise; it's certain that he arrived just in time, and if we had waited only ten years the church would have been a pile of stones.\" This restoration work lasted 19 years.\nSainte-Chapelle and Amboise.\nViollet-le-Duc's success at Vezelay led to a large series of projects. In 1840, in collaboration with his friend the architect Jean-Baptiste Lassus he began the restoration of Sainte-Chapelle in Paris, which had been turned into a storage depot after the Revolution. In February 1843, King Louis Philippe sent him to the Ch\u00e2teau of Amboise, to restore the stained glass windows in the chapel holding the tomb of Leonardo da Vinci. The windows were unfortunately destroyed in 1940 during World War\u00a0II.\nIn 1843, M\u00e9rim\u00e9e took Viollet-le-Duc with him to Burgundy and the south of France, on one of his long inspection tours of monuments. Viollet-le-Duc made drawings of the buildings and wrote detailed accounts of each site, illustrated with his drawing, which were published in architectural journals. With his experience he became the most prominent academic scholar on French medieval architecture and his medieval dictionnary, with over 4000 drawings, contains the largest iconography on the subject to this day.\nNotre-Dame de Paris.\nIn 1844, with the backing of M\u00e9rim\u00e9e, Viollet-le-Duc, just thirty years old, and Lassus, then thirty-seven, won a competition for the restoration of Notre-Dame Cathedral which lasted twenty-five years. Their project involved primarily the facade, where many of the statues over the portals had been beheaded or smashed during the Revolution. They proposed two major changes to the interior: rebuilding two of the bays to their original medieval height of four storeys, and removing the marble neoclassical structures and decoration which had been added to the choir during the reign of Louis XIV. M\u00e9rim\u00e9e warned them to be careful: \"In such a project, one cannot act with too much prudence or discretion...A restoration may be more disastrous for a monument than the ravages of centuries.\" The Commission on Historical Monuments approved most of Viollet-le-Duc's plans, but rejected his proposal to remove the choir built under Louis XIV. Viollet-le-Duc himself turned down a proposal to add two new spires atop the towers, arguing that such a monument \"would be remarkable but would not be Notre-Dame de Paris\". Instead, he proposed to rebuild the original medieval spire and bell tower over the transept, which had been removed in 1786 because it was unstable in the wind.\nOnce the project was approved, Viollet-le-Duc made drawings and photographs of the existing decorative elements; then they were removed and a stream of sculptors began making new statues of saints, gargoyles, chimeras and other architectural elements in a workshop he established, working from his drawings and photographs of similar works in other cathedrals of the same period. He also designed a new treasury in the Gothic style to serve as the museum of the cathedral, replacing the residence of the Archbishop, which had been destroyed in a riot in 1831.\nThe bells in the two towers had been taken out in 1791 and melted down to make cannons. Viollet-le-Duc had new bells cast for the north tower and a new structure built inside to support them. Viollet-le-Duc and Lassus also rebuilt the sacristy, on the south side of the church, which had been built in 1756, but had been burned by rioters during the July Revolution of 1830. The new spire was completed, taller and more strongly built to withstand the weather; it was decorated with statues of the apostles, and the face of Saint Thomas, patron saint of architects, bore a noticeable resemblance to Viollet-le-Duc. The spire was destroyed on 15 April 2019, as a result of the Notre-Dame de Paris fire.\nSaint-Denis and Amiens.\nWhen not engaged in Paris, Viollet-le-Duc continued his long tours into the French provinces, inspecting and checking the progress of more than twenty different restoration projects that were under his control, including seven in Burgundy alone. New projects included the Basilica of Saint-Sernin, Toulouse, and the Basilica of Saint-Denis just outside Paris. Saint-Denis had undergone a restoration by a different architect, Francois Debret, who had rebuilt one of the two towers. However, in 1846, the new tower, overloaded with masonry, began to crack, and Viollet-le-Duc was called in. He found no way the building could be saved and had to oversee the demolition of the tower, saving the stones. He concentrated on restoring the interior of the church, and was able to restore the original burial chamber of the kings of France.\nIn May 1849, he was named the architect for the restoration of Amiens Cathedral, one of the largest in France, which had been built over many centuries in a variety of different styles. He wrote, \"his goal should be to save in each part of the monument its own character, and yet to make it so that the united parts don't conflict with each other; and that can be maintained in a state that is durable and simple.\"\nImperial projects: Carcassonne, Vincennes and Pierrefonds.\nThe French coup d'\u00e9tat of 1851 brought Napoleon III to power and transformed France from a republic to an empire. The coup accelerated some of Viollet-le-Duc's projects as his patron Prosper M\u00e9rim\u00e9e had introduced him to the new Emperor. He moved forward with the slow work of restoration of the Cathedral of Reims and Cathedral of Amiens. In Amiens, he cleared the interior of the French classical decoration added under Louis XIV, and proposed to make it resolutely Gothic. He gave the Emperor a tour of his project in September 1853; the Empress immediately offered to pay two-thirds of the cost of the restoration. In the same year he undertook the restoration of the Ch\u00e2teau de Vincennes, long occupied by the military, along with its chapel, similar to Sainte-Chapelle. A devotee of the pure Gothic, he described the chapel as \"one of the finest specimens of Gothic in decline\".\nIn November 1853, he provided the costs and plans for the medieval ramparts of Carcassonne which he had first begun planning in 1849. The first fortifications had been built by the Visigoths; on top of these, in the Middle Ages Louis XI and then Philip the Bold had built a formidable series of towers, galleries, walls, gates and interlocking defences that resisted all sieges until 1355. The fortifications were largely intact, since the surroundings of the city were still a military defensive zone in the 19th century, but the towers were without tops and a large number of structures had been built up against the old walls. Once he obtained funding and made his plans, he began demolishing all structures which had been added to the ramparts over the centuries, and restored the gates, walls and towers to their original form, including the defence platforms, roofs on the towers and shelters for archers that would have been used during a siege. He found many of the original mountings for weapons still in place. To accompany his work, he published a detailed history of the city and its fortifications, with his drawings. Carcassonne became the best example of medieval military architecture in France, and also an important tourist attraction.\nNapoleon\u00a0III provided additional funding for the continued restoration of Notre-Dame. Viollet-le-Duc was also to replace the great bestiary of mythical beasts and animals which had decorated the cathedral in the 18th century. In 1856, using examples from other medieval churches and debris from Notre-Dame as his model, his workshop produced dragons, chimeras, grotesques, and gargoyles, as well as an assortment of picturesque pinnacles and fleurons. He engaged in a new project for restoration of the Cathedral of Clermont-Ferrand, a project which continued for ten years. He also undertook an unusual project for Napoleon\u00a0III; the design and construction of six railway coaches with neo-Gothic interior d\u00e9cor for the Emperor and his entourage. Two of the cars still exist; the salon of honour car, with a fresco on the ceiling, is at the Ch\u00e2teau de Compi\u00e8gne, and the dining car, with a massive golden eagle as the centrepiece of the d\u00e9cor, is at the Railroad Museum of Mulhouse.\nNapoleon III asked Viollet-le-Duc if he could restore a medieval chateau for the Emperor's own use near Compi\u00e8gne, where the Emperor traditionally passed September and October. Viollet-le-Duc first studied a restoration of the Ch\u00e2teau de Coucy, which had the highest medieval tower in France. When this proved too complicated, he settled upon Ch\u00e2teau de Pierrefonds, a castle begun by Louis of Orleans in 1396, then dismantled in 1617 after several sieges by Louis XIII. Napoleon bought the ruin for 5000 francs in 1812, and M\u00e9rim\u00e9e declared it an historic monument in 1848. In 1857 Viollet-le-Duc began designing an entirely new chateau on the ruins. This structure was not designed to recreate anything exactly that had existed, but a castle which recaptured the spirit of the Gothic, with lavish neo-Gothic decoration and 19th-century comforts. Pierrefonds and its inside decorations would not only influence William Burges and his Cardiff and Coch castles but also the castles of Ludwig II of Bavaria (Neuschwanstein Castle) and the Haut-K\u0153nigsbourg of the Emperor Wilhelm II.\nWhile most of his attention was devoted to restorations, Viollet-le-Duc designed and built a number of private residences and new buildings in Paris. He also participated in the most important competition of the period, for the new Paris Opera. There were one hundred seventy-one projects proposed in the original competition, presented the 1855 Paris Universal Exposition. A jury of noted architects narrowed it down to five, including projects from Viollet-le-Duc and Charles Garnier, age thirty-five. Viollet-le-Duc was finally eliminated and this put an end to Viollet le Duc's wish to construct public buildings. Napoleon III also called upon Viollet-le-Duc for a wide variety of archeological and architectural tasks. When he wished to put up a monument to mark the Battle of Alesia, where Julius Caesar defeated the Gauls, a siege whose actual site was disputed by historians, he asked Viollet-le-Duc to locate the exact battlefield. Viollet-le-Duc conducted excavations at various purported sites, and finally found vestiges of the walls built at the time. He also designed the metal frame for the six-metre-high statue of the Gallic chief Vercingetorix that would be placed on the site. He later designed a similar frame for a much larger statue, the Statue of Liberty, but died before that statue was finished.\nEnd of the Empire and of Restoration.\nIn 1863, Viollet-le-Duc was named a professor at the \u00c9cole des Beaux-Arts, the school where he had refused to become a student. In the fortress of neoclassical Beaux-Arts architecture there was much resistance against him, but he attracted two hundred students to his course, who applauded his lecture at the end. But while he had many supporters, the faculty professors and certain students campaigned against him. His critics complained that, aside from having little formal architectural training himself, he had only built a handful of new buildings. He tired of the confrontations and resigned on 16 May 1863, and continued his writing and teaching outside the Beaux-Arts. In response to the Beaux-Arts he initiated the creation of the \u00c9cole Sp\u00e9ciale d'Architecture in Paris in 1865.\nIn the beginning of 1864, he celebrated the conclusion of his most important project, the restoration of Notre-Dame. In January of the same year he completed the first phase of the restoration of the Cathedral of Saint Sernin in Toulouse, one of the landmarks of French Romanesque architecture. Napoleon\u00a0III invited Viollet-le-Duc to study possible restorations overseas, including in Algeria, Corsica, and in Mexico, where Napoleon had installed a new Emperor, Maximilian, under French sponsorship. He also saw the consecration of the third church that he had designed, the neo-Gothic church of Saint-Denis de l'Estree, in the Paris suburb of Saint-Denis. Between 1866 and 1870, his major project was the ongoing transformation of Pierrefonds from a ruin into a royal residence. His plans for the metal framework he had designed for Pierrefonds were displayed at the Paris Universal Exposition of 1867. He also began a new area of study, researching the geology and geography of the region around Mont Blanc in the Alps. While on his mapping excursion in the Alps in July 1870, he learned that war had been declared between Prussia and France.\nAs the Franco-Prussian War commenced, Viollet-le-Duc hurried back to Paris, and offered his services as a military engineer; he was put into service as a colonel of engineers, preparing the defenses of Paris. In September, the Emperor was captured at the Battle of Sedan, a new Republican government took power, and the Empress Eug\u00e9nie fled into exile, as Germans marched as far as Paris and put it under siege. At the same time, on September 23, Viollet-le-Duc's primary patron and supporter, Prosper M\u00e9rim\u00e9e, died peacefully in the south of France. Viollet-le-Duc supervised the construction of new defensive works outside Paris. The war was a disaster as he wrote in his journal on 14 December 1870: \"Disorganization is everywhere. The officers have no confidence in the troops, and the troops have no confidence in the officers. Each day, new orders and new projects which contravene those of the day before.\" He fought with the French army against the Germans at Buzenval on 24 January 1871. The battle was lost, and the French capitulated on 28 January. Viollet-le-Duc wrote to his wife on February 28, \"I don't know what will become of me, but I do not want to return any more to administration. I am disgusted by it forever, and want nothing more than to pass the years that remain to me in study and in the most modest possible life.\" Always the scholar, he wrote a detailed study of the effectiveness and deficiencies of the fortifications of Paris during the siege, which was to be used for the 1917 defense of Verdun and the construction of the Maginot line in 1938.\nIn May 1871 he left his home in Paris just before national guardsmen arrived to draft him into the armed force of the Paris Commune who subsequently condemned him to death. He escaped to Pierrefonds, where he had a small apartment before going in exile in Lausanne, where he engaged in his passion for mountains, making detailed maps and a series of thirty-two drawings of the alpine scenery. While in Lausanne he was also asked to undertake the restoration of the cathedral.\nHe returned later to Paris after the Commune had been suppressed and saw the ruins of most of the public buildings of the city, burned by the Commune in its last days. He received his only commission from the new government of the French Third Republic; Jules Simon, the new Minister of Culture and Public Instruction, asked him to design a plaque to be placed before Notre-Dame to honor the hostages killed by the Paris Commune in its final days.\nThe new government of the French Third Republic made little use of his expertise in the restoration of the major government buildings which had been burned by the Paris Commune, including the Tuileries Palace, the Palace of the Legion of Honor, the Palais-Royal, the library of the Louvre, the Ministry of Justice and the Ministry of Finance. The only reconstruction on which he was consulted was that of the Hotel de Ville. The writer Edmond de Goncourt called for leaving the ruin of the Hotel de Ville exactly as it was, \"a ruin of a magical palace, A marvel of the picturesque. The country should not condemn it without appeal to restoration by Viollet-le-Duc.\" The government asked Viollet-le-Duc to organize a competition. He presented two options; to either restore the building to its original state, with its historic interior; or to demolish it and build a new city hall. In July 1872 the government decided to preserve the Renaissance facade, but otherwise to completely demolish and rebuild the building.\nAuthor and theorist.\nThroughout his life Viollet le Duc wrote over 100 publications on architecture, decoration, history, archeology etc... some of which would become international best-sellers: \"Dictionary of French Architecture from 11th to 16th Century\" (1854\u20131868), \"Entretiens sur l'architecture\" (1863\u20131872), \"L'histoire d'une Maison\" (1873) and \"Histoire d'un Dessinateur: Comment on Apprend \u00e0 Dessiner\" (1879).\nIn his \"Entretiens sur l'architecture\" he concentrated in particular on the use of iron and other new materials, and the importance of designing buildings whose architecture was adapted to their function, rather than to a particular style. The book was translated into English in 1881 and won a large following in the United States. The Chicago architect Louis Sullivan, one of the inventors of the skyscraper, often invoked the phrase, \"Form follows function.\"\nLausanne Cathedral was his final major restoration project; it was rebuilt following his plans between 1873 and 1876. Work continued after his death. His reconstruction of the bell tower was later criticized; he eliminated the original octagonal base and added a new spire, which rested on the walls, and not on the vaulting, like the original spire. He also added new decoration, crowning the spire at mid-height with gables, another original element, and removing the original tiles. He was also criticized for the materials and ornaments he added to the towers, including gargoyles. His structural design was preserved, but in 1925 his gargoyles and original ornamentation were removed, and the spire was recovered with tiles.\nHis reputation had reached outside of France. The spire and roof of Strasbourg Cathedral had been damaged by German artillery during the Franco-Prussian War, and the city was now part of Germany. The German government invited Viollet-le-Duc to comment on their plans for the restoration, which involved a more grandiose Romanesque tower. Viollet-le-Duc informed the German architect that the planned new tower was completely out of character with the original facade and style of the cathedral. His advice was accepted, and the church was restored to its original form.\nIn 1872 Viollet-le-Duc was engaged in the reconstruction of the Ch\u00e2teau d'Amboise, owned by the descendants of the former King, Louis-Philippe. The chateau had been confiscated by Napoleon\u00a0III in 1848 but was returned to the family in 1872. It was a massive project to turn it into a residence, involving at times three hundred workers. Viollet-le-Duc designed all the work to the finest details, including the floor tiles, the gas lights in the salons, the ovens in the kitchen, and the electric bells for summoning servants.\nIn 1874 Viollet-le-Duc resigned as diocesan architect of Paris and was succeeded by his contemporary, Paul Abadie. In his final years, he continued to supervise the restoration projects that were underway for the Commission of Historical Monuments. He engaged in polemics about architecture in the press, and was elected to the Paris municipal council.\nStatue of Liberty.\nWhile planning the design and construction of the Statue of Liberty (\"Liberty Enlightening the World\") sculptor Fr\u00e9d\u00e9ric Auguste Bartholdi interested Viollet-le-Duc, his friend and mentor, in the project. As chief engineer, Viollet-le-Duc designed a brick pier within the statue, to which the skin would be anchored. After consultations with the metalwork foundry Gaget, Gauthier &amp; Co., Viollet-le-Duc chose the metal which would be used for the skin, copper sheets, and the method used to shape it, repouss\u00e9, in which the sheets were heated and then struck with wooden hammers. An advantage of this choice was that the entire statue would be light for its volume, as the copper need be only thick.\nNational Museum of French Monuments and final years.\nHe became engaged in the planning and construction of the Paris Universal Exposition of 1878. He proposed to the Minister of Education, Jules Ferry, that the Trocad\u00e9ro Palace, the main building of the Exposition on the hilltop of Chaillot, be transformed after the Exposition into a museum of French monuments, displaying models of architecture and sculpture from landmarks around France. This idea was accepted. The National Museum of French Monuments opened in 1882, after his death. The Palais was reconstructed into the Palais de Chaillot in 1937, but the Museum of French Monuments was preserved and can be seen there today.\nIn his final years his son Eug\u00e8ne-Louis became the head of the Commission of Historic Monuments. He took on just one new project, the restoration of the cloister of the Augustines at Toulouse. He completed his series of dictionaries of architectural periods, designed for a general audience. He also devoted more time to studying the geography of the Alps around Mont-Blanc. He spent his summers hiking in the mountains and writing articles about his travels. He launched a public campaign for the re-forestation of the Alps, and published a detailed map of the area in 1876. He spent more and more time at \"La Vedette\", the villa he constructed in Lausanne, a house on the model of a Savoyard chalet, but with a minimum of decoration, illustrating his new doctrine of form following function. He made one last visit to inspect Carcassonne, whose work was now under his son's direction. After an exhausting summer of hiking in the Alps in 1879, he became ill and died in Lausanne on 17 September 1879. He was buried in the cemetery of La Sallaz in Lausanne. In 1946 his grave and monument were transferred to the Cemetery of Bois-le-Vaux (Section XVIII) in Lausanne.\nFamily.\nViollet-le-Duc married Elisabeth Tempier in Paris on 3 May 1834. The couple had two children, but separated a few years after marriage, and spent little time together; he was continually on the road. The writer Genevi\u00e8ve Viollet-le-Duc (winner of the prix Broquette-Gonin in 1978) was his great-granddaughter.\nDoctrine.\nViollet-le-Duc famously defined restoration in volume eight of his \"Dictionnaire raisonn\u00e9 de l'architecture fran\u00e7aise du XI au XVI siecle\" of 1858: \"To restore a building is not to maintain it, repair it or remake it: it is to re-establish it in a complete state which may never have existed at any given moment.\" He then explained that it had to meet four conditions: (1) The \"re-establishment\" had to be scientifically documented with plans and photographs and archeological records, which would guarantee exactness. (2) The restoration had to involve not just the appearance of the monument, or the effect that it produced, but also its structure; it had to use the most efficient means to assure the long life of the building, including using more solid materials, used more wisely. (3) the restoration had to exclude any modification contrary to obvious evidence; but the structure could be adapted to conform to more modern or rational uses and practices, which meant alterations to the original plan; and (4) The restoration should preserve older modifications made to the building, with the exception of those which compromised its stability or its conservation, or those which gravely violated the value of its historical presence.\nHe drew conclusions from medieval architecture that he applied to modern architecture. He noted that it was sometimes necessary to employ an iron frame in restoration to avoid the danger of fires, as long as the new structure was not heavier than the original, and kept the original balance of forces found in medieval structures. \"The monuments of the Middle Ages were carefully calculated, and their organism is delicate. There is nothing in excess in their works, nothing useless. If you change one of the conditions of these organisms, you change all the others. Many people consider this a fault; for us, this is a quality which we too often neglect in our modern construction...Why should we build expensive walls two meters thick, if walls fifty centimeters thick [with reinforced supports], offer sufficient stability? In the structure of the Middle Ages, every portion of a work fulfilled a function and possessed an action.\"\nGothic vs. Beaux-Arts.\nDuring the entire career of Viollet-le-Duc, he was engaged in a dispute with the doctrines of the \u00c9cole des Beaux-Arts, the leading architectural school of France, which he refused to attend as a student, and where he taught briefly as a professor, before being pressured to depart. In 1846 he engaged in a fervent exchange in print with Quatrem\u00e8re de Quincy, the Perpetual Secretary of the French Academy, on the question, \"Is it suitable, in the 19th century, to build churches in the Gothic style?\" De Quincy and his followers denounced the Gothic style as incoherent, disorderly, unintelligent, decadent and without taste. Viollet-le-Duc responded, \"What we want, \"messieurs\", is the return of an art which was born in our country...Leave to Rome what belongs to Rome, and to Athens what belongs to Athens. Rome didn't want our Gothic (and was perhaps the only one in Europe to reject it) and they were right, because when one has the good fortune to possess a national architecture, the best thing is to keep it.\"\n\"If you study for a moment a church of the 13th century\", he wrote, \"you see that all of the construction is carried out according to an invariable system. All the forces and the weights are thrust out to the exterior, a disposition which gives the interior the greatest open space possible. The flying buttresses and contreforts alone support the entire structure, and always have an aspect of resistance, of force and stability which reassures the eye and the spirit; The vaults, built with materials that are easy to mount and to place at a great height, are combined in an easy disposition that places the totality of their weight on the piles; that the most simple means are always employed...and that all the parts of these constructions, independent of each other, even as they rely on each other, present an elasticity and a lightness needed in a building of such great dimensions. We can still see (and this is only found in Gothic architecture) that human proportions are the one fixed rule.\"\nControversy.\nViollet-le-Duc was often accused by certain critics, in his own time and later, of pursuing the spirit of the Gothic style in some of his restorations instead of strict historical accuracy. Many art historians also consider that the British architectural writer John Ruskin and William Morris were ferocious opponents of Viollet le Duc's restorations. But Ruskin never criticised Viollet le Duc's restoration work in itself, but criticised the principle of restoration itself. Indeed, at the beginning of his career Ruskin had a very radical opinion on restoration: \"a building should be looked after and if not it should be left to die\". Viollet le Duc's position on the subject was more nuanced: \"if a building has not been upkept it should be restored\".\nThe existence of an opposition between Ruskin and Viollet le Duc on restoration is today questioned by new research based on Ruskin's own writings: \"there is no book on architecture which has everything correct apart from Viollet le Duc's Dictionary\". And at the end of his life Ruskin expressed the regret that \"no one in England had done the work that Viollet le Duc had done in France\".\nViollet-le-Duc's restorations sometimes involved non-historical additions, either to assure the stability of the building, or sometimes simply to maintain the harmony of the design. The fl\u00e8che or spire of Notre-Dame de Paris, which had been constructed in about 1250, was removed in 1786 after it was damaged by the wind. Viollet-le-Duc designed and constructed a new spire, ornamented with statuary, which was taller than the original and modified to resist the weather, but in harmony with the rest of the design. In the 19th and 20th century, his fl\u00e8che was a target for critics.\nHe was also criticized later for his modifications of the choir of Notre-Dame, which had been rebuilt in the Louis XIV style during the reign of that king. Viollet-le-Duc took out the old choir, including the altar where Napoleon Bonaparte had been crowned Emperor and replaced them with a Gothic altar and decoration which he designed. When he modified the choir, he also constructed new bays with small Gothic rose windows modelled on those in the church of Chars, in the Oise Valley. Some historians condemned these restorations as non-historical invention. His defenders pointed out that Viollet-le-Duc did not make any decisions on the restoration of Notre-Dame by himself; all of his plans were approved by Prosper M\u00e9rim\u00e9e, the Inspector of Historical Monuments, and by the Commission of historic monuments.\nHe was criticized for the abundance of Gothic gargoyles, chimeras, fleurons, and pinnacles which he added to Notre-Dame Cathedral. These decorations had existed in the Middle Ages but had largely been removed during the reign of Louis XIV. The last original gargoyles had been taken down in 1813. He modelled the new gargoyles and monsters on examples from other cathedrals of the period.\nHe was later criticized also for the stained glass windows he designed and had made for the chapels around the ground level of the cathedral, which feature intricate Gothic designs in grisaille, which allow more light into the church. The contemporary view of the controversy of his restoration is summarized on a descriptive panel near the altar of the cathedral: \"The great restoration, carried to fruition by Viollet-le-Duc following the death of Lassus, supplied new radiance to the cathedral \u2013 whatever reservations one might have about the choices that were made. The work of the nineteenth century is now as much a part of the architectural history of Notre-Dame as that undertaken in previous centuries.\"\nThe restoration of ramparts of Carcassonne was also criticized in the 20th century. His critics pointed out that the pointed caps of the towers he constructed were more typical of northern France, not the region where Carcassonne was located, near the Spanish border. Similarly he added roofs of northern slate tiles rather than southern clay tiles, a choice that has been reversed in more recent restorations. His critics also claimed that Viollet-le-Duc sought a \"condition of completeness\" which never actually existed at any given time.\nThe principal counter-argument made by Viollet-le-Duc's defenders was that, without his prompt restorations, many of the buildings that he restored would have been lost, and that he did the best that he could with the knowledge that was then available.\nMortimer Wheeler's entry on English archaeologist Charles R Peers for the Dictionary of National Biography (1971) is worth quoting for its critique of Viollet-le-Duc:\n\"he [Peers] laid down the principles which have governed architectural conservation in the United Kingdom and have served as a model in other parts of the world. His cardinal principle was to retain but not to restore the surviving remains of an ancient structure; and in this respect he departed emphatically from the tradition of Viollet-le-Duc and his successors in France and Italy, where exuberant restoration frequently obscured the evidence upon which it was based ...\"\nPublications.\nThroughout his career Viollet-le-Duc made notes and drawings, not only for the buildings he was working on but also on Romanesque, Gothic and Renaissance buildings that were to be soon demolished. His notes were useful when preparing his published works. His study of medieval and Renaissance periods was not limited to architecture but extended also to such areas as furniture, clothing, musical instruments, armament, and geology.\nHis work was published, first in serial form, and then as full-scale books, as:\nArchitectural theory and new building projects.\nViollet-le-Duc is considered by many to be the first theorist of modern architecture. Sir John Summerson wrote that \"there have been two supremely eminent theorists in the history of European architecture \u2013 Leon Battista Alberti and Eug\u00e8ne Viollet-le-Duc.\"\nHis architectural theory was largely based on finding the ideal forms for specific materials and using these forms to create buildings. His writings centered on the idea that materials should be used \"honestly\". He believed that the outward appearance of a building should reflect the rational construction of the building. In \"Entretiens sur l'architecture\", Viollet-le-Duc praised the Greek temple for its rational representation of its construction. For him, \"Greek architecture served as a model for the correspondence of structure and appearance.\"\nAnother component in Viollet-le-Duc's theory was how the design of a building should start from its program and the plan, and end with its decorations. If this resulted in an asymmetrical exterior, so be it. He dismissed the symmetry of classicist buildings as vain, caring too much about appearances at the expense of practicality and convenience for the inhabitants of the house.\nIn several unbuilt projects for new buildings, Viollet-le-Duc applied the lessons he had derived from Gothic architecture, applying its rational structural systems to modern building materials such as cast iron. For inspiration, he also examined organic structures, such as leaves and animal skeletons. He was especially interested in the wings of bats, an influence represented by his Assembly Hall project.\nViollet-le-Duc's drawings of iron trusswork were innovative for the time. Many of his designs emphasizing iron would later influence the Art Nouveau movement, most noticeably in the work of Hector Guimard, Victor Horta, Antoni Gaud\u00ed and Hendrik Petrus Berlage. His writings inspired several American architects, including Frank Furness, John Wellborn Root, Louis Sullivan, and Frank Lloyd Wright.\nMilitary career and influence.\nViollet-le-Duc had a second career in the military, primarily in the defense of Paris during the Franco-Prussian War (1870\u201371). He was so influenced by the conflict that during his later years he described the idealized defense of France by the analogy of the military history of Le Roche-Pont, an imaginary castle, in his work \"Histoire d'une Forteresse\" (\"Annals of a Fortress\", twice translated into English). Accessible and well researched, it is partly fictional.\n\"Annals of a Fortress\" strongly influenced French military defensive thinking. Viollet-le-Duc's critique of the effect of artillery (applying his practical knowledge from the 1870\u20131871 war) is so complete that it accurately describes the principles applied to the defense of France until World War\u00a0II. The physical results of his theories are present in the fortification of Verdun prior to World War\u00a0I and the Maginot Line prior to World War\u00a0II. His theories are also represented by the French military theory of \"Deliberate Advance\", which stresses that artillery and a strong system of fortresses in the rear of an army are essential.\nLegacy.\nThe English architect Benjamin Bucknall (1833\u20131895) was a devotee of Viollet-le-Duc and during 1874 to 1881 translated several of his publications into English to popularise his principles in Great Britain. The later works of the English designer and architect William Burges were greatly influenced by Viollet-le-Duc, most strongly in Burges's designs for his own home, The Tower House in London's Holland Park district, and Burges's designs for Castell Coch near Cardiff, Wales.\nAn exhibition, \"Eug\u00e8ne Viollet-le-Duc 1814\u20131879\" was presented in Paris in 1965, and there was a larger, centennial exhibition in 1980.\nViollet-le-Duc was the subject of a Google Doodle on January 27, 2014.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9659", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=9659", "title": "Endocarditis", "text": "Inflammation of the heart's inner layer (endocardium)\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nEndocarditis is an inflammation of the inner layer of the heart, the endocardium. It usually involves the heart valves. Other structures that may be involved include the interventricular septum, the chordae tendineae, the mural endocardium, or the surfaces of intracardiac devices. Endocarditis is characterized by lesions, known as \"vegetations\", which are masses of platelets, fibrin, microcolonies of microorganisms, and scant inflammatory cells. In the subacute form of infective endocarditis, a vegetation may also include a center of granulomatous tissue, which may fibrose or calcify.\nThere are several ways to classify endocarditis. The simplest classification is based on cause: either \"infective\" or \"non-infective\", depending on whether a microorganism is the source of the inflammation or not. Regardless, the diagnosis of endocarditis is based on clinical features, investigations such as an echocardiogram, and blood cultures demonstrating the presence of endocarditis-causing microorganisms.\nSigns and symptoms include fever, chills, sweating, malaise, weakness, anorexia, weight loss, splenomegaly, flu-like feeling, cardiac murmur, heart failure, petechia (red spots on the skin), Osler's nodes (subcutaneous nodules found on hands and feet), Janeway lesions (nodular lesions on palms and soles), and Roth's spots (retinal hemorrhages).\nInfective endocarditis.\nInfective endocarditis is an infection of the inner surface of the heart, usually the valves. Symptoms may include fever, small areas of bleeding into the skin, heart murmur, feeling tired, and low red blood cells. Complications may include valvular insufficiency, heart failure, stroke, and kidney failure.\nThe cause is typically a bacterial infection and less commonly a fungal infection. Risk factors include valvular heart disease including rheumatic disease, congenital heart disease, artificial valves, hemodialysis, intravenous drug use, and electronic pacemakers. The bacteria most commonly involved are streptococci or staphylococci.\nThe diagnosis of infective endocarditis relies on the Duke criteria, which were originally described in 1994 and modified in 2000. Clinical features and microbiological examinations are the first steps to diagnose an infective endocarditis. The imaging is also crucial. Echocardiography is the cornerstone of imaging modality in the diagnosis of infective endocarditis. Alternative imaging modalities as computer tomography, magnetic resonance imaging, and positron emission tomography/computer tomography (PET/CT) with 2-[18F]fluorodeoxyglucose (FDG) are playing an increasing role in the diagnosis and management of infective endocarditis.\nThe usefulness of antibiotics following dental procedures has changed over time. Prevention is recommended in patients at high risk. Treatment is generally with intravenous antibiotics. The choice of antibiotics is based on the blood cultures. Occasionally heart surgery is required. Populations at high risk of infective endocarditis include patients with previous infective endocarditis, patients with surgical or transcatheter prosthetic valves or post-cardiac valve repair, and patients with untreated CHD and surgically corrected congenital heart disease.\nThe number of people affected is about 5 per 100,000 per year. Rates, however, vary between regions of the world. Males are affected more often than females. The risk of death among those infected is about 25%. Without treatment it is almost universally fatal.\nNon-infective endocarditis.\nNonbacterial thrombotic endocarditis (NBTE) is most commonly found on previously undamaged valves. As opposed to infective endocarditis, the vegetations in NBTE are small, sterile, and tend to aggregate along the edges of the valve or the cusps. Also unlike infective endocarditis, NBTE does not cause an inflammation response from the body. NBTE usually occurs during a hypercoagulable state such as system-wide bacterial infection, or pregnancy, though it is also sometimes seen in patients with venous catheters. NBTE may also occur in patients with cancer, particularly mucinous adenocarcinoma where Trousseau syndrome can be encountered. Typically NBTE does not cause many problems on its own, but parts of the vegetations may break off and embolize to the heart or brain, or they may serve as a focus where bacteria can lodge, thus causing infective endocarditis.\nAnother form of sterile endocarditis is termed Libman\u2013Sacks endocarditis; this form occurs more often in patients with lupus erythematosus and is thought to be due to the deposition of immune complexes. Like NBTE, Libman-Sacks endocarditis involves small vegetations, while infective endocarditis is composed of large vegetations. These immune complexes precipitate an inflammation reaction, which helps to differentiate it from NBTE. Also unlike NBTE, Libman-Sacks endocarditis does not seem to have a preferred location of deposition and may form on the undersurfaces of the valves or even on the endocardium.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9660", "revid": "49704342", "url": "https://en.wikipedia.org/wiki?curid=9660", "title": "Euler's sum of powers conjecture", "text": "Disproved conjecture in number theory \nIn number theory, Euler's conjecture is a disproved conjecture related to Fermat's Last Theorem. It was proposed by Leonhard Euler in 1769. It states that for all integers n and k greater than 1, if the sum of n many kth powers of positive integers is itself a kth power, then n is greater than or equal to k:\nformula_1\nThe conjecture represents an attempt to generalize Fermat's Last Theorem, which is the special case \"n\" \n 2: if formula_2 then 2 \u2265 \"k\".\nAlthough the conjecture holds for the case \"k\" \n 3 (which follows from Fermat's Last Theorem for the third powers), it was disproved for \"k\" \n 4 and \"k\" \n 5. It is unknown whether the conjecture fails or holds for any value \"k\" \u2265 6.\nBackground.\nEuler was aware of the equality 594 + 1584 \n 1334 + 1344 involving sums of four fourth powers; this, however, is not a counterexample because no term is isolated on one side of the equation. He also provided a complete solution to the four cubes problem as in Plato's number 33 + 43 + 53 \n 63 or the taxicab number 1729. The general solution of the equation formula_3\nis\nformula_4\nwhere a, b and formula_5 are any rational numbers.\nCounterexamples.\nEuler's conjecture was disproven by L. J. Lander and T. R. Parkin in 1966 when, through a direct computer search on a CDC 6600, they found a counterexample for \"k\" \n 5. This was published in a paper comprising just two sentences. A total of three primitive (that is, in which the summands do not all have a common factor) counterexamples are known:\nformula_6\n(Lander &amp; Parkin, 1966); (Scher &amp; Seidl, 1996); (Frye, 2004).\nIn 1988, Noam Elkies published a method to construct an infinite sequence of counterexamples for the \"k\" \n 4 case. His smallest counterexample was\nformula_7\nA particular case of Elkies' solutions can be reduced to the identity\nformula_8\nwhere\nformula_9\nThis is an elliptic curve with a rational point at \"v\"1 \n \u2212. From this initial rational point, one can compute an infinite collection of others. Substituting \"v\"1 into the identity and removing common factors gives the numerical example cited above.\nIn 1988, Roger Frye found the smallest possible counterexample \nformula_10\nfor \"k\" \n 4 by a direct computer search using techniques suggested by Elkies. This solution is the only one with values of the variables below 1,000,000.\nGeneralizations.\nIn 1967, L. J. Lander, T. R. Parkin, and John Selfridge conjectured that if \nformula_11,\nwhere \"ai\" \u2260 \"bj\" are positive integers for all 1 \u2264 \"i\" \u2264 \"n\" and 1 \u2264 \"j\" \u2264 \"m\", then \"m\" + \"n\" \u2265 \"k\". In the special case \"m\" \n 1, the conjecture states that if\nformula_12\n(under the conditions given above) then \"n\" \u2265 \"k\" \u2212 1.\nThe special case may be described as the problem of giving a partition of a perfect power into few like powers. For \"k\" \n 4, 5, 7, 8 and \"n\" \n \"k\" or \"k\" \u2212 1, there are many known solutions. Some of these are listed below. \nSee OEIS:\u00a0 for more data.\n===\"k\" \n 3===\nformula_13 (Plato's number 216)\nThis is the case \"a\" = 1, \"b\" = 0 of Srinivasa Ramanujan's formula \nformula_14\nA cube as the sum of three cubes can also be parameterized in one of two ways:\nformula_15\nThe number 2,100,0003 can be expressed as the sum of three positive cubes in nine different ways.\n===\"k\" \n 4===\nformula_16\n(R. Frye, 1988); (R. Norrie, smallest, 1911).\n===\"k\" \n 5===\nformula_17\n(Lander &amp; Parkin, 1966); (Lander, Parkin, Selfridge, smallest, 1967); (Lander, Parkin, Selfridge, second smallest, 1967); (Sastry, 1934, third smallest).\n===\"k\" \n 6===\nIt has been known since 2002 that there are no solutions for \"k\" = 6 whose final term is \u2264 730000.\n===\"k\" \n 7===\nformula_18\n(M. Dodrill, 1999).\n===\"k\" \n 8===\nformula_19\n(S. Chase, 2000).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "9662", "revid": "24465790", "url": "https://en.wikipedia.org/wiki?curid=9662", "title": "Book of Exodus", "text": "Second book of the Bible\nThe Book of Exodus (from ; \"\u0160\u0259m\u014d\u1e6f\", 'Names'; ) is the second book of the Bible. \nThe book is the first part of the narrative of the Exodus, the origin myth of the Israelites, in which they leave slavery in biblical Egypt through the strength of God, who chose them as his people. The Israelites then journey with the prophet Moses to Mount Sinai, where God gives them the Law of Moses and enters into a covenant with them and their descendants. God promises to make them a \"holy nation, and a kingdom of priests\" on condition of their faithfulness. He gives them laws and instructions to build the Tabernacle, the means by which he will come from heaven and dwell with them and lead them in a holy war to conquer Canaan (the \"Promised Land\"), which had earlier, according to the Book of Genesis, been promised to the \"seed\" of Abraham, the patriarch of the Israelites.\nTraditionally ascribed to Moses himself, the formation of the Pentateuch, including the Book of Exodus, is dated to the fifth through fourth centuries BC. Modern scholars see its initial composition as a product of the Babylonian exile (6th century BCE), based on earlier written sources and oral traditions, with final revisions in the Persian post-exilic period (5th century BCE). American biblical scholar Carol Meyers, in her commentary on Exodus, suggests that it is arguably the most important book in the Bible, as it presents the defining features of Israel's identity\u2014memories of a past marked by hardship and escape, a binding covenant with their God, who chooses Israel, and the establishment of the life of the community and the guidelines for sustaining it. \nThe consensus of modern scholars is that the Pentateuch does not give an accurate account of the origins of the Israelites, who appear instead to have formed as an entity in the central highlands of Canaan in the late second millennium BCE (around the time of the Late Bronze Age collapse) from the indigenous Canaanite culture.\nTitle.\nThe English name \"Exodus\" comes from the , from and . In Hebrew the book's title is \u05e9\u05b0\u05c1\u05de\u05d5\u05b9\u05ea, \"shem\u014dt\", \"Names\", from the beginning words of the text: \"These are the names of the sons of Israel\" ().\nHistoricity.\nMost mainstream scholars do not accept the biblical Exodus account as historical for a number of reasons. It is generally agreed that the Exodus stories were written centuries after the apparent setting of the stories. Archaeologists Israel Finkelstein and Neil Asher Silberman argue that archaeology has not found evidence for even a small band of wandering Israelites living in the Sinai: \"The conclusion \u2013 that Exodus did not happen at the time and in the manner described in the Bible \u2013 seems irrefutable [...] repeated excavations and surveys throughout the entire area have not provided even the slightest evidence\". Instead, they argue how modern archaeology suggests continuity between Canaanite and Israelite settlements, indicating a heavily Canaanite origin for Israel, with little suggestion that a group of foreigners from Egypt comprised early Israel. They also argue that the exodus narrative perhaps evolved from vague memories of the Hyksos expulsion, spun to encourage resistance to the 7th century domination of Judah by Egypt.\nHowever, a majority of scholars also believe that the story has a historical core, despite disagreeing widely about what that historical kernel might have been. Kenton Sparks refers to it as \"charter myth\" and \"mythologized history\". Biblical scholar Graham I. Davies notes that several literary texts from Ancient Egypt document the presence of Semitic peoples working for building projects under the 19th Dynasty of Egypt, suggesting a possible historical basis for the account of Israelite servitude to the Egyptians. Movements of small groups of Ancient Semitic-speaking peoples into and out of Egypt during the Eighteenth and Nineteenth Dynasties have been documented. The book of Exodus also has elements of Egyptian folklore and culture contained within its narrative, and the names Moses, Aaron and Phinehas, seem to have an Egyptian origin. However, there is an increasing trend among scholars to see the biblical exodus traditions as the invention of the exilic and post-exilic Jewish community, with little to no historical basis.\nStructure.\nThere is no unanimous agreement among scholars on the structure of Exodus. One strong possibility is that it is a diptych (i.e., divided into two parts), with the division between parts 1 and 2 at the crossing of the Red Sea or at the beginning of the theophany (appearance of God) in chapter 19. On this plan, the first part tells of God's rescue of his people from Egypt and their journey under his care to Sinai (chapters 1\u201319) and the second tells of the covenant between them (chapters 20\u201340).\nSummary.\nThe text of the Book of Exodus begins after the events at the end of the Book of Genesis where Jacob's sons and their families joined their brother Joseph in Egypt, which Joseph had saved from famine. It is 400 years later and Egypt's new Pharaoh, who does not remember Joseph, is fearful that the enslaved and now numerous Israelites could become a fifth column. He hardens their labor and orders the killing of all newborn boys. A Levite woman named Jochebed saves her baby by setting him adrift on the Nile in an ark of bulrushes. Pharaoh's daughter finds the child, names him Moses, and brings him up as her own.\nLater, a grown Moses goes out to see his kinsmen. He witnesses the abuse of a Hebrew slave by an Egyptian overseer. Angered, Moses kills him and flees into Midian to escape punishment. There, he marries Zipporah, daughter of Jethro, a Midianite priest; meanwhile the Pharaoh dies, and another takes his place. While tending Jethro's flock, Moses encounters God in a burning bush. Moses asks God for his name, to which God replies with three words, often translated as \"I Am that I Am\". This is the book's explanation for the origin of the name Yahweh, as God is thereafter known. God tells Moses to return to Egypt, free the Hebrews from slavery and lead them into Canaan, the land promised to the seed of Abraham in Genesis. On the journey back to Egypt, God seeks to kill Moses. Zipporah circumcises their son and the attack stops. \"(See Zipporah at the inn.)\"\nMoses reunites with his brother Aaron and, returning to Egypt, convenes the Israelite elders, preparing them to go into the wilderness to worship God. Pharaoh refuses to release the Israelites from their work for the festival, and so God curses the Egyptians with ten terrible plagues, such as a , an outbreak of frogs, and the . Moses is commanded by God to fix the spring month of Aviv at the head of the Hebrew calendar. The Israelites are to take a lamb on the 10th day of the month, sacrifice the lamb on the 14th day, daub its blood on their mezuzot (doorposts) and lintels and to observe the Passover meal that night, during the full moon. The 10th plague comes that night, causing the death of all Egyptian firstborn sons and prompting Pharaoh to expel the Israelites. Regretting his decision, Pharaoh commands his chariot army after the Israelites, who appear trapped at the Red Sea. God parts the sea, allowing the Israelites to pass through, before drowning Pharaoh's pursuing forces.\nAs desert life proves arduous, the Israelites complain and long for Egypt, but God miraculously provides manna for them to eat and water to drink. The Israelites arrive at the mountain of God, where Moses's father-in-law Jethro visits Moses; at his suggestion, Moses appoints judges over Israel. God asks whether they will agree to be his people \u2013 they accept. The people gather at the foot of the mountain, and with thunder and lightning, fire and clouds of smoke, the sound of trumpets, and the trembling of the mountain, God appears on the peak, and the people see the cloud and hear the voice (or possibly sound) of God. God tells Moses to ascend the mountain. God pronounces the Ten Commandments (the Ethical Decalogue) in the hearing of all Israel. Moses goes up the mountain into the presence of God, who pronounces the Covenant Code of ritual and civil law and promises Canaan to them if they obey. Moses comes down from the mountain and writes down God's words, and the people agree to keep them. God calls Moses up the mountain again, where he remains for forty days and forty nights, after which he returns, bearing the set of stone tablets.\nGod gives Moses instructions for the construction of the tabernacle so that God may dwell permanently among his chosen people, along with instructions for the priestly vestments, the altar and its appurtenances, procedures for the ordination of priests, and the daily sacrifice offerings. Aaron becomes the first hereditary high priest. God gives Moses the two tablets of stone containing the words of the ten commandments, written with the \"finger of God\".\nWhile Moses is with God, Aaron casts a golden calf, which the people worship. God informs Moses of their apostasy and threatens to kill them all, but relents when Moses pleads for them. Moses comes down from the mountain, smashes the stone tablets in anger, and commands the Levites to massacre the unfaithful Israelites. God commands Moses to construct two new tablets. Moses ascends the mountain again, where God dictates the Ten Commandments for Moses to write on the tablets.\nMoses descends from the mountain with a transformed face; from that time onwards he must hide his face with a veil. Moses assembles the Hebrews and repeats to them the commandments he has received from God, which are to keep the Sabbath and to construct the Tabernacle. The Israelites do as they are commanded. From that time God dwells in the Tabernacle and orders the travels of the Hebrews.\nComposition.\nAuthorship.\nJewish and Christian tradition viewed Moses as the author of Exodus and the entire Torah, but by the end of the 19th century the increasing awareness of discrepancies, inconsistencies, repetitions and other features of the Pentateuch had led scholars to abandon this idea. The formation of the Pentateuch, including the Book of Exodus, is dated to the fifth through fourth centuries BC. In approximate round dates, the process which produced Exodus and the Pentateuch probably began around 600 BCE when existing oral and written traditions were brought together to form books recognizable as those we know, reaching their final form as unchangeable sacred texts around 400 BCE.\nSources.\nAlthough patent mythical elements are not so prominent in Exodus as in Genesis, ancient legends may have an influence on the book's form or content: for example, the story of the infant Moses's salvation from the Nile is argued to be based on an earlier legend of king Sargon of Akkad, while the story of the parting of the Red Sea may trade on Mesopotamian creation mythology. Similarly, the Covenant Code (the law code in Exodus 20:22\u201323:33) has some similarities in both content and structure with the Laws of Hammurabi. These potential influences serve to reinforce the conclusion that the Book of Exodus originated in the exiled Jewish community of 6th-century BCE Babylon, but not all the potential sources are Mesopotamian: the story of Moses's flight to Midian following the murder of the Egyptian overseer may draw on the Egyptian \"Story of Sinuhe\".\nThemes.\nSalvation.\nBiblical scholars describe the Bible's theologically motivated history writing as \"salvation history\", meaning a history of God's saving actions that give identity to Israel \u2013 the promise of offspring and land to the ancestors, the Exodus from Egypt (in which God saves Israel from slavery), the wilderness wandering, the revelation at Sinai, and the hope for the future life in the Promised Land.\nTheophany.\nA theophany is a manifestation (appearance) of a god \u2013 in the Bible, an appearance of the God of Israel, accompanied by storms \u2013 the earth trembles, the mountains quake, the heavens pour rain, thunder peals, and lightning flashes. The theophany in Exodus begins \"the third day\" from their arrival at Sinai in chapter 19: Yahweh and the people meet at the mountain, God appears in the storm and converses with Moses, giving him the Ten Commandments while the people listen. Therefore, theophany is a public experience of divine law.\nThe second half of Exodus marks the point at which, and describes the process through which, God's theophany becomes a permanent presence for Israel via the Tabernacle. That so much of the book (chapters 25\u201331, 35\u201340) describes the plans of the Tabernacle demonstrates the importance it played in the perception of Second Temple Judaism at the time of the text's redaction by the Priestly writers: the Tabernacle is the place where God is physically present, where, through the priesthood, Israel could be in direct, literal communion with him.\nCovenant.\nThe heart of Exodus is the Sinaitic covenant. A covenant is a legal document binding two parties to take on certain obligations towards each other. There are several covenants in the Bible, and in each case, they exhibit at least some of the elements in real-life treaties of the ancient Middle East: a preamble, historical prologue, stipulations, deposition and reading, list of witnesses, blessings, and curses, and ratification by animal sacrifice. Biblical covenants, in contrast to Eastern covenants in general, are between a god, Yahweh, and a people, Israel, instead of between a strong ruler and a weaker vassal.\nElection of Israel.\nGod elects Israel for salvation because the \"sons of Israel\" are \"the firstborn sons\" of the God of Israel, descended through Shem and Abraham to the chosen line of Jacob whose name is changed to Israel. The goal of the divine plan in Exodus is a return to humanity's state in Eden so that God can dwell with the Israelites as he had with Adam and Eve through the Ark and Tabernacle, which together form a model of the universe; in later Abrahamic religions Israel becomes the guardian of God's plan for humanity, to bring \"God's creation blessing to mankind\" begun in Adam.\nWeekly Torah portions.\nList of Torah portions in the Book of Exodus:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nGeneral bibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "9663", "revid": "3632083", "url": "https://en.wikipedia.org/wiki?curid=9663", "title": "Electronics", "text": "Branch of physics and electrical engineering\nElectronics is a scientific and engineering discipline that studies and applies the principles of physics to design, create, and operate devices that manipulate electrons and other electrically charged particles. It is a subfield of physics and electrical engineering which uses active devices such as transistors, diodes, and integrated circuits to control and amplify the flow of electric current and to convert it from one form to another, such as from alternating current (AC) to direct current (DC) or from analog signals to digital signals. Electronics is often contrasted with electrical power engineering, which focuses on generation, transmission, and distribution of electric power rather than signal processing or device level control.\nElectronic devices have significantly influenced the development of many aspects of modern society, such as telecommunications, entertainment, education, health care, industry, and security. The main driving force behind the advancement of electronics is the semiconductor industry, which continually produces ever-more sophisticated electronic devices and circuits in response to global demand. The semiconductor industry is one of the global economy's largest and most profitable industries, with annual revenues exceeding $481 billion in 2018. The electronics industry also encompasses other branches that rely on electronic devices and systems, such as e-commerce, which generated over $29 trillion in online sales in 2017. Practical electronic systems commonly combine analog and digital techniques, using analog front ends with digital processing.\nHistory and development.\nKarl Ferdinand Braun's development of the crystal detector, the first semiconductor device, in 1874 and the identification of the electron in 1897 by Sir Joseph John Thomson, along with the subsequent invention of the vacuum tube which could amplify and rectify small electrical signals, inaugurated the field of electronics and the electron age. Practical applications started with the invention of the diode by Ambrose Fleming and the triode by Lee De Forest in the early 1900s, which made the detection of small electrical voltages, such as radio signals from a radio antenna, practicable. Thermionic vacuum tubes enabled reliable amplification and detection, making long-distance telephony, broadcast radio, and early television feasible by 1920s-1930s.\nVacuum tubes (thermionic valves) were the first active electronic components which controlled current flow by influencing the flow of individual electrons, and enabled the construction of equipment that used current amplification and rectification to give us radio, television, radar, long-distance telephony and much more. The early growth of electronics was rapid, and by the 1920s, commercial radio broadcasting and telecommunications were becoming widespread and electronic amplifiers were being used in such diverse applications as long-distance telephony and the music recording industry.\nThe next big technological step took several decades to appear, when the first working point-contact transistor was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947. The 1947 point contact transistor showed that semiconductors could replace many tube functions with lower power and size.\nHowever, vacuum tubes continued to play a leading role in the field of microwave and high power transmission as well as television receivers until the middle of the 1980s.\nSince then, solid-state devices have all but completely taken over. Vacuum tubes are still used in some specialist applications such as high power RF amplifiers, cathode-ray tubes, specialist audio equipment, guitar amplifiers and some microwave devices.\nIn April 1955, the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the first all-transistorized calculator to be manufactured for the commercial market. The 608 contained more than 3,000 germanium transistors. Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design. From that time on transistors were almost exclusively used for computer logic circuits and peripheral devices. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.\nThe MOSFET was invented at Bell Labs between 1955 and 1960. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. The MOSFET became the most widely used device in VLSI, enabling compact, low power circuits. Its advantages include high scalability, affordability, low power consumption, and high density. The MOSFET's scalability and cost made it dominant in modern electronics. It revolutionized the electronics industry, becoming the most widely used electronic device in the world. The MOSFET is the basic element in most modern electronic equipment.\nAs the complexity of circuits grew, problems arose. One problem was the size of the circuit. A complex circuit like a computer was dependent on speed. If the components were large, the wires interconnecting them must be long. The electric signals took time to go through the circuit, thus slowing the computer. The invention of the integrated circuit by Jack Kilby and Robert Noyce solved this problem by making all the components and the chip out of the same block (monolith) of semiconductor material. The circuits could be made smaller, and the manufacturing process could be automated. This led to the idea of integrating all components on a single-crystal silicon wafer, which led to small-scale integration (SSI) in the early 1960s, and then medium-scale integration (MSI) in the late 1960s, followed by VLSI. In 2008, billion-transistor processors became commercially available. Integrated circuits put many components on one chip, shortening interconnects and increase speed.\nSubfields.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nDevices and components.\nAn electronic component is any component, either active or passive, in an electronic system or electronic device. Components are connected together, usually by being soldered to a printed circuit board (PCB), to create an electronic circuit with a particular function. Components may be packaged singly, or in more complex groups as integrated circuits. Passive electronic components are capacitors, inductors, resistors, whilst active components are such as semiconductor devices; transistors and thyristors, which control current flow at electron level.\nTypes of circuits.\nElectronic circuit functions can be divided into two function groups: analog and digital. A particular device may consist of circuitry that has either or a mix of the two types. Analog circuits are becoming less common, as many of their functions are being digitized.\nAnalog circuits.\nAnalog circuits use a continuous range of voltage or current for signal processing, as opposed to the discrete levels used in digital circuits. Analog circuits were common throughout an electronic device in the early years in devices such as radio receivers and transmitters. Analog electronic computers were valuable for solving problems with continuous variables until digital processing advanced.\nAs semiconductor technology developed, many of the functions of analog circuits were taken over by digital circuits, and modern circuits that are entirely analog are less common; their functions being replaced by hybrid approach which, for instance, uses analog circuits at the \"front end\" of a device receiving an analog signal, and then use digital processing using microprocessor techniques thereafter.\nSometimes it may be difficult to classify some circuits that have elements of both linear and non-linear operation. An example is the voltage comparator which receives a continuous range of voltage but only outputs one of two levels as in a digital circuit. Similarly, an overdriven transistor amplifier can take on the characteristics of a controlled switch, having essentially two levels of output.\nAnalog circuits are still widely used for signal amplification, such as in the entertainment industry, and conditioning signals from analog sensors, such as in industrial measurement and control.\nDigital circuits.\nDigital circuits are electric circuits based on discrete voltage levels. Digital circuits use Boolean algebra and are the basis of all digital computers and microprocessor devices. They range from simple logic gates to large integrated circuits, employing millions of such gates.\nDigital circuits use a binary system with two voltage levels labelled \"0\" and \"1\" to indicated logical status. Often logic \"0\" will be a lower voltage and referred to as \"Low\" while logic \"1\" is referred to as \"High\". However, some systems use the reverse definition (\"0\" is \"High\") or are current based. Quite often the logic designer may reverse these definitions from one circuit to the next as they see fit to facilitate their design. The definition of the levels as \"0\" or \"1\" is arbitrary.\nTernary (with three states) logic has been studied, and some prototype computers made, but have not gained any significant practical acceptance. Universally, Computers and Digital signal processors are constructed with digital circuits using Transistors such as MOSFETs in the electronic logic gates to generate binary states.\nHighly integrated devices:\nDesign.\nElectronic systems design deals with the multi-disciplinary design issues of complex electronic devices and systems, such as mobile phones and computers. The subject covers a broad spectrum, from the design and development of an electronic system (new product development) to assuring its proper function, service life and disposal. Electronic systems design is therefore the process of defining and developing complex electronic devices to satisfy specified requirements of the user.\nDue to the complex nature of electronics theory, laboratory experimentation is an important part of the development of electronic devices. These experiments are used to test or verify the engineer's design and detect errors. Historically, electronics labs have consisted of electronics devices and equipment located in a physical space, although in more recent years the trend has been towards electronics lab simulation software, such as CircuitLogix, Multisim, and PSpice.\nComputer-aided design.\nToday's electronics engineers have the ability to design circuits using premanufactured building blocks such as power supplies, semiconductors (i.e. semiconductor devices, such as transistors), and integrated circuits. Electronic design automation software programs include schematic capture programs and printed circuit board design programs. Popular names in the EDA software world are NI Multisim, Cadence (ORCAD), EAGLE PCB and Schematic, Mentor (PADS PCB and LOGIC Schematic), Altium (Protel), LabCentre Electronics (Proteus), gEDA, KiCad and many others.\nNegative qualities.\nThermal management.\nHeat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.\nNoise.\nElectronic noise is defined as unwanted disturbances superposed on a useful signal that tend to obscure its information content. Noise is not the same as signal distortion caused by a circuit. Noise is associated with all electronic circuits. Noise may be electromagnetically or thermally generated, which can be decreased by lowering the operating temperature of the circuit. Other types of noise, such as shot noise cannot be removed as they are due to limitations in physical properties.\nPackaging methods.\nMany different methods of connecting components have been used over the years. For instance, early electronics often used point to point wiring with components attached to wooden breadboards to construct circuits. Cordwood construction and wire wrap were other methods used. Most modern-day electronics now use printed circuit boards made of materials such as FR-4 and FR-2. Modern PCBs are usually FR-4 epoxy boards with through hole or surface mount parts. Electrical components are generally mounted to PCBs using through-hole or surface mount.\nHealth and environmental concerns associated with electronics assembly have gained increased attention in recent years.\nIndustry.\nThe electronics industry consists of various branches. The central driving force behind the entire electronics industry is the semiconductor industry, which has annual sales of over $ as of 2018. The largest industry sector is e-commerce, which generated over $ in 2017. The most widely manufactured electronic device is the metal-oxide-semiconductor field-effect transistor (MOSFET), with an estimated 13sextillion MOSFETs having been manufactured between 1960 and 2018. In the 1960s, U.S. manufacturers were unable to compete with Japanese companies such as Sony and Hitachi who could produce high-quality goods at lower prices. By the 1980s, however, U.S. manufacturers became the world leaders in semiconductor development and assembly.\nHowever, during the 1990s and subsequently, the industry shifted overwhelmingly to East Asia (a process begun with the initial movement of microchip mass-production there in the 1970s), as plentiful, cheap labor, and increasing technological sophistication, became widely available there.\nOver three decades, the United States' global share of semiconductor manufacturing capacity fell, from 37% in 1990, to 12% in 2022. America's pre-eminent semiconductor manufacturer, Intel Corporation, fell far behind its subcontractor Taiwan Semiconductor Manufacturing Company (TSMC) in manufacturing technology.\nBy that time, Taiwan had become the world's leading source of advanced semiconductors\u2014followed by South Korea, the United States, Japan, Singapore, and China.\nImportant semiconductor industry facilities (which often are subsidiaries of a leading producer based elsewhere) also exist in Europe (notably the Netherlands), Southeast Asia, South America, and Israel.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
