{"id": "5987", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5987", "title": "Coal", "text": "Combustible sedimentary rock composed primarily of carbon\nCoal is a combustible black or brownish-black sedimentary rock, formed as rock strata called coal seams. Coal is mostly carbon with variable amounts of other elements, chiefly hydrogen, sulfur, oxygen, and nitrogen. \nIt is a type of fossil fuel, formed when dead plant matter decays into peat which is converted into coal by the heat and pressure of deep burial over millions of years. Vast deposits of coal originate in former wetlands called coal forests that covered much of the Earth's tropical land areas during the late Carboniferous (Pennsylvanian) and Permian times.\nCoal is used primarily as a fuel. While coal has been known and used for thousands of years, its usage was limited until the Industrial Revolution. With the invention of the steam engine, coal consumption increased. In 2020, coal supplied about a quarter of the world's primary energy and over a third of its electricity. Some iron and steel-making and other industrial processes burn coal.\nThe extraction and burning of coal damages the environment and human health, causing premature death and illness, and it is the largest anthropogenic source of carbon dioxide contributing to climate change. Fourteen billion tonnes of carbon dioxide were emitted by burning coal in 2020, which is 40% of total fossil fuel emissions and over 25% of total global greenhouse gas emissions. As part of worldwide energy transition, many countries have reduced or eliminated their use of coal power. The United Nations Secretary General asked governments to stop building new coal plants by 2020.\nGlobal coal use was 8.3 billion tonnes in 2022, and is set to remain at record levels in 2023. To meet the Paris Agreement target of keeping global warming below coal use needs to halve from 2020 to 2030, and \"phasing down\" coal was agreed upon in the Glasgow Climate Pact.\nThe largest consumer and importer of coal in 2020 was China, which accounts for almost half the world's annual coal production, followed by India with about a tenth. Indonesia and Australia export the most, followed by Russia.\nEtymology.\nThe word originally took the form \"col\" in Old English, from reconstructed Proto-Germanic *\"kula\"(\"n\"), from Proto-Indo-European root *\"g\"(\"e\")\"u-lo-\" \"live coal\". Germanic cognates include the Old Frisian , Middle Dutch , Dutch , Old High German , German and Old Norse . Irish is also a cognate via the Indo-European root.\nFormation of coal.\nThe conversion of dead vegetation into coal is called coalification. At various times in the geologic past, the Earth had dense forests in low-lying areas. In these wetlands, the process of coalification began when dead plant matter was protected from oxidation, usually by mud or acidic water, and was converted into peat. The resulting peat bogs, which trapped immense amounts of carbon, were eventually deeply buried by sediments. Then, over millions of years, the heat and pressure of deep burial caused the loss of water, methane and carbon dioxide and increased the proportion of carbon. The grade of coal produced depended on the maximum pressure and temperature reached, with lignite (also called \"brown coal\") produced under relatively mild conditions, and sub-bituminous coal, bituminous coal, or anthracite coal (also called \"hard coal\" or \"black coal\") produced in turn with increasing temperature and pressure.\nOf the factors involved in coalification, temperature is much more important than either pressure or time of burial. Subbituminous coal can form at temperatures as low as while anthracite requires a temperature of at least .\nAlthough coal is known from most geologic periods, 90% of all coal beds were deposited in the Carboniferous and Permian periods. Paradoxically, this was during the Late Paleozoic icehouse, a time of global glaciation. However, the drop in global sea level accompanying the glaciation exposed continental shelves that had previously been submerged, and to these were added wide river deltas produced by increased erosion due to the drop in base level. These widespread areas of wetlands provided ideal conditions for coal formation. The rapid formation of coal ended with the coal gap in the Permian\u2013Triassic extinction event, where coal is rare.\nFavorable geography alone does not explain the extensive Carboniferous coal beds. Other factors contributing to rapid coal deposition were high oxygen levels, above 30%, that promoted intense wildfires and formation of charcoal that was all but indigestible by decomposing organisms; high carbon dioxide levels that promoted plant growth; and the nature of Carboniferous forests, which included lycophyte trees whose determinate growth meant that carbon was not tied up in heartwood of living trees for long periods.\nOne theory suggested that about 360 million years ago, some plants evolved the ability to produce lignin, a complex polymer that made their cellulose stems much harder and more woody. The ability to produce lignin led to the evolution of the first trees. But bacteria and fungi did not immediately evolve the ability to decompose lignin, so the wood did not fully decay but became buried under sediment, eventually turning into coal. About 300 million years ago, mushrooms and other fungi developed this ability, ending the main coal-formation period of earth's history. Although some authors pointed at some evidence of lignin degradation during the Carboniferous, and suggested that climatic and tectonic factors were a more plausible explanation, reconstruction of ancestral enzymes by phylogenetic analysis corroborated a hypothesis that lignin degrading enzymes appeared in fungi approximately 200 MYa.\nOne likely tectonic factor was the Central Pangean Mountains, an enormous range running along the equator that reached its greatest elevation near this time. Climate modeling suggests that the Central Pangean Mountains contributed to the deposition of vast quantities of coal in the late Carboniferous. The mountains created an area of year-round heavy precipitation, with no dry season typical of a monsoon climate. This is necessary for the preservation of peat in coal swamps.\nCoal is known from Precambrian strata, which predate land plants. This coal is presumed to have originated from residues of algae.\nSometimes coal seams (also known as coal beds) are interbedded with other sediments in a cyclothem. Cyclothems are thought to have their origin in glacial cycles that produced fluctuations in sea level, which alternately exposed and then flooded large areas of continental shelf.\nChemistry of coalification.\nThe woody tissue of plants is composed mainly of cellulose, hemicellulose, and lignin. Modern peat is mostly lignin, with a content of cellulose and hemicellulose ranging from 5% to 40%. Various other organic compounds, such as waxes and nitrogen- and sulfur-containing compounds, are also present. Lignin has a weight composition of about 54% carbon, 6% hydrogen, and 30% oxygen, while cellulose has a weight composition of about 44% carbon, 6% hydrogen, and 49% oxygen. Bituminous coal has a composition of about 84.4% carbon, 5.4% hydrogen, 6.7% oxygen, 1.7% nitrogen, and 1.8% sulfur, on a weight basis. The low oxygen content of coal shows that coalification removed most of the oxygen and much of the hydrogen a process called \"carbonization\".\nCarbonization proceeds primarily by dehydration, decarboxylation, and demethanation. Dehydration removes water molecules from the maturing coal via reactions such as\n2 R\u2013OH \u2192 R\u2013O\u2013R + H2O\nDecarboxylation removes carbon dioxide from the maturing coal:\nRCOOH \u2192 RH + CO2\nwhile demethanation proceeds by reaction such as\n2 R-CH3 \u2192 R-CH2-R + CH4\nR-CH2-CH2-CH2-R \u2192 R-CH=CH-R + CH4\nIn these formulas, R represents the remainder of a cellulose or lignin molecule to which the reacting groups are attached.\nDehydration and decarboxylation take place early in coalification, while demethanation begins only after the coal has already reached bituminous rank. The effect of decarboxylation is to reduce the percentage of oxygen, while demethanation reduces the percentage of hydrogen. Dehydration does both, and (together with demethanation) reduces the saturation of the carbon backbone (increasing the number of double bonds between carbon).\nAs carbonization proceeds, aliphatic compounds convert to aromatic compounds. Similarly, aromatic rings fuse into polyaromatic compounds (linked rings of carbon atoms). The structure increasingly resembles graphene, the structural element of graphite.\nChemical changes are accompanied by physical changes, such as decrease in average pore size.\nMacerals.\nMacerals are coalified plant parts that retain the morphology and some properties of the original plant. In many coals, individual macerals can be identified visually. Some macerals include:\nIn coalification huminite is replaced by vitreous (shiny) \"vitrinite\". Maturation of bituminous coal is characterized by \"bitumenization\", in which part of the coal is converted to bitumen, a hydrocarbon-rich gel. Maturation to anthracite is characterized by \"debitumenization\" (from demethanation) and the increasing tendency of the anthracite to break with a conchoidal fracture, similar to the way thick glass breaks.\nTypes.\nAs geological processes apply pressure to dead biotic material over time, under suitable conditions, its metamorphic grade or rank increases successively into:\nThere are several international standards for coal. The classification of coal is generally based on the content of volatiles. However the most important distinction is between thermal coal (also known as steam coal), which is burnt to generate electricity via steam; and metallurgical coal (also known as coking coal), which is burnt at high temperature to make steel.\nHilt's law is a geological observation that (within a small area) the deeper the coal is found, the higher its rank (or grade). It applies if the thermal gradient is entirely vertical; however, metamorphism may cause lateral changes of rank, irrespective of depth. For example, some of the coal seams of the Madrid, New Mexico coal field were partially converted to anthracite by contact metamorphism from an igneous sill while the remainder of the seams remained as bituminous coal.\nHistory.\nThe oldest intentional use of black coal was documented in Ostrava, Pet\u0159kovice, in a settlement from the older Stone Age on the top of Landek Hill. According to radiocarbon dating, the site falls within the period 25,000\u201323,000 years BC.\nIn China, the earliest recognized use is from the Shenyang where by 4000 BC Neolithic inhabitants had begun carving ornaments from black lignite. Coal from the Fushun mine in northeastern China was used to smelt copper as early as 1000 BC. Marco Polo, the Italian who traveled to China in the 13th century, described coal as \"black stones\u00a0... which burn like logs\", and said coal was so plentiful, people could take three hot baths a week. In Europe, the earliest reference to the use of coal as fuel is from the geological treatise \"On Stones\"(c. 371\u2013287 BC):\nOutcrop coal was used in Britain during the Bronze Age (3000\u20132000 BC), where it formed part of funeral pyres. In Roman Britain, \"the Romans were exploiting coals in all the major coalfields in England and Wales by the end of the second century AD\". Coal from the Midlands was transported via the Car Dyke for use in drying grain. Coal cinders have been found in the hearths of villas and Roman forts, particularly in Northumberland, dated to around AD 400. In the west of England, contemporary writers described the wonder of a permanent brazier of coal on the altar of Minerva at Aquae Sulis (modern day Bath), although in fact easily accessible surface coal from what became the Somerset coalfield was in common use in quite lowly dwellings locally. Evidence of coal's use for iron-working in the city during the Roman period has been found. In Eschweiler, Rhineland, deposits of bituminous coal were used by the Romans for the smelting of iron ore.\nNo evidence exists of coal being of great importance in Britain before about AD 1000, the High Middle Ages. Coal came to be referred to as \"seacoal\" in the 13th century; the wharf where the material arrived in London was known as Seacoal Lane, so identified in a charter of King Henry III granted in 1253. Initially, the name was given because much coal was found on the shore, having fallen from the exposed coal seams on cliffs above or washed out of underwater coal outcrops. In 1257\u20131259, coal from Newcastle upon Tyne was shipped to London for the smiths and lime-burners building Westminster Abbey. Coal continues to arrive on beaches around the world from both natural erosion of exposed coal seams and windswept spills from cargo ships. Many homes in such areas gather this coal as a significant, and sometimes primary, source of home heating fuel.\nThese easily accessible sources had largely become exhausted (or could not meet the growing demand) by the 13th century, when underground extraction by shaft mining or adits was developed. The alternative name was \"pitcoal\", because it came from mines.\nCooking and home heating with coal (in addition to firewood or instead of it) has been done in various times and places throughout human history, especially in times and places where ground-surface coal was available and firewood was scarce, but a widespread reliance on coal for home hearths probably never existed until such a switch in fuels happened in London in the late sixteenth and early seventeenth centuries. Historian Ruth Goodman has traced the socioeconomic effects of that switch and its later spread throughout Britain and suggested that its importance in shaping the industrial adoption of coal has been previously underappreciated.\nThe development of the Industrial Revolution led to the large-scale use of coal, as the steam engine took over from the water wheel. In 1700, five-sixths of the world's coal was mined in Britain. Britain would have run out of suitable sites for watermills by the 1830s if coal had not been available as a source of energy. In 1947 there were some 750,000 miners in Britain, but the last deep coal mine in the UK closed in 2015.\nA grade between bituminous coal and anthracite was once known as \"steam coal\" as it was widely used as a fuel for steam locomotives. In this specialized use, it is sometimes known as \"sea coal\" in the United States. Small \"steam coal\", also called \"dry small steam nuts\" (DSSN), was used as a fuel for domestic water heating.\nCoal played an important role in industry in the 19th and 20th century. The predecessor of the European Union, the European Coal and Steel Community, was based on the trading of this commodity.\nComposition.\nCoal is a mixture of diverse organic compounds and polymers. Several kinds exist, with variable dark colors and composition. Young coals (brown coal, lignite) are not completely black. The two main black coals are bituminous, which is more abundant, and anthracite. The type of coal with the highest percentage of carbon in its chemical composition is anthracite, followed by bituminous, then lignite, and finally brown coal. The fuel value of coal varies in the same order. Some anthracite deposits contain pure carbon in the form of graphite.\nFor bituminous coal, the elemental composition on a dry, ash-free basis is 84.4% carbon, 5.4% hydrogen, 6.7% oxygen, 1.7% nitrogen, and 1.8% sulfur by weight. This composition partly reflects the composition of the precursor plants. The second main fraction of coal is ash, an undesirable, noncombustable mixture of inorganic minerals. The composition of ash is often discussed in terms of oxides obtained after combustion in air:\nOf particular interest is the sulfur content of coal, which can vary from less than 1% to as much as 4%. Most of the sulfur and most of the nitrogen is incorporated into the organic fraction in the form of organosulfur compounds and organonitrogen compounds. This sulfur and nitrogen are strongly bound within the hydrocarbon matrix. These elements are released as SO2 and NOx upon combustion. They cannot be removed, economically at least, otherwise. Some coals contain inorganic sulfur, mainly in the form of iron pyrite (FeS2). Being a dense mineral, iron pyrite can be removed from coal by mechanical means, e.g. by froth flotation. Some sulfate occurs in coal, especially weathered samples. It is not volatilized and can be removed by washing.\nMinor components include:\nAs minerals, Hg, As, and Se are not problematic for the environment, especially since they are only trace components. They become mobile (volatile or water-soluble), however, when these minerals are combusted.\nUses.\nMost coal is used as fuel. 27.6% of world energy was supplied by coal in 2017 and Asia used almost three-quarters of it. Other large-scale applications also exist. The energy density of coal is roughly 24 megajoules per kilogram (approximately 6.7 kilowatt-hours per kg). For a coal power plant with a 40% efficiency, it takes an estimated of coal to power a 100\u00a0W lightbulb for one year.\nElectricity generation.\nIn 2022, 68% of global coal use was used for electricity generation.11 \nCoal burnt in coal power stations to generate electricity is called thermal coal. It is usually pulverized and then burned in a furnace with a boiler. The furnace heat converts boiler water to steam, which is then used to spin turbines which turn generators and create electricity. The thermodynamic efficiency of this process varies between about 25% and 50% depending on the pre-combustion treatment, turbine technology (e.g. supercritical steam generator) and the age of the plant. \nA few integrated gasification combined cycle (IGCC) power plants have been built, which burn coal more efficiently. Instead of pulverizing the coal and burning it directly as fuel in the steam-generating boiler, the coal is gasified to create syngas, which is burned in a gas turbine to produce electricity (just like natural gas is burned in a turbine). Hot exhaust gases from the turbine are used to raise steam in a heat recovery steam generator which powers a supplemental steam turbine. The overall plant efficiency when used to provide combined heat and power can reach as much as 94%. IGCC power plants emit less local pollution than conventional pulverized coal-fueled plants. Other ways to use coal are as coal-water slurry fuel (CWS), which was developed in the Soviet Union, or in an MHD topping cycle. However these are not widely used due to lack of profit.\nIn 2017 38% of the world's electricity came from coal, the same percentage as 30 years previously. In 2018 global installed capacity was 2TW (of which 1TW is in China) which was 30% of total electricity generation capacity. The most dependent major country is South Africa, with over 80% of its electricity generated by coal; but China alone generates more than half of the world's coal-generated electricity. Efforts around the world to reduce the use of coal have led some regions to switch to natural gas and renewable energy. In 2018 coal-fired power station capacity factor averaged 51%, that is they operated for about half their available operating hours.\nCoke.\nCoke is a solid carbonaceous residue that is used in manufacturing steel and other iron-containing products. Coke is made when metallurgical coal (also known as \"coking coal\") is baked in an oven without oxygen at temperatures as high as 1,000\u00a0\u00b0C, driving off the volatile constituents and fusing together the fixed carbon and residual ash. Metallurgical coke is used as a fuel and as a reducing agent in smelting iron ore in a blast furnace. The carbon monoxide produced by its combustion reduces hematite (an iron oxide) to iron.\nPig iron, which is too rich in dissolved carbon, is also produced.\nThe coke must be strong enough to resist the weight of overburden in the blast furnace, which is why coking coal is so important in making steel using the conventional route. Coke from coal is grey, hard, and porous and has a heating value of 29.6 MJ/kg. Some coke-making processes produce byproducts, including coal tar, ammonia, light oils, and coal gas.\nPetroleum coke (petcoke) is the solid residue obtained in oil refining, which resembles coke but contains too many impurities to be useful in metallurgical applications.\nProduction of chemicals.\nChemicals have been produced from coal since the 1950s. Coal can be used as a feedstock in the production of a wide range of chemical fertilizers and other chemical products. The main route to these products was coal gasification to produce syngas. Primary chemicals that are produced directly from the syngas include methanol, hydrogen, and carbon monoxide, which are the chemical building blocks from which a whole spectrum of derivative chemicals are manufactured, including olefins, acetic acid, formaldehyde, ammonia, urea, and others. The versatility of syngas as a precursor to primary chemicals and high-value derivative products provides the option of using coal to produce a wide range of commodities. In the 21st century, however, the use of coalbed methane is becoming more important.\nBecause the slate of chemical products that can be made via coal gasification can in general also use feedstocks derived from natural gas and petroleum, the chemical industry tends to use whatever feedstocks are most cost-effective. Therefore, interest in using coal tended to increase for higher oil and natural gas prices and during periods of high global economic growth that might have strained oil and gas production.\nCoal to chemical processes require substantial quantities of water. Much coal to chemical production is in China where coal dependent provinces such as Shanxi are struggling to control its pollution.\nLiquefaction.\nCoal can be converted directly into synthetic fuels equivalent to gasoline or diesel by hydrogenation or carbonization. Coal liquefaction emits more carbon dioxide than liquid fuel production from crude oil. Mixing in biomass and using carbon capture and storage (CCS) would emit slightly less than the oil process but at a high cost. State owned China Energy Investment runs a coal liquefaction plant and plans to build 2 more.\nCoal liquefaction may also refer to the cargo hazard when shipping coal.\nGasification.\nCoal gasification, as part of an integrated gasification combined cycle (IGCC) coal-fired power station, is used to produce syngas, a mixture of carbon monoxide (CO) and hydrogen (H2) gas to fire gas turbines to produce electricity. Syngas can also be converted into transportation fuels, such as gasoline and diesel, through the Fischer\u2013Tropsch process; alternatively, syngas can be converted into methanol, which can be blended into fuel directly or converted to gasoline via the methanol to gasoline process. Gasification combined with Fischer\u2013Tropsch technology was used by the Sasol chemical company of South Africa to make chemicals and motor vehicle fuels from coal.\nDuring gasification, the coal is mixed with oxygen and steam while also being heated and pressurized. During the reaction, oxygen and water molecules oxidize the coal into carbon monoxide (CO), while also releasing hydrogen gas (H2). This used to be done in underground coal mines, and also to make town gas, which was piped to customers to burn for illumination, heating, and cooking.\n 3C (\"as Coal\") + O2 + H2O \u2192 H2 + 3CO\nIf the refiner wants to produce gasoline, the syngas is routed into a Fischer\u2013Tropsch reaction. This is known as indirect coal liquefaction. If hydrogen is the desired end-product, however, the syngas is fed into the water gas shift reaction, where more hydrogen is liberated:\n CO + H2O \u2192 CO2 + H2\nCoal industry.\nMining.\nAbout 8,000 Mt of coal are produced annually, about 90% of which is hard coal and 10% lignite. As of 2018[ [update]] just over half is from underground mines. The coal mining industry employs almost 2.7 million workers. More accidents occur during underground mining than surface mining. Not all countries publish mining accident statistics so worldwide figures are uncertain, but it is thought that most deaths occur in coal mining accidents in China: in 2017 there were 375 coal mining related deaths in China. Most coal mined is thermal coal (also called steam coal as it is used to make steam to generate electricity) but metallurgical coal (also called \"metcoal\" or \"coking coal\" as it is used to make coke to make iron) accounts for 10% to 15% of global coal use.\nAs a traded commodity.\nChina mines almost half the world's coal, followed by India with about a tenth. At 471 Mt and a 34% share of global exports, Indonesia was the largest exporter by volume in 2022, followed by Australia with 344 Mt and Russia with 224 Mt. Other major exporters of coal are the United States, South Africa, Colombia, and Canada.118 In 2022, China, India, and Japan were the biggest importers of coal, importing 301, 228, and 184 Mt respectively.117 Russia is increasingly orienting its coal exports from Europe to Asia as Europe transitions to renewable energy and subjects Russia to sanctions over its invasion of Ukraine.\nThe price of metallurgical coal is volatile and much higher than the price of thermal coal because metallurgical coal must be lower in sulfur and requires more cleaning. Coal futures contracts provide coal producers and the electric power industry an important tool for hedging and risk management.\nIn some countries, new onshore wind or solar generation already costs less than coal power from existing plants.\nHowever, for China this is forecast for the early 2020s and for southeast Asia not until the late 2020s. In India, building new plants is uneconomic and, despite being subsidized, existing plants are losing market share to renewables.\nIn many countries in the Global North, there is a move away from the use of coal and former mine sites are being used as a tourist attraction.\nMarket trends.\nIn 2022, China used 4520 Mt of coal, comprising more than half of global coal consumption. India, the European Union, and the United States, were the next largest consumers of coal, using 1162, 461, and 455 Mt respectively.114 Over the past decade, China has almost always accounted for the lion's share of the global growth in coal demand. Therefore, international market trends depend on Chinese energy policy. \nAlthough the government effort to reduce air pollution in China means that the global long-term trend is to burn less coal, the short and medium term trends may differ, in part due to Chinese financing of new coal-fired power plants in other countries.\nPreliminary analysis by International Energy Agency (IEA) indicates that global coal exports reached an all-time high in 2023. Through to 2026, the IEA expects global coal trade to decline by about 12%, driven by growing domestic production in coal-intensive economies such as China and India and coal phase-out plans elsewhere, such as in Europe. While thermal coal exports are expected to decline by about 16% by 2026, exports of metallurgical coal are expected to slightly increase by almost 2%. \nDamage to human health.\nThe use of coal as fuel causes health problems and deaths. The mining and processing of coal causes air and water pollution. Coal-powered plants emit nitrogen oxides, sulfur dioxide, particulate pollution, and heavy metals, which adversely affect human health. Coalbed methane extraction is important to avoid mining accidents.\nThe deadly London smog was caused primarily by the heavy use of coal. Globally coal is estimated to cause 800,000 premature deaths every year, mostly in India and China.\nBurning coal is a major contributor to sulfur dioxide emissions, which creates PM2.5 particulates, the most dangerous form of air pollution.\nCoal smokestack emissions cause asthma, strokes, reduced intelligence, artery blockages, heart attacks, congestive heart failure, cardiac arrhythmias, mercury poisoning, arterial occlusion, and lung cancer.\nAnnual health costs in Europe from use of coal to generate electricity are estimated at up to \u20ac43 billion.\nIn China, early deaths due to air pollution coal plants have been estimated at 200 per GW-year, however they may be higher around power plants where scrubbers are not used or lower if they are far from cities. Improvements to China's air quality and human health would grow with more stringent climate policies, mainly because the country's energy is so heavily reliant on coal. And there would be a net economic benefit.\nA 2017 study in the \"Economic Journal\" found that for Britain during the period 1851\u20131860, \"a one standard deviation increase in coal use raised infant mortality by 6\u20138% and that industrial coal use explains roughly one-third of the urban mortality penalty observed during this period.\"\nBreathing in coal dust causes coalworker's pneumoconiosis or \"black lung\", so called because the coal dust literally turns the lungs black. In the US alone, it is estimated that 1,500 former employees of the coal industry die every year from the effects of breathing in coal mine dust.\nHuge amounts of coal ash and other waste is produced annually. Use of coal generates hundreds of millions of tons of ash and other waste products every year. These include fly ash, bottom ash, and flue-gas desulfurization sludge, that contain mercury, uranium, thorium, arsenic, and other heavy metals, along with non-metals such as selenium.\nAround 10% of coal is ash. Coal ash is hazardous and toxic to human beings and some other living things. Coal ash contains the radioactive elements uranium and thorium. Coal ash and other solid combustion byproducts are stored locally and escape in various ways that expose those living near coal plants to radiation and environmental toxics.\nDamage to the environment.\nCoal mining, coal combustion wastes, and flue gas are causing major environmental damage.\nWater systems are affected by coal mining. For example, the mining of coal affects groundwater and water table levels and acidity. Spills of fly ash, such as the Kingston Fossil Plant coal fly ash slurry spill, can also contaminate land and waterways, and destroy homes. Power stations that burn coal also consume large quantities of water. This can affect the flows of rivers, and has consequential impacts on other land uses. In areas of water scarcity, such as the Thar Desert in Pakistan, coal mining and coal power plants contribute to the depletion of water resources.\nOne of the earliest known impacts of coal on the water cycle was acid rain. In 2014, approximately 100 Tg/S of sulfur dioxide (SO2) was released, over half of which was from burning coal. After release, the sulfur dioxide is oxidized to H2SO4 which scatters solar radiation, hence its increase in the atmosphere exerts a cooling effect on the climate. This beneficially masks some of the warming caused by increased greenhouse gases. However, the sulfur is precipitated out of the atmosphere as acid rain in a matter of weeks, whereas carbon dioxide remains in the atmosphere for hundreds of years. Release of SO2 also contributes to the widespread acidification of ecosystems.\nDisused coal mines can also cause issues. Subsidence can occur above tunnels, causing damage to infrastructure or cropland. Coal mining can also cause long lasting fires, and it has been estimated that thousands of coal seam fires are burning at any given time. For example, Brennender Berg has been burning since 1668, and is still burning in the 21st century.\nThe production of coke from coal produces ammonia, coal tar, and gaseous compounds as byproducts which if discharged to land, air or waterways can pollute the environment. The Whyalla steelworks is one example of a coke producing facility where liquid ammonia was discharged to the marine environment.\nClimate change.\nThe largest and most long-term effect of coal use is the release of carbon dioxide, a greenhouse gas that causes climate change. Coal-fired power plants were the single largest contributor to the growth in global CO2 emissions in 2018, 40% of the total fossil fuel emissions, and more than a quarter of total emissions. Coal mining can emit methane, another greenhouse gas.\nIn 2016 world gross carbon dioxide emissions from coal usage were 14.5 gigatonnes. For every megawatt-hour generated, coal-fired electric power generation emits around a tonne of carbon dioxide, which is double the approximately 500\u00a0kg of carbon dioxide released by a natural gas-fired electric plant. The emission intensity of coal varies with type and generator technology and exceeds 1200\u00a0g per kWh in some countries. In 2013, the head of the UN climate agency advised that most of the world's coal reserves should be left in the ground to avoid catastrophic global warming. To keep global warming below 1.5\u00a0\u00b0C or 2\u00a0\u00b0C hundreds, or possibly thousands, of coal-fired power plants will need to be retired early.\nUnderground fires.\nThousands of coal fires are burning around the world. Those burning underground can be difficult to locate and many cannot be extinguished. Fires can cause the ground above to subside, their combustion gases are dangerous to life, and breaking out to the surface can initiate surface wildfires. Coal seams can be set on fire by spontaneous combustion or contact with a mine fire or surface fire. Lightning strikes are an important source of ignition. The coal continues to burn slowly back into the seam until oxygen (air) can no longer reach the flame front. A grass fire in a coal area can set dozens of coal seams on fire. Coal fires in China burn an estimated 120 million tons of coal a year, emitting 360 million metric tons of CO2, amounting to 2\u20133% of the annual worldwide production of CO2 from fossil fuels. \nPollution mitigation and carbon capture.\nSystems and technologies exist to mitigate the health and environmental impact of burning coal for energy. \nPrecombustion treatment.\nRefined coal is the product of a coal-upgrading technology that removes moisture and certain pollutants from lower-rank coals such as sub-bituminous and lignite (brown) coals. It is one form of several precombustion treatments and processes for coal that alter coal's characteristics before it is burned. Thermal efficiency improvements are achievable by improved pre-drying (especially relevant with high-moisture fuel such as lignite or biomass). The goals of precombustion coal technologies are to increase efficiency and reduce emissions when the coal is burned. Precombustion technology can sometimes be used as a supplement to postcombustion technologies to control emissions from coal-fueled boilers.\nPost combustion approaches.\nPost combustion approaches to mitigate pollution include flue-gas desulfurization, selective catalytic reduction, electrostatic precipitators, and fly ash reduction.\nCarbon capture and storage.\nCarbon capture and storage (CCS) can be used to capture carbon dioxide from the flue gas of coal power plants and bury it securely in an underground reservoir. Between 1972 and 2017, plans were made to add CCS to enough coal and gas power plants to sequester 161 million tonnes of CO2 per year, but by 2021 98% of these plans had failed. Cost, the absence of measures to address long-term liability for stored CO2, and limited social acceptability have all contributed to project cancellations.133 As of 2024, CCS is in operation at only four coal power plants and one gas power plant worldwide.\n\"Clean coal\" and \"abated coal\".\nSince the mid-1980s, the term \"clean coal\" has been widely used with various meanings. Initially, \"clean coal technology\" referred to scrubbers and catalytic converters that reduced the pollutants that cause acid rain. The scope then expanded to include reduction of other pollutants such as mercury. Recently, the term has come to encompass the use of carbon capture and storage to reduce greenhouse gas emissions. \nIn political discourse, the term has been erroneously used to imply that coal itself can be clean. For instance, it has been suggested that \"clean coal\" can be produced and exported. Even with scrubbers and some CCS, coal still has a fairly high environmental impact. \nIn discussions on greenhouse gas emissions, another common term is \"abatement\" of coal use. In the 2023 United Nations Climate Change Conference, an agreement was reached to phase down unabated coal use. Since the term \"abated\" was not defined, the agreement was criticized for being open to abuse. Without a clear definition, is possible for fossil fuel use to be called \"abated\" if it uses CCS only in a minimal fashion, such as capturing only 30% of the emissions from a plant.\nThe Intergovernmental Panel on Climate Change (IPCC) considers fossil fuels to be unabated if they are \"produced and used without interventions that substantially reduce the amount of greenhouse gases emitted throughout the life-cycle; for example, capturing 90% or more from power plants.\"\nEconomics.\nIn 2018 US$ was invested in coal supply but almost all for sustaining production levels rather than opening new mines.\nIn the long term coal and oil could cost the world trillions of dollars per year. Coal alone may cost Australia billions, whereas costs to some smaller companies or cities could be on the scale of millions of dollars. The economies most damaged by coal (via climate change) may be India and the US as they are the countries with the highest social cost of carbon. Bank loans to finance coal are a risk to the Indian economy.\nChina is the largest producer of coal in the world. It is the world's largest energy consumer, and coal in China supplies 60% of its primary energy. However two fifths of China's coal power stations are estimated to be loss-making.\nAir pollution from coal storage and handling costs the US almost 200 dollars for every extra ton stored, due to PM2.5. Coal pollution costs the \u20ac each year. Measures to cut air pollution benefit individuals financially and the economies of countries such as China.\nSubsidies.\nSubsidies for coal in 2021 have been estimated at US$, not including electricity subsidies, and are expected to rise in 2022. As of 2019[ [update]] G20 countries provide at least US$ of government support per year for the production of coal, including coal-fired power: many subsidies are impossible to quantify but they include US$ in domestic and international public finance, US$ in fiscal support, and US$ in state-owned enterprise (SOE) investments per year. In the EU state aid to new coal-fired plants is banned from 2020, and to existing coal-fired plants from 2025. As of 2018, government funding for new coal power plants was supplied by Exim Bank of China, the Japan Bank for International Cooperation and Indian public sector banks. Coal in Kazakhstan was the main recipient of coal consumption subsidies totalling US$2 billion in 2017. Coal in Turkey benefited from substantial subsidies in 2021.\nStranded assets.\nSome coal-fired power stations could become stranded assets, for example China Energy Investment, the world's largest power company, risks losing half its capital. However, state-owned electricity utilities such as Eskom in South Africa, Perusahaan Listrik Negara in Indonesia, Sarawak Energy in Malaysia, Taipower in Taiwan, EGAT in Thailand, Vietnam Electricity and E\u00dcA\u015e in Turkey are building or planning new plants. As of 2021 this may be helping to cause a carbon bubble which could cause financial instability if it bursts.\nPolitics.\nCountries building or financing new coal-fired power stations, such as China, India, Indonesia, Vietnam, Turkey and Bangladesh, face mounting international criticism for obstructing the aims of the Paris Agreement. In 2019, the Pacific Island nations (in particular Vanuatu and Fiji) criticized Australia for failing to cut their emissions at a faster rate than they were, citing concerns about coastal inundation and erosion. In May 2021, the G7 members agreed to end new direct government support for international coal power generation.\nCultural usage.\nCoal is the official state mineral of Kentucky, and the official state rock of Utah and West Virginia. These US states have a historic link to coal mining.\nSome cultures hold that children who misbehave will receive only a lump of coal from Santa Claus for Christmas in their stockings instead of presents.\nIt is also customary and considered lucky in Scotland to give coal as a gift on New Year's Day. This occurs as part of first-footing and represents warmth for the year to come.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5992", "revid": "9203640", "url": "https://en.wikipedia.org/wiki?curid=5992", "title": "Traditional Chinese medicine", "text": "Traditional Chinese medicine (TCM) is an alternative medical practice drawn from traditional medicine in China. A large share of its claims are pseudoscientific, with the majority of treatments having no robust evidence of effectiveness or logical mechanism of action. Some TCM ingredients are known to be toxic and cause disease, including cancer.\nMedicine in traditional China encompassed a range of sometimes competing health and healing practices, folk beliefs, literati theory, Taoist and Confucian philosophy, herbal remedies, food, diet, exercise, medical specializations, and schools of thought. TCM as it exists today has been described as a largely 20th century invention. In the early twentieth century, Chinese cultural and political modernizers worked to eliminate traditional practices as backward and unscientific. Traditional practitioners then selected elements of philosophy and practice and organized them into what they called \"Chinese medicine\". In the 1950s, the Chinese government sought to revive traditional medicine (including legalizing previously banned practices) and sponsored the integration of TCM and Western medicine, and in the Cultural Revolution of the 1960s, promoted TCM as inexpensive and popular. The creation of modern TCM was largely spearheaded by Mao Zedong, despite the fact that, according to \"The Private Life of Chairman Mao\", he did not believe in its effectiveness. After the opening of relations between the United States and China after 1972, there was great interest in the West for what is now called traditional Chinese medicine (TCM).\nTCM is said to be based on such texts as \"Huangdi Neijing\" (The Inner Canon of the Yellow Emperor), and \"Compendium of Materia Medica\", a sixteenth-century encyclopedic work, and includes various forms of herbal medicine, acupuncture, cupping therapy, gua sha, massage (tui na), bonesetter (die-da), exercise (qigong), and dietary therapy. TCM is widely used in the Sinosphere. One of the basic tenets is that the body's \"qi\" is circulating through channels called meridians having branches connected to bodily organs and functions. There is no evidence that meridians or vital energy exist. Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to the humoral theory of ancient Greece and ancient Rome.\nThe demand for traditional medicines in China is a major generator of illegal wildlife smuggling, linked to the killing and smuggling of endangered animals. The Chinese authorities have engaged in attempts to crack down on illegal TCM-related wildlife smuggling.\nAncient history.\nScholars in the history of medicine in China distinguish its doctrines and practice from those of present-day TCM. J. A. Jewell and S. M. Hillier state that the term \"Traditional Chinese Medicine\" became an established term due to the work of Dr. Kan-Wen Ma, a Western-trained medical doctor who was persecuted during the Cultural Revolution and immigrated to Britain, joining the University of London's Wellcome Institute for the History of Medicine. Ian Johnson says, on the other hand, that the English-language term \"traditional Chinese medicine\" was coined by \"party propagandists\" in 1955.\nNathan Sivin criticizes attempts to treat medicine and medical practices in traditional China as if they were a single system. Instead, he says, there were 2,000 years of \"medical system in turmoil\" and speaks of a \"myth of an unchanging medical tradition\". He urges that \"Traditional medicine translated purely into terms of modern medicine becomes partly nonsensical, partly irrelevant, and partly mistaken; that is also true the other way around, a point easily overlooked.\" TJ Hinrichs observes that people in modern Western societies divide healing practices into biomedicine for the body, psychology for the mind, and religion for the spirit, but these distinctions are inadequate to describe medical concepts among Chinese historically and to a considerable degree today.\nThe medical anthropologist Charles Leslie writes that Chinese, Greco-Arabic, and Indian traditional medicines were all grounded in systems of correspondence that aligned the organization of society, the universe, and the human body and other forms of life into an \"all-embracing order of things\". Each of these traditional systems was organized with such qualities as heat and cold, wet and dry, light and darkness, qualities that also align the seasons, compass directions, and the human cycle of birth, growth, and death. They provided, Leslie continued, a \"comprehensive way of conceiving patterns that ran through all of nature,\" and they \"served as a classificatory and mnemonic device to observe health problems and to reflect upon, store, and recover empirical knowledge,\" but they were also \"subject to stultifying theoretical elaboration, self-deception, and dogmatism.\"\nThe doctrines of Chinese medicine are rooted in books such as the \"Yellow Emperor's Inner Canon\" and the \"Treatise on Cold Damage\", as well as in cosmological notions such as yin\u2013yang and the five phases. The \"Compendium of Materia Medica\" dates back to around 1,100 BCE when only a few dozen drugs were described. By the end of the 16th century, the number of drugs documented had reached close to 1,900. And by the end of the last century, published records of CMM had reached 12,800 drugs.\" Starting in the 1950s, these precepts were standardized in the People's Republic of China, including attempts to integrate them with modern notions of anatomy and pathology. In the 1950s, the Chinese government promoted a systematized form of TCM.\nShang dynasty.\nTraces of therapeutic activities in China date from the Shang dynasty (14th\u201311th centuries BCE). Though the Shang did not have a concept of \"medicine\" as distinct from other health practices, their oracular inscriptions on bones and tortoise shells refer to illnesses that affected the Shang royal family: eye disorders, toothaches, bloated abdomen, and such. Shang elites usually attributed them to curses sent by their ancestors. There is currently no evidence that the Shang nobility used herbal remedies.\nStone and bone needles found in ancient tombs led Joseph Needham to speculate that acupuncture might have been carried out in the Shang dynasty. This being said, most historians now make a distinction between medical lancing (or bloodletting) and acupuncture in the narrower sense of using metal needles to attempt to treat illnesses by stimulating points along circulation channels (\"meridians\") in accordance with beliefs related to the circulation of \"Qi\". The earliest evidence for acupuncture in this sense dates to the second or first century BCE.\nHan dynasty.\nThe \"Yellow Emperor's Inner Canon (Huangdi Neijing)\", the oldest received work of Chinese medical theory, was compiled during the Han dynasty around the first century BCE on the basis of shorter texts from different medical lineages. Written in the form of dialogues between the legendary Yellow Emperor and his ministers, it offers explanations on the relation between humans, their environment, and the cosmos, on the contents of the body, on human vitality and pathology, on the symptoms of illness, and on how to make diagnostic and therapeutic decisions in light of all these factors. Unlike earlier texts like \"Recipes for Fifty-Two Ailments\", which was excavated in the 1970s from the Mawangdui tomb that had been sealed in 168 BCE, the \"Inner Canon\" rejected the influence of spirits and the use of magic. It was also one of the first books in which the cosmological doctrines of Yinyang and the Five Phases were brought to a mature synthesis.\nThe \"Treatise on Cold Damage Disorders and Miscellaneous Illnesses (Shang Han Lun)\" was collated by Zhang Zhongjing sometime between 196 and 220 CE; at the end of the Han dynasty. Focusing on drug prescriptions rather than acupuncture, it was the first medical work to combine Yinyang and the Five Phases with drug therapy. This formulary was also the earliest public Chinese medical text to group symptoms into clinically useful \"patterns\" (\"zheng\" ) that could serve as targets for therapy. Having gone through numerous changes over time, the formulary now circulates as two distinct books: the \"Treatise on Cold Damage Disorders\" and the \"Essential Prescriptions of the Golden Casket\", which were edited separately in the eleventh century, under the Song dynasty.\nNanjing or \"Classic of Difficult Issues\", originally called \"The Yellow Emperor Eighty-one Nan Jing\", ascribed to Bian Que in the eastern Han dynasty. This book was compiled in the form of question-and-answer explanations. A total of 81 questions have been discussed. Therefore, it is also called \"Eighty-One Nan\". The book is based on basic theory and has also analyzed some disease certificates. Questions one to twenty-two is about pulse study, questions twenty-three to twenty-nine is about meridian study, questions thirty to forty-seven is related to urgent illnesses, questions forty-eight to sixty-one is related to serious diseases, questions sixty-two to sixty-eight is related to acupuncture points, and questions sixty-nine to eighty-one is related to the needlepoint methods.\nThe book is credited as developing its own path, while also inheriting the theories from Huangdi Neijing. The content includes physiology, pathology, diagnosis, treatment contents, and a more essential and specific discussion of pulse diagnosis. It has become one of the four classics for Chinese medicine practitioners to learn from and has impacted the medical development in China.\n\"Shennong Ben Cao Jing\" is one of the earliest written medical books in China. Written during the Eastern Han dynasty between 200 and 250 CE, it was the combined effort of practitioners in the Qin and Han dynasties who summarized, collected and compiled the results of pharmacological experience during their time periods. It was the first systematic summary of Chinese herbal medicine. Most of the pharmacological theories and compatibility rules and the proposed \"seven emotions and harmony\" principle have played a role in the practice of medicine for thousands of years. Therefore, it has been a textbook for medical workers in modern China. The full text of \"Shennong Ben Cao Jing\" in English can be found online.\nPost-Han dynasty.\nIn the centuries that followed, several shorter books tried to summarize or systematize the contents of the \"Yellow Emperor's Inner Canon\". The \"Canon of Problems\" (probably second century CE) tried to reconcile divergent doctrines from the \"Inner Canon\" and developed a complete medical system centered on needling therapy. The \"AB Canon of Acupuncture and Moxibustion\" (\"Zhenjiu jiayi jing\" , compiled by Huangfu Mi sometime between 256 and 282 CE) assembled a consistent body of doctrines concerning acupuncture; whereas the \"Canon of the Pulse\" (\"Maijing\" ; c. 280) presented itself as a \"comprehensive handbook of diagnostics and therapy.\"\nAround 900\u20131000 AD, Chinese were the first to develop a form of vaccination, known as variolation or inoculation, to prevent smallpox. Chinese physicians had realised that when healthy people were exposed to smallpox scab tissue, they had a smaller chance of being infected by the disease later on. The common methods of inoculation at the time was through crushing smallpox scabs into powder and breathing it through the nose.\nProminent medical scholars of the post-Han period included Tao Hongjing (456\u2013536), Sun Simiao of the Sui and Tang dynasties, Zhang Jiegu (c.\u20091151\u20131234), and Li Shizhen (1518\u20131593). \nModern history.\nChinese communities under colonial rule.\nChinese communities living in colonial port cities were influenced by the diverse cultures they encountered, which also led to evolving understandings of medical practices where Chinese forms of medicine were combined with Western medical knowledge. For example, the Tung Wah Hospital was established in Hong Kong in 1869 based on the widespread rejection of Western medicine for pre-existing medical practices, although Western medicine would still be practiced in the hospital alongside Chinese medicinal practices. The Tung Wah Hospital was likely connected to another Chinese medical institution, the Kwong Wai Shiu Hospital of Singapore, which had previous community links to Tung Wah, was established for similar reasons and also provided both Western and Chinese medical care. By 1935, English-language newspapers in Colonial Singapore already used the term \"Traditional Chinese Medicine\" to label Chinese ethnic medical practices.\nPeople's Republic.\nIn 1950, Chinese Communist Party (CCP) chairman Mao Zedong announced support of traditional Chinese medicine; this was despite the fact that Mao did not personally believe in and did not use TCM, according to his personal physician Li Zhisui. In 1952, the president of the Chinese Medical Association said that, \"This One Medicine, will possess a basis in modern natural sciences, will have absorbed the ancient and the new, the Chinese and the foreign, all medical achievements \u2013 and will be China's New Medicine!\"\nDuring the Cultural Revolution (1966\u20131976) the CCP and the government emphasized modernity, cultural identity and China's social and economic reconstruction and contrasted them to the colonial and feudal past. The government established a grassroots health care system as a step in the search for a new national identity and tried to revitalize traditional medicine and made large investments in traditional medicine to try to develop affordable medical care and public health facilities. The Ministry of Health directed health care throughout China and established primary care units. Chinese physicians trained in Western medicine were required to learn traditional medicine, while traditional healers received training in modern methods. This strategy aimed to integrate modern medical concepts and methods and revitalize appropriate aspects of traditional medicine. Therefore, traditional Chinese medicine was re-created in response to Western medicine.\nIn 1968, the CCP supported a new system of health care delivery for rural areas. Villages were assigned a barefoot doctor (a medical staff with basic medical skills and knowledge to deal with minor illnesses) responsible for basic medical care. The medical staff combined the values of traditional China with modern methods to provide health and medical care to poor farmers in remote rural areas. The barefoot doctors became a symbol of the Cultural Revolution, for the introduction of modern medicine into villages where traditional Chinese medicine services were used. The barefoot doctor system represents a hybrid of modern and traditional Chinese medicine (, usually translated \"Integrative Chinese Medicine\"), a guiding principle that has far outlived the barefoot doctor system. Nathan Sivin's 1987 translation of \"Revised Outline of Chinese Medicine: For Western-medicine practitioners to learn Chinese medicine\" (; 1972) serves as a good, though outdated, example of this principle in practice.\nThe State Intellectual Property Office (now known as CNIPA) established a database of patents granted for traditional Chinese medicine.\nIn the second decade of the twenty-first century, Chinese Communist Party general secretary Xi Jinping strongly supported TCM, calling it a \"gem\". As of May 2011, in order to promote TCM worldwide, China had signed TCM partnership agreements with over 70 countries. His government pushed to increase its use and the number of TCM-trained doctors and announced that students of TCM would no longer be required to pass examinations in Western medicine. Chinese scientists and researchers, however, expressed concern that TCM training and therapies would receive equal support with Western medicine. They also criticized a reduction in government testing and regulation of the production of TCMs, some of which were toxic. Government censors have removed Internet posts that question TCM. In 2020 Beijing drafted a local regulation outlawing criticism of TCM. According to \"Caixin\", the regulation was later passed with the provision outlawing criticism of TCM removed.\nHong Kong.\nAt the beginning of Hong Kong's opening up, Western medicine was not yet popular, and Western medicine doctors were mostly foreigners; local residents mostly relied on Chinese medicine practitioners. In 1841, the British government of Hong Kong issued an announcement pledging to govern Hong Kong residents in accordance with all the original rituals, customs and private legal property rights. As traditional Chinese medicine had always been used in China, the use of traditional Chinese medicine was not regulated.\nThe establishment in 1870 of the Tung Wah Hospital was the first use of Chinese medicine for the treatment in Chinese hospitals providing free medical services. As the promotion of Western medicine by the British government started from 1940, Western medicine started being popular among Hong Kong population. In 1959, Hong Kong had researched the use of traditional Chinese medicine to replace Western medicine.\nHistoriography of Chinese medicine.\nThe study of traditional medicine in China is an academic field within the history of science, with its own scholarly associations, journals, graduate programs, and debates with each other. These scholars distinguish traditional medicine in historical China from the more recent traditional Chinese medicine (TCM), which took elements from traditional texts and practices to construct a systematic body. Paul Unschuld, for instance, sees a \"departure of TCM from its historical origins.\" What is called \"Traditional Chinese Medicine\" and practiced today in China and the West is not thousands of years old, but recently constructed using selected traditional terms, some of which have been taken out of context, some badly misunderstood. He has criticized Chinese and Western popular books for selective use of evidence, choosing only those works or parts of historical works that seem to lead to modern medicine, ignoring those elements that do not now seem to be effective.\nHistorians have noted two key aspects of Chinese medical history: understanding conceptual differences when translating the term , and observing the history from the perspective of cosmology rather than biology.\nIn Chinese classical texts, the term is the closest historical translation to the English word \"body\" because it sometimes refers to the physical human body in terms of being weighed or measured, but the term is to be understood as an \"ensemble of functions\" encompassing both the human psyche and emotions. This concept of the human body is opposed to the European duality of a separate mind and body. It is critical for scholars to understand the fundamental differences in concepts of the body in order to connect the medical theory of the classics to the \"human organism\" it is explaining.\nChinese scholars established a correlation between the cosmos and the \"human organism\". The basic components of cosmology, qi, yin yang and the Five Phase theory, were used to explain health and disease in texts such as \"Huangdi neijing\". Yin and yang are the changing factors in cosmology, with \"qi\" as the vital force or energy of life. The Five Phase theory (\"Wuxing\") of the Han dynasty contains the elements wood, fire, earth, metal, and water. By understanding medicine from a cosmology perspective, historians better understand Chinese medical and social classifications such as gender, which was defined by a domination or remission of yang in terms of yin.\nThese two distinctions are imperative when analyzing the history of traditional Chinese medical science.\nA majority of Chinese medical history written after the classical canons comes in the form of primary source case studies where academic physicians record the illness of a particular person and the healing techniques used, as well as their effectiveness. Historians have noted that Chinese scholars wrote these studies instead of \"books of prescriptions or advice manuals;\" in their historical and environmental understanding, no two illnesses were alike so the healing strategies of the practitioner was unique every time to the specific diagnosis of the patient. Medical case studies existed throughout Chinese history, but \"individually authored and published case history\" was a prominent creation of the Ming dynasty. An example such case studies would be the literati physician, Cheng Congzhou, collection of 93 cases published in 1644.\nCritiques.\nCritics say that TCM theory and practice have no basis in modern science, and that TCM practitioners do not agree on what diagnosis and treatments should be used for any given person. A 2007 editorial in the journal \"Nature\" wrote that TCM \"remains poorly researched and supported, and most of its treatments have no logical mechanism of action.\" It also described TCM as \"fraught with pseudoscience\". A review of the literature in 2008 found that scientists are \"still unable to find a shred of evidence\" according to standards of science-based medicine for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points, and that the traditional principles of acupuncture are deeply flawed. \"Acupuncture points and meridians are not a reality\", the review continued, but \"merely the product of an ancient Chinese philosophy\". In June 2019, the World Health Organization included traditional Chinese medicine in a global diagnostic compendium, but a spokesman said this was \"not an endorsement of the scientific validity of any Traditional Medicine practice or the efficacy of any Traditional Medicine intervention.\"\nA 2012 review of cost-effectiveness research for TCM found that studies had low levels of evidence, with no beneficial outcomes. Pharmaceutical research on the potential for creating new drugs from traditional remedies has few successful results. Proponents suggest that research has so far missed key features of the art of TCM, such as unknown interactions between various ingredients and complex interactive biological systems. One of the basic tenets of TCM is that the body's \"qi\" (sometimes translated as vital energy) is circulating through channels called meridians having branches connected to bodily organs and functions. The concept of vital energy is pseudoscientific. Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to Classical humoral theory.\nTCM has also been controversial within China. In 2006, the Chinese philosopher Zhang Gongyao triggered a national debate with an article entitled \"Farewell to Traditional Chinese Medicine\", arguing that TCM was a pseudoscience that should be abolished in public healthcare and academia. The Chinese government took the stance that TCM is a science and continued to encourage its development.\nThere are concerns over a number of potentially toxic plants, animal parts, and mineral Chinese compounds, as well as the facilitation of disease. Trafficked and farm-raised animals used in TCM are a source of several fatal zoonotic diseases. There are additional concerns over the illegal trade and transport of endangered species including rhinoceroses and tigers, and the welfare of specially farmed animals, including bears.\nPhilosophical background.\nTraditional Chinese medicine includes a broad range of practices sharing common concepts which developed in China for more than 2,000 years, including various forms of herbal medicine, acupuncture, massage (), exercise (), and dietary therapy. It is primarily used as a complementary alternative medicine approach. Traditional practices are widely used in China and in adapted form also in the West. Its philosophy is based on School of Yin Yang, which incorporates Five Phases theory and Yin\u2013Yang theory, and elements of which were absorbed by Daoism. Philosophical texts influenced medical thought and practice, mostly by being grounded in the same theories of \"qi\", \"yin-yang\" and \"wuxing\" and microcosm-macrocosm analogies.\nYin and yang.\nYin and yang are ancient Chinese deductive reasoning concepts used within Chinese medical diagnosis which can be traced back to the Shang dynasty (1600\u20131100\u00a0BCE). They represent two abstract and complementary aspects that every phenomenon in the universe can be divided into. Primordial analogies for these aspects are the sun-facing (yang) and the shady (yin) side of a hill. Two other commonly used representational allegories of yin and yang are water and fire. In the yin\u2013yang theory, detailed attributions are made regarding the yin or yang character of things:\nThe concept of yin and yang is also applicable to the human body; for example, the upper part of the body and the back are assigned to yang, while the lower part of the body is believed to have the yin character. Yin and yang characterization also extends to the various body functions, and \u2013 more importantly \u2013 to disease symptoms (e.g., cold and heat sensations are assumed to be yin and yang symptoms, respectively). Thus, yin and yang of the body are seen as phenomena whose lack (or over-abundance) comes with characteristic symptom combinations:\nTCM also identifies drugs believed to treat these specific symptom combinations, i.e., to reinforce yin and yang.\nStrict rules are identified to apply to the relationships between the Five Phases in terms of sequence, of acting on each other, of counteraction, etc. All these aspects of Five Phases theory constitute the basis of the z\u00e0ng-f\u01d4 concept, and thus have great influence regarding the TCM model of the body. Five Phase theory is also applied in diagnosis and therapy.\nCorrespondences between the body and the universe have historically not only been seen in terms of the Five Elements, but also of the \"Great Numbers\" () For example, the number of acu-points has at times been seen to be 365, corresponding with the number of days in a year; and the number of main meridians\u201312\u2013has been seen as corresponding with the number of rivers flowing through the ancient Chinese empire.\nModel of the body.\nTCM \"holds that the body's vital energy (\"chi\" or \"qi\") circulates through channels, called \"jingluo\" (\"meridians and collaterals\"), that have branches connected to bodily organs and functions.\" Its view of the human body is only marginally concerned with anatomical structures, but focuses primarily on the body's \"functions\" (such as digestion, breathing, temperature maintenance, etc.):\nThese functions are aggregated and then associated with a primary functional entity \u2013 for instance, nourishment of the tissues and maintenance of their moisture are seen as connected functions, and the entity postulated to be responsible for these functions is xi\u011b (blood). These functional entities thus constitute \"concepts\" rather than something with biochemical or anatomical properties.\nThe primary functional entities used by traditional Chinese medicine are q\u00ec, xu\u011b, the five z\u00e0ng organs, the six f\u01d4 organs, and the meridians which extend through the organ systems. These are all theoretically interconnected: each z\u00e0ng organ is paired with a f\u01d4 organ, which are nourished by the blood and concentrate qi for a particular function, with meridians being extensions of those functional systems throughout the body.\nConcepts of the body and of disease used in TCM are pseudoscientific, similar to Mediterranean humoral theory. TCM's model of the body is characterized as full of pseudoscience. Some practitioners no longer consider yin and yang and the idea of an energy flow to apply. Scientific investigation has not found any histological or physiological evidence for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points. It is a generally held belief within the acupuncture community that acupuncture points and meridians structures are special conduits for electrical signals but no research has established any consistent anatomical structure or function for either acupuncture points or meridians. The scientific evidence for the anatomical existence of either meridians or acupuncture points is not compelling. Stephen Barrett of Quackwatch writes that, \"TCM theory and practice are not based upon the body of knowledge related to health, disease, and health care that has been widely accepted by the scientific community. TCM practitioners disagree among themselves about how to diagnose patients and which treatments should go with which diagnoses. Even if they could agree, the TCM theories are so nebulous that no amount of scientific study will enable TCM to offer rational care.\"\n\"Qi\".\n\"Qi\" is a polysemous word that traditional Chinese medicine distinguishes as being able to transform into many different qualities of \"qi\" (). In a general sense, \"qi\" is something that is defined by five \"cardinal functions\":\nA lack of \"qi\" will be characterized especially by pale complexion, lassitude of spirit, lack of strength, spontaneous sweating, laziness to speak, non-digestion of food, shortness of breath (especially on exertion), and a pale and enlarged tongue.\n\"Qi\" is believed to be partially generated from food and drink, and partially from air (by breathing). Another considerable part of it is inherited from the parents and will be consumed in the course of life.\nTCM uses special terms for \"qi\" running inside of the blood vessels and for qi that is distributed in the skin, muscles, and tissues between them. The former is called \"yingqi\" (); its function is to complement xu\u00e8 and its nature has a strong yin aspect (although \"qi\" in general is considered to be yang). The latter is called \"weiqi\" (); its main function is defence and it has pronounced yang nature.\n\"Qi\" is said to circulate in the meridians. Just as the \"qi\" held by each of the zang-fu organs, this is considered to be part of the 'principal' \"qi\" of the body.\nXie.\nIn contrast to the majority of other functional entities, or (, \"blood\") is correlated with a physical form \u2013 the red liquid running in the blood vessels. Its concept is, nevertheless, defined by its functions: nourishing all parts and tissues of the body, safeguarding an adequate degree of moisture, and sustaining and soothing both consciousness and sleep.\nTypical symptoms of a lack of (usually termed \"blood vacuity\" []) are described as: Pale-white or withered-yellow complexion, dizziness, flowery vision, palpitations, insomnia, numbness of the extremities; pale tongue; \"fine\" pulse.\n\"Jinye\".\nClosely related to xu\u011b are the \"jinye\" (, usually translated as \"body fluids\"), and just like xu\u011b they are considered to be yin in nature, and defined first and foremost by the functions of nurturing and moisturizing the different structures of the body. Their other functions are to harmonize yin and yang, and to help with the secretion of waste products.\n\"Jinye\" are ultimately extracted from food and drink, and constitute the raw material for the production of xu\u011b; conversely, xu\u011b can also be transformed into \"jinye\". Their palpable manifestations are all bodily fluids: tears, sputum, saliva, gastric acid, joint fluid, sweat, urine, etc.\n\"Zangfu\".\nThe \"zangfu\" () are the collective name of eleven entities (similar to organs) that constitute the centre piece of TCM's systematization of bodily functions. The term \"zang\" refers to the five considered to be yin in nature \u2013 Heart, Liver, Spleen, Lung, Kidney \u2013 while \"fu\" refers to the six associated with yang \u2013 Small Intestine, Large Intestine, Gallbladder, Urinary Bladder, Stomach and San Jiao. Despite having the names of organs, they are only loosely tied to (rudimentary) anatomical assumptions. Instead, they are primarily understood to be certain \"functions\" of the body. To highlight the fact that they are not equivalent to anatomical organs, their names are usually capitalized.\nThe \"zang\"'s essential functions consist in production and storage of \"qi\" and xu\u011b; they are said to regulate digestion, breathing, water metabolism, the musculoskeletal system, the skin, the sense organs, aging, emotional processes, and mental activity, among other structures and processes. The f\u01d4 organs' main purpose is merely to transmit and digest () substances such as waste and food.\nSince their concept was developed on the basis of W\u01d4 X\u00edng philosophy, each z\u00e0ng is paired with a f\u01d4, and each z\u00e0ng-f\u01d4 pair is assigned to one of five elemental qualities (i.e., the Five Elements or Five Phases). These correspondences are stipulated as:\nThe z\u00e0ng-f\u01d4 are also connected to the twelve standard meridians \u2013 each yang meridian is attached to a f\u01d4 organ, and five of the yin meridians are attached to a z\u00e0ng. As there are only five z\u00e0ng but six yin meridians, the sixth is assigned to the Pericardium, a peculiar entity almost similar to the Heart z\u00e0ng.\nJing-luo.\nThe meridians (, ) are believed to be channels running from the z\u00e0ng-f\u01d4 in the interior (, ) of the body to the limbs and joints (\"the surface\" [, ]), transporting qi and xu\u0115. TCM identifies 12 \"regular\" and 8 \"extraordinary\" meridians; the Chinese terms being (, lit. \"the Twelve Vessels\") and () respectively. There's also a number of less customary channels branching from the \"regular\" meridians.\nGender in traditional medicine.\n\"Fuke\" () is the traditional Chinese term for women's medicine (it means gynecology and obstetrics in modern medicine). However, there are few or no ancient works on it except for Fu Qingzhu's \"Fu Qingzhu Nu Ke\" (Fu Qingzhu's Gynecology ), with \"Nu Ke\" literally meaning \"Women studies\", much like the etymology of \"gynecology\". In traditional China, as in many other cultures, the health and medicine of female bodies was less understood than that of male bodies. Women's bodies were often secondary to male bodies, since women were thought of as the weaker, sicklier sex.\nIn clinical encounters, women and men were treated differently. Diagnosing women was not as simple as diagnosing men. First, when a woman fell ill, an appropriate adult man was to call the doctor and remain present during the examination, for the woman could not be left alone with the male doctor. The physician would discuss the female's problems and diagnosis only through the male. However, in certain cases, when a woman dealt with complications of pregnancy or birth, older women assumed the role of the formal authority. Men in these situations would not have much power to interfere. Second, women were often silent about their issues with doctors due to the societal expectation of female modesty when a male figure was in the room. Third, patriarchal society also caused doctors to call women and children patients \"the anonymous category of family members (\"Jia Ren\") or household (\"Ju Jia\")\" in their journals. This anonymity and lack of conversation between the doctor and woman patient led to the inquiry diagnosis of the Four Diagnostic Methods being the most challenging. Doctors used a medical doll known as a Doctor's lady, on which female patients could indicate the location of their symptoms.\nCheng Maoxian (b. 1581), who practiced medicine in Yangzhou, described the difficulties doctors had with the norm of female modesty. One of his case studies was that of Fan Jisuo's teenage daughter, who could not be diagnosed because she was unwilling to speak about her symptoms, since the illness involved discharge from her intimate areas. As Cheng describes, there were four standard methods of diagnosis \u2013 looking, asking, listening and smelling and touching (for pulse-taking). To maintain some form of modesty, women would often stay hidden behind curtains and screens. The doctor was allowed to touch enough of her body to complete his examination, often just the pulse taking. This would lead to situations where the symptoms and the doctor's diagnosis did not agree and the doctor would have to ask to view more of the patient.\nThese social and cultural beliefs were often barriers to learning more about female health, with women themselves often being the most formidable barrier. Women were often uncomfortable talking about their illnesses, especially in front of the male chaperones that attended medical examinations. Women would choose to omit certain symptoms as a means of upholding their chastity and honor. One such example is the case in which a teenage girl was unable to be diagnosed because she failed to mention her symptom of vaginal discharge. Silence was their way of maintaining control in these situations, but it often came at the expense of their health and the advancement of female health and medicine. This silence and control were most obviously seen when the health problem was related to the core of Ming \"fuke\", or the sexual body. It was often in these diagnostic settings that women would choose silence. In addition, there would be a conflict between patient and doctor on the probability of her diagnosis. For example, a woman who thought herself to be past the point of child-bearing age, might not believe a doctor who diagnoses her as pregnant. This only resulted in more conflict.\nYin yang and gender.\nYin and yang were critical to the understanding of women's bodies, but understood only in conjunction with male bodies. Yin and yang ruled the body, the body being a microcosm of the universe and the earth. In addition, gender in the body was understood as homologous, the two genders operating in synchronization. Gender was presumed to influence the movement of energy and a well-trained physician would be expected to read the pulse and be able to identify two dozen or more energy flows. Yin and yang concepts were applied to the feminine and masculine aspects of all bodies, implying that the differences between men and women begin at the level of this energy flow. According to \"Bequeathed Writings of Master Chu\" the male's yang pulse movement follows an ascending path in \"compliance [with cosmic direction] so that the cycle of circulation in the body and the Vital Gate are felt...The female's yin pulse movement follows a defending path against the direction of cosmic influences, so that the nadir and the Gate of Life are felt at the inch position of the left hand\". In sum, classical medicine marked yin and yang as high and low on bodies which in turn would be labeled normal or abnormal and gendered either male or female.\nBodily functions could be categorized through systems, not organs. In many drawings and diagrams, the twelve channels and their visceral systems were organized by yin and yang, an organization that was identical in female and male bodies. Female and male bodies were no different on the plane of yin and yang. Their gendered differences were not acknowledged in diagrams of the human body. Medical texts such as the \"Yuzuan yizong jinjian\" were filled with illustrations of male bodies or androgynous bodies that did not display gendered characteristics.\nAs in other cultures, fertility and menstruation dominate female health concerns. Since male and female bodies were governed by the same forces, traditional Chinese medicine did not recognize the womb as the place of reproduction. The abdominal cavity presented pathologies that were similar in both men and women, which included tumors, growths, hernias, and swellings of the genitals. The \"master system\", as Charlotte Furth calls it, is the kidney visceral system, which governed reproductive functions. Therefore, it was not the anatomical structures that allowed for pregnancy, but the difference in processes that allowed for the condition of pregnancy to occur.\nPregnancy.\nTraditional Chinese medicine's dealings with pregnancy are documented from at least the seventeenth century. According to Charlotte Furth, \"a pregnancy (in the seventeenth century) as a known bodily experience emerged [...] out of the liminality of menstrual irregularity, as uneasy digestion, and a sense of fullness\". These symptoms were common among other illness as well, so the diagnosis of pregnancy often came late in the term. The \"Canon of the Pulse\", which described the use of pulse in diagnosis, stated that pregnancy was \"a condition marked by symptoms of the disorder in one whose pulse is normal\" or \"where the pulse and symptoms do not agree\". Women were often silent about suspected pregnancy, which led to many men not knowing that their wife or daughter was pregnant until complications arrived. Complications through the misdiagnosis and the woman's reluctance to speak often led to medically induced abortions. Cheng Maoxian, Furth wrote, \"was unapologetic about endangering a fetus when pregnancy risked a mother's well being\". The method of abortion was the ingestion of certain herbs and foods. Disappointment at the loss of the fetus often led to family discord.\nPostpartum.\nIf the baby and mother survived the term of the pregnancy, childbirth was then the next step. The tools provided for birth were: towels to catch the blood, a container for the placenta, a pregnancy sash to support the belly, and an infant swaddling wrap. With these tools, the baby was born, cleaned, and swaddled; however, the mother was then immediately the focus of the doctor to replenish her \"qi\". In his writings, Cheng Maoxian places a large amount of emphasis on the Four Diagnostic methods to deal with postpartum issues and instructs all physicians to \"not neglect any [of the four methods]\". The process of birthing was thought to deplete a woman's blood level and \"qi\" so the most common treatments for postpartum were food (commonly garlic and ginseng), medicine, and rest. This process was followed up by a month check-in with the physician, a practice known as \"zuo yuezi\".\nInfertility.\nInfertility, not very well understood, posed serious social and cultural repercussions. The seventh-century scholar Sun Simiao is often quoted: \"those who have prescriptions for women's distinctiveness take their differences of pregnancy, childbirth and [internal] bursting injuries as their basis.\" Even in contemporary \"fuke\" placing emphasis on reproductive functions, rather than the entire health of the woman, suggests that the main function of \"fuke\" is to produce children.\nOnce again, the kidney visceral system governs the \"source \"Qi\"\", which governs the reproductive systems in both sexes. This source \"Qi\" was thought to \"be slowly depleted through sexual activity, menstruation and childbirth.\" It was also understood that the depletion of source Qi could result from the movement of an external pathology that moved through the outer visceral systems before causing more permanent damage to the home of source Qi, the kidney system. In addition, the view that only very serious ailments ended in the damage of this system means that those who had trouble with their reproductive systems or fertility were seriously ill.\nAccording to traditional Chinese medical texts, infertility can be summarized into different syndrome types. These were spleen and kidney depletion (yang depletion), liver and kidney depletion (yin depletion), blood depletion, phlegm damp, liver oppression, and damp heat. This is important because, while most other issues were complex in Chinese medical physiology, women's fertility issues were simple. Most syndrome types revolved around menstruation, or lack thereof. The patient was entrusted with recording not only the frequency, but also the \"volume, color, consistency, and odor of menstrual flow.\" This placed responsibility of symptom recording on the patient, and was compounded by the earlier discussed issue of female chastity and honor. This meant that diagnosing female infertility was difficult, because the only symptoms that were recorded and monitored by the physician were the pulse and color of the tongue.\nConcept of disease.\nIn general, disease is perceived as a disharmony (or imbalance) in the functions or interactions of yin, yang, qi, xu\u0115, z\u00e0ng-f\u01d4, meridians etc. and/or of the interaction between the human body and the environment. Therapy is based on which \"pattern of disharmony\" can be identified. Thus, \"pattern discrimination\" is the most important step in TCM diagnosis. It is also known to be the most difficult aspect of practicing TCM.\nTo determine which pattern is at hand, practitioners will examine things like the color and shape of the tongue, the relative strength of pulse-points, the smell of the breath, the quality of breathing or the sound of the voice. For example, depending on tongue and pulse conditions, a TCM practitioner might diagnose bleeding from the mouth and nose as: \"Liver fire rushes upwards and scorches the Lung, injuring the blood vessels and giving rise to reckless pouring of blood from the mouth and nose.\" He might then go on to prescribe treatments designed to clear heat or supplement the Lung.\nDisease entities.\nIn TCM, a disease has two aspects: \"b\u00ecng\" and \"zh\u00e8ng\". The former is often translated as \"disease entity\", \"disease category\", \"illness\", or simply \"diagnosis\". The latter, and more important one, is usually translated as \"pattern\" (or sometimes also as \"syndrome\"). For example, the disease entity of a common cold might present with a pattern of wind-cold in one person, and with the pattern of wind-heat in another.\nFrom a scientific point of view, most of the disease entities () listed by TCM constitute symptoms. Examples include headache, cough, abdominal pain, constipation etc.\nSince therapy will not be chosen according to the disease entity but according to the pattern, two people with the same disease entity but different patterns will receive different therapy. Vice versa, people with similar patterns might receive similar therapy even if their disease entities are different. This is called \"y\u00ec b\u00ecng t\u00f3ng zh\u00ec, t\u00f3ng b\u00ecng y\u00ec zh\u00ec\" ().\nPatterns.\nIn TCM, \"pattern\" () refers to a \"pattern of disharmony\" or \"functional disturbance\" within the functional entities of which the TCM model of the body is composed. There are disharmony patterns of qi, xu\u011b, the body fluids, the z\u00e0ng-f\u01d4, and the meridians. They are ultimately defined by their symptoms and signs (i.e., for example, pulse and tongue findings).\nIn clinical practice, the identified pattern usually involves a combination of affected entities (compare with typical examples of patterns). The concrete pattern identified should account for \"all\" the symptoms a person has.\nSix Excesses.\nThe Six Excesses (, sometimes also translated as \"Pathogenic Factors\", or \"Six Pernicious Influences\"; with the alternative term of , \u2013 \"Six Evils\" or \"Six Devils\") are allegorical terms used to describe disharmony patterns displaying certain typical symptoms. These symptoms resemble the effects of six climatic factors. In the allegory, these symptoms can occur because one or more of those climatic factors (called , \"the six qi\") were able to invade the body surface and to proceed to the interior. This is sometimes used to draw causal relationships (i.e., prior exposure to wind/cold/etc. is identified as the cause of a disease), while other authors explicitly deny a direct cause-effect relationship between weather conditions and disease, pointing out that the Six Excesses are primarily descriptions of a certain combination of symptoms translated into a pattern of disharmony. It is undisputed, though, that the Six Excesses can manifest inside the body without an external cause. In this case, they might be denoted \"internal\", e.g., \"internal wind\" or \"internal fire (or heat)\".\nThe Six Excesses and their characteristic clinical signs are:\nSix-Excesses-patterns can consist of only one or a combination of Excesses (e.g., wind-cold, wind-damp-heat). They can also transform from one into another.\nTypical examples of patterns.\nFor each of the functional entities (qi, xu\u0115, z\u00e0ng-f\u01d4, meridians etc.), typical disharmony patterns are recognized; for example: qi vacuity and qi stagnation in the case of qi; blood vacuity, blood stasis, and blood heat in the case of xu\u0115; Spleen qi vacuity, Spleen yang vacuity, Spleen qi vacuity with down-bearing qi, Spleen qi vacuity with lack of blood containment, cold-damp invasion of the Spleen, damp-heat invasion of Spleen and Stomach in case of the Spleen z\u00e0ng; wind/cold/damp invasion in the case of the meridians.\nTCM gives detailed prescriptions of these patterns regarding their typical symptoms, mostly including characteristic tongue and/or pulse findings. For example:\nEight principles of diagnosis.\nThe process of determining which actual pattern is on hand is called (, usually translated as \"pattern diagnosis\", \"pattern identification\" or \"pattern discrimination\"). Generally, the first and most important step in pattern diagnosis is an evaluation of the present signs and symptoms on the basis of the \"Eight Principles\" (). These eight principles refer to four pairs of fundamental qualities of a disease: exterior/interior, heat/cold, vacuity/repletion, and yin/yang. Out of these, heat/cold and vacuity/repletion have the biggest clinical importance. The yin/yang quality, on the other side, has the smallest importance and is somewhat seen aside from the other three pairs, since it merely presents a general and vague conclusion regarding what other qualities are found. In detail, the Eight Principles refer to the following:\nAfter the fundamental nature of a disease in terms of the Eight Principles is determined, the investigation focuses on more specific aspects. By evaluating the present signs and symptoms against the background of typical disharmony patterns of the various entities, evidence is collected whether or how specific entities are affected. This evaluation can be done\nThere are also three special pattern diagnosis systems used in case of febrile and infectious diseases only (\"Six Channel system\" or \"six division pattern\" []; \"Wei Qi Ying Xue system\" or \"four division pattern\" []; \"San Jiao system\" or \"three burners pattern\" []).\nConsiderations of disease causes.\nAlthough TCM and its concept of disease do not strongly differentiate between cause and effect, pattern discrimination can include considerations regarding the disease cause; this is called (, \"disease-cause pattern discrimination\").\nThere are three fundamental categories of disease causes () recognized:\nDiagnostics.\nIn TCM, there are five major diagnostic methods: inspection, auscultation, olfaction, inquiry, and palpation. These are grouped into what is known as the \"Four pillars\" of diagnosis, which are Inspection, Auscultation/ Olfaction, Inquiry, and Palpation ().\nTongue and pulse.\nExamination of the tongue and the pulse are among the principal diagnostic methods in TCM. Details of the tongue, including shape, size, color, texture, cracks, teeth marks, as well as tongue coating are all considered as part of tongue diagnosis. Various regions of the tongue's surface are believed to correspond to the z\u00e0ng-f\u016d organs. For example, redness on the tip of the tongue might indicate heat in the Heart, while redness on the sides of the tongue might indicate heat in the Liver.\nPulse palpation involves measuring the pulse both at a superficial and at a deep level at three different locations on the radial artery (\"Cun, Guan, Chi\", located two fingerbreadths from the wrist crease, one fingerbreadth from the wrist crease, and right at the wrist crease, respectively, usually palpated with the index, middle and ring finger) of each arm, for a total of twelve pulses, all of which are thought to correspond with certain z\u00e0ng-f\u016d. The pulse is examined for several characteristics including rhythm, strength and volume, and described with qualities like \"floating, slippery, bolstering-like, feeble, thready and quick\"; each of these qualities indicates certain disease patterns. Learning TCM pulse diagnosis can take several years.\nHerbal medicine.\nThe term \"herbal medicine\" is somewhat misleading in that, while plant elements are by far the most commonly used substances in TCM, other, non-botanic substances are used as well: animal, human, fungi, and mineral products are also used. Thus, the term \"medicinal\" (instead of herb) may be used. A 2019 review of traditional herbal treatments found they are widely used but lacking in scientific evidence, and urged a more rigorous approach by which genuinely useful medicinals might be identified.\nRaw materials.\nThere are roughly 13,000 compounds used in China and over 100,000 TCM recipes recorded in the ancient literature. Plant elements and extracts are by far the most common elements used. In the classic \"Handbook of Traditional Drugs\" from 1941, 517 drugs were listed \u2013 out of these, 45 were animal parts, and 30 were minerals.\nAnimal substances.\nSome animal parts used include cow gallstones, hornet nests, leeches, and scorpion. Other examples of animal parts include horn of the antelope or buffalo, deer antlers, testicles and penis bone of the dog, and snake bile. Some TCM textbooks still recommend preparations containing animal tissues, but there has been little research to justify the claimed clinical efficacy of many TCM animal products.\nSome compounds can include the parts of endangered species, including tiger bones and rhinoceros horn\nwhich is used for many ailments (though not as an aphrodisiac as is commonly misunderstood in the West).\nThe black market in rhinoceros horns (driven not just by TCM but also unrelated status-seeking) has reduced the world's rhino population by more than 90 percent over the past 40 years.\nConcerns have also arisen over the use of pangolin scales, turtle plastron, seahorses, and the gill plates of mobula and manta rays.\nPoachers hunt restricted or endangered species to supply the black market with TCM products. There is no scientific evidence of efficacy for tiger medicines. Concern over China considering to legalize the trade in tiger parts prompted the 171-nation Convention on International Trade in Endangered Species (CITES) to endorse a decision opposing the resurgence of trade in tigers. Fewer than 30,000 saiga antelopes remain, which are exported to China for use in traditional fever therapies. Organized gangs illegally export the horn of the antelopes to China. The pressures on seahorses (\"Hippocampus\" spp.) used in traditional medicine is enormous; tens of millions of animals are unsustainably caught annually. Many species of syngnathid are currently part of the IUCN Red List of Threatened Species or national equivalents.\nSince TCM recognizes bear bile as a treatment compound, more than 12,000 asiatic black bears are held in bear farms. The bile is extracted through a permanent hole in the abdomen leading to the gall bladder, which can cause severe pain. This can lead to bears trying to kill themselves. As of 2012, approximately 10,000 bears are farmed in China for their bile. This practice has spurred public outcry across the country. The bile is collected from live bears via a surgical procedure. As of March 2020 bear bile as ingredient of \"Tan Re Qing\" injection remains on the list of remedies recommended for treatment of \"severe cases\" of COVID-19 by National Health Commission of China and the National Administration of Traditional Chinese Medicine.\nThe deer penis is believed to have therapeutic benefits according to traditional Chinese medicine. Tiger parts from poached animals include tiger penis, believed to improve virility, and tiger eyes. The illegal trade for tiger parts in China has driven the species to near-extinction because of its popularity in traditional medicine. Laws protecting even critically endangered species such as the Sumatran tiger fail to stop the display and sale of these items in open markets. Shark fin soup is traditionally regarded in Chinese medicine as beneficial for health in East Asia, and its status as an elite dish has led to huge demand with the increase of affluence in China, devastating shark populations. The shark fins have been a part of traditional Chinese medicine for centuries. Shark finning is banned in many countries, but the trade is thriving in Hong Kong and China, where the fins are part of shark fin soup, a dish considered a delicacy, and used in some types of traditional Chinese medicine.\nThe tortoise (freshwater turtle, \"guiban\") and turtle (Chinese softshell turtle, \"biejia\") species used in traditional Chinese medicine are raised on farms, while restrictions are made on the accumulation and export of other endangered species. However, issues concerning the overexploitation of Asian turtles in China have not been completely solved. Australian scientists have developed methods to identify medicines containing DNA traces of endangered species. Finally, although not an endangered species, sharp rises in exports of donkeys and donkey hide from Africa to China to make the traditional remedy \"ejiao\" have prompted export restrictions by some African countries.\nHuman body parts.\nTraditional Chinese medicine also includes some human parts: the classic \"Materia medica\" (Bencao Gangmu) describes (also criticizes) the use of 35 human body parts and excreta in medicines, including bones, fingernail, hairs, dandruff, earwax, impurities on the teeth, feces, urine, sweat, organs, but most are no longer in use.\nHuman placenta has been used an ingredient in certain traditional Chinese medicines, including using dried human placenta, known as \"Ziheche\", to treat infertility, impotence and other conditions. The consumption of the human placenta is a potential source of infection.\nTraditional categorization.\nThe traditional categorizations and classifications that can still be found today are:\nEfficacy.\nAs of 2007[ [update]] there were not enough good-quality trials of herbal therapies to allow their effectiveness to be determined. A high percentage of relevant studies on traditional Chinese medicine are in Chinese databases. Fifty percent of systematic reviews on TCM did not search Chinese databases, which could lead to a bias in the results. Many systematic reviews of TCM interventions published in Chinese journals are incomplete, some contained errors or were misleading. The herbs recommended by traditional Chinese practitioners in the US are unregulated.\nDrug research.\nWith an eye to the enormous Chinese market, pharmaceutical companies have explored creating new drugs from traditional remedies. The journal \"Nature\" commented that \"claims made on behalf of an uncharted body of knowledge should be treated with the customary scepticism that is the bedrock of both science and medicine.\"\nThere was success in the 1970s, however, with the development of the antimalarial drug artemisinin, which is a chemical compound isolated the herb \"Artemisia annua\" that has been used traditionally as a treatment for fever. \"Artemisia annua\" has been used by Chinese herbalists in traditional Chinese medicines for 2,000 years. In 1596, Li Shizhen recommended tea made from qinghao specifically to treat malaria symptoms in his \"Compendium of Materia Medica\". Researcher Tu Youyou discovered that a low-temperature extraction process could isolate an effective antimalarial substance from the plant. Tu says she was influenced by a traditional Chinese herbal medicine source, \"The Handbook of Prescriptions for Emergency Treatments\", written in 340 by Ge Hong, which states that this herb should be steeped in cold water. The extracted substance, once subject to detoxification and purification processes, is a usable antimalarial drug \u2013 a 2012 review found that artemisinin-based remedies were the most effective drugs for the treatment of malaria. For her work on malaria, Tu received the 2015 Nobel Prize in Physiology or Medicine. Despite global efforts in combating malaria, it remains a large burden for the population. Although WHO recommends artemisinin-based remedies for treating uncomplicated malaria, resistance to the drug can no longer be ignored.\nAlso in the 1970s Chinese researcher Zhang TingDong and colleagues investigated the potential use of the traditionally used substance arsenic trioxide to treat acute promyelocytic leukemia (APL). Building on his work, research both in China and the West eventually led to the development of the drug Trisenox, which was approved for leukemia treatment by the FDA in 2000.\nHuperzine A, an extract from the herb, \"Huperzia serrata\", is under preliminary research as a possible therapeutic for Alzheimer's disease, but poor methodological quality of the research restricts conclusions about its effectiveness.\nEphedrine in its natural form, known as \"m\u00e1 hu\u00e1ng\" () in TCM, has been documented in China since the Han dynasty (206 BCE \u2013 220 CE) as an antiasthmatic and stimulant. In 1885, the chemical synthesis of ephedrine was first accomplished by Japanese organic chemist Nagai Nagayoshi based on his research on Japanese and Chinese traditional herbal medicines\nPien tze huang was first documented in the Ming dynasty.\nCost-effectiveness.\nA 2012 systematic review found there is a lack of available cost-effectiveness evidence in TCM.\nSafety.\nFrom the earliest records regarding the use of compounds to today, the toxicity of certain substances has been described in all Chinese materiae medicae. Since TCM has become more popular in the Western world, there are increasing concerns about the potential toxicity of many traditional Chinese plants, animal parts and minerals. Traditional Chinese herbal remedies are conveniently available from grocery stores in most Chinese neighborhoods; some of these items may contain toxic ingredients, are imported into the U.S. illegally, and are associated with claims of therapeutic benefit without evidence. For most compounds, efficacy and toxicity testing are based on traditional knowledge rather than laboratory analysis. The toxicity in some cases could be confirmed by modern research (i.e., in scorpion); in some cases it could not (i.e., in \"Curculigo\"). Traditional herbal medicines can contain extremely toxic chemicals and heavy metals, and naturally occurring toxins, which can cause illness, exacerbate pre-existing poor health or result in death. Botanical misidentification of plants can cause toxic reactions in humans. The description of some plants used in TCM has changed, leading to unintended poisoning by using the wrong plants. A concern is also contaminated herbal medicines with microorganisms and fungal toxins, including aflatoxin. Traditional herbal medicines are sometimes contaminated with toxic heavy metals, including lead, arsenic, mercury and cadmium, which inflict serious health risks to consumers. Also, adulteration of some herbal medicine preparations with conventional drugs which may cause serious adverse effects, such as corticosteroids, phenylbutazone, phenytoin, and glibenclamide, has been reported.\nSubstances known to be potentially dangerous include \"Aconitum\", secretions from the Asiatic toad, powdered centipede, the Chinese beetle (\"Mylabris phalerata\"), certain fungi, \"Aristolochia\" (which is known to cause cancer). arsenic sulfide (realgar), mercury sulfide, and cinnabar. Asbestos ore (Actinolite, Yang Qi Shi, \u9633\u8d77\u77f3) is used to treat impotence in TCM. Due to galena's (litharge, lead(II) oxide) high lead content, it is known to be toxic. Lead, mercury, arsenic, copper, cadmium, and thallium have been detected in TCM products sold in the U.S. and China.\nTo avoid its toxic adverse effects \"Xanthium sibiricum\" must be processed. Hepatotoxicity has been reported with products containing \"Reynoutria multiflora\" (synonym \"Polygonum multiflorum\"), glycyrrhizin, \"Senecio\" and \"Symphytum\". The herbs indicated as being hepatotoxic included \"Dictamnus dasycarpus\", \"Astragalus membranaceus\", and \"Paeonia lactiflora\". Contrary to popular belief, \"Ganoderma lucidum\" mushroom extract, as an adjuvant for cancer immunotherapy, appears to have the potential for toxicity. A 2013 review suggested that although the antimalarial herb \"Artemisia annua\" may not cause hepatotoxicity, haematotoxicity, or hyperlipidemia, it should be used cautiously during pregnancy due to a potential risk of embryotoxicity at a high dose.\nHowever, many adverse reactions are due to misuse or abuse of Chinese medicine. For example, the misuse of the dietary supplement \"Ephedra\" (containing ephedrine) can lead to adverse events including gastrointestinal problems as well as sudden death from cardiomyopathy. Products adulterated with pharmaceuticals for weight loss or erectile dysfunction are one of the main concerns. Chinese herbal medicine has been a major cause of acute liver failure in China.\nThe harvesting of guano from bat caves (\"yemingsha\") brings workers into close contact with these animals, increasing the risk of zoonosis. The Chinese virologist Shi Zhengli has identified dozens of SARS-like coronaviruses in samples of bat droppings.\nAcupuncture and moxibustion.\nAcupuncture is the insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) \u2013 usually at acupuncture points (acupoints) \u2013 and their subsequent manipulation; this aims at influencing the flow of qi. According to TCM it relieves pain and treats (and prevents) various diseases. The US FDA classifies single-use acupuncture needles as Class II medical devices, under CFR 21.\nAcupuncture is often accompanied by moxibustion \u2013 the Chinese characters for acupuncture () literally meaning \"acupuncture-moxibustion\" \u2013 which involves burning mugwort on or near the skin at an acupuncture point. According to the American Cancer Society, \"available scientific evidence does not support claims that moxibustion is effective in preventing or treating cancer or any other disease\".\nIn electroacupuncture, an electric current is applied to the needles once they are inserted, to further stimulate the respective acupuncture points.\nA recent historian of Chinese medicine remarked that it is \"nicely ironic that the specialty of acupuncture \u2013 arguably the most questionable part of their medical heritage for most Chinese at the start of the twentieth century \u2013 has become the most marketable aspect of Chinese medicine.\" She found that acupuncture as we know it today has hardly been in existence for sixty years. Moreover, the fine, filiform needle we think of as the acupuncture needle today was not widely used a century ago. Present day acupuncture was developed in the 1930s and put into wide practice only as late as the 1960s.\nEfficacy.\nA 2013 editorial in the American journal \"Anesthesia and Analgesia\" stated that acupuncture studies produced inconsistent results, (i.e. acupuncture relieved pain in some conditions but had no effect in other very similar conditions) which suggests the presence of false positive results. These may be caused by factors like biased study design, poor blinding, and the classification of electrified needles (a type of TENS) as a form of acupuncture. The inability to find consistent results despite more than 3,000 studies, the editorial continued, suggests that the treatment seems to be a placebo effect and the existing equivocal positive results are the type of noise one expects to see after a large number of studies are performed on an inert therapy. The editorial concluded that the best controlled studies showed a clear pattern, in which the outcome does not rely upon needle location or even needle insertion, and since \"these variables are those that define acupuncture, the only sensible conclusion is that acupuncture does not work.\"\nAccording to the US NIH National Cancer Institute, a review of 17,922 patients reported that real acupuncture relieved muscle and joint pain, caused by aromatase inhibitors, much better than sham acupuncture. Regarding cancer patients, the review hypothesized that acupuncture may cause physical responses in nerve cells, the pituitary gland, and the brain \u2013 releasing proteins, hormones, and chemicals that are proposed to affect blood pressure, body temperature, immune activity, and endorphin release.\nA 2012 meta-analysis concluded that the mechanisms of acupuncture \"are clinically relevant, but that an important part of these total effects is not due to issues considered to be crucial by most acupuncturists, such as the correct location of points and depth of needling ... [but is] ... associated with more potent placebo or context effects\". Commenting on this meta-analysis, both Edzard Ernst and David Colquhoun said the results were of negligible clinical significance.\nA 2011 overview of Cochrane reviews found evidence that suggests acupuncture is effective for some but not all kinds of pain. A 2010 systematic review found that there is evidence \"that acupuncture provides a short-term clinically relevant effect when compared with a waiting list control or when acupuncture is added to another intervention\" in the treatment of chronic low back pain. Two review articles discussing the effectiveness of acupuncture, from 2008 and 2009, have concluded that there is not enough evidence to conclude that it is effective beyond the placebo effect.\nAcupuncture is generally safe when administered using Clean Needle Technique (CNT). Although serious adverse effects are rare, acupuncture is not without risk. Severe adverse effects, including very rarely death (five case reports), have been reported.\nTui na.\nTui na () is a form of massage, based on the assumptions of TCM, from which shiatsu is thought to have evolved. Techniques employed may include thumb presses, rubbing, percussion, and assisted stretching.\n\"Qigong\".\nQ\u00ecg\u014dng () is a TCM system of exercise and meditation that combines regulated breathing, slow movement, and focused awareness, purportedly to cultivate and balance qi. One branch of qigong is qigong massage, in which the practitioner combines massage techniques with awareness of the acupuncture channels and points.\n\"Qi\" is air, breath, energy, or primordial life source that is neither matter or spirit. While \"Gong\" is a skillful movement, work, or exercise of the \"qi\".\nOther therapies.\nCupping.\nCupping () is a type of Chinese massage, consisting of placing several glass \"cups\" (open spheres) on the body. A match is lit and placed inside the cup and then removed before placing the cup against the skin. As the air in the cup is heated, it expands, and after placing in the skin, cools, creating lower pressure inside the cup that allows the cup to stick to the skin via suction. When combined with massage oil, the cups can be slid around the back, offering \"reverse-pressure massage\".\nGua sha.\nGua sha () is abrading the skin with pieces of smooth jade, bone, animal tusks or horns or smooth stones; until red spots then bruising cover the area to which it is done. It is believed that this treatment is for almost any ailment. The red spots and bruising take three to ten days to heal, there is often some soreness in the area that has been treated.\nDie-da.\nDi\u0113-d\u01ce () or Dit Da, is a traditional Chinese bone-setting technique, usually practiced by martial artists who know aspects of Chinese medicine that apply to the treatment of trauma and injuries such as bone fractures, sprains, and bruises. Some of these specialists may also use or recommend other disciplines of Chinese medical therapies if serious injury is involved. Such practice of bone-setting () is not common in the West.\nChinese food therapy.\nThe concepts \"yin\" and \"yang\" are associated with different classes of foods, and tradition considers it important to consume them in a balanced fashion. However, there is no scientific evidence supporting such claims, nor their implied notions.\nRegulations.\nMany governments have enacted laws to regulate TCM practice.\nAustralia.\nFrom 1 July 2012 Chinese medicine practitioners must be registered under the national registration and accreditation scheme with the Chinese Medicine Board of Australia and meet the Board's Registration Standards, to practice in Australia.\nCanada.\nTCM is regulated in five provinces in Canada: Alberta, British Columbia, Ontario, Quebec, and Newfoundland &amp; Labrador.\nChina (mainland).\nThe National Administration of Traditional Chinese Medicine was created in 1949, which then absorbed existing TCM management in 1986 with major changes in 1998.\nChina's National People's Congress Standing Committee passed the country's first law on TCM in 2016, which came into effect on 1 July 2017. The new law standardized TCM certifications by requiring TCM practitioners to (i) pass exams administered by provincial-level TCM authorities, and (ii) obtain recommendations from two certified practitioners. TCM products and services can be advertised only with approval from the local TCM authority.\nReady-to-use TCM preparations, also known as Chinese patent medicines, are regulated by the National Medical Products Administration (and its predecessor CFDA) similar to preparations used in modern medicine since 1984. The barrier for entry, however, is much lower than medications based on modern/non-TCM principles; the rules allow for omitting clinical testing in a variety of circumstances. As of 2025, the latest (2020) rules allow a simplified procedure for preparations derived from an approved list of \"classic prescriptions\".\nThe government-run healthcare system covers a number of TCM procedures and preparations. In 2021, a total of 7114.5 billion yuan went into healthcare, amounting for 6.59% of the year's national GDP. Of these, 1111.5 billion yuan went into covering costs associated with TCM preparations (0.97% of national GDP), with 592.4 billion yuan covering the actual medications.\nHong Kong.\nDuring British rule, Chinese medicine practitioners in Hong Kong were not recognized as \"medical doctors\", which meant they could not issue prescription drugs, give injections, and such. However, TCM practitioners could register and operate TCM as \"herbalists\".\nThe Chinese Medicine Council of Hong Kong, established in 1999, regulates compounds and professional standards for TCM practitioners. All TCM practitioners in Hong Kong are required to register with the council. The eligibility for registration includes a recognised 5-year university degree of TCM, a 30-week minimum supervised clinical internship, and passing the licensing exam.\nThe approved Chinese medicine institutions are Hong Kong University, Chinese University of Hong Kong and Hong Kong Baptist University.\nMacau.\nThe Portuguese Macau government seldom interfered in the affairs of Chinese society, including with regard to regulations on the practice of TCM. There were a few TCM pharmacies in Macau during the colonial period. In 1994, the Portuguese Macau government published Decree-Law no. 53/94/M that officially started to regulate the TCM market. After the sovereign handover, the Macau S.A.R. government also published regulations on the practice of TCM. In 2000, Macau University of Science and Technology and Nanjing University of Traditional Chinese Medicine established the Macau College of Traditional Chinese Medicine to offer a degree course in Chinese medicine.\nIn 2022, a new law regulating TCM, Law no. 11/2021, came into effect. The same law also repealed Decree-Law no. 53/94/M.\nIndonesia.\nAll traditional medicines, including TCM, are regulated by Indonesian Minister of Health Regulation of 2013 on traditional medicine. Traditional medicine license (\"Surat Izin Pengobatan Tradisional\" \u2013 SIPT) is granted to the practitioners whose methods are recognized as safe and may benefit health. The TCM clinics are registered but there is no explicit regulation for it. The only TCM method which is accepted by medical logic and is empirically proofed is acupuncture. The acupuncturists can get SIPT and participate in health care facilities.\nJapan.\nKampo or Kanp\u014d medicine (\u6f22\u65b9\u533b\u5b66, Kanp\u014d igaku), often known simply as Kanp\u014d (\u6f22\u65b9; Japanese medicine) literally means \"method from the Han period of Chinese history, but took on specific Japanese characteristics during the Edo period of Japanese history after 1600. One authority writes that Kampo medicine is not the same as modern traditional Chinese medicine (TCM). Japanese Kampo favors diagnostic methods that directly relate the symptoms to the therapy, rather than speculative concepts of traditional philosophy, such as Yin and Yang and the theory of the five elements.\nUnder modern Japanese medical law, it is possible for doctors to perform acupuncture and massage, but because there is a separate law regarding acupuncture and massage, these treatments are mainly performed by massage therapists, acupuncturists, and moxibustion practitioners.\nKorea.\nUnder the Medical Service Act (), an oriental medical doctor, whose obligation is to administer oriental medical treatment and provide guidance for health based on oriental medicine, shall be treated in the same manner as a medical doctor or dentist.\nThe Korea Institute of Oriental Medicine is the top research center of TCM in Korea.\nMalaysia.\nThe Traditional and Complementary Medicine Bill was passed by parliament in 2012 establishing the Traditional and Complementary Medicine Council to register and regulate traditional and complementary medicine practitioners, including TCM practitioners as well as other traditional and complementary medicine practitioners such as those in traditional Malay medicine and traditional Indian medicine.\nNetherlands.\nThere are no specific regulations in the Netherlands on TCM; TCM is neither prohibited nor recognised by the government of the Netherlands. Chinese herbs as well as Chinese herbal products that are used in TCM are classified as foods and food supplements, and these Chinese herbs can be imported into the Netherlands as well as marketed as such without any type registration or notification to the government.\nDespite its status, some private health insurance companies reimburse a certain amount of annual costs for acupuncture treatments, this depends on one's insurance policy, as not all insurance policies cover it, and if the acupuncture practitioner is or is not a member of one of the professional organisations that are recognised by private health insurance companies.\u00a0The recognized professional organizations include the Nederlandse Vereniging voor Acupunctuur (NVA), Nederlandse Artsen Acupunctuur Vereniging (NAAV), ZHONG, (Nederlandse Vereniging voor Traditionele Chinese Geneeskunde), Nederlandse Beroepsvereniging Chinese Geneeswijzen Yi (NBCG Yi), and Wetenschappelijke Artsen Vereniging voor Acupunctuur in Nederland (WAVAN).\nNew Zealand.\nAlthough there are no regulatory standards for the practice of TCM in New Zealand, in the year 1990, acupuncture was included in the Governmental Accident Compensation Corporation (ACC) Act. This inclusion granted qualified and professionally registered acupuncturists to provide subsidised care and treatment to citizens, residents, and temporary visitors for work or sports related injuries that occurred within and upon the land of New Zealand. The two bodies for the regulation of acupuncture and attainment of ACC treatment provider status in New Zealand are Acupuncture NZ and The New Zealand Acupuncture Standards Authority.\nSingapore.\nThe TCM Practitioners Act was passed by Parliament in 2000 and the TCM Practitioners Board was established in 2001 as a statutory board under the Ministry of Health, to register and regulate TCM practitioners. The requirements for registration include possession of a diploma or degree from a TCM educational institution/university on a gazetted list, either structured TCM clinical training at an approved local TCM educational institution or foreign TCM registration together with supervised TCM clinical attachment/practice at an approved local TCM clinic, and upon meeting these requirements, passing the Singapore TCM Physicians Registration Examination (STRE) conducted by the TCM Practitioners Board.\nIn 2024, Nanyang Technological University will offer the four-year Bachelor of Chinese Medicine programme, which is the first local programme accredited by the Ministry of Health.\nTaiwan.\nIn Taiwan, TCM practitioners are physicians and are regulated by the Physicians Act. They possess the authority to independently diagnose medical conditions, issue prescriptions, dispense Traditional Chinese Medicine, and prescribe a variety of diagnostic tests including X-rays, ECG, and blood and urine test.\nUnder current law, those who wish to qualify for the Chinese medicine exam must have obtained a 7-year university degree in TCM.\nThe National Research Institute of Chinese Medicine, established in 1963, is the largest Chinese herbal medicine research center in Taiwan.\nUnited States.\nAs of July 2012, only six states lack legislation to regulate the professional practice of TCM: Alabama, Kansas, North Dakota, South Dakota, Oklahoma, and Wyoming. In 1976, California established an Acupuncture Board and became the first state licensing professional acupuncturists.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5993", "revid": "32452", "url": "https://en.wikipedia.org/wiki?curid=5993", "title": "Chemical bond", "text": "Association of atoms to form chemical compounds\nA chemical bond is the association of atoms or ions to form molecules, crystals, and other structures. The bond may result from the electrostatic force between oppositely charged ions as in ionic bonds or through the sharing of electrons as in covalent bonds, or some combination of these effects. Chemical bonds are described as having different strengths: there are \"strong bonds\" or \"primary bonds\" such as covalent, ionic and metallic bonds, and \"weak bonds\" or \"secondary bonds\" such as dipole\u2013dipole interactions, the London dispersion force, and hydrogen bonding. \nSince opposite electric charges attract, the negatively charged electrons surrounding the nucleus and the positively charged protons within a nucleus attract each other. Electrons shared between two nuclei will be attracted to both of them. \"Constructive quantum mechanical wavefunction interference\" stabilizes the paired nuclei (see Theories of chemical bonding). Bonded nuclei maintain an optimal distance (the bond distance) balancing attractive and repulsive effects explained quantitatively by quantum theory.\nThe atoms in molecules, crystals, metals and other forms of matter are held together by chemical bonds, which determine the structure and properties of matter.\nAll bonds can be described by quantum theory, but, in practice, simplified rules and other theories allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are examples. More sophisticated theories are valence bond theory, which includes orbital hybridization and resonance, and molecular orbital theory which includes the linear combination of atomic orbitals and ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.\nOverview of main types of chemical bonds.\nA chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost or valence electrons of atoms. These behaviors merge into each other seamlessly in various circumstances, so that there is no clear line to be drawn between them. However it remains useful and customary to differentiate between different types of bond, which result in different properties of condensed matter.\nIn the simplest view of a covalent bond, one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. Energy is released by bond formation. This is not as a result of reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the release of energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e. longer de Broglie wavelength) orbital compared with each electron being confined closer to its respective nucleus. These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.\nIn a polar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms called molecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such as nylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such as diamond or quartz or the silicate minerals in many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds. Also, the melting points of such covalent polymers and networks increase greatly.\nIn a simplified view of an \"ionic\" bond, the bonding electron is not shared at all, but transferred. In this type of bond, the outer atomic orbital of one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. The \"bond\" then results from electrostatic attraction between the positive and negatively charged ions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.\nA less often mentioned type of bonding is \"metallic\" bonding. In this type of bonding, each atom in a metal donates one or more electrons to a \"sea\" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of its wave nature) to be associated with a great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example of delocalization of electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in the tensile strength of metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The cloud of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their shiny lustre that reflects most frequencies of white light.\nHistory.\nEarly speculations about the nature of the chemical bond, from as early as the 12th century, supposed that certain types of chemical species were joined by a type of chemical affinity. In 1704, Sir Isaac Newton famously outlined his atomic bonding theory, in \"Query 31\" of his \"Opticks\", whereby atoms attach to each other by some \"force\". Specifically, after acknowledging the various popular theories in vogue at the time, of how atoms were reasoned to attach to each other, i.e. \"hooked atoms\", \"glued together by rest\", or \"stuck together by conspiring motions\", Newton states that he would rather infer from their cohesion, that \"particles attract one another by some force, which in immediate contact is exceedingly strong, at small distances performs the chemical operations, and reaches not far from the particles with any sensible effect.\"\nIn 1819, on the heels of the invention of the voltaic pile, J\u00f6ns Jakob Berzelius developed a theory of chemical combination stressing the electronegative and electropositive characters of the combining atoms. By the mid 19th century, Edward Frankland, F.A. Kekul\u00e9, A.S. Couper, Alexander Butlerov, and Hermann Kolbe, building on the theory of radicals, developed the theory of valency, originally called \"combining power\", in which compounds were joined owing to an attraction of positive and negative poles. In 1904, Richard Abegg proposed his rule that the difference between the maximum and minimum valencies of an element is often eight. At this point, valency was still an empirical number based only on chemical properties.\nHowever the nature of the atom became clearer with Ernest Rutherford's 1911 discovery that of an atomic nucleus surrounded by electrons in which he quoted Nagaoka rejected Thomson's model on the grounds that opposite charges are impenetrable. In 1904, Nagaoka proposed an alternative planetary model of the atom in which a positively charged center is surrounded by a number of revolving electrons, in the manner of Saturn and its rings.\nNagaoka's model made two predictions:\nRutherford mentions Nagaoka's model in his 1911 paper in which the atomic nucleus is proposed.\nAt the 1911 Solvay Conference, in the discussion of what could regulate energy differences between atoms, Max Planck stated: \"The intermediaries could be the electrons.\" These nuclear models suggested that electrons determine chemical behavior.\nNext came Niels Bohr's 1913 model of a nuclear atom with electron orbits. In 1916, chemist Gilbert N. Lewis developed the concept of electron-pair bonds, in which two atoms may share one to six electrons, thus forming the single electron bond, a single bond, a double bond, or a triple bond; in Lewis's own words, \"An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively.\"\nAlso in 1916, Walther Kossel put forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model of ionic bonding. Both Lewis and Kossel structured their bonding models on that of Abegg's rule (1904).\nNiels Bohr also proposed a model of the chemical bond in 1913. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion \u2013 the electrons in the ring are at the maximum distance from each other.\nIn 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion, H2+, was derived by the Danish physicist \u00d8yvind Burrau. This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year by Walter Heitler and Fritz London. The Heitler\u2013London method forms the basis of what is now called valence bond theory. In 1929, the linear combination of atomic orbitals molecular orbital method (LCAO) approximation was introduced by Sir John Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F2 (fluorine) and O2 (oxygen) molecules, from basic quantum principles. This molecular orbital theory represented a covalent bond as an orbital formed by combining the quantum mechanical Schr\u00f6dinger atomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e., \"analytically\"), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modern quantum chemistry use either valence bond or molecular orbital theory as a starting point, although a third approach, density functional theory, has become increasingly popular in recent years.\nIn 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons. With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.\nBonds in chemical formulas.\nBecause atoms and molecules are three-dimensional, it is difficult to use a single method to indicate orbitals and bonds. In molecular formulas the chemical bonds (binding orbitals) between atoms are indicated in different ways depending on the type of discussion. Sometimes, some details are neglected. For example, in organic chemistry one is sometimes concerned only with the functional group of the molecule. Thus, the molecular formula of ethanol may be written in conformational form, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH3\u2013CH2\u2013OH), by separating the functional group from another part of the molecule (C2H5OH), or by its atomic constituents (C2H6O), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, e.g. for elemental carbon .'C'. Some chemists may also mark the respective orbitals, e.g. the hypothetical ethene\u22124 anion (\\/C=C/\\ \u22124) indicating the possibility of bond formation.\nStrong chemical bonds.\nStrong chemical bonds are the \"intramolecular\" forces that hold atoms together in molecules. A strong chemical bond is formed from the transfer or sharing of electrons between atomic centers and relies on the electrostatic attraction between the protons in nuclei and the electrons in the orbitals.\nThe types of strong bond differ due to the difference in electronegativity of the constituent elements. Electronegativity is the tendency for an atom of a given chemical element to attract shared electrons when forming a chemical bond, where the higher the associated electronegativity then the more it attracts electrons. Electronegativity serves as a simple way to quantitatively estimate the bond energy, which characterizes a bond along the continuous scale from covalent to ionic bonding. A large difference in electronegativity leads to more polar (ionic) character in the bond.\nIonic bond.\nIonic bonding is a type of electrostatic interaction between atoms that have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but an electronegativity difference of over 1.7 is likely to be ionic while a difference of less than 1.7 is likely to be covalent. Ionic bonding leads to separate positive and negative ions. Ionic charges are commonly between \u22123e to +3e. Ionic bonding commonly occurs in metal salts such as sodium chloride (table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques as X-ray diffraction.\nIonic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids such as sodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodium cations (Na+) and the cyanide anions (CN\u2212) are \"ionic\", with no sodium ion associated with any particular cyanide. However, the bonds between the carbon (C) and nitrogen (N) atoms in cyanide are of the \"covalent\" type, so that each carbon is strongly bound to \"just one\" nitrogen, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.\nWhen such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN\u2212 ions, move independently through the solution, as do sodium ions, as Na+. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules than to each other. The attraction between ions and water molecules in such solutions is due to a type of weak dipole-dipole type chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.\nCovalent bond.\nCovalent bonding is a common type of bonding in which two or more atoms share valence electrons more or less equally. The simplest and most common type is a single bond in which two atoms share two electrons. Other types include the double bond, the triple bond, one- and three-electron bonds, the three-center two-electron bond and three-center four-electron bond.\nIn non-polar covalent bonds, the electronegativity difference between the bonded atoms is small, typically 0 to 0.3. Bonds within most organic compounds are described as covalent. The figure shows methane (CH4), in which each hydrogen forms a covalent bond with the carbon. See sigma bonds and pi bonds for LCAO descriptions of such bonding.\nMolecules that are formed primarily from non-polar covalent bonds are often immiscible in water or other polar solvents, but much more soluble in non-polar solvents such as hexane.\nA polar covalent bond is a covalent bond with a significant ionic character. This means that the two shared electrons are closer to one of the atoms than the other, creating an imbalance of charge. Such bonds occur between two atoms with moderately different electronegativities and give rise to dipole\u2013dipole interactions. The electronegativity difference between the two atoms in these bonds is 0.3 to 1.7.\nSingle and multiple bonds.\nA single bond between two atoms corresponds to the sharing of one pair of electrons. The Hydrogen (H) atom has one valence electron. Two Hydrogen atoms can then form a molecule, held together by the shared pair of electrons. Each H atom now has the noble gas electron configuration of helium (He). The pair of shared electrons forms a single covalent bond. The electron density of these two bonding electrons in the region between the two atoms increases from the density of two non-interacting H atoms.\nA double bond has two shared pairs of electrons, one in a sigma bond and one in a pi bond with electron density concentrated on two opposite sides of the internuclear axis. A triple bond consists of three shared electron pairs, forming one sigma and two pi bonds. An example is nitrogen. Quadruple and higher bonds are very rare and occur only between certain transition metal atoms.\nCoordinate covalent bond (dipolar bond).\nA coordinate covalent bond is a covalent bond in which the two shared bonding electrons are from the same one of the atoms involved in the bond. For example, boron trifluoride (BF3) and ammonia (NH3) form an adduct or coordination complex F3B\u2190NH3 with a B\u2013N bond in which a lone pair of electrons on N is shared with an empty atomic orbital on B. BF3 with an empty orbital is described as an electron pair acceptor or Lewis acid, while NH3 with a lone pair that can be shared is described as an electron-pair donor or Lewis base. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding is shown by an arrow pointing to the Lewis acid. (In the Figure, solid lines are bonds in the plane of the diagram, wedged bonds point towards the observer, and dashed bonds point away from the observer.)\nTransition metal complexes are generally bound by coordinate covalent bonds. For example, the ion Ag+ reacts as a Lewis acid with two molecules of the Lewis base NH3 to form the complex ion Ag(NH3)2+, which has two Ag\u2190N coordinate covalent bonds.\nMetallic bonding.\nIn metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The free movement or delocalization of bonding electrons leads to classical metallic properties such as luster (surface light reflectivity), electrical and thermal conductivity, ductility, and high tensile strength.\nIntermolecular bonding.\nThere are several types of weak bonds that can be formed between two or more molecules which are not covalently bound. Intermolecular forces cause molecules to attract or repel each other. Often, these forces influence physical characteristics (such as the melting point) of a substance.\nVan der Waals forces are interactions between closed-shell molecules. They include both Coulombic interactions between partial charges in polar molecules, and Pauli repulsions between closed electrons shells.\nKeesom forces are the forces between the permanent dipoles of two polar molecules.701 London dispersion forces are the forces between induced dipoles of different molecules.703 There can also be an interaction between a permanent dipole in one molecule and an induced dipole in another molecule.702\nHydrogen bonds of the form A--H\u2022\u2022\u2022B occur when A and B are two highly electronegative atoms (usually N, O or F) such that A forms a highly polar covalent bond with H so that H has a partial positive charge, and B has a lone pair of electrons which is attracted to this partial positive charge and forms a hydrogen bond.702 Hydrogen bonds are responsible for the high boiling points of water and ammonia with respect to their heavier analogues. In some cases a similar halogen bond can be formed by a halogen atom located between two electronegative atoms on different molecules.\nAt short distances, repulsive forces between atoms also become important.705-6\nTheories of chemical bonding.\nIn the (unrealistic) limit of \"pure\" ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood by classical physics. The force between the atoms depends on isotropic continuum electrostatic potentials. The magnitude of the force is in simple proportion to the product of the two ionic charges according to Coulomb's law.\nCovalent bonds are better understood by valence bond (VB) theory or molecular orbital (MO) theory. The properties of the atoms involved can be understood using concepts such as oxidation number, formal charge, and electronegativity. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, bonding is conceptualized as being built up from electron pairs that are localized and shared by two atoms via the overlap of atomic orbitals. The concepts of orbital hybridization and resonance augment this basic notion of the electron pair bond. In molecular orbital theory, bonding is viewed as being delocalized and apportioned in orbitals that extend throughout the molecule and are adapted to its symmetry properties, typically by considering linear combinations of atomic orbitals (LCAO). Valence bond theory is more chemically intuitive by being spatially localized, allowing attention to be focused on the parts of the molecule undergoing chemical change. In contrast, molecular orbitals are more \"natural\" from a quantum mechanical point of view, with orbital energies being physically significant and directly linked to experimental ionization energies from photoelectron spectroscopy. Consequently, valence bond theory and molecular orbital theory are often viewed as competing but complementary frameworks that offer different insights into chemical systems. As approaches for electronic structure theory, both MO and VB methods can give approximations to any desired level of accuracy, at least in principle. However, at lower levels, the approximations differ, and one approach may be better suited for computations involving a particular system or property than the other.\nUnlike the spherically symmetrical Coulombic forces in pure ionic bonds, covalent bonds are generally directed and anisotropic. These are often classified based on their symmetry with respect to a molecular plane as sigma bonds and pi bonds. In the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relative electronegativity of the atoms involved. Bonds of this type are known as polar covalent bonds.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5995", "revid": "18779361", "url": "https://en.wikipedia.org/wiki?curid=5995", "title": "Cell", "text": "Cell most often refers to:\nCell may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5999", "revid": "48731005", "url": "https://en.wikipedia.org/wiki?curid=5999", "title": "Climate", "text": "Long-term weather pattern of a region\nClimate is the long-term weather pattern in a region, typically averaged over 30 years. More rigorously, it is the mean and variability of meteorological variables over a time spanning from months to millions of years. Some of the meteorological variables that are commonly measured are temperature, humidity, atmospheric pressure, wind, and precipitation. In a broader sense, climate is the state of the components of the climate system, including the atmosphere, hydrosphere, cryosphere, lithosphere and biosphere and the interactions between them. The climate of a location is affected by its latitude, longitude, terrain, altitude, land use and nearby water bodies and their currents.\nClimates can be classified according to the average and typical variables, most commonly temperature and precipitation. The most widely used classification scheme is the K\u00f6ppen climate classification. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change affects it. The major classifications in Thornthwaite's climate classification are microthermal, mesothermal, and megathermal. Finally, the Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.\nPaleoclimatology is the study of ancient climates. Paleoclimatologists seek to explain climate variations for all parts of the Earth during any given geologic period, beginning with the time of the Earth's formation. Since very few direct observations of climate were available before the 19th century, paleoclimates are inferred from proxy variables. They include non-biotic evidence\u2014such as sediments found in lake beds and ice cores\u2014and biotic evidence\u2014such as tree rings and coral. Climate models are mathematical models of past, present, and future climates. Climate change may occur over long and short timescales due to various factors. Recent warming is discussed in terms of global warming, which results in redistributions of biota. For example, as climate scientist Lesley Ann Hughes has written: \"a 3\u00a0\u00b0C [5\u00a0\u00b0F] change in mean annual temperature corresponds to a shift in isotherms of approximately in latitude (in the temperate zone) or in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones.\"\nDefinition.\nClimate (from grc \" \u03ba\u03bb\u03af\u03bc\u03b1\"\u00a0'inclination') is commonly defined as the weather averaged over a long period. The standard averaging period is 30\u00a0years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Climate in a narrow sense is usually defined as the \"average weather\", or more rigorously, as the statistical description in terms of the mean and variability of relevant quantities over a period ranging from months to thousands or millions of years. The classical period is 30\u00a0years, as defined by the World Meteorological Organization (WMO). These quantities are most often surface variables such as temperature, precipitation, and wind. Climate in a wider sense is the state, including a statistical description, of the climate system.\"\nThe World Meteorological Organization (WMO) describes \"climate normals\" as \"reference points used by climatologists to compare current climatological trends to that of the past or what is considered typical. A climate normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30-year period is used as it is long enough to filter out any interannual variation or anomalies such as El Ni\u00f1o\u2013Southern Oscillation, but also short enough to be able to show longer climatic trends.\"\nThe WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting, the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982, the WMO agreed to update climate normals, and these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990. The 1961\u20131990 climate normals serve as the baseline reference period. The next set of climate normals to be published by WMO is from 1991 to 2020. Aside from collecting from the most common atmospheric variables (air temperature, pressure, precipitation and wind), other variables such as humidity, visibility, cloud amount, solar radiation, soil temperature, pan evaporation rate, days with thunder and days with hail are also collected to measure change in climate conditions.\nThe difference between climate and weather is usefully summarized by the popular phrase \"Climate is what you expect, weather is what you get.\" Over historical time spans, there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. All of these variables change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5\u00a0\u00b0C (9\u00a0\u00b0F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases (particularly carbon dioxide and methane) determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.\nClimate classification.\nClimate classifications are systems that categorize the world's climates. A climate classification may correlate closely with a biome classification, as climate is a major influence on life in a region. One of the most used is the K\u00f6ppen climate classification scheme first developed in 1899.\nThere are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into \"genetic\" methods, which focus on the causes of climate, and \"empiric\" methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the K\u00f6ppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.\nRecord.\nPaleoclimatology.\nPaleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence with different time scales (from decades to millennia) from ice sheets, tree rings, sediments, pollen, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.\nModern.\nDetails of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their observation frequency, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past. Long-term modern climate records skew towards population centres and affluent countries. Since the 1960s, the launch of satellites allow records to be gathered on a global scale, including areas with little to no human presence, such as the Arctic region and oceans.\nClimate variability.\nClimate variability is the term to describe variations in the mean state and other characteristics of climate (such as chances or possibility of extreme weather, etc.) \"on all spatial and temporal scales beyond that of individual weather events.\" Some of the variability does not appear to be caused systematically and occurs at random times. Such variability is called \"random variability\" or \"noise\". On the other hand, periodic variability occurs relatively regularly and in distinct modes of variability or climate patterns.\nThere are close correlations between Earth's climate oscillations and astronomical factors (barycenter changes, solar variation, cosmic ray flux, cloud albedo feedback, Milankovic cycles), and modes of heat distribution between the ocean-atmosphere climate system. In some cases, current, historical and paleoclimatological natural oscillations may be masked by significant volcanic eruptions, impact events, irregularities in climate proxy data, positive feedback processes or anthropogenic emissions of substances such as greenhouse gases.\nOver the years, the definitions of \"climate variability\" and the related term \"climate change\" have shifted. While the term \"climate change\" now implies change that is both long-term and of human causation, in the 1960s the word climate change was used for what we now describe as climate variability, that is, climatic inconsistencies and anomalies.\nClimate change.\nClimate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or human activities, as found recently. Scientists have identified Earth's Energy Imbalance (EEI) to be a fundamental metric of the status of global change.\nIn recent usage, especially in the context of environmental policy, the term \"climate change\" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses \"climate variability\" for non-human caused variations.\nEarth has undergone periodic climate shifts in the past, including four major ice ages. These consist of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism. However, these naturally caused changes in climate occur on a much slower time scale than the present rate of change which is caused by the emission of greenhouse gases by human activities.\nAccording to the EU's Copernicus Climate Change Service, average global air temperature has passed 1.5C of warming the period from February 2023 to January 2024.\nClimate models.\nClimate models use quantitative methods to simulate the interactions and transfer of radiative energy between the atmosphere, oceans, land surface and ice through a series of physics equations. They are used for a variety of purposes, from the study of the dynamics of the weather and climate system to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the Earth with outgoing energy as long wave (infrared) electromagnetic radiation from the Earth. Any imbalance results in a change in the average temperature of the Earth.\nClimate models are available on different resolutions ranging from &gt;100\u00a0km to 1\u00a0km. High resolutions in global climate models require significant computational resources, and so only a few global datasets exist. Global climate models can be dynamically or statistically downscaled to regional climate models to analyze impacts of climate change on a local scale. Examples are ICON or mechanistically downscaled data such as CHELSA (Climatologies at high resolution for the earth's land surface areas).\nThe most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.\nModels can range from relatively simple to quite complex. Simple radiant heat transfer models treat the Earth as a single point and average outgoing energy. This can be expanded vertically (as in radiative-convective models), or horizontally. Finally, more complex (coupled) atmosphere\u2013ocean\u2013sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6000", "revid": "49402000", "url": "https://en.wikipedia.org/wiki?curid=6000", "title": "History of the Comoros", "text": "The history of the Comoros extends back to about 800\u20131000 AD when the archipelago was first inhabited. The Comoros have been inhabited by various groups and sultanates throughout this time. France colonised the islands in the 19th century, and they became independent in 1975.\nPrehistory.\nThe Comoros archipelago, of volcanic origin, was formed during the Cenozoic era. Seismic reflection data acquired during the SISMAORE survey indicate that Mayotte is the oldest edifice, with volcanism starting around 26.5 Ma; at the other end, Grande Comore remains active, dominated by Mount Karthala and a recent tectono-volcanic field identified offshore around the archipelago.\nBefore the first human settlements, the archipelago likely had no native terrestrial mammals, apart from fruit bats of the genus \"Pteropus\" and other chiropterans, natural colonizers of oceanic islands.\nEarly settlement and migrations.\nGenetic studies based on uniparental markers indicate that the Comorian population has a tri-continental ancestry\u2014African, Island Southeast Asian, and Middle Eastern\u2014resulting from early admixture events that began during the first millennium CE.\nAustronesian settlement (8th\u201313th centuries CE).\nMultiple lines of evidence suggest that the Comoros were initially settled by Austronesian-speaking sailors, probably from Island Southeast Asia, between the 8th and 13th centuries CE, in parallel with the colonisation of Madagascar.\nArchaeobotanical findings from early sites such as Sima (Anjouan) show Asian crops including rice (\"indica\" and \"japonica\"), mung bean and cotton predominating over African staples, supporting an Island Southeast Asian origin. \nGenome-wide analyses identify an Austronesian genetic contribution of around 20 percent in Comorian populations, with variation between islands. \nDembeni phase and early Islam (9th\u201312th centuries).\nArchaeology identifies a Dembeni cultural horizon (Mayotte and neighboring islands) with multiple sites (9th\u201310th and 11th\u201312th centuries) engaged in long-distance Indian Ocean trade. Finds include early Chinese and Persian ceramics and abundant Malagasy rock crystal, indicating wealth and integration into Abbasid then Fatimid trade circuits; funerary evidence (Mecca-oriented burials) points to early Islamisation by the 11th\u201312th centuries.\nBantu\u2013Swahili integrations and regional networks (late 1st millennium onward).\nFrom the late first millennium, Bantu-speaking groups from the East African coast settled and intermingled with earlier Austronesian settlers, embedding the islands within Swahili cultural and commercial networks across the western Indian Ocean.\nShirazi and Hadhrami influences (15th\u201316th centuries).\nBy the 15th century, Persian-Gulf\u2013linked Shirazi elites and merchants established ports and ruling houses, consolidating Comorian links to the Swahili corridor; in the 16th century, Hadhrami lineages reinforced these dynasties through kinship and religious authority.\nOver the centuries, the Comoros have been settled by a succession of diverse groups from the coast of Africa, the Persian Gulf, Southeast Asia and Madagascar.\nEuropean contact and expansion.\nPortuguese explorers first visited the archipelago in 1505.\nApart from a visit by the French Parmentier brothers in 1529, for much of the 16th century the only Europeans to visit the islands were Portuguese. British and Dutch ships began arriving around the start of the 17th century and the island of Ndzwani soon became a major supply point on the route to the East Indies. Ndzwani was generally ruled by a single sultan, who occasionally attempted to extend his authority to Mayotte and Mwali; Ngazidja was more fragmented, on occasion being divided into as many as 12 small kingdoms.\nSir James Lancaster's voyage to the Indian Ocean in 1591 was the first attempt by the English to break into the spice trade, which was dominated by the Portuguese. Only one of his four ships made it back from the Indies on that voyage, and that one with a decimated crew of 5 men and a boy. Lancaster himself was marooned by a cyclone on the Comoros. Many of his crew were speared to death by angry islanders although Lancaster found his way home in 1594. (Dalrymple W. 2019; Bloomsbury Publishing ).\nBoth the British and the French turned their attention to the Comoros islands in the middle of the 19th century. The French finally acquired the islands through a cunning mixture of strategies, including the policy of \"divide and conquer\", chequebook politics and a serendipitous affair between a sultana and a French trader that was put to good use by the French, who kept control of the islands, quelling unrest and the occasional uprising.\nWilliam Sunley, a planter and British Consul from 1848 to 1866, was an influence on Anjouan.\nFrench Comoros.\nFrance's presence in the western Indian Ocean dates to the early 17th century. The French established a settlement in southern Madagascar in 1634 and occupied the islands of R\u00e9union and Rodrigues; in 1715 France claimed Mauritius (), and in 1756 Seychelles. When France ceded Mauritius, Rodrigues, and Seychelles to Britain in 1814, it lost its Indian Ocean ports; Reunion, which remained French, did not offer a suitable natural harbor. In 1840 France acquired the island of Nosy-Be off the northwestern coast of Madagascar, but its potential as a port was limited. In 1841 the governor of Reunion, Admiral de Hell, negotiated with Andrian Souli, the Malagasy ruler of Mayotte, to cede Mayotte to France. Mahore offered a suitable site for port facilities, and its acquisition was justified by de Hell on the grounds that if France did not act, Britain would occupy the island.\nAlthough France had established a foothold in Comoros, the acquisition of the other islands proceeded fitfully. At times the French were spurred on by the threat of British intervention, especially on Nzwani, and at other times, by the constant anarchy resulting from the sultans' wars upon each other. In the 1880s, Germany's growing influence on the East African coast added to the concerns of the French. Not until 1908, however, did the four Comoro Islands become part of France's colony of Madagascar and not until 1912 did the last sultan abdicate. Then, a colonial administration took over the islands and established a capital at Dzaoudzi on Mahore. Treaties of protectorate status marked a transition point between independence and annexation; such treaties were signed with the rulers of Njazidja, Nzwani, and Mwali in 1886.\nThe effects of French colonialism were mixed, at best. Colonial rule brought an end to the institution of Slavery in the Comoros, but economic and social differences between former slaves and free persons and their descendants persisted. Health standards improved with the introduction of modern medicine, and the population increased about 50 percent between 1900 and 1960. France continued to dominate the economy. Food crop cultivation was neglected as French (companies) established cash crop plantations in the coastal regions. The result was an economy dependent on the exporting of vanilla, ylang-ylang, cloves, cocoa, copra, and other tropical crops. Most profits obtained from exports were diverted to France \nrather than invested in the infrastructure of the islands. Development was further limited by the colonial government's practice of concentrating public services on Madagascar. One consequence of this policy was the migration of large numbers of Comorans to Madagascar, where their presence would be a long-term source of tension between Comoros and its giant island neighbor. The Shirazi elite continued to play a prominent role as large landowners and civil servants. On the eve of independence, Comoros remained poor and undeveloped, \nhaving only one secondary school and practically nothing in the way of national media. Isolated from important trade routes by the opening of the Suez Canal in 1869, having few natural resources, and largely neglected by France, the islands were poorly equipped for independence.\nOn September 25, 1942, British forces landed in the Comoros, occupying them until October 13, 1946.\nIn 1946 the Comoro Islands became an overseas department of France with representation in the French National Assembly. The following year, the islands' administrative ties to Madagascar were severed; Comoros established its own customs regime in 1952. A Governing Council was elected in August 1957 on the four islands in conformity with the loi-cadre (enabling law) of June 23, 1956. A constitution providing for internal self-government was promulgated in 1961, following a 1958 referendum in which Comorans voted overwhelmingly to remain a part of France. This government consisted of a territorial assembly having, in 1975, thirty-nine members, and a Governing Council of six to nine ministers responsible to it.\nAgreement was reached with France in 1973 for the Comoros to become independent in 1978. On July 6, 1975, however, the Comorian parliament passed a resolution declaring unilateral independence as a republic. The deputies of Mayotte abstained. The first president of the Comoros, Ahmed Abdallah Abderemane, did not last long before being ousted in a coup d'\u00e9tat by Ali Soilih, an atheist with an Islamic background.\nSoilih began with a set of solid socialist ideals designed to modernize the country. However, the regime faced problems. A French mercenary by the name of Bob Denard, arrived in the Comoros at dawn on 13 May 1978, and removed Soilih from power. Solih was shot and killed during the coup. The mercenaries returned Abdallah to power and the mercenaries were given key positions in government.\nIn two referendums, in December 1974 and February 1976, the population of Mayotte voted against independence from France (by 63.8% and 99.4% respectively). Mayotte thus remains under French administration, and the Comorian Government has effective control over only Grande Comore, Anjouan, and Moh\u00e9li.\nLater, French settlers, French-owned companies, and Arab merchants established a plantation-based economy that now uses about one-third of the land for export crops.\nAbdallah regime.\nIn 1978, president Ali Soilih, who had a firm anti-French line, was killed and Ahmed Abdallah came to power. Under the reign of Abdallah, Denard was commander of the Presidential Guard (PG) and \"de facto\" ruler of the country. He was trained, supported and funded by the white regimes in South Africa (SA) and Rhodesia (now Zimbabwe) in return for permission to set up a secret listening post on the islands. South-African agents kept an ear on the important ANC bases in Lusaka and Dar es Salaam and watched the war in Mozambique, in which SA played an active role. The Comoros were also used for the evasion of arms sanctions.\nWhen in 1981 Fran\u00e7ois Mitterrand was elected president Denard lost the support of the French intelligence service, but he managed to strengthen the link between SA and the Comoros. Besides the military, Denard established his own company SOGECOM, for both the security and construction, and seemed to profit by the arrangement. Between 1985 and 1987 the relationship of the PG with the local Comorians became worse.\nAt the end of the 1980s the South Africans did not wish to continue to support the mercenary regime and France was in agreement. Also President Abdallah wanted the mercenaries to leave. Their response was a (third) coup resulting in the death of President Abdallah, in which Denard and his men were probably involved. South Africa and the French government subsequently forced Denard and his mercenaries to leave the islands in 1989.\n1989\u20131996.\nSaid Mohamed Djohar became president. His time in office was turbulent, including an impeachment attempt in 1991 and a coup attempt in 1992.\nOn September 28, 1995 Bob Denard and a group of mercenaries took over the Comoros islands in a coup (named operation Kaskari by the mercenaries) against President Djohar. France immediately and severely denounced the coup, and backed by the 1978 defense agreement with the Comoros, President Jacques Chirac ordered his special forces to retake the island. Bob Denard began to take measures to stop the coming invasion. A new presidential guard was created. Strong points armed with heavy machine guns were set up around the island, particularly around the island's two airports.\nOn October 3, 1995, 11 p.m., the French deployed 600 men against a force of 33 mercenaries and a 300-man dissident force. Denard however ordered his mercenaries not to fight. Within 7 hours the airports at Iconi and Hahaya and the French Embassy in Moroni were secured. By 3:00\u00a0p.m. the next day Bob Denard and his mercenaries had surrendered. This (response) operation, codenamed \"Azal\u00e9e\", was remarkable, because there were no casualties, and just in seven days, plans were drawn up and soldiers were deployed. Denard was taken to France and jailed. Prime minister Caambi El-Yachourtu became acting president until Djohar returned from exile in January, 1996. In March 1996, following presidential elections, Mohamed Taki Abdoulkarim, a member of the civilian government that Denard had tried to set up in October 1995, became president. On 23 November 1996, Ethiopian Airlines Flight 961 crashed near a beach on the island after it was hijacked and ran out of fuel killing 125 people and leaving 50 survivors.\nSecession of Anjouan and Moh\u00e9li.\nIn 1997, the islands of Anjouan and Moh\u00e9li declared their independence from the Comoros. A subsequent attempt by the government to re-establish control over the rebellious islands by force failed, and presently the African Union is brokering negotiations to effect a reconciliation. This process is largely complete, at least in theory. According to some sources, Moh\u00e9li did return to government control in 1998. In 1999, Anjouan had internal conflicts and on August 1 of that year, the 80-year-old first president Foundi Abdallah Ibrahim resigned, transferring power to a national coordinator, Said Abeid. The government was overthrown in a coup by army and navy officers on August 9, 2001. Mohamed Bacar soon rose to leadership of the junta that took over and by the end of the month he was the leader of the country. Despite two coup attempts in the following three months, including one by Abeid, Bacar's government remained in power, and was apparently more willing to negotiate with the Comoros. Presidential elections were held for all of the Comoros in 2002, and presidents have been chosen for all three islands as well, which have become a confederation. Most notably, Mohammed Bacar was elected for a 5-year term as president of Anjouan. Grande Comore had experienced troubles of its own in the late 1990s, when President Taki died on November 6, 1998. Colonel Azali Assoumani became president following a military coup in 1999. There have been several coup attempts since, but he gained firm control of the country after stepping down temporarily and winning a presidential election in 2002.\nIn May 2006, Ahmed Abdallah Sambi was elected from the island of Anjouan to be the president of the Union of the Comoros. He is a Sunni cleric who studied in the Sudan, Iran and Saudi Arabia. He is nicknamed \"Ayatollah\" due to his time in Iran and his penchant for turbans. Sambi was sentenced to life in imprisonment in the Comoros passport sales scandal.\nAzali Assoumani in power since 2016.\nAzali Assoumani is a former army officer, first came to power in a coup in 1999. Then he won presidency in 2002 election, having power until 2006. After ten years, he was elected again in 2016 election. In March 2019, he was re-elected in the elections opposition claimed to be full of irregularities.\nBefore the 2019 election president Azali Assoumani had arranged a constitutional referendum in 2018 that approved extending the presidential mandate from one five-year term to two. The opposition had boycotted the referendum.\nIn January 2020, his party The Convention for the Renewal of the Comoros (CRC) won 20 out of 24 parliamentary seats in the parliamentary election.\nOn 18 February 2023 the Comoros assumed the presidency of the African Union. In January 2024, President Azali Assoumani was re-elected with 63% of the vote in the disputed presidential election.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nAttribution:"}
{"id": "6001", "revid": "2304267", "url": "https://en.wikipedia.org/wiki?curid=6001", "title": "Geography of the Comoros", "text": " \nThe Comoros archipelago consists of four main islands aligned along a northwest\u2013southeast axis at the north end of the Mozambique Channel, between Mozambique and the island of Madagascar. Still widely known by their French names, the islands officially have been called by their Swahili names by the Comorian government. They are Grande Comore (Njazidja), Moh\u00e9li (Mwali), Anjouan (Nzwani), and Mayotte (Mahor\u00e9). The islands' distance from each other\u2014Grande Comore is some 200 kilometers from Mayotte, forty kilometers from Moh\u00e9li, and eighty kilometers from Anjouan\u2014along with a lack of good harbor facilities, make transportation and communication difficult. Comoros are sunny islands.\nDetails.\nThe islands have a total land area of 2,236 square kilometers (including Mayotte), and claim territorial waters of 320 square kilometers. Mount Karthala (2316\u00a0m) on Grande Comore is an active volcano. From April 17 to 19, 2005, the volcano began spewing ash and gas, forcing as many as 10,000 people to flee. Comoros is located within the Somali Plate.\nGrande Comore.\nGrande Comore is the largest island, sixty-seven kilometers long and twenty-seven kilometers wide, with a total area of 1,146 square kilometers. The most recently formed of the four islands in the archipelago, it is also of volcanic origin. Two volcanoes form the island's most prominent topographic features: La Grille in the north, with an elevation of 1,000 meters, is extinct and largely eroded; Kartala in the south, rising to a height of 2,361 meters, last erupted in 1977. A plateau averaging 600 to 700 meters high connects the two mountains. Because Grande Comore is geologically a relatively new island, its soil is thin and rocky and cannot hold water. As a result, water from the island's heavy rainfall must be stored in catchment tanks. There are no coral reefs along the coast, and the island lacks a good harbor for ships. One of the largest remnants of the Comoros' once-extensive rain forests is on the slopes of Kartala. The national capital has been at Moroni since 1962.\nAnjouan.\nAnjouan, triangular shaped and forty kilometers from apex to base, has an area of 424 square kilometers. Three mountain chains \u2014 Sima, Nioumakele, and Jimilime\u2014emanate from a central peak, Mtingui (1,575 m), giving the island its distinctive shape. Older than Grande Comore, Anjouan has deeper soil cover, but overcultivation has caused serious erosion. A coral reef lies close to shore; the island's capital of Mutsamudu is also its main port.\nMoh\u00e9li.\nMoh\u00e9li is thirty kilometers long and twelve kilometers wide, with an area of 290 square kilometers. It is the smallest of the four islands and has a central mountain chain reaching 860 meters at its highest. Like Grande Comore, it retains stands of rain forest. Moh\u00e9li's capital is Fomboni.\nMayotte.\nMayotte, geologically the oldest of the four islands, is thirty-nine kilometers long and twenty-two kilometers wide, totaling 375 square kilometers, and its highest points are between 500 and 600 meters above sea level. Because of greater weathering of the volcanic rock, the soil is relatively rich in some areas. A well-developed coral reef that encircles much of the island ensures protection for ships and a habitat for fish. Dzaoudzi, capital of the Comoros until 1962 and now Mayotte's administrative center, is situated on a rocky outcropping off the east shore of the main island. Dzaoudzi is linked by a causeway to le Pamanzi, which at ten kilometers in area is the largest of several islets adjacent to Mayotte. Islets are also scattered in the coastal waters of Mayotte just as in Grande Comore, Anjouan, and Moh\u00e9li.\nFlora and fauna.\nComorian waters are the habitat of the coelacanth, a rare fish with limblike fins and a cartilaginous skeleton, the fossil remains of which date as far back as 400 million years and which was once thought to have become extinct about 70 million years ago. A live specimen was caught in 1938 off southern Africa; other coelacanths have since been found in the vicinity of the Comoro Islands.\nSeveral mammals are unique to the islands themselves. Livingstone's fruit bat, although plentiful when discovered by explorer David Livingstone in 1863, has been reduced to a population of about 120, entirely on Anjouan. The world's largest bat, the jet-black Livingstone fruit bat has a wingspan of nearly two meters. A British preservation group sent an expedition to the Comoros in 1992 to bring some of the bats to Britain to establish a breeding population.\nA hybrid of the common brown lemur (\"Eulemur fulvus\") originally from Madagascar, was introduced by humans prior to European colonization and is found on Mayotte. The mongoose lemur (\"Eulemur mongoz\"), also introduced from Madagascar by humans, can be found on the islands of Moh\u00e9li and Anjouan.\n22 species of bird are unique to the archipelago and 17 of these are restricted to the Union of the Comoros. These include the Karthala scops-owl, Anjouan scops-owl and Humblot's flycatcher.\nPartly in response to international pressures, Comorians in the 1990s have become more concerned about the environment. Steps are being taken not only to preserve the rare fauna, but also to counteract degradation of the environment, especially on densely populated Anjouan. Specifically, to minimize the cutting down of trees for fuel, kerosene is being subsidized, and efforts are being made to replace the loss of the forest cover caused by ylang-ylang distillation for perfume. The Community Development Support Fund, sponsored by the International Development Association (IDA, a World Bank affiliate) and the Comorian government, is working to improve water supply on the islands as well.\nClimate.\nThe climate is marine tropical, with two seasons: hot and humid from November to April, the result of the northeastern monsoon, and a cooler, drier season the rest of the year. Average monthly temperatures range from along the coasts. Although the average annual precipitation is , water is a scarce commodity in many parts of the Comoros. Moh\u00e9li and Mayotte possess streams and other natural sources of water, but Grande Comore and Anjouan, whose mountainous landscapes retain water poorly, are almost devoid of naturally occurring running water. Cyclones, occurring during the hot and wet season, can cause extensive damage, especially in coastal areas. On the average, at least twice each decade houses, farms, and harbor facilities are devastated by these great storms.\nTropical cyclones.\nDue to their low latitude, the islands are rarely affected by tropical cyclones. However, several cyclones have had damaging and deadly effects. Cyclones in December 1905 and again in December 1906 led to a famine that killed 490\u00a0people between August 1905 and January 1906. A tropical cyclone in 1950 killed 585\u00a0people while moving through Anjouan and Moheli, injuring 70,000\u00a0others. The cyclone left 40,000\u00a0people homeless, and also caused \u20a33.5\u00a0worth of damage to crops and infrastructure. \nExtreme points.\nThis is a list of the extreme points of the Comoros, the points that are farther north, south, east or west than any other location. This list excludes the French-administered island of Mayotte which is claimed by the Comorian government.\nStatistics.\nArea:\n2,235\u00a0km2\nCoastline:\n340\u00a0km\nClimate:\ntropical marine; rainy season (November to May)\nTerrain:\nvolcanic islands, interiors vary from steep mountains to low hills\nElevation extremes:\n\"lowest point:\"\nIndian Ocean 0 m\n\"highest point:\"\nKarthala 2,360 m\nNatural resources:\nfish\nLand use:\n\"arable land:\"\n47.29%\n\"permanent crops:\"\n29.55%\n\"other:\"\n23.16% (2012 est.)\nIrrigated land:\n1.3\u00a0km2 (2003)\nTotal renewable water resources:\n1.2\u00a0km3 (2011)\nFreshwater withdrawal (domestic/industrial/agricultural):\n\"total:\"\n0.01\u00a0km3/yr (48%/5%/47%)\n\"per capital:\"\n16.86 m3/yr (1999)\nNatural hazards:\ncyclones possible during rainy season (December to April); volcanic activity on Grand Comore\nEnvironmental - current issues:\nsoil degradation and erosion results from crop cultivation on slopes without proper terracing; deforestation\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6002", "revid": "50028146", "url": "https://en.wikipedia.org/wiki?curid=6002", "title": "Demographics of the Comoros", "text": "The Comorians () inhabiting Grande Comore, Anjouan, and Moh\u00e9li (86% of the population) share African-Arab origins. Islam is the dominant religion, and Quranic schools for children reinforce its influence. Although Islamic culture is firmly established throughout, a small minority are Christian.\nThe most common language is Comorian, related to Swahili. French and Arabic also are spoken. About 89% of the population is literate.\nThe Comoros have had eight censuses since World War II:\nThe official estimate as of 1 July 2020 is 897,219.\nPopulation density figures conceal a great disparity between the republic's most crowded island, Anjouan, which had a density of 772 persons per square kilometer in 2017; Grande Comore, which had a density of 331 persons per square kilometer in 2017; and Moh\u00e9li, where the 2017 population density figure was 178 persons per square kilometer. \nBy comparison, estimates of the population density per square kilometer of the Indian Ocean's other island microstates ranged from 241 (Seychelles) to 690 (Maldives) in 1993. Given the rugged terrain of Grande Comore and Anjouan, and the dedication of extensive tracts to agriculture on all three islands, population pressures on the Comoros are becoming increasingly critical.\nThe age structure of the population of the Comoros is similar to that of many developing countries, in that the republic has a very large proportion of young people. In 1989, 46.4 percent of the population was under fifteen years of age, an above-average proportion even for sub-Saharan Africa. The population's rate of growth was a relatively high 3.5 percent per annum in the mid-1980s, up substantially from 2.0 percent in the mid-1970s and 2.1 percent in the mid-1960s.\nIn 1983 the Abdallah regime borrowed US$2.85 million from the International Development Association to devise a national family planning program. However, Islamic reservations about contraception made forthright advocacy and implementation of birth control programs politically hazardous, and consequently little was done in the way of public policy.\nThe Comorian population has become increasingly urbanized in recent years. In 1991 the percentage of Comorians residing in cities and towns of more than 5,000 persons was about 30 percent, up from 25 percent in 1985 and 23 percent in 1980. The Comoros' largest cities were the capital, Moroni, with about 30,000 people, and the port city of Mutsamudu, on the island of Anjouan, with about 20,000 people.\nMigration among the various islands is important. Natives of Anjouan have settled in significant numbers on less crowded Moh\u00e9li, causing social tensions, and many Anjouan also migrate to Mayotte, a French territory. In 1977 Mayotte, then called Maori, expelled peasants from Grande Comore and Anjouan who had recently settled in large numbers on the island. Some were allowed to reenter starting in 1981 but solely as migrant labor.\nThe number of Comorians living abroad has been estimated at between 80,000 and 100,000; during the colonial period, most of them lived in Tanzania, Madagascar, and other parts of Southeast Africa. The number of Comorians residing in Madagascar was drastically reduced after anti-Comorian rioting in December 1976 in Mahajanga, in which at least 1,400 Comorians were killed. As many as 17,000 Comorians left Madagascar to seek refuge in their native land in 1977 alone. About 100,000 Comorians live in France; many of them had gone there for a university education and never returned. Small numbers of Indians, Malagasy, South Africans, and Europeans (mostly French) live on the islands and play an important role in the economy. Most French left after independence in 1975.\nSome Persian Gulf countries started buying Comorian citizenship for their stateless Bedoon residents and deporting them to Comoros.\nPopulation.\nUN population projections.\n&lt;templatestyles src=\"Template:Bar chart/styles.css\"/&gt;\nVital statistics.\nStatistics as of 2010[ [update]]:\nDemographic and Health Surveys.\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\nStructure of the population (DHS 2012) (Males 11 088, Females 12 284 = 23 373) :\nFertility data as of 2012 (DHS Program):\nArabic (official), French (official), Comorian (official)\nReligion.\nSunni Muslim 98%, other (including Shia Muslim, Roman Catholic, Jehovah's Witness, Protestant) 2%\nnote: Sunni Islam is the state religion\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAttribution:\n This article incorporates text from this source, which is in the public domain. "}
{"id": "6003", "revid": "197202", "url": "https://en.wikipedia.org/wiki?curid=6003", "title": "Politics of the Comoros", "text": "Overview of Comorian politics\nThe politics of the Union of the Comoros take place in a framework of a federal presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. The precolonial legacies of the sultanates linger while the political situation in Comoros has been extremely fluid since the country's independence in 1975, subject to the volatility of coups and political insurrection.\nAs of 2008, the Comoros and Mauritania were considered by US-based organization Freedom House as the only real \u201celectoral democracies\u201d of the Arab world.\nPrecolonial and colonial political structures.\nSultanates in the late nineteenth century used a cyclic age system and hierarchical lineage membership to provide the foundation for participation in the political process. In the capital, \"the sultan was assisted by his ministers and by a madjelis, an advisory council composed of elders, whom he consulted regularly\". Apart from local administration, the age system was used to include the population in decision making, depending on the scope of the decision being made. For example, the elders of the island of Njazidja held considerable influence on the authority of the sultan. Though sultanates granted rights to their free inhabitants, were provided with warriors during war and taxed the towns under their authority, their definition as a state is open to debate. The islands' incorporation as a province of the colony of Madagascar into the French colonial empire marked the end of the sultanates.\nDespite French colonization, Comorans identify first with kinship or regional ties and rarely ever with the central government. This is a lingering effect of the sovereign sultanates of pre-colonial times. French colonial administration was based on a misconception that the sultanates operated as absolute monarchs: district boundaries were the same as the sultanates', multiple new taxes forced men into wage labor on colonial plantations and was reinforced through a compulsory public labor system that had little effect on infrastructure. French policy was hampered by an absence of settlers, effective communication across islands, rough geographical terrain and hostility towards the colonial government. Policies were made to apply to Madagascar as a whole and seldom to the nuances of each province: civil servants were typically Christian, unaware of local customs and unable to speak the local language. The French established the Ouatou Akouba in 1915, a local form of governance based on \"customary structures\" already in place that attempted to model itself after the age system in place under the sultanates. Their understanding of the elders' council as a corporate group bypassed the reality that there were men \"who had accomplished the necessary customary rituals to be accorded the status of elder and thus be eligible to participate in the political process in the village\", which effectively rendered the French elders' council ineffective. Though the Ouatou Akouba was disbanded, it resulted in the consolidation and formalization of the age system as access to power in the customary and local government spheres. The French failure to establish a functioning state in the Comoros has had repercussions in the post-independence era.\nPost-independence.\nAt independence there were five main political parties: OUDZIMA, UMMA, the Comoro People's Democratic Rally, the Comoro National Liberation Movement and the Socialist Objective Party. The political groups previously known simply as the 'green' and 'white' party became the Rassemblement D\u00e9mocratique du Peuple Comorien (RDPC) and the Union D\u00e9mocratique des Comores (UDC), headed by Sayyid Muhammad Cheikh and Sayyid Ibrahim. Members from both parties later merged to form OUDZIMA under the leadership of first president Ahmad Abdallah while dissidents from both created UMMA under the leadership of future president Ali Soilih.\nPrince Said Ibrahim took power in 1970 but was democratically elected out of office in 1972 in favor of former French senator Ahmed Abdallah. President Abdallah declared independence for all islands, except Mayotte which remained under French administration, in 1975. The threat of renewed socioeconomic marginalization following the transfer of the capital to Ngazidja in 1962, more than social or cultural differences, underlay the island's subsequent rejection of independence. France withdrew all economic and technical support for the now independent state, which would encourage a revolutionary regime under future president Ali Soilih. French military and financial aid to mercenaries brought Prince Said Mohammed Jaffar to power after the United National Front of the Comoros (FNU) party toppled Abdallah's government. This mercenary coup was unique in that, unlike other coups on the continent, it was \"uninspired by any ideological convictions\". The Jaffar regime's inefficient distribution of resources and poor mismanagement was shown through the expulsion of French civil servants as well as endemic unemployment and food shortages. The regime used famine as \"an opportunity to switch food patronage from France to the World Food Programme's emergency aid\".\nPresident Jaffar's ousting by Minister of Defense and Justice, Ali Soilih, brought about the \"periode noire\" (dark period) of the country; the voting age was lowered to 14, most civil servants were dismissed and there was a ban on some Islamic customs. He implemented revolutionary social reforms such as replacing French with Shikomoro, burning down the national archives and nationalizing land. His government received support from Egypt, Iraq, and the Sudan. Soilih's attacks on religious and customary authority contributed to his eventual ousting through a French-backed coup consisting of mercenaries and ex-politicians who together formed the Politico Military Doctorate.\nAbdallah was reinstated and constructed a mercantile state by resuscitating the structures of the colonial era. His establishment of a one party state and intolerance for dissent further alienated civil society from the state. In May 1978 the Comoros were renamed the Islamic Republic of the Comoros and continued strengthening ties with the Arab world which resulted in their joining the Arab League. Abdallah's government sought to reverse Soilih's 'de-sacralization' by re-introducing the grand marriage, declaring Arabic the second official language behind French, and creating the office of the Grand Mufti. The doctorate &amp; compromise government was dissolved, constitutional changes removed succession from a politician and neutralized the post of another possible challenger in abolishing the position of prime minister, which effectively cemented a client-patron network by making the civil service position dependent on Abdallah's political base. The Democratic Front's (DF) internal opposition to Abdallah was suppressed through the incarceration of over 600 people allegedly involved in a failed coup attempt. Abdallah then stocked the House of Assembly with loyal clientelist supporters through rigged parliamentary elections. All of these actions effectively consolidated Abdallah's position.\nMuhammed Djohar succeeded president Abdallah after his assassination in 1989 but was evacuated by French troops after a failed coup attempt in 1996. The Comoros were led by Muhammed Taki Abd al-Karim beginning in 1996 and he was followed by interim president Said Massunde who eventually gave way to Assoumani Azali. Taki's lack of Arab heritage led to his lack of understanding Nzwani's cultural differences and economic problems, as seen by the establishment of the elders council with only loyal Taki supporters. As a result, the\u00a0council was ignored by the true elders of the island. After Taki's death, a military coup in 1999, the nation's eighteenth since independence in 1975, installed Azali in to power. Colonel Azali Assoumani seized power in a bloodless coup in April 1999, overthrowing Interim President Tadjidine Ben Said Massounde, who himself had held the office since the death of democratically elected President Mohamed Taki Abdoulkarim in November, 1998. In May 1999, Azali decreed a constitution that gave him both executive and legislative powers. Bowing somewhat to international criticism, Azali appointed a civilian Prime Minister, Bainrifi Tarmidi, in December 1999; however, Azali retained the mantle of Head of State and army Commander. In December 2000, Azali named a new civilian Prime Minister, Hamada Madi, and formed a new civilian Cabinet. When Azali took power he also pledged to step down in April 2000 and relinquish control to a democratically elected president\u2014a pledge with mixed results. Under Mohammed Taki and Assoumani Azali, access to the state was used to support client networks which led to crumbling infrastructure that cultivated in the islands of Nzwani and Mwali declaring independence only\u00a0to be stopped by French troops. Azali lacked the social obligations required to address the elders and when combined with his gross mismanagement and increasing economic and social dependence on foreign entities, made managing daily life near nonexistent in the state. Therefore, local administrative structures began popping up and drifting away from reliance on the state, funded by remittances from the expatriate community in France.\nAzali Assoumani is a former army officer, first came to power in a coup in 1999. Then he won presidency in 2002 election, having power until 2006. After ten years, he was elected again in 2016 election. In March 2019, he was re-elected in the elections opposition claimed to be full of irregularities.\nBefore the 2019 election, president Azali Assoumani had arranged a constitutional referendum in 2018 that approved extending the presidential mandate from one five-year term to two. The opposition had boycotted the referendum.\nIn January 2020, his party the Convention for the Renewal of the Comoros (CRC) won 20 out of 24 parliamentary seats in the parliamentary election.\nOn 18 February 2023 the Comoros assumed the presidency of the African Union. In January 2024, President Azali Assoumani was re-elected with 63% of the vote in the disputed presidential election.\nThe Comoros Islands have experienced five different constitutions.\nFirst Constitution: Federal Islamic Republic of the Comoros, 1978-1989.\nSource:\nSecond Constitution: Federal Islamic Republic of the Comoros, 1992 - 1999.\nSource:\nThird Constitution: The Union of the Comoros, 2001.\nSource:\nFourth Constitution.\nIn a separate nod to pressure to restore civilian rule, the government organized several committees to compose a new constitution, including the August 2000 National Congress and November 2000 Tripartite Commission. The opposition parties initially refused to participate in the Tripartite Commission, but on 17 February, representatives of the government, the Anjouan separatists, the political opposition, and civil society organizations signed a \"Framework Accord for Reconciliation in Comoros,\" brokered by the Organization of African Unity\nThe accord called for the creation of a new Tripartite Commission for National Reconciliation to develop a \"New Comorian Entity\" with a new constitution. The new federal Constitution came into effect in 2002; it included elements of consociationalism, including a presidency that rotates every four years among the islands and extensive autonomy for each island. Presidential elections were held in 2002, at which Azali Assoumani was elected president. In April 2004, legislative elections were held, completing the implementation of the new constitution.\nThe new Union of the Comoros consists of three islands: Anjouan, Grande Comore, and Moh\u00e9li. Each island has a president, who shares the presidency of the Union on a rotating basis. The president and his vice-presidents are elected for a term of four years. The constitution states that, \"the islands enjoy financial autonomy, freely draw up and manage their budgets\".\nPresident Assoumani Azali of Grande Comore is the first Union president. President Mohamed Bacar of Anjouan formed his 13-member government at the end of April, 2003.\nOn 15 May 2006, Ahmed Abdallah Sambi, a cleric and successful businessman educated in Iran, Saudi Arabia, and the Sudan, was declared the winner of elections for President of the Republic. He is considered a moderate Islamist and is called Ayatollah by his supporters. He beat out retired French air force officer Mohamed Djaanfari and long-time politician Ibrahim Halidi, whose candidacy was backed by Azali Assoumani, the outgoing president.\nA referendum took place on May 16, 2009, to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councilors.\nAutonomous islands.\nThe constitution gives Anjouan, Grande Comore, and Moh\u00e9li the right to govern most of their internal affairs with their own presidents, except the powers assigned to the Union of the Comoros such as banking, foreign affairs, national security, nationalities, and others. Comoros considers Mayotte, an overseas department and region of France, to be a part of its sovereign territory, with an autonomous status.\nAs of 2011, the three autonomous islands are subdivided into 16 prefectures, 54 communes, and 318 villes or villages.\nExecutive branch.\nThe federal presidency is rotated between the islands' presidents.\nThe Union of the Comoros abolished the position of Prime Minister in 2002. The position of Vice-President of the Comoros was used 2002\u20132019.\nLegislative branch.\nThe Assembly of the Union has 33 seats, 24 elected in single seat constituencies and 9 representatives of the regional assemblies.\nJudicial branch.\nThe Supreme Court or Cour Supreme, has two members appointed by the president, two members elected by the Federal Assembly, one by the Council of each island, and former presidents of the republic.\nInternational organization participation.\nThe Comoros is a member state of the following international organizations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6005", "revid": "1960810", "url": "https://en.wikipedia.org/wiki?curid=6005", "title": "Telecommunications in the Comoros", "text": " \nIn large part thanks to international aid programs, Moroni has international telecommunications service. Telephone service, however, is largely limited to the islands' few towns.\nOverview.\nTelephones \u2013 main lines in use:\n5,000 (1995)\nTelephones \u2013 mobile cellular:\n0 (1995)\nTelephone system:\nsparse system of microwave radio relay and HF radiotelephone communication stations\n&lt;br&gt;\"domestic:\"\nHF radiotelephone communications and microwave radio relay&lt;br&gt;\nCMDA mobile network (Huri, operated by Comores Telecom)\n&lt;br&gt;\"international:\"\nHF radiotelephone communications to Madagascar and R\u00e9union\nRadio broadcast stations:\nAM 1, FM 2, shortwave 1 (1998)\nRadios:\n90,000 (1997)\nTelevision broadcast stations:\n0 (1998)\nTelevisions:\n1,000 (1997)\nInternet Service Providers (ISPs): 1 (1999)\nCountry code (Top-level domain): .km\nSpecial projects.\nIn October 2011 the State of Qatar launched a special program for the construction of a wireless network to interconnect the three islands of the archipelago, by means of low cost, repeatable technology. The project has been developed by Qatar University and Politecnico di Torino, under the supervision of prof. Mazen Hasna and prof. Daniele Trinchero, with a major participation of local actors (Comorian Government, NRTIC, University of the Comoros). The project has been referred as an example of technology transfer and Sustainable Inclusion in developing countries\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n This article incorporates text from this source, which is in the public domain. "}
{"id": "6006", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=6006", "title": "Transport in the Comoros", "text": " \nThere are a number of systems of transport in the Comoros. The Comoros possesses of road, of which are paved. It has three seaports: Fomboni, Moroni and Moutsamoudou, but does not have a merchant marine, and no longer has any railway network. It has four airports, all with paved runways, one with runways over long, with the others having runways shorter than .\nThe isolation of the Comoros had made air traffic a major means of transportation. One of President Abdallah's accomplishments was to make the Comoros more accessible by air. During his administration, he negotiated agreements to initiate or enhance commercial air links with Tanzania and Madagascar. The Djohar regime reached an agreement in 1990 to link Moroni and Brussels by air. By the early 1990s, commercial flights connected the Comoros with France, Mauritius, Kenya, South Africa, Tanzania, and Madagascar. The national airline was Air Comores. Daily flights linked the three main islands, and air service was also available to Mahor\u00e9; each island had airstrips. In 1986 the republic received a grant from the French government's CCCE to renovate and expand Hahaya airport, near Moroni. Because of the absence of scheduled sea transport between the islands, nearly all interisland passenger traffic is by air.\nMore than 99% of freight is transported by sea. Both Moroni on Njazidja and Mutsamudu on Nzwani have artificial harbors. There is also a harbor at Fomboni, on Mwali. Despite extensive internationally financed programs to upgrade the harbors at Moroni and Mutsamudu, by the early 1990s only Mutsamudu was operational as a deepwater facility. Its harbor could accommodate vessels of up to eleven meters' draught. At Moroni, ocean-going vessels typically lie offshore and are loaded or unloaded by smaller craft, a costly and sometimes dangerous procedure. Most freight continues to be sent to Tanzania, Kenya, Reunion, or Madagascar for transshipment to the Comoros. Use of Comoran ports is further restricted by the threat of cyclones from December through March. The privately operated Comoran Navigation Company (\"Soci\u00e9t\u00e9 Comorienne de Navigation\") is based in Moroni, and provides services to Madagascar.\nRoads serve the coastal areas, rather than the interior, and the mountainous terrain makes surface travel difficult."}
{"id": "6007", "revid": "7447121", "url": "https://en.wikipedia.org/wiki?curid=6007", "title": "Foreign relations of the Comoros", "text": "In November 1975, Comoros became the 143rd member of the United Nations. The new nation was defined as consisting of the entire archipelago, despite the fact that France maintains control over Mayotte.\nOverview.\nComoros also is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission, and the African Development Bank.\nThe government fostered close relationships with the more conservative (and oil-rich) Arab states, such as Saudi Arabia and Kuwait. It frequently received aid from those countries and the regional financial institutions they influenced, such as the Arab Bank for Economic Development in Africa and the Arab Fund for Economic and Social Development. In October 1993, Comoros joined the League of Arab States, after having been rejected when it applied for membership initially in 1977.\nRegional relations generally were good. In 1985 Madagascar, Mauritius, and Seychelles agreed to admit Comoros as the fourth member of the Indian Ocean Commission (IOC), an organization established in 1982 to encourage regional cooperation. In 1993 Mauritius and Seychelles had two of the five embassies in Moroni, and Mauritius and Madagascar were connected to the republic by regularly scheduled commercial flights.\nIn November 1975, Comoros became the 143d member of the UN. In the 1990s, the republic continued to represent Mahor\u00e9 in the UN. Comoros was also a member of the OAU, the EDF, the World Bank, the IMF, the IOC, and the African Development Bank.\nComoros thus cultivated relations with various nations, both East and West, seeking to increase trade and obtain financial assistance. In 1994, however, it was increasingly facing the need to control its expenditures and reorganize its economy so that it would be viewed as a sounder recipient of investment. Comoros also confronted domestically the problem of the degree of democracy the government was prepared to grant to its citizens, a consideration that related to its standing in the world community.\nDiplomatic relations.\nList of countries which the Comoros maintains diplomatic relations with:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6008", "revid": "2325890", "url": "https://en.wikipedia.org/wiki?curid=6008", "title": "Army of National Development", "text": "Military of the Comoros\nThe Comorian Armed Forces (; lit.\u2009'Army of National Development') are the national military of the Comoros. The armed forces consist of a small standing army and a 500-member police force, as well as a 500-member defense force. A defense treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a small troop presence in the Comoros at government request. France maintains a small Navy base and a Foreign Legion Detachment (DLEM) (now the 5th Foreign Infantry Regiment) in Mayotte.\nStructure.\nThe AND consists of the following components:\nAircraft.\nNote: The last comprehensive aircraft inventory list was from \"Aviation Week &amp; Space Technology\" in 2007.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6010", "revid": "50666491", "url": "https://en.wikipedia.org/wiki?curid=6010", "title": "Computer worm", "text": "Self-replicating malware program\nA computer worm is a standalone malware computer program that replicates itself in order to spread to other computers. It often uses a computer network to spread itself, relying on security failures on the target computer to access it. It will use this machine as a host to scan and infect other computers. When these new worm-invaded computers are controlled, the worm will continue to scan and infect other computers using these computers as hosts, and this behavior will continue. Computer worms use recursive methods to copy themselves without host programs and distribute themselves based on exploiting the advantages of exponential growth, thus controlling and infecting more and more computers in a short time. Worms almost always cause at least some harm to the network, even if only by consuming bandwidth, whereas viruses almost always corrupt or modify files on a targeted computer.\nMany worms are designed only to spread, and do not attempt to change the systems they pass through. However, as the Morris worm and Mydoom showed, even these \"payload-free\" worms can cause major disruption by increasing network traffic and other unintended effects.\nHistory.\nThe first ever computer worm is generally accepted to be a self-replicating version of Creeper created by Ray Tomlinson and Bob Thomas at BBN in 1971 to replicate itself across the ARPANET. Tomlinson also devised the first antivirus software, named Reaper, to delete the Creeper program.\nThe term \"worm\" was first used in this sense in John Brunner's 1975 novel, \"The Shockwave Rider\". In the novel, Nichlas Haflinger designs and sets off a data-gathering worm in an act of revenge against the powerful people who run a national electronic information web that induces mass conformity. \"You have the biggest-ever worm loose in the net, and it automatically sabotages any attempt to monitor it. There's never been a worm with that tough a head or that long a tail!\" \"Then the answer dawned on him, and he almost laughed. Fluckner had resorted to one of the oldest tricks in the store and turned loose in the continental net a self-perpetuating tapeworm, probably headed by a denunciation group \"borrowed\" from a major corporation, which would shunt itself from one nexus to another every time his credit-code was punched into a keyboard. It could take days to kill a worm like that, and sometimes weeks.\"\nXerox PARC was studying the use of \"worm\" programs for distributed computing in 1979.\nOn November 2, 1988, Robert Tappan Morris, a Cornell University computer science graduate student, unleashed what became known as the Morris worm, disrupting many computers then on the Internet, guessed at the time to be one tenth of all those connected. During the Morris appeal process, the U.S. Court of Appeals estimated the cost of removing the worm from each installation at between $200 and $53,000; this work prompted the formation of the CERT Coordination Center and Phage mailing list. Morris himself became the first person tried and convicted under the 1986 Computer Fraud and Abuse Act.\nConficker, a computer worm discovered in 2008 that primarily targeted Microsoft Windows operating systems, is a worm that employs three different spreading strategies: local probing, neighborhood probing, and global probing. This worm was considered a hybrid epidemic and affected millions of computers. The term \"hybrid epidemic\" is used because of the three separate methods it employed to spread, which was discovered through code analysis.\nFeatures.\nIndependence\nComputer viruses generally require a host program. The virus writes its own code into the host program. When the program runs, the written virus program is executed first, causing infection and damage. A worm does not need a host program, as it is an independent program or code chunk. Therefore, it is not restricted by the host program, but can run independently and actively carry out attacks.\nExploit attacks\nBecause a worm is not limited by the host program, worms can take advantage of various operating system vulnerabilities to carry out active attacks. For example, the \"Nimda\" virus exploits vulnerabilities to attack.\nComplexity\nSome worms are combined with web page scripts, and are hidden in HTML pages using VBScript, ActiveX and other technologies. When a user accesses a webpage containing a virus, the virus automatically resides in memory and waits to be triggered. There are also some worms that are combined with backdoor programs or Trojan horses, such as \"Code Red\".\nContagiousness\nWorms are more infectious than traditional viruses. They not only infect local computers, but also all servers and clients on the network based on the local computer. Worms can easily spread through shared folders, e-mails, malicious web pages, and servers with a large number of vulnerabilities in the network.\nHarm.\nAny code designed to do more than spread the worm is typically referred to as the \"payload\". Typical malicious payloads might delete files on a host system (e.g., the ExploreZip worm), encrypt files in a ransomware attack (e.g., the WannaCry worm), or exfiltrate data such as confidential documents or passwords.\nSome worms may install a backdoor. This allows the computer to be remotely controlled by the worm author as a \"zombie\". Networks of such machines are often referred to as botnets and are very commonly used for a range of malicious purposes, including sending spam or performing DoS attacks.\nSome special worms attack industrial systems in a targeted manner. Stuxnet was primarily transmitted through LANs and infected thumb-drives, as its targets were never connected to untrusted networks, like the internet. This virus can destroy the core production control computer software used by chemical, power generation and power transmission companies in various countries around the world - in Stuxnet's case, Iran, Indonesia and India were hardest hit - it was used to \"issue orders\" to other equipment in the factory, and to hide those commands from being detected. Stuxnet used multiple vulnerabilities and four different zero-day exploits (e.g.: http://) in Windows systems and Siemens SIMATICWinCC systems to attack the embedded programmable logic controllers of industrial machines. Although these systems operate independently from the network, if the operator inserts a virus-infected drive into the system's USB interface, the virus will be able to gain control of the system without any other operational requirements or prompts.\nCountermeasures.\nWorms spread by exploiting vulnerabilities in operating systems.\nVendors with security problems supply regular security updates (see \"Patch Tuesday\"), and if these are installed to a machine, then the majority of worms are unable to spread to it. If a vulnerability is disclosed before the security patch released by the vendor, a zero-day attack is possible.\nUsers need to be wary of opening unexpected emails, and should not run attached files or programs, or visit web sites that are linked to such emails. However, as with the ILOVEYOU worm, and with the increased growth and efficiency of phishing attacks, it remains possible to trick the end-user into running malicious code.\nAnti-virus and anti-spyware software are helpful, but must be kept up-to-date with new pattern files at least every few days. The use of a firewall is also recommended.\nUsers can minimize the threat posed by worms by keeping their computers' operating system and other software up to date, avoiding opening unrecognized or unexpected emails and running firewall and antivirus software.\nMitigation techniques include:\nInfections can sometimes be detected by their behavior - typically scanning the Internet randomly, looking for vulnerable hosts to infect. In addition, machine learning techniques can be used to detect new worms, by analyzing the behavior of the suspected computer.\nHelpful worms.\nA helpful worm or anti-worm is a worm designed to do something that its author feels is helpful, though not necessarily with the permission of the executing computer's owner. Beginning with the first research into worms at Xerox PARC, there have been attempts to create useful worms. Those worms allowed John Shoch and Jon Hupp to test the Ethernet principles on their network of Xerox Alto computers. Similarly, the Nachi family of worms tried to download and install patches from Microsoft's website to fix vulnerabilities in the host system by exploiting those same vulnerabilities. In practice, although this may have made these systems more secure, it generated considerable network traffic, rebooted the machine in the course of patching it, and did its work without the consent of the computer's owner or user. Another example of this approach is Roku OS patching a bug allowing for Roku OS to be rooted via an update to their screensaver channels, which the screensaver would attempt to connect to the telnet and patch the device. Regardless of their payload or their writers' intentions, security experts regard all worms as malware.\nOne study proposed the first computer worm that operates on the second layer of the OSI model (Data link Layer), utilizing topology information such as Content-addressable memory (CAM) tables and Spanning Tree information stored in switches to propagate and probe for vulnerable nodes until the enterprise network is covered.\nAnti-worms have been used to combat the effects of the Code Red, Blaster, and Santy worms. Welchia is an example of a helpful worm. Utilizing the same deficiencies exploited by the Blaster worm, Welchia infected computers and automatically began downloading Microsoft security updates for Windows without the users' consent. Welchia automatically reboots the computers it infects after installing the updates. One of these updates was the patch that fixed the exploit.\nOther examples of helpful worms are \"Den_Zuko\", \"Cheeze\", \"CodeGreen\", and \"Millenium\".\nArt worms support artists in the performance of massive scale ephemeral artworks. It turns the infected computers into nodes that contribute to the artwork.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6011", "revid": "36504275", "url": "https://en.wikipedia.org/wiki?curid=6011", "title": "Chomsky hierarchy", "text": "Hierarchy of classes of formal grammars\nThe Chomsky hierarchy in the fields of formal language theory, computer science, and linguistics, is a containment hierarchy of classes of formal grammars. A formal grammar describes how to form strings from a formal language's alphabet that are valid according to the language's syntax. The linguist Noam Chomsky theorized that four different classes of formal grammars existed that could generate increasingly complex languages. Each class can also completely generate the language of all inferior classes (set inclusive).\nHistory.\nThe general idea of a hierarchy of grammars was first described by Noam Chomsky in \"Three models for the description of language\" during the formalization of transformational-generative grammar (TGG). Marcel-Paul Sch\u00fctzenberger also played a role in the development of the theory of formal languages; the paper \"The algebraic theory of context free languages\" describes the modern hierarchy, including context-free grammars.\nIndependently, alongside linguists, mathematicians were developing models of computation (via automata). Parsing a sentence in a language is similar to computation, and the grammars described by Chomsky proved to both resemble and be equivalent in computational power to various machine models.\nThe hierarchy.\nThe following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have. The classes are defined by the constraints on the productions rules.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNote that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.\nEvery regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.\nRegular (Type-3) grammars.\nType-3 grammars generate the regular languages. Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal, in which case the grammar is \"right regular\". Alternatively, all the rules can have their right-hand sides consist of a single terminal, possibly \"preceded\" by a single nonterminal (\"left regular\"). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The rule formula_1 is also allowed here if formula_2 does not appear on the right side of any rule. These languages are exactly all languages that can be decided by a finite-state automaton. Additionally, this family of formal languages can be obtained by regular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.\nFor example, the regular language formula_3 is generated by the Type-3 grammar formula_4 with the productions formula_5 being the following.\n\"S\" \u2192 \"aS\"\n\"S\" \u2192 \"a\"\nContext-free (Type-2) grammars.\nType-2 grammars generate the context-free languages. These are defined by rules of the form formula_6 with formula_7 being a nonterminal and formula_8 being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton. Context-free languages\u2014or rather its subset of deterministic context-free languages\u2014are the theoretical basis for the phrase structure of most programming languages, though their syntax also includes context-sensitive name resolution due to declarations and scope. Often a subset of grammars is used to make parsing easier, such as by an LL parser.\nFor example, the context-free language formula_9 is generated by the Type-2 grammar formula_10 with the productions formula_5 being the following.\n\"S\" \u2192 \"aSb\"\n\"S\" \u2192 \"ab\"\nThe language is context-free but not regular (by the pumping lemma for regular languages).\nContext-sensitive (Type-1) grammars.\nType-1 grammars generate context-sensitive languages. These grammars have rules of the form formula_12 with formula_7 a nonterminal and formula_8, formula_15 and formula_16 strings of terminals and/or nonterminals. The strings formula_8 and formula_15 may be empty, but formula_16 must be nonempty. The rule formula_20 is allowed if formula_2 does not appear on the right side of any rule. The languages described by these grammars are exactly all languages that can be recognized by a linear bounded automaton (a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)\nFor example, the context-sensitive language formula_22 is generated by the Type-1 grammar formula_23 with the productions formula_5 being the following.\n\"S\" \u2192 \"aBC\"\n\"S\" \u2192 \"aSBC\"\n\"CB\" \u2192 \"CZ\"\n\"CZ\" \u2192 \"WZ\"\n\"WZ\" \u2192 \"WC\"\n\"WC\" \u2192 \"BC\"\n\"aB\" \u2192 \"ab\"\n\"bB\" \u2192 \"bb\" \n\"bC\" \u2192 \"bc\"\n\"cC\" \u2192 \"cc\"\nThe language is context-sensitive but not context-free (by the pumping lemma for context-free languages).\nA proof that this grammar generates formula_22 is sketched in the article on Context-sensitive grammars.\nRecursively enumerable (Type-0) grammars.\nType-0 grammars include all formal grammars. There are no constraints on the productions rules. They generate exactly all languages that can be recognized by a Turing machine, thus any language that is possible to be generated can be generated by a Type-0 grammar. These languages are also known as the \"recursively enumerable\" or \"Turing-recognizable\" languages. Note that this is different from the recursive languages, which can be \"decided\" by an always-halting Turing machine.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6012", "revid": "13467261", "url": "https://en.wikipedia.org/wiki?curid=6012", "title": "Churchs thesis", "text": ""}
{"id": "6013", "revid": "2902776", "url": "https://en.wikipedia.org/wiki?curid=6013", "title": "CRT", "text": "CRT or Crt most commonly refers to:\nCRT may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "6014", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=6014", "title": "Cathode ray tube", "text": "Vacuum tube often used to display images\nA cathode-ray tube (CRT) is a vacuum tube containing one or more electron guns, which emit electron beams that are manipulated to display images on a phosphorescent screen. The images may represent electrical waveforms on an oscilloscope, a frame of video on an analog television set (TV), digital raster graphics on a computer monitor, or other phenomena like radar targets. A CRT in a TV is commonly called a picture tube. CRTs have also been used as memory devices, in which case the screen is not intended to be visible to an observer. The term \"cathode ray\" was used to describe electron beams when they were first discovered, before it was understood that what was emitted from the cathode was a beam of electrons.\nIn CRT TVs and computer monitors, the entire front area of the tube is scanned repeatedly and systematically in a fixed pattern called a raster. In color devices, an image is produced by controlling the intensity of each of three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In modern CRT monitors and TVs the beams are bent by magnetic deflection, using a deflection yoke. Electrostatic deflection is commonly used in oscilloscopes.\nThe tube is a glass envelope which is heavy, fragile, and long from front screen face to rear end. Its interior must be close to a vacuum to prevent the emitted electrons from colliding with air molecules and scattering before they hit the tube's face. Thus, the interior is evacuated to less than a millionth of atmospheric pressure. As such, handling a CRT carries the risk of violent implosion that can hurl glass at great velocity. The face is typically made of thick lead glass or special barium-strontium glass to be shatter-resistant and to block most X-ray emissions. This tube makes up most of the weight of CRT TVs and computer monitors.\nSince the late 2000s, CRTs have been superseded by flat-panel display technologies such as LCD, plasma display, and OLED displays which are cheaper to manufacture and run, as well as significantly lighter and thinner. Flat-panel displays can also be made in very large sizes whereas was about the largest size of a CRT.\nA CRT works by electrically heating a tungsten coil which in turn heats a cathode in the rear of the CRT, causing it to emit electrons which are modulated and focused by electrodes. The electrons are steered by deflection coils or plates, and an anode accelerates them towards the phosphor-coated screen, which generates light when hit by the electrons.\nHistory.\nDiscoveries.\nCathode rays were discovered by Julius Pl\u00fccker and Johann Wilhelm Hittorf. Hittorf observed that some unknown rays were emitted from the cathode (negative electrode) which could cast shadows on the glowing wall of the tube, indicating the rays were traveling in straight lines. In 1890, Arthur Schuster demonstrated cathode rays could be deflected by electric fields, and William Crookes showed they could be deflected by magnetic fields. In 1897, J. J. Thomson succeeded in measuring the mass-to-charge ratio of cathode rays, showing that they consisted of negatively charged particles smaller than atoms, the first \"subatomic particles\", which had already been named \"electrons\" by Irish physicist George Johnstone Stoney in 1891.\nThe earliest version of the CRT was known as the Braun tube, invented by the German physicist Ferdinand Braun in 1897. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen. Braun was the first to conceive the use of a CRT as a display device. The \"Braun tube\" became the foundation of 20th century TV.\nIn 1908, Alan Archibald Campbell-Swinton, fellow of the Royal Society (UK), published a letter in the scientific journal \"Nature\", in which he described how \"distant electric vision\" could be achieved by using a cathode-ray tube (or Braun tube) as both a transmitting and receiving device. He expanded on his vision in a speech given in London in 1911 and reported in \"The Times\" and the \"Journal of the R\u00f6ntgen Society\".\nThe first cathode-ray tube to use a hot cathode was developed by John Bertrand Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922. The introduction of hot cathodes allowed for lower acceleration anode voltages and higher electron beam currents, since the anode now only accelerated the electrons emitted by the hot cathode, and no longer had to have a very high voltage to induce electron emission from the cold cathode.\nDevelopment.\nThe technology of a cathode-ray tube derives from a paper of Karl Ferdinand Braun in 1897 which describes his development of cathode-ray oscilloscope. Braun's paper came out just a few months before JJ Thomson's work that lead to the discovery that cathode-rays are streams of corpuscles now called electrons.\nIn 1926, Kenjiro Takayanagi demonstrated a CRT TV receiver with a mechanical video camera that received images with a 40-line resolution. By 1927, he improved the resolution to 100 lines, which was unrivaled until 1931. By 1928, he was the first to transmit human faces in half-tones on a CRT display.\nIn 1927, Philo Farnsworth created a TV prototype.\nThe CRT was named in 1929 by inventor Vladimir K. Zworykin. He was subsequently hired by RCA, which was granted a trademark for the term \"Kinescope\", RCA's term for a CRT, in 1932; it voluntarily released the term to the public domain in 1950.\nIn the 1930s, Allen B. DuMont made the first CRTs to last 1,000\u00a0hours of use, which was one of the factors that led to the widespread adoption of TV.\nThe first commercially made electronic TV sets with cathode-ray tubes were manufactured by Telefunken in Germany in 1934.\nIn 1947, the cathode-ray tube amusement device, the earliest known interactive electronic game as well as the first to incorporate a cathode-ray tube screen, was created.\nFrom 1949 to the early 1960s, there was a shift from circular CRTs to rectangular CRTs, although the first rectangular CRTs were made in 1938 by Telefunken. While circular CRTs were the norm, European TV sets often blocked portions of the screen to make it appear somewhat rectangular while American sets often left the entire front of the CRT exposed or only blocked the upper and lower portions of the CRT.\nIn 1954, RCA produced some of the first color CRTs, the 15GP22 CRTs used in the CT-100, the first color TV set to be mass produced. The first rectangular color CRTs were also made in 1954. However, the first rectangular color CRTs to be offered to the public were made in 1963. One of the challenges that had to be solved to produce the rectangular color CRT was convergence at the corners of the CRT. In 1965, brighter rare earth phosphors began replacing dimmer and cadmium-containing red and green phosphors. Eventually blue phosphors were replaced as well.\nThe size of CRTs increased over time, from 20 inches in 1938, to 21 inches in 1955, 25 inches by 1974, 30 inches by 1980, 35 inches by 1985, and 43\u00a0inches by 1989. The world's largest was the Sony KX-45ED1 at 45 inches but only one known working model exists.\nIn 1960, the Aiken tube was invented. It was a CRT in a flat-panel display format with a single electron gun. Deflection was electrostatic and magnetic, but due to patent problems, it was never put into production. It was also envisioned as a head-up display in aircraft. By the time patent issues were solved, RCA had already invested heavily in conventional CRTs.\n1968 marked the release of Sony Trinitron brand with the model KV-1310, which was based on Aperture Grille technology. It was acclaimed to have improved the output brightness. The Trinitron screen was identical with its upright cylindrical shape due to its unique triple cathode single gun construction. \nIn 1987, flat-screen CRTs were developed by Zenith for computer monitors, reducing reflections and helping increase image contrast and brightness. Such CRTs were expensive, which limited their use to computer monitors. Attempts were made to produce flat-screen CRTs using inexpensive and widely available float glass.\nIn 1990, the first CRT with HD resolution, the Sony KW-3600HD, was released to the market. It is considered to be \"historical material\" by Japan's national museum.\nThe Sony KWP-5500HD, an HD CRT projection TV, was released in 1992.\nIn the mid-1990s, some 160 million CRTs were made per year.\nIn the mid-2000s, Canon and Sony presented the surface-conduction electron-emitter display and field-emission displays, respectively. They both were flat-panel displays that had one (SED) or several (FED) electron emitters per subpixel in place of electron guns. The electron emitters were placed on a sheet of glass and the electrons were accelerated to a nearby sheet of glass with phosphors using an anode voltage. The electrons were not focused, making each subpixel essentially a flood beam CRT. They were never put into mass production as LCD technology was significantly cheaper, eliminating the market for such displays.\nThe last large-scale manufacturer of (in this case, recycled) CRTs, Videocon, ceased in 2015. CRT TVs stopped being made around the same time.\nIn 2012, Samsung SDI and several other major companies were fined by the European Commission for price fixing of TV cathode-ray tubes.\nThe same occurred in 2015 in the US and in Canada in 2018.\nWorldwide sales of CRT computer monitors peaked in 2000, at 90 million units, while those of CRT TVs peaked in 2005 at 130 million units.\nDecline.\nBeginning in the late 1990s to the early 2000s, CRTs began to be replaced with LCDs, starting first with computer monitors smaller than 15 inches in size, largely because of their lower bulk. Among the first manufacturers to stop CRT production was Hitachi in 2001, followed by Sony in Japan in 2004. Flat-panel displays dropped in price and started significantly displacing cathode-ray tubes in the 2000s. LCD monitor sales began exceeding those of CRTs in 2003\u20132004 and LCD TV sales started exceeding those of CRTs in some markets in 2005. Samsung SDI stopped CRT production in 2012.\nDespite being a mainstay of display technology for decades, CRT-based computer monitors and TVs are now obsolete. Demand for CRT screens dropped in the late 2000s. Despite efforts from Samsung and LG to make CRTs competitive with their LCD and plasma counterparts, offering slimmer and cheaper models to compete with similarly sized and more expensive LCDs, CRTs eventually became obsolete and were relegated to developing markets and vintage enthusiasts once LCDs fell in price, with their lower bulk, weight and ability to be wall mounted coming as advantages.\nSome industries still use CRTs because it is too much effort, downtime, or cost to replace them, or there is no substitute available; a notable example is the airline industry. Planes such as the Boeing 747-400 and the Airbus A320 used CRT instruments in their glass cockpits instead of mechanical instruments. Airlines such as Lufthansa still use CRT technology, which also uses floppy disks for navigation updates. They are also used in some military equipment for similar reasons. As of 2022[ [update]], at least one company manufactures new CRTs for these markets.\nA popular consumer usage of CRTs is for retro gaming. Some games are impossible to play without CRT display hardware. Light guns only work on CRTs because they depend on the progressive timing properties of CRTs. Another reason people use CRTs is due to the natural blending of the image on the displays. Some games designed for CRT displays exploit this, and use the blending of detail and color to turn raw pixels into softer images for aesthetic appeal and variety. In addition, compared to LCD Displays, CRTs have a reduced input latency between when one touches the controller and the action is reflected on screen; allowing for more precise control for consumers.\nConstructions.\nBody.\nThe body of a CRT is usually made up of three parts: A screen/faceplate/panel, a cone/funnel, and a neck. The joined screen, funnel and neck are known as the bulb or envelope.\nThe neck is made from a glass tube while the funnel and screen are made by pouring and then pressing glass into a mold. The glass, known as CRT glass or TV glass, needs special properties to shield against x-rays while providing adequate light transmission in the screen or being very electrically insulating in the funnel and neck. The formulation that gives the glass its properties is also known as the melt. The glass is of very high quality, being almost contaminant and defect free. Most of the costs associated with glass production come from the energy used to melt the raw materials into glass. Glass furnaces for CRT glass production have several taps to allow molds to be replaced without stopping the furnace, to allow production of CRTs of several sizes. Only the glass used on the screen needs to have precise optical properties.\nThe optical properties of the glass used on the screen affect color reproduction and purity in color CRTs. Transmittance, or how transparent the glass is, may be adjusted to be more transparent to certain colors (wavelengths) of light. Transmittance is measured at the center of the screen with a 546\u00a0nm wavelength light, and a 10.16mm thick screen. Transmittance goes down with increasing thickness. Standard transmittances for Color CRT screens are 86%, 73%, 57%, 46%, 42% and 30%. Lower transmittances are used to improve image contrast but they put more stress on the electron gun, requiring more power on the electron gun for a higher electron beam power to light the phosphors more brightly to compensate for the reduced transmittance. The transmittance must be uniform across the screen to ensure color purity. The radius (curvature) of screens has increased (grown less curved) over time, from 30 to 68 inches, ultimately evolving into completely flat screens, reducing reflections. The thickness of both curved and flat screens gradually increases from the center outwards, and with it, transmittance is gradually reduced. This means that flat-screen CRTs may not be completely flat on the inside.\nThe glass used in CRTs arrives from the glass factory to the CRT factory as either separate screens and funnels with flame-fused necks, for Color CRTs, or as bulbs made up of a flame-fused screen, funnel and neck. There were several glass formulations for different types of CRTs, that were classified using codes specific to each glass manufacturer. The compositions of the melts were also specific to each manufacturer. Those optimized for high color purity and contrast were doped with Neodymium, while those for monochrome CRTs were tinted to differing levels, depending on the formulation used and had transmittances of 42% or 30%. Purity is ensuring that the correct colors are activated (for example, ensuring that red is displayed uniformly across the screen) while convergence ensures that images are not distorted. Convergence may be modified using a cross hatch pattern.\nCRT glass used to be made by dedicated companies such as AGC Inc., O-I Glass, Samsung Corning Precision Materials, Corning Inc., and Nippon Electric Glass; others such as Videocon, Sony for the US market and Thomson made their own glass.\nThe funnel and the neck are made of leaded potash-soda glass or lead silicate glass formulation to shield against x-rays generated by high voltage electrons as they decelerate after striking a target, such as the phosphor screen or shadow mask of a color CRT. The velocity of the electrons depends on the anode voltage of the CRT; the higher the voltage, the higher the speed. The amount of x-rays emitted by a CRT can also lowered by reducing the brightness of the image. Leaded glass is used because it is inexpensive, while also shielding heavily against x-rays, although some funnels may also contain barium. The screen is usually instead made out of a special lead-free silicate glass formulation with barium and strontium to shield against x-rays, as it doesn't brown unlike glass containing lead. Another glass formulation uses 2\u20133% of lead on the screen. Alternatively zirconium can also be used on the screen in combination with barium, instead of lead.\nMonochrome CRTs may have a tinted barium-lead glass formulation in both the screen and funnel, with a potash-soda lead glass in the neck; the potash-soda and barium-lead formulations have different thermal expansion coefficients. The glass used in the neck must be an excellent electrical insulator to contain the voltages used in the electron optics of the electron gun, such as focusing lenses. The lead in the glass causes it to brown (darken) with use due to x-rays, usually the CRT cathode wears out due to cathode poisoning before browning becomes apparent. The glass formulation determines the highest possible anode voltage and hence the maximum possible CRT screen size. For color, maximum voltages are often 24\u201332\u00a0kV, while for monochrome it is usually 21 or 24.5\u00a0kV, limiting the size of monochrome CRTs to 21\u00a0inches, or ~1\u00a0kV per inch. The voltage needed depends on the size and type of CRT. Since the formulations are different, they must be compatible with one another, having similar thermal expansion coefficients. The screen may also have an anti-glare or anti-reflective coating, or be ground to prevent reflections. CRTs may also have an anti-static coating.\nThe leaded glass in the funnels of CRTs may contain 21\u201325% of lead oxide (PbO), The neck may contain 30\u201340% of lead oxide, and the screen may contain 12% of barium oxide, and 12% of strontium oxide. A typical CRT contains several kilograms of lead as lead oxide in the glass depending on its size; 12 inch CRTs contain 0.5\u00a0kg of lead in total while 32\u00a0inch CRTs contain up to 3\u00a0kg. Strontium oxide began being used in CRTs, its major application, in the 1970s. Before this, CRTs used lead on the faceplate.\nSome early CRTs used a metal funnel insulated with polyethylene instead of glass with conductive material. Others had ceramic or blown Pyrex instead of pressed glass funnels. Early CRTs did not have a dedicated anode cap connection; the funnel was the anode connection, so it was live during operation.\nThe funnel is coated on the inside and outside with a conductive coating, making the funnel a capacitor, helping stabilize and filter the anode voltage of the CRT, and significantly reducing the amount of time needed to turn on a CRT. The stability provided by the coating solved problems inherent to early power supply designs, as they used vacuum tubes. Because the funnel is used as a capacitor, the glass used in the funnel must be an excellent electrical insulator (dielectric). The inner coating has a positive voltage (the anode voltage that can be several kV) while the outer coating is connected to ground. CRTs powered by more modern power supplies do not need to be connected to ground, due to the more robust design of modern power supplies. The value of the capacitor formed by the funnel is 5\u201310\u00a0nF, although at the voltage the anode is normally supplied with. The capacitor formed by the funnel can also suffer from dielectric absorption, similarly to other types of capacitors. Because of this CRTs have to be discharged before handling to prevent injury.\nThe depth of a CRT is related to its screen size. Usual deflection angles were 90\u00b0 for computer monitor CRTs and small CRTs and 110\u00b0 which was the standard in larger TV CRTs, with 120 or 125\u00b0 being used in slim CRTs made since 2001\u20132005 in an attempt to compete with LCD TVs. Over time, deflection angles increased as they became practical, from 50\u00b0 in 1938 to 110\u00b0 in 1959, and 125\u00b0 in the 2000s. 140\u00b0 deflection CRTs were researched but never commercialized, as convergence problems were never resolved.\nSize and weight.\nThe size of a CRT can be measured by the screen's \"entire\" area (or face diagonal) or alternatively by only its \"viewable\" area (or diagonal) that is coated by phosphor and surrounded by black edges.\nWhile the viewable area may be rectangular, the edges of the CRT may have a curvature (e.g. black stripe CRTs, first made by Toshiba in 1972) or the edges may be black and truly flat (e.g. Flatron CRTs), or the viewable area may follow the curvature of the edges of the CRT (with or without black edges or curved edges).\nSmall CRTs below 3\u00a0inches were made for handheld TVs such as the MTV-1 and viewfinders in camcorders. In these, there may be no black edges, that are however truly flat.\nMost of the weight of a CRT comes from the thick glass screen, which comprises 65% of the total weight of a CRT and limits its practical size (see ). The funnel and neck glass comprise the remaining 30% and 5% respectively. The glass in the funnel can vary in thickness, to join the thin neck with the thick screen. Chemically or thermally tempered glass may be used to reduce the weight of the CRT glass.\nAnode.\nThe outer conductive coating is connected to ground while the inner conductive coating is connected using the anode button/cap through a series of capacitors and diodes (a Cockcroft\u2013Walton generator) to the high voltage flyback transformer; the inner coating is the anode of the CRT, which, together with an electrode in the electron gun, is also known as the final anode. The inner coating is connected to the electrode using springs. The electrode forms part of a bipotential lens. The capacitors and diodes serve as a voltage multiplier for the current delivered by the flyback.\nFor the inner funnel coating, monochrome CRTs use aluminum while color CRTs use aquadag; Some CRTs may use iron oxide on the inside. On the outside, most CRTs use aquadag. Aquadag is an electrically conductive graphite-based paint. In color CRTs, the aquadag is sprayed onto the interior of the funnel whereas historically aquadag was painted into the interior of monochrome CRTs.\nThe anode is used to accelerate the electrons towards the screen and also collects the secondary electrons that are emitted by the phosphor particles in the vacuum of the CRT.\nThe anode cap connection in modern CRTs must be able to handle up to 55\u201360kV depending on the size and brightness of the CRT. Higher voltages allow for larger CRTs, higher image brightness, or a tradeoff between the two. It consists of a metal clip that expands on the inside of an anode button that is embedded on the funnel glass of the CRT. The connection is insulated by a silicone suction cup, possibly also using silicone grease to prevent corona discharge.\nThe anode button must be specially shaped to establish a hermetic seal between the button and funnel. X-rays may leak through the anode button, although that may not be the case in newer CRTs starting from the late 1970s to early 1980s, thanks to a new button and clip design. The button may consist of a set of 3 nested cups, with the outermost cup being made of a Nickel\u2013Chromium\u2013Iron alloy containing 40\u201349% of Nickel and 3\u20136% of Chromium to make the button easy to fuse to the funnel glass, with a first inner cup made of thick inexpensive iron to shield against x-rays, and with the second innermost cup also being made of iron or any other electrically conductive metal to connect to the clip. The cups must be heat resistant enough and have similar thermal expansion coefficients similar to that of the funnel glass to withstand being fused to the funnel glass. The inner side of the button is connected to the inner conductive coating of the CRT. The anode button may be attached to the funnel while its being pressed into shape in a mold. Alternatively, the x-ray shielding may instead be built into the clip.\nThe flyback transformer is also known as an IHVT (Integrated High Voltage Transformer) if it includes a voltage multiplier. The flyback uses a ceramic or powdered iron core to enable efficient operation at high frequencies. The flyback contains one primary and many secondary windings that provide several different voltages. The main secondary winding supplies the voltage multiplier with voltage pulses to ultimately supply the CRT with the high anode voltage it uses, while the remaining windings supply the CRT's filament voltage, keying pulses, focus voltage and voltages derived from the scan raster. When the transformer is turned off, the flyback's magnetic field quickly collapses which induces high voltage in its windings. The speed at which the magnetic field collapses determines the voltage that is induced, so the voltage increases alongside its speed. A capacitor (Retrace Timing Capacitor) or series of capacitors (to provide redundancy) is used to slow the collapse of the magnetic field.\nThe design of the high voltage power supply in a product using a CRT has an influence in the amount of x-rays emitted by the CRT. The amount of emitted x-rays increases with both higher voltages and currents. If the product such as a TV set uses an unregulated high voltage power supply, meaning that anode and focus voltage go down with increasing electron current when displaying a bright image, the amount of emitted x-rays is as its highest when the CRT is displaying a moderately bright images, since when displaying dark or bright images, the higher anode voltage counteracts the lower electron beam current and vice versa respectively. The high voltage regulator and rectifier vacuum tubes in some old CRT TV sets may also emit x-rays.\nElectron gun.\nThe electron gun emits the electrons that ultimately hit the phosphors on the screen of the CRT. The electron gun contains a heater, which heats a cathode, which generates electrons that, using grids, are focused and ultimately accelerated into the screen of the CRT. The acceleration occurs in conjunction with the inner aluminum or aquadag coating of the CRT. The electron gun is positioned so that it aims at the center of the screen. It is inside the neck of the CRT, and it is held together and mounted to the neck using glass beads or glass support rods, which are the glass strips on the electron gun. The electron gun is made separately and then placed inside the neck through a process called \"winding\", or sealing. The electron gun has a glass wafer that is fused to the neck of the CRT. The connections to the electron gun penetrate the glass wafer. Once the electron gun is inside the neck, its metal parts (grids) are arced between each other using high voltage to smooth any rough edges in a process called spot knocking, to prevent the rough edges in the grids from generating secondary electrons.\nConstruction and method of operation.\nThe electron gun has an indirectly heated hot cathode that is heated by a tungsten filament heating element; the heater may draw 0.5\u20132\u00a0A of current depending on the CRT. The voltage applied to the heater can affect the life of the CRT. Heating the cathode energizes the electrons in it, aiding electron emission, while at the same time current is supplied to the cathode; typically anywhere from 140\u00a0mA at 1.5\u00a0V to 600\u00a0mA at 6.3\u00a0V. The cathode creates an electron cloud (emits electrons) whose electrons are extracted, accelerated and focused into an electron beam. Color CRTs have three cathodes: one for red, green and blue. The heater sits inside the cathode but does not touch it; the cathode has its own separate electrical connection. The cathode is a material coated onto a piece of nickel which provides the electrical connection and structural support; the heater sits inside this piece without touching it.\nThere are several short circuits that can occur in a CRT electron gun. One is a heater-to-cathode short, that causes the cathode to permanently emit electrons which may cause an image with a bright red, green or blue tint with retrace lines, depending on the cathode (s) affected. Alternatively, the cathode may short to the control grid, possibly causing similar effects, or, the control grid and screen grid (G2) can short causing a very dark image or no image at all. The cathode may be surrounded by a shield to prevent sputtering.\nThe cathode is a layer of barium oxide which is coated on a piece of nickel for electrical and mechanical support. The barium oxide must be activated by heating to enable it to release electrons. Activation is necessary because barium oxide is not stable in air, so it is applied to the cathode as barium carbonate, which cannot emit electrons. Activation heats the barium carbonate to decompose it into barium oxide and carbon dioxide while forming a thin layer of metallic barium on the cathode. Activation is done when forming the vacuum (described in ). After activation, the oxide can become damaged by several common gases such as water vapor, carbon dioxide, and oxygen. Alternatively, barium strontium calcium carbonate may be used instead of barium carbonate, yielding barium, strontium and calcium oxides after activation. During operation, the barium oxide is heated to 800\u20131000\u00b0C, at which point it starts shedding electrons.\nSince it is a hot cathode, it is prone to cathode poisoning, which is the formation of a positive ion layer that prevents the cathode from emitting electrons, reducing image brightness significantly or completely and causing focus and intensity to be affected by the frequency of the video signal preventing detailed images from being displayed by the CRT. The positive ions come from leftover air molecules inside the CRT or from the cathode itself that react over time with the surface of the hot cathode. Reducing metals such as manganese, zirconium, magnesium, aluminum or titanium may be added to the piece of nickel to lengthen the life of the cathode, as during activation, the reducing metals diffuse into the barium oxide, improving its lifespan, especially at high electron beam currents. In color CRTs with red, green and blue cathodes, one or more cathodes may be affected independently of the others, causing total or partial loss of one or more colors. CRTs can wear or burn out due to cathode poisoning. Cathode poisoning is accelerated by increased cathode current (overdriving). In color CRTs, since there are three cathodes, one for red, green and blue, a single or more poisoned cathode may cause the partial or complete loss of one or more colors, tinting the image. The layer may also act as a capacitor in series with the cathode, inducing thermal lag. The cathode may instead be made of scandium oxide or incorporate it as a dopant, to delay cathode poisoning, extending the life of the cathode by up to 15%.\nThe rate of emission of electrons from the cathodes is related to their surface area. A cathode with more surface area creates more electrons, in a larger electron cloud, which makes focusing the electron cloud into an electron beam more difficult. Normally, only a part of the cathode emits electrons unless the CRT displays images with parts that are at full image brightness; only the parts at full brightness cause all of the cathode to emit electrons. The area of the cathode that emits electrons grows from the center outwards as brightness increases, so cathode wear may be uneven. When only the center of the cathode is worn, the CRT may light brightly those parts of images that have full image brightness but not show darker parts of images at all, in such a case the CRT displays a poor gamma characteristic.\nA voltage negative with respect to the cathode is applied to the first (control) grid (G1) to control the emission of electrons into the rest of the electron gun. G1 in practice is a Wehnelt cylinder. The brightness of the image on the screen depends on both the anode voltage and the electron beam current and in practise the latter is constant, while the former is controlled by varying the difference in voltage between the cathode and the G1 control grid. The second (screen) grid of the gun (G2) then accelerates the electrons towards the screen using several hundred DC volts. Then a third grid (G3) electrostatically focuses the electron beam before it is deflected and later accelerated by the anode voltage onto the screen. Electrostatic focusing of the electron beam may be accomplished using an einzel lens energized at up to 600 volts. Before electrostatic focusing, focusing the electron beam required a large, heavy and complex mechanical focusing system placed outside the electron gun.\nHowever, electrostatic focusing cannot be accomplished near the final anode of the CRT due to its high voltage in the dozens of Kilovolts, so a high voltage (\u2248600\u20138000\u00a0V) electrode, together with an electrode at the final anode voltage of the CRT, may be used for focusing instead. Such an arrangement is called a bipotential lens, which also offers higher performance than an einzel lens, or, focusing may be accomplished using a magnetic focusing coil together with a high anode voltage of dozens of kilovolts. However, magnetic focusing is expensive to implement, so it is rarely used in practice. Some CRTs may use two grids and lenses to focus the electron beam. The focus voltage is generated in the flyback using a subset of the flyback's high voltage winding in conjunction with a resistive voltage divider. The focus electrode is connected alongside the other connections that are in the neck of the CRT.\nThere is a voltage called cutoff voltage which is the voltage that creates black on the screen since it causes the image on the screen created by the electron beam to disappear, the voltage is applied to G1. In a color CRT with three guns, the guns have different cutoff voltages. Many CRTs share grid G1 and G2 across all three guns, increasing image brightness and simplifying adjustment since on such CRTs there is a single cutoff voltage for all three guns (since G1 is shared across all guns). but placing additional stress on the video amplifier used to feed video into the electron gun's cathodes, since the cutoff voltage becomes higher. Monochrome CRTs do not suffer from this problem. In monochrome CRTs video is fed to the gun by varying the voltage on the first control grid.\nDuring retracing of the electron beam, the preamplifier that feeds the video amplifier is disabled and the video amplifier is biased to a voltage higher than the cutoff voltage to prevent retrace lines from showing, or G1 can have a large negative voltage applied to it to prevent electrons from getting out of the cathode. This is known as blanking. (see Vertical blanking interval and Horizontal blanking interval.) Incorrect biasing can lead to visible retrace lines on one or more colors, creating retrace lines that are tinted or white (for example, tinted red if the red color is affected, tinted magenta if the red and blue colors are affected, and white if all colors are affected). Alternatively, the amplifier may be driven by a video processor that also introduces an OSD (On Screen Display) into the video stream that is fed into the amplifier, using a fast blanking signal. TV sets and computer monitors that incorporate CRTs need a DC restoration circuit to provide a video signal to the CRT with a DC component, restoring the original brightness of different parts of the image.\nThe electron beam may be affected by the Earth's magnetic field, causing it to normally enter the focusing lens off-center; this can be corrected using astigmation controls. Astigmation controls are both magnetic and electronic (dynamic); magnetic does most of the work while electronic is used for fine adjustments. One of the ends of the electron gun has a glass disk, the edges of which are fused with the edge of the neck of the CRT, possibly using frit; the metal leads that connect the electron gun to the outside pass through the disk.\nSome electron guns have a quadrupole lens with dynamic focus to alter the shape and adjust the focus of the electron beam, varying the focus voltage depending on the position of the electron beam to maintain image sharpness across the entire screen, specially at the corners. They may also have a bleeder resistor to derive voltages for the grids from the final anode voltage.\nAfter the CRTs were manufactured, they were aged to allow cathode emission to stabilize.\nThe electron guns in color CRTs are driven by a video amplifier which takes a signal per color channel and amplifies it to 40\u2013170\u00a0V per channel, to be fed into the electron gun's cathodes; each electron gun has its own channel (one per color) and all channels may be driven by the same amplifier, which internally has three separate channels. The amplifier's capabilities limit the resolution, refresh rate and contrast ratio of the CRT, as the amplifier needs to provide high bandwidth and voltage variations at the same time; higher resolutions and refresh rates need higher bandwidths (speed at which voltage can be varied and thus switching between black and white) and higher contrast ratios need higher voltage variations or amplitude for lower black and higher white levels. 30\u00a0MHz of bandwidth can usually provide 720p or 1080i resolution, while 20\u00a0MHz usually provides around 600 (horizontal, from top to bottom) lines of resolution, for example. The difference in voltage between the cathode and the control grid is what modulates the electron beam, modulating its current and thus creating shades of colors which create the image line by line and this can also affect the brightness of the image. The phosphors used in color CRTs produce different amounts of light for a given amount of energy, so to produce white on a color CRT, all three guns must output differing amounts of energy. The gun that outputs the most energy is the red gun since the red phosphor emits the least amount of light.\nGamma.\nCRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).\nDeflection.\nThere are two types of deflection: magnetic and electrostatic. Magnetic is usually used in TVs and monitors as it allows for higher deflection angles (and hence shallower CRTs) and deflection power (which allows for higher electron beam current and hence brighter images) while avoiding the need for high voltages for deflection of up to 2\u00a0kV, while oscilloscopes often use electrostatic deflection since the raw waveforms captured by the oscilloscope can be applied directly (after amplification) to the vertical electrostatic deflection plates inside the CRT.\nMagnetic deflection.\nThose that use magnetic deflection may use a yoke that has two pairs of deflection coils; one pair for vertical, and another for horizontal deflection. The yoke can be bonded (be integral) or removable. Those that were bonded used glue or a plastic to bond the yoke to the area between the neck and the funnel of the CRT while those with removable yokes are clamped. The yoke generates heat whose removal is essential since the conductivity of glass goes up with increasing temperature, the glass needs to be insulating for the CRT to remain usable as a capacitor. The temperature of the glass below the yoke is thus checked during the design of a new yoke. The yoke contains the deflection and convergence coils with a ferrite core to reduce loss of magnetic force as well as the magnetized rings used to align or adjust the electron beams in color CRTs (The color purity and convergence rings, for example) and monochrome CRTs. The yoke may be connected using a connector, the order in which the deflection coils of the yoke are connected determines the orientation of the image displayed by the CRT. The deflection coils may be held in place using polyurethane glue.\nThe deflection coils are driven by sawtooth signals that may be delivered through VGA as horizontal and vertical sync signals. A CRT needs two deflection circuits: a horizontal and a vertical circuit, which are similar except that the horizontal circuit runs at a much higher frequency (a Horizontal scan rate) of 15\u2013240\u00a0kHz depending on the refresh rate of the CRT and the number of horizontal lines to be drawn (the vertical resolution of the CRT). The higher frequency makes it more susceptible to interference, so an automatic frequency control (AFC) circuit may be used to lock the phase of the horizontal deflection signal to that of a sync signal, to prevent the image from becoming distorted diagonally. The vertical frequency varies according to the refresh rate of the CRT. So a CRT with a 60\u00a0Hz refresh rate has a vertical deflection circuit running at 60\u00a0Hz. The horizontal and vertical deflection signals may be generated using two circuits that work differently; the horizontal deflection signal may be generated using a voltage controlled oscillator (VCO) while the vertical signal may be generated using a triggered relaxation oscillator. In many TVs, the frequencies at which the deflection coils run is in part determined by the inductance value of the coils. CRTs had differing deflection angles; the higher the deflection angle, the shallower the CRT for a given screen size, but at the cost of more deflection power and lower optical performance.\nHigher deflection power means more current is sent to the deflection coils to bend the electron beam at a higher angle, which in turn may generate more heat or require electronics that can handle the increased power. Heat is generated due to resistive and core losses. The deflection power is measured in mA per inch. The vertical deflection coils may require ~24 volts while the horizontal deflection coils require ~120 volts to operate.\nThe deflection coils are driven by deflection amplifiers. The horizontal deflection coils may also be driven in part by the horizontal output stage of a TV set. The stage contains a capacitor that is in series with the horizontal deflection coils that performs several functions, among them are: shaping the sawtooth deflection signal to match the curvature of the CRT and centering the image by preventing a DC bias from developing on the coil. At the beginning of retrace, the magnetic field of the coil collapses, causing the electron beam to return to the center of the screen, while at the same time the coil returns energy into capacitors, the energy of which is then used to force the electron beam to go to the left of the screen.\nDue to the high frequency at which the horizontal deflection coils operate, the energy in the deflection coils must be recycled to reduce heat dissipation. Recycling is done by transferring the energy in the deflection coils' magnetic field to a set of capacitors. The voltage on the horizontal deflection coils is negative when the electron beam is on the left side of the screen and positive when the electron beam is on the right side of the screen. The energy required for deflection is dependent on the energy of the electrons. Higher energy (voltage and/or current) electron beams need more energy to be deflected, and are used to achieve higher image brightness.\nElectrostatic deflection.\nMostly used in oscilloscopes. Deflection is carried out by applying a voltage across two pairs of plates, one for horizontal, and the other for vertical deflection. The electron beam is steered by varying the voltage difference across plates in a pair; For example, applying a voltage to the upper plate of the vertical deflection pair, while keeping the voltage in the bottom plate at 0 volts, will cause the electron beam to be deflected towards the upper part of the screen; increasing the voltage in the upper plate while keeping the bottom plate at 0 will cause the electron beam to be deflected to a higher point in the screen (will cause the beam to be deflected at a higher deflection angle). The same applies with the horizontal deflection plates. Increasing the length and proximity between plates in a pair can also increase the deflection angle.\nBurn-in.\nBurn-in is when images are physically \"burned\" into the screen of the CRT; this occurs due to degradation of the phosphors due to prolonged electron bombardment of the phosphors, and happens when a fixed image or logo is left for too long on the screen, causing it to appear as a \"ghost\" image or, in severe cases, also when the CRT is off. To counter this, screensavers were used in computers to minimize burn-in. Burn-in is not exclusive to CRTs, as it also happens to plasma displays and OLED displays.\nEvacuation.\nThe CRT's partial vacuum of to or less is evacuated or exhausted in a ~375\u2013475\u00a0\u00b0C oven in a process called \"baking\" or \"bake-out\". The evacuation process also outgasses any materials inside the CRT, while decomposing others such as the polyvinyl alcohol used to apply the phosphors. The heating and cooling are done gradually to avoid inducing stress, stiffening and possibly cracking the glass; the oven heats the gases inside the CRT, increasing the speed of the gas molecules which increases the chances of them getting drawn out by the vacuum pump. The temperature of the CRT is kept to below that of the oven, and the oven starts to cool just after the CRT reaches 400\u00a0\u00b0C, or, the CRT was kept at a temperature higher than 400\u00a0\u00b0C for up to 15\u201355 minutes. The CRT was heated during or after evacuation, and the heat may have been used simultaneously to melt the frit in the CRT, joining the screen and funnel. The pump used is a turbomolecular pump or a diffusion pump. Formerly mercury vacuum pumps were also used. After baking, the CRT is disconnected (\"sealed or tipped off\") from the vacuum pump. The getter is then fired using an RF (induction) coil. The getter is usually in the funnel or in the neck of the CRT. The getter material which is often barium-based, catches any remaining gas particles as it evaporates due to heating induced by the RF coil (that may be combined with exothermic heating within the material); the vapor fills the CRT, trapping any gas molecules that it encounters and condenses on the inside of the CRT forming a layer that contains trapped gas molecules. Hydrogen may be present in the material to help distribute the barium vapor. The material is heated to temperatures above 1000\u00a0\u00b0C, causing it to evaporate. Partial loss of vacuum in a CRT can result in a hazy image, blue glowing in the neck of the CRT, flashovers, loss of cathode emission or focusing problems.\nRebuilding.\nCRTs used to be rebuilt; repaired or refurbished. The rebuilding process included the disassembly of the CRT, the disassembly and repair or replacement of the electron gun(s), the removal and redeposition of phosphors and aquadag, etc. Rebuilding was popular until the 1960s because CRTs were expensive and wore out quickly, making repair worth it. The last CRT rebuilder in the US closed in 2010, and the last in Europe, RACS, which was located in France, closed in 2013.\nReactivation.\nAlso known as rejuvenation, the goal is to temporarily restore the brightness of a worn CRT. This is often done by carefully increasing the voltage on the cathode heater and the current and voltage on the control grids of the electron gun manually. Some rejuvenators can also fix heater-to-cathode shorts by running a capacitive discharge through the short.\nPhosphors.\nPhosphors in CRTs emit secondary electrons due to them being inside the vacuum of the CRT. The secondary electrons are collected by the anode of the CRT. Secondary electrons generated by phosphors need to be collected to prevent charges from developing in the screen, which would lead to reduced image brightness since the charge would repel the electron beam.\nThe phosphors used in CRTs often contain rare earth metals, replacing earlier dimmer phosphors. Early red and green phosphors contained Cadmium, and some black and white CRT phosphors also contained beryllium in the form of Zinc beryllium silicate, although white phosphors containing cadmium, zinc and magnesium with silver, copper or manganese as dopants were also used. The rare earth phosphors used in CRTs are more efficient (produce more light) than earlier phosphors. The phosphors adhere to the screen because of Van der Waals and electrostatic forces. Phosphors composed of smaller particles adhere more strongly to the screen. The phosphors together with the carbon used to prevent light bleeding (in color CRTs) can be easily removed by scratching.\nSeveral dozen types of phosphors were available for CRTs. Phosphors were classified according to color, persistence, luminance rise and fall curves, color depending on anode voltage (for phosphors used in penetration CRTs), Intended use, chemical composition, safety, sensitivity to burn-in, and secondary emission properties. Examples of rare earth phosphors are yttrium oxide for red and yttrium silicide for blue in beam index tubes, while examples of earlier phosphors are copper cadmium sulfide for red,\nSMPTE-C phosphors have properties defined by the SMPTE-C standard, which defines a color space of the same name. The standard prioritizes accurate color reproduction, which was made difficult by the different phosphors and color spaces used in the NTSC and PAL color systems. PAL TV sets have subjectively better color reproduction due to the use of saturated green phosphors, which have relatively long decay times that are tolerated in PAL since there is more time in PAL for phosphors to decay, due to its lower framerate. SMPTE-C phosphors were used in professional video monitors.\nThe phosphor coating on monochrome and color CRTs may have an aluminum coating on its rear side used to reflect light forward, provide protection against ions to prevent ion burn by negative ions on the phosphor, manage heat generated by electrons colliding against the phosphor, prevent static build up that could repel electrons from the screen, form part of the anode and collect the secondary electrons generated by the phosphors in the screen after being hit by the electron beam, providing the electrons with a return path. The electron beam passes through the aluminum coating before hitting the phosphors on the screen; the aluminum attenuates the electron beam voltage by about 1\u00a0kV. A film or lacquer may be applied to the phosphors to reduce the surface roughness of the surface formed by the phosphors to allow the aluminum coating to have a uniform surface and prevent it from touching the glass of the screen. This is known as filming. The lacquer contains solvents that are later evaporated; the lacquer may be chemically roughened to cause an aluminum coating with holes to be created to allow the solvents to escape.\nPhosphor persistence.\nVarious phosphors are available depending upon the needs of the measurement or display application. The brightness, color, and persistence of the illumination depends upon the type of phosphor used on the CRT screen. Phosphors are available with persistences ranging from less than one microsecond to several seconds. For visual observation of brief transient events, a long persistence phosphor may be desirable. For events which are fast and repetitive, or high frequency, a short-persistence phosphor is generally preferable. The phosphor persistence must be low enough to avoid smearing or ghosting artifacts at high refresh rates.\nLimitations and workarounds.\nBlooming.\nVariations in anode voltage can lead to variations in brightness in parts or all of the image, in addition to blooming, shrinkage or the image getting zoomed in or out. Lower voltages lead to blooming and zooming in, while higher voltages do the opposite. Some blooming is unavoidable, which can be seen as bright areas of an image that expand, distorting or pushing aside surrounding darker areas of the same image. Blooming occurs because bright areas have a higher electron beam current from the electron gun, making the beam wider and harder to focus. Poor voltage regulation causes focus and anode voltage to go down with increasing electron beam current.\nDoming.\nDoming is a phenomenon found on some CRT TVs in which parts of the shadow mask become heated. In TVs that exhibit this behavior, it tends to occur in high-contrast scenes in which there is a largely dark scene with one or more localized bright spots. As the electron beam hits the shadow mask in these areas it heats unevenly. The shadow mask warps due to the heat differences, which causes the electron gun to hit the wrong colored phosphors and incorrect colors to be displayed in the affected area. Thermal expansion causes the shadow mask to expand by around 100 microns.\nDuring normal operation, the shadow mask is heated to around 80\u201390\u00a0\u00b0C. Bright areas of images heat the shadow mask more than dark areas, leading to uneven heating of the shadow mask and warping (blooming) due to thermal expansion caused by heating by increased electron beam current. The shadow mask is usually made of steel but it can be made of Invar (a low-thermal expansion Nickel-Iron alloy) as it withstands two to three times more current than conventional masks without noticeable warping, while making higher resolution CRTs easier to achieve. Coatings that dissipate heat may be applied on the shadow mask to limit blooming in a process called blackening.\nBimetal springs may be used in CRTs used in TVs to compensate for warping that occurs as the electron beam heats the shadow mask, causing thermal expansion. The shadow mask is installed to the screen using metal pieces or a rail or frame that is fused to the funnel or the screen glass respectively, holding the shadow mask in tension to minimize warping (if the mask is flat, used in flat-screen CRT computer monitors) and allowing for higher image brightness and contrast.\nAperture grille screens are brighter since they allow more electrons through, but they require support wires. They are also more resistant to warping. Color CRTs need higher anode voltages than monochrome CRTs to achieve the same brightness since the shadow mask blocks most of the electron beam. Slot masks and specially Aperture grilles do not block as many electrons resulting in a brighter image for a given anode voltage, but aperture grille CRTs are heavier. Shadow masks block 80\u201385% of the electron beam while Aperture grilles allow more electrons to pass through.\nHigh voltage.\nImage brightness is related to the anode voltage and to the CRTs size, so higher voltages are needed for both larger screens and higher image brightness. Image brightness is also controlled by the current of the electron beam. Higher anode voltages and electron beam currents also mean higher amounts of x-rays and heat generation since the electrons have a higher speed and energy. Leaded glass and special barium-strontium glass are used to block most x-ray emissions.\nSize.\nA practical limit on the size of a CRT is the weight of the thick glass needed to safely sustain its vacuum, since a CRT's exterior is exposed to the full atmospheric pressure, which for instance totals on a 27-inch (400\u00a0in2) screen. For example, the large 43-inch Sony PVM-4300 weighs , much heavier than 32-inch CRTs (up to ) and 19-inch CRTs (up to ). Much lighter flat panel TVs are only ~ for 32-inch and for 19-inch.\nSize is also limited by anode voltage, as it would require a higher dielectric strength to prevent arcing and the electrical losses and ozone generation it causes, without sacrificing image brightness.\nShadow masks also become more difficult to make with increasing resolution and size.\nLimits imposed by deflection.\nAt high deflection angles, resolutions and refresh rates (since higher resolutions and refresh rates require significantly higher frequencies to be applied to the horizontal deflection coils), the deflection yoke starts to produce large amounts of heat, due to the need to move the electron beam at a higher angle, which in turn requires exponentially larger amounts of power. As an example, to increase the deflection angle from 90 to 120\u00b0, power consumption of the yoke must also go up from 40 watts to 80 watts, and to increase it further from 120 to 150\u00b0, deflection power must again go up from 80 to 160 watts. This normally makes CRTs that go beyond certain deflection angles, resolutions and refresh rates impractical, since the coils would generate too much heat due to resistance caused by the skin effect, surface and eddy current losses, and/or possibly causing the glass underneath the coil to become conductive (as the electrical conductivity of glass increases with increasing temperature). Some deflection yokes are designed to dissipate the heat that comes from their operation. Higher deflection angles in color CRTs directly affect convergence at the corners of the screen which requires additional compensation circuitry to handle electron beam power and shape, leading to higher costs and power consumption. Higher deflection angles allow a CRT of a given size to be slimmer, however they also impose more stress on the CRT envelope, specially on the panel, the seal between the panel and funnel and on the funnel. The funnel needs to be long enough to minimize stress, as a longer funnel can be better shaped to have lower stress.\nComparison with other technologies.\nOne of the defining points of comparison\nOn CRTs, refresh rates depend on resolution, both of which are ultimately limited by the maximum horizontal scanning frequency of the CRT. Motion blur also depends on the decay time of the phosphors. Phosphors that decay too slowly for a given refresh rate may cause smearing or motion blur on the image. In practice, CRTs are limited to a refresh rate of 160\u00a0Hz. LCDs that can compete with OLED (Dual Layer, and mini-LED LCDs) are not available in high refresh rates, although quantum dot LCDs (QLEDs) are available in high refresh rates (up to 144\u00a0Hz) and are competitive in color reproduction with OLEDs.\nCRT monitors can still outperform LCD and OLED monitors in input lag, as there is no signal processing between the CRT and the display connector of the monitor, since CRT monitors often use VGA which provides an analog signal that can be fed to a CRT directly. Video cards designed for use with CRTs may have a RAMDAC to generate the analog signals needed by the CRT. Also, CRT monitors are often capable of displaying sharp images at several resolutions, an ability known as multisyncing. Due to these reasons, CRTs are often preferred for playing video games made in the early 2000s and prior in spite of their bulk, weight and heat generation, with some pieces of technology requiring a CRT to function due to not being built with the functionality of modern displays in mind.\nCRTs tend to be more durable than their flat panel counterparts, though specialised LCDs that have similar durability also exist.\nTypes.\nCRTs were produced in two major categories, picture tubes and display tubes. Picture tubes were used in TVs while display tubes were used in computer monitors. Display tubes were of higher resolution and when used in computer monitors sometimes had adjustable overscan, or sometimes underscan.\nPicture tube CRTs have overscan, meaning the actual edges of the image are not shown; this is deliberate to allow for adjustment variations between CRT TVs, preventing the ragged edges (due to blooming) of the image from being shown on screen. The shadow mask may have grooves that reflect away the electrons that do not hit the screen due to overscan. Color picture tubes used in TVs were also known as CPTs. CRTs are also sometimes called Braun tubes.\nMonochrome CRTs.\nIf the CRT is in black and white (B&amp;W or monochrome), there is a single electron gun in the neck and the funnel is coated on the inside with aluminum that has been applied by evaporation; the aluminum is evaporated in a vacuum and allowed to condense on the inside of the CRT. This was often done by placing the CRT in a special machine to draw a vacuum within the CRT, evaporate the aluminum inside the CRT using a heater surrounding a piece of aluminum, and then release the vacuum. Aluminum eliminates the need for ion traps, necessary to prevent ion burn on the phosphor, while also reflecting light generated by the phosphor towards the screen, managing heat and absorbing electrons providing a return path for them; previously funnels were coated on the inside with aquadag, used because it can be applied like paint; the phosphors were left uncoated. Aluminum started being applied to CRTs in the 1950s, coating the inside of the CRT including the phosphors, which also increased image brightness since the aluminum reflected light (that would otherwise be lost inside the CRT) towards the outside of the CRT. In aluminized monochrome CRTs, Aquadag is used on the outside. There is a single aluminum coating covering the funnel and the screen.\nThe screen, funnel and neck are fused together into a single envelope, possibly using lead enamel seals, a hole is made in the funnel onto which the anode cap is installed and the phosphor, aquadag and aluminum are applied afterwards. Previously monochrome CRTs used ion traps that required magnets; the magnet was used to deflect the electrons away from the more difficult to deflect ions, letting the electrons through while letting the ions collide into a sheet of metal inside the electron gun. Ion burn results in premature wear of the phosphor. Since ions are harder to deflect than electrons, ion burn leaves a black dot in the center of the screen.\nThe interior aquadag or aluminum coating was the anode and served to accelerate the electrons towards the screen, collect them after hitting the screen while serving as a capacitor together with the outer aquadag coating. The screen has a single uniform phosphor coating and no shadow mask, technically having no resolution limit.\nMonochrome CRTs may use ring magnets to adjust the centering of the electron beam and magnets around the deflection yoke to adjust the geometry of the image.\nWhen a monochrome CRT is shut off, the screen itself retracts to a small, white dot in the center, along with the phosphors shutting down, shot by the electron gun; it sometimes takes a while for it to go away.\nColor CRTs.\nColor CRTs use three different phosphors which emit red, green, and blue light respectively. They are packed together in stripes (as in aperture grille designs) or clusters called \"triads\" (as in shadow mask CRTs).\nColor CRTs have three electron guns, one for each primary color, (red, green and blue) arranged either in a straight line (in-line) or in an equilateral triangular configuration (the guns are usually constructed as a single unit). The triangular configuration is often called \"delta-gun\", based on its relation to the shape of the Greek letter delta (\u0394). The arrangement of the phosphors is the same as that of the electron guns. A grille or mask absorbs the electrons that would otherwise hit the wrong phosphor.\nA shadow mask tube uses a metal plate with tiny holes, typically in a delta configuration, placed so that the electron beam only illuminates the correct phosphors on the face of the tube; blocking all other electrons. Shadow masks that use slots instead of holes are known as slot masks. The holes or slots are tapered so that the electrons that strike the inside of any hole will be reflected back, if they are not absorbed (e.g. due to local charge accumulation), instead of bouncing through the hole to strike a random (wrong) spot on the screen. Another type of color CRT (Trinitron) uses an aperture grille of tensioned vertical wires to achieve the same result. The shadow mask has a single hole for each triad. The shadow mask is usually &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442\u00a0inch behind the screen.\nTrinitron CRTs were different from other color CRTs in that they had a single electron gun with three cathodes, an aperture grille which lets more electrons through, increasing image brightness (since the aperture grille does not block as many electrons), and a vertically cylindrical screen, rather than a curved screen.\nThe three electron guns are in the neck (except for Trinitrons) and the red, green and blue phosphors on the screen may be separated by a black grid or matrix (called black stripe by Toshiba).\nThe funnel is coated with aquadag on both sides while the screen has a separate aluminum coating applied in a vacuum, deposited after the phosphor coating is applied, facing the electron gun. The aluminum coating protects the phosphor from ions, absorbs secondary electrons, providing them with a return path, preventing them from electrostatically charging the screen which would then repel electrons and reduce image brightness, reflects the light from the phosphors forwards and helps manage heat. It also serves as the anode of the CRT together with the inner aquadag coating. The inner coating is electrically connected to an electrode of the electron gun using springs, forming the final anode. The outer aquadag coating is connected to ground, possibly using a series of springs or a harness that makes contact with the aquadag.\nShadow mask.\nThe shadow mask absorbs or reflects electrons that would otherwise strike the wrong phosphor dots, causing color purity issues (discoloration of images); in other words, when set up correctly, the shadow mask helps ensure color purity. When the electrons strike the shadow mask, they release their energy as heat and x-rays. If the electrons have too much energy due to an anode voltage that is too high for example, the shadow mask can warp due to the heat, which can also happen during the Lehr baking at ~435\u00a0\u00b0C of the frit seal between the faceplate and the funnel of the CRT.\nShadow masks were replaced in TVs by slot masks in the 1970s, since slot masks let more electrons through, increasing image brightness. Shadow masks may be connected electrically to the anode of the CRT. Trinitron used a single electron gun with three cathodes instead of three complete guns. CRT PC monitors usually use shadow masks, except for Sony's Trinitron, Mitsubishi's Diamondtron and NEC's Cromaclear; Trinitron and Diamondtron use aperture grilles while Cromaclear uses a slot mask. Some shadow mask CRTs have color phosphors that are smaller in diameter than the electron beams used to light them, with the intention being to cover the entire phosphor, increasing image brightness. Shadow masks may be pressed into a curved shape.\nScreen manufacture.\nEarly color CRTs did not have a black matrix, which was introduced by Zenith in 1969, and Panasonic in 1970. The black matrix eliminates light leaking from one phosphor to another since the black matrix isolates the phosphor dots from one another, so part of the electron beam touches the black matrix. This is also made necessary by warping of the shadow mask. Light bleeding may still occur due to stray electrons striking the wrong phosphor dots. At high resolutions and refresh rates, phosphors only receive a very small amount of energy, limiting image brightness.\nSeveral methods were used to create the black matrix. One method coated the screen in photoresist such as dichromate-sensitized polyvinyl alcohol photoresist which was then dried and exposed; the unexposed areas were removed and the entire screen was coated in colloidal graphite to create a carbon film, and then hydrogen peroxide was used to remove the remaining photoresist alongside the carbon that was on top of it, creating holes that in turn created the black matrix. The photoresist had to be of the correct thickness to ensure sufficient adhesion to the screen, while the exposure step had to be controlled to avoid holes that were too small or large with ragged edges caused by light diffraction, ultimately limiting the maximum resolution of large color CRTs. The holes were then filled with phosphor using the method described above. Another method used phosphors suspended in an aromatic diazonium salt that adhered to the screen when exposed to light; the phosphors were applied, then exposed to cause them to adhere to the screen, repeating the process once for each color. Then carbon was applied to the remaining areas of the screen while exposing the entire screen to light to create the black matrix, and a fixing process using an aqueous polymer solution was applied to the screen to make the phosphors and black matrix resistant to water. Black chromium may be used instead of carbon in the black matrix. Other methods were also used.\nThe phosphors are applied using photolithography. The inner side of the screen is coated with phosphor particles suspended in PVA photoresist slurry, which is then dried using infrared light, exposed, and developed. The exposure is done using a \"lighthouse\" that uses an ultraviolet light source with a corrector lens to allow the CRT to achieve color purity. Removable shadow masks with spring-loaded clips are used as photomasks. The process is repeated with all colors. Usually the green phosphor is the first to be applied. After phosphor application, the screen is baked to eliminate any organic chemicals (such as the PVA that was used to deposit the phosphor) that may remain on the screen. Alternatively, the phosphors may be applied in a vacuum chamber by evaporating them and allowing them to condense on the screen, creating a very uniform coating. Early color CRTs had their phosphors deposited using silkscreen printing. Phosphors may have color filters over them (facing the viewer), contain pigment of the color emitted by the phosphor, or be encapsulated in color filters to improve color purity and reproduction while reducing glare. Such technology was sold by Toshiba under the Microfilter brand name. Poor exposure due to insufficient light leads to poor phosphor adhesion to the screen, which limits the maximum resolution of a CRT, as the smaller phosphor dots required for higher resolutions cannot receive as much light due to their smaller size.\nAfter the screen is coated with phosphor and aluminum and the shadow mask installed onto it the screen is bonded to the funnel using a glass frit that may contain 65\u201388% of lead oxide by weight. The lead oxide is necessary for the glass frit to have a low melting temperature. Boron oxide (III) may also present to stabilize the frit, with alumina powder as filler powder to control the thermal expansion of the frit. The frit may be applied as a paste consisting of frit particles suspended in amyl acetate or in a polymer with an alkyl methacrylate monomer together with an organic solvent to dissolve the polymer and monomer. The CRT is then baked in an oven in what is called a Lehr bake, to cure the frit, sealing the funnel and screen together. The frit contains a large quantity of lead, causing color CRTs to contain more lead than their monochrome counterparts. Monochrome CRTs on the other hand do not require frit; the funnel can be fused directly to the glass by melting and joining the edges of the funnel and screen using gas flames. Frit is used in color CRTs to prevent deformation of the shadow mask and screen during the fusing process. The edges of the screen and the edges of funnel of the CRT that mate with the screen, are never melted. A primer may be applied on the edges of the funnel and screen before the frit paste is applied to improve adhesion. The Lehr bake consists of several successive steps that heat and then cool the CRT gradually until it reaches a temperature of 435\u2013475\u00a0\u00b0C (other sources may state different temperatures, such as 440\u00a0\u00b0C) After the Lehr bake, the CRT is flushed with air or nitrogen to remove contaminants, the electron gun is inserted and sealed into the neck of the CRT, and a vacuum is formed on the CRT.\nConvergence and purity in color CRTs.\nDue to limitations in the dimensional precision with which CRTs can be manufactured economically, it has not been practically possible to build color CRTs in which three electron beams could be aligned to hit phosphors of respective color in acceptable coordination, solely on the basis of the geometric configuration of the electron gun axes and gun aperture positions, shadow mask apertures, etc. The shadow mask ensures that one beam will only hit spots of certain colors of phosphors, but minute variations in physical alignment of the internal parts among individual CRTs will cause variations in the exact alignment of the beams through the shadow mask, allowing some electrons from, for example, the red beam to hit, say, blue phosphors, unless some individual compensation is made for the variance among individual tubes.\nColor convergence and color purity are two aspects of this single problem. Firstly, for correct color rendering it is necessary that regardless of where the beams are deflected on the screen, all three hit the same spot (and nominally pass through the same hole or slot) on the shadow mask. This is called convergence. More specifically, the convergence at the center of the screen (with no deflection field applied by the yoke) is called static convergence, and the convergence over the rest of the screen area (specially at the edges and corners) is called dynamic convergence. The beams may converge at the center of the screen and yet stray from each other as they are deflected toward the edges; such a CRT would be said to have good static convergence but poor dynamic convergence. Secondly, each beam must only strike the phosphors of the color it is intended to strike and no others. This is called purity. Like convergence, there is static purity and dynamic purity, with the same meanings of \"static\" and \"dynamic\" as for convergence. Convergence and purity are distinct parameters; a CRT could have good purity but poor convergence, or vice versa. Poor convergence causes color \"shadows\" or \"ghosts\" along displayed edges and contours, as if the image on the screen were intaglio printed with poor registration. Poor purity causes objects on the screen to appear off-color while their edges remain sharp. Purity and convergence problems can occur at the same time, in the same or different areas of the screen or both over the whole screen, and either uniformly or to greater or lesser degrees over different parts of the screen.\nThe solution to the static convergence and purity problems is a set of color alignment ring magnets installed around the neck of the CRT. These movable weak permanent magnets are usually mounted on the back end of the deflection yoke assembly and are set at the factory to compensate for any static purity and convergence errors that are intrinsic to the unadjusted tube. Typically there are two or three pairs of two magnets in the form of rings made of plastic impregnated with a magnetic material, with their magnetic fields parallel to the planes of the magnets, which are perpendicular to the electron gun axes. Often, one pair of rings has 2 poles, another has 4, and the remaining ring has 6 poles. Each pair of magnetic rings forms a single effective magnet whose field vector can be fully and freely adjusted (in both direction and magnitude). By rotating a pair of magnets relative to each other, their relative field alignment can be varied, adjusting the effective field strength of the pair. (As they rotate relative to each other, each magnet's field can be considered to have two opposing components at right angles, and these four components [two each for two magnets] form two pairs, one pair reinforcing each other and the other pair opposing and canceling each other. Rotating away from alignment, the magnets' mutually reinforcing field components decrease as they are traded for increasing opposed, mutually cancelling components.) By rotating a pair of magnets together, preserving the relative angle between them, the direction of their collective magnetic field can be varied. Overall, adjusting all of the convergence/purity magnets allows a finely tuned slight electron beam deflection or lateral offset to be applied, which compensates for minor static convergence and purity errors intrinsic to the uncalibrated tube. Once set, these magnets are usually glued in place, but normally they can be freed and readjusted in the field (e.g. by a TV repair shop) if necessary.\nOn some CRTs, additional fixed adjustable magnets are added for dynamic convergence or dynamic purity at specific points on the screen, typically near the corners or edges. Further adjustment of dynamic convergence and purity typically cannot be done passively, but requires active compensation circuits, one to correct convergence horizontally and another to correct it vertically. In this case the deflection yoke contains convergence coils, a set of two per color, wound on the same core, to which the convergence signals are applied. That means 6 convergence coils in groups of 3, with 2 coils per group, with one coil for horizontal convergence correction and another for vertical convergence correction, with each group sharing a core. The groups are separated 120\u00b0 from one another. Dynamic convergence is necessary because the front of the CRT and the shadow mask are not spherical, compensating for electron beam defocusing and astigmatism. The fact that the CRT screen is not spherical leads to geometry problems which may be corrected using a circuit. The signals used for convergence are parabolic waveforms derived from three signals coming from a vertical output circuit. The parabolic signal is fed into the convergence coils, while the other two are sawtooth signals that, when mixed with the parabolic signals, create the necessary signal for convergence. A resistor and diode are used to lock the convergence signal to the center of the screen to prevent it from being affected by the static convergence. The horizontal and vertical convergence circuits are similar. Each circuit has two resonators, one usually tuned to 15,625\u00a0Hz and the other to 31,250\u00a0Hz, which set the frequency of the signal sent to the convergence coils. Dynamic convergence may be accomplished using electrostatic quadrupole fields in the electron gun. Dynamic convergence means that the electron beam does not travel in a perfectly straight line between the deflection coils and the screen, since the convergence coils cause it to become curved to conform to the screen.\nThe convergence signal may instead be a sawtooth signal with a slight sine wave appearance, the sine wave part is created using a capacitor in series with each deflection coil. In this case, the convergence signal is used to drive the deflection coils. The sine wave part of the signal causes the electron beam to move more slowly near the edges of the screen. The capacitors used to create the convergence signal are known as the s-capacitors. This type of convergence is necessary due to the high deflection angles and flat screens of many CRT computer monitors. The value of the s-capacitors must be chosen based on the scan rate of the CRT, so multi-syncing monitors must have different sets of s-capacitors, one for each refresh rate.\nDynamic convergence may instead be accomplished in some CRTs using only the ring magnets, magnets glued to the CRT, and by varying the position of the deflection yoke, whose position may be maintained using set screws, a clamp and rubber wedges. 90\u00b0 deflection angle CRTs may use \"self-convergence\" without dynamic convergence, which together with the in-line triad arrangement, eliminates the need for separate convergence coils and related circuitry, reducing costs. complexity and CRT depth by 10 millimeters. Self-convergence works by means of \"nonuniform\" magnetic fields. Dynamic convergence is necessary in 110\u00b0 deflection angle CRTs, and quadrupole windings on the deflection yoke at a certain frequency may also be used for dynamic convergence.\nDynamic color convergence and purity are one of the main reasons why until late in their history, CRTs were long-necked (deep) and had biaxially curved faces; these geometric design characteristics are necessary for intrinsic passive dynamic color convergence and purity. Only starting around the 1990s did sophisticated active dynamic convergence compensation circuits become available that made short-necked and flat-faced CRTs workable. These active compensation circuits use the deflection yoke to finely adjust beam deflection according to the beam target location. The same techniques (and major circuit components) also make possible the adjustment of display image rotation, skew, and other complex raster geometry parameters through electronics under user control.\nAlternatively, the guns can be aligned with one another (converged) using convergence rings placed right outside the neck; with one ring per gun. The rings can have north and south poles. There can be 4 sets of rings, one to adjust RGB convergence, a second to adjust Red and Blue convergence, a third to adjust vertical raster shift, and a fourth to adjust purity. The vertical raster shift adjusts the straightness of the scan line. CRTs may also employ dynamic convergence circuits, which ensure correct convergence at the edges of the CRT. Permalloy magnets may also be used to correct the convergence at the edges. Convergence is carried out with the help of a crosshatch (grid) pattern. Other CRTs may instead use magnets that are pushed in and out instead of rings. In early color CRTs, the holes in the shadow mask became progressively smaller as they extended outwards from the center of the screen, to aid in convergence.\nMagnetic shielding and degaussing.\nIf the shadow mask or aperture grille becomes magnetized, its magnetic field alters the paths of the electron beams. This causes errors of \"color purity\" as the electrons no longer follow only their intended paths, and some will hit some phosphors of colors other than the one intended. For example, some electrons from the red beam may hit blue or green phosphors, imposing a magenta or yellow tint to parts of the image that are supposed to be pure red. (This effect is localized to a specific area of the screen if the magnetization is localized.) Therefore, it is important that the shadow mask or aperture grille not be magnetized. The earth's magnetic field may have an effect on the color purity of the CRT. Because of this, some CRTs have external magnetic shields over their funnels. The magnetic shield may be made of soft iron or mild steel and contain a degaussing coil. The magnetic shield and shadow mask may be permanently magnetized by the earth's magnetic field, adversely affecting color purity when the CRT is moved. This problem is solved with a built-in degaussing coil, found in many TVs and computer monitors. Degaussing may be automatic, occurring whenever the CRT is turned on. The magnetic shield may also be internal, being on the inside of the funnel of the CRT.\nColor CRT displays in TV sets and computer monitors often have a built-in degaussing (demagnetizing) coil mounted around the perimeter of the CRT face. Upon power-up of the CRT display, the degaussing circuit produces a brief, alternating current through the coil which fades to zero over a few seconds, producing a decaying alternating magnetic field from the coil. This degaussing field is strong enough to remove shadow mask magnetization in most cases, maintaining color purity. In unusual cases of strong magnetization where the internal degaussing field is not sufficient, the shadow mask may be degaussed externally with a stronger portable degausser or demagnetizer. However, an excessively strong magnetic field, whether alternating or constant, may mechanically deform (bend) the shadow mask, causing a permanent color distortion on the display which looks very similar to a magnetization effect.\nResolution.\nDot pitch defines the maximum resolution of the display, assuming delta-gun CRTs. In these, as the scanned resolution approaches the dot pitch resolution, moir\u00e9 appears, as the detail being displayed is finer than what the shadow mask can render. Aperture grille monitors do not suffer from vertical moir\u00e9, however, because their phosphor stripes have no vertical detail. In smaller CRTs, these strips maintain position by themselves, but larger aperture-grille CRTs require one or two crosswise (horizontal) support strips; one for smaller CRTs, and two for larger ones. The support wires block electrons, causing the wires to be visible. In aperture grille CRTs, dot pitch is replaced by stripe pitch. Hitachi developed the Enhanced Dot Pitch (EDP) shadow mask, which uses oval holes instead of circular ones, with respective oval phosphor dots. Moir\u00e9 is reduced in shadow mask CRTs by arranging the holes in the shadow mask in a honeycomb-like pattern.\nProjection CRTs.\nProjection CRTs were used in CRT projectors and CRT rear-projection TVs, and are usually small (being 7\u20139\u00a0inches across); have a phosphor that generates either red, green or blue light, thus making them monochrome CRTs; and are similar in construction to other monochrome CRTs. Larger projection CRTs in general lasted longer, and were able to provide higher brightness levels and resolution, but were also more expensive. Projection CRTs have an unusually high anode voltage for their size (such as 27 or 25\u00a0kV for a 5 or 7-inch projection CRT respectively), and a specially made tungsten/barium cathode (instead of the pure barium oxide normally used) that consists of barium atoms embedded in 20% porous tungsten or barium and calcium aluminates or of barium, calcium and aluminum oxides coated on porous tungsten; the barium diffuses through the tungsten to emit electrons. The special cathode can deliver 2\u00a0mA of current instead of the 0.3mA of normal cathodes, which makes them bright enough to be used as light sources for projection. The high anode voltage and the specially made cathode increase the voltage and current, respectively, of the electron beam, which increases the light emitted by the phosphors, and also the amount of heat generated during operation; this means that projector CRTs need cooling. The screen is usually cooled using a container (the screen forms part of the container) with glycol; the glycol may itself be dyed, or colorless glycol may be used inside a container which may be colored (forming a lens known as a c-element). Colored lenses or glycol are used for improving color reproduction at the cost of brightness, and are only used on red and green CRTs. Each CRT has its own glycol, which has access to an air bubble to allow the glycol to shrink and expand as it cools and warms. Projector CRTs may have adjustment rings just like color CRTs to adjust astigmatism, which is flaring of the electron beam (stray light similar to shadows). They have three adjustment rings; one with two poles, one with four poles, and another with 6 poles. When correctly adjusted, the projector can display perfectly round dots without flaring. The screens used in projection CRTs were more transparent than usual, with 90% transmittance. The first projection CRTs were made in 1933.\nProjector CRTs were available with electrostatic and electromagnetic focusing, the latter being more expensive. Electrostatic focusing used electronics to focus the electron beam, together with focusing magnets around the neck of the CRT for fine focusing adjustments. This type of focusing degraded over time. Electromagnetic focusing was introduced in the early 1990s and included an electromagnetic focusing coil in addition to the already existing focusing magnets. Electromagnetic focusing was much more stable over the lifetime of the CRT, retaining 95% of its sharpness by the end of life of the CRT.\nBeam-index tube.\nBeam-index tubes, also known as Uniray, Apple CRT or Indextron, was an attempt in the 1950s by Philco to create a color CRT without a shadow mask, eliminating convergence and purity problems, and allowing for shallower CRTs with higher deflection angles. It also required a lower voltage power supply for the final anode since it did not use a shadow mask, which normally blocks around 80% of the electrons generated by the electron gun. The lack of a shadow mask also made it immune to the earth's magnetic field while also making degaussing unnecessary and increasing image brightness. It was constructed similarly to a monochrome CRT, with an aquadag outer coating, an aluminum inner coating, and a single electron gun but with a screen with an alternating pattern of red, green, blue and UV (index) phosphor stripes (similarly to a Trinitron) with a side mounted photomultiplier tube or photodiode pointed towards the rear of the screen and mounted on the funnel of CRT, to track the electron beam to activate the phosphors separately from one another using the same electron beam. Only the index phosphor stripe was used for tracking, and it was the only phosphor that was not covered by an aluminum layer. It was shelved because of the precision required to produce it. It was revived by Sony in the 1980s as the Indextron but its adoption was limited, at least in part due to the development of LCD displays. Beam-index CRTs also suffered from poor contrast ratios of only around 50:1 since some light emission by the phosphors was required at all times by the photodiodes to track the electron beam. It allowed for single CRT color CRT projectors due to a lack of shadow mask; normally CRT projectors use three CRTs, one for each color, since a lot of heat is generated due to the high anode voltage and beam current, making a shadow mask impractical and inefficient since it would warp under the heat produced (shadow masks absorb most of the electron beam, and, hence, most of the energy carried by the relativistic electrons); the three CRTs meant that an involved calibration and adjustment procedure had to be carried out during installation of the projector, and moving the projector would require it to be recalibrated. A single CRT meant the need for calibration was eliminated, but brightness was decreased since the CRT screen had to be used for three colors instead of each color having its own CRT screen. A stripe pattern also imposes a horizontal resolution limit; in contrast, three-screen CRT projectors have no theoretical resolution limit, due to them having single, uniform phosphor coatings.\nFlat CRTs.\nFlat CRTs are those with a flat screen. Despite having a flat screen, they may not be completely flat, especially on the inside, instead having a greatly increased curvature. A notable exception is the LG Flatron (made by LG.Philips Displays, later LP Displays) which is truly flat on the outside and inside, but has a bonded glass pane on the screen with a tensioned rim band to provide implosion protection. Such completely flat CRTs were first introduced by Zenith in 1986, and used flat tensioned shadow masks, where the shadow mask is held under tension, providing increased resistance to blooming. LG's Flatron technology is based on this technology developed by Zenith, now a subsidiary of LG. \nFlat CRTs have a number of challenges, like deflection. Vertical deflection boosters are required to increase the amount of current that is sent to the vertical deflection coils to compensate for the reduced curvature. The CRTs used in the Sinclair TV80, and in many Sony Watchmans were flat in that they were not deep and their front screens were flat, but their electron guns were put to a side of the screen. The TV80 used electrostatic deflection while the Watchman used magnetic deflection with a phosphor screen that was curved inwards. Similar CRTs were used in video door bells.\nRadar CRTs.\nRadar CRTs such as the 7JP4 had a circular screen and scanned the beam from the center outwards. The deflection yoke rotated, causing the beam to rotate in a circular fashion. The screen often had two colors, often a bright short persistence color that only appeared as the beam scanned the display and a long persistence phosphor afterglow. When the beam strikes the phosphor, the phosphor brightly illuminates, and when the beam leaves, the dimmer long persistence afterglow would remain lit where the beam struck the phosphor, alongside the radar targets that were \"written\" by the beam, until the beam re-struck the phosphor.\nOscilloscope CRTs.\nIn oscilloscope CRTs, electrostatic deflection is used, rather than the magnetic deflection commonly used with TV and other large CRTs. The beam is deflected horizontally by applying an electric field between a pair of plates to its left and right, and vertically by applying an electric field to plates above and below. TVs use magnetic rather than electrostatic deflection because the deflection plates obstruct the beam when the deflection angle is as large as is required for tubes that are relatively short for their size. Some Oscilloscope CRTs incorporate post deflection anodes (PDAs) that are spiral-shaped to ensure even anode potential across the CRT and operate at up to 15\u00a0kV. In PDA CRTs the electron beam is deflected before it is accelerated, improving sensitivity and legibility, specially when analyzing voltage pulses with short duty cycles.\nMicrochannel plate.\nWhen displaying fast one-shot events, the electron beam must deflect very quickly, with few electrons impinging on the screen, leading to a faint or invisible image on the display. Oscilloscope CRTs designed for very fast signals can give a brighter display by passing the electron beam through a micro-channel plate just before it reaches the screen. Through the phenomenon of secondary emission, this plate multiplies the number of electrons reaching the phosphor screen, giving a significant improvement in writing rate (brightness) and improved sensitivity and spot size as well.\nGraticules.\nMost oscilloscopes have a graticule as part of the visual display, to facilitate measurements. The graticule may be permanently marked inside the face of the CRT, or it may be a transparent external plate made of glass or acrylic plastic. An internal graticule eliminates parallax error, but cannot be changed to accommodate different types of measurements. Oscilloscopes commonly provide a means for the graticule to be illuminated from the side, which improves its visibility.\nImage storage tubes.\nThese are found in \"analog phosphor storage oscilloscopes\". These are distinct from \"digital storage oscilloscopes\" which rely on solid state digital memory to store the image.\nWhere a single brief event is monitored by an oscilloscope, such an event will be displayed by a conventional tube only while it actually occurs. The use of a long persistence phosphor may allow the image to be observed after the event, but only for a few seconds at best. This limitation can be overcome by the use of a direct view storage cathode-ray tube (storage tube). A storage tube will continue to display the event after it has occurred until such time as it is erased. A storage tube is similar to a conventional tube except that it is equipped with a metal grid coated with a dielectric layer located immediately behind the phosphor screen. An externally applied voltage to the mesh initially ensures that the whole mesh is at a constant potential. This mesh is constantly exposed to a low velocity electron beam from a 'flood gun' which operates independently of the main gun. This flood gun is not deflected like the main gun but constantly 'illuminates' the whole of the storage mesh. The initial charge on the storage mesh is such as to repel the electrons from the flood gun which are prevented from striking the phosphor screen.\nWhen the main electron gun writes an image to the screen, the energy in the main beam is sufficient to create a 'potential relief' on the storage mesh. The areas where this relief is created no longer repel the electrons from the flood gun which now pass through the mesh and illuminate the phosphor screen. Consequently, the image that was briefly traced out by the main gun continues to be displayed after it has occurred. The image can be 'erased' by resupplying the external voltage to the mesh restoring its constant potential. The time for which the image can be displayed was limited because, in practice, the flood gun slowly neutralises the charge on the storage mesh. One way of allowing the image to be retained for longer is temporarily to turn off the flood gun. It is then possible for the image to be retained for several days. The majority of storage tubes allow for a lower voltage to be applied to the storage mesh which slowly restores the initial charge state. By varying this voltage a variable persistence is obtained. Turning off the flood gun and the voltage supply to the storage mesh allows such a tube to operate as a conventional oscilloscope tube.\nVector monitors.\nVector monitors were used in early computer aided design systems and are in some late-1970s to mid-1980s arcade games such as \"Asteroids\".\nThey draw graphics point-to-point, rather than scanning a raster. Either monochrome or color CRTs can be used in vector displays, and the essential principles of CRT design and operation are the same for either type of display; the main difference is in the beam deflection patterns and circuits.\nData storage tubes.\nThe Williams tube or Williams-Kilburn tube was a cathode-ray tube used to electronically store binary data. It was used in computers of the 1940s as a random-access digital storage device. In contrast to other CRTs in this article, the Williams tube was not a display device, and in fact could not be viewed since a metal plate covered its screen.\nCat's eye.\nIn some vacuum tube radio sets, a \"Magic Eye\" or \"Tuning Eye\" tube was provided to assist in tuning the receiver. Tuning would be adjusted until the width of a radial shadow was minimized. This was used instead of a more expensive electromechanical meter, which later came to be used on higher-end tuners when transistor sets lacked the high voltage required to drive the device. The same type of device was used with tape recorders as a recording level meter, and for various other applications including electrical test equipment.\nCharactrons.\nSome displays for early computers (those that needed to display more text than was practical using vectors, or that required high speed for photographic output) used Charactron CRTs. These incorporate a perforated metal character mask (stencil), which shapes a wide electron beam to form a character on the screen. The system selects a character on the mask using one set of deflection circuits, but that causes the extruded beam to be aimed off-axis, so a second set of deflection plates has to re-aim the beam so it is headed toward the center of the screen. A third set of plates places the character wherever required. The beam is unblanked (turned on) briefly to draw the character at that position. Graphics could be drawn by selecting the position on the mask corresponding to the code for a space (in practice, they were simply not drawn), which had a small round hole in the center; this effectively disabled the character mask, and the system reverted to regular vector behavior. Charactrons had exceptionally long necks, because of the need for three deflection systems.\nNimo.\nNimo was the trademark of a family of small specialised CRTs manufactured by Industrial Electronic Engineers. These had 10 electron guns which produced electron beams in the form of digits in a manner similar to that of the charactron. The tubes were either simple single-digit displays or more complex 4- or 6- digit displays produced by means of a suitable magnetic deflection system. Having little of the complexities of a standard CRT, the tube required a relatively simple driving circuit, and as the image was projected on the glass face, it provided a much wider viewing angle than competitive types (e.g., nixie tubes). However, their requirement for several voltages and their high voltage made them uncommon.\nFlood-beam CRT.\nFlood-beam CRTs are small tubes that are arranged as pixels for large video walls like Jumbotrons. The first screen using this technology (called Diamond Vision by Mitsubishi Electric) was introduced by Mitsubishi Electric for the 1980 Major League Baseball All-Star Game. It differs from a normal CRT in that the electron gun within does not produce a focused controllable beam. Instead, electrons are sprayed in a wide cone across the entire front of the phosphor screen, basically making each unit act as a single light bulb. Each one is coated with a red, green or blue phosphor, to make up the color sub-pixels. This technology has largely been replaced with light-emitting diode displays. Unfocused and undeflected CRTs were used as grid-controlled stroboscope lamps since 1958. Electron-stimulated luminescence (ESL) lamps, which use the same operating principle, were released in 2011.\nPrint-head CRT.\nCRTs with an unphosphored front glass but with fine wires embedded in it were used as electrostatic print heads in the 1960s. The wires would pass the electron beam current through the glass onto a sheet of paper where the desired content was therefore deposited as an electrical charge pattern. The paper was then passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image.\nZeus \u2013 thin CRT display.\nIn the late 1990s and early 2000s Philips Research Laboratories experimented with a type of thin CRT known as the \"Zeus\" display, which contained CRT-like functionality in a flat-panel display. The cathode of this display was mounted under the front of the display, and the electrons from the cathode would be directed to the back to the display where they would stay until extracted by electrodes near the front of the display, and directed to the front of the display which had phosphor dots. The devices were demonstrated but never marketed.\nSlimmer CRT.\nSome CRT manufacturers, both LG.Philips Displays (later LP Displays) and Samsung SDI, innovated CRT technology by creating a slimmer tube. Slimmer CRT had the trade names Superslim, Ultraslim, Vixlim (by Samsung) and Cybertube and Cybertube+ (both by LG Philips displays). A flat CRT has a depth. The depth of Superslim was and Ultraslim was .\nHealth concerns.\nIonizing radiation.\nCRTs can emit a small amount of X-ray radiation; this is a result of the electron beam's bombardment of the shadow mask/aperture grille and phosphors, which produces bremsstrahlung (braking radiation) as the high-energy electrons are decelerated. The amount of radiation escaping the front of the monitor is widely considered to be not harmful. The United States Food and Drug Administration (FDA) regulations in https:// are used to strictly limit, for instance, TV receivers to 0.5 milliroentgens per hour at a distance of from any external surface; since 2007, most CRTs have emissions that fall well below this limit. Note that the roentgen is an outdated unit and does not account for dose absorption. The conversion rate is about .877 roentgen per rem. Assuming that the viewer absorbed the entire dose (which is unlikely), and that they watched TV for 2 hours a day, a .5 milliroentgen hourly dose would increase the viewers yearly dose by 320 millirem. For comparison, the average background radiation in the United States is 310 millirem a year. Negative effects of chronic radiation are not generally noticeable until doses over 20,000 millirem.\nThe density of the X-rays that would be generated by a CRT is low because the raster scan of a typical CRT distributes the energy of the electron beam across the entire screen. Voltages above 15,000 volts are enough to generate \"soft\" X-rays. However, since CRTs may stay on for several hours at a time, the amount of X-rays generated by the CRT may become significant, hence the importance of using materials to shield against X-rays, such as the thick leaded glass and barium-strontium glass used in CRTs.\nConcerns about X-rays emitted by CRTs began in 1967 when it was found that TV sets made by General Electric were emitting \"X-radiation in excess of desirable levels\". It was later found that TV sets from all manufacturers were also emitting radiation. This caused TV industry representatives to be brought before a U.S. congressional committee, which later proposed a federal radiation regulation bill, which became the 1968 Radiation Control for Health and Safety Act. It was recommended to TV set owners to always be at a distance of at least 6 feet from the screen of the TV set, and to avoid \"prolonged exposure\" at the sides, rear or underneath a TV set. It was discovered that most of the radiation was directed downwards. Owners were also told to not modify their set's internals to avoid exposure to radiation. Headlines about \"radioactive\" TV sets continued until the end of the 1960s. There once was a proposal by two New York congressmen that would have forced TV set manufacturers to \"go into homes to test all of the nation's 15 million color sets and to install radiation devices in them\". The FDA eventually began regulating radiation emissions from all electronic products in the US.\nToxicity.\nOlder color and monochrome CRTs may have been manufactured with toxic substances, such as cadmium, in the phosphors. The rear glass tube of modern CRTs may be made from leaded glass, which represent an environmental hazard if disposed of improperly. Since 1970, glass in the front panel (the viewable portion of the CRT) used strontium oxide rather than lead, though the rear of the CRT was still produced from leaded glass. Monochrome CRTs typically do not contain enough leaded glass to fail EPA TCLP tests. While the TCLP process grinds the glass into fine particles in order to expose them to weak acids to test for leachate, intact CRT glass does not leach (The lead is vitrified, contained inside the glass itself, similar to leaded glass crystalware).\nFlicker.\nAt low refresh rates (60\u00a0Hz and below), the periodic scanning of the display may produce a flicker that some people perceive more easily than others, especially when viewed with peripheral vision. Flicker is commonly associated with CRT as most TVs run at 50\u00a0Hz (PAL) or 60\u00a0Hz (NTSC), although there are some 100\u00a0Hz PAL TVs that are flicker-free. Typically only low-end monitors run at such low frequencies, with most computer monitors supporting at least 75\u00a0Hz and high-end monitors capable of 100\u00a0Hz or more to eliminate any perception of flicker. Though the 100\u00a0Hz PAL was often achieved using interleaved scanning, dividing the circuit and scan into two beams of 50\u00a0Hz. Non-computer CRTs or CRT for sonar or radar may have long persistence phosphor and are thus flicker free. If the persistence is too long on a video display, moving images will be blurred.\nHigh-frequency audible noise.\n50\u00a0Hz/60\u00a0Hz CRTs used for TV operate with horizontal scanning frequencies of 15,750 and 15,734.27\u00a0Hz (for NTSC systems) or 15,625\u00a0Hz (for PAL systems). These frequencies are at the upper range of human hearing and are inaudible to many people; however, some people (especially children) will perceive a high-pitched tone near an operating CRT TV. The sound is due to magnetostriction in the magnetic core and periodic movement of windings of the flyback transformer but the sound can also be created by movement of the deflection coils, yoke or ferrite beads.\nThis problem does not occur on 100/120\u00a0Hz TVs and on non-CGA (Color Graphics Adapter) computer displays, because they use much higher horizontal scanning frequencies that produce sound which is inaudible to humans (22\u00a0kHz to over 100\u00a0kHz).\nImplosion.\nIf the glass wall is damaged, atmospheric pressure can implode the vacuum tube into dangerous fragments which accelerate inward and then spray at high speed in all directions. Although modern cathode-ray tubes used in TVs and computer displays have epoxy-bonded face-plates or other measures to prevent shattering of the envelope, CRTs must be handled carefully to avoid injury.\nImplosion protection.\nEarly CRTs had a glass plate over the screen that was bonded to it using glue, creating a laminated glass screen: initially the glue was polyvinyl acetate (PVA), while later versions such as the LG Flatron used a resin, perhaps a UV-curable resin. The PVA degrades over time creating a \"cataract\", a ring of degraded glue around the edges of the CRT that does not allow light from the screen to pass through. Later CRTs instead use a tensioned metal rim band mounted around the perimeter that also provides mounting points for the CRT to be mounted to a housing. In a 19-inch CRT, the tensile stress in the rim band is 70\u00a0kg/cm2.\nOlder CRTs were mounted to the TV set using a frame. The band is tensioned by heating it, then mounting it on the CRT; the band cools afterwards, shrinking in size and putting the glass under compression, which strengthens the glass and reduces the necessary thickness (and hence weight) of the glass. This makes the band an integral component that should never be removed from an intact CRT that still has a vacuum; attempting to remove it may cause the CRT to implode.\nThe rim band prevents the CRT from imploding should the screen be broken. The rim band may be glued to the perimeter of the CRT using epoxy, preventing cracks from spreading beyond the screen and into the funnel.\nAlternatively the compression caused by the rim band may be used to cause any cracks in the screen to propagate laterally at a high speed so that they reach the funnel and fully penetrate it before they fully penetrate the screen. This is possible because the funnel has walls that are thinner than the screen. Fully penetrating the funnel first allows air to enter the CRT from a short distance behind the screen, and prevent an implosion by ensuring the screen is fully penetrated by the cracks and breaks only when the CRT already has air.\nElectric shock.\nTo accelerate the electrons from the cathode to the screen with enough energy to achieve sufficient image brightness, a very high voltage (EHT or extra-high tension) is required, from a few thousand volts for a small oscilloscope CRT to tens of thousands for a larger screen color TV. This is many times greater than household power supply voltage. Even after the power supply is turned off, some associated capacitors and the CRT itself may retain a charge for some time and therefore dissipate that charge suddenly through a ground such as an inattentive human grounding a capacitor discharge lead. An average monochrome CRT may use 1\u20131.5\u00a0kV of anode voltage per inch.\nSecurity concerns.\nUnder some circumstances, the signal radiated from the electron guns, scanning circuitry, and associated wiring of a CRT can be captured remotely and used to reconstruct what is shown on the CRT using a process called Van Eck phreaking. Special TEMPEST shielding can mitigate this effect. Such radiation of a potentially exploitable signal, however, occurs also with other display technologies and with electronics in general.\nRecycling.\nDue to the toxins contained in CRT monitors the United States Environmental Protection Agency (EPA) created rules (in October 2001) stating that CRTs must be brought to special e-waste recycling facilities. In November 2002, the EPA began fining companies that disposed of CRTs through landfills or incineration. Regulatory agencies, local and statewide, monitor the disposal of CRTs and other computer equipment.\nAs electronic waste, CRTs are considered one of the hardest types to recycle. CRTs have relatively high concentration of lead and &lt;templatestyles src=\"Template:Tooltip/styles.css\" /&gt;, both of which are necessary for the display. There are several companies in the United States that charge a small fee to collect CRTs, then subsidize their labor by selling the harvested copper, wire, and printed circuit boards. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of \"hazardous household waste\" but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.\nVarious states participate in the recycling of CRTs, each with their reporting requirements for collectors and recycling facilities. For example, in California the recycling of CRTs is governed by CALRecycle, the California Department of Resources Recycling and Recovery through their Payment System. Recycling facilities that accept CRT devices from business and residential sector must obtain contact information such as address and phone number to ensure the CRTs come from a California source in order to participate in the CRT Recycling Payment System.\nIn Europe, disposal of CRT TVs and monitors is covered by the Waste Electrical and Electronic Equipment Directive (WEEE Directive).\nMultiple methods have been proposed for the recycling of CRT glass. The methods involve thermal, mechanical and chemical processes. All proposed methods remove the lead oxide content from the glass. Some companies operated furnaces to separate the lead from the glass. A coalition called the Recytube project was once formed by several European companies to devise a method to recycle CRTs. The phosphors used in CRTs often contain rare earth metals. A CRT contains about 7\u00a0grams of phosphor.\nThe funnel can be separated from the screen of the CRT using laser cutting, diamond saws or wires or using a resistively heated nichrome wire.\nLeaded CRT glass was sold to be remelted into other CRTs, or even broken down and used in road construction or used in tiles, concrete, concrete and cement bricks, fiberglass insulation or used as flux in metals smelting.\nA considerable portion of CRT glass is landfilled, where it can pollute the surrounding environment. It is more common for CRT glass to be disposed of than being recycled.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nApplying CRT in different display-purpose:\nHistorical aspects:\nSafety and precautions:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6015", "revid": "45179772", "url": "https://en.wikipedia.org/wiki?curid=6015", "title": "Crystal", "text": "Solid material with highly ordered microscopic structure\nA crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.\nThe word \"crystal\" derives from the Ancient Greek word (), meaning both \"ice\" and \"rock crystal\", from (), \"icy cold, frost\".\nExamples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.\nDespite the name, lead crystal, crystal glass, and related products are \"not\" crystals, but rather types of glass, i.e. amorphous solids.\nCrystals, or crystalline solids, are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.\nCrystal structure (microscopic).\nThe scientific definition of a \"crystal\" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below).\nNot all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a \"polycrystalline\" structure. In the final block of ice, each of the small crystals (called \"crystallites\" or \"grains\") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does \"not\" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called \"amorphous solids\", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.\nA crystal structure (an arrangement of atoms in a crystal) is characterized by its \"unit cell\", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.\nThe symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries (230 is commonly cited, but this treats chiral equivalents as separate entities), called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as halite shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).\nCrystal faces, shapes and crystallographic forms.\nCrystals are commonly recognized, macroscopically, by their shape, consisting of flat faces with sharp angles. These shape characteristics are not \"necessary\" for a crystal\u2014a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape\u2014but the characteristic macroscopic shape is often present and easy to see.\nEuhedral crystals are those that have obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.\nThe flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: they are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)\nOne of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.\nA crystal's crystallographic forms are sets of possible faces of the crystal that are related by one of the symmetries of the crystal. For example, crystals of galena often take the shape of cubes, and the six faces of the cube belong to a crystallographic form that displays one of the symmetries of the isometric crystal system. Galena also sometimes crystallizes as octahedrons, and the eight faces of the octahedron belong to another crystallographic form reflecting a different symmetry of the isometric system. A crystallographic form is described by placing the Miller indices of one of its faces within brackets. For example, the octahedral form is written as {111}, and the other faces in the form are implied by the symmetry of the crystal.\nForms may be closed, meaning that the form can completely enclose a volume of space, or open, meaning that it cannot. The cubic and octahedral forms are examples of closed forms. All the forms of the isometric system are closed, while all the forms of the monoclinic and triclinic crystal systems are open. A crystal's faces may all belong to the same closed form, or they may be a combination of multiple open or closed forms.\nA crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.\nOccurrence in nature.\nRocks.\nBy volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. As of 1999[ [update]], the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .\nSome crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.\nOther rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins. Evaporites such as halite, gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.\nIce.\nWater-based ice in the form of snow, sea ice, and glaciers are common crystalline/polycrystalline structures on Earth and other planets. A single snowflake is a single crystal or a collection of crystals, while an ice cube is a polycrystal. Ice crystals may form from cooling liquid water below its freezing point, such as ice cubes or a frozen lake. Frost, snowflakes, or small ice crystals suspended in the air (ice fog) more often grow from a supersaturated gaseous-solution of water vapor and air, when the temperature of the air drops below its dew point, without passing through a liquid state. Another unusual property of water is that it expands rather than contracts when it crystallizes.\nOrganigenic crystals.\nMany living organisms are able to produce crystals grown from an aqueous solution, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of bones and teeth in vertebrates.\nPolymorphism and allotropy.\nThe same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice Ih, but can also exist as the cubic Ice Ic, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different \"phases\".\nIn addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO2 can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.\nFor pure chemical elements, polymorphism is referred to as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have very different properties. For example, diamond is the hardest substance known, while graphite is so soft that it is used as a lubricant. Chocolate can form six different types of crystals, but only one has the suitable hardness and melting point for candy bars and confections. Polymorphism in steel is responsible for its ability to be heat treated, giving it a wide range of properties.\nPolyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.\nCrystallization.\nCrystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see: epitaxy and frost.)\nCrystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.\nSpecific industrial techniques to produce large single crystals (called \"boules\") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.\nLarge single crystals can be created by geological processes. For example, selenite crystals in excess of 10\u00a0m are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.\nCrystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to \"prevent\" crystallization from occurring, such as antifreeze proteins.\nDefects, impurities, and twinning.\nAn \"ideal\" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects: places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.\nA few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.\nAnother common type of crystallographic defect is an impurity, meaning that the \"wrong\" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.\nIn semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.\nTwinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.\nMosaicity is a spread of crystal plane orientations. A mosaic crystal consists of smaller crystalline units that are somewhat misaligned with respect to each other.\nChemical bonds.\nIn general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows:\nMetals crystallize rapidly and are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically, for example, fighter-jet turbines are typically made by first growing a single crystal of titanium alloy, increasing its strength and melting point over polycrystalline titanium. A small piece of metal may naturally form into a single crystal, such as Type 2 telluric iron, but larger pieces generally do not unless extremely slow cooling occurs. For example, iron meteorites are often composed of single crystal, or many large crystals that may be several meters in size, due to very slow cooling in the vacuum of space. The slow cooling may allow the precipitation of a separate phase within the crystal lattice, which form at specific angles determined by the lattice, called Widmanstatten patterns.\nIonic compounds typically form when a metal reacts with a non-metal, such as sodium with chlorine. These often form substances called salts, such as sodium chloride (table salt) or potassium nitrate (saltpeter), with crystals that are often brittle and cleave relatively easily. Ionic materials are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Some ionic compounds can be very hard, such as oxides like aluminium oxide found in many gemstones such as ruby and synthetic sapphire.\nCovalently bonded solids (sometimes called covalent network solids) are typically formed from one or more non-metals, such as carbon or silicon and oxygen, and are often very hard, rigid, and brittle. These are also very common, notable examples being diamond and quartz respectively.\nWeak van der Waals forces also help hold together certain crystals, such as crystalline molecular solids, as well as the interlayer bonding in graphite. Substances such as fats, lipids and wax form molecular bonds because the large molecules do not pack as tightly as atomic bonds. This leads to crystals that are much softer and more easily pulled apart or broken. Common examples include chocolates, candles, or viruses. Water ice and dry ice are examples of other materials with molecular bonding.Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization\u2014and sometimes polymers are completely amorphous.\nQuasicrystals.\nA quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.\nQuasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).\nThe International Union of Crystallography has redefined the term \"crystal\" to include both ordinary periodic crystals and quasicrystals (\"any solid having an essentially discrete diffraction diagram\").\nQuasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals known in 2004. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.\nSpecial properties from anisotropy.\nCrystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.\nNot all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress\u2014for example, stress-induced birefringence.\nCrystallography.\n\"Crystallography\" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6016", "revid": "628775", "url": "https://en.wikipedia.org/wiki?curid=6016", "title": "Cytosine", "text": "Chemical compound in nucleic acids\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nCytosine (symbol C or Cyt) is one of the four nucleotide bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine. In Watson\u2013Crick base pairing, it forms three hydrogen bonds with guanine.\nHistory.\nCytosine was discovered and named by Albrecht Kossel and Albert Neumann in 1894 when it was hydrolyzed from calf thymus tissues. A structure was proposed in 1903, and was synthesized (and thus confirmed) in the laboratory in the same year.\nIn 1998, cytosine was used in an early demonstration of quantum information processing when Oxford University researchers implemented the Deutsch\u2013Jozsa algorithm on a two qubit nuclear magnetic resonance quantum computer (NMRQC).\nIn March 2015, NASA scientists reported the formation of cytosine, along with uracil and thymine, from pyrimidine under the space-like laboratory conditions, which is of interest because pyrimidine has been found in meteorites although its origin is unknown.\nChemical reactions.\nCytosine can be found as part of DNA, as part of RNA, or as a part of a nucleotide. As cytidine triphosphate (CTP), it can act as a co-factor to enzymes, and can transfer a phosphate to convert adenosine diphosphate (ADP) to adenosine triphosphate (ATP).\nIn DNA and RNA, cytosine is paired with guanine. However, it is inherently unstable, and can change into uracil (spontaneous deamination). This can lead to a point mutation if not repaired by the DNA repair enzymes such as uracil glycosylase, which cleaves a uracil in DNA.\nCytosine can also be methylated into 5-methylcytosine by an enzyme called DNA methyltransferase or be methylated and hydroxylated to make 5-hydroxymethylcytosine. The difference in rates of deamination of cytosine and 5-methylcytosine (to uracil and thymine) forms the basis of bisulfite sequencing.\nBiological function.\nWhen found third in a codon of RNA, cytosine is synonymous with uracil, as they are interchangeable as the third base.\nWhen found as the second base in a codon, the third is always interchangeable. For example, UCU, UCC, UCA and UCG are all serine, regardless of the third base.\nActive enzymatic deamination of cytosine or 5-methylcytosine by the APOBEC family of cytosine deaminases could have both beneficial and detrimental implications on various cellular processes as well as on organismal evolution. The implications of deamination on 5-hydroxymethylcytosine, on the other hand, remains less understood.\nTheoretical aspects.\nUntil October 2021, Cytosine had not been found in meteorites, which suggested the first strands of RNA and DNA had to look elsewhere to obtain this building block. Cytosine likely formed within some meteorite parent bodies, however did not persist within these bodies due to an effective deamination reaction into uracil.\nIn October 2021, Cytosine was announced as having been found in meteorites by researchers in a joint Japan/NASA project, that used novel methods of detection which avoided damaging nucleotides as they were extracted from meteorites.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6017", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6017", "title": "Cruise Missile", "text": ""}
{"id": "6018", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=6018", "title": "Call Of Cthulhu", "text": ""}
{"id": "6019", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6019", "title": "Computational chemistry", "text": "Branch of chemistry\nComputational chemistry is a branch of chemistry that uses computer simulations to assist in solving chemical problems. It uses methods of theoretical chemistry incorporated into computer programs to calculate the structures and properties of molecules, groups of molecules, and solids. The importance of this subject stems from the fact that, with the exception of some relatively recent findings related to the hydrogen molecular ion (dihydrogen cation), achieving an accurate quantum mechanical depiction of chemical systems analytically, or in a closed form, is not feasible. The complexity inherent in the many-body problem exacerbates the challenge of providing detailed descriptions of quantum mechanical systems. While computational results normally complement information obtained by chemical experiments, it can occasionally predict unobserved chemical phenomena.\nOverview.\nComputational chemistry differs from theoretical chemistry, which involves a mathematical description of chemistry. However, computational chemistry involves the usage of computer programs and additional mathematical skills in order to accurately model various chemical problems. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.\nHistorically, computational chemistry has had two different aspects:\nAs a result, a whole host of algorithms has been put forward by computational chemists.\nHistory.\nBuilding on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927, using valence bond theory. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 \"Introduction to Quantum Mechanics \u2013 with Applications to Chemistry\", Eyring, Walter and Kimball's 1944 \"Quantum Chemistry\", Heitler's 1945 \"Elementary Wave Mechanics \u2013 with Applications to Quantum Chemistry\", and later Coulson's 1952 textbook \"Valence\", each of which served as primary references for chemists in the decades to follow.\nWith the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One significant advancement was marked by Clemens C. J. Roothaan's 1951 paper in the Reviews of Modern Physics. This paper focused largely on the \"LCAO MO\" approach (Linear Combination of Atomic Orbitals Molecular Orbitals). For many years, it was the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first \"ab initio\" Hartree\u2013Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of \"ab initio\" calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in \"ab initio\" theory have been published by Schaefer.\nIn 1964, H\u00fcckel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of \u03c0 electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.\nIn the early 1970s, efficient \"ab initio\" computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed \"ab initio\" calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.\nOne of the first mentions of the term \"computational chemistry\" can be found in the 1970 book \"Computers and Their Role in the Physical Sciences\" by Sidney Fernbach and Abraham Haskell Taub, where they state \"It seems, therefore, that 'computational chemistry' can finally be more and more of a reality.\" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of \"computational chemistry\". The \"Journal of Computational Chemistry\" was first published in 1980.\nComputational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, \"for his development of the density-functional theory\", and John Pople, \"for his development of computational methods in quantum chemistry\", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for \"the development of multiscale models for complex chemical systems\".\nApplications.\nThere are several fields within computational chemistry.\nThese fields can give rise to several applications as shown below.\nCatalysis.\nComputational chemistry is a tool for analyzing catalytic systems without doing experiments. Modern electronic structure theory and density functional theory has allowed researchers to discover and understand catalysts. Computational studies apply theoretical chemistry to catalysis research. Density functional theory methods calculate the energies and orbitals of molecules to give models of those structures. Using these methods, researchers can predict values like activation energy, site reactivity and other thermodynamic properties.\nData that is difficult to obtain experimentally can be found using computational methods to model the mechanisms of catalytic cycles. Skilled computational chemists provide predictions that are close to experimental data with proper considerations of methods and basis sets. With good computational data, researchers can predict how catalysts can be improved to lower the cost and increase the efficiency of these reactions.\nDrug development.\nComputational chemistry is used in drug development to model potentially useful drug molecules and help companies save time and cost in drug development. The drug discovery process involves analyzing data, finding ways to improve current molecules, finding synthetic routes, and testing those molecules. Computational chemistry helps with this process by giving predictions of which experiments would be best to do without conducting other experiments. Computational methods can also find values that are difficult to find experimentally like pKa's of compounds. Methods like density functional theory can be used to model drug molecules and find their properties, like their HOMO and LUMO energies and molecular orbitals. Computational chemists also help companies with developing informatics, infrastructure and designs of drugs.\nAside from drug synthesis, drug carriers are also researched by computational chemists for nanomaterials. It allows researchers to simulate environments to test the effectiveness and stability of drug carriers. Understanding how water interacts with these nanomaterials ensures stability of the material in human bodies. These computational simulations help researchers optimize the material find the best way to structure these nanomaterials before making them.\nComputational chemistry databases.\nDatabases are useful for both computational and non computational chemists in research and verifying the validity of computational methods. Empirical data is used to analyze the error of computational methods against experimental data. Empirical data helps researchers with their methods and basis sets to have greater confidence in the researchers results. Computational chemistry databases are also used in testing software or hardware for computational chemistry.\nDatabases can also use purely calculated data. Purely calculated data uses calculated values over experimental values for databases. Purely calculated data avoids dealing with these adjusting for different experimental conditions like zero-point energy. These calculations can also avoid experimental errors for difficult to test molecules. Though purely calculated data is often not perfect, identifying issues is often easier for calculated data than experimental.\nDatabases also give public access to information for researchers to use. They contain data that other researchers have found and uploaded to these databases so that anyone can search for them. Researchers use these databases to find information on molecules of interest and learn what can be done with those molecules. Some publicly available chemistry databases include the following.\nMethods.\n\"Ab initio\" method.\nThe programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schr\u00f6dinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations\u00a0\u2013 being derived directly from theory, with no inclusion of experimental data\u00a0\u2013 are called \"ab initio methods\". A theoretical approximation is rigorously defined on first principles and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).\nAb initio methods need to define a level of theory (the method) and a basis set. A basis set consists of functions centered on the molecule's atoms. These sets are then used to describe molecular orbitals via the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz.\nA common type of \"ab initio\" electronic structure calculation is the Hartree\u2013Fock method (HF), an extension of molecular orbital theory, where electron-electron repulsions in the molecule are not specifically taken into account; only the electrons' average effect is included in the calculation. As the basis set size increases, the energy and wave function tend towards a limit called the Hartree\u2013Fock limit.\nMany types of calculations begin with a Hartree\u2013Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. These types of calculations are termed post\u2013Hartree\u2013Fock methods. By continually improving these methods, scientists can get increasingly closer to perfectly predicting the behavior of atomic and molecular systems under the framework of quantum mechanics, as defined by the Schr\u00f6dinger equation. To obtain exact agreement with the experiment, it is necessary to include specific terms, some of which are far more important for heavy atoms than lighter ones.\nIn most cases, the Hartree\u2013Fock wave function occupies a single configuration or determinant. In some cases, particularly for bond-breaking processes, this is inadequate, and several configurations must be used.\nThe total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without full knowledge of the complete surface.\nComputational thermochemistry.\nA particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1\u00a0kcal/mol or 4\u00a0kJ/mol. To reach that accuracy in an economic way, it is necessary to use a series of post\u2013Hartree\u2013Fock methods and combine the results. These methods are called quantum chemistry composite methods.\nChemical dynamics.\nAfter the electronic and nuclear variables are separated within the Born\u2013Oppenheimer representation, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schr\u00f6dinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schr\u00f6dinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.\nThe most popular methods for propagating the wave packet associated to the molecular geometry are:\nSplit operator technique.\nHow a computational method solves quantum equations impacts the accuracy and efficiency of the method. The split operator technique is one of these methods for solving differential equations. In computational chemistry, split operator technique reduces computational costs of simulating chemical systems. Computational costs are about how much time it takes for computers to calculate these chemical systems, as it can take days for more complex systems. Quantum systems are difficult and time-consuming to solve for humans. Split operator methods help computers calculate these systems quickly by solving the sub problems in a quantum differential equation. The method does this by separating the differential equation into two different equations, like when there are more than two operators. Once solved, the split equations are combined into one equation again to give an easily calculable solution.\nThis method is used in many fields that require solving differential equations, such as biology. However, the technique comes with a splitting error. For example, with the following solution for a differential equation.\nformula_1\nThe equation can be split, but the solutions will not be exact, only similar. This is an example of first order splitting.\nformula_2\nThere are ways to reduce this error, which include taking an average of two split equations.\nAnother way to increase accuracy is to use higher order splitting. Usually, second order splitting is the most that is done because higher order splitting requires much more time to calculate and is not worth the cost. Higher order methods become too difficult to implement, and are not useful for solving differential equations despite the higher accuracy.\nComputational chemists spend much time making systems calculated with split operator technique more accurate while minimizing the computational cost. Calculating methods is a massive challenge for many chemists trying to simulate molecules or chemical environments.\nDensity functional methods.\nDensity functional theory (DFT) methods are often considered to be \"ab initio methods\" for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree\u2013Fock exchange term and are termed hybrid functional methods.\nSemi-empirical methods.\nSemi-empirical quantum chemistry methods are based on the Hartree\u2013Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60s to the 90s, especially for treating large molecules where the full Hartree\u2013Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.\nPrimitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For \u03c0-electron systems, this was the H\u00fcckel method proposed by Erich H\u00fcckel, and for all valence electron systems, the extended H\u00fcckel method proposed by Roald Hoffmann. Sometimes, H\u00fcckel methods are referred to as \"completely empirical\" because they do not derive from a Hamiltonian. Yet, the term \"empirical methods\", or \"empirical force fields\" is usually used to describe molecular mechanics.\nMolecular mechanics.\nIn many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance, the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or \"ab initio\" calculations.\nThe database of compounds used for parameterization, i.e. the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance, proteins, would be expected to only have any relevance when describing other molecules of the same class. These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.\nMolecular dynamics.\nMolecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behavior of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time. The phase point of a system described by the positions and momenta of all its particles on a previous time point will determine the next phase point in time by integrating over Newton's laws of motion.\nMonte Carlo.\nMonte Carlo (MC) generates configurations of a system by making random changes to the positions of its particles, together with their orientations and conformations where appropriate. It is a random sampling method, which makes use of the so-called \"importance sampling\". Importance sampling methods are able to generate low energy states, as this enables properties to be calculated accurately. The potential energy of each configuration of the system can be calculated, together with the values of other properties, from the positions of the atoms.\nQuantum mechanics/molecular mechanics (QM/MM).\nQM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.\nQuantum Computational Chemistry.\nQuantum computational chemistry aims to exploit quantum computing to simulate chemical systems, distinguishing itself from the QM/MM (Quantum Mechanics/Molecular Mechanics) approach. While QM/MM uses a hybrid approach, combining quantum mechanics for a portion of the system with classical mechanics for the remainder, quantum computational chemistry exclusively uses quantum computing methods to represent and process information, such as Hamiltonian operators.\nConventional computational chemistry methods often struggle with the complex quantum mechanical equations, particularly due to the exponential growth of a quantum system's wave function. Quantum computational chemistry addresses these challenges using quantum computing methods, such as qubitization and quantum phase estimation, which are believed to offer scalable solutions.\nQubitization involves adapting the Hamiltonian operator for more efficient processing on quantum computers, enhancing the simulation's efficiency. Quantum phase estimation, on the other hand, assists in accurately determining energy eigenstates, which are critical for understanding the quantum system's behavior.\nWhile these techniques have advanced the field of computational chemistry, especially in the simulation of chemical systems, their practical application is currently limited mainly to smaller systems due to technological constraints. Nevertheless, these developments may lead to significant progress towards achieving more precise and resource-efficient quantum chemistry simulations.\nComputational costs in chemistry algorithms.\nThe computational cost and algorithmic complexity in chemistry are used to help understand and predict chemical phenomena. They help determine which algorithms/computational methods to use when solving chemical problems. This section focuses on the scaling of computational complexity with molecule size and details the algorithms commonly used in both domains.\nIn quantum chemistry, particularly, the complexity can grow exponentially with the number of electrons involved in the system. This exponential growth is a significant barrier to simulating large or complex systems accurately.\nAdvanced algorithms in both fields strive to balance accuracy with computational efficiency. For instance, in MD, methods like Verlet integration or Beeman's algorithm are employed for their computational efficiency. In quantum chemistry, hybrid methods combining different computational approaches (like QM/MM) are increasingly used to tackle large biomolecular systems.\nAlgorithmic complexity examples.\nThe following list illustrates the impact of computational complexity on algorithms used in chemical computations. It is important to note that while this list provides key examples, it is not comprehensive and serves as a guide to understanding how computational demands influence the selection of specific computational methods in chemistry.\nMolecular dynamics.\nAlgorithm.\nSolves Newton's equations of motion for atoms and molecules.\nComplexity.\nThe standard pairwise interaction calculation in MD leads to an formula_3complexity for formula_4 particles. This is because each particle interacts with every other particle, resulting in formula_5 interactions. Advanced algorithms, such as the Ewald summation or Fast Multipole Method, reduce this to formula_6 or even formula_7 by grouping distant particles and treating them as a single entity or using clever mathematical approximations.\nQuantum mechanics/molecular mechanics (QM/MM).\nAlgorithm.\nCombines quantum mechanical calculations for a small region with molecular mechanics for the larger environment.\nComplexity.\nThe complexity of QM/MM methods depends on both the size of the quantum region and the method used for quantum calculations. For example, if a Hartree-Fock method is used for the quantum part, the complexity can be approximated as formula_8, where formula_9 is the number of basis functions in the quantum region. This complexity arises from the need to solve a set of coupled equations iteratively until self-consistency is achieved.\nHartree-Fock method.\nAlgorithm.\nFinds a single Fock state that minimizes the energy.\nComplexity.\nNP-hard or NP-complete as demonstrated by embedding instances of the Ising model into Hartree-Fock calculations. The Hartree-Fock method involves solving the Roothaan-Hall equations, which scales as formula_10 to formula_7 depending on implementation, with formula_4 being the number of basis functions. The computational cost mainly comes from evaluating and transforming the two-electron integrals. This proof of NP-hardness or NP-completeness comes from embedding problems like the Ising model into the Hartree-Fock formalism.\nDensity functional theory.\nAlgorithm.\nInvestigates the electronic structure or nuclear structure of many-body systems such as atoms, molecules, and the condensed phases.\nComplexity.\nTraditional implementations of DFT typically scale as formula_10, mainly due to the need to diagonalize the Kohn-Sham matrix. The diagonalization step, which finds the eigenvalues and eigenvectors of the matrix, contributes most to this scaling. Recent advances in DFT aim to reduce this complexity through various approximations and algorithmic improvements.\nStandard CCSD and CCSD(T) method.\nAlgorithm.\nCCSD and CCSD(T) methods are advanced electronic structure techniques involving single, double, and in the case of CCSD(T), perturbative triple excitations for calculating electronic correlation effects.\nComplexity.\nCCSD.\nScales as formula_14 where formula_9 is the number of basis functions. This intense computational demand arises from the inclusion of single and double excitations in the electron correlation calculation.\nCCSD(T).\nWith the addition of perturbative triples, the complexity increases to formula_16. This elevated complexity restricts practical usage to smaller systems, typically up to 20-25 atoms in conventional implementations.\nLinear-scaling CCSD(T) method.\nAlgorithm.\nAn adaptation of the standard CCSD(T) method using local natural orbitals (NOs) to significantly reduce the computational burden and enable application to larger systems.\nComplexity.\nAchieves linear scaling with the system size, a major improvement over the traditional fifth-power scaling of CCSD. This advancement allows for practical applications to molecules of up to 100 atoms with reasonable basis sets, marking a significant step forward in computational chemistry's capability to handle larger systems with high accuracy.\nProving the complexity classes for algorithms involves a combination of mathematical proof and computational experiments. For example, in the case of the Hartree-Fock method, the proof of NP-hardness is a theoretical result derived from complexity theory, specifically through reductions from known NP-hard problems.\nFor other methods like MD or DFT, the computational complexity is often empirically observed and supported by algorithm analysis. In these cases, the proof of correctness is less about formal mathematical proofs and more about consistently observing the computational behaviour across various systems and implementations.\nAccuracy.\nComputational chemistry is not an \"exact\" description of real-life chemistry, as the mathematical and physical models of nature can only provide an approximation. However, the majority of chemical phenomena can be described to a certain degree in a qualitative or approximate quantitative computational scheme.\nMolecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schr\u00f6dinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schr\u00f6dinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.\nAccuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometers and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).\nThere is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM).In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).\nSoftware packages.\nMany self-sufficient exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6020", "revid": "49489870", "url": "https://en.wikipedia.org/wiki?curid=6020", "title": "Crash (Ballard novel)", "text": "1973 novel by J. G. Ballard\nCrash is a novel by British author J. G. Ballard, first published in 1973 with cover designed by Bill Botten. It follows a group of car-crash fetishists who, inspired by the famous crashes of celebrities, become sexually aroused by staging and participating in car accidents.\nThe novel was released to divided critical reception, with many reviewers horrified by its provocative content. It was adapted into a controversial 1996 film of the same name by David Cronenberg.\nSynopsis.\nThe story is told through the eyes of narrator James Ballard, named after the author himself, but it centers on the sinister figure of Dr. Robert Vaughan, a former TV scientist turned \"nightmare angel of the highways\". James meets Vaughan after being injured in a car crash near London Airport. Gathering around Vaughan is a group of alienated people, all of them former crash victims, who follow him in his pursuit to re-enact the crashes of Hollywood celebrities such as Jayne Mansfield and James Dean, in order to experience what the narrator calls \"a new sexuality, born from a perverse technology\". Vaughan's ultimate fantasy is to die in a head-on collision with movie star Elizabeth Taylor.\nDevelopment.\nThe Papers of J. G. Ballard at the British Library include two revised drafts of \"Crash\" (Add MS 88938/3/8). Scanned extracts from Ballard's drafts are included in \"Crash: The Collector's Edition,\" ed. Chris Beckett.\nIn 1971, Harley Cokeliss directed a short film entitled \"Crash!\" based on a chapter in J. G. Ballard's book \"The Atrocity Exhibition\", where Ballard is featured, talking about the ideas in his book. British actress Gabrielle Drake appeared as a passenger and car-crash victim. Ballard later developed the idea, resulting in \"Crash\". In his draft of the novel he mentioned Drake by name, but references to her were removed from the published version. \nInterpretation.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt; Throughout \"Crash\" I have used the car not only as a sexual image, but as a total metaphor for man's life in today's society. As such the novel has a political role quite apart from its sexual content, but I would still like to think that \"Crash\" is the first pornographic novel based on technology. In a sense, pornography is the most political form of fiction, dealing with how we use and exploit each other in the most urgent and ruthless way. Needless to say, the ultimate role of \"Crash\" is cautionary, a warning against that brutal, erotic and overlit realm that beckons more and more persuasively to us from the margins of the technological landscape.\n\"Crash\" has been difficult to characterize as a novel. At some points in his career, Ballard claimed that \"Crash\" was a \"cautionary tale\", a view that he would later regret, asserting that it is in fact \"a psychopathic hymn. But it is a psychopathic hymn which has a point\". Likewise, Ballard previously characterized it a science fiction novel, a position he would later take back.\nJean Baudrillard wrote an analysis of \"Crash\" in \"Simulacra and Simulation\" in which he declared it \"the first great novel of the universe of simulation\". He made note of how the fetish in the story conflates the functionality of the automobiles with that of the human body and how the characters' injuries and the damage to the vehicles are used as equivalent signs. To him, the hyperfunctionality leads to the dysfunction in the story. Quotes were used extensively to illustrate that the language of the novel employs plain, mechanical terms for the parts of the automobile and proper, medical language for human sex organs and acts. The story is interpreted as showing a merger between technology, sexuality, and death, and he further argued that by pointing out Vaughan's character takes and keeps photos of the car crashes and the mutilated bodies involved. Baudrillard stated that there is no moral judgment about the events within the novel but that Ballard himself intended it as a warning against a cultural trend.\nThe story can be classed as dystopic.\nCritical reception.\nThe novel received divided reviews when originally published. One publisher's reader returned the verdict \"This author is beyond psychiatric help. Do Not Publish!\" A 1973 review in \"The New York Times\" was equally horrified: \"\"Crash\" is, hands-down, the most repulsive book I've yet to come across.\"\nHowever, retrospective opinion now considers \"Crash\" to be one of Ballard's best and most challenging works. Reassessing \"Crash\" in \"The Guardian\", Zadie Smith wrote, \"\"Crash\" is an existential book about how \"everybody uses everything\". How everything uses everybody. And yet it is not a hopeless vision.\" On Ballard's legacy, she writes: \"In Ballard's work there is always this mix of futuristic dread and excitement, a sweet spot where dystopia and utopia converge. For we cannot say we haven't got precisely what we dreamed of, what we always wanted, so badly.\"\nReferences in popular art.\nMusic.\nThe Normal's 1978 song \"Warm Leatherette\" was inspired by the novel, and later covered in 1980 by Grace Jones. Similarly inspired was \"Miss the Girl,\" a 1983 single by The Creatures.\nThe Manic Street Preachers' song \"Mausoleum\" from 1994's \"The Holy Bible\" contains the famous Ballard quote about his reasons for writing the book, \"I wanted to rub the human face in its own vomit. I wanted to force it to look in the mirror.\" John Foxx's album \"Metamatic\" contains songs that have Ballardian themes, such as \"No-one Driving\".\nOther film adaptations.\nAn apparently unauthorized adaptation of \"Crash\" called \"Nightmare Angel\" was filmed in 1986 by Susan Emerling and Zoe Beloff. This short film bears the credit \"Inspired by J. G. Ballard\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n "}
{"id": "6021", "revid": "69412", "url": "https://en.wikipedia.org/wiki?curid=6021", "title": "C (programming language)", "text": "General-purpose programming language\nC is a general-purpose programming language. It was created in the 1970s by Dennis Ritchie and remains widely used and influential. By design, C gives the programmer relatively direct access to the features of the typical CPU architecture, customized for the target instruction set. It has been and continues to be used to implement operating systems (especially kernels), device drivers, and protocol stacks, but its use in application software has been decreasing. C is used on computers that range from the largest supercomputers to the smallest microcontrollers and embedded systems.\nA successor to the programming language B, C was originally developed at Bell Labs by Ritchie between 1972 and 1973 to construct utilities running on Unix. It was applied to re-implementing the kernel of the Unix operating system. During the 1980s, C gradually gained popularity. It has become one of the most widely used programming languages, with C compilers available for practically all modern computer architectures and operating systems. The book \"The C Programming Language\", co-authored by the original language designer, served for many years as the \"de facto\" standard for the language. C has been standardized since 1989 by the American National Standards Institute (ANSI) and, subsequently, jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC).\nC is an imperative procedural language, supporting structured programming, lexical variable scope, and recursion, with a static type system. It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code.\nAlthough neither C nor its standard library provide some popular features found in other languages, it is flexible enough to support them. For example, object orientation and garbage collection are provided by external libraries GLib Object System and Boehm garbage collector, respectively.\nSince 2000, C has consistently ranked among the top four languages in the TIOBE index, a measure of the popularity of programming languages.\nCharacteristics.\nThe C language exhibits the following characteristics:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n\"Hello, world\" example.\nThe \"Hello, World!\" program example that appeared in the first edition of \"K&amp;R\" has become the model for an introductory program in most programming textbooks. The program prints \"hello, world\" to the standard output.\nThe original version was:\nmain()\n printf(\"hello, world\\n\");\nA more modern version is:\nint main(void)\n printf(\"hello, world\\n\");\nThe first line is a preprocessor directive, indicated by codice_11, which causes the preprocessor to replace that line of code with the text of the codice_12 header file, which contains declarations for input and output functions including codice_13. The angle brackets around codice_12 indicate that the header file can be located using a search strategy that selects header files provided with the compiler over files with the same name that may be found in project-specific directories.\nThe next code line declares the entry point function codice_15. The run-time environment calls this function to begin program execution. The type specifier codice_16 indicates that the function returns an integer value. The codice_8 parameter list indicates that the function consumes no arguments. The run-time environment actually passes two arguments (typed codice_16 and codice_19), but this implementation ignores them. The ISO C standard (section 5.1.2.2.1) requires syntax that either is void or these two arguments\u00a0\u2013 a special treatment not afforded to other functions.\nThe opening curly brace indicates the beginning of the code that defines the function.\nThe next line of code calls (diverts execution to) the C standard library function codice_13 with the address of the first character of a null-terminated string specified as a string literal. The text codice_21 is an escape sequence that denotes the newline character which when output in a terminal results in moving the cursor to the beginning of the next line. Even though codice_13 returns an codice_16 value, it is silently discarded. The semicolon codice_24 terminates the call statement.\nThe closing curly brace indicates the end of the codice_15 function. Prior to C99, an explicit codice_26 statement was required at the end of codice_15 function, but since C99, the codice_15 function (as being the initial function call) implicitly returns codice_29 upon reaching its final closing curly brace.\nHistory.\nEarly developments.\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was also developed in assembly language.\nB.\nThompson wanted a programming language for developing utilities for the new platform. He first tried writing a Fortran compiler, but he soon gave up the idea and instead created a cut-down version of the recently developed systems programming language called BCPL. The official description of BCPL was not available at the time, and Thompson modified the syntax to be less 'wordy' and similar to a simplified ALGOL known as SMALGOL. He called the result \"B\", describing it as \"BCPL semantics with a lot of SMALGOL syntax\". Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines. Ultimately, few utilities were written in B because it was too slow and could not take advantage of PDP-11 features such as byte addressability.\nUnlike BCPL's codice_30 marking comments up to the end of the line, B adopted codice_31 as the comment delimiter, more akin to PL/1, and allowing comments to appear in the middle of lines. (BCPL's comment style would be reintroduced in C++.)\nNew B and first C release.\nIn 1971 Ritchie started to improve B, to use the features of the more-powerful PDP-11. A significant addition was a character data type. He called this \"New B\" (NB). Thompson started to use NB to write the Unix kernel, and his requirements shaped the direction of the language development.\nThrough to 1972, richer types were added to the NB language. NB had arrays of codice_16 and codice_33, and to these types were added pointers, the ability to generate pointers to other types, arrays of all types, and types to be returned from functions. Arrays within expressions were effectively treated as pointers. A new compiler was written, and the language was renamed C.\nThe C compiler and some utilities made with it were included in Version 2 Unix, which is also known as Research Unix.\nStructures and Unix kernel re-write.\nAt Version 4 Unix, released in November 1973, the Unix kernel was extensively re-implemented in C. By this time, the C language had acquired some powerful features such as codice_34 types.\nThe preprocessor was introduced around 1973 at the urging of Alan Snyder and also in recognition of the usefulness of the file-inclusion mechanisms available in BCPL and PL/I. Its original version provided only included files and simple string replacements: codice_11 and codice_36 of parameterless macros. Soon after that, it was extended, mostly by Mike Lesk and then by John Reiser, to incorporate macros with arguments and conditional compilation.\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In and around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.\nK&amp;R C.\nIn 1978 Brian Kernighan and Dennis Ritchie published the first edition of \"The C Programming Language\". Known as \"K&amp;R\" from the initials of its authors, the book served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as \"K&amp;R C\". As this was released in 1978, it is now also referred to as \"C78\". The second edition of the book covers the later ANSI C standard, described below.\n\"K&amp;R\" introduced several language features:\nEven after the publication of the 1989 ANSI standard, for many years K&amp;R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&amp;R C code can be legal Standard C as well.\nAlthough later versions of C require functions to have an explicit type declaration, K&amp;R C only requires functions that return a type other than codice_16 to be declared before use. Functions used without prior declaration were presumed to return codice_16.\nFor example:\nlong long_function();\ncalling_function()\n long longvar;\n register intvar;\n longvar = long_function();\n if (longvar &gt; 1)\n intvar = 0;\n else\n intvar = int_function();\n return intvar;\nThe declaration of (on line 1) is required since it returns ; not . Function can be called (line 11) even though it is not declared since it returns . Also, variable does not need to be declared as type since that is the default type for keyword.\nSince function declarations did not include information about arguments, type checks were not performed, although some compilers would issue a warning if different calls to a function used different numbers or types of arguments. Tools such as Unix's lint utility were developed that (among other things) checked for consistency of function use across multiple source files.\nIn the years following the publication of K&amp;R C, several features were added to the language, supported by compilers from AT&amp;T (in particular PCC) and other vendors. These included:\nThe popularity of the language, lack of agreement on standard library interfaces, and lack of compliance to the K&amp;R specification, led to standardization efforts.\nANSI C and ISO C.\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity increased significantly.\nIn 1983 the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\nIn 1990 the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\nOne of the aims of the C standardization process was to produce a superset of K&amp;R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_8 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&amp;R interface continued to be permitted, for compatibility with existing source code.\nC89 is supported by current C compilers, and most modern C code is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\nIn cases where code must be compilable by either standard-conforming or K&amp;R C-based compilers, the codice_55 macro can be used to split the code into Standard and K&amp;R sections to prevent the use on a K&amp;R C-based compiler of features available only in Standard C.\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.\nC99.\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.\nC99 introduced several new features, including inline functions, several new data types (including codice_56 and a codice_57 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_58, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_16 implicitly assumed. A standard macro codice_60 is defined with value codice_61 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.\nIn addition, the C99 standard requires support for identifiers using Unicode in the form of escaped characters (e.g. or ) and suggests support for raw Unicode names.\nC11.\nWork began in 2007 on another revision of the C standard, informally called \"C1X\" until its official publication of ISO/IEC 9899:2011 on December 8, 2011. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_60 is defined as codice_63 to indicate that C11 support is available.\nC17.\nC17 is an informal name for ISO/IEC 9899:2018, a standard for the C programming language published in June 2018. It introduces no new language features, only technical corrections, and clarifications to defects in C11. The standard macro codice_60 is defined as codice_65 to indicate that C17 support is available.\nC23.\nC23 is an informal name for the current major C language standard revision and was known as \"C2X\" through most of its development. It builds on past releases, introducing features like new keywords, types including codice_66 and codice_67, and expansions to the standard library.\nC23 was published in October 2024 as ISO/IEC 9899:2024. The standard macro codice_60 is defined as codice_69 to indicate that C23 support is available.\nC2Y.\nC2Y is an informal name for the next major C language standard revision, after C23 (C2X), that is hoped to be released later in the 2020s, hence the '2' in \"C2Y\". An early working draft of C2Y was released in February 2024 as N3220 by the working group ISO/IEC JTC1/SC22/WG14.\nEmbedded C.\nHistorically, embedded C programming requires non-standard extensions to the C language to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\nIn 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\nDefinition.\nC has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_70 and codice_71, or (since C99) following codice_58 until the end of the line. Comments delimited by codice_70 and codice_71 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_34, codice_52, and codice_77, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_33 and codice_16 specify built-in types. Sections of code are enclosed in braces (codice_80 and codice_81, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\nAs an imperative language, C uses \"statements\" to specify actions. The most common statement is an \"expression statement\", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_1 ... [codice_83] conditional execution and by codice_3 ... codice_4, codice_4, and codice_2 iterative execution (looping). The codice_2 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_89 and codice_90 can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured codice_91 statement, which branches directly to the designated label within the function. codice_5 selects a codice_93 to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next codice_93 unless terminated by a codice_89.\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_96, codice_97, codice_6 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\nKernighan and Ritchie say in the Introduction of \"The C Programming Language\": \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\nCharacter set.\nThe basic C source character set includes the following characters:\nThe \"newline\" character indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as such.\nThe POSIX standard mandates a portable character set which adds a few characters (notably \"@\") to the basic C source character set. Both standards do not prescribe any particular value encoding\u2014ASCII and EBCDIC both comply with these standards, since they include at least those basic characters, even though they use different encoded values for those characters.\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. Since C99 multi-national Unicode characters can be embedded portably within C source text by using codice_106 or codice_107 encoding (where codice_108 denotes a hexadecimal character).\nThe basic C execution character set contains the same characters, along with representations for the null character, alert, backspace, and carriage return.\nRun-time support for extended character sets has increased with each revision of the C standard.\nReserved words.\nAll versions of C have reserved words that are case sensitive. As reserved words, they cannot be used for variable names.\nC89 has 32 reserved words:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nC99 added five more reserved words: (\u2021 indicates an alternative spelling alias for a C23 keyword)\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nC11 added seven more reserved words: (\u2021 indicates an alternative spelling alias for a C23 keyword)\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nC23 reserved fifteen more words:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. Some of those words were added as keywords with their conventional spelling in C23 and the corresponding macros were removed.\nPrior to C89, codice_168 was reserved as a keyword. In the second edition of their book \"The C Programming Language\", which describes what became known as C89, Kernighan and Ritchie wrote, \"The ... [keyword] codice_168, formerly reserved but never used, is no longer reserved.\" and \"The stillborn codice_168 keyword is withdrawn.\"\nOperators.\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\nC uses the operator codice_176 (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator codice_187 to test for equality. The similarity between the operators for assignment and equality may result in the accidental use of one in place of the other, and in many cases the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression codice_209 might mistakenly be written as codice_210, which will be evaluated as codice_161 unless the value of codice_99 is codice_29 after the assignment.\nThe C operator precedence is not always intuitive. For example, the operator codice_187 binds more tightly than (is executed prior to) the operators codice_179 (bitwise AND) and codice_180 (bitwise OR) in expressions such as codice_217, which must be written as codice_218 if that is the coder's intent.\nData types.\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (codice_77). Integer type codice_33 is often used for single-byte characters. C99 added a Boolean data type. There are also derived types including arrays, pointers, records (codice_34), and unions (codice_52).\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a \"type cast\" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)\nC's \"usual arithmetic conversions\" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\nPointers.\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be \"dereferenced\" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type.\nPointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers; the result of a codice_223 is usually cast to the data type of the data to be stored. Many data types, such as trees, are commonly implemented as dynamically allocated codice_34 objects linked together using pointers. Pointers to other pointers are often used in multi-dimensional arrays and arrays of codice_34 objects. Pointers to functions (\"function pointers\") are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch), in dispatch tables, or as callbacks to event handlers.\nA \"null pointer value\" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a \"null pointer constant\" can be written as codice_29, with or without explicit casting to a pointer type, as the codice_227 macro defined by several standard headers or, since C23 with the constant codice_158. In conditional contexts, null pointer values evaluate to codice_157, while all other pointer values evaluate to codice_161.\nVoid pointers (codice_231) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\nArrays.\nArray types in C are traditionally of a fixed, static size specified at compile time. The more recent C99 standard also allows a form of variable-length arrays. However, it is also possible to allocate a block of memory (of arbitrary size) at run time, using the standard library's codice_223 function, and treat it as an array.\nSince arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions.\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order. Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, in early versions of C the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this was to allocate the array with an additional \"row vector\" of pointers to the columns.) C99 introduced \"variable-length arrays\" which address this issue.\nThe following example using modern C (C99 or later) shows allocation of a two-dimensional array on the heap and the use of multi-dimensional array indexing for accesses (which can use bounds-checking on many C compilers):\nint func(int n, int m) {\n float (*p)[n][m] = malloc(sizeof *p);\n if (p == NULL) {\n return -1;\n for (int i = 0; i &lt; n; i++) {\n for (int j = 0; j &lt; m; j++) {\n (*p)[i][j] = i + j;\n print_array(n, m, p);\n free(p);\n return 1;\nAnd here is a similar implementation using C99's \"Auto VLA\" feature:\nint func(int n, int m) {\n // Caution: checks should be made to ensure n * m * sizeof(float) does NOT exceed limitations for auto VLAs and is within available size of stack.\n float p[n][m]; // auto VLA is held on the stack, and sized when the function is invoked\n for (int i = 0; i &lt; n; i++) {\n for (int j = 0; j &lt; m; j++) {\n p[i][j] = i + j;\n print_array(n, m, p);\n // no need to free(p) since it will disappear when the function exits, along with the rest of the stack frame\n return 1;\nArray\u2013pointer interchangeability.\nThe subscript notation codice_233 (where codice_234 designates a pointer) is syntactic sugar for codice_235. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_236 points to is not the base address (pointed to by codice_234) incremented by codice_45 bytes, but rather is defined to be the base address incremented by codice_45 multiplied by the size of an element that codice_234 points to. Thus, codice_233 designates the codice_242th element of the array.\nFurthermore, in most expression contexts (a notable exception is as operand of codice_131), an expression of array type is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\nThe total size of an array codice_234 can be determined by applying codice_131 to an expression of array type. The size of an element can be determined by applying the operator codice_131 to any dereferenced element of an array codice_101, as in codice_248. Thus, the number of elements in a declared array codice_101 can be determined as codice_250. Note, that if only a pointer to the first element is available as it is often the case in C code because of the automatic conversion described above, the information about the full type of the array and its length are lost.\nMemory management.\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run time. Most C programs make extensive use of all three.\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at run time, and since static allocations (and automatic allocations before C99) must have a fixed size at compile time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on C dynamic memory allocation for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\nHeap memory allocation has to be synchronized with its actual usage in any program to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before it is deallocated explicitly, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a \"memory leak\". Conversely, it is possible for memory to be freed but referenced subsequently, leading to unpredictable results. Typically, the failure symptoms appear in a portion of the program unrelated to the code that causes the error, making it difficult to diagnose the failure. Such issues are ameliorated in languages with automatic garbage collection.\nLibraries.\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. For a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_254, shorthand for \"link the math library\").\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_12) specify the interfaces for these and other standard library facilities.\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.\nFile handling and streams.\nFile input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. codice_12). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high-level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it is sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid-state drive. Low-level I/O functions are not part of the standard C library but are generally part of \"bare metal\" programming (programming that is independent of any operating system such as most embedded programming). With few exceptions, implementations include low-level I/O.\nLanguage tools.\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler.\nAutomated source code checking and auditing tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\nMemory management checking tools like Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover run-time errors in memory usage.\nUses.\nC has been widely used to implement end-user and system-level applications.\nRationale for use in systems programming.\nC is widely used for systems programming in implementing operating systems and embedded system applications. This is for several reasons:\nGames.\nComputer games are often built from a combination of languages. C has featured significantly, especially for those games attempting to obtain best performance from computer platforms. Examples include Doom from 1993.\nWorld Wide Web.\nHistorically, C was sometimes used for web development using the Common Gateway Interface (CGI) as a \"gateway\" for information between the web application, the server, and the browser. C may have been chosen over interpreted languages because of its speed, stability, and near-universal availability. It is no longer common practice for web development to be done in C, and many other web development languages are popular. Applications where C-based web development continues include the HTTP configuration pages on routers, IoT devices and similar, although even here some projects have parts in higher-level languages e.g. the use of Lua within OpenWRT.\nTwo popular web servers, Apache HTTP Server and Nginx, are written in C. C's close-to-the-metal approach allows for the construction of these high-performance software systems.\nC as an intermediate language.\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--. Also, contemporary major compilers GCC and LLVM both feature an intermediate representation that is not C, and those compilers support front ends for many languages including C.\nComputationally intensive libraries.\nC enables programmers to create efficient implementations of algorithms and data structures, because the layer of abstraction from hardware is thin, and its overhead is low, an important criterion for computationally intensive programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C. Many languages support calling library functions in C; for example, the Python-based framework NumPy uses C for the high-performance and hardware-interacting aspects.\nOther languages are written in C.\nA consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. For example, the reference implementations of Python, Perl, Ruby, and PHP are written in C.\nLimitations.\nRitchie himself joked about the limitations of the language that he created:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;the power of assembly language and the convenience of ... assembly language\u2014\u200a\nWhile C is popular, influential and hugely successful, it has drawbacks, including:\nFor some purposes, restricted styles of C have been adopted, e.g. MISRA C or CERT C, in an attempt to reduce the opportunity for glitches. Databases such as CWE attempt to count the ways that C has potential vulnerabilities, along with recommendations for mitigation.\nThere are tools that can mitigate some of the drawbacks. Contemporary C compilers include checks which may generate warnings to help identify many potential bugs.\nRelated languages.\nMany languages developed after C were influenced by and borrowed aspects of C, including C++, C#, C shell, D, Go, Java, JavaScript, Julia, Limbo, LPC, Objective-C, Perl, PHP, Python, Ruby, Rust, Swift, Verilog and SystemVerilog. Some claim that the most pervasive influence has been syntactical \u2013 that these languages combine the statement and expression syntax of C with type systems, data models and large-scale program structures that differ from those of C, sometimes radically.\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\nWhen object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\nThe C++ programming language (originally named \"C with Classes\") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\nObjective-C was originally a thin layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\nIn addition to C++ and Objective-C, Ch, Cilk, and Unified Parallel C are nearly supersets of C.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6022", "revid": "10044298", "url": "https://en.wikipedia.org/wiki?curid=6022", "title": "Cytology", "text": "Study of cells in terms of structure, function and chemistry"}
{"id": "6023", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6023", "title": "Castle of the Winds", "text": "1989 video game\nCastle of the Winds is a tile-based roguelike video game for Microsoft Windows. It was developed by Rick Saada in 1989 and distributed by Epic MegaGames in 1993. The game was released around 1998 as a freeware download by the author. Though it is secondary to its hack and slash gameplay, \"Castle of the Winds\" has a plot loosely based on Norse mythology, told with setting changes, unique items, and occasional passages of text. The game is composed of two parts: A Question of Vengeance, released as shareware, and Lifthransir's Bane, sold commercially. A combined license for both parts was also sold.\nGameplay.\nThe game differs from most roguelikes in a number of ways. Its interface is mouse-dependent, but supports keyboard shortcuts (such as 'g' to get an item). \"Castle of the Winds\" also allows the player to restore saved games after dying.\nThe game favors the use of magic in combat, as spells are the only weapons that work from a distance. The player character automatically gains a spell with each experience level, and can permanently gain others using corresponding books, until all thirty spells available are learned. There are two opposing pairs of elements: cold vs. fire and lightning vs. acid/poison. Spells are divided into six categories: attack, defense, healing, movement, divination, and miscellaneous.\n\"Castle of the Winds\" possesses an inventory system that limits a player's load based on weight and bulk, rather than by number of items. It allows the character to use different containers, including packs, belts, chests, and bags. Other items include weapons, armor, protective clothing, purses, and ornamental jewellery. Almost every item in the game can be normal, cursed, or enchanted, with curses and enchantments working in a manner similar to \"NetHack\". Although items do not break with use, they may already be broken or rusted when found. Most objects that the character currently carries can be renamed.\nWherever the player goes before entering the dungeon, there is always a town which offers the basic services of a temple for healing and curing curses, a junk store where anything can be sold for a few copper coins, a sage who can identify items and (from the second town onwards) a bank for storing the total capacity of coins to lighten the player's load. Other services that differ and vary in what they sell are outfitters, weaponsmiths, armoursmiths, magic shops and general stores.\nThe game tracks how much time has been spent playing the game. Although story events are not triggered by the passage of time, it does determine when merchants rotate their stock. Victorious players are listed as \"Valhalla's Champions\" in the order of time taken, from fastest to slowest. If the player dies, they are still put on the list, but are categorized as \"Dead\", with their experience point total listed as at the final killing blow. The amount of time spent also determines the difficulty of the last boss.\nPlot.\nThe player begins in a tiny hamlet, near which they used to live. Their farm has been destroyed and godparents killed. After clearing out an abandoned mine, the player finds a scrap of parchment that reveals the death of the player's godparents was ordered by an unknown enemy. The player then returns to the hamlet to find it pillaged, and decides to travel to Bjarnarhaven.\nOnce in Bjarnarhaven, the player explores the levels beneath a nearby fortress, eventually facing Hrungnir, the Hill Giant Lord, responsible for ordering the player's godparents' death. Hrungnir carries the Enchanted Amulet of Kings. Upon activating the amulet, the player is informed of their past by their dead father, after which the player is transported to the town of Crossroads, and \"Part I\" ends. The game can be imported or started over in \"Part II\".\nThe town of Crossroads is run by a Jarl who at first does not admit the player, but later (on up to three occasions) provides advice and rewards. The player then enters the nearby ruined titular Castle of the Winds. There the player meets his/her deceased grandfather, who instructs them to venture into the dungeons below, defeat Surtur, and reclaim their birthright. Venturing deeper, the player encounters monsters run rampant, a desecrated crypt, a necromancer, and the installation of various special rooms for elementals. The player eventually meets and defeats the Wolf-Man leader, Bear-Man leader, the four Jotun kings, a Demon Lord, and finally Surtur. Upon defeating Surtur and escaping the dungeons, the player sits upon the throne, completing the game.\nDevelopment.\nInspired by his love of RPGs and while learning Windows programming in the 80s, Rick Saada designed and completed \"Castle of the Winds\". The game sold 13,500 copies. By 1998, the game's author, Rick Saada, decided to distribute the entirety of \"Castle of the Winds\" free of charge.\nThe game is public domain per Rick Saada's words: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Rick Saada, creator of \"Castle of the Winds\", decided to give permission for anyone to distribute it for free. Epic doesn't have an \"exclusive\" license to sell it.\nGraphics.\nAll terrain tiles, some landscape features, all monsters and objects, and some spell/effect graphics take the form of Windows 3.1 icons and were done by Paul Canniff. Multi-tile graphics, such as ball spells and town buildings, are bitmaps included in the executable file. No graphics use colors other than the Windows-standard 16-color palette, plus transparency. They exist in monochrome versions as well, meaning that the game will display well on monochrome monitors.\nThe map view is identical to the playing-field view, except for scaling to fit on one screen. A simplified map view is available to improve performance on slower computers. The latter functionality also presents a cleaner display, as the aforementioned scaling routine does not always work correctly.\nReception.\n\"Computer Gaming World\" rated the gameplay as good and the graphics simple but effective, while noticing the lack of audio, but regarded the game itself enjoyable.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6024", "revid": "50885473", "url": "https://en.wikipedia.org/wiki?curid=6024", "title": "Reformed Christianity", "text": "Protestant denominational family\nReformed Christianity, also called Calvinism, is a major branch of Protestantism that began during the 16th-century Protestant Reformation. In the modern day, it is largely represented by the Continental Reformed, Presbyterian, and Congregational traditions, as well as parts of the Anglican (known as \"Episcopal\" in some regions), Baptist and Waldensian traditions, in addition to a minority of persons belonging to the Methodist faith (who are known as Calvinistic Methodists).\nReformed theology emphasizes the authority of the Bible and the sovereignty of God, as well as covenant theology, a framework for understanding the Bible based on God's covenants with people. Reformed churches emphasize simplicity in worship. Several forms of ecclesiastical polity are exercised by Reformed churches, including presbyterian, congregational, and some episcopal. Articulated by John Calvin, the Reformed faith holds to a spiritual (pneumatic) presence of Christ in the Lord's Supper.\nEmerging in the 16th century, the Reformed tradition developed over several generations, especially in Switzerland, Scotland and the Netherlands. In the 17th century, Jacobus Arminius and the Remonstrants were expelled from the Dutch Reformed Church over disputes regarding predestination and salvation, and from that time Arminians are usually considered to be a distinct tradition from the Reformed. This dispute produced the Canons of Dort, the basis for the \"doctrines of grace\" also known as the \"five points\" of Calvinism.\nCalvinism influenced social, economic, and political life by promoting hard work, trade, and wealth accumulation within ethical limits, laying the groundwork for modern capitalism, especially in Northern Europe and the United States. Its emphasis on elected church elders, the priesthood of all believers, and mixed government inspired early democratic practices, separation of powers, and protections for religious minorities, shaping colonies in North America and liberal political thought in England. Calvinist-inspired reforms also advanced social causes like abolition, women\u2019s suffrage, education, and humanitarian efforts worldwide.\nDefinition and terminology.\nThe term Reformed Christianity is derived from the denomination's self designation of \"Reformed Church\", beginning in Switzerland and Germany, shortly thereafter followed by the Dutch Republic. \"Calvinism\" is the name derived from its most famous leader, John Calvin (born Jehan Cauvin), an influential Reformation-era theologian from Geneva, Switzerland. The term was first used by opposing Lutherans in the 1550s. Calvin did not approve of the use of this term, and religious scholars have argued its use is misleading, inaccurate, unhelpful, and \"inherently distortive.\" \nThe definitions and boundaries of the terms \"Reformed Christianity\" and \"Calvinism\" are contested by scholars. As a historical movement, Reformed Christianity began during the Reformation with Huldrych Zwingli in Z\u00fcrich, Switzerland. Following the failure of the Marburg Colloquy between Zwingli's followers and those of Martin Luther in 1529 to mediate disputes regarding the real presence of Christ in the Lord's Supper, Zwingli's followers were defined by their opposition to Lutherans (while Lutherans affirmed a corporeal presence of Christ in the Eucharist through a sacramental union, the Reformed came to hold a real spiritual presence of Christ in the Eucharist as propunded by Calvin and Bullinger). They also opposed Anabaptist radicals thus remaining within the Magisterial Reformation. During the 17th-century Arminian Controversy, followers of Jacobus Arminius were forcibly removed from the Dutch Reformed Church for their views regarding predestination and salvation, and thenceforth Arminians would be considered outside the pale of Reformed orthodoxy, though some use the term \"Reformed\" to include Arminians while using the term \"Calvinist\" to exclude Arminians.\nReformed Christianity has historically included Anglicanism, the branch of Christianity originating in the Church of England. The Anglican confessions are considered Reformed Protestant and leaders of the Protestant Reformation in England, such as the guiding Reformer who shaped Anglican theology Thomas Cranmer, were influenced by and counted among Reformed (Calvinist) theologians. As with Lutheranism, the Church of England retained elements of Catholicism such as bishops and vestments, thus sometimes being called \"but halfly Reformed\" or a middle way between Lutheranism and Reformed Christianity, being closer liturgically to the former and theologically aligned with the latter. Beginning in the 17th century, Anglicanism broadened to the extent that Reformed theology is no longer the sole dominant theology of Anglicanism.\nSome scholars argue that the Particular Baptist (Reformed Baptist) strand of the Baptist tradition, who hold many of the same beliefs as Reformed Christians but not infant baptism, as expressed in the Second London Confession of Faith of 1689, should be considered part of Reformed Christianity, though this might not have been the view of early Reformed theologians. Others disagree, asserting that any type of Baptist should be considered separate from the Reformed branch of Christianity.\nHistory.\nThe first wave of Reformed theologians included Zwingli, Martin Bucer, Wolfgang Capito, John Oecolampadius, and Guillaume Farel. While from diverse academic backgrounds, their work already contained key themes within Reformed theology, especially the priority of scripture as a source of authority. Scripture was also viewed as a unified whole, which led to a covenantal theology of the sacraments of baptism and the Lord's Supper as visible signs of the covenant of grace. Another shared perspective was their denial of the real presence of Christ in the Eucharist. Each understood salvation to be by grace alone and affirmed a doctrine of unconditional election, the teaching that some people are chosen by God to be saved. Luther and his successor Philipp Melanchthon were significant influences on these theologians and, to a larger extent, those who followed. The doctrine of justification by faith alone, also known as \"sola fide\", was a direct inheritance from Luther.\nThe second generation featured John Calvin, Heinrich Bullinger, Thomas Cranmer, Wolfgang Musculus, Peter Martyr Vermigli, Andreas Hyperius and John \u00e0 Lasco. Written between 1536 and 1539, Calvin's \"Institutes of the Christian Religion\" was one of the most influential works of the era. Toward the middle of the 16th century, these beliefs were formed into one consistent creed which would shape the future definition of the Reformed faith. The 1549 \"Consensus Tigurinus\" unified Zwingli and Bullinger's memorialist theology of the Eucharist, which taught that it was simply a reminder of Christ's death, with Calvin's view of it as a means of grace with Christ actually present, though spiritually rather than bodily as in Catholic doctrine. The document demonstrates the diversity as well as unity in early Reformed theology, giving it a stability that enabled it to spread rapidly throughout Europe. This stands in marked contrast to the bitter controversy experienced by Lutherans prior to the 1579 Formula of Concord.\nThrough Calvin's missionary work in France, his program of reform eventually reached the French-speaking provinces of the Netherlands. Calvinism was adopted in the Electorate of the Palatinate under Frederick III, which led to the formulation of the Heidelberg Catechism in 1563. This and the Belgic Confession were adopted as confessional standards in the first synod of the Dutch Reformed Church in 1571.\nIn 1573, William the Silent joined the Calvinist Church. Calvinism was declared the official religion of the Kingdom of Navarre by the queen regnant Jeanne d'Albret after her conversion in 1560. Leading divines, either Calvinist or those sympathetic to Calvinism, settled in England, including Bucer, Martyr, and John \u0141aski, as did John Knox in Scotland. During the First English Civil War, English and Scots Presbyterians produced the Westminster Confession, which became the confessional standard for Presbyterians in the English-speaking world. Having established itself in Europe, the movement continued to spread to areas including North America, South Africa and Korea. While Calvin did not live to see the foundation of his work grow into an international movement, his death allowed his ideas to spread far beyond their city of origin and their borders and to establish their own distinct character.\nSpread.\nAlthough much of Calvin's work was in Geneva, his publications spread his ideas of a correctly Reformed church to many parts of Europe. In Switzerland, some cantons are still Reformed, and some are Catholic. Calvinism became the dominant doctrine within the Church of Scotland (Presbyterian Church), the Dutch Republic and parts of Germany, especially those adjacent to the Netherlands in the Palatinate, Kassel, and Lippe, spread by Caspar Olevian and Zacharias Ursinus among others. Protected by the local nobility, Calvinism became a significant religion in eastern Hungary and Hungarian-speaking areas of Transylvania. As of 2007[ [update]], there are about 3.5 million Hungarian Reformed people worldwide.\nCalvinism was also initially spreading in Flanders, Wallonia, France, Lithuania, and Poland before being mostly erased during the Counter-Reformation. One of the most important Polish reformed theologists was \u0141aski, who was also involved into organising churches in East Frisia and Stranger's Church in London. Later, a faction called the Polish Brethren broke away from Calvinism on January 22, 1556, when Piotr of Goni\u0105dz, a Polish student, spoke out against the doctrine of the Trinity during the general synod of the Reformed churches of Poland held in the village of Secemin. Calvinism gained some popularity in Scandinavia, especially Sweden, but was rejected in favor of Lutheranism after the Synod of Uppsala in 1593.\nMany 17th century European settlers in the Thirteen Colonies in British America were Calvinists, who emigrated because of arguments over church structure, including the Pilgrim Fathers. Others were forced into exile, including the French Huguenots. Dutch and French Calvinist settlers were also among the first European colonizers of South Africa, beginning in the 17th century, who became known as Boers or Afrikaners.\nSierra Leone was largely colonized by Calvinist settlers from Nova Scotia, many of whom were Black Loyalists who fought for the British Empire during the American War of Independence. John Marrant had organized a congregation there under the auspices of the Huntingdon Connection. Some of the largest Calvinist communions were started by 19th- and 20th-century missionaries. Especially large are those in Indonesia, Korea and Nigeria. In South Korea there are 20,000 Presbyterian congregations with about 9\u201310 million church members, scattered in more than 100 Presbyterian denominations. In South Korea, Presbyterianism is the largest Christian denomination.\nDemography.\nA 2011 report of the Pew Forum on Religious and Public Life estimates that members of Presbyterian or Reformed churches make up 7% of the estimated 801 million Protestants globally, or approximately 56 million people.\nThough the broadly defined Reformed faith is much larger, as it constitutes Congregationalist (0.5%), most of the United and uniting churches (unions of different denominations) (7.2%) and most likely some of the other Protestant denominations (38.2%). All three are distinct categories from Presbyterian or Reformed (7%) in this report. The Reformed family of churches is one of the largest Christian denominations, representing 75 million believers worldwide.\nAccording to \"Global Christianity: A Guide to the World's Largest Religion from Afghanistan to Zimbabwe\", in 2020, Presbyterian and Reformed Christians numbered around 65,446,000 people, or 0.8% of the world's population. Congregationalists were listed at 4,986,000, with 0.1% of the world's population. Therefore, the three branches of Reformed Christianity totaled 70,432,000 people, or 0.9% of the global population.\nThe survey also listed 77,792,000 members (1% of the world's population) in United Churches, the majority of which are formed by the merger of churches of the Reformed Tradition with churches of other branches of Protestantism.\nWorld Communions.\nThe World Communion of Reformed Churches (WCRC), which includes some United Churches, has 80 million believers. WCRC is the fourth largest Christian communion in the world, after the Roman Catholic Church, the Eastern Orthodox Churches, and the Anglican Communion. Many conservative Reformed churches which are strongly Calvinistic formed the World Reformed Fellowship which has about 70 member denominations. Most are not part of the WCRC because of its ecumenical attire. The International Conference of Reformed Churches is another conservative association.\nTheology.\nRevelation and scripture.\nReformed theologians believe that God communicates knowledge of himself to people through the Word of God. People are not able to know anything about God except through this self-revelation. (With the exception of general revelation of God; \"His invisible attributes, His eternal power and divine nature, have been clearly seen, being understood through what has been made, so that they are without excuse\" (Romans 1:20).) Speculation about anything which God has not revealed through his Word is not warranted. The knowledge people have of God is different from that which they have of anything else because God is infinite, and finite people are incapable of comprehending an infinite being. While the knowledge revealed by God to people is never incorrect, it is also never comprehensive.\nAccording to Reformed theologians, God's self-revelation is always through his son Jesus Christ, because Christ is the only mediator between God and people. Revelation of God through Christ comes through two basic channels. The first is creation and providence, which is God's creating and continuing to work in the world. This action of God gives everyone knowledge about God, but this knowledge is only sufficient to make people culpable for their sin; it does not include knowledge of the gospel. The second channel through which God reveals himself is redemption, which is the gospel of salvation from condemnation which is punishment for sin.\nIn Reformed theology, the Word of God takes several forms. Jesus Christ is the Word Incarnate. The prophecies about him said to be found in the Old Testament and the ministry of the apostles who saw him and communicated his message are also the Word of God. Further, the preaching of ministers about God is the very Word of God because God is considered to be speaking through them. God also speaks through human writers in the Bible, which is composed of texts set apart by God for self-revelation. Reformed theologians emphasize the Bible as a uniquely important means by which God communicates with people. People gain knowledge of God from the Bible which cannot be gained in any other way.\nReformed theologians affirm that the Bible is true, but differences emerge among them over the meaning and extent of its truthfulness. Conservative followers of the Princeton theologians take the view that the Bible is true and inerrant, or incapable of error or falsehood, in every place. This view is similar to that of Catholic orthodoxy as well as modern Evangelicalism. Another view, influenced by the teaching of Karl Barth and neo-orthodoxy, is found in the Presbyterian Church (U.S.A.)'s Confession of 1967. Those who take this view believe the Bible to be the primary source of our knowledge of God, but also that some parts of the Bible may be false, not witnesses to Christ, and not normative for the church. In this view, Christ is the revelation of God, and the scriptures witness to this revelation rather than being the revelation itself.\nCovenant theology.\nReformed theologians use the concept of covenant to describe the way God enters into fellowship with people in history. The concept of covenant is so prominent in Reformed theology that Reformed theology as a whole is sometimes called \"covenant theology\". However, sixteenth- and seventeenth-century theologians developed a particular theological system called \"covenant theology\" or \"federal theology\" which many conservative Reformed churches continue to affirm. This framework orders God's life with people primarily in two covenants: the covenant of works and the covenant of grace.\nThe covenant of works is made with Adam and Eve in the Garden of Eden. The terms of the covenant are that God provides a blessed life in the garden on condition that Adam and Eve obey God's law perfectly. Because Adam and Eve broke the covenant by eating the forbidden fruit, they became subject to death and were banished from the garden. This sin was passed down to all mankind because all people are said to be in Adam as a covenantal or \"federal\" head. Federal theologians usually imply that Adam and Eve would have gained immortality had they obeyed perfectly.\nA second covenant, called the covenant of grace, is said to have been made immediately following Adam and Eve's sin. In it, God graciously offers salvation from death on condition of faith in God. This covenant is administered in different ways throughout the Old and New Testaments, but retains the substance of being free of a requirement of perfect obedience.\nThrough the influence of Karl Barth, many contemporary Reformed theologians have discarded the covenant of works, along with other concepts of federal theology. Barth saw the covenant of works as disconnected from Christ and the gospel, and rejected the idea that God works with people in this way. Instead, Barth argued that God always interacts with people under the covenant of grace, and that the covenant of grace is free of all conditions whatsoever. Barth's theology and that which follows him has been called \"mono covenantal\" as opposed to the \"bi-covenantal\" scheme of classical federal theology. Conservative contemporary Reformed theologians, such as John Murray, have also rejected the idea of covenants based on law rather than grace. Michael Horton, however, has defended the covenant of works as combining principles of law and love.\nGod.\nFor the most part, the Reformed tradition did not modify the medieval consensus on the doctrine of God. God's character is described primarily using three adjectives: eternal, infinite, and unchangeable. Reformed theologians such as Shirley Guthrie have proposed that rather than conceiving of God in terms of his attributes and freedom to do as he pleases, the doctrine of God is to be based on God's work in history and his freedom to live with and empower people.\nReformed theologians have also traditionally followed the medieval tradition going back to before the early church councils of Nicaea and Chalcedon on the doctrine of the Trinity. God is affirmed to be one God in three persons: Father, Son, and Holy Spirit. The Son (Christ) is held to be eternally begotten by the Father and the Holy Spirit eternally proceeding from the Father and Son. However, contemporary theologians have been critical of aspects of Western views here as well. Drawing on the Eastern tradition, these Reformed theologians have proposed a \"social trinitarianism\" where the persons of the Trinity only exist in their life together as persons-in-relationship. Contemporary Reformed confessions such as the Barmen Confession and Brief Statement of Faith of the Presbyterian Church (USA) have avoided language about the attributes of God and have emphasized his work of reconciliation and empowerment of people. Feminist theologian Letty Russell used the image of partnership for the persons of the Trinity. According to Russell, thinking this way encourages Christians to interact in terms of fellowship rather than reciprocity. Conservative Reformed theologian Michael Horton, however, has argued that social trinitarianism is untenable because it abandons the essential unity of God in favor of a community of separate beings.\nChrist and atonement.\nReformed theologians affirm the historic Christian belief that Christ is eternally one person with a divine and a human nature. Reformed Christians have especially emphasized that Christ truly became human so that people could be saved. Christ's human nature has been a point of contention between Reformed and Lutheran Christology. In accord with the belief that finite humans cannot comprehend infinite divinity, Reformed theologians hold that Christ's human body cannot be in multiple locations at the same time. Because Lutherans believe that Christ is bodily present in the Eucharist, they hold that Christ is bodily present in many locations simultaneously. For Reformed Christians, such a belief denies that Christ actually became human. Some contemporary Reformed theologians have moved away from the traditional language of one person in two natures, viewing it as unintelligible to contemporary people. Instead, theologians tend to emphasize Jesus's context and particularity as a first-century Jew.\nJohn Calvin and many Reformed theologians who followed him describe Christ's work of redemption in terms of three offices: prophet, priest, and king. Christ is said to be a prophet in that he teaches perfect doctrine, a priest in that he intercedes to the Father on believers' behalf and offered himself as a sacrifice for sin, and a king in that he rules the church and fights on believers' behalf. The threefold office links the work of Christ to God's work in ancient Israel. Many, but not all, Reformed theologians continue to make use of the threefold office as a framework because of its emphasis on the connection of Christ's work to Israel. They have, however, often reinterpreted the meaning of each of the offices. For example, Karl Barth interpreted Christ's prophetic office in terms of political engagement on behalf of the poor.\nChristians believe Jesus' death and resurrection make it possible for believers to receive forgiveness for sin and reconciliation with God through the atonement. Reformed Protestants generally subscribe to a particular view of the atonement called penal substitutionary atonement, which explains Christ's death as a sacrificial payment for sin. Christ is believed to have died in place of the believer, who is accounted righteous as a result of this sacrificial payment.\nSin.\nIn Christian theology, people are created good and in the image of God but have become corrupted by sin, which causes them to be imperfect and overly self-interested. Reformed Christians, following the tradition of Augustine of Hippo, believe that this corruption of human nature was brought on by Adam and Eve's first sin, a doctrine called original sin.\nAlthough earlier Christian authors taught the elements of physical death, moral weakness, and a sin propensity within original sin, Augustine was the first Christian to add the concept of inherited guilt (\"reatus\") from Adam whereby every infant is born eternally damned and humans lack any residual ability to respond to God. Reformed theologians emphasize that this sinfulness affects all of a person's nature, including their will. This view, that sin so dominates people that they are unable to avoid sin, has been called total depravity. As a consequence, every one of their descendants inherited a stain of corruption and depravity. This condition, innate to all humans, is known in Christian theology as \"original sin\".\nCalvin thought original sin was \"a hereditary corruption and depravity of our nature, extending to all the parts of the soul.\" Calvin asserted people were so warped by original sin that \"everything which our mind conceives, meditates, plans, and resolves, is always evil.\" The depraved condition of every human being is not the result of sins people commit during their lives. Instead, before we are born, while we are in our mother's womb, \"we are in God's sight defiled and polluted.\" Calvin thought people were justly condemned to hell because their corrupted state is \"naturally hateful to God.\"\nIn colloquial English, the term \"total depravity\" can be easily misunderstood to mean that people are absent of any goodness or unable to do any good. However the Reformed teaching is actually that while people continue to bear God's image and may do things that appear outwardly good, their sinful intentions affect all of their nature and actions so that they are not pleasing to God.\nSalvation.\nReformed theologians, along with other Protestants, believe salvation from punishment for sin is to be given to all those who have faith in Christ. Faith is not purely intellectual, but involves trust in God's promise to save. Protestants do not hold there to be any other requirement for salvation, but that faith alone is sufficient. However, this faith in the Lord Jesus is understood as one that effects obedience. In a commentary on Ezekiel 18, Calvin stated: \"faith cannot justify when it is without works, because it is dead, and a mere fiction ... Thus faith can be no more separated from works than the sun from his heat.\"\nJustification is the part of salvation where God pardons the sin of those who believe in Christ. It is historically held by Protestants to be the most important article of Christian faith, though more recently it is sometimes given less importance out of ecumenical concerns. People are not on their own able to fully repent of their sin or prepare themselves to repent because of their sinfulness. Therefore, justification is held to arise solely from God's free and gracious act.\nSanctification is the part of salvation in which God makes believers holy, by enabling them to exercise greater love for God and for other people. The good works accomplished by believers as they are sanctified are considered to be the necessary outworking of the believer's salvation, though they do not cause the believer to be saved. Sanctification, like justification, is by faith, because doing good works is simply living as the child of God one has become.\nPredestination.\nStemming from the theology of John Calvin, Reformed theologians teach that sin so affects human nature that they are unable even to exercise faith in Christ by their own will. While people are said to retain free will, in that they willfully sin, they are unable not to sin because of the corruption of their nature due to original sin. Reformed Christians believe that God predestined some people to be saved and others were predestined to eternal damnation. This choice by God to save some is held to be unconditional and not based on any characteristic or action on the part of the person chosen. The Calvinist view is opposed to the Arminian view that God's choice of whom to save is conditional or based on his foreknowledge of who would respond positively to God.\nKarl Barth reinterpreted the doctrine of predestination to apply only to Christ. Individual people are only said to be elected through their being in Christ. Reformed theologians who followed Barth, including J\u00fcrgen Moltmann, David Migliore, and Shirley Guthrie, have argued that the traditional Reformed concept of predestination is speculative and have proposed alternative models. These theologians claim that a properly trinitarian doctrine emphasizes God's freedom to love all people, rather than choosing some for salvation and others for damnation. God's justice towards and condemnation of sinful people is spoken of by these theologians as out of his love for them and a desire to reconcile them to himself.\nFive Points of Calvinism.\nMuch attention surrounding Calvinism focuses on the \"Five Points of Calvinism\" (also called the \"doctrines of grace\"). The five points have been summarized under the acrostic TULIP. The five points are popularly said to summarize the Canons of Dort; however, there is no historical relationship between them, and some scholars argue that their language distorts the meaning of the Canons, Calvin's theology, and the theology of 17th-century Calvinistic orthodoxy, particularly in the language of total depravity and limited atonement. The five points were more recently popularized in the 1963 booklet \"The Five Points of Calvinism Defined, Defended, Documented\" by David N. Steele and Curtis C. Thomas. The origins of the five points and the acrostic are uncertain, but they appear to be outlined in the Counter Remonstrance of 1611, a lesser-known Reformed reply to the Arminians, which was written prior to the Canons of Dort. The acrostic was used by Cleland Boyd McAfee as early as circa 1905. An early printed appearance of the acrostic can be found in Loraine Boettner's 1932 book, \"The Reformed Doctrine of Predestination\".\nChurch.\nReformed Christians see the Christian Church as the community with which God has made the covenant of grace, a promise of eternal life and relationship with God. This covenant extends to those under the \"old covenant\" whom God chose, beginning with Abraham and Sarah. The church is conceived of as both invisible and visible. The invisible church is the body of all believers, known only to God. The visible church is the institutional body which contains both members of the invisible church as well as those who appear to have faith in Christ, but are not truly part of God's elect.\nIn order to identify the visible church, Reformed theologians have spoken of certain marks of the Church. For some, the only mark is the pure preaching of the gospel of Christ. Others, including John Calvin, also include the right administration of the sacraments. Others, such as those following the Scots Confession, include a third mark of rightly administered church discipline, or exercise of censure against unrepentant sinners. These marks allowed the Reformed to identify the church based on its conformity to the Bible rather than the magisterium or church tradition.\nWorship.\nRegulative principle of worship.\nThe regulative principle of worship is a teaching shared by some Calvinists and Anabaptists on how the Bible orders public worship. The substance of the doctrine regarding worship is that God institutes in the Scriptures everything he requires for worship in the Church and that everything else is prohibited. As the regulative principle is reflected in Calvin's own thought, it is driven by his evident antipathy toward the Roman Catholic Church and its worship practices, and it associates musical instruments with icons, which he considered violations of the Ten Commandments' prohibition of graven images.\nOn this basis, many early Calvinists also eschewed musical instruments and advocated a cappella exclusive psalmody in worship, though Calvin himself allowed other scriptural songs as well as psalms, and this practice typified Presbyterian worship and the worship of other Reformed churches for some time. The original Lord's Day service designed by John Calvin was a highly liturgical service with the Creed, Alms, Confession and Absolution, the Lord's supper, Doxologies, prayers, Psalms being sung, the Lords prayer being sung, and Benedictions.\nSince the 19th century, however, some of the Reformed churches have modified their understanding of the regulative principle and make use of musical instruments, believing that Calvin and his early followers went beyond the biblical requirements and that such things are circumstances of worship requiring biblically rooted wisdom, rather than an explicit command. Despite the protestations of those who hold to a strict view of the regulative principle, today hymns and musical instruments are in common use, as are contemporary worship music styles with elements such as worship bands.\nSacraments.\nThe Westminster Confession of Faith limits the sacraments to baptism and the Lord's Supper. Sacraments are denoted \"signs and seals of the covenant of grace.\" Westminster speaks of \"a sacramental relation, or a sacramental union, between the sign and the thing signified; whence it comes to pass that the names and effects of the one are attributed to the other.\" Baptism is for infant children of believers as well as believers, as it is for all the Reformed except Baptists and some Congregationalists. Baptism admits the baptized into the visible church, and in it all the benefits of Christ are offered to the baptized. On the Lord's supper, the Westminster Confession takes a position between Lutheran sacramental union and Zwinglian memorialism: \"the Lord's supper really and indeed, yet not carnally and corporally, but spiritually, receive and feed upon Christ crucified, and all benefits of his death: the body and blood of Christ being then not corporally or carnally in, with, or under the bread and wine; yet, as really, but spiritually, present to the faith of believers in that ordinance as the elements themselves are to their outward senses.\"\nThe 1689 London Baptist Confession of Faith does not use the term sacrament, but describes baptism and the Lord's supper as ordinances, as do most Baptists, Calvinist or otherwise. Baptism is only for those who \"actually profess repentance towards God\", and not for the children of believers. Baptists also insist on immersion or dipping, in contradistinction to other Reformed Christians. The Baptist Confession describes the Lord's supper as \"the body and blood of Christ being then not corporally or carnally, but spiritually present to the faith of believers in that ordinance\", similarly to the Westminster Confession. There is significant latitude in Baptist congregations regarding the Lord's supper, and many hold the Zwinglian view.\nLogical order of God's decree.\nThere are two schools of thought regarding the logical order of God's decree to ordain the fall of man: supralapsarianism (from the Latin: , \"above\", here meaning \"before\" + , \"fall\") and infralapsarianism (from the Latin: , \"beneath\", here meaning \"after\" + \"\", \"fall\"). The former view, sometimes called \"high Calvinism\", argues that the Fall occurred partly to facilitate God's purpose to choose some individuals for salvation and some for damnation. Infralapsarianism, sometimes called \"low Calvinism\", is the position that, while the Fall was indeed planned, it was not planned with reference to who would be saved.\nSupralapsarianism is based on the belief that God chose which individuals to save logically prior to the decision to allow the race to fall and that the Fall serves as the means of realization of that prior decision to send some individuals to hell and others to heaven (that is, it provides the grounds of condemnation in the reprobate and the need for salvation in the elect). In contrast, infralapsarians hold that God planned the race to fall logically prior to the decision to save or damn any individuals because, it is argued, in order to be \"saved\", one must first need to be saved from something and therefore the decree of the Fall must precede predestination to salvation or damnation.\nThese two views vied with each other at the Synod of Dort, an international body representing Calvinist Christian churches from around Europe, and the judgments that came out of that council sided with infralapsarianism (Canons of Dort, First Point of Doctrine, Article 7). The Westminster Confession of Faith also teaches (in Hodge's words \"clearly impl[ies]\") the infralapsarian view, but is sensitive to those holding to supralapsarianism. The Lapsarian controversy has a few vocal proponents on each side today, but overall it does not receive much attention among modern Calvinists.\nBranches.\nThe Reformed tradition is historically represented by the Continental, Presbyterian, Reformed Anglican, Congregationalist, Calvinistic Methodist and Reformed Baptist denominational families.\nReformed churches practice several forms of church government, primarily presbyterian and congregational, but some adhere to episcopal polity. The largest interdenominational association is the World Communion of Reformed Churches with more than 100 million members in 211 member denominations around the world. Smaller, conservative Reformed associations include the World Reformed Fellowship and the International Conference of Reformed Churches.\nContinental.\n\"Continental\" Reformed churches originate in continental Europe, a term used by English speakers to distinguish them from traditions from the British Isles. Many uphold the Helvetic Confessions and Heidelberg Catechism, which were adopted in Zurich and Heidelberg, respectively. In the United States, immigrants belonging to the continental Reformed churches joined the Dutch Reformed Church there, as well as the Anglican Church.\nPresbyterian.\nPresbyterian churches are named for their order of government by assemblies of elders, or \"presbyters\". They are especially influenced by John Knox, who brought Reformed theology and polity to the Church of Scotland after spending time on the continent in Calvin's Geneva. Presbyterians historically uphold the Westminster Confession of Faith.\nCongregational.\nCongregationalism originates in Puritanism, a sixteenth-century movement to reform the Church of England. Unlike the Presbyterians, Congregationalists consider the local church to be rightfully self-ruled by their own officers, not higher ecclesiastical courts. The Savoy Declaration, a revision of Westminster, is the primary confession of historic Congregationalism. Evangelical Congregationalists are internationally represented by the World Evangelical Congregational Fellowship. Christian denominations in the Congregationalist tradition include the United Church of Christ, the National Association of Congregational Christian Churches and the Conservative Congregational Christian Conference in the United States, Evangelical Congregational Church in Argentina and Evangelical Fellowship of Congregational Churches in the United Kingdom, among others.\nAnglican.\nThough Anglicanism today is often described its own branch of Protestantism, historic Anglicanism is a part of the wider Reformed tradition. The foundational documents of the Anglican church \"express a theology in keeping with the Reformed theology of the Swiss and South German Reformation.\" The Most Rev. Peter Robinson, presiding bishop of the United Episcopal Church of North America, writes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Cranmer's personal journey of faith left its mark on the Church of England in the form of a Liturgy that remains to this day more closely allied to Lutheran practice, but that liturgy is couple to a doctrinal stance that is broadly, but decidedly Reformed. ... The 42 Articles of 1552 and the 39 Articles of 1563, both commit the Church of England to the fundamentals of the Reformed Faith. Both sets of Articles affirm the centrality of Scripture, and take a monergist position on Justification. Both sets of Articles affirm that the Church of England accepts the doctrine of predestination and election as a 'comfort to the faithful' but warn against over much speculation concerning that doctrine. Indeed a casual reading of the Wurttemburg Confession of 1551, the Second Helvetic Confession, the Scots Confession of 1560, and the XXXIX Articles of Religion reveal them to be cut from the same bolt of cloth.\nReformed Methodist.\nReformed Methodists, also known as Calvinistic Methodists, form a minority of the Methodist tradition. The majority of Methodism falls outside the Reformed faith, being Wesleyan Methodism, which subscribes to Wesleyan-Arminian theology. Calvinistic Methodists adhere to Reformed theology codified in the \"Confession of Faith of the Calvinistic Methodists\" (1823). In the United Kingdom, the Calvinistic Methodist Church is also known as the Presbyterian Church of Wales. Calvinistic Methodists are characterized by their emphasis on Methodist worship distinctives\u2014preaching, hymn singing, lovefeasts, revival services, and camp meetings, as well as the Methodist doctrines of the New Birth and growth in grace. Reformed Methodist divines include George Whitefield and Howell Harris.\nReformed Baptist.\nReformed Baptists or Calvinistic Baptists, unlike other Reformed groups, exclusively practice believer's baptism. They observe a more congregational polity, taken from the Congregationalists. Their primary confession is the Second London Confession of Faith of 1689, a revision of the Savoy Declaration from the Congregationalists, and the Westminster Confession of Faith, from the Presbyterians, but other Baptist confessions like the First London Confession are also used. Not all Baptists are Particular Baptists, and, in fact, the Baptist tradition didn't start Particular Baptist, but General Baptist. Many Reformed Baptists accept Reformed theology, especially soteriology, and a covenantal theology, named the Baptist covenant theology.\nVariants in Reformed theology.\nAmyraldism.\nAmyraldism (or sometimes Amyraldianism, also known as the School of Saumur, hypothetical universalism, post redemptionism, moderate Calvinism, or four-point Calvinism) is the belief that God, prior to his decree of election, decreed Christ's atonement for all alike if they believe, but seeing that none would believe on their own, he then elected those whom he will bring to faith in Christ, thereby preserving the Calvinist doctrine of unconditional election. The efficacy of the atonement remains limited to those who believe.\nNamed after its formulator Moses Amyraut, this doctrine is still viewed as a variety of Calvinism in that it maintains the particularity of sovereign grace in the application of the atonement. However, detractors like B. B. Warfield have termed it \"an inconsistent and therefore unstable form of Calvinism.\"\nHyper-Calvinism.\nHyper-Calvinism is the belief that emphasizes God's sovereignty in election and salvation to such an extent that it rejects the responsibility of all people to \"repent and believe\" the gospel. This belief system became prominent among some of the early English Particular Baptists in the 18th century. Historically, it has been associated with theologians such as John Gill and Joseph Hussey who contributed to the development of its distinct views. This variant of Reformed Theology was opposed by ministers such as Andrew Fuller and missionaries such as William Carey who argued against the Hyper-Calvinistic mindset that \"if God wants to save the heathen, He will do it without your help or mine.\"\nThe Westminster Confession of Faith says that the gospel is to be freely offered to sinners, and the Larger Catechism makes clear that the gospel is offered to the non-elect.\nThe term is also used as a pejorative and occasionally appears in both theological and secular controversial contexts. It usually connotes a negative opinion about some variety of theological determinism, predestination, or a version of Evangelical Christianity or Calvinism that is deemed by the critic to be unenlightened, harsh, or extreme.\nNeo-Calvinism.\nBeginning in the 1880s, Neo-Calvinism, a form of Dutch Calvinism, is the movement initiated by the theologian and later Dutch prime minister Abraham Kuyper. James Bratt has identified a number of different types of Dutch Calvinism: The Seceders\u2014split into the Reformed Church \"West\" and the Confessionalists; and the Neo-Calvinists\u2014the Positives and the Antithetical Calvinists. The Seceders were largely infralapsarian and the Neo-Calvinists usually supralapsarian.\nKuyper wanted to awaken the church from what he viewed as its pietistic slumber. He declared:\nNo single piece of our mental world is to be sealed off from the rest and there is not a square inch in the whole domain of human existence over which Christ, who is sovereign over all, does not cry: 'Mine!' \nThis refrain has become something of a rallying call for Neo-Calvinists.\nChristian Reconstructionism.\nChristian Reconstructionism is a fundamentalist Calvinist theonomic movement that has remained rather obscure. Founded by R. J. Rushdoony, the movement has had an important influence on the Christian Right in the United States. The movement peaked in the 1990s. However, it lives on in small denominations such as the Reformed Presbyterian Church in the United States and as a minority position in other denominations. Christian Reconstructionists are usually postmillennialists and followers of the presuppositional apologetics of Cornelius Van Til. They tend to support a decentralized political order resulting in laissez-faire capitalism.\nNew Calvinism.\nNew Calvinism is a growing perspective within conservative Evangelicalism that embraces the fundamentals of 16th century Calvinism while also trying to be relevant in the present day world. In March 2009, \"Time\" magazine described the New Calvinism as one of the \"10 ideas changing the world\". Some of the major figures who have been associated with the New Calvinism are John Piper, Mark Driscoll, Al Mohler, Mark Dever, C. J. Mahaney, and Tim Keller. New Calvinists have been criticized for blending Calvinist soteriology with popular Evangelical positions on the sacraments and continuationism and for rejecting tenets seen as crucial to the Reformed faith such as confessionalism and covenant theology.\nSocial and economic influences.\nCalvin expressed himself on usury in a 1545 letter to a friend, Claude de Sachin, in which he criticized the use of certain passages of scripture invoked by people opposed to the charging of interest. He reinterpreted some of these passages, and suggested that others of them had been rendered irrelevant by changed conditions. He also dismissed the argument (based upon the writings of Aristotle) that it is wrong to charge interest for money because money itself is barren. He said that the walls and the roof of a house are barren, too, but it is permissible to charge someone for allowing him to use them. In the same way, money can be made fruitful.\nHe qualified his view, however, by saying that money should be lent to people in dire need without hope of interest, while a modest interest rate of 5% should be permitted in relation to other borrowers.\nIn \"The Protestant Ethic and the Spirit of Capitalism\", Max Weber wrote that capitalism in Northern Europe evolved when the Protestant (particularly Calvinist) ethic influenced large numbers of people to engage in work in the secular world, developing their own enterprises and engaging in trade and the accumulation of wealth for investment. In other words, the Protestant work ethic was an important force behind the unplanned and uncoordinated emergence of modern capitalism.\nExpert researchers and authors have referred to the United States as a \"Protestant nation\" or \"founded on Protestant principles,\" specifically emphasizing its Calvinist heritage.\nPolitics and society.\nCalvin's concepts of God and man led to ideas which were gradually put into practice after his death, in particular in the fields of politics and society. After their fight for independence from Spain (1579), the Netherlands, under Calvinist leadership, granted asylum to religious minorities, including French Huguenots, English Independents (Congregationalists), and Jews from Spain and Portugal. The ancestors of the philosopher Baruch Spinoza were Portuguese Jews. Aware of the trial against Galileo, Ren\u00e9 Descartes lived in the Netherlands, out of reach of the Inquisition, from 1628 to 1649. Pierre Bayle, a Reformed Frenchman, also felt safer in the Netherlands than in his home country. He was the first prominent philosopher who demanded tolerance for atheists. Hugo Grotius (1583\u20131645) was able to publish a rather liberal interpretation of the Bible and his ideas about natural law in the Netherlands. Moreover, the Calvinist Dutch authorities allowed the printing of books that could not be published elsewhere, such as Galileo's \"Discorsi\" (1638).\nAlongside the liberal development of the Netherlands came the rise of modern democracy in England and North America. In the Middle Ages, state and church had been closely connected. Martin Luther's doctrine of the two kingdoms separated state and church in principle. His doctrine of the priesthood of all believers raised the laity to the same level as the clergy, although Lutherans were content to allow the state to control the administration of the church.\nIn Geneva Calvin was more careful than Luther to keep church structures and city authorities apart and going one step further than Luther he included elected laymen (church elders, presbyters) in his concept of church government. In general the Reformed followed Calvin's lead in insisting that the church's external administration, including the right to excommunicate, not be handed over to the state. The Huguenots added synods whose members were also elected by the congregations. The other Reformed churches took over this system of church self-government, which was essentially a representative democracy. Baptists, Quakers, and Methodists are organized in a similar way. These denominations and the Anglican Church were influenced by Calvin's theology in varying degrees.\nIn another factor in the rise of democracy in the Anglo-American world, Calvin favored a mixture of democracy and aristocracy as the best form of government (mixed government). He appreciated the advantages of democracy. His political thought aimed to safeguard the rights and freedoms of ordinary men and women. In order to minimize the misuse of political power he suggested dividing it among several institutions in a system of checks and balances (separation of powers). Finally, Calvin taught that if worldly rulers rise up against God they should be put down. In this way, he and his followers stood in the vanguard of resistance to political absolutism and furthered the cause of democracy, although Calvin himself was alarmed about his arguments being used for revolutionary movements. The Congregationalists who founded Plymouth Colony (1620) and Massachusetts Bay Colony (1628) were convinced that the democratic form of government was the will of God. Enjoying self-rule, they practiced separation of powers. Rhode Island, Connecticut, and Pennsylvania, founded by Roger Williams, Thomas Hooker, and William Penn, respectively, combined democratic government with a limited freedom of religion that did not extend to Catholics (Congregationalism being the established, tax-supported religion in Connecticut). These colonies became safe havens for persecuted religious minorities, including Jews.\nIn England, Baptists Thomas Helwys (c. 1575\u2013c. 1616), and John Smyth (c. 1554\u2013c.\u20091612) influenced the liberal political thought of the Presbyterian poet and politician John Milton (1608\u20131674) and of the philosopher John Locke (1632\u20131704), who in turn had both a strong impact on the political development in their home country (English Civil War of 1642\u20131651, Glorious Revolution of 1688) as well as in North America. The ideological basis of the American Revolution was largely provided by the radical Whigs, who had been inspired by Milton, Locke, James Harrington (1611\u20131677), Algernon Sidney (1623\u20131683), and other thinkers. The Whigs' \"perceptions of politics attracted widespread support in America because they revived the traditional concerns of a Protestantism that had always verged on Puritanism\". The United States Declaration of Independence, the United States Constitution and (American) Bill of Rights initiated a tradition of human and civil rights that continued in the French Declaration of the Rights of Man and of the Citizen and the constitutions of numerous countries around the world, e.g. Latin America, Japan, India, Germany, and other European countries. It is also echoed in the United Nations Charter and the Universal Declaration of Human Rights.\nIn the 19th century, churches based on or influenced by Calvin's theology became deeply involved in social reforms, e.g. the abolition of slavery (William Wilberforce, Harriet Beecher Stowe, Abraham Lincoln, and others), women suffrage, and prison reforms. Members of these churches formed co-operatives to help the impoverished masses. The founders of the Red Cross Movement, including Henry Dunant, were Reformed Christians. Their movement also initiated the Geneva Conventions.\nThroughout the world, the Reformed churches operate hospitals, homes for handicapped or elderly people, and educational institutions on all levels. For example, American Congregationalists founded Harvard University (1636), Yale University (1701), and about a dozen other colleges. A particular stream of influence of Calvinism concerns art. Visual art cemented society in the first modern nation state, the Netherlands, and also Neo-Calvinism put much weight on this aspect of life. Hans Rookmaaker is the most prolific example. In literature the non-fiction of Marilynne Robinson\nargues for the modernity of Calvin's thinking, calling him a humanist scholar (p.\u00a0174, The Death of Adam).\nCriticism.\nOthers view Calvinist influence as not always being solely positive. The Boers and Afrikaner Calvinists combined ideas from Calvinism and Kuyperian theology to justify apartheid in South Africa. As late as 1974 the majority of the Dutch Reformed Church in South Africa was convinced that their theological stances (including the story of the Tower of Babel) could justify apartheid. In 1990 the Dutch Reformed Church document \"Church and Society\" maintained that although they were changing their stance on apartheid, they believed that within apartheid and under God's sovereign guidance, \"...everything was not without significance, but was of service to the Kingdom of God.\" These views were not universal and were condemned by many Calvinists outside South Africa. Pressure from both outside and inside the Dutch Reformed Calvinist church helped reverse apartheid in South Africa.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6026", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6026", "title": "Countable set", "text": "Mathematical set that can be enumerated\nIn mathematics, a set is countable if either it is finite or it can be made in one to one correspondence with the set of natural numbers. Equivalently, a set is \"countable\" if there exists an injective function from it into the natural numbers; this means that each element in the set may be associated to a unique natural number, or that the elements of the set can be counted one at a time, although the counting may never finish due to an infinite number of elements.\nIn more technical terms, assuming the axiom of countable choice, a set is \"countable\" if its cardinality (the number of elements of the set) is not greater than that of the natural numbers. A countable set that is not finite is said to be countably infinite.\nThe concept is attributed to Georg Cantor, who proved the existence of uncountable sets, that is, sets that are not countable; for example the set of the real numbers.\nA note on terminology.\nAlthough the terms \"countable\" and \"countably infinite\" as defined here are quite common, the terminology is not universal. An alternative style uses \"countable\" to mean what is here called countably infinite, and \"at most countable\" to mean what is here called countable.\nThe terms \"enumerable\" and denumerable may also be used, e.g. referring to countable and countably infinite respectively, definitions vary and care is needed respecting the difference with recursively enumerable.\nDefinition.\nA set formula_1 is \"countable\" if:\nAll of these definitions are equivalent.\nA set formula_1 is \"countably infinite\" if:\nA set is \"uncountable\" if it is not countable, i.e. its cardinality is greater than formula_3.\nHistory.\nIn 1874, in his first set theory article, Cantor proved that the set of real numbers is uncountable, thus showing that not all infinite sets are countable. In 1878, he used one-to-one correspondences to define and compare cardinalities. In 1883, he extended the natural numbers with his infinite ordinals, and used sets of ordinals to produce an infinity of sets having different infinite cardinalities.\nIntroduction.\nA \"set\" is a collection of \"elements\", and may be described in many ways. One way is simply to list all of its elements; for example, the set consisting of the integers 3, 4, and 5 may be denoted formula_28, called roster form. This is only effective for small sets, however; for larger sets, this would be time-consuming and error-prone. Instead of listing every single element, sometimes an ellipsis (\"...\") is used to represent many elements between the starting element and the end element in a set, if the writer believes that the reader can easily guess what ... represents; for example, formula_29 presumably denotes the set of integers from 1 to 100. Even in this case, however, it is still \"possible\" to list all the elements, because the number of elements in the set is finite. If we number the elements of the set 1, 2, and so on, up to formula_30, this gives us the usual definition of \"sets of size formula_30\".\nSome sets are \"infinite\"; these sets have more than formula_30 elements where formula_30 is any integer that can be specified. (No matter how large the specified integer formula_30 is, such as formula_35, infinite sets have more than formula_30 elements.) For example, the set of natural numbers, denotable by formula_37, has infinitely many elements, and we cannot use any natural number to give its size. It might seem natural to divide the sets into different classes: put all the sets containing one element together; all the sets containing two elements together; ...; finally, put together all infinite sets and consider them as having the same size. This view works well for countably infinite sets and was the prevailing assumption before Georg Cantor's work. For example, there are infinitely many odd integers, infinitely many even integers, and also infinitely many integers overall. We can consider all these sets to have the same \"size\" because we can arrange things such that, for every integer, there is a distinct even integer:\nformula_38\nor, more generally, formula_39 (see picture). What we have done here is arrange the integers and the even integers into a \"one-to-one correspondence\" (or \"bijection\"), which is a function that maps between two sets such that each element of each set corresponds to a single element in the other set. This mathematical notion of \"size\", cardinality, is that two sets are of the same size if and only if there is a bijection between them. We call all sets that are in one-to-one correspondence with the integers \"countably infinite\" and say they have cardinality formula_3.\nGeorg Cantor showed that not all infinite sets are countably infinite. For example, the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers). The set of real numbers has a greater cardinality than the set of natural numbers and is said to be uncountable.\nFormal overview.\nBy definition, a set formula_1 is \"countable\" if there exists a bijection between formula_1 and a subset of the natural numbers formula_43. For example, define the correspondence\nformula_44\nSince every element of formula_45 is paired with \"precisely one\" element of formula_46, \"and\" vice versa, this defines a bijection, and shows that formula_1 is countable. Similarly we can show all finite sets to be countable.\nAs for the case of infinite sets, a set formula_1 is countably infinite if there is a bijection between formula_1 and all of formula_4. As examples, consider the sets formula_51, the set of positive integers, and formula_52, the set of even integers. We can show these sets are countably infinite by exhibiting a bijection to the natural numbers. This can be achieved using the assignments formula_53 and formula_54, so that\nformula_55\nEvery countably infinite set is countable, and every infinite countable set is countably infinite. Furthermore, any subset of the natural numbers is countable, and more generally:\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014A subset of a countable set is countable.\nThe set of all ordered pairs of natural numbers (the Cartesian product of two sets of natural numbers, formula_56) is countably infinite, as can be seen by following a path like the one in the picture: The resulting mapping proceeds as follows:\nformula_57\nThis mapping covers all such ordered pairs.\nThis form of triangular mapping recursively generalizes to formula_30-tuples of natural numbers, i.e., formula_59 where formula_23 and formula_30 are natural numbers, by repeatedly mapping the first two elements of an formula_30-tuple to a natural number. For example, formula_63 can be written as formula_64. Then formula_65 maps to 5 so formula_64 maps to formula_67, then formula_67 maps to 39. Since a different 2-tuple, that is a pair such as formula_69, maps to a different natural number, a difference between two n-tuples by a single element is enough to ensure the n-tuples being mapped to different natural numbers. So, an injection from the set of formula_30-tuples to the set of natural numbers formula_4 is proved. For the set of formula_30-tuples made by the Cartesian product of finitely many different sets, each element in each tuple has the correspondence to a natural number, so every tuple can be written in natural numbers then the same logic is applied to prove the theorem.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014 The Cartesian product of finitely many countable sets is countable.\nThe set of all integers formula_73 and the set of all rational numbers formula_74 may intuitively seem much bigger than formula_4. But looks can be deceiving. If a pair is treated as the numerator and denominator of a vulgar fraction (a fraction in the form of formula_76 where formula_77 and formula_78 are integers), then for every positive fraction, we can come up with a distinct natural number corresponding to it. This representation also includes the natural numbers, since every natural number formula_30 is also a fraction formula_80. So we can conclude that there are exactly as many positive rational numbers as there are positive integers. This is also true for all rational numbers, as can be seen below.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014formula_73 (the set of all integers) and formula_74 (the set of all rational numbers) are countable.\nSometimes more than one mapping is useful: if a set formula_85 to be shown as countable is one-to-one mapped (injection) to another set formula_86, then formula_85 is proved as countable if formula_86 is one-to-one mapped to the set of natural numbers. For example, the set of positive rational numbers can easily be one-to-one mapped to the set of natural number pairs (2-tuples) because formula_89 maps to formula_90. Since the set of natural number pairs is one-to-one mapped (actually one-to-one correspondence or bijection) to the set of natural numbers as shown above, the positive rational number set is proved as countable.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014 Any finite union of countable sets is countable.\nWith the foresight of knowing that there are uncountable sets, we can wonder whether or not this last result can be pushed any further. The answer is \"yes\" and \"no\", we can extend it, but we need to assume a new axiom to do so.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014 (Assuming the axiom of countable choice) The union of countably many countable sets is countable.\nFor example, given countable sets formula_91, we first assign each element of each set a tuple, then we assign each tuple an index using a variant of the triangular enumeration we saw above:\nformula_92\nWe need the axiom of countable choice to index \"all\" the sets formula_91 simultaneously.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014 The set of all finite-length sequences of natural numbers is countable.\nThis set is the union of the length-1 sequences, the length-2 sequences, the length-3 sequences, and so on, each of which is a countable set (finite Cartesian product). Thus the set is a countable union of countable sets, which is countable by the previous theorem.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014 The set of all finite subsets of the natural numbers is countable.\nThe elements of any finite subset can be ordered into a finite sequence. There are only countably many finite sequences, so also there are only countably many finite subsets.\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nTheorem\u2014Let formula_1 and formula_95 be sets.\nThese follow from the definitions of countable set as injective / surjective functions.\nCantor's theorem asserts that if formula_85 is a set and formula_103 is its power set, i.e. the set of all subsets of formula_85, then there is no surjective function from formula_85 to formula_103. A proof is given in the article Cantor's theorem. As an immediate consequence of this and the Basic Theorem above we have:\n&lt;templatestyles src=\"Math_theorem/styles.css\" /&gt;\nProposition\u2014 The set formula_107 is not countable; i.e. it is uncountable.\nFor an elaboration of this result see Cantor's diagonal argument.\nThe set of real numbers is uncountable, and so is the set of all infinite sequences of natural numbers.\nMinimal model of set theory is countable.\nIf there is a set that is a standard model (see inner model) of ZFC set theory, then there is a minimal standard model (see Constructible universe). The L\u00f6wenheim\u2013Skolem theorem can be used to show that this minimal model is countable. The fact that the notion of \"uncountability\" makes sense even in this model, and in particular that this model \"M\" contains elements that are:\nwas seen as paradoxical in the early days of set theory; see Skolem's paradox for more.\nThe minimal standard model includes all the algebraic numbers and all effectively computable transcendental numbers, as well as many other kinds of numbers.\nTotal orders.\nCountable sets can be totally ordered in various ways, for example:\nIn both examples of well orders here, any subset has a \"least element\"; and in both examples of non-well orders, \"some\" subsets do not have a \"least element\".\nThis is the key definition that determines whether a total order is also a well order.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6030", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=6030", "title": "Chronic Fatigue Syndrome/The name", "text": ""}
{"id": "6031", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=6031", "title": "Chronic Fatigue Syndrome/Long term course", "text": ""}
{"id": "6032", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=6032", "title": "Chronic Fatigue Syndrome/Day to day patterns", "text": ""}
{"id": "6033", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=6033", "title": "Chronic Fatigue Syndrome/Demographics", "text": ""}
{"id": "6034", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6034", "title": "Cahn\u2013Ingold\u2013Prelog priority rules", "text": "Naming convention for stereoisomers of molecules\nIn organic chemistry, the Cahn\u2013Ingold\u2013Prelog (CIP) sequence rules (also the CIP priority convention; named after Robert Sidney Cahn, Christopher Kelk Ingold, and Vladimir Prelog) are a standard process to completely and unequivocally name a stereoisomer of a molecule. The purpose of the CIP system is to assign an to each stereocenter and an \"E\" or \"Z\" descriptor to each double bond so that the configuration of the entire molecule can be specified uniquely by including the descriptors in its systematic name. A molecule may contain any number of stereocenters and any number of double bonds, and each usually gives rise to two possible isomers. A molecule with an integer n describing the number of stereocenters will usually have 2\"n\" stereoisomers, and 2\"n\"\u22121 diastereomers each having an associated pair of enantiomers. The CIP sequence rules contribute to the precise naming of every stereoisomer of every organic molecule with all atoms of ligancy of fewer than 4 (but including ligancy of 6 as well, this term referring to the \"number of neighboring atoms\" bonded to a center).\nThe key article setting out the CIP sequence rules was published in 1966, and was followed by further refinements, before it was incorporated into the rules of the International Union of Pure and Applied Chemistry (IUPAC), the official body that defines organic nomenclature, in 1974. The rules have since been revised, most recently in 2013, as part of the IUPAC book Nomenclature of Organic Chemistry. The IUPAC presentation of the rules constitute the official, formal standard for their use, and it notes that \"the method has been developed to cover all compounds with ligancy up to 4... and... [extended to the case of] ligancy 6... [as well as] for all configurations and conformations of such compounds.\" Nevertheless, though the IUPAC documentation presents a thorough introduction, it includes the caution that \"it is essential to study the original papers, especially the 1966 paper, before using the sequence rule for other than fairly simple cases.\"\nA recent paper argues for changes to some of the rules (sequence rules 1b and 2) to address certain molecules for which the correct descriptors were unclear. However, a different problem remains: in rare cases, two different stereoisomers of the same molecule can have the same CIP descriptors, so the CIP system may not be able to unambiguously name a stereoisomer, and other systems may be preferable.27\nSteps for naming.\nThe steps for naming molecules using the CIP system are often presented as:\nAssignment of priorities.\n and \"E\"/\"Z\" descriptors are assigned by using a system for ranking priority of the groups attached to each stereocenter. This procedure, often known as \"the sequence rules\", is the heart of the CIP system. The overview in this section omits some rules that are needed only in rare cases.\nIsotopes.\nIf two groups differ only in isotopes, then the larger atomic mass is used to set the priority.\nDouble and triple bonds.\nIf an atom, A, is double-bonded to another atom, then atom A should be treated as though it is \"connected to the same atom twice\". An atom that is double-bonded has a higher priority than an atom that is single bonded. When dealing with double bonded priority groups, one is allowed to visit the same atom twice as one creates an arc.\nWhen B is replaced with a list of attached atoms, A itself, but not its \"phantom\", is excluded in accordance with the general principle of not doubling back along a bond that has just been followed. A triple bond is handled the same way except that A and B are each connected to two phantom atoms of the other.\nGeometrical isomers.\nIf two substituents on an atom are geometric isomers of each other, the \"Z\"-isomer has higher priority than the \"E\"-isomer. A stereoisomer that contains two higher priority groups on the same face of the double bond (\"cis\") is classified as \"Z.\" The stereoisomer with two higher priority groups on opposite sides of a carbon-carbon double bond (\"trans\") is classified as \"E.\"\nCyclic molecules.\nTo handle a molecule containing one or more cycles, one must first expand it into a tree (called a hierarchical digraph) by traversing bonds in all possible paths starting at the stereocenter. When the traversal encounters an atom through which the current path has already passed, a phantom atom is generated in order to keep the tree finite. A single atom of the original molecule may appear in many places (some as phantoms, some not) in the tree.572\nAssigning descriptors.\nStereocenters: \"R\"/\"S\".\nA chiral sp3 hybridized isomer contains four different substituents. All four substituents are assigned priorities based on its atomic numbers. After the substituents of a stereocenter have been assigned their priorities, the molecule is oriented in space so that the group with the lowest priority is pointed away from the observer. If the substituents are numbered from 1 (highest priority) to 4 (lowest priority), then the sense of rotation of a curve passing through 1, 2 and 3 distinguishes the stereoisomers. In a configurational isomer, the lowest priority group (most times hydrogen) is positioned behind the plane or the hatched bond going away from the reader. The highest priority group will have an arc drawn connecting to the rest of the groups, finishing at the group of third priority. An arc drawn clockwise, has the \"rectus\" (\"R\") assignment. An arc drawn counterclockwise, has the \"sinister\" (\"S\") assignment. The names are derived from the Latin for 'right' and 'left', respectively. When naming an organic isomer, the abbreviation for either rectus or sinister assignment is placed in front of the name in parentheses. For example, 3-methyl-1-pentene with a rectus assignment is formatted as (\"R\")-3-methyl-1-pentene. \nA practical method of determining whether an enantiomer is \"R\" or \"S\" is by using the right-hand rule: one wraps the molecule with the fingers in the direction 1 \u2192 2 \u2192 3. If the thumb points in the direction of the fourth substituent, the enantiomer is \"R\"; otherwise, it is \"S\".\nIt is possible in rare cases that two substituents on an atom differ only in their absolute configuration (\"R\" or \"S\"). If the relative priorities of these substituents need to be established, \"R\" takes priority over \"S\". When this happens, the descriptor of the stereocenter is a lowercase letter (\"r\" or \"s\") instead of the uppercase letter normally used.\nDouble bonds: \"E\"/\"Z\".\nFor double bonded molecules, Cahn\u2013Ingold\u2013Prelog priority rules (CIP rules) are followed to determine the priority of substituents of the double bond. If both of the high priority groups are on the same side of the double bond (\"cis\" configuration), then the stereoisomer is assigned the configuration \"Z\" (\"zusammen,\" German word meaning \"together\"). If the high priority groups are on opposite sides of the double bond (\"trans\" configuration), then the stereoisomer is assigned the configuration \"E\" (\"entgegen\", German word meaning \"opposed\")\nCoordination compounds.\nIn some cases where stereogenic centers are formed, the configuration must be specified. Without the presence of a non-covalent interaction, a compound is achiral. Some professionals have proposed a new rule to account for this. This rule states that \"non-covalent interactions have a fictitious number between 0 and 1\" when assigning priority. Compounds in which this occurs are referred to as coordination compounds.\nSpiro compounds.\nSome spiro compounds, for example the SDP ligands ((\"R\")- and (\"S\")-7,7'-bis(diphenylphosphaneyl)-2,2',3,3'-tetrahydro-1,1'-spirobi[indene]), represent chiral, C2-symmetrical molecules where the rings lie approximately at right angles to each other and each molecule cannot be superposed on its mirror image. The spiro carbon, C, is a stereogenic centre, and priority can be assigned a&gt;a\u2032&gt;b&gt;b\u2032, in which one ring (both give the same answer) contains atoms a and b adjacent to the spiro carbon, and the other contains a\u2032 and b\u2032. The configuration at C may then be assigned as for any other stereocentre.\nExamples.\nThe following are examples of application of the nomenclature.\nDescribing multiple centers.\nIf a compound has more than one chiral stereocenter, each center is denoted by either \"R\" or \"S\". For example, ephedrine exists in (1\"R\",2\"S\") and (1\"S\",2\"R\") stereoisomers, which are distinct mirror-image forms of each other, making them enantiomers. This compound also exists as the two enantiomers written (1\"R\",2\"R\") and (1\"S\",2\"S\"), which are named pseudoephedrine rather than ephedrine. All four of these isomers are named 2-methylamino-1-phenyl-1-propanol in systematic nomenclature. However, ephedrine and pseudoephedrine are diastereomers, or stereoisomers that are not enantiomers because they are not related as mirror-image copies. Pseudoephedrine and ephedrine are given different names because, as diastereomers, they have different chemical properties, even for racemic mixtures of each.\nMore generally, for any pair of enantiomers, all of the descriptors are opposite: (\"R\",\"R\") and (\"S\",\"S\") are enantiomers, as are (\"R\",\"S\") and (\"S\",\"R\"). Diastereomers have at least one descriptor in common; for example (\"R\",\"S\") and (\"R\",\"R\") are diastereomers, as are (\"S\",\"R\") and (\"S\",\"S\"). This holds true also for compounds having more than two stereocenters: if two stereoisomers have at least one descriptor in common, they are diastereomers. If all the descriptors are opposite, they are enantiomers.\nA meso compound is an achiral molecule, despite having two or more stereogenic centers. A meso compound is superposable on its mirror image, therefore it reduces the number of stereoisomers predicted by the 2n rule. This occurs because the molecule obtains a plane of symmetry that causes the molecule to rotate around the central carbon\u2013carbon bond. One example is meso-tartaric acid, in which (\"R\",\"S\") is the same as the (\"S\",\"R\") form. In meso compounds the \"R\" and \"S\" stereocenters occur in symmetrically positioned pairs.\nRelative configuration.\nThe relative configuration of two stereoisomers may be denoted by the descriptors \"R\" and \"S\" with an asterisk (*). (\"R\"*,\"R\"*) means two centers having identical configurations, (\"R\",\"R\") or (\"S\",\"S\"); (\"R\"*,\"S\"*) means two centers having opposite configurations, (\"R\",\"S\") or (\"S\",\"R\"). To begin, the lowest-numbered (according to IUPAC systematic numbering) stereogenic center is given the \"R\"* descriptor.\nTo designate two anomers the relative stereodescriptors alpha (\u03b1) and beta (\u03b2) are used. In the \u03b1 anomer the \"anomeric carbon atom\" and the \"reference atom\" do have opposite configurations (\"R\",\"S\") or (\"S\",\"R\"), whereas in the \u03b2 anomer they are the same (\"R\",\"R\") or (\"S\",\"S\").\nFaces.\nStereochemistry also plays a role assigning \"faces\" to trigonal molecules such as ketones. A nucleophile in a nucleophilic addition can approach the carbonyl group from two opposite sides or faces. When an achiral nucleophile attacks acetone, both faces are identical and there is only one reaction product. When the nucleophile attacks butanone, the faces are not identical (\"enantiotopic\") and a racemic product results. When the nucleophile is a chiral molecule diastereoisomers are formed. When one face of a molecule is shielded by substituents or geometric constraints compared to the other face the faces are called diastereotopic. The same rules that determine the stereochemistry of a stereocenter (\"R\" or \"S\") also apply when assigning the face of a molecular group. The faces are then called the Re\"-face and Si\"-face. In the example displayed on the right, the compound acetophenone is viewed from the \"Re\"-face. Hydride addition as in a reduction process from this side will form the (\"S\")-enantiomer and attack from the opposite \"Si\"-face will give the (\"R\")-enantiomer. However, one should note that adding a chemical group to the prochiral center from the \"Re\"-face will not always lead to an (\"S\")-stereocenter, as the priority of the chemical group has to be taken into account. That is, the absolute stereochemistry of the product is determined on its own and not by considering which face it was attacked from. In the above-mentioned example, if chloride (\"Z\" = 17) were added to the prochiral center from the \"Re\"-face, this would result in an (\"R\")-enantiomer.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6035", "revid": "3138265", "url": "https://en.wikipedia.org/wiki?curid=6035", "title": "Celibacy", "text": "State of voluntary sexual abstinence\nCelibacy (from Latin \"caelibatus\") is the state of voluntarily being unmarried, sexually abstinent, or both. It is often in association with the role of a religious official or devotee. In its narrow sense, the term \"celibacy\" is applied only to those for whom the unmarried state is the result of a sacred vow, act of renunciation, or religious conviction. In a wider sense, it is commonly understood to only mean abstinence from sexual activity.\nCelibacy has existed in one form or another throughout history, in virtually all the major religions of the world, and views on it have varied. The Hindu concept of brahmacharya encourages celibacy during adolescence, to allow one to focus on learning, and in later years, as a way of attaining spiritual liberation. Jainism, on the other hand, preached complete celibacy even for young monks and considered celibacy to be an essential behavior to attain moksha. Buddhism is similar to Jainism in this respect. There were, however, significant cultural differences in the various areas where Buddhism spread, which affected the local attitudes toward celibacy. A somewhat similar situation existed in Japan, where the Shinto tradition also opposed celibacy. In most native African and Native American religious traditions, celibacy has been viewed negatively as well, although there were exceptions like periodic celibacy practiced by some Mesoamerican warriors.\nThe Romans viewed celibacy as an aberration and legislated fiscal penalties against it, with the exception of the Vestal Virgins, who took a 30-year vow of chastity in order to devote themselves to the study and correct observance of state rituals. In Christianity, celibacy means the promise to live either virginal or celibate in the future. Such a vow of celibacy has been normal for some centuries for Catholic priests, Catholic and Eastern Orthodox monks, and nuns. In addition, a promise or vow of celibacy may be made in the Anglican Communion and some Protestant churches or communities, such as the Shakers; for members of religious orders and religious congregations; and for hermits, consecrated virgins, and deaconesses. Judaism and Islam have denounced celibacy, as both religions emphasize marriage and family life; however, the priests of the Essenes, a Jewish sect during the Second Temple period, practised celibacy. It is reported in one famous \"hadith\" that the Islamic prophet Muhammad explicitly rejected the practice of celibacy.\nEtymology.\nThe English word \"celibacy\" derives from the Latin \"caelibatus\", \"state of being unmarried\", from Latin , meaning \"unmarried\". This word derives from two Proto-Indo-European stems, \"alone\" and \"living\".\nAbstinence and celibacy.\nThe words \"abstinence\" and \"celibacy\" are often used interchangeably, but are not necessarily the same thing. Sexual abstinence, also known as \"continence\", is abstaining from some or all aspects of sexual activity, often for some limited period of time, while celibacy may be defined as a voluntary religious vow not to marry or engage in sexual activity. Asexuality is commonly conflated with celibacy and sexual abstinence, but it is considered distinct from the two, as celibacy and sexual abstinence are behavioral and those who use those terms for themselves are generally motivated by factors such as an individual's personal or religious beliefs.\nA. W. Richard Sipe, while focusing on the topic of celibacy in Catholicism, states that \"the most commonly assumed definition of \"celibate\" is simply an unmarried or single person, and celibacy is perceived as synonymous with sexual abstinence or restraint.\" Sipe adds that even in the relatively uniform milieu of Catholic priests in the United States there seems to be \"simply no clear operational definition of celibacy\". Elizabeth Abbott commented on the terminology in her \"A History of Celibacy\" (2001) writing that she \"drafted a definition of celibacy that discarded the rigidly pedantic and unhelpful distinctions between celibacy, chastity, and virginity\".\nThe concept of \"new\" celibacy was introduced by Gabrielle Brown in her 1980 book \"The New Celibacy\". In a revised version (1989) of her book, she claims abstinence to be \"a response on the outside to what's going on, and celibacy is a response from the inside\". According to her definition, celibacy (even short-term celibacy that is pursued for non-religious reasons) is much more than not having sex. It is more intentional than abstinence, and its goal is personal growth and empowerment. Although Brown repeatedly states that celibacy is a matter of choice, she clearly suggests that those who do not choose this route are somehow missing out. This new perspective on celibacy is echoed by several authors, including Elizabeth Abbott, Wendy Keller, and Wendy Shalit.\nBuddhism.\nThe rule of celibacy in the Buddhist religion, whether Mahayana or Theravada, has a long history. Celibacy was advocated as an ideal rule of life for all monks and nuns by Gautama Buddha, except in Japan where it is not strictly followed due to historical and political developments following the Meiji Restoration. In Japan, celibacy was an ideal among Buddhist clerics for hundreds of years. But violations of clerical celibacy were so common for so long that finally, in 1872, state laws made marriage legal for Buddhist clerics. Subsequently, ninety percent of Buddhist monks/clerics married. An example is Higashifushimi Kunihide, a prominent Buddhist priest of Japanese royal ancestry who was married and a father whilst serving as a monk for most of his lifetime.\nGautama, later known as the Buddha, is known for his renunciation of his wife, Princess Yasodhar\u0101, and son, Rahula. In order to pursue an ascetic life, he needed to renounce aspects of the impermanent world, including his wife and son. Later on both his wife and son joined the ascetic community and are mentioned in the Buddhist texts to have become enlightened.\nChristianity.\nThere is no commandment in the New Testament that Jesus Christ's disciples have to live in celibacy. However, it is a general view that Christ himself lived a life of perfect chastity; thus, \"Voluntary chastity is the imitation of him who was the virgin Son of a virgin Mother\". One of his invocations is \"King of virgins and lover of stainless chastity\" \"(Rex virginum, amator castitatis)\".\nFurthermore, Christ, when his disciples suggest it is \"better not to marry,\" stated \"Not everyone can accept this word, but only those to whom it has been given. For there are eunuchs who have been so from birth, and there are eunuchs who have been made eunuchs by others, and there are eunuchs who have made themselves eunuchs for the sake of the kingdom of heaven. Let anyone accept this who can\" (Matthew 19:10-12, NRSV). While eunuchs were not generally celibate, over subsequent centuries this statement has come to be interpreted as referring to celibacy.\nPaul the Apostle emphasized the importance of overcoming the desires of the flesh and saw the state of celibacy being superior to that of marriage. Paul made parallels between the relations between spouses and God's relationship with the church. \"Husbands, love your wives even as Christ loved the church. Husbands should love their wives as their own bodies\" (Ephesians 5:25\u201328). Paul himself was celibate and said that his wish was \"that all of you were as I am\" (1 Corinthians 7:7). In fact, this entire chapter endorses celibacy while also clarifying that marriage is also acceptable.\nThe early Christians lived in the belief that the end of the world would soon come upon them, and saw no point in planning new families and having children. According to Chadwick, this was why Paul encouraged both celibate and marital lifestyles among the members of the Corinthian congregation, regarding celibacy as the preferable of the two.\nIn the counsels of perfection (evangelical counsels), which include chastity alongside poverty and obedience, Jesus is said to have \"[given] the rule of the higher life, founded upon his own most perfect life\", for those who seek \"the highest perfection\" and feel \"called to follow Christ in this way\"\u2014i.e. through such \"exceptional sacrifices\".\nA number of early Christian martyrs were women or girls who had given themselves to Christ in perpetual virginity, such as Saint Agnes and Saint Lucy. According to most Christian thought, the first sacred virgin was Mary, the mother of Jesus, who was consecrated by the Holy Spirit during the Annunciation. Tradition also has it that the Apostle Matthew consecrated virgins. In the Catholic Church and the Orthodox churches, a consecrated virgin is a woman who has been consecrated by the church to a life of perpetual virginity in the service of the church.\nDesert Fathers.\nThe Desert Fathers were Christian hermits and ascetics who had a major influence on the development of Christianity and celibacy. Paul of Thebes is often credited with being the first hermit or anchorite to go to the desert, but it was Anthony the Great who launched the movement that became the Desert Fathers. Sometime around AD 270, Anthony heard a Sunday sermon stating that perfection could be achieved by selling all of one's possessions, giving the proceeds to the poor, and following Christ (Matthew 19:21). He followed the advice and made the further step of moving deep into the desert to seek complete solitude.\nOver time, the model of Anthony and other hermits attracted many followers, who lived alone in the desert or in small groups. They chose a life of extreme asceticism, renouncing all the pleasures of the senses, rich food, baths, rest, and anything that made them comfortable. Thousands joined them in the desert, mostly men but also a handful of women. Religious seekers also began going to the desert seeking advice and counsel from the early Desert Fathers. By the time of Anthony's death, there were so many men and women living in the desert in celibacy that it was described as \"a city\" by Anthony's biographer.\nThe first Conciliar document on clerical celibacy of the Western Church (Synod of Elvira, c.\u2009\u00a0305 can. xxxiii) states that the discipline of celibacy is to refrain from the use of marriage, i.e. refrain from having carnal contact with one's spouse.\nAccording to the later St. Jerome (c.\u2009347\u00a0\u2013 420), celibacy is a moral virtue, consisting of living in the flesh, but outside the flesh, and so being not corrupted by it (\"vivere in carne praeter carnem\"). Celibacy excludes not only libidinous acts, but also sinful thoughts or desires of the flesh. Jerome referred to marriage prohibition for priests when he claimed in \"Against Jovinianus\" that Peter and the other apostles had been married before they were called, but subsequently gave up their marital relations.\nIn the Catholic, Orthodox and Oriental Orthodox traditions, bishops are required to be celibate. In the Eastern Catholic and Orthodox traditions, priests and deacons are allowed to be married, yet have to remain celibate if they are unmarried at the time of ordination.\nAugustinian view.\nIn the early Church, higher clerics lived in marriages. Augustine taught that the original sin of Adam and Eve was either an act of foolishness \"(insipientia)\" followed by pride and disobedience to God, or else inspired by pride. The first couple disobeyed God, who had told them not to eat of the tree of the knowledge of good and evil (Gen 2:17). The tree was a symbol of the order of creation. Self-centeredness made Adam and Eve eat of it, thus failing to acknowledge and respect the world as it was created by God, with its hierarchy of beings and values. They would not have fallen into pride and lack of wisdom, if Satan had not sown into their senses \"the root of evil\" \"(radix mali)\". Their nature was wounded by concupiscence or libido, which affected human intelligence and will, as well as affections and desires, including sexual desire.\nThe sin of Adam is inherited by all human beings. Already in his pre-Pelagian writings, Augustine taught that original sin was transmitted by concupiscence, which he regarded as the passion of both soul and body, making humanity a \"massa damnata\" (mass of perdition, condemned crowd) and much enfeebling, though not destroying, the freedom of the will.\nIn the early 3rd century, the Canons of the Apostolic Constitutions decreed that only lower clerics might still marry after their ordination, but marriage of bishops, priests, and deacons were not allowed.\nAfter Augustine.\nOne explanation for the origin of obligatory celibacy is that it is based on the writings of Saint Paul, who wrote of the advantages of celibacy allowed a man in serving the Lord. Celibacy was popularised by the early Christian theologians like Saint Augustine of Hippo and Origen. Another possible explanation for the origins of obligatory celibacy revolves around more practical reason, \"the need to avoid claims on church property by priests' offspring\". It remains a matter of Canon Law (and often a criterion for certain religious orders, especially Franciscans) that priests may not own land and therefore cannot pass it on to legitimate or illegitimate children. The land belongs to the Church through the local diocese as administered by the Local Ordinary (usually a bishop), who is often an \"ex officio\" corporation sole. Celibacy is viewed differently by the Catholic Church and the various Protestant communities. It includes clerical celibacy, celibacy of the consecrated life and voluntary celibacy.\nThe Protestant Reformation rejected celibate life and sexual continence for preachers. Protestant celibate communities, including religious orders exist, especially from Lutheran and Anglican backgrounds. The Daughters of Mary is a Lutheran religious order of nuns who have taken vows of chastity, poverty, and obedience. A few minor Christian sects advocate celibacy as a better way of life. These groups included the Shakers, the Harmony Society and the Ephrata Cloister.\nMany evangelicals prefer the term \"abstinence\" to \"celibacy\". Assuming everyone will marry, they focus their discussion on refraining from premarital sex and focusing on the joys of a future marriage. But some evangelicals, particularly older singles, desire a positive message of celibacy that moves beyond the \"wait until marriage\" message of abstinence campaigns. They seek a new understanding of celibacy that is focused on God rather than a future marriage or a lifelong vow to the Church.\nThere are also many Pentecostal churches which practice celibate ministry. For instance, the full-time ministers of the Pentecostal Mission are celibate and generally single. Married couples who enter full-time ministry may become celibate and could be sent to different locations.\nCatholic Church.\nDuring the first three or four centuries, no law was promulgated prohibiting clerical marriage. Celibacy was a matter of choice for bishops, priests, and deacons.\nStatutes forbidding clergy from having wives were written beginning with the Council of Elvira (306) but these early statutes were not universal and were often defied by clerics and then retracted by hierarchy. The Synod of Gangra (345) condemned a false asceticism whereby worshipers boycotted celebrations presided over by married clergy. The Apostolic Constitutions (c.\u2009400) excommunicated a priest or bishop who left his wife \"under the pretense of piety\" (Mansi, 1:51).\n\"A famous letter of Synesius of Cyrene (c.\u2009414) is evidence both for the respecting of personal decision in the matter and for contemporary appreciation of celibacy. For priests and deacons clerical marriage continued to be in vogue\".\n\"The Second Lateran Council (1139) seems to have enacted the first written law making sacred orders a direct impediment to marriage for the universal Church.\" Celibacy was first required of some clerics in 1123 at the First Lateran Council. Because clerics resisted it, the celibacy mandate was restated at the Second Lateran Council (1139) and the Council of Trent (1545\u201364). In places, coercion and enslavement of clerical wives and children was apparently involved in the enforcement of the law. \"The earliest decree in which the children [of clerics] were declared to be slaves and never to be enfranchised [freed] seems to have been a canon of the Synod of Pavia in 1018. Similar penalties were promulgated against wives and concubines (see the Synod of Melfi, 1189 can. xii), who by the very fact of their unlawful connexion with a subdeacon or clerk of higher rank became liable to be seized by the over-lord\".\nIn the Roman Catholic Church, the Twelve Apostles are considered to have been the first priests and bishops of the Church. Some say the call to be eunuchs for the sake of Heaven in Matthew 19 was a call to be sexually continent and that this developed into celibacy for priests as the successors of the apostles. Others see the call to be sexually continent in Matthew 19 to be a caution for men who were too readily divorcing and remarrying.\nThe view of the Church is that celibacy is a reflection of life in Heaven, a source of detachment from the material world which aids in one's relationship with God. Celibacy is designed to \"consecrate themselves with undivided heart to the Lord and to \"the affairs of the Lord, they give themselves entirely to God and to men. It is a sign of this new life to the service of which the Church's minister is consecrated; accepted with a joyous heart celibacy radiantly proclaims the Reign of God.\" In contrast, Saint Peter, whom the Church considers its first Pope, was married given that he had a mother-in-law whom Christ healed (Matthew 8). But some argue that Peter was a widower, due to the fact that this passage does not mention his wife, and that his mother-in-law is the one who serves Christ and the apostles after she is healed. Furthermore, Peter himself states: \"Then Peter spoke up, 'We have left everything to follow you!' 'Truly I tell you', Jesus replied, 'no one who has left home or brothers or sisters or mother or father or children or fields for me and the gospel will fail to receive a hundred times as much'\" (Mark 10,28\u201330).\nUsually, only celibate men are ordained as priests in the Latin Church. Married clergy who have converted from other Christian denominations can be ordained Roman Catholic priests without becoming celibate. Priestly celibacy is not \"doctrine\" of the Church (such as the belief in the Assumption of Mary) but a matter of discipline, like the use of the vernacular (local) language in Mass or Lenten fasting and abstinence. As such, it can theoretically change at any time though it still must be obeyed by Catholics until the change were to take place. The Eastern Catholic Churches ordain both celibate and married men. However, in both the East and the West, bishops are chosen from among those who are celibate. In Ireland, several priests have fathered children, the two most prominent being bishop Eamonn Casey and Michael Cleary.\nThe classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. When discerning the population of Christendom in medieval Europe during the Middle Ages, Will Durant, referring to Plato's ideal community, stated on the \"oratores\" (clergy):\n\"The clergy, like Plato's guardians, were placed in authority not by the suffrages of the people, but by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and (perhaps it should be added) by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [AD\u00a0800 onwards], the clergy were as free from family cares as even Plato could desire; and in some cases it would seem they enjoyed no little of the reproductive freedom accorded to the guardians. Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them and to the readiness of these sinners to bare their lives in the confessional.\"\nWith respect to clerical celibacy, Richard P. O'Brien stated in 1995, that in his opinion, \"greater understanding of human psychology has led to questions regarding the impact of celibacy on the human development of the clergy. The realization that many non-European countries view celibacy negatively has prompted questions concerning the value of retaining celibacy as an absolute and universal requirement for ordained ministry in the Roman Catholic Church\".\nCelibate homosexual Christians.\nSome homosexual Christians choose to be celibate following their denomination's teachings on homosexuality.\nIn 2014, the American Association of Christian Counselors amended its code of ethics to eliminate the promotion of conversion therapy for homosexuals and encouraged them to be celibate instead.\nHinduism.\nIn Hinduism, celibacy is usually associated with the \"sadhus\" (\"holy men\"), ascetics who withdraw from society and renounce all worldly ties. Celibacy, termed \"brahmacharya\" in Vedic scripture, is the fourth of the \"yamas\" and the word literally translated means \"dedicated to the Divinity of Life\". The word is often used in yogic practice to refer to celibacy or denying pleasure, but this is only a small part of what \"brahmacharya\" represents. The purpose of practicing \"brahmacharya\" is to keep a person focused on the purpose in life, the things that instill a feeling of peace and contentment. It is also used to cultivate occult powers and many supernatural feats, called siddhi.\nIn the religious movement of Brahma Kumaris, celibacy is also promoted for peace and to defeat power of lust.\nIslam.\nIslamic attitudes toward celibacy have been complex, Muhammad denounced it, however some Sufi orders embrace it. Islam does not promote celibacy; rather it condemns premarital sex and extramarital sex. In fact, according to Islam, marriage enables one to attain the highest form of righteousness within this sacred spiritual bond but the Qur'an does not state it as an obligation. The Qur'an () states, \"But the Monasticism which they (who followed Jesus) invented for themselves, We did not prescribe for them but only to please God therewith, but that they did not observe it with the right observance.\" Therefore, religion is clearly not a reason to stay unmarried although people are allowed to live their lives however they are comfortable; but relationships and sex outside of marriage, let alone forced marriage, is definitely a sin, \"Oh you who believe! You are forbidden to inherit women against their will\" (). In addition, marriage partners can be distractions from practicing religion at the same time, \"Your mates and children are only a trial for you\" () however that still does not mean Islam does not encourage people who have sexual desires and are willing to marry. Anyone who does not (intend to) get married in this life can always do it in the Hereafter instead.\nCelibacy appears as a peculiarity among some Sufis.\nCelibacy was practiced by women saints in Sufism. Celibacy was debated along with women's roles in Sufism in medieval times.\nCelibacy, poverty, meditation, and mysticism within an ascetic context along with worship centered around saints' tombs were promoted by the Qadiri Sufi order among Hui Muslims in China. In China, unlike other Muslim sects, the leaders (Shaikhs) of the Qadiriyya Sufi order are celibate. Unlike other Sufi orders in China, the leadership within the order is not a hereditary position, rather, one of the disciples of the celibate Shaikh is chosen by the Shaikh to succeed him. The 92-year-old celibate Shaikh Yang Shijun was the leader of the Qadiriya order in China as of 1998.\nCelibacy is practiced by Haydariya Sufi dervishes.\nZoroastrianism.\nZoroastrian text Videvdad (4:47) praises a married man by saying:[T]he man who has a wife is far above him who lives in continence\nMeher Baba.\nThe spiritual teacher Meher Baba stated that \"[F]or the [spiritual] aspirant a life of strict celibacy is preferable to married life, if restraint comes to him easily without undue sense of self-repression. Such restraint is difficult for most persons and sometimes impossible, and for them married life is decidedly more helpful than a life of celibacy. For ordinary persons, married life is undoubtedly advisable unless they have a special aptitude for celibacy\". Baba also asserted that \"The value of celibacy lies in the habit of restraint and the sense of detachment and independence which it gives\" and that \"The aspirant must choose one of the two courses which are open to him. He must take to the life of celibacy or to the married life, and he must avoid at all costs a cheap compromise between the two. Promiscuity in sex gratification is bound to land the aspirant in a most pitiful and dangerous chaos of ungovernable lust.\"\nAncient Greece and Rome.\nIn Sparta and many other Greek cities, failure to marry was grounds for loss of citizenship, and could be prosecuted as a crime. Both Cicero and Dionysius of Halicarnassus stated that Roman law forbade celibacy. There are no records of such a prosecution, nor is the Roman punishment for refusing to marry known.\nPythagoreanism was the system of esoteric and metaphysical beliefs held by Pythagoras and his followers. Pythagorean thinking was dominated by a profoundly mystical view of the world. The Pythagorean code further restricted his members from eating meat, fish, and beans which they practised for religious, ethical and ascetic reasons, in particular the idea of metempsychosis \u2013 the transmigration of souls into the bodies of other animals.\n\"Pythagoras himself established a small community that set a premium on study, vegetarianism, and sexual restraint or abstinence. Later philosophers believed that celibacy would be conducive to the detachment and equilibrium required by the philosopher's calling.\"\nThe Balkans.\nThe tradition of sworn virgins developed out of the \"Kanuni i Lek\u00eb Dukagjinit\" (, or simply the \"Kanun\"). The \"Kanun\" is not a religious document\u00a0\u2013 many groups follow this code, including Roman Catholics, the Albanian Orthodox, and Muslims.\nWomen who become sworn virgins make a vow of celibacy, and are allowed to take on the social role of men: inheriting land, wearing male clothing, etc.\nPolitical contexts.\nDuring the May Fourth Movement in China, pledges of celibacy were a means through which participants resisted traditional marriage and devoted themselves to revolutionary causes.97\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6036", "revid": "525927", "url": "https://en.wikipedia.org/wiki?curid=6036", "title": "Coalition government", "text": "Governmental style in which political parties cooperate to form a government\nA coalition government, or coalition cabinet, is a government by political parties that enter into a power-sharing arrangement of the executive. Coalition governments usually occur when no single party has achieved an absolute majority after an election. A party not having majority is common under proportional representation, but not in nations with majoritarian electoral systems.\nThere are different forms of coalition governments, minority coalitions and surplus majority coalition governments. A surplus majority coalition government controls more than the absolute majority of seats in parliament necessary to have a majority in the government, whereas minority coalition governments do not hold the majority of legislative seats.\nA coalition government may also be created in a time of national difficulty or crisis (for example, during wartime or economic crisis) to give a government the high degree of perceived political legitimacy or collective identity, it can also play a role in diminishing internal political strife. In such times, parties have formed all-party coalitions (national unity governments, grand coalitions).\nIf a coalition collapses, the prime minister and cabinet may be ousted by a vote of no confidence, call snap elections, form a new majority coalition, or continue as a minority government.\nFormation of coalition governments.\nFor a coalition to come about the coalition partners need to compromise on their policy expectations. One coalition or probing partner must lose for the other one to win, to achieve a Nash equilibrium, which is necessary for a coalition to form. If the parties are not willing to compromise, the coalition will not come about.\nBefore parties form a coalition government, they formulate a coalition agreement, in which they state what policies they try to adapt in the legislative period.\nCoalition agreement.\nIn multi-party states, a coalition agreement is an agreement negotiated between the parties that form a coalition government. It codifies the most important shared goals and objectives of the cabinet. It is often written by the leaders of the parliamentary groups. Coalitions that have a written agreement are more productive than those that do not.\nIf an issue is discussed more deeply and in more detail in chamber than what appears in the coalition agreement, it indicates that the coalition parties do not share the same policy ideas. Hence, a more detailed written formulation of the issue helps parties in the coalition to limit 'agency loss' when the ministry overseeing that issue is managed by another coalition party.\nElectoral accountability.\nCoalition governments can also impact voting behavior by diminishing the clarity of responsibility.\nElectoral accountability is harder to achieve in coalition governments than in single party governments because there is no direct responsibility within the governing parties in the coalition.\nRetrospective voting has a huge influence on the outcome of an election. However, the risk of retrospective voting is a lot weaker with coalition governments than in single party governments. Within the coalition, the party with the head of state has the biggest risk of retrospective voting.\nGoverning cost.\nGoverning parties lose votes in the election after their legislative period, this is called \u201cthe governing cost\u201d. In comparison, a single- party government has a higher electoral cost, than a party that holds the office of the prime minister. Furthermore, the party that holds the office of prime minister suffer less electoral costs, then a junior coalition partner, when looking only on the electoral cost created by being in the coalition government.\nDistribution.\nCountries which often operate with coalition cabinets include: the Nordic countries, the Benelux countries, Australia, Austria, Brazil, Chile, Cyprus, East Timor, France, Germany, Greece, Guinea-Bissau, India, Indonesia, Ireland, Israel, Italy, Japan, Kenya, Kosovo, Latvia, Lebanon, Lesotho, Lithuania, Malaysia, Nepal, New Zealand, Pakistan, Thailand, Spain, Trinidad and Tobago, Turkey, and Ukraine. Switzerland has been ruled by a consensus government with a coalition of the four strongest parties in parliament since 1959, called the \"Magic Formula\". Between 2010 and 2015, the United Kingdom also operated a formal coalition between the Conservative and the Liberal Democrat parties, but this was unusual: the UK usually has a single-party majority government. Not every parliament forms a coalition government, for example the European Parliament.\nArmenia.\nArmenia became an independent state in 1991, following the collapse of the Soviet Union. Since then, many political parties were formed in it, who mainly work with each other to form coalition governments. The country was governed by the My Step Alliance coalition after successfully gaining a majority in the National Assembly of Armenia following the 2018 Armenian parliamentary election.\nAustralia.\nIn federal Australian politics, the conservative Liberal, National, Country Liberal and Liberal National parties are united in a coalition, known simply as the Coalition.\nWhile nominally two parties, the Coalition has become so stable, at least at the federal level, that in practice the lower house of Parliament has become a two-party system, with the Coalition and the Labor Party being the major parties. This coalition is also found in the states of New South Wales and Victoria. In South Australia and Western Australia the Liberal and National parties compete separately, while in the Northern Territory and Queensland the two parties have merged, forming the Country Liberal Party, in 1978, and the Liberal National Party, in 2008, respectively.\nCoalition governments involving the Labor Party and the Australian Greens have occurred at state and territory level, for example following the 2010 Tasmanian state election and the 2016 and 2020 Australian Capital Territory elections.\nBelgium.\nIn Belgium, a nation internally divided along linguistic lines (primarily between Dutch-speaking Flanders in the north and French-speaking Wallonia in the south, with Brussels also being by and large Francophone), each main political disposition (Social democracy, liberalism, right-wing populism, etc.) is, with the exception of the far-left Workers' Party of Belgium, split between Francophone and Dutch-speaking parties (e.g. the Dutch-speaking Vooruit and French-speaking Socialist Party being the two social-democratic parties). In the 2019 federal election, no party got more than 17% of the vote. Thus, forming a coalition government is an expected and necessary part of Belgian politics. In Belgium, coalition governments containing ministers from six or more parties are not uncommon; consequently, government formation can take an exceptionally long time. Between 2007 and 2011, Belgium operated under a caretaker government as no coalition could be formed.\nCanada.\nIn Canada, the Great Coalition was formed in 1864 by the Clear Grits, , and Liberal-Conservative Party. During the First World War, Prime Minister Robert Borden attempted to form a coalition with the opposition Liberals to broaden support for controversial conscription legislation. The Liberal Party refused the offer but some of their members did cross the floor and join the government. Although sometimes referred to as a coalition government, according to the definition above, it was not. It was disbanded after the end of the war.\nDuring the 2008\u201309 Canadian parliamentary dispute, two of Canada's opposition parties signed an agreement to form what would become the country's second federal coalition government since Canadian Confederation if the minority Conservative government was defeated on a vote of non-confidence, unseating Stephen Harper as prime minister. The agreement outlined a formal coalition consisting of two opposition parties, the Liberal Party and the New Democratic Party. The Bloc Qu\u00e9b\u00e9cois agreed to support the proposed coalition on confidence matters for 18 months. In the end, parliament was prorogued by the Governor General, and the coalition dispersed before parliament was reconvened.\nAccording to historian Christopher Moore, coalition governments in Canada became much less possible in 1919, when the leaders of parties were no longer chosen by elected MPs but instead began to be chosen by party members. Such a manner of leadership election had never been tried in any parliamentary system before. According to Moore, as long as that kind of leadership selection process remains in place and concentrates power in the hands of the leader, as opposed to backbenchers, then coalition governments will be very difficult to form. Moore shows that the diffusion of power within a party tends to also lead to a diffusion of power in the parliament in which that party operates, thereby making coalitions more likely.\nProvincial.\nSeveral coalition governments have been formed within provincial politics. As a result of the 1919 Ontario election, the United Farmers of Ontario and the Labour Party, together with three independent MLAs, formed a coalition that governed Ontario until 1923.\nIn British Columbia, the governing Liberals formed a coalition with the opposition Conservatives in order to prevent the surging, left-wing Cooperative Commonwealth Federation from taking power in the 1941 British Columbia general election. Liberal premier Duff Pattullo refused to form a coalition with the third-place Conservatives, so his party removed him. The Liberal\u2013Conservative coalition introduced a winner-take-all preferential voting system (the \"Alternative Vote\") in the hopes that their supporters would rank the other party as their second preference; however, this strategy backfired in the subsequent 1952 British Columbia general election where, to the surprise of many, the right-wing populist BC Social Credit Party won a minority. They were able to win a majority in the subsequent election as Liberal and Conservative supporters shifted their anti-CCF vote to Social Credit.\nManitoba has had more formal coalition governments than any other province. Following gains by the United Farmer's/Progressive movement elsewhere in the country, the United Farmers of Manitoba unexpectedly won the 1921 election. Like their counterparts in Ontario, they had not expected to win and did not have a leader. They asked John Bracken, a professor in animal husbandry, to become leader and premier. Bracken changed the party's name to the Progressive Party of Manitoba. During the Great Depression, Bracken survived at a time when other premiers were being defeated by forming a coalition government with the Manitoba Liberals (eventually, the two parties would merge into the , and decades later, the party would change its name to the Manitoba Liberal Party). In 1940, Bracken formed a wartime coalition government with almost every party in the Manitoba Legislature (the Conservatives, CCF, and Social Credit; however, the CCF broke with the coalition after a few years over policy differences). The only party not included was the small, communist Labor-Progressive Party, which had a handful of seats.\nIn Saskatchewan, NDP premier Roy Romanow formed a formal coalition with the Saskatchewan Liberals in 1999 after being reduced to a minority. After two years, the newly elected Liberal leader David Karwacki ordered the coalition be disbanded, the Liberal caucus disagreed with him and left the Liberals to run as New Democrats in the upcoming election. The Saskatchewan NDP was re-elected with a majority under its new leader Lorne Calvert, while the Saskatchewan Liberals lost their remaining seats and have not been competitive in the province since.\nDenmark.\nFrom the creation of the Folketing in 1849 through the introduction of proportional representation in 1918, there were only single-party governments in Denmark. Thorvald Stauning formed his second government and Denmark's first coalition government in 1929. Since then, the norm has been coalition governments, though there have been periods where single-party governments were frequent, such as the decade after the end of World War II, during the 1970s, and in the late 2010s. Every government from 1982 until the 2015 elections were coalitions. While Mette Frederiksen's first government only consisted of her own Social Democrats, her second government is a coalition of the Social Democrats, Venstre, and the Moderates.\nWhen the Social Democrats under Stauning won 46% of the votes in the 1935 election, this was the closest any party has gotten to winning an outright majority in parliament since 1918. One party has thus never held a majority alone, and even one-party governments have needed to have confidence agreements with at least one other party to govern. For example, though Frederiksen's first government only consisted of the Social Democrats, it also relied on the support of the Social Liberal Party, the Socialist People's Party, and the Red\u2013Green Alliance.\nFinland.\nIn Finland, no party has had an absolute majority in the parliament since independence, and multi-party coalitions have been the norm. Finland experienced its most stable government (Lipponen I and II) since independence with a five-party governing coalition, a so-called \"rainbow government\". The Lipponen cabinets set the stability record and were unusual in the respect that both the centre-left (SDP) and radical left-wing (Left Alliance) parties sat in the government with the major centre-right party (National Coalition). The Katainen cabinet was also a rainbow coalition of a total of five parties.\nGermany.\nIn Germany, coalition governments are the norm, as it is rare for any single party to win a majority in parliament. The German political system makes extensive use of the constructive vote of no confidence, which requires governments to control an absolute majority of seats. Every government since the foundation of the Federal Republic in 1949 has involved at least two political parties. Typically, governments involve one of the two major parties forming a coalition with a smaller party. For example, from 1982 to 1998, the country was governed by a coalition of the CDU/CSU with the minor Free Democratic Party (FDP); from 1998 to 2005, a coalition of the Social Democratic Party of Germany (SPD) and the minor Greens held power. The CDU/CSU comprises an alliance of the Christian Democratic Union of Germany and Christian Social Union in Bavaria, described as \"sister parties\" which form a joint parliamentary group, and for this purpose are always considered a single party. Coalition arrangements are often given names based on the colours of the parties involved, such as \"red-green\" for the SPD and Greens. Coalitions of three parties are often named after countries whose flags contain those colours, such as the black-yellow-green Jamaica coalition.\nGrand coalitions of the two major parties also occur, but these are relatively rare, as they typically prefer to associate with smaller ones. However, if the major parties are unable to assemble a majority, a grand coalition may be the only practical option. This was the case following the 2005 federal election, in which the incumbent SPD\u2013Green government was defeated but the opposition CDU/CSU\u2013FDP coalition also fell short of a majority. A grand coalition government was subsequently formed between the CDU/CSU and the SPD. Partnerships like these typically involve carefully structured cabinets: Angela Merkel of the CDU/CSU became Chancellor while the SPD was granted the majority of cabinet posts.\nCoalition formation has become increasingly complex as voters increasingly migrate away from the major parties during the 2000s and 2010s. While coalitions of more than two parties were extremely rare in preceding decades, they have become common on the state level. These often include the liberal FDP and the Greens alongside one of the major parties, or \"red\u2013red\u2013green\" coalitions of the SPD, Greens, and The Left. In the eastern states, dwindling support for moderate parties has seen the rise of new forms of grand coalitions such as the Kenya coalition. The rise of populist parties also increases the time that it takes for a successful coalition to form. By 2016, the Greens were participating eleven governing coalitions on the state level in seven different constellations. During campaigns, parties often declare which coalitions or partners they prefer or reject. This tendency toward fragmentation also spread to the federal level, particularly during the 2021 federal election, which saw the CDU/CSU and SPD fall short of a combined majority of votes for the first time in history.\nIndia.\nAfter India's Independence on 15 August 1947, the Indian National Congress, the major political party instrumental in the Indian independence movement, ruled the nation. The first Prime Minister, Jawaharlal Nehru, his successor Lal Bahadur Shastri, and the third Prime Minister, Indira Gandhi, were all members of the Congress party. However, Raj Narain, who had unsuccessfully contested an election against Indira from the constituency of Rae Bareli in 1971, lodged a case alleging electoral malpractice. In June 1975, Indira was found guilty and barred by the High Court from holding public office for six years. In response, a state of emergency was declared under the pretext of national security. The next election resulted in the formation of India's first ever national coalition government under the prime ministership of Morarji Desai, which was also the first non-Congress national government. It existed from 24 March 1977 to 15 July 1979, headed by the Janata Party, an amalgam of political parties opposed to the emergency imposed between 1975 and 1977. As the popularity of the Janata Party dwindled, Desai had to resign, and Chaudhary Charan Singh, a rival of his, became the fifth Prime Minister. However, due to lack of support, this coalition government did not complete its five-year term.\nCongress returned to power in 1980 under Indira Gandhi, and later under Rajiv Gandhi as the sixth prime minister. However, the general election of 1989 once again brought a coalition government under National Front, which lasted until 1991, with two prime ministers, the second one being supported by Congress. The 1991 election resulted in a Congress-led stable minority government for five years. The eleventh parliament produced three prime ministers in two years and forced the country back to the polls in 1998. The first successful coalition government in India which completed a whole five-year term was the Bharatiya Janata Party (BJP)-led National Democratic Alliance with Atal Bihari Vajpayee as prime minister from 1999 to 2004. Then another coalition, the Congress-led United Progressive Alliance, consisting of 13 separate parties, ruled India for two terms from 2004 to 2014 with Manmohan Singh as PM. However, in the 16th general election in May 2014, the BJP secured a majority on its own (becoming the first party to do so since the 1984 election), and the National Democratic Alliance came into power, with Narendra Modi as prime minister. In 2019, Narendra Modi was re-elected as prime minister as the National Democratic Alliance again secured a majority in the 17th general election. India returned to an NDA led coalition government in 2024 as the BJP failed to achieve an outright majority.\nIndonesia.\nAs a result of the toppling of Suharto, political freedom was significantly increased. Compared to only three parties allowed to exist in the New Order era, a total of 48 political parties participated in the 1999 election and always a total of more than 10 parties in next elections. There are no majority winner of those elections and coalition governments are inevitable. The current government is a coalition of five parliamentary parties led by the major centre-right Gerindra to let governing big tent Advanced Indonesia Coalition.\nIreland.\nIn Ireland, coalition governments are common; not since 1977 has a single party formed a majority government. Coalition governments to date have been led by either Fianna F\u00e1il or Fine Gael. They have been joined in government by one or more smaller parties or independent members of parliament (TDs).\nIreland's first coalition government was formed after the 1948 general election, with five parties and independents represented at cabinet. Before 1989, Fianna F\u00e1il had opposed participation in coalition governments, preferring single-party minority government instead. It formed a coalition government with the Progressive Democrats in that year.\nThe Labour Party has been in government on eight occasions. On all but one of those occasions, it was as a junior coalition party to Fine Gael. The exception was a government with Fianna F\u00e1il from 1993 to 1994. The 29th Government of Ireland (2011\u201316), was a grand coalition of the two largest parties, as Fianna F\u00e1il had fallen to third place in the D\u00e1il.\nThe current government is a Fianna F\u00e1il, Fine Gael and the Independents. Although Fianna F\u00e1il and Fine Gael have been serving in government together since 2020, they haven't formed coalition before due to their different roots that goes back to Irish Civil War (1922\u201323). \nIsrael.\nA similar situation exists in Israel, which typically has at least 10 parties holding representation in the Knesset. The only faction to ever gain the majority of Knesset seats was Alignment, an alliance of the Labor Party and Mapam that held an absolute majority for a brief period from 1968 to 1969. Historically, control of the Israeli government has alternated between periods of rule by the right-wing Likud in coalition with several right-wing and religious parties and periods of rule by the center-left Labor in coalition with several left-wing parties. Ariel Sharon's formation of the centrist Kadima party in 2006 drew support from former Labor and Likud members, and Kadima ruled in coalition with several other parties.\nIsrael also formed a national unity government from 1984\u20131988. The premiership and foreign ministry portfolio were held by the head of each party for two years, and they switched roles in 1986.\nJapan.\nIn Japan, controlling a majority in the House of Representatives is enough to decide the election of the prime minister (=recorded, two-round votes in both houses of the National Diet, yet the vote of the House of Representatives decision eventually overrides a dissenting House of Councillors vote automatically after the mandatory conference committee procedure fails which, by precedent, it does without real attempt to reconcile the different votes). Therefore, a party that controls the lower house can form a government on its own. It can also pass a budget on its own. But passing any law (including important budget-related laws) requires either majorities in both houses of the legislature or, with the drawback of longer legislative proceedings, a two-thirds majority in the House of Representatives.\nIn recent decades, single-party full legislative control is rare, and coalition governments are the norm: Most governments of Japan since the 1990s and, as of 2020, all since 1999 have been coalition governments, some of them still fell short of a legislative majority. The Liberal Democratic Party (LDP) held a legislative majority of its own in the National Diet until 1989 (when it initially continued to govern alone), and between the 2016 and 2019 elections (when it remained in its previous ruling coalition). The Democratic Party of Japan (through accessions in the House of Councillors) briefly controlled a single-party legislative majority for a few weeks before it lost the 2010 election (it, too, continued to govern as part of its previous ruling coalition).\nFrom the constitutional establishment of parliamentary cabinets and the introduction of the new, now directly elected upper house of parliament in 1947 until the formation of the LDP and the reunification of the Japanese Socialist Party in 1955, no single party formally controlled a legislative majority on its own. Only few formal coalition governments (46th, 47th, initially 49th cabinet) interchanged with technical minority governments and cabinets without technical control of the House of Councillors (later called \"twisted Diets\", \"nejire kokkai\", when they were not only technically, but actually divided). But during most of that period, the centrist Ryokuf\u016bkai was the strongest overall or decisive cross-bench group in the House of Councillors, and it was willing to cooperate with both centre-left and centre-right governments even when it was not formally part of the cabinet; and in the House of Representatives, minority governments of Liberals or Democrats (or their precursors; loose, indirect successors to the two major pre-war parties) could usually count on support from some members of the other major conservative party or from smaller conservative parties and independents. Finally in 1955, when Hatoyama Ichir\u014d's Democratic Party minority government called early House of Representatives elections and, while gaining seats substantially, remained in the minority, the Liberal Party refused to cooperate until negotiations on a long-debated \"conservative merger\" of the two parties were agreed upon, and eventually successful.\nAfter it was founded in 1955, the Liberal Democratic Party dominated Japan's governments for a long period: The new party governed alone without interruption until 1983, again from 1986 to 1993 and most recently between 1996 and 1999. The first time the LDP entered a coalition government followed its third loss of its House of Representatives majority in the 1983 House of Representatives general election. The LDP-New Liberal Club coalition government lasted until 1986 when the LDP won landslide victories in simultaneous double elections to both houses of parliament.\nThere have been coalition cabinets where the post of prime minister was given to a junior coalition partner: the JSP-DP-Cooperativist coalition government in 1948 of prime minister Ashida Hitoshi (DP) who took over after his JSP predecessor Tetsu Katayama had been toppled by the left wing of his own party, the JSP-Renewal-K\u014dmei-DSP-JNP-Sakigake-SDF-DRP coalition in 1993 with Morihiro Hosokawa (JNP) as compromise PM for the Ichir\u014d Ozawa-negotiated rainbow coalition that removed the LDP from power for the first time to break up in less than a year, and the LDP-JSP-Sakigake government that was formed in 1994 when the LDP had agreed, if under internal turmoil and with some defections, to bury the main post-war partisan rivalry and support the election of JSP prime minister Tomiichi Murayama in exchange for the return to government.\nMalaysia.\nEver since Malaysia gained independence in 1957, none of its federal governments have ever been controlled by a single political party. Due to the social nature of the country, the first federal government was formed by a three-party Alliance coalition, composed of the United Malays National Organisations (UMNO), the Malaysian Chinese Association (MCA), and the Malaysian Indian Congress (MIC). It was later expanded and rebranded as Barisan Nasional (BN), which includes parties representing the Malaysian states of Sabah and Sarawak.\nThe 2018 Malaysian general election saw the first non-BN coalition federal government in the country's electoral history, formed through an alliance between the Pakatan Harapan (PH) coalition and the Sabah Heritage Party (WARISAN). The federal government formed after the 2020\u20132022 Malaysian political crisis was the first to be established through coordination between multiple political coalitions. This occurred when the newly formed Perikatan Nasional (PN) coalition partnered with BN and Gabungan Parti Sarawak (GPS). In 2022 after its registration, Sabah-based Gabungan Rakyat Sabah (GRS) formally joined the government (though it had been a part of an informal coalition since 2020). The current government led by Prime Minister Anwar Ibrahim is composed of four political coalitions and 19 parties.\nNew Zealand.\nMMP was introduced in New Zealand in the 1996 election. \nIn order to get into power, parties need to get a total of 50% of the approximately (there can be more if an Overhang seat exists) 120 seats in parliament \u2013 61. Since it is rare for a party to win a full majority, they must form coalitions with other parties. For example, from 1996 to 1998, the country was governed by a coalition of the National with the minor NZ First; from 1999 to 2002, a coalition of the Labour and the minor Alliance and with confidence and supply from the Green Party held power. Between 2017 and 2020, Labour, New Zealand First formed a Coalition Government with confidence and supply from the Green Party. During the 2023 general election, National,ACT and New Zealand First formed a coalition government following three weeks of negotiations.\nSpain.\nSince 2015, there are many more coalition governments than previously in municipalities, autonomous regions and, since 2020 (coming from the November 2019 Spanish general election), in the Spanish Government. There are two ways of conforming them: all of them based on a program and its institutional architecture, one consists on distributing the different areas of government between the parties conforming the coalition and the other one is, like in the Valencian Community, where the ministries are structured with members of all the political parties being represented, so that conflicts that may occur are regarding competences and not fights between parties.\nCoalition governments in Spain had already existed during the 2nd Republic, and have been common in some specific Autonomous Communities since the 1980s. Nonetheless, the prevalence of two big parties overall has been eroded and the need for coalitions appears to be the new normal since around 2015.\nTurkey.\nTurkey's first coalition government was formed after the 1961 general election, with two political parties and independents represented at cabinet. It was also Turkey's first grand coalition as the two largest political parties of opposing political ideologies (Republican People's Party and Justice Party) united. Between 1960 and 2002, 17 coalition governments were formed in Turkey. The media and the general public view coalition governments as unfavorable and unstable due to their lack of effectiveness and short lifespan. Following Turkey's transition to a presidential system in 2017, political parties focussed more on forming electoral alliances. Due to separation of powers, the government doesn't have to be formed by parliamentarians and therefore not obliged to result in a coalition government. However, the parliament can dissolve the cabinet if the parliamentary opposition is in majority.\nUnited Kingdom.\nIn the United Kingdom, coalition governments (sometimes known as \"national governments\") usually have only been formed at times of national crisis. The most prominent was the National Government of 1931 to 1940. There were multi-party coalitions during both world wars. Apart from this, when no party has had a majority, minority governments normally have been formed with one or more opposition parties agreeing to vote in favour of the legislation which governments need to function: for instance the Labour government of James Callaghan formed a pact with the Liberals from March 1977 until July 1978, following a series of by-election defeats which had eroded Labour's majority of three seats which had been gained at the October 1974 election. However, in the run-up to the 1997 general election, Labour opposition leader Tony Blair was in talks with Liberal Democrat leader Paddy Ashdown about forming a coalition government if Labour failed to win a majority at the election; but there proved to be no need for a coalition as Labour won the election by a landslide. The 2010 general election resulted in a hung parliament (Britain's first for 36 years), and the Conservatives, led by David Cameron, which had won the largest number of seats, formed a coalition with the Liberal Democrats in order to gain a parliamentary majority, ending 13 years of Labour government. This was the first time that the Conservatives and Lib Dems had made a power-sharing deal at Westminster. It was also the first full coalition in Britain since 1945, having been formed 70 years virtually to the day after the establishment of Winston Churchill's wartime coalition,\nLabour and the Liberal Democrats have entered into a coalition twice in the Scottish Parliament, as well as twice in the Welsh Assembly.\nUruguay.\nSince the 1989 election, there have been 4 coalition governments, all including at least both the conservative National Party and the liberal Colorado Party. The first one was after the election of the blanco Luis Alberto Lacalle and lasted until 1992 due to policy disagreements, the longest lasting coalition was the Colorado-led coalition under the second government of Julio Mar\u00eda Sanguinetti, in which the national leader Alberto Volont\u00e9 was frequently described as a \"Prime Minister\", the next coalition (under president Jorge Batlle) was also Colorado-led, but it lasted only until after the 2002 Uruguay banking crisis, when the blancos abandoned the government. Following the 2019 Uruguayan general election, the blanco Luis Lacalle Pou formed the coalici\u00f3n multicolor, composed of his own National Party, the liberal Colorado Party, the eclectic Open Cabildo and the center left Independent Party.\nSupport and criticism.\nAdvocates of proportional representation suggest that a coalition government leads to more consensus-based politics, as a government comprising differing parties (often based on different ideologies) need to compromise about governmental policy. Another stated advantage is that a coalition government better reflects the popular opinion of the electorate within a country; this means, for instance, that the political system contains just one majority-based mechanism. Contrast this with district voting in which the majority mechanism occurs twice: first, the majority of voters pick the representative and, second, the body of representatives make a subsequent majority decision. The doubled majority decision undermines voter support for that decision. The benefit of proportional representation is that it contains that majority mechanism just once. Additionally, coalition partnership may play an important role in moderating the level of affective polarization over parties, that is, the animosity and hostility against the opponent party identifiers/supporters.\nThose who disapprove of coalition governments believe that such governments have a tendency to be fractious and prone to disharmony, as their component parties hold differing beliefs and thus may not always agree on policy. Sometimes the results of an election mean that the coalitions which are mathematically most probable are ideologically infeasible, for example in Flanders or Northern Ireland. A second difficulty might be the ability of minor parties to play \"kingmaker\" and, particularly in close elections, gain far more power in exchange for their support than the size of their vote would otherwise justify.\nGermany is the largest nation ever to have had proportional representation during the interbellum. After WW II, the German system, district based but then proportionally adjusted afterward, contains a threshold that keeps the number of parties limited. The threshold is set at five percent, resulting in empowered parties with at least a minimum amount of political gravity.\nCoalition governments have also been criticized for sustaining a consensus on issues when disagreement and the consequent discussion would be more fruitful. To forge a consensus, the leaders of ruling coalition parties can agree to silence their disagreements on an issue to unify the coalition against the opposition. The coalition partners, if they control the parliamentary majority, can collude to make the parliamentary discussion on the issue irrelevant by consistently disregarding the arguments of the opposition and voting against the opposition's proposals \u2014 even if there is disagreement within the ruling parties about the issue. However, in winner-take-all this seems always to be the case.\nPowerful parties can also act in an oligocratic way to form an alliance to stifle the growth of emerging parties. Of course, such an event is rare in coalition governments when compared to two-party systems, which typically exist because of stifling of the growth of emerging parties, often through discriminatory nomination rules regulations and plurality voting systems, and so on.\nA single, more powerful party can shape the policies of the coalition disproportionately. Smaller or less powerful parties can be intimidated to not openly disagree. In order to maintain the coalition, they would have to vote against their own party's platform in the parliament. If they do not, the party has to leave the government and loses executive power. However, this is contradicted by the \"kingmaker\" factor mentioned above.\nFinally, a strength that can also be seen as a weakness is that proportional representation puts the emphasis on collaboration. All parties involved are looking at the other parties in the best light possible, since they may be (future) coalition partners. The pendulum may therefore show less of a swing between political extremes. Still, facing external issues may then also be approached from a collaborative perspective, even when the outside force is not benevolent.\nLegislative coalitions and agreements.\nA legislative coalition or voting coalition is when political parties in a legislature align on voting to push forward specific policies or legislation, but do not engage in power-sharing of the executive branch like in coalition governments.\nIn a parliamentary system, political parties may form a confidence and supply arrangement, pledging to support the governing party on legislative bills and motions that carry a vote of confidence. Unlike a coalition government, which is a more formalised partnership characterised by the sharing of the executive branch, a confidence and supply arrangement does not entail executive \"power-sharing\". Instead, it involves the governing party supporting specific proposals and priorities of the other parties in the arrangement, in return for their continued support on motions of confidence.\nUnited States.\nIn the United States, political parties have formed legislative coalitions in the past in order to push forward specific policies or legislation in the United States Congress. In 1855, a coalition was formed between members of the American Party, Opposition Party and Republican Party to elect Nathaniel P. Banks speaker of the House.\nLater, in 1917, at the start of the 65th Congress, a coalition was formed between members of the Democratic Party, Progressive Party and Socialist Party of America to elect Champ Clark as the speaker of the United States House of Representatives. This was the only time a socialist party entered coalition government in the House on a national level. More recently, during the 118th Congress, an informal legislative coalition formed between Democrats and mainline Republicans to pass critical legislation opposed by the Freedom Caucus, an extreme right-wing faction controlling a minority of seats in the Republican Conference.\nA coalition government, in which \"power-sharing\" of executive offices is performed, has not occurred in the United States. The norms that allow coalition governments to form and persist do not exist in the United States.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6037", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6037", "title": "Continuity property", "text": ""}
{"id": "6038", "revid": "51005847", "url": "https://en.wikipedia.org/wiki?curid=6038", "title": "Chemical engineering", "text": "Engineering discipline focused on the design and operation of chemical plants\nChemical engineering is an engineering field which deals with the study of the operation and design of chemical plants as well as methods of improving production. Chemical engineers develop economical commercial processes to convert raw materials into useful products. Chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. The work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering, construction specification, and operating instructions.\nChemical engineers typically hold a degree in Chemical Engineering or Process Engineering. Practicing engineers may have professional certification and be accredited members of a professional body. Such bodies include the Institution of Chemical Engineers (IChemE) or the American Institute of Chemical Engineers (AIChE) and respective states in the U.S., which ultimately confer licensure and title of Professional Engineer. A degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.\nEtymology.\nA 1996 article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. In the same paper, however, George E. Davis, an English consultant, was credited with having coined the term. Davis also tried to found a Society of Chemical Engineering, but instead, it was named the Society of Chemical Industry (1881), with Davis as its first secretary. The \"History of Science in United States: An Encyclopedia\" puts the use of the term around 1890. \"Chemical engineering\", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850. By 1910, the profession, \"chemical engineer,\" was already in common use in Britain and the United States.\nHistory.\nNew concepts and innovations.\nIn the 1940s, it became clear that unit operations alone were insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to receive greater focus. Along with other novel concepts, such as process systems engineering (PSE), a \"second paradigm\" was defined. Transport phenomena gave an analytical approach to chemical engineering while PSE focused on its synthetic elements, such as those of a control system and process design. Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry; however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin. Meanwhile, progress in polymer science in the 1950s paved way for the \"age of plastics\".\nSafety and hazard developments.\nConcerns regarding large-scale chemical manufacturing facilities' safety and environmental impact were also raised during this period. \"Silent Spring\", published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide. The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages. 1984 Bhopal disaster in India resulted in at least 4,000 deaths. These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus. In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States. In time, the systematic application of safety principles to chemical and other process plants began to be considered a specific discipline, known as process safety.\nRecent progress.\nAdvancements in computer science found applications for designing and managing plants, simplifying calculations and drawings that previously had to be done manually. Programs such as Aspen HYSYS were developed to complete multiple chemical engineering calculations. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well. Chemical engineering principles were used to produce DNA sequences in large quantities.\nConcepts.\nPlant design and construction.\nChemical engineering design concerns the creation of plans, specifications, and economic analyses for pilot plants, new plants, or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by several factors, including funding, government regulations, and safety standards. These constraints dictate a plant's choice of process, materials, and equipment.\nPlant construction is coordinated by project engineers and project managers, depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills or act as a consultant to the project group. In the USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.\nProcess design and analysis.\nA unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors. On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration, hydrogenation, and oxidation) involve the conversion of materials by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.\nProcess design requires the definition of equipment types and sizes as well as how they are connected and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or existing chemical factory. \nEducation for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.\nTransport phenomena.\nModeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species, respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena, therefore, requires an understanding of applied mathematics.\nApplications and practice.\nChemical engineers develop economic ways of using materials and energy. Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals, and plastics on a large-scale, industrial setting. They are also involved in waste management and research. Both applied and research facets could make extensive use of computers.\nChemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments, by scaling up theoretical chemical reactions, to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, troubleshooting, and daily operations in either full-time or consulting roles.\nSee also.\nRelated topics.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nRelated fields and concepts.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nAssociations.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6041", "revid": "45293124", "url": "https://en.wikipedia.org/wiki?curid=6041", "title": "List of comedians", "text": "A comedian is one who entertains through comedy, such as jokes and other forms of humour. Following is a list of comedians, comedy groups, and comedy writers.\n&lt;templatestyles src = \"Nonumtoc/styles.css\" /&gt;&lt;templatestyles src=\"Horizontal TOC/styles.css\" /&gt;&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\nComedians.\n\"(sorted alphabetically by surname)\"\nA.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nB.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nC.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nD.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nE.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nF.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nG.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nH.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nI.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nJ.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nK.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nL.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nM.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nN.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nO.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nP.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nQ.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nR.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nS.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nT.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nU.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nV.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nW.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nY.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nZ.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nComedy groups.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nComedy writers.\n\"(sorted alphabetically by surname)\"\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSee also.\nLists of comedians by nationality\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOther related lists"}
{"id": "6042", "revid": "19574118", "url": "https://en.wikipedia.org/wiki?curid=6042", "title": "Compact space", "text": "Type of mathematical space\nIn mathematics, specifically general topology, compactness is a property that seeks to generalize the notion of a closed and bounded subset of Euclidean space. The idea is that every infinite sequence of points has \"limiting values\". For example, the real line is not compact since the sequence of natural numbers has no real limiting value. The open interval (0,1) is not compact because it excludes the limiting values 0 and 1, whereas the closed interval [0,1] is compact. Similarly, the space of rational numbers formula_1 is not compact, because every irrational number is the limit of the rational numbers that are lower than it. On the other hand, the \"extended\" real number line is compact, since it contains both infinities. There are many ways to make this heuristic notion precise. These ways usually agree in a metric space, but may not be equivalent in other topological spaces.\nOne such generalization is that a topological space is \"sequentially\" compact if every infinite sequence of points sampled from the space has an infinite subsequence that converges to some point of the space. The Bolzano\u2013Weierstrass theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the closed unit interval [0, 1], some of those points will get arbitrarily close to some real number in that space. \nFor instance, some of the numbers in the sequence , , , , , , ... accumulate to 0 (while others accumulate to 1). \nSince neither 0 nor 1 are members of the open unit interval (0, 1), those same sets of points would not accumulate to any point of it, so the open unit interval is not compact. Although subsets (subspaces) of Euclidean space can be compact, the entire space itself is not compact, since it is not bounded. For example, considering formula_2 (the real number line), the sequence of points 0, \u20091,\u2009 2, \u20093,\u2009... has no subsequence that converges to any real number.\nCompactness was formally introduced by Maurice Fr\u00e9chet in 1906 to generalize the Bolzano\u2013Weierstrass theorem from spaces of geometrical points to spaces of functions. The Arzel\u00e0\u2013Ascoli theorem and the Peano existence theorem exemplify applications of this notion of compactness to classical analysis. Following its initial introduction, various equivalent notions of compactness, including sequential compactness and limit point compactness, were developed in general metric spaces. In general topological spaces, however, these notions of compactness are not necessarily equivalent. The most useful notion\u2014and the standard definition of the unqualified term \"compactness\"\u2014is phrased in terms of families of open sets that \"cover\" the space, in the sense that each point of the space lies in some set contained in the family. Specifically, compactness is the condition that any such family has a finite subfamily that also covers the space. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally\u2014that is, in a neighborhood of each point\u2014into corresponding statements that hold throughout the space, and many theorems are of this character.\nThe term compact set is sometimes used as a synonym for compact space, but also often refers to a compact subspace of a topological space.\nHistorical development.\nIn the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. \nBolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. \nThe process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts\u2014until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.\nIn the 1880s, it became clear that results similar to the Bolzano\u2013Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. \nThe idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzel\u00e0. \nThe culmination of their investigations, the Arzel\u00e0\u2013Ascoli theorem, was a generalization of the Bolzano\u2013Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's \"limit point\". Towards the beginning of the twentieth century, results similar to that of Arzel\u00e0 and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. \nFor a certain class of Green's functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzel\u00e0\u2013Ascoli theorem held in the sense of mean convergence \u2013 or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. \nIt was Maurice Fr\u00e9chet who, in 1906, had distilled the essence of the Bolzano\u2013Weierstrass property and coined the term \"compactness\" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis).\nHowever, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. \nIn 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. \nThe significance of this lemma was recognized by \u00c9mile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine\u2013Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.\nThis property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately, the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine\u2013Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fr\u00e9chet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.\nBasic examples.\nAny finite space is compact; a finite subcover can be obtained by selecting, for each point, an open set containing it. A nontrivial example of a compact space is the (closed) unit interval [0,1] of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point among these points in that interval. For instance, the odd-numbered terms of the sequence 1,\u2009,\u2009,\u2009,\u2009,\u2009,\u2009,\u2009,\u2009... get arbitrarily close to\u00a00, while the even-numbered ones get arbitrarily close to\u00a01. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself\u2014an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval [0,\u221e), one could choose the sequence of points 0,\u20091,\u20092,\u20093,\u2009..., of which no sub-sequence ultimately gets arbitrarily close to any given real number.\nIn two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary\u2014without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can still tend to the missing point, thereby not getting arbitrarily close to any point \"within\" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.\nDefinitions.\nVarious definitions of compactness may apply, depending on the level of generality. \nA subset of Euclidean space in particular is compact if and only if it is closed and bounded. This implies, by the Bolzano\u2013Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.\nIn contrast, the different notions of compactness are not equivalent in general topological spaces, and the most useful notion of compactness\u2014originally called \"bicompactness\"\u2014is defined using covers consisting of open sets (see \"Open cover definition\" below). \nThat this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine\u2013Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally\u2014in a neighbourhood of each point of the space\u2014and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.\nOpen cover definition.\nFormally, a topological space X is called \"compact\" if every open cover of X has a finite subcover. That is, X is compact if for every collection C of open subsets of X such that\nformula_3\nthere is a finite subcollection F \u2286 C such that\nformula_4\nSome branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term \"quasi-compact\" for the general notion, and reserve the term \"compact\" for topological spaces that are both Hausdorff and \"quasi-compact\". A compact set is sometimes referred to as a \"compactum\", plural \"compacta\".\nCompactness of subsets.\nA subset K of a topological space X is compact if for every arbitrary collection C of open subsets of X such that\nformula_5\nthere is a finite subcollection F \u2286 C such that\nformula_6\nEquivalently, K is compact as a subset of X if and only if the topological space K is compact in the subspace topology. In particular, if formula_7, with subset Y equipped with the subspace topology, then K is compact in Y if and only if K is compact in X. Furthermore, the compactness of K as a subset of a topological space X is independent of the embedding, provided that the subspace topology on K is the same.\nCharacterization.\nIf X is a topological space then the following are equivalent:\nBourbaki defines a compact space (quasi-compact space) as a topological space where each filter has a cluster point (i.e., 8. in the above).\nEuclidean space.\nFor any subset A of Euclidean space, A is compact if and only if it is closed and bounded; this is the Heine\u2013Borel theorem.\nAs a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed n-ball.\nMetric spaces.\nFor any metric space (\"X\", \"d\"), the following are equivalent (assuming countable choice):\nA compact metric space (\"X\", \"d\") also satisfies the following properties:\nOrdered spaces.\nFor an ordered space (\"X\", &lt;) (i.e. a totally ordered set equipped with the order topology), the following are equivalent:\nAn ordered space satisfying (any one of) these conditions is called a complete lattice.\nIn addition, the following are equivalent for all ordered spaces (\"X\", &lt;), and (assuming countable choice) are true whenever (\"X\", &lt;) is compact (the converse in general fails if (\"X\", &lt;) is not also metrizable):\nCharacterization by continuous functions.\nLet X be a topological space and C(\"X\") the ring of real continuous functions on X. \nFor each \"p\" \u2208 \"X\", the evaluation map formula_9\ngiven by ev\"p\"(\"f\") = \"f\"(\"p\") is a ring homomorphism. \nThe kernel of ev\"p\" is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space X is pseudocompact if and only if every maximal ideal in C(\"X\") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.\nIn general, for non-pseudocompact spaces there are always maximal ideals m in C(\"X\") such that the residue field C(\"X\")/\"m\" is a (non-Archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space X is compact if and only if every point x of the natural extension \"*X\" is infinitely close to a point \"x\"0 of X (more precisely, x is contained in the monad of \"x\"0).\nHyperreal definition.\nA space X is compact if its hyperreal extension \"*X\" (constructed, for example, by the ultrapower construction) has the property that every point of \"*X\" is infinitely close to some point of \"X\" \u2282 \"*X\". For example, an open real interval is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of X.\nProperties of compact spaces.\nFunctions and compact spaces.\nSince a continuous image of a compact space is compact, the extreme value theorem holds for such spaces: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. \n(Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.\nCompactifications.\nEvery topological space X is an open dense subspace of a compact space having at most one point more than X, by the Alexandroff one-point compactification. \nBy the same construction, every locally compact Hausdorff space X is an open dense subspace of a compact Hausdorff space having at most one point more than X.\nOrdered compact spaces.\nA nonempty compact subset of the real numbers has a greatest element and a least element.\nLet X be a simply ordered set endowed with the order topology. \nThen X is compact if and only if X is a complete lattice (i.e. all subsets have suprema and infima).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n\"This article incorporates material from http:// on PlanetMath, which is licensed under the .\""}
{"id": "6043", "revid": "39680754", "url": "https://en.wikipedia.org/wiki?curid=6043", "title": "Critical temperature", "text": ""}
{"id": "6045", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=6045", "title": "Clodius", "text": "Historiography of the Roman name, with list of historic and legendary examples\nClodius is an alternate form of the Roman \"nomen\" Claudius, a patrician \"gens\" that was traditionally regarded as Sabine in origin. The alternation of \"o\" and \"au\" is characteristic of the Sabine dialect. The feminine form is Clodia.\nRepublican era.\nOther Clodii of the Republic.\nIn addition to Clodius, Clodii from the Republican era include:\nWomen of the Claudii Marcelli branch were often called \"Clodia\" in the late Republic.\nImperial era.\nPeople using the name \"Clodius\" during the period of the Roman Empire include:\nClodii Celsini.\nThe Clodii Celsini continued to practice the traditional religions of antiquity in the face of Christian hegemony through at least the 4th century, when Clodius Celsinus Adelphius (see below) converted. Members of this branch include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6046", "revid": "516710", "url": "https://en.wikipedia.org/wiki?curid=6046", "title": "Cicero", "text": "Roman statesman and lawyer (106\u201343 BC)\nMarcus Tullius Cicero ( ; ; 3 January 106 BC\u00a0\u2013 7 December 43\u00a0BC) was a Roman statesman, lawyer, scholar, philosopher, orator, writer and Academic skeptic, who tried to uphold optimate principles during the political crises that led to the establishment of the Roman Empire. His extensive writings include treatises on rhetoric, philosophy and politics. He is considered one of Rome's greatest orators and prose stylists and the innovator of what became known as \"Ciceronian rhetoric\". Cicero was educated in Rome and in Greece. He came from a wealthy municipal family of the Roman equestrian order, and served as consul in 63\u00a0BC.\nHe greatly influenced both ancient and modern reception of the Latin language. A substantial part of his work has survived, and he was admired by both ancient and modern authors alike. Cicero adapted the arguments of the chief schools of Hellenistic philosophy in Latin and coined a large portion of Latin philosophical vocabulary via lexical innovation (e.g. neologisms such as , \"generator\", , \"infinitio\", , ), almost 150 of which were the result of translating Greek philosophical terms.\nThough he was an accomplished orator and successful lawyer, Cicero believed his political career was his most important achievement. During his consulship in 63\u00a0BC, he suppressed the Catilinarian conspiracy. However, because he had summarily and controversially executed five of the conspirators without trial, he was exiled in 58 but recalled the next year. Spending much of the 50s unhappy with the state of Roman politics, he took a governorship in Cilicia in 51 and returned to Italy on the eve of Caesar's civil war. Supporting Pompey during the war, Cicero was pardoned after Caesar's victory. After Caesar's assassination in 44\u00a0BC, he led the Senate against Mark Antony, attacking him in a series of speeches. He elevated Caesar's heir Octavian to rally support against Antony in the ensuing violent conflict. But after Octavian and Antony reconciled to form the triumvirate (with Lepidus), Cicero was proscribed and executed in late 43\u00a0BC while attempting to escape Italy for safety. His severed hands and head (taken by order of Antony and displayed representing the repercussions of his anti-Antonian actions as a writer and as an orator, respectively) were then displayed on the rostra.\nPetrarch's rediscovery of Cicero's letters is often credited for initiating the 14th-century Renaissance in public affairs, humanism, and classical Roman culture. According to Polish historian Tadeusz Zieli\u0144ski, \"the Renaissance was above all things a revival of Cicero, and only after him and through him of the rest of Classical antiquity.\" The peak of Cicero's authority and prestige came during the 18th-century Enlightenment, and his impact on leading Enlightenment thinkers and political theorists such as John Locke, David Hume, Montesquieu, and Edmund Burke was substantial. His works rank among the most influential in global culture, and today still constitute one of the most important bodies of primary material for the writing and revision of Roman history, especially the last days of the Roman Republic.\nEarly life.\nMarcus Tullius Cicero was born on 3 January 106\u00a0BC in Arpinum, a hill town southeast of Rome. He belonged to the \"tribus\" Cornelia. His father was a wealthy member of the equestrian order and possessed good connections in Rome. However, not being of robust health (he experienced poor digestion and inflammation of the eyes), he could not enter public life and studied extensively to compensate. Little is known about Cicero's mother Helvia, but Cicero's brother Quintus wrote in a letter that she was a thrifty housewife.\nCicero's cognomen, a hereditary nickname, comes from the Latin for chickpea, . Plutarch explains that the name was originally given to one of Cicero's ancestors who had a cleft in the tip of his nose resembling a chickpea. The famous family names of Fabius, Lentulus, and Piso come from the Latin names of beans, lentils, and peas, respectively. Plutarch writes that Cicero was urged to change this deprecatory name when he entered politics, but refused, saying that he would make \"Cicero\" more glorious than \"Scaurus\" (\"Swollen-ankled\") and \"Catulus\" (\"Puppy\").\nAt the age of 15, in 90\u00a0BC, Cicero started serving under Pompey Strabo and later Sulla in the Social war between Rome and its Italian allies. When in Rome during the turbulent plebeian tribunate of Publius Sulpicius Rufus in 88\u00a0BC which saw a short bout of fighting between the Sulpicius and Sulla, who had been elected consul for that year, Cicero found himself greatly impressed by Sulpicius' oratory even if he disagreed with his politics. He continued his studies at Rome, writing a pamphlet titled \"On Invention\" relating to rhetorical argumentation and studying philosophy with Greek academics who had fled the ongoing First Mithridatic War.\nEducation.\nDuring this period in Roman history, Greek language and cultural studies were highly valued by the elite classes. Cicero was therefore educated in the teachings of the ancient Greek philosophers, poets and historians; as he obtained much of his understanding of the theory and practice of rhetoric from the Greek poet Archias. Cicero used his knowledge of Greek to translate many of the theoretical concepts of Greek philosophy into Latin, thus translating Greek philosophical works for a larger audience. It was precisely his broad education that tied him to the traditional Roman elite.\nCicero's interest in philosophy figured heavily in his later career and led to him providing a comprehensive account of Greek philosophy for a Roman audience, including creating a philosophical vocabulary in Latin. In 87 BC, Philo of Larissa, the head of the Platonic Academy that had been founded by Plato in Athens about 300 years earlier, arrived in Rome. Cicero, \"inspired by an extraordinary zeal for philosophy\", sat enthusiastically at his feet and absorbed Carneades' Academic Skeptic philosophy.\nAccording to Plutarch, Cicero was an extremely talented student, whose learning attracted attention from all over Rome, affording him the opportunity to study Roman law under Quintus Mucius Scaevola. Cicero's fellow students were Gaius Marius Minor, Servius Sulpicius Rufus (who became a famous lawyer, one of the few whom Cicero considered superior to himself in legal matters), and Titus Pomponius. The latter two became Cicero's friends for life, and Pomponius (who later received the nickname \"Atticus\", and whose sister married Cicero's brother) would become, in Cicero's own words, \"as a second brother\", with both maintaining a lifelong correspondence.\nIn 79 BC, Cicero left for Greece, Asia Minor and Rhodes. This was perhaps to avoid the potential wrath of Sulla, as Plutarch claims, though Cicero himself says it was to hone his skills and improve his physical fitness. In Athens he studied philosophy with Antiochus of Ascalon, the 'Old Academic' and initiator of Middle Platonism. In Asia Minor, he met the leading orators of the region and continued to study with them. Cicero then journeyed to Rhodes to meet his former teacher, Apollonius Molon, who had taught him in Rome. Molon helped Cicero hone the excesses in his style, as well as train his body and lungs for the demands of public speaking. Charting a middle path between the competing Attic and Asiatic styles, Cicero would ultimately become considered second only to Demosthenes among history's orators.\nEarly career.\nEarly legal activity.\nWhile Cicero had feared that the law courts would be closed forever, they were reopened in the aftermath of Sulla's civil war and the purging of Sulla's political opponents in the proscriptions. Many of the orators whom Cicero had admired in his youth were now dead from age or political violence. His first major appearance in the courts was in 81\u00a0BC at the age of 26 when he delivered \"Pro Quinctio\", a speech defending certain commercial transactions which Cicero had recorded and disseminated.\nHis more famous speech defending Sextus Roscius of Ameria \u2013 \u2013 on charges of parricide in 80\u00a0BC was his first appearance in criminal court. In this high-profile case, Cicero accused a freedman of the dictator Sulla, Chrysogonus, of fabricating Roscius' father's proscription to obtain Roscius' family's property. Successful in his defence, Cicero tactfully avoided incriminating Sulla of any wrongdoing and developed a positive oratorical reputation for himself.\nWhile Plutarch claims that Cicero left Rome shortly thereafter out of fear of Sulla's response, according to Kathryn Tempest, \"most scholars now dismiss this suggestion\" because Cicero left Rome after Sulla resigned his dictatorship. Cicero, for his part, later claimed that he left Rome, headed for Asia, to develop his physique and develop his oratory. After marrying his wife, Terentia, in 80\u00a0BC, he eventually left for Asia Minor with his brother Quintus, his friend Titus Atticus, and others on a long trip spanning most of 79 through 77\u00a0BC. Returning to Rome in 77\u00a0BC, Cicero again busied himself with legal defence.\nEarly political career.\nIn 76\u00a0BC, at the quaestorian elections, Cicero was elected at the minimum age required \u2013 30 years \u2013 in the first returns from the \"comitia tributa\", to the post of quaestor. Ex officio, he also became a member of the Senate. In the quaestorian lot, he was assigned to Sicily for 75\u00a0BC. The post, which was largely one related to financial administration in support of the state or provincial governors, proved for Cicero an important place where he could gain clients in the provinces. His time in Sicily saw him balance his duties \u2013 largely in terms of sending more grain back to Rome \u2013 with his support for the provincials, Roman businessmen in the area, and local potentates. Adeptly balancing those responsibilities, he won their gratitude. He was also appreciated by local Syracusans for the rediscovery of the lost tomb of Archimedes, which he personally financed.\nPromising to lend the Sicilians his oratorical voice, he was called on a few years after his quaestorship to prosecute the Roman province's governor Gaius Verres, for abuse of power and corruption. In 70\u00a0BC, at the age of 36, Cicero launched his first high-profile prosecution against Verres, an emblem of the corrupt Sullan supporters who had risen in the chaos of the civil war.\nThe prosecution of Gaius Verres was a great forensic success for Cicero. While Verres hired the prominent lawyer, Quintus Hortensius, after a lengthy period in Sicily collecting testimonials and evidence and persuading witnesses to come forward, Cicero returned to Rome and won the case in a series of dramatic court battles. His unique style of oratory set him apart from the flamboyant Hortensius. On the conclusion of this case, Cicero came to be considered the greatest orator in Rome. The view that Cicero may have taken the case for reasons of his own is viable. Hortensius was, at this point, known as the best lawyer in Rome; to beat him would guarantee much success and the prestige that Cicero needed to start his career. Cicero's oratorical ability is shown in his character assassination of Verres and various other techniques of persuasion used on the jury. One such example is found in the speech \"In Verrem\", where he states \"with you on this bench, gentlemen, with Marcus Acilius Glabrio as your president, I do not understand what Verres can hope to achieve\". Oratory was considered a great art in ancient Rome and an important tool for disseminating knowledge and promoting oneself in elections, in part because there were no regular newspapers or mass media. Cicero was neither a patrician nor a plebeian noble; his rise to political office despite his relatively humble origins has traditionally been attributed to his brilliance as an orator.\nCicero grew up in a time of civil unrest and war. Sulla's victory in the first of a series of civil wars led to a new constitutional framework that undermined (liberty), the fundamental value of the Roman Republic. Nonetheless, Sulla's reforms strengthened the position of the equestrian class, contributing to that class's growing political power. Cicero was both an Italian and a , but more importantly he was a Roman constitutionalist. His social class and loyalty to the Republic ensured that he would \"command the support and confidence of the people as well as the Italian middle classes\". He successfully ascended the cursus honorum, holding each magistracy at or near the youngest possible age: quaestor in 75 BC (age 30), aedile in 69 BC (age 36), and praetor in 66 BC (age 39), when he served as president of the extortion court. He was then elected consul at age 42.\nConsulship.\nCicero, seizing the opportunity offered by optimate fear of reform, was elected consul for the year 63 BC; he was elected with the support of every unit of the centuriate assembly, rival members of the post-Sullan establishment, and the leaders of municipalities throughout post-Social War Italy. His co-consul for the year, Gaius Antonius Hybrida, played a minor role.\nHe began his consular year by opposing a land bill proposed by a plebeian tribune which would have appointed commissioners with semi-permanent authority over land reform. Cicero was also active in the courts, defending Gaius Rabirius from accusations of participating in the unlawful killing of plebeian tribune Lucius Appuleius Saturninus in 100 BC. The prosecution occurred before the and threatened to reopen conflict between the Marian and Sullan factions at Rome. Cicero defended the use of force as being authorised by a , which would prove similar to his own use of force under such conditions.\nCatilinarian conspiracy.\nMost famously\u00a0\u2013 in part because of his own publicity\u00a0\u2013 he thwarted a conspiracy led by Lucius Sergius Catilina to overthrow the Roman Republic with the help of foreign armed forces. Cicero procured a \"senatus consultum ultimum\" (a recommendation from the senate attempting to legitimise the use of force) and drove Catiline from the city with four vehement speeches (the Catilinarian orations), which remain outstanding examples of his rhetorical style. The Orations listed Catiline and his followers' debaucheries, and denounced Catiline's senatorial sympathizers as roguish and dissolute debtors clinging to Catiline as a final and desperate hope. Cicero demanded that Catiline and his followers leave the city. At the conclusion of Cicero's first speech (which was made in the Temple of Jupiter Stator), Catiline hurriedly left the Senate. In his following speeches, Cicero did not directly address Catiline. He delivered the second and third orations before the people, and the last one again before the Senate. By these speeches, Cicero wanted to prepare the Senate for the worst possible case; he also delivered more evidence, against Catiline.\nCatiline fled and left behind his followers to start the revolution from within while he himself assaulted the city with an army of \"moral and financial bankrupts, or of honest fanatics and adventurers\". It is alleged that Catiline had attempted to involve the Allobroges, a tribe of Transalpine Gaul, in their plot, but Cicero, working with the Gauls, was able to seize letters that incriminated the five conspirators and forced them to confess in front of the Senate. The senate then deliberated upon the conspirators' punishment. As it was the dominant advisory body to the various legislative assemblies rather than a judicial body, there were limits to its power; however, martial law was in effect, and it was feared that simple house arrest or exile\u00a0\u2013 the standard options\u00a0\u2013 would not remove the threat to the state. At first Decimus Junius Silanus spoke for the \"extreme penalty\"; but during the debate many were swayed by Julius Caesar, who decried the precedent it would set and argued in favor of life imprisonment in various Italian towns. Cato the Younger then rose in defense of the death penalty and the Senate finally agreed on the matter, and came down in support of the death penalty. Cicero had the conspirators taken to the Tullianum, the notorious Roman prison, where they were strangled. Cicero himself accompanied the former consul Publius Cornelius Lentulus Sura, one of the conspirators, to the Tullianum.\nCicero received the honorific \"pater patriae\" for his efforts to suppress the conspiracy, but lived thereafter in fear of trial or exile for having put Roman citizens to death without trial. While the \"senatus consultum ultimum\" gave some legitimacy to the use of force against the conspirators, Cicero also argued that Catiline's conspiracy, by virtue of its treason, made the conspirators enemies of the state and forfeited the protections intrinsically possessed by Roman citizens. The consuls moved decisively. Antonius Hybrida was dispatched to defeat Catiline in battle that year, preventing Crassus or Pompey from exploiting the situation for their own political aims.\nAfter the suppression of the conspiracy, Cicero was proud of his accomplishment. Some of his political enemies argued that though the act gained Cicero popularity, he exaggerated the extent of his success. He overestimated his popularity again several years later after being exiled from Italy and then allowed back from exile. At this time, he claimed that the republic would be restored along with him.\nShortly after completing his consulship, in late 62\u00a0BC, Cicero arranged the purchase of a large townhouse on the Palatine Hill previously owned by Rome's richest citizen, Marcus Licinius Crassus. To finance the purchase, Cicero borrowed some two million sesterces from Publius Cornelius Sulla, whom he had previously defended from court. Cicero boasted his house was \"in conspectu prope totius urbis\" (\"in sight of nearly the whole city\"), only a short walk from the Roman Forum.\nExile and return.\nIn 60 BC, Julius Caesar invited Cicero to be the fourth member of his existing partnership with Pompey and Marcus Licinius Crassus, an assembly that would eventually be called the First Triumvirate. Cicero refused the invitation because he suspected it would undermine the Republic, and because he was strongly opposed to anything unconstitutional that limited the powers of the consuls and replaced them with non-elected officials.\nDuring Caesar's consulship of 59 BC, the triumvirate had achieved many of their goals of land reform, publicani debt forgiveness, ratification of Pompeian conquests, etc. With Caesar leaving for his provinces, they wished to maintain their hold on politics. They engineered the adoption of patrician Publius Clodius Pulcher into a plebeian family and had him elected as one of the ten tribunes of the plebs for 58 BC. Clodius used the triumvirate's backing to push through legislation that benefited them. He introduced several laws (the \"leges Clodiae\") that made him popular with the people, strengthening his power base, then he turned on Cicero. Clodius passed a law which made it illegal to offer \"fire and water\" (i.e. shelter or food) to anyone who executed a Roman citizen without a trial. \nCicero, having executed members of the Catiline conspiracy four years previously without formal trial, was clearly the intended target. Furthermore, many believed that Clodius acted in concert with the triumvirate who feared that Cicero would seek to abolish many of Caesar's accomplishments while consul the year before. Cicero argued that the \"senatus consultum ultimum\" indemnified him from punishment, and he attempted to gain the support of the senators and consuls, especially of Pompey.\nCicero grew out his hair, dressed in mourning and toured the streets. Clodius' gangs dogged him, hurling abuse, stones and even excrement. Hortensius, trying to rally to his old rival's support, was almost lynched. The Senate and the consuls were cowed. Caesar, who was still encamped near Rome, was apologetic but said he could do nothing when Cicero brought himself to grovel in the proconsul's tent. Everyone seemed to have abandoned Cicero.\nAfter Clodius passed a law to deny to Cicero fire and water (i.e. shelter) within four hundred miles of Rome, Cicero went into exile. He arrived at Thessalonica, on 23 May 58\u00a0BC. In his absence, Clodius, who lived next door to Cicero on the Palatine, arranged for Cicero's house to be confiscated by the state, and was even able to purchase a part of the property in order to extend his own house. After demolishing Cicero's house, Clodius had the land consecrated and symbolically erected a temple of Liberty (\"aedes Libertatis\") on the vacant land.\nCicero's exile caused him to fall into depression. He wrote to Atticus: \"Your pleas have prevented me from committing suicide. But what is there to live for? Don't blame me for complaining. My afflictions surpass any you ever heard of earlier\". After the intervention of recently elected tribune Titus Annius Milo, acting on the behalf of Pompey who wanted Cicero as a client, the Senate voted in favor of recalling Cicero from exile. Clodius cast the single vote against the decree. Cicero returned to Italy on 5 August 57\u00a0BC, landing at Brundisium. He was greeted by a cheering crowd, and, to his delight, his beloved daughter Tullia. In his \"Oratio De Domo Sua Ad Pontifices\", Cicero convinced the College of Pontiffs to rule that the consecration of his land was invalid, thereby allowing him to regain his property and rebuild his house on the Palatine.\nCicero tried to re-enter politics as an independent operator, but his attempts to attack portions of Caesar's legislation were unsuccessful and encouraged Caesar to re-solidify his political alliance with Pompey and Crassus. The conference at Luca in 56\u00a0BC left the three-man alliance in domination of the republic's politics; this forced Cicero to recant and support the triumvirate out of fear from being entirely excluded from public life. After the conference, Cicero lavishly praised Caesar's achievements, got the Senate to vote a thanksgiving for Caesar's victories, and grant money to pay his troops. He also delivered a speech 'On the consular provinces' () which checked an attempt by Caesar's enemies to strip him of his provinces in Gaul. After this, a cowed Cicero concentrated on his literary works. It is uncertain whether he was directly involved in politics for the following few years. His legal work largely consisted of defending allies of the ruling and his own personal friends and allies; he defended his former pupil Marcus Caelius Rufus against a charge of murder in 56. Under the influence of the triumvirs, he had also defended his former enemies Publius Vatinius (in August 54\u00a0BC), Marcus Aemilius Scaurus (between July and September) and Gnaeus Plancius (with the ) in September, which weakened his prestige and sparked attacks on his integrity: Luca Grillo has suggested these cases as the source of the poet Catullus's double-edged comment that Cicero was \"the best defender of anybody\".\nGovernorship of Cilicia.\nIn 51 BC he reluctantly accepted a promagistracy (as proconsul) in Cilicia for the year; there were few other former consuls eligible as a result of a legislative requirement enacted by Pompey in 52\u00a0BC specifying an interval of five years between a consulship or praetorship and a provincial command. He served as proconsul of Cilicia from May 51 BC, arriving in the provinces three months later around August.\nIn 53 BC Marcus Licinius Crassus had been defeated by the Parthians at the Battle of Carrhae. This opened the Roman East for a Parthian invasion, causing unrest in Syria and Cilicia. Cicero restored calm by his mild system of government. He discovered that a great amount of public property had been embezzled by corrupt previous governors and members of their staff, and did his utmost to restore it. Thus he greatly improved the condition of the cities. He retained the civil rights of, and exempted from penalties, the men who gave the property back. Besides this, he was extremely frugal in his outlays for staff and private expenses during his governorship, and this made him highly popular among the natives.\nBesides his activity in ameliorating the hard pecuniary situation of the province, Cicero was also creditably active in the military sphere. Early in his governorship he received information that prince Pacorus, son of Orodes II the king of the Parthians, had crossed the Euphrates, and was ravaging the Syrian countryside and had even besieged Cassius (the interim Roman commander in Syria) in Antioch. Cicero eventually marched with two understrength legions and a large contingent of auxiliary cavalry to Cassius's relief. Pacorus and his army had already given up on besieging Antioch and were heading south through Syria, ravaging the countryside again. Cassius and his legions followed them, harrying them wherever they went, eventually ambushing and defeating them near Antigonea.\nAnother large troop of Parthian horsemen was defeated by Cicero's cavalry who happened to run into them while scouting ahead of the main army. Cicero next defeated some robbers who were based on Mount Amanus and was hailed as imperator by his troops. Afterwards he led his army against the independent Cilician mountain tribes, besieging their fortress of Pindenissum. It took him 47 days to reduce the place, which fell in December. On 30 July 50 BC Cicero left the province to his brother Quintus, who had accompanied him on his governorship as his legate. On his way back to Rome he stopped in Rhodes and then went to Athens, where he caught up with his old friend Titus Pomponius Atticus and met men of great learning.\nJulius Caesar's civil war.\nCicero arrived in Rome on 4 January 49 BC. He stayed outside the pomerium, to retain his promagisterial powers: either in expectation of a triumph or to retain his independent command authority in the coming civil war. The struggle between Pompey and Julius Caesar grew more intense in 50 BC. Cicero favored Pompey, seeing him as a defender of the senate and Republican tradition, but at that time avoided openly alienating Caesar. When Caesar invaded Italy in 49 BC, Cicero fled Rome. Caesar, seeking an endorsement by a senior senator, courted Cicero's favor, but even so Cicero slipped out of Italy and traveled to Dyrrhachium where Pompey's staff was situated. Cicero traveled with the Pompeian forces to Pharsalus in Macedonia in 48 BC, though he was quickly losing faith in the competence and righteousness of the Pompeian side. Eventually, he provoked the hostility of his fellow senator Cato, who told him that he would have been of more use to the cause of the \"optimates\" if he had stayed in Rome. After Caesar's victory at the Battle of Pharsalus on 9 August, Cicero refused to take command of the Pompeian forces and continue the war. He returned to Rome, still as a promagistrate with his lictors, in 47 BC, and dismissed them upon his crossing the pomerium and renouncing his command.\nIn a letter to Varro on c.\u200920 April 46 BC, Cicero outlined his strategy under Caesar's dictatorship. Cicero, however, was taken by surprise when the \"Liberatores\" assassinated Caesar on the ides of March, 44 BC. Cicero was not included in the conspiracy, even though the conspirators were sure of his sympathy. Marcus Junius Brutus called out Cicero's name, asking him to restore the republic when he lifted his bloodstained dagger after the assassination. A letter Cicero wrote in February 43 BC to Trebonius, one of the conspirators, began, \"How I could wish that you had invited me to that most glorious banquet on the Ides of March!\" Cicero became a popular leader during the period of instability following the assassination. He had no respect for Mark Antony, who was scheming to take revenge upon Caesar's murderers. In exchange for amnesty for the assassins, he arranged for the Senate to agree not to declare Caesar to have been a tyrant, which allowed the Caesarians to have lawful support and kept Caesar's reforms and policies intact.\nOpposition to Mark Antony and death.\nIn April 43 BC, \"diehard republicans\" may have revived the ancient position of \"princeps senatus\" (leader of the senate) for Cicero. This position had been very prestigious until the constitutional reforms of Sulla in 82\u201380 BC, which removed most of its importance.\nOn the other side, Antony was consul and leader of the Caesarian faction, and unofficial executor of Caesar's public will. Relations between the two were never friendly and worsened after Cicero claimed that Antony was taking liberties in interpreting Caesar's wishes and intentions. Octavian was Caesar's adopted son and heir. After he returned to Italy, Cicero began to play him against Antony. He praised Octavian, declaring he would not make the same mistakes as his father. He attacked Antony in a series of speeches he called the \"Philippics\", named after Demosthenes's denunciations of Philip\u00a0II of Macedon. At the time, Cicero's popularity as a public figure was unrivalled.\nCicero supported Decimus Junius Brutus Albinus as governor of Cisalpine Gaul (\"Gallia Cisalpina\") and urged the Senate to name Antony an enemy of the state. The speech of Lucius Piso, Caesar's father-in-law, delayed proceedings against Antony. Antony was later declared an enemy of the state when he refused to lift the siege of Mutina, which was in the hands of Decimus Brutus. Cicero's plan to drive out Antony failed. Antony and Octavian reconciled and allied with Lepidus to form the Second Triumvirate after the successive battles of Forum Gallorum and Mutina. The alliance came into official existence with the \"lex Titia\", passed on 27 November 43 BC, which gave each triumvir a consular \"imperium\" for five years. The Triumvirate immediately began a proscription of their enemies, modeled after that of Sulla in 82 BC. Cicero and all of his contacts and supporters were numbered among the enemies of the state, even though Octavian argued for two days against Cicero being added to the list.\nCicero was one of the most viciously and doggedly hunted among the proscribed. He was viewed with sympathy by a large segment of the public and many people refused to report that they had seen him. He was caught on 7 December 43\u00a0BC leaving his villa in Formiae in a litter heading to the seaside, where he hoped to embark on a ship destined for Macedonia. When his killers \u2013 Herennius (a Centurion) and Popilius (a Tribune) \u2013 arrived, Cicero's own slaves said they had not seen him, but he was given away by Philologus, a freedman of his brother Quintus Cicero.\nAs reported by Seneca the Elder, according to the historian Aufidius Bassus, Cicero's last words are said to have been:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I go no further: approach, veteran soldier, and, if you can at least do so much properly, sever this neck.\nHe bowed to his captors, leaning his head out of the litter in a gladiatorial gesture to ease the task. By baring his neck and throat to the soldiers, he was indicating that he would not resist. According to Plutarch, Herennius first slew him, then cut off his head. On Antony's instructions his hands, which had penned the Philippics against Antony, were cut off as well; these were nailed along with his head on the Rostra in the Forum Romanum according to the tradition of Marius and Sulla, both of whom had displayed the heads of their enemies in the Forum. Cicero was the only victim of the proscriptions who was displayed in that manner. According to Cassius Dio, in a story often mistakenly attributed to Plutarch, Antony's wife Fulvia took Cicero's head, pulled out his tongue, and jabbed it repeatedly with her hairpin in final revenge against Cicero's power of speech.\nCicero's son, Marcus Tullius Cicero Minor, during his year as a consul in 30 BC, avenged his father's death, to a certain extent, when he announced to the Senate Mark Antony's naval defeat at Actium in 31 BC by Octavian.\nOctavian is reported to have praised Cicero as a patriot and a scholar of meaning in later times, within the circle of his family. However, it was Octavian's acquiescence that had allowed Cicero to be killed, as Cicero was condemned by the new triumvirate.\nCicero's career as a statesman was marked by inconsistencies and a tendency to shift his position in response to changes in the political climate. His indecision may be attributed to his sensitive and impressionable personality; he was prone to overreaction in the face of political and private change. \"Would that he had been able to endure prosperity with greater self-control, and adversity with more fortitude!\" wrote C. Asinius Pollio, a contemporary Roman statesman and historian.\nPersonal life and family.\nCicero married Terentia probably at the age of 27, in 79 BC. According to the upper-class mores of the day it was a marriage of convenience but lasted harmoniously for nearly 30 years. Terentia's family was wealthy, probably the plebeian noble house of Terenti Varrones, thus meeting the needs of Cicero's political ambitions in both economic and social terms. She had a half-sister named Fabia, who as a child had become a Vestal Virgin, a great honour. Terentia was a strong-willed woman and (citing Plutarch) \"took more interest in her husband's political career than she allowed him to take in household affairs\".\nIn the 50s BC, Cicero's letters to Terentia became shorter and colder. He complained to his friends that Terentia had betrayed him but did not specify in which sense. Perhaps the marriage could not outlast the strain of the political upheaval in Rome, Cicero's involvement in it, and various other disputes between the two. The divorce appears to have taken place in 51 BC or shortly before. In 46 or 45 BC, Cicero married a young girl, Publilia, who had been his ward. It is thought that Cicero needed her money, particularly after having to repay the dowry of Terentia, who came from a wealthy family.\nAlthough his marriage to Terentia was one of convenience, it is commonly known that Cicero held great love for his daughter Tullia. When she suddenly became ill in February 45 BC and died after having seemingly recovered from giving birth to a son in January, Cicero was stunned. \"I have lost the one thing that bound me to life,\" he wrote to Atticus. Atticus told him to come for a visit during the first weeks of his bereavement, so that he could comfort him when his pain was at its greatest. In Atticus's large library, Cicero read everything that the Greek philosophers had written about overcoming grief, \"but my sorrow defeats all consolation.\" Caesar and Brutus, as well as Servius Sulpicius Rufus, sent him letters of condolence.\nCicero hoped that his son Marcus would become a philosopher like him, but Marcus himself wished for a military career. He joined the army of Pompey in 49 BC, and after Pompey's defeat at Pharsalus 48 BC, he was pardoned by Caesar. Cicero sent him to Athens to study as a disciple of the peripatetic philosopher Kratippos in 48 BC, but he used this absence from \"his father's vigilant eye\" to \"eat, drink, and be merry.\" After Cicero's death, he joined the army of the \"Liberatores\" but was later pardoned by Augustus. Augustus's bad conscience for having given in to Cicero's being put on the proscription list during the Second Triumvirate led him to aid considerably Marcus Minor's career. He became an augur and was nominated consul in 30 BC together with Augustus. As such, he was responsible for revoking the honors of Mark Antony, who was responsible for the proscription and could in this way take revenge. Later he was appointed proconsul of Syria and the province of Asia.\nLegacy.\nCicero has been traditionally considered the master of Latin prose, with Quintilian declaring that Cicero was \"not the name of a man, but of eloquence itself.\" The English words \"Ciceronian\" (meaning \"eloquent\") and \"cicerone\" (meaning \"local guide\") derive from his name. He is credited with transforming Latin from a modest utilitarian language into a versatile literary medium capable of expressing abstract and complicated thoughts with clarity. Julius Caesar praised Cicero's achievement by saying \"it is more important to have greatly extended the frontiers of the Roman spirit than the frontiers of the Roman empire\". According to John William Mackail, \"Cicero's unique and imperishable glory is that he created the language of the civilized world, and used that language to create a style which nineteen centuries have not replaced, and in some respects have hardly altered.\"\nCicero was also an energetic writer with an interest in a wide variety of subjects, in keeping with the Hellenistic philosophical and rhetorical traditions in which he was trained. The quality and ready accessibility of Ciceronian texts favored very wide distribution and inclusion in teaching curricula, as suggested by a graffito at Pompeii, admonishing: \"You will like Cicero, or you will be whipped\".\nCicero was greatly admired by influential Church Fathers such as Augustine of Hippo, who credited Cicero's lost \"Hortensius\" for his eventual conversion to Christianity, and St. Jerome, who had a feverish vision in which he was accused of being \"follower of Cicero and not of Christ\" before the judgment seat.\nThis influence further increased after the Early Middle Ages in Europe, where more of his writings survived than any other Latin author's. Medieval philosophers were influenced by Cicero's writings on natural law and innate rights.\nPetrarch's rediscovery of Cicero's letters provided the impetus for searches for ancient Greek and Latin writings scattered throughout European monasteries, and the subsequent rediscovery of classical antiquity led to the Renaissance. Subsequently, Cicero became synonymous with classical Latin to such an extent that a number of humanist scholars began to assert that no Latin word or phrase should be used unless it appeared in Cicero's works, a stance criticised by Erasmus.\nHis voluminous correspondence, much of it addressed to his friend Atticus, has been especially influential, introducing the art of refined letter writing to European culture. Cornelius Nepos, the first century BC biographer of Atticus, remarked that Cicero's letters contained such a wealth of detail \"concerning the inclinations of leading men, the faults of the generals, and the revolutions in the government\" that their reader had little need for a history of the period.\nAmong Cicero's admirers were Desiderius Erasmus, Martin Luther, and John Locke. Following the invention of Johannes Gutenberg's printing press, \"De Officiis\" was the second book printed in Europe, after the Gutenberg Bible. Scholars note Cicero's influence on the rebirth of religious toleration in the 17th century.\nCicero was especially popular with the Philosophes of the 18th century, including Edward Gibbon, Diderot, David Hume, Montesquieu, and Voltaire. Gibbon wrote of his first experience reading the author's collective works thus: \"I tasted the beauty of the language; I breathed the spirit of freedom; and I imbibed from his precepts and examples the public and private sense of a man...after finishing the great author, a library of eloquence and reason, I formed a more extensive plan of reviewing the Latin classics...\"\nVoltaire called Cicero \"the greatest as well as the most elegant of Roman philosophers\" and even staged a play based on Cicero's role in the Catilinarian conspiracy, called \"Rome Sauv\u00e9e, ou Catilina\", to \"make young people who go to the theatre acquainted with Cicero.\" Voltaire was spurred to pen the drama as a rebuff to his rival Claude Prosper Jolyot de Cr\u00e9billon's own play \"Catilina\", which had portrayed Cicero as a coward and villain who hypocritically married his own daughter to Catiline.\nMontesquieu produced his \"Discourse on Cicero\" in 1717, in which he heaped praise on the author because he rescued \"philosophy from the hands of scholars, and freed it from the confusion of a foreign language\". Montesquieu went on to declare that Cicero was \"of all the ancients, the one who had the most personal merit, and whom I would prefer to resemble.\"\nCicero the republican inspired the Founding Fathers of the United States and the revolutionaries of the French Revolution. John Adams said, \"As all the ages of the world have not produced a greater statesman and philosopher united than Cicero, his authority should have great weight.\" Thomas Jefferson names Cicero as one of a handful of major figures who contributed to a tradition \"of public right\" that informed his draft of the Declaration of Independence and shaped American understandings of \"the common sense\" basis for the right of revolution. Camille Desmoulins said of the French republicans in 1789 that they were \"mostly young people who, nourished by the reading of Cicero at school, had become passionate enthusiasts for liberty\".\nIn the modern era, American libertarian Jim Powell starts his history of liberty with the sentence: \"Marcus Tullius Cicero expressed principles that became the bedrock of liberty in the modern world.\"\nLikewise, no other ancient personality has inspired as much venomous dislike as Cicero, especially in more modern times. His commitment to the values of the Republic accommodated a hatred of the poor and persistent opposition to the advocates and mechanisms of popular representation. Friedrich Engels referred to him as \"the most contemptible scoundrel in history\" for upholding republican \"democracy\" while at the same time denouncing land and class reforms. Cicero has faced criticism for exaggerating the democratic qualities of republican Rome, and for defending the Roman oligarchy against the popular reforms of Caesar. Michael Parenti admits Cicero's abilities as an orator, but finds him a vain, pompous and hypocritical personality who, when it suited him, could show public support for popular causes that he privately despised. Parenti presents Cicero's prosecution of the Catiline conspiracy as legally flawed at least, and possibly unlawful.\nCicero also had an influence on modern astronomy. Nicolaus Copernicus, searching for ancient views on earth motion, said that he \"first\u00a0... found in Cicero that Hicetas supposed the earth to move.\"\nNotably, \"Cicero\" was the name attributed to size 12 font in typesetting table drawers. For ease of reference, type sizes 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, and 20 were all given different names.\nWorks.\nCicero was declared a righteous pagan by the Early Church. Subsequent Roman and medieval Christian writers quoted liberally from his works \"De re publica\" (\"On the Republic\") and \"De Legibus\" (\"On the Laws\"), and much of his work has been recreated from these surviving fragments. Cicero also articulated an early, abstract conceptualization of rights, based on ancient law and custom. Of Cicero's books, six on rhetoric have survived, as well as parts of seven on philosophy. Of his speeches, 88 were recorded, but only 52 survive.\nIn archaeology.\nCicero's great repute in Italy has led to numerous ruins being identified as having belonged to him, though none have been substantiated with absolute certainty. In Formia, two Roman-era ruins are popularly believed to be Cicero's mausoleum, the \"Tomba di Cicerone\", and the villa where he was assassinated in 43 BC. The latter building is centered around a central hall with Doric columns and a coffered vault, with a separate nymphaeum, on five acres of land near Formia. A modern villa was built on the site after the Rubino family purchased the land from Ferdinand II of the Two Sicilies in 1868. Cicero's supposed tomb is a 24-meter (79 feet) tall tower on an \"opus quadratum\" base on the ancient Via Appia outside of Formia. Some suggest that it is not in fact Cicero's tomb, but a monument built on the spot where Cicero was intercepted and assassinated while trying to reach the sea.\nIn Pompeii, a large villa excavated in the mid 18th century just outside the Herculaneum Gate was widely believed to have been Cicero's, who was known to have owned a holiday villa in Pompeii he called his \"Pompeianum\". The villa was stripped of its fine frescoes and mosaics and then re-buried after 1763 \u2013 it has yet to be re-excavated. However, contemporaneous descriptions of the building from the excavators combined with Cicero's own references to his \"Pompeianum\" differ, making it unlikely that it is Cicero's villa.\nIn Rome, the location of Cicero's house has been roughly identified from excavations of the Republican-era stratum on the northwestern slope of the Palatine Hill. Cicero's \"domus\" has long been known to have stood in the area, according to his own descriptions and those of later authors, but there is some debate about whether it stood near the base of the hill, very close to the Roman Forum, or nearer to the summit. During his life the area was the most desirable in Rome, densely occupied with Patrician houses including the \"Domus Publica\" of Julius Caesar and the home of Cicero's mortal enemy Clodius.\nNotable fictional portrayals.\nIn Dante's 1320 poem the \"Divine Comedy\", the author encounters Cicero, among other philosophers, in Limbo. Ben Jonson dramatised the conspiracy of Catiline in his play \"Catiline His Conspiracy\", featuring Cicero as a character. Cicero also appears as a minor character in William Shakespeare's play \"Julius Caesar\".\nCicero was portrayed on the motion picture screen by British actor Alan Napier in the 1953 film \"Julius Caesar\", based on Shakespeare's play. He has also been played by such noted actors as Michael Hordern (in \"Cleopatra\"), and Andr\u00e9 Morell (in the 1970 \"Julius Caesar\"). Most recently, Cicero was portrayed by David Bamber in the HBO series \"Rome\" (2005\u20132007) and appeared in both seasons.\nIn the historical novel series \"Masters of Rome\", Colleen McCullough presents a not-so-flattering depiction of Cicero's career, showing him struggling with an inferiority complex and vanity, morally flexible and fatally indiscreet, while his rival Julius Caesar is shown in a more approving light. Cicero is portrayed as a hero in the novel \"A Pillar of Iron\" by Taylor Caldwell (1965). Robert Harris' novels \"Imperium\", \"Lustrum\" (published under the name \"Conspirata\" in the United States) and \"Dictator\" comprise a three-part series based on the life of Cicero. In these novels Cicero's character is depicted in a more favorable way than in those of McCullough, with his positive traits equaling or outweighing his weaknesses (while conversely Caesar is depicted as more sinister than in McCullough). Cicero is a major recurring character in the \"Roma Sub Rosa\" series of mystery novels by Steven Saylor. He also appears several times as a peripheral character in John Maddox Roberts' \"SPQR\" series.\nSamuel Barnett portrays Cicero in a 2017 audio drama series pilot produced by Big Finish Productions. A full series was released the following year. All episodes are written by David Llewellyn and directed and produced by Scott Handcock.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nModern sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nAncient sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nWorks by Cicero\nBiographies and descriptions of Cicero's time\nPlutarch's biography of Cicero contained in the "}
{"id": "6047", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=6047", "title": "Consul", "text": "Magistrate or title in various republics and city-states\nConsul (abbrev. \"cos.\"; Latin plural \"consules\") was the title of one of the two chief magistrates of the Roman Republic, and subsequently also an important title under the Roman Empire. The title was used in other European city-states through antiquity and the Middle Ages, in particular in the Republics of Genoa and Pisa, then revived in modern states, notably in the First French Republic. The related adjective is consular, from the Latin \"consularis\".\nThis usage contrasts with modern terminology, where a consul is a type of diplomat.\nRoman consul.\nA consul held the highest elected political office of the Roman Republic (509 to 27 BC), and ancient Romans considered the consulship the highest level of the \"cursus honorum\" (an ascending sequence of public offices to which politicians aspired). Consuls were elected to office and held power for one year. There were always two consuls in power at any time.\nOther uses in antiquity.\nPrivate sphere.\nIt was not uncommon for an organization under Roman private law to copy the terminology of state and city institutions for its own statutory agents. The founding statute, or contract, of such an organisation was called \"lex\", 'law'. The people elected each year were patricians, members of the upper class.\nCity-states.\nWhile many cities, including the Gallic states and the Carthaginian Republic, had a double-headed chief magistracy, another title was often used, such as the Punic \"sufet\", \"Duumvir\", or native styles like \"Meddix\".\nMedieval city-states, communes and municipalities.\nRepublic of Genoa.\nThe city-state of Genoa, unlike ancient Rome, bestowed the title of \"consul\" on various state officials, not necessarily restricted to the highest. Among these were Genoese officials stationed in various Mediterranean ports, whose role included helping Genoese merchants and sailors in difficulties with the local authorities. Great Britain reciprocated by appointing consuls to Genoa from 1722. This institution, with its name, was later emulated by other powers and is reflected in the modern usage of the word (see Consul (representative)).\nRepublic of Pisa.\nIn addition to the Genoese Republic, the Republic of Pisa also took the form of \"Consul\" in the early stages of its government. The Consulate of the Republic of Pisa was the major government institution present in Pisa from 1087 to 1189. Despite losing space within the government since 1190 in favor of the Podest\u00e0, for some periods of the 13th century some citizens were again elected as consuls.\nOther uses in the Medieval period.\nThroughout most of southern France, a consul ( or \"\") was an office equivalent to the \u00e9chevins of the north and roughly similar with English aldermen. The most prominent were those of Bordeaux and Toulouse, which came to be known as jurats and capitouls, respectively. The capitouls of Toulouse were granted transmittable nobility. In many other smaller towns the first consul was the equivalent of a mayor today, assisted by a variable number of secondary consuls and jurats. His main task was to levy and collect tax.\nThe Dukes of Gaeta often used also the title of \"consul\" in its Greek form \"Hypatos\" (see List of Hypati and Dukes of Gaeta).\nFrench Revolution.\nFrench Republic 1799\u20131804.\nAfter Napoleon Bonaparte staged a coup against the Directory government in November 1799, the French Republic adopted a constitution which conferred executive powers upon three consuls, elected for a period of ten years. In reality, the first consul, Bonaparte, dominated his two colleagues and held supreme power, soon making himself consul for life (1802) and eventually, in 1804, emperor.\nThe office was held by:\nBolognese Republic, 1796.\nThe short-lived Bolognese Republic, proclaimed in 1796 as a French client republic in the Central Italian city of Bologna, had a government consisting of nine consuls and its head of state was the \"Presidente del Magistrato\", i.e., chief magistrate, a presiding office held for four months by one of the consuls.\nRoman Republic, 1798\u20131800.\nThe French-sponsored Roman Republic (15 February 1798 \u2013 23 June 1800) was headed by multiple consuls:\nConsular rule was interrupted by the Neapolitan occupation (27 November \u2013 12 December 1798), which installed a Provisional Government:\nRome was occupied by France (11 July \u2013 28 September 1799) and again by Naples (30 September 1799 \u2013 23 June 1800), bringing an end to the Roman Republic.\nRevolutionary Greece, 1821.\nAmong the many petty local republics that were formed during the first year of the Greek Revolution, prior to the creation of a unified Provisional Government at the First National Assembly at Epidaurus, were:\n\"Note: in Greek, the term for \"consul\" is \"hypatos\" (\u1f55\u03c0\u03b1\u03c4\u03bf\u03c2), which translates as \"supreme one\", and hence does not necessarily imply a joint office.\"\nParaguay, 1813\u20131844.\nIn between a series of juntas and various other short-lived regimes, the young republic was governed by \"consuls of the republic\", with two consuls alternating in power every 4 months:\nAfter a few presidents of the Provisional Junta, there were again consuls of the republic, 14 March 1841 \u2013 13 March 1844 (ruling jointly, but occasionally styled \"first consul\", \"second consul\"): Carlos Antonio L\u00f3pez Ynsfr\u00e1n (b. 1792 \u2013 d. 1862) + Mariano Roque Alonzo Romero (d. 1853) (the lasts of the aforementioned juntistas, Commandant-General of the Army)\nThereafter all republican rulers were styled \"president\".\nModern uses of the term.\nIn modern terminology, a consul is a type of diplomat. The \"American Heritage Dictionary\" defines consul as \"an official appointed by a government to reside in a foreign country and represent its interests there.\" \"The Devil's Dictionary\" defines Consul as \"in American politics, a person who having failed to secure an office from the people is given one by the Administration on condition that he leave the country\".\nIn most governments, the consul is the head of the consular section of an embassy, and is responsible for all consular services such as immigrant and non-immigrant visas, passports, and citizen services for expatriates living or traveling in the host country.\nA less common modern usage is when the consul of one country takes a governing role in the host country.\nSee also.\nDifferently named, but same function\nModern UN System \nSources and references.\nSpecific\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6050", "revid": "50564026", "url": "https://en.wikipedia.org/wiki?curid=6050", "title": "List of equations in classical mechanics", "text": "Classical mechanics is the branch of physics used to describe the motion of macroscopic objects. It is the most familiar of the theories of physics. The concepts it covers, such as mass, acceleration, and force, are commonly used and known. The subject is based upon a three-dimensional Euclidean space with fixed axes, called a frame of reference. The point of concurrency of the three axes is known as the origin of the particular space.\nClassical mechanics utilises many equations\u2014as well as other mathematical concepts\u2014which relate various physical quantities to one another. These include differential equations, manifolds, Lie groups, and ergodic theory. This article gives a summary of the most important of these.\nThis article lists equations from Newtonian mechanics, see analytical mechanics for the more general formulation of classical mechanics (which includes Lagrangian and Hamiltonian mechanics).\nClassical mechanics.\nGeneral energy definitions.\nEvery conservative force has a potential energy. By following two principles one can consistently assign a non-relative value to \"U\":\nKinematics.\nIn the following rotational definitions, the angle can be any angle about the specified axis of rotation. It is customary to use \"\u03b8\", but this does not have to be the polar angle used in polar coordinate systems. The unit axial vector\nformula_1\ndefines the axis of rotation, formula_2 = unit vector in direction of r, formula_3 = unit vector tangential to the angle.\nDynamics.\nPrecession.\nThe precession angular speed of a spinning top is given by:\nformula_4\nwhere \"w\" is the weight of the spinning flywheel.\nEnergy.\nThe mechanical work done by an external agent on a system is equal to the change in kinetic energy of the system:\nGeneral work-energy theorem (translation and rotation).\nThe work done \"W\" by an external agent which exerts a force F (at r) and torque \u03c4 on an object along a curved path \"C\" is:\nformula_5\nwhere \u03b8 is the angle of rotation about an axis defined by a unit vector n.\nKinetic energy.\nThe change in kinetic energy for an object initially traveling at speed formula_6 and later at speed formula_7 is:\nformula_8\nElastic potential energy.\nFor a stretched spring fixed at one end obeying Hooke's law, the elastic potential energy is\nformula_9\nwhere \"r\"2 and \"r\"1 are collinear coordinates of the free end of the spring, in the direction of the extension/compression, and k is the spring constant.\nEuler's equations for rigid body dynamics.\nEuler also worked out analogous laws of motion to those of Newton, see Euler's laws of motion. These extend the scope of Newton's laws to rigid bodies, but are essentially the same as above. A new equation Euler formulated is:\nformula_10\nwhere I is the moment of inertia tensor.\nGeneral planar motion.\nThe previous equations for planar motion can be used here: corollaries of momentum, angular momentum etc. can immediately follow by applying the above definitions. For any object moving in any path in a plane,\nformula_11\nthe following general results apply to the particle.\nCentral force motion.\nFor a massive body moving in a central potential due to another object, which depends only on the radial separation between the centers of masses of the two objects, the equation of motion is:\nformula_12\nEquations of motion (constant acceleration).\nThese equations can be used only when acceleration is constant. If acceleration is not constant then the general calculus equations above must be used, found by integrating the definitions of position, velocity and acceleration (see above).\nGalilean frame transforms.\nFor classical (Galileo-Newtonian) mechanics, the transformation law from one inertial or accelerating (including rotation) frame (reference frame traveling at constant velocity - including zero) to another is the Galilean transform.\nUnprimed quantities refer to position, velocity and acceleration in one frame F; primed quantities refer to position, velocity and acceleration in another frame F' moving at translational velocity V or angular velocity \u03a9 relative to F. Conversely F moves at velocity (\u2014V or \u2014\u03a9) relative to F'. The situation is similar for relative accelerations.\nMechanical oscillators.\nSHM, DHM, SHO, and DHO refer to simple harmonic motion, damped harmonic motion, simple harmonic oscillator and damped harmonic oscillator respectively."}
{"id": "6051", "revid": "50299307", "url": "https://en.wikipedia.org/wiki?curid=6051", "title": "Cursus honorum", "text": "Sequential order of public offices held by politicians in Ancient Rome\nThe , or more colloquially 'ladder of offices'; ) was the sequential order of public offices held by aspiring politicians in the Roman Republic and the early Roman Empire. It was designed for men of senatorial rank. The comprised a mixture of military and political administration posts; the ultimate prize for winning election to each \"rung\" in the sequence was to become one of the two consuls in a given year.\nThese rules were altered and flagrantly ignored in the course of the last century of the Republic. For example, Gaius Marius held consulships for five years in a row between 104 and 100\u00a0BC. He was consul seven times in all, also serving in 107 and 86. Officially presented as opportunities for public service, the offices often became mere opportunities for self-aggrandizement. The constitutional reforms of Sulla between 82 and 79\u00a0BC required a ten-year interval before holding the same office again for another term.\nTo have held each office at the youngest possible age (, 'in his year') was considered a great political success. For instance, to miss out on a praetorship at 39 meant that one could not become consul at 42. Cicero expressed extreme pride not only in being a ('new man'; comparable to a \"self-made man\") who became consul even though none of his ancestors had ever served as a consul, but also in having become consul \"in his year\".\nMilitary service.\nPrior to entering political life and the \"cursus honorum\", a young man of senatorial rank was expected to serve around ten years of military duty. The years of service were intended to be mandatory in order to qualify for political office.\nAdvancement and honors would improve his political prospects, and a successful military career might culminate in the office of military tribune, to which 24 men were elected by the Tribal Assembly each year. The rank of military tribune is sometimes described as the first office of the \"cursus honorum\".\nQuaestor.\nThe first official post was that of quaestor. Ever since the reforms of Sulla, candidates had to be at least 30 years old to hold the office. From the time of Augustus onwards, twenty quaestors served in the financial administration at Rome or as second-in-command to a governor in the provinces. They could also serve as the paymaster for a legion.\nAedile.\nAt 36 years of age, a promagistrate could stand for election to one of the aediles (pronounced , from \"aedes\", \"temple edifice\") positions. Of these aediles, two were plebeian and two were patrician, with the patrician aediles called curule aediles. The plebeian aediles were elected by the Plebeian Council and the curule aediles were either elected by the Tribal Assembly or appointed by the reigning consul. The aediles had administrative responsibilities in Rome. They had to take care of the temples (whence their title, from the Latin \"aedes\", \"temple\"), organize games, and be responsible for the maintenance of the public buildings in Rome. Moreover, they took charge of Rome's water and food supplies; in their capacity as market superintendents, they served sometimes as judges in mercantile affairs.\nThe aedile was the supervisor of public works; the words \"edifice\" and \"edification\" stem from the same root. He oversaw the public works, temples and markets. Therefore, the aediles would have been in some cooperation with the current censors, who had similar or related duties. Also, they oversaw the organization of festivals and games (\"ludi\"), which made this a very sought-after office for a career minded politician of the late Republic, as it was a good means of gaining popularity by staging spectacles.\nCurule aediles were added at a later date in the 4th century BC; their duties do not differ substantially from plebeian aediles. However, unlike plebeian aediles, curule aediles were allowed certain symbols of rank\u2014the \"sella curulis\" or curule chair, for example\u2014and only patricians could stand for election to curule aedile. This later changed, and both plebeians and patricians could stand for curule aedileship.\nThe elections for curule aedile were at first alternated between patricians and plebeians, until late in the 2nd century BC, when the practice was abandoned and both classes became free to run during all years.\nWhile part of the \"cursus honorum\", this step was optional and not required to hold future offices. Though the office was usually held after the quaestorship and before the praetorship, there are some cases with former praetors serving as aediles.\nPraetor.\nAfter serving either as quaestor or as aedile, a man of 39 years could run for praetor. During the reign of Augustus this requirement was lowered to 30, at the request of Gaius Maecenas. The number of praetors elected varied through history, generally increasing with time. During the republic, six or eight were generally elected each year to serve judicial functions throughout Rome and other governmental responsibilities. In the absence of the consuls, a praetor would be given command of the garrison in Rome or in Italy. Also, a praetor could exercise the functions of the consuls throughout Rome, but their main function was that of a judge. They would preside over trials involving criminal acts, grant court orders and validate \"illegal\" acts as acts of administering justice. A praetor was escorted by six lictors, and wielded \"imperium\". After a term as praetor, the magistrate could serve as a provincial governor with the title of propraetor, wielding \"propraetor imperium\", commanding the province's legions, and possessing ultimate authority within his province(s).\nTwo of the praetors were more prestigious than the others. The first was the Praetor Peregrinus, who was the chief judge in trials involving one or more foreigners. The other was the Praetor Urbanus, the chief judicial office in Rome. He had the power to overturn any verdict by any other courts, and served as judge in cases involving criminal charges against provincial governors. The Praetor Urbanus was not allowed to leave the city for more than ten days. If one of these two praetors was absent from Rome, other praetors can perform their duties.\nConsul.\nThe office of consul was the most prestigious of all of the offices on the \"cursus honorum\", and represented the summit of a successful career. The minimum age was 42. Years were identified by the names of the two consuls elected for a particular year; for instance, \"M. Messalla et M. Pisone consulibus\", \"in the consulship of Messalla and Piso\", dates an event to 61 BC. Consuls were responsible for the city's political agenda, commanded large-scale armies and controlled important provinces. The consuls served for only a year (a restriction intended to limit the amassing of power by individuals) and could only rule when they agreed, because each consul could veto the other's decision.\nThe consuls would alternate monthly as the chairman of the Senate. They also were the supreme commanders in the Roman army, with each being granted two legions during their consular year. Consuls also exercised the highest juridical power in the Republic, being the only office with the power to override the decisions of the Praetor Urbanus. Only laws and the decrees of the Senate or the People's assembly limited their powers, and only the veto of a fellow consul or a tribune of the plebs could supersede their decisions.\nA consul was escorted by twelve lictors, held \"imperium\" and wore the toga \"praetexta\". Because the consul was the highest executive office within the Republic, they had the power to veto any action or proposal by any other magistrate, save that of the Tribune of the Plebs. After a consulship, a consul was assigned one of the more important provinces and acted as the governor in the same way that a propraetor did, only owning proconsular \"imperium\". A second consulship could only be attempted after an interval of 10 years to prevent one man holding too much power.\nGovernor.\nAlthough not part of the \"cursus honorum\", upon completing a term as either praetor or consul, an officer was required to serve a term as propraetor and proconsul, respectively, in one of Rome's many provinces. These propraetors and proconsuls held near autocratic authority within their selected province or provinces. Because each governor held equal \"imperium\" to the equivalent magistrate, they were escorted by the same number of lictors (12) and could only be vetoed by a reigning consul or praetor. Their abilities to govern were only limited by the decrees of the Senate or the people's assemblies, and the Tribune of the Plebs was unable to veto their acts as long as the governor remained at least a mile outside of Rome.\nCensor.\nAfter a term as consul, the final step in the \"cursus honorum\" was the office of \"censor\". This was the only office in the Roman Republic whose term was a period of eighteen months instead of the usual twelve. Censors were elected every five years and although the office held no military \"imperium\", it was considered a great honour. The censors took a regular census of the people and then apportioned the citizens into voting classes on the basis of income and tribal affiliation. The censors enrolled new citizens in tribes and voting classes as well. The censors were also in charge of the membership roll of the Senate, every five years adding new senators who had been elected to the requisite offices. Censors could also remove unworthy members from the Senate. This ability was lost during the dictatorship of Sulla. Censors were also responsible for construction of public buildings and the moral status of the city.\nCensors also had financial duties, in that they had to put out to tender projects that were to be financed by the state. Also, the censors were in charge of the leasing out of conquered land for public use and auction. Though this office owned no \"imperium\", meaning no lictors for protection, they were allowed to wear the toga \"praetexta\".\nTribune of the Plebs.\nThe office of Tribune of the Plebs was an important step in the political career of plebeians. Patricians could not hold the office. They were not an official step in the \"cursus honorum\". The Tribune was an office first created to protect the right of the common man in Roman politics and served as the head of the Plebeian Council. In the mid-to-late Republic, however, plebeians were often just as, and sometimes more, wealthy and powerful than patricians. Those who held the office were granted sacrosanctity (the right to be legally protected from any physical harm), the power to rescue any plebeian from the hands of a patrician magistrate, and the right to veto any act or proposal of any magistrate, including another tribune of the people and the consuls. The tribune also had the power to exercise capital punishment against any person who interfered in the performance of his duties. The tribunes could even convene a Senate meeting and lay legislation before it and arrest magistrates. Their houses had to remain open for visitors even during the night, and they were not allowed to be more than a day's journey from Rome. Due to their unique power of sacrosanctity, the Tribune had no need for lictors for protection and owned no \"imperium\", nor could they wear the toga \"praetexta\". For a period after Sulla's reforms, a person who had held the office of Tribune of the Plebs could no longer qualify for any other office, and the powers of the tribunes were more limited, but these restrictions were subsequently lifted.\n\"Princeps senatus\".\nAnother office not officially a step in the \"cursus honorum\" was the \"princeps senatus\", an extremely prestigious office for a patrician. The \"princeps senatus\" served as the leader of the Senate and was chosen to serve a five-year term by each pair of Censors every five years. Censors could, however, confirm a \"princeps senatus\" for a period of another five years. The \"princeps senatus\" was chosen from all Patricians who had served as a Consul, with former Censors usually holding the office. The office originally granted the holder the ability to speak first at session on the topic presented by the presiding magistrate, but eventually gained the power to open and close the senate sessions, decide the agenda, decide where the session should take place, impose order and other rules of the session, meet in the name of the senate with embassies of foreign countries, and write in the name of the senate letters and dispatches. This office, like the Tribune, did not own \"imperium\", was not escorted by lictors, and could not wear the \"toga praetexta\".\nDictator and \"magister equitum\".\nOf all the offices within the Roman Republic, none granted as much power and authority as the position of dictator, known as the Master of the People. In times of emergency, the Senate would declare that a dictator was required, and the current consuls would appoint a dictator. This was the only decision that could not be vetoed by the Tribune of the Plebs. The dictator was the sole exception to the Roman legal principles of having multiple magistrates in the same office and being legally able to be held to answer for actions in office. Essentially by definition, only one dictator could serve at a time, and no dictator could ever be held legally responsible for any action during his time in office for any reason.\nThe dictator was the highest magistrate in degree of \"imperium\" and was attended by twenty-four lictors (as were the former Kings of Rome). Although his term lasted only six months instead of twelve (except for the Dictatorships of Sulla and Caesar), all other magistrates reported to the dictator (except for the tribunes of the plebs \u2013 although they could not veto any of the dictator's acts), granting the dictator absolute authority in both civil and military matters throughout the Republic. The dictator was free from the control of the Senate in all that he did, could execute anyone without a trial for any reason, and could ignore any law in the performance of his duties. The dictator was the sole magistrate under the Republic that was truly independent in discharging his duties. All of the other offices were extensions of the Senate's executive authority and thus answerable to the Senate. Since the dictator exercised his own authority, he did not suffer this limitation, which was the cornerstone of the office's power.\nWhen a dictator entered office, he appointed to serve as his second-in-command a \"magister equitum\", the Master of the Horse, whose office ceased to exist once the dictator left office. The \"magister equitum\" held \"praetorian imperium\", was attended by six lictors, and was charged with assisting the dictator in managing the State. When the dictator was away from Rome, the \"magister equitum\" usually remained behind to administer the city. The \"magister equitum\", like the dictator, had unchallengeable authority in all civil and military affairs, with his decisions only being overturned by the dictator himself.\nThe dictatorship was definitively abolished in 44 BC after the assassination of Gaius Julius Caesar (\"Lex Antonia\").\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6053", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=6053", "title": "C.E.", "text": ""}
{"id": "6054", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=6054", "title": "C.E", "text": ""}
{"id": "6055", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6055", "title": "CD ROM", "text": ""}
{"id": "6056", "revid": "50967060", "url": "https://en.wikipedia.org/wiki?curid=6056", "title": "Continental drift", "text": "Movement of Earth's continents relative to each other\nContinental drift is a highly supported scientific theory, originating in the early 20th century, stating that Earth's continents move or drift relative to each other over geologic time. The theory of continental drift has since been validated and incorporated into the science of plate tectonics, which studies the movement of the continents as they ride on plates of the Earth's lithosphere.\nThe speculation that continents might have \"drifted\" was first put forward by Abraham Ortelius in 1596. A pioneer of the modern view of mobilism was the Austrian geologist Otto Ampferer. The concept was independently and more fully developed by Alfred Wegener in his 1915 publication, \"The Origin of Continents and Oceans\". However, at that time his hypothesis was rejected by many, largely because there was no known geological mechanism which could propel such massive movements. In 1931, the English geologist Arthur Holmes proposed mantle convection for that mechanism, which is now known to be powered by radioactive decay and primordial heat .\nHistory.\nEarly history.\nAbraham Ortelius , Theodor Christoph Lilienthal (1756), Alexander von Humboldt (1801 and 1845), Antonio Snider-Pellegrini , and others had noted earlier that the shapes of continents on opposite sides of the Atlantic Ocean (most notably, Africa and South America) seem to fit together. W. J. Kious described Ortelius's thoughts in this way:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Abraham Ortelius in his work Thesaurus Geographicus ... suggested that the Americas were \"torn away from Europe and Africa ... by earthquakes and floods\" and went on to say: \"The vestiges of the rupture reveal themselves if someone brings forward a map of the world and considers carefully the coasts of the three [continents].\"\nIn 1889, Alfred Russel Wallace remarked, \"It was formerly a very general belief, even amongst geologists, that the great features of the earth's surface, no less than the smaller ones, were subject to continual mutations, and that during the course of known geological time the continents and great oceans had, again and again, changed places with each other.\" He quotes Charles Lyell as saying, \"Continents, therefore, although permanent for whole geological epochs, shift their positions entirely in the course of ages.\" and claims that the first to throw doubt on this was James Dwight Dana in 1849.\nIn his \"Manual of Geology\" (1863), Dana wrote, \"The continents and oceans had their general outline or form defined in earliest time. This has been proved with regard to North America from the position and distribution of the first beds of the Lower Silurian, \u2013 those of the Potsdam epoch. The facts indicate that the continent of North America had its surface near tide-level, part above and part below it (p.196); and this will probably be proved to be the condition in Primordial time of the other continents also. And, if the outlines of the continents were marked out, it follows that the outlines of the oceans were no less so\". Dana was enormously influential in America\u2014his \"Manual of Mineralogy\" is still in print in revised form\u2014and the theory became known as the \"Permanence theory\".\nThis appeared to be confirmed by the exploration of the deep sea beds conducted by the \"Challenger\" expedition, 1872\u20131876, which showed that contrary to expectation, land debris brought down by rivers to the ocean is deposited comparatively close to the shore on what is now known as the continental shelf. This suggested that the oceans were a permanent feature of the Earth's surface, rather than them having \"changed places\" with the continents.\nEduard Suess had proposed a supercontinent Gondwana in 1885 and the Tethys Ocean in 1893, assuming a land-bridge between the present continents submerged in the form of a geosyncline, and John Perry had written an 1895 paper proposing that the Earth's interior was fluid, and disagreeing with Lord Kelvin on the age of the Earth.\nWegener and his predecessors.\nApart from the earlier speculations mentioned above, the idea that the American continents had once formed a single landmass with Eurasia and Africa was postulated by several scientists before Alfred Wegener's 1912 paper. Although Wegener's theory was formed independently and was more complete than those of his predecessors, Wegener later credited a number of past authors with similar ideas: Franklin Coxworthy (between 1848 and 1890), Roberto Mantovani (between 1889 and 1909), William Henry Pickering (1907) and Frank Bursley Taylor (1908).\nThe similarity of southern continent geological formations had led Roberto Mantovani to conjecture in 1889 and 1909 that all the continents had once been joined into a supercontinent; Wegener noted the similarity of Mantovani's and his own maps of the former positions of the southern continents. In Mantovani's conjecture, this continent broke due to volcanic activity caused by thermal expansion, and the new continents drifted away from each other because of further expansion of the rip-zones, where the oceans now lie. This led Mantovani to propose a now-discredited Expanding Earth theory.\nContinental drift without expansion was proposed by Frank Bursley Taylor, who suggested in 1908 (published in 1910) that the continents were moved into their present positions by a process of \"continental creep\", later proposing a mechanism of increased tidal forces during the Cretaceous dragging the crust towards the equator. He was the first to realize that one of the effects of continental motion would be the formation of mountains, attributing the formation of the Himalayas to the collision between the Indian subcontinent with Asia. Wegener said that of all those theories, Taylor's had the most similarities to his own. For a time in the mid-20th century, the theory of continental drift was referred to as the \"Taylor-Wegener hypothesis\".\nAlfred Wegener first presented his hypothesis to the German Geological Society on 6 January 1912. He proposed that the continents had once formed a single landmass, which he called Pangaea, before breaking apart and drifting to their present locations.\nWegener was the first to use the phrase \"continental drift\" (1912, 1915) () and to publish the hypothesis that the continents had somehow \"drifted\" apart. Although he presented much evidence for continental drift, he was unable to provide a convincing explanation for the physical processes which might have caused this drift. He suggested that the continents had been pulled apart by the centrifugal pseudoforce () of the Earth's rotation or by a small component of astronomical precession, but calculations showed that the force was not sufficient. The hypothesis was also studied by Paul Sophus Epstein in 1920 and found to be implausible.\nRejection of Wegener's theory, 1910s\u20131950s.\nContinental drift is now generally accepted, and it always had a minority of scientific proponents since Wegener. However, the theory was largely rejected for many years, with evidence in its favor considered insufficient, and Wegener did not live to see his hypothesis triumph. One problem was that a plausible driving force was missing. A second was that Wegener's estimate of the speed of continental motion, , was implausibly high. (The currently accepted rate for the separation of the Americas from Europe and Africa is about .) Furthermore, Wegener was treated less seriously because he was not a geologist. Even today, the details of the forces propelling the plates are poorly understood.\nThe English geologist Arthur Holmes championed the theory of continental drift at a time when it was deeply unfashionable. He proposed in 1931 that the Earth's mantle contained convection cells which dissipated heat produced by radioactive decay and moved the crust at the surface. His \"Principles of Physical Geology\", ending with a chapter on continental drift, was published in 1944.\nGeological maps of the time showed huge land bridges spanning the Atlantic and Indian oceans to account for the similarities of fauna and flora and the divisions of the Asian continent in the Permian period, but failing to account for glaciation in India, Australia and South Africa.\nThe fixists.\nHans Stille and Leopold Kober opposed the idea of continental drift and worked on a \"fixist\" geosyncline model with Earth contraction playing a key role in the formation of orogens. Other geologists who opposed continental drift were Bailey Willis, Charles Schuchert, Rollin Chamberlin, Walther Bucher and Walther Penck. Willem van der Gracht was virtually the only tectonicist who supported mobilism. In 1939 an international geological conference was held in Frankfurt, organized by the fixist Hans Cloos, which expounded abundant criticism of continental drift and mobilism from the perspectives of tectonics, sedimentology (N\u00f6lke), paleontology (N\u00f6lke), mechanics (Lehmann) and oceanography (Troll, W\u00fcst). Cloos and Troll maintained that excepting the Pacific Ocean, continents were not radically different from oceans in their behaviour. The mobilist theory of \u00c9mile Argand for the Alpine orogeny was criticized by Kurt Leuchs. The few drifters and mobilists at the conference appealed to biogeography (Kirsch, Wittmann), paleoclimatology (Kurt Wegener, brother of Alfred Wegener), paleontology (Gerth) and geodetic measurements (Kurt Wegener). F. Bernauer correctly equated Reykjanes in south-west Iceland with the Mid-Atlantic Ridge, arguing with this that the floor of the Atlantic Ocean was undergoing extension just like Reykjanes. Bernauer thought this extension had drifted the continents only apart, the approximate width of the volcanic zone in Iceland.\nDavid Attenborough, who attended university in the second half of the 1940s, recounted an incident illustrating the dismissal of the theory: \"I once asked one of my lecturers why he was not talking to us about continental drift and I was told, sneeringly, that if I could prove there was a force that could move continents, then he might think about it. The idea was moonshine, I was informed.\"\nAs late as 1953\u2014just five years before Carey introduced the theory of plate tectonics\u2014the theory of continental drift was rejected by the physicist Scheidegger on the following grounds.\nRoad to acceptance.\nFrom the 1930s to the late 1950s, works by Vening-Meinesz, Holmes, Umbgrove, and numerous others outlined concepts that were close or nearly identical to modern plate tectonics theory, which has encompassed and superseded continental drift. The English geologist Arthur Holmes proposed in 1920 that plate junctions might lie beneath the sea, and in 1928 that convection currents within the mantle might be the driving force. Holmes's views were particularly influential: in his bestselling textbook, \"Principles of Physical Geology,\" he included a chapter on continental drift, proposing that Earth's mantle contained convection cells which dissipated radioactive heat and moved the crust at the surface. Holmes's proposal resolved the phase disequilibrium objection (the underlying fluid was kept from solidifying by radioactive heating from the core). However, scientific communication in the 1930s and 1940s was inhibited by World War II, and the theory still required work to overcome the orogeny and isostasy objections. Worse, the most viable forms of the theory predicted the existence of convection cell boundaries reaching deep into the Earth, which had not been observed.\nIn 1947, a team of scientists led by Maurice Ewing confirmed the existence of a rise in the central Atlantic Ocean, and found that the floor of the seabed beneath the sediments was chemically and physically different from continental crust. As oceanographers continued to bathymeter the ocean basins, a system of mid-oceanic ridges was detected. An important conclusion was that along this system, new ocean floor was being created, which led to the concept of the \"Great Global Rift\".\nMeanwhile, scientists began recognizing odd magnetic variations across the ocean floor using devices developed during World War II to detect submarines. Over the next decade, it became increasingly clear that the magnetization patterns were not anomalies, as had been originally supposed. In a series of papers published between 1959 and 1963, Heezen, Dietz, Hess, Mason, Vine, Matthews, and Morley collectively realized that the magnetization of the ocean floor formed extensive, zebra-like patterns: one stripe would exhibit normal polarity and the adjoining stripes reversed polarity. The best explanation was the \"conveyor belt\" or Vine\u2013Matthews\u2013Morley hypothesis. New magma from deep within the Earth rises easily through these weak zones and eventually erupts along the crest of the ridges to create new oceanic crust. The new crust is magnetized by the Earth's magnetic field, which undergoes occasional reversals. Formation of new crust then displaces the magnetized crust away from the rift, akin to a conveyor belt.\nWithout workable alternatives to explain the stripes, geophysicists were forced to conclude that Holmes had been right: ocean rifts were sites of perpetual orogeny at the boundaries of convection cells. By 1967, barely two decades after discovery of the mid-oceanic rifts, and a decade after discovery of the striping, plate tectonics had become axiomatic to modern geophysics.\nIn addition, Marie Tharp provided essential corroboration using her skills in cartography and seismographic data. She collaborated with Bruce Heezen, who was initially sceptical of Tharp's assertions that her maps confirmed continental drift.\nModern evidence.\nGeophysicist Jack Oliver provided seismologic evidence for plate tectonics with the 1968 article \"Seismology and the New Global Tectonics\", using data from seismologic stations including those he set up in the South Pacific. The modern theory of plate tectonics, refining Wegener, explains that there are two kinds of crust of different composition, continental and oceanic, both floating on a much deeper \"plastic\" mantle. Continental crust is inherently lighter. Oceanic crust is created at spreading centers and descends back into the mantle at subduction zones, driving the system of plates chaotically, with continuous orogeny and areas of isostatic imbalance.\nEvidence for the movement of continents on tectonic plates is now extensive. Similar plant and animal fossils are found around the shores of different continents, suggesting that they were once joined. The fossils of \"Mesosaurus\", a freshwater reptile rather like a small crocodile, found both in Brazil and South Africa, are one example; another is the discovery of fossils of the land reptile \"Lystrosaurus\" in rocks of the same age at locations in Africa, India, and Antarctica. There is also living evidence, with the same animals being found on two continents. Some earthworm families (such as Ocnerodrilidae, Acanthodrilidae, Octochaetidae) are found in South America and Africa.\nThe complementary arrangement of the facing sides of South America and Africa is an obvious and temporary coincidence. In millions of years, slab pull, ridge-push, and other forces of tectonophysics will further separate and rotate those two continents. It was that temporary feature that inspired Wegener to propose continental drift.\nThe widespread distribution of Permo-Carboniferous glacial sediments in South America, Africa, Madagascar, Arabia, India, Antarctica and Australia was one of the major pieces of evidence for the theory of continental drift. The continuity of glaciers, inferred from oriented glacial striations and tillite deposits, suggested the existence of the supercontinent of Gondwana, which became a central element of the concept of continental drift. Striations indicated glacial flow away from the equator and toward the poles, based on continents' current positions and orientations, and supported the idea that the southern continents had previously been in dramatically different locations contiguous with one another.\nGPS evidence.\nToday continental drift can be directly measured with Global Positioning Satellite systems. A GPS device placed in Maui, Hawaii moved about 48 cm latitudinally and about 84 cm longitudinally over 14 years."}
{"id": "6057", "revid": "50761366", "url": "https://en.wikipedia.org/wiki?curid=6057", "title": "Commodores", "text": "American funk and soul band\nCommodores, often billed as The Commodores, is an American funk and soul group. The group's most successful period was in the late 1970s and early 1980s when Lionel Richie was the co-lead singer.\nThe members of the group met as mostly freshmen at Tuskegee Institute (now Tuskegee University) in 1968, and signed with Motown in November 1972, having first caught the public eye opening for the Jackson 5 while on tour.\nThe band's biggest hit singles are ballads such as \"Easy\", \"Three Times a Lady\", and \"Nightshift\"; and funk-influenced dance songs, including \"Brick House\", \"Fancy Dancer\", \"Lady (You Bring Me Up)\", and \"Too Hot ta Trot\". \nCommodores were inducted into the Alabama Music Hall of Fame and Vocal Group Hall of Fame. The band has also won one Grammy Award out of nine nominations. The Commodores have sold over 70 million albums worldwide.\nHistory.\nCommodores were formed from two former student groups: the Mystics and the Jays. Richie described some members of the Mystics as \"jazz buffs\". The new six-man band featured Lionel Richie, Thomas McClary, and William King from the Mystics, and Andre Callahan, Michael Gilbert, and Milan Williams from the Jays. They chose their present name when King flipped open a dictionary and ran his finger down the page. \"We lucked out,\" he remarked with a laugh when telling this story to \"People\" magazine. \"We almost became 'The Commodes.'\"\nThe bandmembers attended Tuskegee Institute in Alabama. After winning the college's annual freshman talent contest, they played at fraternity parties as well as a weekend gig at the Black Forest Inn, one of a few clubs in Tuskegee that catered to college students. They performed cover tunes and some original songs with their first singer, James Ingram (not the famous solo artist). Ingram, older than the rest of the band, left to serve in Vietnam, and was later replaced by drummer Walter \"Clyde\" Orange, who wrote or co-wrote many of their hits. Lionel Richie and Orange alternated as lead singers. Orange was the lead singer on the Top 10 hits \"Brick House\" (1977) and \"Nightshift\" (1985).\nThe early band was managed by Benny Ashburn, who brought them to his family's vacation lodge on Martha's Vineyard in 1971 and 1972. There, Ashburn test-marketed the group by having them play in parking lots and summer festivals.\n\"Machine Gun\" (1974), the instrumental title track from the band's debut album, became a staple at American sporting events, and is also heard in many films, including \"Boogie Nights\" and \"Looking for Mr. Goodbar\". It reached No. 22 on the \"Billboard\" Hot 100 in 1974. Another 1974 song \"I Feel Sanctified\" has been called a \"prototype\" of Wild Cherry's 1976 big hit \"Play That Funky Music\". Their three albums released in 1975 and 1976, \"Caught in the Act\", \"Movin' On\" and \"Hot on the Tracks\" were funk albums, with the latter being their first to reach number 1 on the Billboard R&amp;B Albums chart in 1976. After those recordings the group developed the mellower sound hinted at in their 1976 top-ten hits, \"Sweet Love\" and \"Just to Be Close to You\". In 1977, the Commodores released \"Easy\", which became the group's biggest hit yet, reaching No. 4 in the US, followed by funky single \"Brick House\", also top 5, both from their album \"Commodores\", as was \"Zoom\". The group reached No. 1 in 1978 with \"Three Times a Lady\". In 1979, the Commodores scored another top-five ballad, \"Sail On\", before reaching the top of the charts once again with another ballad, \"Still\". In 1981 they released two top-ten hits with \"Oh No\" (No. 4) and their first upbeat single in almost five years, \"Lady (You Bring Me Up)\" (No. 8).\nCommodores made a brief appearance in the 1978 film \"Thank God It's Friday\". They performed the song \"Too Hot ta Trot\" during the dance contest; the songs \"Brick House\" and \"Easy\" were also played in the movie\nIn 1982, the group decided to take a hiatus from touring and recording, during which time Lionel Richie recorded a solo album at the suggestion of Motown and the other group members. Its success encouraged Richie to pursue a solo career, and Skyler Jett replaced him as co-lead singer. Also in 1982, Ashburn died of a heart attack at the age of 54.\nFounding member McClary left in 1984 (shortly after Richie) to pursue a solo career, and to develop a gospel music company. McClary was replaced by guitarist-vocalist Sheldon Reynolds. Then LaPread left in 1986 and moved to Auckland, New Zealand. Reynolds departed for Earth, Wind &amp; Fire in 1987, which prompted trumpeter William \"WAK\" King to take over primary guitar duties for live performances. Keyboardist Milan Williams exited the band in 1989 after allegedly refusing to tour South Africa.\nThe group gradually abandoned its funk roots and moved into the more commercial pop arena. In 1984, former Heatwave singer James Dean \"J.D.\" Nicholas assumed co-lead vocal duties with drummer Walter Orange. That line-up was hitless until 1985 when their final Motown album \"Nightshift\", produced by Dennis Lambert (prior albums were produced by James Anthony Carmichael, who would continue to work with Richie on his albums), delivered the title track \"Nightshift\", a loving tribute to Marvin Gaye and Jackie Wilson, both of whom had died the previous year. \"Nightshift\" hit no. 3 in the US and won the Commodores their first Grammy for Best R&amp;B Performance by a Duo or Group With Vocals in 1985.\nIn 2010 a new version was recorded, dedicated to Michael Jackson. The Commodores were on a European tour performing at Wembley Arena, London, on June 25, 2009, when they walked off the stage after they were told that Michael Jackson had died. Initially the band thought it was a hoax. However, back in their dressing rooms they received confirmation and broke down in tears. The next night at Birmingham's NIA Arena, J.D. Nicholas added Jackson's name to the lyrics of the song, and henceforth the Commodores have mentioned Jackson and other deceased R&amp;B singers. Thus came the inspiration upon the first anniversary of Jackson's death to re-record, with new lyrics, the hit song \"Nightshift\" as a tribute.\nIn 1990, they formed Commodores Records and re-recorded their 20 greatest hits as \"Commodores Hits Vol. I &amp; II\". They have recorded a live album, \"Commodores Live\", along with a DVD of the same name, and a Christmas album titled \"Commodores Christmas\". In 2012, the band was working on new material, with some contributions written by current and former members.\nCommodores as of 2020 consist of Walter \"Clyde\" Orange, James Dean \"J.D.\" Nicholas, and William \"WAK\" King, along with their five-piece band The Mean Machine.They continue to perform, playing at arenas, theaters, and festivals around the world.\nAccolades.\nGrammy awards.\nThe Commodores have won one Grammy Award out of ten nominations.\nAlabama Music Hall of Fame.\nDuring 1995 the Commodores were inducted into the Alabama Music Hall of Fame.\nVocal Group Hall of Fame.\nDuring 2003 the Commodores were also inducted into the Vocal Group Hall of Fame.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6058", "revid": "312589", "url": "https://en.wikipedia.org/wiki?curid=6058", "title": "Collagen", "text": "Most abundant structural protein in animals\nCollagen () is the main structural protein in the extracellular matrix of the connective tissues of many animals. It is the most abundant protein in mammals, making up 25% to 35% of protein content. Amino acids are bound together to form a triple helix of elongated fibril known as a collagen helix. It is mostly found in cartilage, bones, tendons, ligaments, and skin. Vitamin C is vital for collagen synthesis.\nDepending on the degree of mineralization, collagen tissues may be rigid (bone) or compliant (tendon) or have a gradient from rigid to compliant (cartilage). Collagen is also abundant in corneas, blood vessels, the gut, intervertebral discs, and dentin. In muscle tissue, it serves as a major component of the endomysium. Collagen constitutes 1% to 2% of muscle tissue and 6% by weight of skeletal muscle. The fibroblast is the most common cell creating collagen in animals. Gelatin, which is used in food and industry, is collagen that was irreversibly hydrolyzed using heat, basic solutions, or weak acids.\nEtymology.\nThe name \"collagen\" comes from the Greek \u03ba\u03cc\u03bb\u03bb\u03b1 (\"k\u00f3lla\"), meaning \"glue\", and suffix -\u03b3\u03ad\u03bd, \"-gen\", denoting \"producing\".\nTypes.\nAs of 2011, 28 types of human collagen have been identified, described, and classified according to their structure. This diversity shows collagen's diverse functionality. All of the types contain at least one triple helix. Over 90% of the collagen in humans is Type I and Type III collagen.\nThe five most common types are:\nIn humans.\nCardiac.\nThe collagenous cardiac skeleton, which includes the four heart valve rings, is histologically, elastically and uniquely bound to cardiac muscle. The cardiac skeleton also includes the separating septa of the heart chambers \u2013 the interventricular septum and the atrioventricular septum. Collagen contribution to the measure of cardiac performance summarily represents a continuous torsional force opposed to the fluid mechanics of blood pressure emitted from the heart. The collagenous structure that divides the upper chambers of the heart from the lower chambers is an impermeable membrane that excludes both blood and electrical impulses through typical physiological means. With support from collagen, atrial fibrillation never deteriorates to ventricular fibrillation. Collagen is layered in variable densities with smooth muscle mass. The mass, distribution, age, and density of collagen all contribute to the compliance required to move blood back and forth. Individual cardiac valvular leaflets are folded into shape by specialized collagen under variable pressure. Gradual calcium deposition within collagen occurs as a natural function of aging. Calcified points within collagen matrices show contrast in a moving display of blood and muscle, enabling methods of cardiac imaging technology to arrive at ratios essentially stating blood in (cardiac input) and blood out (cardiac output). Pathology of the collagen underpinning of the heart is understood within the category of connective tissue disease.\nBone grafts.\nAs the skeleton forms the structure of the body, it is vital that it maintains its strength and its structure, even after breaks and injuries. Collagen is used in bone grafting because its triple-helix structure makes it a very strong molecule. It is ideal for use in bones, as it does not compromise the structural integrity of the skeleton. The triple helical structure prevents collagen from being broken down by enzymes, it enables adhesiveness of cells, and it is important for the proper assembly of the extracellular matrix.\nTissue regeneration.\nCollagen scaffolds are used in tissue regeneration, whether in sponges, thin sheets, gels, or fibers. Collagen has favorable properties for tissue regeneration, such as pore structure, permeability, hydrophilicity, and stability in vivo. Collagen scaffolds also support deposition of cells, such as osteoblasts and fibroblasts, and once inserted, facilitate growth to proceed normally.\nReconstructive surgery.\nCollagens are widely used in the construction of artificial skin substitutes used for managing severe burns and wounds. These collagens may be derived from cow, horse, pig, or even human sources; and are sometimes used in combination with silicones, glycosaminoglycans, fibroblasts, growth factors, and other substances.\nWound healing.\nCollagen is one of the body's key natural resources and a component of skin tissue that can benefit all stages of wound healing. When collagen is made available to the wound bed, closure can occur. This avoids wound deterioration and procedures such as amputation.\nCollagen is used as a natural wound dressing because it has properties that artificial wound dressings do not have. It resists bacteria, which is vitally important in wound dressing. As a burn dressing, collagen helps it heal fast by helping granulation tissue to grow over the burn.\nThroughout the four phases of wound healing, collagen performs the following functions:\nUse in basic research.\nCollagen is used in laboratory studies for cell culture, studying cell behavior and cellular interactions with the extracellular environment. Collagen is also widely used as a bioink for 3D bioprinting and biofabrication of 3D tissue models.\nBiology.\nThe collagen protein is composed of a triple helix, which generally consists of two identical chains (\u03b11) and an additional chain that differs slightly in its chemical composition (\u03b12). The amino acid composition of collagen is atypical for proteins, particularly with respect to its high hydroxyproline content. The most common motifs in collagen's amino acid sequence are glycine-proline-X and glycine-X-hydroxyproline, where X is any amino acid other than glycine, proline or hydroxyproline.\nThe table below lists average amino acid composition for fish and mammal skin.\nSynthesis.\nFirst, a three-dimensional stranded structure is assembled, mostly composed of the amino acids glycine and proline. This is the collagen precursor procollagen. Then, procollagen is modified by the addition of hydroxyl groups to the amino acids proline and lysine. This step is important for later glycosylation and the formation of collagen's triple helix structure. Because the hydroxylase enzymes performing these reactions require vitamin C as a cofactor, a long-term deficiency in this vitamin results in impaired collagen synthesis and scurvy. These hydroxylation reactions are catalyzed by the enzymes prolyl 4-hydroxylase and lysyl hydroxylase. The reaction consumes one ascorbate molecule per hydroxylation. Collagen synthesis occurs inside and outside cells. \nThe most common form of collagen is fibrillary collagen. Another common form is meshwork collagen, which is often involved in the formation of filtration systems. All types of collagen are triple helices, but differ in the make-up of their alpha peptides created in step 2. Below we discuss the formation of fibrillary collagen.\nAmino acids.\nCollagen has an unusual amino acid composition and sequence:\nCortisol stimulates degradation of (skin) collagen into amino acids.\nCollagen I formation.\nMost collagen forms in a similar manner, but the following process is typical for type I:\nMolecular structure.\nA single collagen molecule, tropocollagen, is used to make up larger collagen aggregates, such as fibrils. It is approximately 300\u00a0nm long and 1.5\u00a0nm in diameter, and it is made up of three polypeptide strands (called alpha peptides, see step 2), each of which has the conformation of a left-handed helix \u2013 this should not be confused with the right-handed alpha helix. These three left-handed helices are twisted together into a right-handed triple helix or \"super helix\", a cooperative quaternary structure stabilized by many hydrogen bonds. With type I collagen and possibly all fibrillar collagens, if not all collagens, each triple-helix associates into a right-handed super-super-coil referred to as the collagen microfibril. Each microfibril is interdigitated with its neighboring microfibrils to a degree that might suggest they are individually unstable, although within collagen fibrils, they are so well ordered as to be crystalline.\nA distinctive feature of collagen is the regular arrangement of amino acids in each of the three chains of these collagen subunits. The sequence often follows the pattern Gly-Pro-X or Gly-X-Hyp, where X may be any of various other amino acid residues. Proline or hydroxyproline constitute about 1/6 of the total sequence. With glycine accounting for the 1/3 of the sequence, this means approximately half of the collagen sequence is not glycine, proline or hydroxyproline, a fact often missed due to the distraction of the unusual GX1X2 character of collagen alpha-peptides. The high glycine content of collagen is important with respect to stabilization of the collagen helix, as this allows the very close association of the collagen fibers within the molecule, facilitating hydrogen bonding and the formation of intermolecular cross-links. This kind of regular repetition and high glycine content is found in only a few other fibrous proteins, such as silk fibroin.\nCollagen is not only a structural protein. Due to its key role in the determination of cell phenotype, cell adhesion, tissue regulation, and infrastructure, many sections of its non-proline-rich regions have cell or matrix association/regulation roles. The relatively high content of proline and hydroxyproline rings, with their geometrically constrained carboxyl and (secondary) amino groups, along with the rich abundance of glycine, accounts for the tendency of the individual polypeptide strands to form left-handed helices spontaneously, without any intrachain hydrogen bonding.\nBecause glycine is the smallest amino acid with no side chain, it plays a unique role in fibrous structural proteins. In collagen, Gly is required at every third position because the assembly of the triple helix puts this residue at the interior (axis) of the helix, where there is no space for a larger side group than glycine's single hydrogen atom. For the same reason, the rings of the Pro and Hyp must point outward. These two amino acids help stabilize the triple helix \u2013 Hyp even more so than Pro because of a stereoelectronic effect; a lower concentration of them is required in animals such as fish, whose body temperatures are lower than most warm-blooded animals. Lower proline and hydroxyproline contents are characteristic of cold-water, but not warm-water fish; the latter tend to have similar proline and hydroxyproline contents to mammals. The lower proline and hydroxyproline contents of cold-water fish and other poikilotherm animals lead to their collagen having a lower thermal stability than mammalian collagen. This lower thermal stability means that gelatin derived from fish collagen is not suitable for many food and industrial applications.\nThe tropocollagen subunits spontaneously self-assemble, with regularly staggered ends, into even larger arrays in the extracellular spaces of tissues. Additional assembly of fibrils is guided by fibroblasts, which deposit fully formed fibrils from fibripositors. In the fibrillar collagens, molecules are staggered to adjacent molecules by about 67\u00a0nm (a unit that is referred to as 'D' and changes depending upon the hydration state of the aggregate). In each D-period repeat of the microfibril, there is a part containing five molecules in cross-section, called the \"overlap\", and a part containing only four molecules, called the \"gap\". These overlap and gap regions are retained as microfibrils assemble into fibrils, and are thus viewable using electron microscopy. The triple helical tropocollagens in the microfibrils are arranged in a quasihexagonal packing pattern.\nThere is some covalent crosslinking within the triple helices and a variable amount of covalent crosslinking between tropocollagen helices forming well-organized aggregates (such as fibrils). Larger fibrillar bundles are formed with the aid of several different classes of proteins (including different collagen types), glycoproteins, and proteoglycans to form the different types of mature tissues from alternate combinations of the same key players. Collagen's insolubility was a barrier to the study of monomeric collagen until it was found that tropocollagen from young animals can be extracted because it is not yet fully crosslinked. However, advances in microscopy techniques (i.e. electron microscopy (EM) and atomic force microscopy (AFM)) and X-ray diffraction have enabled researchers to obtain increasingly detailed images of collagen structure \"in situ\". These later advances are particularly important to better understanding the way in which collagen structure affects cell\u2013cell and cell\u2013matrix communication and how tissues are constructed in growth and repair and changed in development and disease. For example, using AFM\u2013based nanoindentation it has been shown that a single collagen fibril is a heterogeneous material along its axial direction with significantly different mechanical properties in its gap and overlap regions, correlating with its different molecular organizations in these two regions.\nCollagen fibrils/aggregates are arranged in different combinations and concentrations in various tissues to provide varying tissue properties. In bone, entire collagen triple helices lie in a parallel, staggered array. 40\u00a0nm gaps between the ends of the tropocollagen subunits (approximately equal to the gap region) probably serve as nucleation sites for the deposition of long, hard, fine crystals of the mineral component, which is hydroxylapatite (approximately) Ca10(OH)2(PO4)6. Type I collagen gives bone its tensile strength.\nAssociated disorders.\nCollagen-related diseases most commonly arise from genetic defects or nutritional deficiencies that affect the biosynthesis, assembly, posttranslational modification, secretion, or other processes involved in normal collagen production.\nIn addition to the above-mentioned disorders, excessive deposition of collagen occurs in scleroderma.\nDiseases.\nOne thousand mutations have been identified in 12 out of more than 20 types of collagen. These mutations can lead to various diseases at the tissue level.\nOsteogenesis imperfecta \u2013 Caused by a mutation in \"type 1 collagen\", a dominant autosomal disorder, results in weak bones and irregular connective tissue; some cases can be mild while others can be lethal. Mild cases have lowered levels of collagen type 1, while severe cases have structural defects in collagen.\nChondrodysplasias \u2013 Skeletal disorder believed to be caused by a mutation in \"type 2 collagen\", further research is being conducted to confirm this.\nEhlers\u2013Danlos syndrome \u2013 Thirteen different types of this disorder, which lead to deformities in connective tissue, are known. Some of the rarer types can be lethal, leading to the rupture of arteries. Each syndrome is caused by a different mutation. For example, the vascular type (vEDS) of this disorder is caused by a mutation in \"collagen type 3\".\nAlport syndrome \u2013 Can be passed on genetically, usually as X-linked dominant, but also as both an autosomal dominant and autosomal recessive disorder. Those with the condition have problems with their kidneys and eyes, and loss of hearing can also develop during childhood or adolescence.\nKnobloch syndrome \u2013 Caused by a mutation in the COL18A1 gene that codes for the production of collagen XVIII. Patients present with protrusion of the brain tissue and degeneration of the retina; an individual who has family members with the disorder is at an increased risk of developing it themselves since there is a hereditary link.\nAnimal harvesting.\nWhen not synthesized, collagen can be harvested from animal skin. This has led to deforestation, as has occurred in Paraguay, where large collagen producers buy large amounts of cattle hides from regions that have been clear-cut for cattle grazing.\nCharacteristics.\nCollagen is one of the long, fibrous structural proteins whose functions are quite different from those of globular proteins, such as enzymes. Tough bundles of collagen called \"collagen fibers\" are a major component of the extracellular matrix that supports most tissues and gives cells structure from the outside, but collagen is also found inside certain cells. Collagen has great tensile strength and is the main component of fascia, cartilage, ligaments, tendons, bone, and skin. Along with elastin and soft keratin, it is responsible for skin strength and elasticity, and its degradation leads to wrinkles that accompany aging. It strengthens blood vessels and plays a role in tissue development. It is present in the cornea and lens of the eye in crystalline form. It may be one of the most abundant proteins in the fossil record, given that it appears to fossilize frequently, even in bones from the Mesozoic and Paleozoic.\nMechanical properties.\nCollagen is a complex hierarchical material with mechanical properties that vary significantly across different scales.\nOn the molecular scale, atomistic and coarse-grained modeling simulations, as well as numerous experimental methods, have led to several estimates of the Young's modulus of collagen at the molecular level. Only above a certain strain rate is there a strong relationship between elastic modulus and strain rate, possibly due to the large number of atoms in a collagen molecule. The length of the molecule is also important, where longer molecules have lower tensile strengths than shorter ones due to short molecules having a large proportion of hydrogen bonds being broken and reformed.\nOn the fibrillar scale, collagen has a lower modulus compared to the molecular scale, and varies depending on geometry, scale of observation, deformation state, and hydration level. By increasing the crosslink density from zero to 3 per molecule, the maximum stress the fibril can support increases from 0.5 GPa to 6 GPa.\nLimited tests have been done on the tensile strength of the collagen fiber, but generally it has been shown to have a lower Young's modulus compared to fibrils.\nWhen studying the mechanical properties of collagen, tendon is often chosen as the ideal material because it is close to a pure and aligned collagen structure. However, at the macro, tissue scale, the vast number of structures that collagen fibers and fibrils can be arranged into results in highly variable properties. For example, tendon has primarily parallel fibers, whereas skin consists of a net of wavy fibers, resulting in a much higher strength and lower ductility in tendon compared to skin. The mechanical properties of collagen at multiple hierarchical levels are given.\nCollagen is known to be a viscoelastic solid. When the collagen fiber is modeled as two Kelvin-Voigt models in series, each consisting of a spring and a dashpot in parallel, the strain in the fiber can be modeled according to the following equation:\nformula_1\nwhere \u03b1, \u03b2, and \u03b3 are defined materials properties, \u03b5D is fibrillar strain, and \u03b5T is total strain.\nUses.\nCollagen has a wide variety of applications. In the medical industry, it is used in cosmetic surgery and burn surgery. An example of collagen use for food manufacturing is in casings for sausages.\nIf collagen is subject to sufficient denaturation, such as by heating, the three tropocollagen strands separate partially or completely into globular domains, containing a different secondary structure to the normal collagen polyproline II (PPII) of random coils. This process describes the formation of gelatin, which is used in many foods, including flavored gelatin desserts. Besides food, gelatin has been used in pharmaceutical, cosmetic, and photography industries. It is also used as a dietary supplement, and has been advertised as a potential remedy against the ageing process.\nFrom the Greek for glue, \"kolla\", the word collagen means \"glue producer\" and refers to the early process of boiling the skin and sinews of horses and other animals to obtain glue. Collagen adhesive was used by Egyptians about 4,000 years ago, and Native Americans used it in bows about 1,500 years ago. The oldest glue in the world, carbon-dated as more than 8,000 years old, was found to be collagen \u2013 used as a protective lining on rope baskets and embroidered fabrics, to hold utensils together, and in crisscross decorations on human skulls. Collagen normally converts to gelatin, but survived due to dry conditions. Animal glues are thermoplastic, softening again upon reheating, so they are still used in making musical instruments such as fine violins and guitars, which may have to be reopened for repairs \u2013 an application incompatible with tough, synthetic plastic adhesives, which are permanent. Animal sinews and skins, including leather, have been used to make useful articles for millennia.\nGelatin-resorcinol-formaldehyde glue (and with formaldehyde replaced by less-toxic pentanedial and ethanedial) has been used to repair experimental incisions in rabbit lungs.\nCosmetics.\nBovine collagen is widely used in dermal fillers for aesthetic correction of wrinkles and skin aging. Collagen cremes are also widely sold even though collagen cannot penetrate the skin because its fibers are too large. Collagen is a vital protein in skin, hair, nails, and other tissues. Its production decreases with age and factors like sun damage and smoking. Collagen supplements, derived from sources like fish and cattle, are marketed to improve skin, hair, and nails. Studies show some skin benefits, but these supplements often contain other beneficial ingredients, making it unclear if collagen alone is effective. There's minimal evidence supporting collagen's benefits for hair and nails. Overall, the effectiveness of oral collagen supplements is not well-proven, and focusing on a healthy lifestyle and proven skincare methods like sun protection is recommended.\nHistory.\nThe molecular and packing structures of collagen eluded scientists over decades of research. The first evidence that it possesses a regular structure at the molecular level was presented in the mid-1930s. Research then concentrated on the conformation of the collagen monomer, producing several competing models, although correctly dealing with the conformation of each individual peptide chain. The triple-helical \"Madras\" model, proposed by G. N. Ramachandran in 1955, provided an accurate model of quaternary structure in collagen. This model was supported by further studies of higher resolution in the late 20th century.\nThe packing structure of collagen has not been defined to the same degree outside of the fibrillar collagen types, although it has been long known to be hexagonal. As with its monomeric structure, several conflicting models propose either that the packing arrangement of collagen molecules is 'sheet-like', or is microfibrillar. The microfibrillar structure of collagen fibrils in tendon, cornea and cartilage was imaged directly by electron microscopy in the late 20th century and early 21st century. The microfibrillar structure of rat tail tendon was modeled as being closest to the observed structure, although it oversimplified the topological progression of neighboring collagen molecules, and so did not predict the correct conformation of the discontinuous D-periodic pentameric arrangement termed \"microfibril\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6059", "revid": "50912812", "url": "https://en.wikipedia.org/wiki?curid=6059", "title": "Calvin and Hobbes", "text": "Comic strip by Bill Watterson\nCalvin and Hobbes is a daily American comic strip created by cartoonist Bill Watterson that was syndicated from November 18, 1985, to December 31, 1995. Commonly described as \"the last great newspaper comic\", \"Calvin and Hobbes\" has enjoyed enduring popularity and influence while also attracting significant academic and philosophical interest.\n\"Calvin and Hobbes\" follows the humorous antics of the title characters: Calvin, a mischievous and adventurous six-year-old boy; and his friend Hobbes, a stuffed tiger. Set in the suburban United States of the 1980s and 1990s, the strip depicts Calvin's frequent flights of fancy and friendship with Hobbes. It also examines Calvin's relationships with his long-suffering parents and with his classmates, especially his neighbor Susie Derkins. Hobbes's dual nature is a defining motif for the strip: to Calvin, Hobbes is a living anthropomorphic tiger, while all the other characters seem to see Hobbes as an inanimate stuffed toy, though Watterson has not clarified exactly how Hobbes is perceived by others, or whether he is real or an imaginary friend. Though the series does not frequently mention specific political figures or ongoing events, it does explore broad issues like environmentalism, public education, and philosophical quandaries.\nAt the height of its popularity, \"Calvin and Hobbes\" was featured in over 2,400 newspapers worldwide. As of 2010, reruns of the strip appeared in more than 50 countries, and nearly 45 million copies of the \"Calvin and Hobbes\" books had been sold worldwide.\nHistory.\nDevelopment.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"I thought it was perhaps too 'adult,' too literate. When my then-8-year-old son remarked, 'This is the \"Doonesbury\" for kids!' I suspected we had something unusual on our hands.\"\n\u2014Lee Salem, Watterson's editor at Universal, recalling his reaction after seeing Watterson's first submission\n\"Calvin and Hobbes\" was conceived when Bill Watterson, while working in an advertising job he detested, began devoting his spare time to developing a newspaper comic for potential syndication. He explored various strip ideas but all were rejected by the syndicates. United Feature Syndicate finally responded positively to one strip called \"The Doghouse\", which featured a side character (the main character's little brother) who had a stuffed tiger. United identified these characters as the strongest and encouraged Watterson to develop them as the center of their own strip. Ironically, United Feature ultimately rejected the new strip as lacking in marketing potential, although Universal Press Syndicate took it up.\nLaunch and early success (1985\u20131990).\nThe first \"Calvin and Hobbes\" strip was published on November 18, 1985 in 35 newspapers. The strip quickly became popular. Within a year of syndication, the strip was published in roughly 250 newspapers and proved to have international appeal with translation and wide circulation outside the United States.\nAlthough \"Calvin and Hobbes\" underwent continual artistic development and creative innovation over the period of syndication, the earliest strips demonstrated a remarkable consistency with the latest. Watterson introduced all the major characters within the first three weeks and made no changes to the central cast over the strip's 10-year history.\nBy April 5, 1987, Watterson was featured in an article in the \"Los Angeles Times\". \"Calvin and Hobbes\" earned Watterson the Reuben Award from the National Cartoonists Society in the Outstanding Cartoonist of the Year category, first in 1986 and again in 1988. He was nominated another time in 1992. The Society awarded him the Humor Comic Strip Award for 1988. \"Calvin and Hobbes\" has also won several more awards.\nAs his creation grew in popularity, there was strong interest from the syndicate to merchandise the characters and expand into other forms of media. Watterson's contract with the syndicate allowed the characters to be licensed without the creator's consent, as was standard at the time. Nevertheless, Watterson had leverage by threatening to simply walk away from the comic strip.\nThis dynamic played out in a long and emotionally draining battle between Watterson and his syndicate editors. By 1991, Watterson had achieved his goal of securing a new contract that granted him legal control over his creation and all future licensing arrangements.\nCreative control (1991\u20131995).\nHaving achieved his objective of creative control, Watterson's desire for privacy subsequently reasserted itself and he ceased all media interviews, relocated to New Mexico, and largely disappeared from public engagements, refusing to attend the ceremonies of any of the cartooning awards he won. The pressures of the battle over merchandising led to Watterson taking an extended break from May 5, 1991, to February 1, 1992, a move that was virtually unprecedented in the world of syndicated cartoonists.\nDuring Watterson's first sabbatical from the strip, Universal Press Syndicate continued to charge newspapers full price to re-run old \"Calvin and Hobbes\" strips. Few editors approved of the move, but the strip was so popular that they had no choice but to continue to run it for fear that competing newspapers might pick it up and draw its fans away. Watterson returned to the strip in 1992 with plans to produce his Sunday strip as an unbreakable half of a newspaper or tabloid page. This made him only the second cartoonist since Garry Trudeau to have sufficient popularity to demand more space and control over the presentation of his work.\nWatterson took a second sabbatical from April 3 through December 31, 1994. His return came with an announcement that \"Calvin and Hobbes\" would be concluding at the end of 1995. Stating his belief that he had achieved everything that he wanted to within the medium, he announced his intention to work on future projects at a slower pace with fewer artistic compromises.\nThe final strip ran on Sunday, December 31, 1995, depicting Calvin and Hobbes sledding down a snowy hill after a fresh snowfall with Calvin exclaiming \"Let's go exploring!\"\nSpeaking to NPR in 2005, animation critic Charles Solomon opined that the final strip \"left behind a hole in the comics page that no strip has been able to fill.\"\nSunday formatting.\nSyndicated comics were typically published six times a week in black and white, with a Sunday supplement version in a larger, full color format. This larger format version of the strip was constrained by mandatory layout requirements that made it possible for newspaper editors to format the strip for different page sizes and layouts.\nWatterson grew increasingly frustrated by the shrinking of the available space for comics in the newspapers and the mandatory panel divisions that restricted his ability to produce better artwork and more creative storytelling. He felt that without space for anything more than simple dialogue or sparse artwork, comics as an art form were becoming dilute, bland, and unoriginal.\nWatterson longed for the artistic freedom allotted to classic strips such as \"Little Nemo\" and \"Krazy Kat\", and in 1989 he gave a sample of what could be accomplished with such liberty in the opening pages of the Sunday strip compilation, \"The Calvin and Hobbes Lazy Sunday Book\u2014\"an 8-page previously unpublished Calvin story fully illustrated in watercolor. The same book contained an afterword from the artist himself, reflecting on a time when comic strips were allocated a whole page of the newspaper and every comic was like a \"color poster\".\nWithin two years, Watterson was ultimately successful in negotiating a deal that provided him more space and creative freedom. Following his 1991 sabbatical, Universal Press announced that Watterson had decided to sell his Sunday strip as an unbreakable half of a newspaper or tabloid page. Many editors and even a few cartoonists including Bil Keane (\"The Family Circus\") and Bruce Beattie (\"Snafu\") criticized him for what they perceived as arrogance and an unwillingness to abide by the normal practices of the cartoon business. Others, including Bill Amend (\"Foxtrot\"), Johnny Hart (\"BC\", \"The Wizard of Id\") and Barbara Brandon (\"Where I'm Coming From\") supported him. The American Association of Sunday and Feature Editors even formally requested that Universal reconsider the changes. Watterson's own comments on the matter was that \"editors will have to judge for themselves whether or not Calvin and Hobbes deserves the extra space. If they don't think the strip carries its own weight, they don't have to run it.\" Ultimately only 15 newspapers cancelled the strip in response to the layout changes.\nSabbaticals.\nBill Watterson took two sabbaticals from the daily requirements of producing the strip. The first took place from May 5, 1991, to February 1, 1992, and the second from April 3 through December 31, 1994. These sabbaticals were included in the new contract Watterson managed to negotiate with Universal Features in 1990. The sabbaticals were proposed by the syndicate themselves, who, fearing Watterson's complete burnout, endeavored to get another five years of work from their star artist.\nWatterson remains only the third cartoonist with sufficient popularity and stature to receive a sabbatical from their syndicate, the first two being Garry Trudeau (\"Doonesbury\") in 1983 and Gary Larson (\"The Far Side\") in 1989. Typically, cartoonists are expected to produce sufficient strips to cover any period that they may wish to take off. Watterson's lengthy sabbaticals received some mild criticism from his fellow cartoonists including Greg Evans (\"Luann\"), and Charles Schulz (\"Peanuts\"), one of Watterson's major artistic influences, who even called it a \"puzzle\". Some cartoonists resented the idea that Watterson worked harder than others, while others supported it. At least one newspaper editor noted that the strip was the most popular in the country and stated that he \"earned it\".\nMerchandising.\n\"Calvin and Hobbes\" had almost no official product merchandising. Watterson held that comic strips should stand on their own as an art form and although he did not start out completely opposed to merchandising in all forms (or even for all comic strips), he did reject an early syndication deal that involved incorporating a more marketable, licensed character into his strip. In spite of being an unproven cartoonist, and having been flown all the way to New York to discuss the proposal, Watterson reflexively resented the idea of \"cartooning by committee\" and turned it down.\nWhen \"Calvin and Hobbes\" was accepted by Universal Syndicate, and began to grow in popularity, Watterson found himself at odds with the syndicate, which urged him to begin merchandising the characters and touring the country to promote the first collections of comic strips. Watterson refused, believing that the integrity of the strip and its artist would be undermined by commercialization, which he saw as a major negative influence in the world of cartoon art, and that licensing his character would only violate the spirit of his work. He gave an example of this in discussing his opposition to a Hobbes plush toy: that if the essence of Hobbes' nature in the strip is that it remain unresolved whether he is a real tiger or a stuffed toy, then creating a real stuffed toy would only destroy the magic. However, having initially signed away control over merchandising in his initial contract with the syndicate, Watterson commenced a lengthy and emotionally draining battle with Universal to gain control over his work. Ultimately Universal did not approve any products against Watterson's wishes, understanding that, unlike other comic strips, it would be nearly impossible to separate the creator from the strip if Watterson chose to walk away.\nOne estimate places the value of licensing revenue forgone by Watterson at $300\u2013$400 million. Almost no legitimate \"Calvin and Hobbes\" merchandise exists. Exceptions produced during the strip's original run include two 16-month calendars (1988\u201389 and 1989\u201390), a t-shirt for the Smithsonian Exhibit, \"Great American Comics: 100 Years of Cartoon Art\" (1990) and the textbook \"Teaching with Calvin and Hobbes\", which has been described as \"perhaps the most difficult piece of official \"Calvin and Hobbes\" memorabilia to find.\" In 2010, Watterson did allow his characters to be included in a series of United States Postal Service stamps honoring five classic American comics. Licensed prints of \"Calvin and Hobbes\" were made available and have also been included in various academic works.\nThe strip's immense popularity has led to the appearance of various counterfeit items such as window decals and T-shirts that often feature crude humor, binge drinking and other themes that are not found in Watterson's work. Images from one strip in which Calvin and Hobbes dance to loud music at night were commonly used for copyright violations. After threat of a lawsuit alleging infringement of copyright and trademark, some sticker makers replaced Calvin with a different boy, while other makers made no changes. Watterson wryly commented, \"I clearly miscalculated how popular it would be to show Calvin urinating on a Ford logo,\" but later added, \"long after the strip is forgotten, [they] are my ticket to immortality\".\nAnimation.\nWatterson has expressed admiration for animation as an artform. In a 1989 interview in \"The Comics Journal\" he described the appeal of being able to do things with a moving image that cannot be done by a simple drawing: the distortion, the exaggeration and the control over the length of time an event is viewed. However, although the visual possibilities of animation appealed to Watterson, the idea of finding a voice for Calvin made him uncomfortable, as did the idea of working with a team of animators. Ultimately, \"Calvin and Hobbes\" was never made into an animated series. Watterson later stated in \"The Calvin and Hobbes Tenth Anniversary Book\" that he liked the fact that his strip was a \"low-tech, one-man operation,\" and that he took great pride in the fact that he drew every line and wrote every word on his own. Calls from major Hollywood figures interested in an adaptation of his work, including Jim Henson, George Lucas and Steven Spielberg, were never returned and in a 2013 interview Watterson stated that he had \"zero interest\" in an animated adaptation as there was really no upside for him in doing so.\nStyle and influences.\nThe strip borrows several elements and themes from three major influences: Walt Kelly's \"Pogo\", George Herriman's \"Krazy Kat\" and Charles M. Schulz's \"Peanuts\". Schulz and Kelly particularly influenced Watterson's outlook on comics during his formative years.\nElements of Watterson's artistic style are his characters' diverse and often exaggerated expressions (particularly those of Calvin), elaborate and bizarre backgrounds for Calvin's flights of imagination, expressions of motion and frequent visual jokes and metaphors. In the later years of the strip, with more panel space available for his use, Watterson experimented more freely with different panel layouts, art styles, stories without dialogue and greater use of white space. He also experimented with his tools, once inking a strip with a stick from his yard in order to achieve a particular look. He also makes a point of not showing certain things explicitly: the \"Noodle Incident\" and the children's book \"Hamster Huey and the Gooey Kablooie\" are left to the reader's imagination, where Watterson was sure they would be \"more outrageous\" than he could portray.\nProduction and technique.\nWatterson's technique started with minimalist pencil sketches drawn with a light pencil (though the larger Sunday strips often required more elaborate work) on a piece of Bristol board, with his brand of choice being Strathmore because he felt it held the drawings better on the page as opposed to the cheaper brands (Watterson said he initially used any cheap pad of Bristol board his local supply store had but switched to Strathmore after he found himself growing more and more displeased with the results). He would then use a small sable brush and India ink to fill in the rest of the drawing, saying that he did not want to simply trace over his penciling and thus make the inking more spontaneous. He lettered dialogue with a Rapidograph fountain pen, and he used a crowquill pen for odds and ends. Mistakes were covered with various forms of correction fluid, including the type used on typewriters. Watterson was careful in his use of color, often spending a great deal of time in choosing the right colors to employ for the weekly Sunday strip; his technique was to cut the color tabs the syndicate sent him into individual squares, lay out the colors, and then paint a watercolor approximation of the strip on tracing paper over the Bristol board and then mark the strip accordingly before sending it on. When \"Calvin and Hobbes\" began there were 64 colors available for the Sunday strips. For the later Sunday strips Watterson had 125 colors as well as the ability to fade the colors into each other.\nCharacters.\nIn addition to the two titular characters, six-year-old Calvin and his stuffed tiger Hobbes, the strip features a small recurring cast that also includes Calvin's unnamed parents, his classmate and neighbor Susie Derkins, his teacher Miss Wormwood, his school bully Moe, and his babysitter Rosalyn.\nRecurring elements and themes.\nArt and academia.\nWatterson used the strip to poke fun at the art world, principally through Calvin's unconventional creations of snowmen but also through other expressions of childhood art. When Miss Wormwood complains that he is wasting class time drawing impossible things (a \"Stegosaurus\" in a rocket ship, for example), Calvin proclaims himself \"on the cutting edge of the \"avant-garde\".\" He begins exploring the medium of snow when a warm day melts his snowman. His next sculpture \"speaks to the horror of our own mortality, inviting the viewer to contemplate the evanescence of life.\" In later strips, Calvin's creative instincts diversify to include sidewalk drawings (or, as he terms them, examples of \"suburban postmodernism\").\nWatterson also lampooned academia. In one example, Calvin carefully crafts an \"artist's statement\", claiming that such essays convey more messages than artworks themselves ever do (Hobbes blandly notes, \"You misspelled \"Weltanschauung\"\"). He indulges in what Watterson calls \"pop psychobabble\" to justify his destructive rampages and shift blame to his parents, citing \"toxic codependency.\" In one instance, he pens a book report based on the theory that the purpose of academic writing is to \"inflate weak ideas, obscure poor reasoning and inhibit clarity,\" entitled \"The Dynamics of Interbeing and Monological Imperatives in Dick and Jane: A Study in Psychic Transrelational Gender Modes\". Displaying his creation to Hobbes, he remarks, \"Academia, here I come!\" Watterson explains that he adapted this jargon (and similar examples from several other strips) from an actual book of art criticism.\nOverall, Watterson's satirical essays serve to attack both sides, criticizing both the commercial mainstream and the artists who are supposed to be \"outside\" it. The strip on Sunday, June 21, 1992, criticized the naming of the Big Bang theory as not evocative of the wonders behind it and coined the term \"Horrendous Space Kablooie\", an alternative that achieved some informal popularity among scientists and was often shortened to \"the HSK\". The term has also been referred to in newspapers, books and university courses.\nCalvin's alter-egos.\nCalvin imagines himself as many great creatures and other people, including dinosaurs, elephants, jungle-farers and superheroes. Three of his alter egos are well-defined and recurrent:\nCardboard boxes.\nCalvin also has several adventures involving corrugated cardboard boxes, which he adapts for many imaginative and elaborate uses. In one strip, when Calvin shows off his Transmogrifier, a device that transforms its user into any desired creature or item, Hobbes remarks, \"It's amazing what they do with corrugated cardboard these days.\" Calvin is able to change the function of the boxes by rewriting the label and flipping the box onto another side. In this way, a box can be used not only for its conventional purposes (a storage container for water balloons, for example), but also as a flying time machine, a duplicator, a transmogrifier or, with the attachment of a few wires and a colander, a \"Cerebral Enhance-o-tron.\"\nIn the real world, Calvin's antics with his box have had varying effects. When he transmogrified into a tiger, he still appeared as a regular human child to his parents. However, in a story where he made several duplicates of himself, his parents are seen interacting with what does seem like multiple Calvins, including in a strip where two of him are seen in the same panel as his father. It is ultimately unknown what his parents do or do not see, as Calvin tries to hide most of his creations (or conceal their effects) so as not to traumatize them.\nIn addition, Calvin uses a cardboard box as a sidewalk kiosk to sell things. Often, Calvin offers merchandise no one would want, such as \"suicide drink\", \"a swift kick in the butt\" for one dollar or a \"frank appraisal of your looks\" for fifty cents. In one strip, he sells \"happiness\" for ten cents, hitting the customer in the face with a water balloon and explaining that he meant his own happiness. In another strip, he sold \"insurance\", firing a slingshot at those who refused to buy it. In some strips, he tried to sell \"great ideas\" and, in one earlier strip, he attempted to sell the family car to obtain money for a grenade launcher. In yet another strip, he sells \"life\" for five cents, where the customer receives nothing in return, which, in Calvin's opinion, is life.\nThe box has also functioned as an alternate secret meeting place for G.R.O.S.S., as the \"Box of Secrecy\".\nCalvinball.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nOther kids' games are all such a bore!\nThey've gotta have rules and they gotta keep score!\nCalvinball is better by far!\nIt's never the same! It's always bizarre!\nYou don't need a team or a referee!\nYou know that it's great, 'cause it's named after me!\n&lt;/poem&gt;\n \u2014Excerpt from the Calvinball theme song\nCalvinball is an improvisational sport/game introduced in a 1990 storyline that involved Calvin's negative experience of joining the school baseball team. Calvinball is a nomic or self-modifying game, a contest of wits, skill and creativity rather than stamina or athletic skill. The game is portrayed as a rebellion against conventional team sports and became a staple of the final five years of the comic. The only consistent rules of the game are that Calvinball may never be played with the same rules twice and that each participant must wear a mask.\nWhen asked how to play, Watterson stated: \"It's pretty simple: you make up the rules as you go.\" In most appearances of the game, a comical array of conventional and non-conventional sporting equipment is involved, including a croquet set, a badminton set, assorted flags, bags, signs, a hobby horse, water buckets and balloons, with humorous allusions to unseen elements such as \"time-fracture wickets\". Scoring is portrayed as arbitrary and nonsensical (\"Q to 12\" and \"oogy to boogy\") and the lack of fixed rules leads to lengthy argument between the participants as to who scored, where the boundaries are, and when the game is finished. Usually, the contest results in Calvin being outsmarted by Hobbes. The game has been described in one academic work not as a new game based on fragments of an older one, but as the \"constant connecting and disconnecting of parts, the constant evasion of rules or guidelines based on collective creativity.\" In an August 2025 opinion, United States Supreme Court Justice Ketanji Brown Jackson referenced Calvinball in describing the behavior of the Court's majority, saying: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;This is Calvinball jurisprudence with a twist. Calvinball has only one rule: There are no fixed rules. We seem to have two: that one, and this Administration always wins.\nSnowmen and other snow art.\nCalvin often creates horrendous/dark humor scenes with his snowmen and other snow sculptures. He uses the snowman for social commentary, revenge or pure enjoyment. Examples include Snowman Calvin being yelled at by Snowman Dad to shovel the snow; one snowman eating snow cones scooped out of a second snowman, who is lying on the ground with an ice-cream scoop in his back; a \"snowman house of horror\"; and snowmen representing people he hates. \"The ones I \"really\" hate are small, so they'll melt faster,\" he says. There was even an occasion on which Calvin accidentally brought a snowman to life and it made itself and a small army into \"deranged mutant killer monster snow goons.\"\nCalvin's snow art is often used as a commentary on art in general. For example, Calvin has complained more than once about the lack of originality in other people's snow art and compared it with his own grotesque snow sculptures. In one of these instances, Calvin and Hobbes claim to be the sole guardians of high culture; in another, Hobbes admires Calvin's willingness to put artistic integrity above marketability, causing Calvin to reconsider and make an ordinary snowman.\nWagon and sled rides.\nCalvin and Hobbes frequently ride downhill in a wagon or sled (depending on the season), as a device to add some physical comedy to the strip and because, according to Watterson, \"it's a lot more interesting ... than talking heads.\" While the ride is sometimes the focus of the strip, it also frequently serves as a counterpoint or visual metaphor while Calvin ponders the meaning of life, death, God, philosophy or a variety of other weighty subjects. Many of their rides end in spectacular crashes which leave them battered, beaten up and broken, a fact which convinces Hobbes to sometimes hop off before a ride even begins. In the final strip, Calvin and Hobbes depart on their sled to go exploring. This theme is similar (perhaps even an homage) to scenes in Walt Kelly's \"Pogo\". Calvin and Hobbes' sled has been described as the most famous sled in American arts since \"Citizen Kane\".\nG.R.O.S.S. (Get Rid of Slimy GirlS).\nG.R.O.S.S. (which is a backronym for Get Rid Of Slimy GirlS, \"otherwise it doesn't spell anything\") is a club in which Calvin and Hobbes are the only members. The club was founded in the garage of their house, but to clear space for its activities, Calvin and (purportedly) Hobbes push Calvin's parents' car, causing it to roll into a ditch (but not suffer damage); the incident prompts the duo to change the club's location to Calvin's treehouse. They hold meetings that involve finding ways to annoy and discomfort Susie Derkins, a girl and enemy of their club. Actions include planting a fake secret tape near her in an attempt to draw her into a trap, trapping her in a closet at their house and creating elaborate water balloon traps. Calvin gave himself and Hobbes important positions in the club, Calvin being \"Dictator-for-Life\" and Hobbes being \"President-and-First-Tiger\". They go into Calvin's treehouse for their club meetings and often get into fights during them. The password to get into the treehouse is intentionally long and difficult, which has, on at least one occasion, ruined Calvin's plans. As Hobbes is able to climb the tree without the rope, he is usually the one who comes up with the password, which often involves heaping praise upon tigers. An example of this can be seen in the comic strip where Calvin, rushing to get into the treehouse to throw things at a passing Susie Derkins, insults Hobbes, who is in the treehouse and thus has to let down the rope. Hobbes forces Calvin to say the password for insulting him. By the time Susie arrives, in time to hear Calvin saying some of the password, causing him to stumble, Calvin is on \"\"Verse Seven:\" Tigers are perfect!/The E-pit-o-me/of good looks and grace/and quiet..uh..um..dignity\". The opportunity to pelt Susie with something having passed, Calvin threatens to turn Hobbes into a rug.\nDinosaurs.\nDinosaurs play a heavy role in many of Calvin's imagination sequences. These strips will often begin with hyper-realistic scenes of dinosaur interactions, only to end with a cut to Calvin acting out these scenes as part of a daydream, often to his embarrassment. Watterson placed a heavy focus on accurately depicting dinosaurs, due to his own interest in them as well as to reinforce how real they are to Calvin.\nBooks.\nThere are 18 \"Calvin and Hobbes\" books, published from 1987 to 1997. These include 11 collections, which form a complete archive of the newspaper strips, except for a single daily strip from November 28, 1985. (The collections \"do\" contain a strip for this date, but it is not the same strip that appeared in some newspapers.) Treasuries usually combine the two preceding collections with bonus material and include color reprints of Sunday comics.\nWatterson included some new material in the treasuries. In \"The Essential Calvin and Hobbes\", which includes cartoons from the collections \"Calvin and Hobbes\" and \"Something Under the Bed Is Drooling\", the back cover features a scene of a giant Calvin rampaging through a town. The scene is based on Watterson's home town of Chagrin Falls, Ohio, and Calvin is holding the Chagrin Falls Popcorn Shop, an iconic candy and ice cream shop overlooking the town's namesake falls. Several of the treasuries incorporate additional poetry; \"The Indispensable Calvin and Hobbes\" book features a set of poems, ranging from just a few lines to an entire page, that cover topics such as Calvin's mother's \"hindsight\" and exploring the woods. In \"The Essential Calvin and Hobbes\", Watterson presents a long poem explaining a night's battle against a monster from Calvin's perspective. \"The Authoritative Calvin and Hobbes\" includes a story based on Calvin's use of the Transmogrifier to finish his reading homework.\nA complete collection of \"Calvin and Hobbes\" strips, in three hardcover volumes totaling 1440 pages, was released on October 4, 2005, by Andrews McMeel Publishing. It includes color prints of the art used on paperback covers, the treasuries' extra illustrated stories and poems and a new introduction by Bill Watterson in which he talks about his inspirations and his story leading up to the publication of the strip. The alternate 1985 strip is still omitted, and three other strips (January 7 and November 24, 1987, and November 25, 1988) have altered dialogue. A four-volume paperback version was released November 13, 2012.\nTo celebrate the release (which coincided with the strip's 20th anniversary and the tenth anniversary of its absence from newspapers), Bill Watterson answered 15 questions submitted by readers.\nEarly books were printed in smaller format in black and white. These were later reproduced in twos in color in the \"Treasuries\" (\"Essential\", \"Authoritative\" and \"Indispensable\"), except for the contents of \"Attack of the Deranged Mutant Killer Monster Snow Goons\". Those Sunday strips were not reprinted in color until the \"Complete\" collection was finally published in 2005.\nWatterson claims he named the books the \"\"Essential\", \"Authoritative\" and \"Indispensable\"\" because, as he says in \"The Calvin and Hobbes Tenth Anniversary Book\", the books are \"obviously none of these things.\"\n\"Teaching with Calvin and Hobbes\".\nAn officially licensed children's textbook entitled \"Teaching with Calvin and Hobbes\" was published in a single print run in Fargo, North Dakota, in 1993. The book is composed of \"Calvin and Hobbes\" strips that form story arcs, including \"The Binoculars\" and \"The Bug Collection\", followed by lessons based on the stories.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;What do you think the principal meant when he said they had \"quite a file\" on Calvin?\u2014\u200a\nThe book is rare and highly sought. It has been called the \"Holy Grail\" for \"Calvin and Hobbes\" collectors.\nReception.\nReviewing \"Calvin and Hobbes\" in 1990, \"Entertainment Weekly\"'s Ken Tucker gave the strip an A+ rating, writing \"Watterson summons up the pain and confusion of childhood as much as he does its innocence and fun.\"\nAcademic response.\nIn 1993, Gregory S. Paul, a paleontologist and paleoartist, praised Bill Watterson for the scientific accuracy of the dinosaurs appearing in \"Calvin and Hobbes\".\nIn her 1994 book \"When Toys Come Alive\", Lois Rostow Kuznets theorizes that Hobbes serves both as a figure of Calvin's childish fantasy life and as an outlet for the expression of libidinous desires more associated with adults. Kuznets also analyzes Calvin's other fantasies, suggesting that they are a second tier of fantasies utilized in places like school where transitional objects such as Hobbes would not be socially acceptable.\nPolitical scientist James Q. Wilson, in a paean to \"Calvin and Hobbes\" upon Watterson's decision to end the strip in 1995, characterized it as \"our only popular explication of the moral philosophy of Aristotle.\"\nA collection of original Sunday strips was exhibited at Ohio State University's Billy Ireland Cartoon Library &amp; Museum in 2001. Watterson himself selected the strips and provided his own commentary for the exhibition catalog, which was later published by Andrews McMeel as \"Calvin and Hobbes: Sunday Pages 1985\u20131995\".\nSince the discontinuation of \"Calvin and Hobbes\", individual strips have been licensed for reprint in schoolbooks, including the Christian homeschooling book \"The Fallacy Detective\" in 2002, and the university-level philosophy reader \"Open Questions: Readings for Critical Thinking and Writing\" in 2005; in the latter, the ethical views of Watterson and his characters Calvin and Hobbes are discussed in relation to the views of professional philosophers. In a 2009 evaluation of the entire body of \"Calvin and Hobbes\" strips using grounded theory methodology, Christijan D. Draper found that: \"Overall, \"Calvin and Hobbes\" suggests that meaningful time use is a key attribute of a life well lived,\" and that \"the strip suggests one way to assess the meaning associated with time use is through preemptive retrospection by which a person looks at current experiences through the lens of an anticipated future...\"\n\"Calvin and Hobbes\" strips were again exhibited at the Billy Ireland Cartoon Library &amp; Museum at The Ohio State University in 2014, in an exhibition entitled \"Exploring Calvin and Hobbes\". An exhibition catalog by the same title, which also contained an interview with Watterson conducted by Jenny Robb, the curator of the museum, was published by Andrews McMeel in 2015.\nLegacy.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nSince its concluding panel in 1995, \"Calvin and Hobbes\" has remained one of the most influential and well-loved comic strips of our time.\n \u2013\"The Atlantic\", \"How \"Calvin and Hobbes\" Inspired a Generation,\" October 25, 2013\nYears after its original newspaper run, \"Calvin and Hobbes\" has continued to exert influence in entertainment, art, and fandom.\nIn television, Calvin and Hobbes have been satirically depicted in stop motion animation in the 2006 and 2018 \"Robot Chicken\" episodes \"Lust for Puppets\" and \"Jew No. 1 Opens a Treasure Chest\" respectively, and in traditional animation in the 2009 \"Family Guy\" episode \"Not All Dogs Go to Heaven.\" In the 2013 \"Community\" episode \"Paranormal Parentage,\" the characters Abed Nadir (Danny Pudi) and Troy Barnes (Donald Glover) dress as Calvin and Hobbes, respectively, for Halloween.\nBritish artists, merchandisers, booksellers, and philosophers were interviewed for a 2009 BBC Radio 4 half-hour programme about the abiding popularity of the comic strip, narrated by Phill Jupitus.\nThe first book-length study of the strip, \"Looking for Calvin and Hobbes: The Unconventional Story of Bill Watterson and His Revolutionary Comic Strip\" by Nevin Martell, was first published in 2009; an expanded edition was published in 2010. The book chronicles Martell's quest to tell the story of \"Calvin and Hobbes\" and Watterson through research and interviews with people connected to the cartoonist and his work. The director of the later documentary \"Dear Mr. Watterson\" referenced \"Looking for Calvin and Hobbes\" in discussing the production of the movie, and Martell appears in the film.\nThe American documentary film \"Dear Mr. Watterson\", released in 2013, explores the impact and legacy of \"Calvin and Hobbes\" through interviews with authors, curators, historians, and numerous professional cartoonists.\nThe enduring significance of \"Calvin and Hobbes\" to international cartooning was recognized by the jury of the Angoul\u00eame International Comics Festival in 2014 by the awarding of its Grand Prix to Watterson, only the fourth American to ever receive the honor (after Will Eisner, Robert Crumb, and Art Spiegelman).\nFrom 2016 to 2021, author Berkeley Breathed included \"Calvin and Hobbes\" in various \"Bloom County\" cartoons. He launched the first cartoon on April Fool's Day 2016 and jokingly issued a statement suggesting that he had acquired \"Calvin and Hobbes\" from Bill Watterson, who was \"out of the Arizona facility, continent and looking forward to some well-earned financial security.\" While bearing Watterson's signature and drawing style as well as featuring characters from both \"Calvin and Hobbes\" and Breathed's \"Bloom County\", it is unclear whether Watterson had any input into these cartoons or not.\nSeveral artists published comics that were identified as being inspired by \"Calvin and Hobbes\". Some of them were the 2002 comic strip \"Macuando\", by Liniers, and 2005 Marvel comic book \"Fantastic Four Presents: Franklin Richards - Son of A Genius\", by Chris Eliopoulos and Marc Sumerak.\nSome of other notorious art made in \"Calvin and Hobbes\" style are the 2015 drawings of characters from \"\", from Disney artist Brian Kesinger, and memes created by the subreddit \"Donald and Hobbes\", parodying Donald Trump during the 2016 elections.\n\"Calvin and Hobbes\" has also been published on the internet. GoComics publishes the comic strips and it remains their most viewed comic, which cycles through old strips with an approximately 30-year delay. Michael Yingling created \"Calvin and Hobbes: The Search Engine\", capable of interacting with GoComics archive for reading online or downloading the comic strips. The comic strip was published online in Portuguese by the website CalvinBR, hosted on iG, and the blog Dep\u00f3sito do Calvin.\nGrown-up Calvin.\nPortraying Calvin as a teenager/adult has inspired writers.\nIn 2011, a comic strip appeared by cartoonists Dan and Tom Heyerman called \"Hobbes and Bacon\". The strip depicts Calvin as an adult, married to Susie Derkins with a young daughter named after philosopher Francis Bacon, to whom Calvin gives Hobbes. Though consisting of only four strips originally, \"Hobbes and Bacon\" received considerable attention when it appeared and was continued by other cartoonists and artists.\nA novel titled \"Calvin\" by CLA Young Adult Book Award\u2013winning author Martine Leavitt was published in 2015. The story tells of seventeen-year-old Calvin\u2014who was born on the day that \"Calvin and Hobbes\" ended, and who has now been diagnosed with schizophrenia\u2014and his hallucination of Hobbes, his childhood stuffed tiger. With his friend Susie, who might also be a hallucination, Calvin sets off to find Bill Watterson in the hope that the cartoonist can provide aid for Calvin's condition.\nThe titular character of the comic strip \"Frazz\" has been noted for his similar appearance and personality to a grown-up Calvin. Creator Jef Mallett has stated that although Watterson is an inspiration to him, the similarities are unintentional.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6060", "revid": "42907343", "url": "https://en.wikipedia.org/wiki?curid=6060", "title": "Campaign for Real Ale", "text": "British consumer organization\nThe Campaign for Real Ale (CAMRA) is an independent voluntary consumer organisation headquartered in St Albans, which promotes real ale, cider and perry and traditional British pubs and clubs.\nHistory.\nThe organisation was founded on 16 March 1971 in Kruger's Bar, Dunquin, County Kerry, Ireland, by Michael Hardman, Graham Lees, Jim Makin, and Bill Mellor, who were opposed to the growing mass production of beer and the homogenisation of the British brewing industry. The original name was the Campaign for the Revitalisation of Ale. Following the formation of the Campaign, the first annual general meeting took place in 1972, at the Rose Inn in Coton Road, Nuneaton. \nEarly membership consisted of the four founders and their friends. Interest in CAMRA and its objectives spread rapidly, with 5,000 members signed up by 1973. Other early influential members included Christopher Hutt, author of \"Death of the English Pub\", who succeeded Hardman as chairman, Frank Baillie, author of \"The Beer Drinker's Companion\", and later succeeded by frequent \"Good Beer Guide\" editor, Roger Protz.\nIn 1991, CAMRA had 30,000 members across the UK and abroad and, a year later, helped to launch the European Beer Consumers Union.\nIn December 2022, CAMRA issued guidelines advising volunteers to avoid terms like \"pub crawl\" or \"few beers with the lads\" in order to promote a more inclusive drinking culture. The guidelines also discouraged \"lad culture\" and emphasised the need for \"inclusive banter.\" This move generated public debate, including criticism from Conservative MP Bob Blackman, who questioned the necessity of such changes.\nIn 2023, the White Hart Inn in Grays, Essex, \u2014 winner of South West Essex CAMRA Pub of the Year in 2007, 2008, 2009, 2011, 2015, 2017, 2019, and 2020 \u2014 was banned from being considered for awards and excluded from the Good Beer Guide after displaying golliwog dolls. CAMRA stated that pubs should not display material considered offensive under its inclusivity guidelines. \nAt the 2025 AGM, CAMRA reported that it was facing significant financial strain. Membership had fallen to a six-year low, recruitment had stalled, and the organisation failed to return to pre-COVID levels.\nActivities.\nCAMRA's campaigns include promoting small brewing and pub businesses, reforming licensing laws, reducing tax on beer, and stopping continued consolidation among local British brewers. It also makes an effort to promote less common varieties of beer, including stout, porter, and mild, as well as traditional cider and perry.\nCAMRA's states that real ale should be served without the use of additional carbonation. This means that \"any beer brand which is produced in both cask and keg versions\" is not admitted to CAMRA festivals if the brewery's marketing is deemed to imply an equivalence of quality or character between the two versions.\nOrganisation.\nCAMRA is organised on a federal basis, over 200 local branches, each covering a particular geographical area of the UK, that contribute to the central body of the organisation based in St Albans. It is governed by a National Executive, made up of 12 voluntary unpaid directors elected by the membership. The local branches are grouped into 16 regions across the UK, such as the West Midlands or Wessex.\nPublications.\nCAMRA publishes the \"Good Beer Guide\", an annually compiled directory of the best 4,500 real ale outlets and listing of real ale brewers. CAMRA members received a monthly newspaper called \"What's Brewing\" until its April 2021 issue and there is a quarterly colour magazine called \"Beer\". It also maintains a National Inventory of Historic Pub Interiors to help bring greater recognition and protection to Britain's most historic pubs. In 2025, the printed winter edition of Beer Magazine was not distributed for reasons of financial austerity. It was published online only. \nIn addition to the \"Good Beer Guide\", Camra published \"Good Cider Guide\" between 1996\u20132005, listing cider and perry outlets and producers in the United Kingdom. The intention of the book is to be a \"guide for real cider-loving connoisseurs\". The lasst edition, published in 2005, contains details of over 550 cider outlets (pubs, clubs and off licences), and claims to include every producer in the UK. Producers and outlets are listed separately by county with maps and full directions, and there are also articles about cider and perry history, cider and perry making, and cider abroad.\nFestivals.\nCAMRA supports and promotes beer and cider festivals around the country, which are organised by local CAMRA branches. Generally, each festival charges an entry fee which either covers entry only or also includes a commemorative glass showing the details of the festival. A festival programme is usually also provided, with a list and description of the drinks available. Members may get discounted entrance to CAMRA festivals.\nThe Campaign also organises the annual Great British Beer Festival in August. It is now held in the Great, National &amp; West Halls at the Olympia Exhibition Centre, in Kensington, London, having been held for a few years at Earl's Court as well as regionally in the past at venues such as Brighton and Leeds. This is the UK's largest beer festival, with over 900 beers, ciders and perries available over the week long event.\nFor many years, CAMRA also organised the National Winter Ales Festival. However, in 2017 this was re-branded as the Great British Beer Festival Winter where they award the Champion Winter Beer of Britain. Unlike the Great British Beer Festival, the Winter event does not have a permanent venue and is rotated throughout the country every three years. Recent hosts have been Derby and Norwich, with the event currently held each February in Birmingham. In 2020 CAMRA also launched the Great Welsh Beer Festival, to be held in Cardiff in April.\nIn October 2024, CAMRA announced the relocation of the Great British Beer Festival from London to Birmingham, ending its 34-year run in the capital. Attendance at the new venue had failed to meet expectations resulting in a substantial loss. CAMRA announced that the GBBF and its Winter Festival would not take place in 2026.\nAwards.\nCAMRA presents awards for beers and pubs, such as the National Pub of the Year. The competition begins in the preceding year with branches choosing their local pub of the year through either a ballot or a panel of judges. The branch winners are entered into 16 regional competitions which are then visited by several individuals who agree the best using a scoring system that considers beer quality, aesthetic and welcome. The four finalists are announced each year before a ceremony to crown the winner in the spring. There are also the Pub Design Awards, which are held in association with English Heritage and the Victorian Society. These comprise several categories, including new build, refurbished and converted pubs.\nThe best known CAMRA award is the Champion Beer of Britain, which is selected at the Great British Beer Festival. Other awards include the Champion Beer of Scotland and the Champion Beer of Wales.\nNational Beer Scoring Scheme.\nCAMRA developed the National Beer Scoring Scheme (NBSS) as an easy to use scheme for judging beer quality in pubs, to assist CAMRA branches in selecting pubs for the \"Good Beer Guide\". CAMRA members input their beer scores online via WhatPub or through the Good Beer Guide app.\nPub heritage.\nThe CAMRA Pub Heritage Group identifies, records and helps to protect pub interiors of historic and/or architectural importance, and seeks to get them listed.\nThe group maintains two inventories of Heritage pubs, the National Inventory (NI), which contains only those pubs that have been maintained in their original condition (or have been modified very little) for at least thirty years, but usually since at least World War II. The second, larger, inventory is the Regional Inventory (RI), which is broken down by county and contains both those pubs listed in the NI and other pubs that are not eligible for the NI, for reasons such as having been overly modified, but are still considered historically important, or have particular architectural value.\nLocAle.\nThe LocAle scheme was launched in 2007 to promote locally brewed beers. The scheme functions slightly differently in each area, and is managed by each branch, but each is similar: if the beer is to be promoted as a LocAle it must come from a brewery within a predetermined number of miles set by each CAMRA branch, generally around 20, although the North London branch has set it at 30 miles from brewery to pub, even if it comes from a distribution centre further away; in addition, each participating pub must keep at least one LocAle for sale at all times.\nInvestment club.\nCAMRA members may join the CAMRA Members' Investment Club which, since 1989, has invested in real ale breweries and pub chains. As of January 2021 the club had over 3,000 members and owned investments worth over \u00a317 million. Although all investors must be CAMRA members, the CAMRA Members' Investment Club is not part of CAMRA Ltd.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6061", "revid": "42539290", "url": "https://en.wikipedia.org/wiki?curid=6061", "title": "CNO cycle", "text": "Nuclear fusion reaction\nIn astrophysics, the carbon\u2013nitrogen\u2013oxygen (CNO) cycle, sometimes called Bethe\u2013Weizs\u00e4cker cycle, after Hans Albrecht Bethe and Carl Friedrich von Weizs\u00e4cker, is one of the two known sets of fusion reactions by which stars convert hydrogen to helium, the other being the proton\u2013proton chain reaction (p\u2013p cycle), which is more efficient at the Sun's core temperature. The CNO cycle is hypothesized to be dominant in stars that are more than 1.3\u00a0times as massive as the Sun.\nUnlike the proton-proton reaction, which consumes all its constituents, the CNO cycle is a catalytic cycle. In the CNO cycle, four protons fuse, using isotopes of carbon, nitrogen, and oxygen as catalysts, each of which is consumed at one step of the CNO cycle, but re-generated in a later step. The end product is one alpha particle (a stable helium nucleus), two positrons, and two electron neutrinos.\nThere are various alternative paths and catalysts involved in the CNO cycles, but all these cycles have the same net result:\n4 H + 2 \n \u2192 He + 2 + 2 + 2 + 3 + \n \u2192 He + 2 + 7 + \nThe positrons will almost instantly annihilate with electrons, releasing energy in the form of gamma rays. The neutrinos escape from the star carrying away some energy. One nucleus goes on to become carbon, nitrogen, and oxygen isotopes through a number of transformations in a repeating cycle.\nThe proton\u2013proton chain is more prominent in stars the mass of the Sun or less. This difference stems from temperature dependency differences between the two reactions; pp-chain reaction starts at temperatures around (4\u00a0megakelvins), making it the dominant energy source in smaller stars. A self-maintaining CNO chain starts at approximately , but its energy output rises much more rapidly with increasing temperatures so that it becomes the dominant source of energy at approximately .\nThe Sun has a core temperature of around , and only of [&lt;noinclude /&gt;[helium-4|He]&lt;noinclude /&gt;] nuclei produced in the Sun are\nborn in the CNO cycle.\nThe CNO-I process was independently proposed by Carl von\u00a0Weizs\u00e4cker and Hans Bethe in the late 1930s.\nThe first reports of the experimental detection of the neutrinos produced by the CNO cycle in the Sun were published in 2020 by the BOREXINO collaboration. This was also the first experimental confirmation that the Sun had a CNO cycle, that the proposed magnitude of the cycle was accurate, and that von Weizs\u00e4cker and Bethe were correct.\nCold CNO cycles.\nUnder typical conditions found in stars, catalytic hydrogen burning by the CNO cycles is limited by proton captures. Specifically, the timescale for beta decay of the radioactive nuclei produced is faster than the timescale for fusion. Because of the long timescales involved, the cold CNO cycles convert hydrogen to helium slowly, allowing them to power stars in quiescent equilibrium for many years.\nCNO-I.\nThe first proposed catalytic cycle for the conversion of hydrogen into helium was initially called the carbon\u2013nitrogen cycle (CN-cycle), also referred to as the Bethe\u2013Weizs\u00e4cker cycle in honor of the independent work of Carl Friedrich von\u00a0Weizs\u00e4cker in 1937\u201338 and Hans Bethe. Bethe's 1939 papers on the CN-cycle drew on three earlier papers written in collaboration with Robert Bacher and Milton Stanley Livingston and which came to be known informally as \"Bethe's Bible\". It was considered the standard work on nuclear physics for many years and was a significant factor in his being awarded the 1967 Nobel Prize in Physics. Bethe's original calculations suggested the CN-cycle was the Sun's primary source of energy. This conclusion arose from a belief that is now known to be mistaken, that the abundance of nitrogen in the sun is approximately 10%; it is actually less than half a percent. The CN-cycle, named as it contains no stable isotope of oxygen, involves the following cycle of transformations:\n [&lt;noinclude /&gt;[carbon-12|C]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[nitrogen-13|N]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[carbon-13|C]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[nitrogen-14|N]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[oxygen-15|O]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[nitrogen-15|N]&lt;noinclude /&gt;] \u2192 C\nThis cycle is now understood as being the first part of a larger process, the CNO-cycle, and the main reactions in this part of the cycle (CNO-I) are: \nwhere the carbon-12 nucleus used in the first reaction is regenerated in the last reaction. After the two positrons emitted annihilate with two ambient electrons producing an additional , the total energy released in one cycle is 26.73\u00a0MeV; in some texts, authors are erroneously including the positron annihilation energy in with the beta-decay Q-value and then neglecting the equal amount of energy released by annihilation, leading to possible confusion. All values are calculated with reference to the Atomic Mass Evaluation 2003.\nThe limiting (slowest) reaction in the CNO-I cycle is the proton capture on N. In 2006 it was experimentally measured down to stellar energies, revising the calculated age of globular clusters by around 1 billion years.\nThe neutrinos emitted in beta decay will have a spectrum of energy ranges, because although momentum is conserved, the momentum can be shared in any way between the positron and neutrino, with either emitted at rest and the other taking away the full energy, or anything in between, so long as all the energy from the Q-value is used. The total momentum received by the positron and the neutrino is not great enough to cause a significant recoil of the much heavier daughter nucleus and hence, its contribution to kinetic energy of the products, for the precision of values given here, can be neglected. Thus the neutrino emitted during the decay of nitrogen-13 can have an energy from zero up to , and the neutrino emitted during the decay of oxygen-15 can have an energy from zero up to . On average, about 1.7\u00a0MeV of the total energy output is taken away by neutrinos for each loop of the cycle, leaving about available for producing luminosity.\nCNO-II.\nIn a minor branch of the above reaction, occurring in the Sun's core 0.04% of the time, the final reaction involving [&lt;noinclude /&gt;[nitrogen-15|N]&lt;noinclude /&gt;] shown above does not produce carbon-12 and an alpha particle, but instead produces oxygen-16 and a photon and continues\nIn detail:\nLike the carbon, nitrogen, and oxygen involved in the main branch, the fluorine produced in the minor branch is merely an intermediate product; at steady state, it does not accumulate in the star.\nCNO-III.\nThis subdominant branch is significant only for massive stars. The reactions are started when one of the reactions in CNO-II results in fluorine-18 and a photon instead of nitrogen-14 and an alpha particle, and continues\n [&lt;noinclude /&gt;[oxygen-17|O]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[fluorine-18|F]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[oxygen-18|O]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[nitrogen-15|N]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[oxygen-16|O]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[fluorine-17|F]&lt;noinclude /&gt;] \u2192 O\nIn detail:\nCNO-IV.\nLike the CNO-III, this branch is also only significant in massive stars. The reactions are started when one of the reactions in CNO-III results in fluorine-19 and a photon instead of nitrogen-15 and an alpha particle, and continues\nIn detail:\nIn some instances F can combine with a helium nucleus, forming Na, to start a neon-sodium cycle, in which:\nThe sodium-23 can also turn into magnesium-24 after proton bombardment, initiating a magnesium-aluminum cycle, in which:\nHot CNO cycles.\nUnder conditions of higher temperature and pressure, such as those found in novae and X-ray bursts, the rate of proton captures exceeds the rate of beta-decay, pushing the burning to the proton drip line. The essential idea is that a radioactive species will capture a proton before it can beta decay, opening new nuclear burning pathways that are otherwise inaccessible. Because of the higher temperatures involved, these catalytic cycles are typically referred to as the hot CNO cycles; because the timescales are limited by beta decays instead of proton captures, they are also called the beta-limited CNO cycles.\nHCNO-I.\nThe difference between the CNO-I cycle and the HCNO-I cycle is that [&lt;noinclude /&gt;[nitrogen-13|N]&lt;noinclude /&gt;] captures a proton instead of decaying, leading to the total sequence\n[&lt;noinclude /&gt;[carbon-12|C]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[nitrogen-13|N]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[oxygen-14|O]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[nitrogen-14|N]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[oxygen-15|O]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[nitrogen-15|N]&lt;noinclude /&gt;]\u2192C\nIn detail:\nHCNO-II.\nThe notable difference between the CNO-II cycle and the HCNO-II cycle is that [&lt;noinclude /&gt;[fluorine-17|F]&lt;noinclude /&gt;] captures a proton instead of decaying, and neon is produced in a subsequent reaction on [&lt;noinclude /&gt;[fluorine-18|F]&lt;noinclude /&gt;], leading to the total sequence\n[&lt;noinclude /&gt;[nitrogen-15|N]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[oxygen-16|O]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[fluorine-17|F]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[neon-18|Ne]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[fluorine-18|F]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[oxygen-15|O]&lt;noinclude /&gt;]\u2192N\nIn detail:\nHCNO-III.\nAn alternative to the HCNO-II cycle is that [&lt;noinclude /&gt;[fluorine-18|F]&lt;noinclude /&gt;] captures a proton moving towards higher mass and using the same helium production mechanism as the CNO-IV cycle as\nF\u2192[&lt;noinclude /&gt;[neon-19|Ne]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[fluorine-19|F]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[oxygen-16|O]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[fluorine-17|F]&lt;noinclude /&gt;]\u2192[&lt;noinclude /&gt;[neon-18|Ne]&lt;noinclude /&gt;]\u2192F\nIn detail:\nUse in astronomy.\nWhile the total number of \"catalytic\" nuclei are conserved in the cycle, in stellar evolution the relative proportions of the nuclei are altered. When the cycle is run to equilibrium, the ratio of the carbon-12/carbon-13 nuclei is driven to 3.5, and nitrogen-14 becomes the most numerous nucleus, regardless of initial composition. During a star's evolution, convective mixing episodes moves material, within which the CNO cycle has operated, from the star's interior to the surface, altering the observed composition of the star. Red giant stars are observed to have lower carbon-12/carbon-13 and carbon-12/nitrogen-14 ratios than do main sequence stars, which is considered to be convincing evidence for the operation of the CNO cycle.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6062", "revid": "5217210", "url": "https://en.wikipedia.org/wiki?curid=6062", "title": "Craps", "text": "Dice game\nCraps is a dice game in which players bet on the outcomes of the roll of a pair of dice. Players can wager money against each other (playing \"street craps\") or against a bank (\"casino craps\"). Because it requires little equipment, \"street craps\" can be played in informal settings. While shooting craps, players may use slang terminology to place bets and actions.\nHistory.\nCraps developed in the United States from a simplification of the western European game of Hazard, also spelled Hazzard or Hasard. The origins of Hazard are obscure and may date to the Crusades; a detailed description of Hazard was provided by Edmond Hoyle in \"Hoyle's Games, Improved\" (1790). At approximately the same time (1788), \"Krabs\" was documented as a French variation on Hazard.\nIn aristocratic London, crabs was the epithet for the sum combinations of two and three for two rolled dice, which in Hazard are instant-losing numbers for the first dice roll, regardless of the shooter's selected main number. The name craps is derived from the corruption of this term crabs (or Krabs) to creps and then craps.\nAccording to some accounts, Hazard was brought from London to New Orleans in approximately 1805 by the returning Bernard Xavier Philippe de Marigny de Mandeville, the young gambler and scion of a family of wealthy landowners in colonial Louisiana. Hazard allows the dice shooter to choose any number from five to nine as their \"main\" number; in a pamphlet published in 1933, Edward Tinker claimed that Marigny simplified the game by making the main always seven, which is the mathematically optimal choice, i.e., the choice with the lowest disadvantage for the shooter. However, more recent research indicates that Marigny played an unmodified version of Hazard, which had been played in America since at least the 1600s. Instead, John Scarne credits anonymous Black American inventors with simplifying and streamlining Hazard, increasing the pace of the game and adding a variety of wagers.\nRegardless of who deserves credit for simplifying Hazard, the game initially was called Pass from the French word \"pas\" (meaning \"pace\" or \"step\"), and was popularized by the underclass starting in the early 19th century. Field hands taught their friends and deckhands, who carried the new game up the Mississippi River and its tributaries, although the game was never popular amongst the riverboat gamblers. Marigny gave the name Rue de Craps to a street in his new subdivision in New Orleans; in that city, craps experienced a resurgence of popularity in the late 1830s, but was not played in gaming houses until the 1890s. Budd Theobald credits the cultural exchange between attendants and railroad passengers on Pullman cars for popularizing the game, which eventually spread throughout America by the 1910s, when it was described as \"the gambling game of [the country]\" in \"Foster's Complete Hoyle\" (1914).\nThe craps numbers of 2, 3, and 12 are similarly derived from Hazard. If the main is seven, then the two-dice sum of twelve is added to the crabs as a losing number on the first dice roll. This condition is retained in the simplified game called Pass. All three losing numbers (2, 3, and 12) on the first roll of Pass are jointly called the craps numbers. The central game Pass gradually has been supplemented over the decades by many companion games and wagers which can be played simultaneously with Pass; these are now collectively known as craps.\nEarly versions of bank craps played in casinos made money either by charging a commission to shooters or offering short odds on the various wagers, primarily on the \"Pass line\" bet for the shooter to win against the house. In approximately 1907, a dicemaker named John H. Winn in Philadelphia introduced a layout which featured a space to wager on \"Don't Pass\" (i.e., for the shooter to lose) in addition to \"Pass\". Virtually all modern casinos use his innovation, which incentivizes casinos to use fair dice. As introduced by Winn, \"Don't Pass\" bets were taken with a 5 percent commission to ensure the house retained an edge in running the game; this was replaced by the Bar-3 push for \"Don't Pass\", and later by the Bar-12 (or Bar-2) push.\nCraps exploded in popularity during World War II, which brought most young American men of every social class into the military. The street version of craps was popular among service members who often played it using a blanket as a shooting surface. Their military memories led to craps becoming the dominant casino game in postwar Las Vegas and the Caribbean.\nAfter 1960, a few casinos in Europe, Australia, and Macau began offering craps, and, after 2004, online casinos extended the game's spread globally. Craps has been featured in a number of newer casinos, including the idea of expanding into formerly unavailable locals on the coastline.\nBank craps.\nBank craps or casino craps is played by one or more players betting against the casino rather than each other. Both the players and the dealers stand around a large rectangular craps table. Sitting is discouraged by most casinos unless a player has medical reasons for requiring a seat.\nThe basic flow of a single game is:\nCraps table.\nPlayers use casino chips rather than cash to bet on the Craps \"layout\", a fabric surface which displays the various bets. The bets vary somewhat among casinos in availability, locations, and payouts. The tables roughly resemble bathtubs and come in various sizes. In some locations, chips may be called checks, tokens, or plaques.\nAgainst one long side is the casino's table bank: as many as two thousand casino chips in stacks of 20. The opposite long side is usually a long mirror. The U-shaped ends of the table have duplicate layouts and standing room for approximately eight players. In the center of the layout is an additional group of side bets which are used by players from both ends. The vertical walls at each end are usually covered with a rubberized target surface covered with small pyramid shapes to randomize the dice which strike them. The top edges of the table walls have one or two horizontal grooves in which players may store their reserve chips.\nThe table is run by up to four casino employees: a boxman seated (usually the only seated employee) behind the casino's bank, who manages the chips, supervises the dealers, and handles \"coloring up\" players (exchanging small chip denominations for larger denominations in order to preserve the chips at a table); two base dealers who stand to either side of the boxman and collect and pay bets to players around their half of the table; and a stickman who stands directly across the table from the boxman, takes and pays (or directs the base dealers to do so) the bets in the center of the table, announces the results of each roll (usually with a distinctive patter), and moves the dice across the layout with an elongated wooden stick.\nSome smaller casinos have introduced \"mini-craps\" tables which are operated with only two dealers; rather than being two essentially identical sides and the center area, a single set of major bets is presented, split by the center bets. Responsibility of the dealers is adjusted: while the stickman continues to handle the center bets, it is the base dealer who handles all other bets (as well as cash and chip exchanges).\nBy contrast, in \"street craps\", there is no marked table and often the game is played with no back-stop against which the dice are to hit. Despite the name \"street craps\", this game is often played in houses, usually on an un-carpeted garage or kitchen floor. The wagers are made in cash, never in chips, and are usually thrown down onto the ground or floor by the players. There are no attendants, and so the progress of the game, fairness of the throws, and the way that the payouts are made for winning bets are self-policed by the players.\nDice.\nThe dice used at casinos for craps and many other games are sometimes called \"perfect\" or \"gambling house dice\". These are generally made from translucent extruded cellulose, with perfectly square edges each in length, with pips drilled deep and filled with opaque paint matching the density of cellulose, which ensures the dice remain balanced. The dice are buffed and polished to a high glossy finish after the pips are set, and the edges usually are left sharp, also called square or razor edge. To discourage cheating and dice substitution, each die carries a serial number and the casino's logo or name. New Jersey specifies the maximum size of the die is on a side.\nUnder New Jersey regulations, the shooter selects two dice from a set of at least five.\nRules of play.\nEach casino may set which bets are offered and different payouts for them, though a core set of bets and payouts is typical. Players take turns rolling two dice and whoever is throwing the dice is called the \"shooter\". Players can bet on the various options by placing chips directly on the appropriately-marked sections of the layout, or asking the base dealer or stickman to do so, depending on which bet is being made.\nWhile acting as the shooter, a player must have a bet on either the \"Pass\" or the \"Don't Pass\" line or both. \"Pass\" and \"Don't Pass\" are sometimes called \"Win\" and \"Lose\", \"Do\" and \"Don't\", or \"Right\" and \"Wrong\". The game is played in rounds and these \"Pass\" and \"Don't Pass\" bets are betting on the outcome of a single round. The shooter is presented with multiple dice (typically five) by the \"stickman\", and must choose two for the round. The remaining dice are returned to the stickman's bowl and are not used.\nEach round has two phases: \"come-out\" and \"point\". Dice are passed to the left.\nPhase 1 (Come-out).\nTo start a round, the shooter makes one or more \"come-out\" rolls. While the come-out roll may specifically refer to the first roll of a new shooter, any roll where no point is established may be referred to as a come-out. By this definition the start of any new round regardless of whether it is the shooter's first toss can be referred to as a come-out roll. The shooter must shoot toward the farther back wall and is generally required to hit the farther back wall with both dice. Casinos may allow a few warnings before enforcing the dice to hit the back wall and are generally lenient if at least one die hits the back wall. Both dice must be tossed in one throw. If only one die is thrown the shot is invalid.\nA come-out roll of 2, 3, or 12 is called \"craps\" or \"crapping out\", and anyone betting the Pass line loses. On the other hand, anyone betting the Don't Pass line on come out wins with a roll of 2 or 3 and ties (pushes) if a 12 is rolled; in some rules, the 2 pushes instead of the 12, in which case the 3 and 12 win a Don't Pass bet. Shooters may keep rolling after crapping out; the dice are only required to be passed if a shooter sevens out (rolls a seven after a point has been established). A come-out roll of 7 or 11 is a \"natural\"; the Pass line wins and Don't Pass loses. The other possible numbers are the point numbers: 4, 5, 6, 8, 9, and 10. If the shooter rolls one of these numbers on the come-out roll, this establishes the \"point\" \u2013 to \"pass\" or \"win\", the point number must be rolled again before a seven.\nPhase 2 (Point).\nThe dealer flips a button to the \"On\" side and moves it to the point number signifying the second phase of the round. If the shooter \"hits\" the point value again (any value of the dice that sum to the point will do; the shooter does not have to exactly repeat the exact combination of the come-out roll) before rolling a seven, the Pass line wins and a new round starts. If the shooter rolls any seven before repeating the point number (a \"seven-out\"), the Pass line loses, the Don't Pass line wins, and the dice pass clockwise to the next new shooter for the next round. Once a point has been established, any multi-roll bets (including line bets and odds for Pass, Don't Pass, or both) are unaffected by the 2, 3, 11, or 12; the only numbers which affect the round are the established point, any specific bet on a number, or any 7. Any single roll bet is always affected (win or lose) by the outcome of any roll.\nBasic wagering rules.\nAny player can make a bet on Pass or Don't Pass as long as a point has not been established, or Come or Don't Come as long as a point is established. All other bets, including an increase in odds behind the Pass and Don't Pass lines, may be made at any time. All bets other than Pass line and Come may be removed or reduced any time before the bet loses. This is known as \"taking it down\" in craps.\nThe maximum bet for Place, Buy, Lay, Pass, and Come bets are generally equal to table maximum. Lay bet maximum are equal to the table maximum win, so players wishing to lay the 4 or 10 may bet twice that amount of the table maximum for the win to be table maximum. Odds behind Pass, Come, Don't Pass, and Don't Come may be however larger than the odds offered allows and can be greater than the table maximum in some casinos. Don't odds are capped on the maximum allowed win. Some casino allow the odds bet itself to be larger than the maximum bet allowed as long as the win is capped at maximum odds. Single rolls bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum allowed single roll bet is based on the maximum allowed win from a single roll.\nIn all the above scenarios, whenever the Pass line wins, the Don't Pass line loses, and vice versa, with one exception: on the come-out roll, a roll of 12 will cause Pass Line bets to lose, but Don't Pass bets are pushed (or \"barred\"), neither winning nor losing; this is done to establish a house edge for Don't Pass bets. (The same applies to \"Come\" and \"Don't Come\" bets, discussed below.)\nJoining a game.\nA player wishing to play craps without being the shooter should approach the craps table and first check to see if the dealer's \"On\" button is on any of the point numbers.\nIn either case, all single or multi-roll proposition bets may be placed in either of the two phases.\nBetween dice rolls there is a period for dealers to make payouts and collect losing bets, after which players can place new bets. The stickman monitors the action at a table and decides when to give the shooter the dice, after which no more betting is allowed.\nWhen joining the game, one should place money on the table rather than passing it directly to a dealer. The dealer's exaggerated movements during the process of \"making change\" or \"change only\" (converting currency to an equivalent in casino cheques) are required so that any disputes can be later reviewed against security camera footage.\nRolling.\nThe dealers will insist that the shooter roll with one hand and that the dice bounce off the far wall surrounding the table. These requirements are meant to keep the game fair (preventing switching the dice or making a \"controlled shot\"). If a die leaves the table, the shooter will usually be asked to select another die from the remaining three but can request permission to use the same die if it passes the boxman's inspection. This requirement exists to keep the game fair and reduce the chance of loaded dice.\nNames of rolls.\nThere are many local variants of the calls made by the stickman for rolls during a craps game. These frequently incorporate a reminder to the dealers as to which bets to pay or collect.\n The two ones that compose it look like a pair of small, beady eyes. During actual play, more common terms are \"two craps two\" during the comeout roll because the Pass line bet is lost on a comeout crap roll and / or because a bet on any craps would win. \"Aces; double the field\" would be a more common call when not on the comeout roll to remind the dealers to pay double on the field bets and encourage the field bettor to place subsequent bets and / or when no crap bets have been placed. Another name for the two is \"loose deuce\" or \"Snickies\" due to it sounding like \"Snake eyes\" but spoken with an accent.\n Typically called as \"three craps three\" during the comeout roll, or \"three, ace deuce, come away single\" when not on the comeout to signify the come bet has been lost and to pay single to any field bettors. Three may also be referred to as \"ace caught a deuce\", \"Tracy\", or even less often \"acey deucey\".\n usually hard, is sometimes referred to as \"Little Joe from Kokomo\" or \"Little Joe on the front row\" or just \"Little Joe\". A hard four can be called a \"ballerina\" because it is two-two (\"tutu\").\n is frequently called \"no field five\" in casinos in which five is not one of the field rolls and thus not paid in the field bets. Other names for a five are \"fever\" and \"little Phoebe\".\n may be referred to as \"Jimmie Hicks\" or \"Jimmie Hicks from the sticks\", examples of rhyming slang. On a win, the six is often called \"666 winner 6\" followed by \"came hard\" or \"came easy\".\n rolled as six-one is sometimes called \"six ace\" or \"up pops the Devil\". Older dealers and players may use the term \"Big Red\" because craps tables once prominently featured a large red \"7\" in the center of the layout for the one-roll seven bet. During the comeout, the seven is called \"seven, front line winner\", frequently followed by \"pay the line\" and / or \"take the don'ts\". After the point is established, a seven is typically called by simply \"7 out\" or \"7 out 7\"..\n rolled the hard way, as opposed to an \"easy eight\", is sometimes called an \"eighter from Decatur\". It can also be known as a \"square pair\", \"mom and dad\", or \"Ozzie and Harriet\".\n is called a \"centerfield nine\" in casinos in which nine is one of the field rolls, because nine is the center number shown on the layout in such casinos (2\u20133\u20134\u20139\u201310\u201311\u201312). In Atlantic City, a four-five is called a \"railroad nine\". The four-five nine is also known as \"Jesse James\" because the outlaw Jesse James was killed by a .45 caliber pistol. Other names for the nine include \"Nina from Pasadena\", \"Nina at the Marina\", and \"niner from Carolina\". Nine can also be referred to as \"Old Mike\", named after NBA Hall-of-Famer Michael Jordan, who wore No. 9 in his FIBA international career, when players could only wear numbers 4 to 15.\n the hard way is \"a hard ten\", \"dos equis\" (Spanish, meaning \"two X's\", because the pip arrangement on both dice on this roll resembles \"XX\"), or \"Hard ten \u2013 a woman's best friend\", an example of both rhyming slang and sexual double entendre. Ten as a pair of fives may also be known as \"puppy paws\" or \"a pair of sunflowers\" or \"Big Dick\" or \"Big John.\" Another slang for a hard ten is \"moose head\", because it resembles a moose's antlers. This phrase came from players in the Pittsburgh area.\n called out as \"yo\" or \"yo-leven\" to prevent being misheard as \"seven\". An older term for eleven is \"six five, no jive\" because it is a winning roll. During the comeout, eleven is typically followed by \"front line winner\". After the point is established, \"good field and come\" is often added.\n known as \"boxcars\" because the spots on the two dice that show six-six look like schematic drawings of railroad boxcars; it is also called \"midnight\", referring to twelve o'clock; and also as \"double-action field traction\", because of the (standard) 2-to-1 pay on Field bets for this roll and the fact that the arrangement of the pips on the two dice, when laid end-to-end, resemble tire tracks. On tables that pay triple the field on a twelve roll, the stickman will often loudly exclaim \"triple\" either alone or in combination with \"12 craps 12\" or \"come away triple\".\nRolls of 4, 6, 8, and 10 are called \"hard\" or \"gag\", when rolled as a double, or \"easy\", when rolled with two different numbers. For example, rolls will be called \"six the hard way\", \"easy eight\", \"hard ten\", etc., because of their significance in center table bets known as the \"hard ways\". Hard way rolls are so named because there is only one way to roll them (i.e., the value on each die is the same when the number is rolled). Consequently, it is more likely to roll the number in different-number combinations (easy) rather than as a double (hard).\nNote: Individual casinos may pay some of these bets at different payout ratios than those listed below. Some bets are listed more than once below\u00a0\u2013 the most common payout in North American casinos is listed first, followed by other known variants.\nNote: \"True Odds\" do not vary.\nBet odds and summary.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nThe probability of dice combinations determine the odds of the payout. There are a total of 36 (6 \u00d7 6) possible combinations when rolling two dice. The following chart shows the dice combinations needed to roll each number. The two and twelve are the hardest to roll since only one combination of dice is possible. The game of craps is built around the dice roll of seven, since it is the most easily rolled dice combination.\nViewed another way:\nThe expected value of all bets is usually negative, such that the average player will always lose money. This is because the house always sets the paid odds to below the actual odds. The only exception is the \"odds\" bet that the player is allowed to make after a point is established on a pass / come Don't Pass / Don't Come bet (the odds portion of the bet has a long-term expected value of 0). However, this \"free odds\" bet cannot be made independently, so the expected value of the entire bet, including odds, is still negative. Since there is no correlation between die rolls, there is normally no possible long-term winning strategy in craps.\nThere are occasional promotional variants that provide either no house edge or even a player edge. One example is a field bet that pays 3:1 on 12 and 2:1 on either 3 or 11. Overall, given the 5:4 true odds of this bet, and the weighted average paid odds of approximately 7:5, the player has a 5% advantage on this bet. This is sometimes seen at casinos running limited-time incentives, in jurisdictions or gaming houses that require the game to be fair, or in layouts for use in informal settings using play money. No casino currently runs a craps table with a bet that yields a player edge full-time.\nMaximizing the size of the odds bet in relation to the line bet will reduce, but never eliminate the house edge, and will increase variance. Most casinos have a limit on how large the odds bet can be in relation to the line bet, with single, double, and five times odds common. Some casinos offer 3\u20134\u20135 odds, referring to the maximum multiple of the line bet a player can place in odds for the points of 4 and 10, 5 and 9, and 6 and 8, respectively. During promotional periods, a casino may even offer 100\u00d7 odds bets, which reduces the house edge to almost nothing, but dramatically increases variance, as the player will be betting in large betting units.\nSince several of the multiple roll bets pay off in ratios of fractions on the dollar, it is important that the player bets in multiples that will allow a correct payoff in complete dollars. Normally, payoffs will be rounded down to the nearest dollar, resulting in a higher house advantage. These bets include all place bets, taking odds, and buying on numbers 6, 8, 5, and 9, as well as laying all numbers.\nTypes of wagers.\nLine bets.\nThe shooter is required to make either a Pass line bet or a Don't Pass bet if he wants to shoot. On the come out roll each player may only make one bet on the Pass or Don't Pass, but may bet both if desired. The Pass Line and Don't Pass bet is optional for any player not shooting. In rare cases, some casinos require all players to make a minimum Pass Line or Don't Pass bet (if they want to make any other bet), whether they are currently shooting or not.\nPass line.\nThe basic bet in craps is the Pass line bet, which is a bet for the shooter to win. This bet must be at least the table minimum and at most the table maximum.\nThe Pass line bet pays even money.\nThe Pass line bet is a contract bet. Once a Pass line bet is made, it is always working and cannot be turned \"Off\", taken down, or reduced until a decision is reached \u2013 the point is made, or the shooter sevens out. A player may increase any corresponding odds (up to the table limit) behind the Pass line at any time after a point is established. Players may only bet the Pass line on the come out roll when no point has been established, unless the casino allows put betting where the player can bet Pass line or increase an existing Pass line bet whenever desired and may take odds immediately if the point is already on.\nDon't Pass.\nA Don't Pass bet is a bet for the shooter to lose (\"seven out, line away\") and is almost the opposite of the Pass line bet. Like the Pass bet, this bet must be at least the table minimum and at most the table maximum.\nThe Don't Pass bet pays even money.\nThe Don't Pass bet is a no-contract bet. After a point is established, a player may take down or reduce a Don't Pass bet and any corresponding odds at any time because odds of rolling a 7 before the point is in the player's favor. Once taken down or reduced, however, the Don't Pass bet may not be restored or increased. Because the shooter must have a line bet the shooter generally may not reduce a Don't Pass bet below the table minimum. In Las Vegas, a majority of casinos will allow the shooter to move the bet to the Pass line in lieu of taking it down; however, in other areas such as Pennsylvania and Atlantic City, this is not allowed. Even though players are allowed to remove the Don't Pass line bet after a point has been established, the bet cannot be turned \"Off\" without being removed. Players choosing to remove the Don't Pass line bet can no longer lay odds behind the Don't Pass line. The player can, however, still make standard lay bets on any of the point numbers (4, 5, 6, 8, 9, 10).\nThe casino chooses either Bar-2 or Bar-12, but not both. The push on 12 or 2 is mathematically necessary to maintain the house edge over the player. Other casinos allow the player to choose to either push on 2 (\"Bar Aces\") or push on 12 (\"Bar Sixes\") depending on where it is placed on the layout. Some older bank crap games used Bar-3 (\"Bar Ace-Deuce\"), which increases the house edge.\nThere are two different ways to calculate the odds and house edge of this bet. The summary table gives the numbers considering that the game ends in a push when a 12 is rolled, rather than being undetermined. Betting on Don't Pass is often called \"playing the dark side\", and it is considered by some players to be in poor taste, or even taboo, because it goes directly against conventional play, winning when most of the players lose.\nPass odds.\nIf a 4, 5, 6, 8, 9, or 10 is thrown on the come-out roll (i.e., when a point is established), most casinos allow Pass line players to take odds by placing up to some predetermined multiple of the Pass line bet, behind the Pass line. This additional bet wins if the point is rolled again before a 7 is rolled (the point is made) and pays at the true odds:\nUnlike the Pass line bet itself, the Pass line odds bet can be turned \"Off\" (not working), removed or reduced anytime before it loses. In Las Vegas, generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and Pass bet must be table minimum so players can bet the minimum single unit on odds depending on the point. If the point is a 4 or 10, players can bet as little as $1 on odds if the table minimum is low such as is $5, $10 or $15. If the player requests the Pass odds be not working (\"Off\") and the shooter sevens-out or hits the point, the Pass line bet will be lost or doubled and the Pass odds returned.\nIndividual casinos (and sometimes tables within a casino) vary greatly in the maximum odds they offer, from single or double odds (one or two times the Pass line bet) up to 100\u00d7 or even unlimited odds. A variation often seen is \"3-4-5\u00d7 Odds\", where the maximum allowed odds bet depends on the point: three times if the point is 4 or 10; four times on points of 5 or 9; or five times on points of 6 or 8. This rule simplifies the calculation of winnings: a maximum Pass odds bet on a 3\u20134\u20135\u00d7 table will always be paid at six times the Pass line bet regardless of the point.\nAs odds bets are paid at true odds, in contrast with the Pass line which is always even money, taking odds on a minimum Pass line bet lessens the house advantage compared with betting the same total amount on the Pass line only. A maximum odds bet on a minimum Pass line bet often gives the lowest house edge available in any game in the casino. However, the odds bet cannot be made independently, so the house retains an edge on the Pass line bet itself.\nDon't Pass odds.\nIf a player is playing Don't Pass instead of pass, they also may lay odds by placing chips behind the Don't Pass line. If a 7 comes before the point is rolled, the Don't Pass odds pay at true odds:\nTypically the maximum lay bet will be expressed such that a player may win up to an amount equal to the maximum odds multiple at the table. If a player lays maximum odds with a point of 4 or 10 on a table offering five-times odds, he would be able to lay a maximum of ten times the amount of his Don't Pass bet. At 5\u00d7 odds table, the maximum amount the combined bet can win will always be 6\u00d7 the amount of the Don't Pass bet. Players can bet table minimum odds if desired and win less than table minimum.\nLike the Don't Pass bet the odds can be removed or reduced. Unlike the Don't Pass bet itself, the Don't Pass odds can be turned \"Off\" (not working). In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine lay odds and Don't Pass bet must be table minimum so players may bet as little as the minimum two units on odds depending on the point. If the point is a 4 or 10 players can bet as little as $2 if the table minimum is low such as $5, $10 or $15 tables. If the player requests the Don't Pass odds to be not working (\"Off\") and the shooter hits the point or sevens-out, the Don't Pass bet will be lost or doubled and the Don't Pass odds returned. Unlike a standard lay bet on a point, lay odds behind the Don't Pass line does not charge commission (vig).\nCome bet.\nA player making a Come bet is wagering on the first number that \"comes\" from the shooter's next roll, regardless of the table's phase. In other words, a Come bet can be considered as starting an entirely new Pass line bet, unique to that player. \nThe Come bet pays off at even money, like the Pass line bet.\nCome bets can only be made after a point has been established since, on the come-out roll, a Come bet would be the same as a Pass line bet. Like the Pass line bet, each player may only make one Come bet per roll; this does not exclude a player from betting odds on an already established come-bet point. The Come bet must be at least the table minimum and at most the table maximum. Players may bet both the Come and Don't Come on the same roll if desired.\nAlso like a Pass line bet, the come bet is a contract bet and is always working, and cannot be turned \"Off\", removed or reduced until it wins or loses. However, the odds taken behind a Come bet can be turned \"Off\" (not working), removed or reduced anytime before the bet loses. In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and Pass bet must be table minimum so players can bet the minimum single unit depending on the point. If the point is a 4 or 10, players can bet as little as $1 if the table minimum is low such as $5, $10, or $15 minimums. If the player requests the Come odds to be not working (\"Off\") and the shooter sevens-out or hits the Come bet point, the Come bet will be lost or doubled and the Come odds returned. If the casino allows put betting a player may increase a Come bet after a point has been established and bet larger odds behind if desired. Put betting also allows a player to bet on a Come and take odds immediately on a point number without a Come bet point being established.\nThe dealer will place the odds on top of the come bet, but slightly off center in order to differentiate between the original bet and the odds. The second round wins if the shooter rolls the come bet point again before a seven. Winning come bets are paid the same as winning Pass line bets: even money for the original bet and true odds for the odds bet. If, instead, the seven is rolled before the come-bet point, the come bet (and any odds bet) loses.\nBecause of the come bet, if the shooter makes their point, a player can find themselves in the situation where they still have a come bet (possibly with odds on it) and the next roll is a come-out roll. In this situation, odds bets on the come wagers are usually presumed to be not working for the come-out roll. That means that if the shooter rolls a 7 on the come-out roll, any players with active come bets waiting for a come-bet point lose their initial wager but will have their odds bets returned to them.\nIf the come-bet point is rolled on the come-out roll, the odds do not win but the come bet does and the odds bet is returned (along with the come bet and its payoff). The player can tell the dealer that they want their odds working, such that if the shooter rolls a number that matches the come point, the odds bet will win along with the come bet, and if a seven is rolled, both lose.\nMany players will use a come bet as \"insurance\" against sevening out: if the shooter rolls a seven, the come bet pays 1:1, offsetting the loss of the Pass line bet. The risk in this strategy is the situation where the shooter does not hit a seven for several rolls, leading to multiple come bets that will be lost if the shooter eventually sevens out.\nDon't Come bet.\nIn the same way that a Come bet is similar to a Pass line bet, a Don't Come bet is similar to a Don't Pass bet. Like the Come, the Don't Come can only be bet after a point has already been established as it is the same as a Don't Pass line bet when no point is established. This bet must be at least the table minimum and at most the table maximum. A Don't Come bet is played in two phases, just like the Don't Pass line bet.\nLike the Don't Pass each player may only make one Don't Come bet per roll, this does not exclude a player from laying odds on an already established Don't Come points. Players may bet both the Don't Come and Come on the same roll if desired.\nThe player may lay odds on a Don't Come bet, just like a Don't Pass bet; in this case, the dealer (not the player) places the odds bet on top of the bet in the box, because of limited space, slightly offset to signify that it is an odds bet and not part of the original Don't Come bet. Lay odds behind a Don't Come are subject to the same rules as Don't Pass lay odds. Unlike a standard lay bet on a point, lay odds behind a Don't Come point does not charge commission (vig) and gives the player true odds. Like the Don't Pass line bet, Don't Come bets are no-contract, and can be removed or reduced after a Don't Come point has been established, but cannot be turned off (\"not working\") without being removed. A player may also call, \"No Action\" when a point is established, and the bet will not be moved to its point. This play is not to the player's advantage. If the bet is removed, the player can no longer lay odds behind the Don't Come point and cannot restore or increase the same Don't Come bet. Players must wait until next roll as long as a Pass line point has been established (players cannot bet Don't Come on come out rolls) before they can make a new Don't Come bet. Las Vegas casinos which allow put betting allows players to move the Don't Come directly to any Come point as a put; however, this is not allowed in Atlantic City or Pennsylvania. Unlike the Don't Come bet itself, the Don't Come odds can be turned \"Off\" (not working), removed, or reduced if desired. In Las Vegas, players generally must lay at least table minimum on odds if desired and win less than table minimum; in Atlantic City and Pennsylvania a player's combined bet must be at least table minimum, so depending on the point number players may lay as little as 2 minimum units (e.g. if the point is 4 or 10). If the player requests the Don't Come odds be not working (\"Off\") and the shooter hits the Don't Come point or sevens-out, the Don't Come bet will be lost or doubled and the Don't Come odds returned.\nWinning Don't Come bets are paid the same as winning Don't Pass bets: even money for the original bet and true odds for the odds lay. Unlike come bets, the odds laid behind points established by Don't Come bets are always working including come out rolls unless the player specifies otherwise.\nMulti-roll bets.\nThese are bets that may not be settled on the first roll and may need one or more subsequent rolls before an outcome is determined.\nMost multi-roll bets may fall into the situation where a point is made by the shooter before the outcome of the multi-roll bet is decided. These bets are often considered \"not working\" on the new come-out roll until the next point is established, unless the player calls the bet as \"working.\"\nCasino rules vary on this; some of these bets may not be callable, while others may be considered \"working\" during the come-out. Dealers will usually announce if bets are working unless otherwise called off. If a non-working point number placed, bought or laid becomes the new point as the result of a come-out, the bet is usually refunded, or can be moved to another number for free.\nPlace.\nPlayers can bet any point number (4, 5, 6, 8, 9, 10) by placing their wager in the come area and telling the dealer how much and on what number(s), \"30 on the 6\", \"5 on the 5\", or \"25 on the 10\". These are typically \"Place Bets to Win\". These are bets that the number bet on will be rolled before a 7 is rolled, similar to the Pass odds bets. These bets are considered working bets, and will continue to be paid out each time a shooter rolls the number bet. On a come-out roll, a place bet is considered to be not in effect unless the player who made it specifies otherwise. This bet may be removed or reduced at any time until it loses; in the latter case, the player must abide by any table minimums.\nPlace bets to win pay out at slightly worse than the true odds: 9-to-5 on points 4 or 10, 7-to-5 on points 5 or 9, and 7-to-6 on points 6 or 8. The place bets on the outside numbers (4,5,9,10) should be made in units of $5, (on a $5 minimum table), in order to receive the correct exact payout of $5 paying $7 or $5 paying $9. The place bets on the 6 &amp; 8 should be made in units of $6, (on a $5 minimum table), in order to receive the correct exact payout of $6 paying $7. For the 4 and 10, it is to the player's advantage to 'buy' the bet (see below).\nAn alternative form, rarely offered by casinos, is the \"place bet to lose.\" This bet is the opposite of the place bet to win and pays off if a 7 is rolled before the specific point number. The place bet to lose typically carries a lower house edge than a place bet to win. Payouts are 4-to-5 on points 6 or 8, 5-to-8 on 5 or 9, and 5-to-11 on 4 or 10.\nBuy.\nPlayers can also buy a bet which are paid at true odds, but a 5% commission is charged on the amount of the bet. Buy bets are placed with the shooter betting at a specific number will come out before a player sevens out. The buy bet must be at least table minimum excluding commission; however, some casinos require the minimum buy bet amount to be at least $20 to match the $1 charged on the 5% commission. Traditionally, the buy bet commission is paid no matter what, but in recent years a number of casinos have changed their policy to charge the commission only when the buy bet wins. Some casinos charge the commission as a one-time fee to buy the number; payouts are then always at true odds. Most casinos usually charge only $1 for a $25 green-chip bet (4% commission), or $2 for $50 (two green chips), reducing the house advantage a bit more. Players may remove or reduce this bet (bet must be at least table minimum excluding vig) anytime before it loses. Buy bets like place bets are not working when no point has been established unless the player specifies otherwise.\nWhere commission is charged only on wins, the commission is often deducted from the winning payoff\u2014a winning $25 buy bet on the 10 would pay $49, for instance. The house edges stated in the table assume the commission is charged on all bets. They are reduced by at least a factor of two if commission is charged on winning bets only.\nLay.\nA lay bet is the opposite of a buy bet, where a player bets on a 7 to roll before the number that is laid. Players may only lay the 4, 5, 6, 8, 9, or 10 and may lay multiple numbers if desired. Just like the buy bet lay bets pay true odds, but because the lay bet is the opposite of the buy bet, the payout is reversed. Therefore, players get 1 to 2 for the numbers 4 and 10, 2 to 3 for the numbers 5 and 9, and 5 to 6 for the numbers 6 and 8. A 5% commission (vigorish, vig, juice) is charged up front on the possible winning amount. For example: A $40 Lay Bet on the 4 would pay $20 on a win. The 5% vig would be $1 based on the $20 win. (not $2 based on the $40 bet as the way buy bet commissions are figured.) Like the buy bet the commission is adjusted to suit the betting unit such that fraction of a dollar payouts are not needed. Casinos may charge the vig up front thereby requiring the player to pay a vig win or lose, other casinos may only take the vig if the bet wins. Taking vig only on wins lowers house edge. Players may removed or reduce this bet (bet must be at least table minimum) anytime before it loses. Some casinos in Las Vegas allow players to lay table minimum plus vig if desired and win less than table minimum. Lay bet maximums are equal to the table maximum win, so if a player wishes to lay the 4 or 10, they may bet twice at amount of the table maximum for the win to be table maximum. Other casinos require the minimum bet to win at $20 even at the lowest minimum tables in order to match the $1 vig, this requires a $40 bet. Similar to buy betting, some casinos only take commission on win reducing house edge. Unlike place and buy bets, lay bets are always working even when no point has been established. The player must specify otherwise if they wish to have the bet not working.\nIf a player is unsure of whether a bet is a single or multi-roll bet, it can be noted that all single-roll bets will be displayed on the playing surface in one color (usually red), while all multi-roll bets will be displayed in a different color (usually yellow).\nPut.\nA put bet is a bet which allows players to increase or make a Pass line bet after a point has been established (after come-out roll). Players may make a put bet on the Pass line and take odds immediately or increase odds behind if a player decides to add money to an already existing Pass line bet. Put betting also allows players to increase an existing come bet for additional odds after a come point has been established or make a new come bet and take odds immediately behind if desired without a come bet point being established. If increased or added put bets on the Pass line and Come cannot be turned \"Off\", removed or reduced, but odds bet behind can be turned \"Off\", removed or reduced. The odds bet is generally required to be the table minimum. Player cannot put bet the Don't Pass or Don't Come. Put betting may give a larger house edge over place betting unless the casino offers high odds.\nPut bets are generally allowed in Las Vegas, but not allowed in Atlantic City and Pennsylvania.\nPut bets are better than place bets (to win) when betting more than 5-times odds over the flat bet portion of the put bet. For example, a player wants a $30 bet on the six. Looking at two possible bets: 1) Place the six, or 2) Put the six with odds. A $30 place bet on the six pays $35 if it wins. A $30 put bet would be a $5 flat line bet plus $25 (5-times) in odds, and also would pay $35 if it wins. Now, with a $60 bet on the six, the place bet wins $70, where the put bet ($5 + $55 in odds) would pay $71. The player needs to be at a table which not only allows put bets, but also high-times odds, to take this advantage.\nHard way.\nThis bet can only be placed on the numbers 4, 6, 8, and 10. In order for this bet to win, the chosen number must be rolled the \"hard way\" (as doubles) before a 7 or any other non-double combination (\"easy way\") totaling that number is rolled. For example, a player who bets a hard 6 can only win by seeing a three-three roll come up before any 7 or any easy roll totaling 6 (four-two or five-one); otherwise, the player loses.\nIn Las Vegas casinos, this bet is generally working, including when no point has been established, unless the player specifies otherwise. In other casinos such as those in Atlantic City, hard ways are not working when the point is off unless the player requests to have it working on the come out roll.\nLike single-roll bets, hard way bets can be lower than the table minimum; however, the maximum bet allowed is also lower than the table maximum. The minimum hard way bet can be a minimum one unit. For example, lower stake table minimums of $5 or $10, generally allow minimum hard ways bets of $1. The maximum bet is based on the maximum allowed win from a single roll.\nEasy way is not a specific bet offered in standard casinos, but a term used to define any number combination which has two ways to roll. For example, (six-four, or four-six) would be a \"10 easy\". The 4, 6, 8 or 10 can be made both hard and easy ways. Betting point numbers (which pays off on easy or hard rolls of that number) or single-roll (\"hop\") bets (e.g., \"hop the 2\u20134\" is a bet for the next roll to be an easy six rolled as a two and four) are methods of betting easy ways.\nBig 6 and Big 8.\nA player can wager on either the 6 or 8 being rolled before the shooter throws a seven. These wagers are usually avoided by experienced craps players since they create a large house edge by paying even money (1:1) while the true odds are 6:5; experienced players realize the house edge would be reduced by instead making place bets on the 6 or the 8, since those pay more (7:6) and are closer to the true odds. Some casinos (especially all those in Atlantic City) do not even offer the Big 6 &amp; 8. The bets are located in the corners behind the Pass line, and bets may be placed directly by players.\nThe only real advantage offered by the Big 6 &amp; 8 is that they can be bet for the table minimum, whereas a place bet minimum may sometimes be greater than the table minimum (e.g. $6 place bet on a $3 minimum game.) In addition place bets are usually not working, except by agreement, when the shooter is \"coming out\" i.e. shooting for a point, and Big 6 and 8 bets always work. Some modern layouts no longer show the Big 6 / Big 8 bet.\nSingle-roll bets.\nSingle-roll (proposition) bets are resolved in one dice roll by the shooter. Most of these are called \"service bets\", and they are located at the center of most craps tables. Only the stickman or a dealer can place a service bet. Single-roll bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum bet is based on the maximum allowed win from a single roll. The lowest single-roll bet can be a minimum one unit bet. For example, tables with minimums of $5 or $10 generally allow minimum single-roll bets of $1. Single bets are always working by default unless the player specifies otherwise. The bets include:\nPlayer bets.\nFire Bet: Before the shooter begins, some casinos will allow a bet known as a fire bet to be placed. A fire bet is a bet of as little as $1 and generally up to a maximum of $5 to $10 sometimes higher, depending on casino, made in the hope that the next shooter will have a hot streak of setting and getting many points of different values. As different individual points are made by the shooter, they will be marked on the craps layout with a fire symbol.\nThe first three points will not pay out on the fire bet, but the fourth, fifth, and sixth will pay out at increasing odds. The fourth point pays at 24-to-1, the fifth point pays at 249-to-1, and the 6th point pays at 999-to-1. (The points must all be different numbers for them to count toward the fire bet.) For example, a shooter who successfully hits a point of 10 twice will only garner credit for the first one on the fire bet. Players must hit the established point in order for it to count toward the fire bet. The payout is determine by the number of points which have been established and hit after the shooter sevens out.\nBonus Craps: Prior to the initial \"come out roll\", players may place an optional wager (usually a $1 minimum to a maximum $25) on one or more of the three Bonus Craps wagers, \"All Small\", \"All Tall\", or \"All or Nothing at All.\" For players to win the \"All Small\" wager, the shooter must hit all five small numbers (2, 3, 4, 5, 6) before a seven is rolled; similarly, \"All Tall\" wins if all five high numbers (8, 9, 10, 11, 12) are hit before a seven is rolled.\nThese bets pay 35-for-1, for a house advantage of 7.76%. \"All or Nothing at All\" wins if the shooter hits all 10 numbers before a seven is rolled. This pays 176-for-1, for a house edge of 7.46%. For all three wagers, the order in which the numbers are hit does not matter. Whenever a seven is hit, including on the come out roll, all bonus bets lose, the bonus board is reset, and new bonus bets may be placed.\nMultiple different bets.\nA player may wish to make multiple different bets. For example, a player may be wish to bet $1 on all hard ways and the horn. If one of the bets win the dealer may automatically replenish the losing bet with profits from the winning bet. In this example, if the shooter rolls a hard 8 (pays 9:1), the horn loses. The dealer may return $5 to the player and place the other $4 on the horn bet which lost. If the player does not want the bet replenished, they should request any or all bets be taken down.\nWorking and not working bets.\nA working bet is a live bet. Bets may also be on the board, but not in play and therefore not working. Pass line and come bets are always working meaning the chips are in play and the player is therefore wagering live money. Other bets may be working or not working depending whether a point has been established or player's choice. Place and buy bets are working by default when a point is established and not working when the point is off unless the player specifies otherwise. Lay bets are always working even if a point has not been established unless the player requests otherwise. At any time, a player may wish to take any bet or bets out of play. The dealer will put an \"Off\" button on the player's specific bet or bets; this allows the player to keep his chips on the board without a live wager. For example, if a player decides not to wager a place bet mid-roll but wishes to keep the chips on the number, they may request the bet be \"not working\" or \"Off\". The chips remain on the table, but the player cannot win from or lose chips which are not working.\nThe opposite is also allowed. By default place and buy bets are not working without an established point; a player may wish to wager chips before a point has been established. In this case, the player would request the bet be working in which the dealer will place an \"On\" button on the specified chips.\nBetting variants.\nThese variants depend on the casino and the table, and sometimes a casino will have different tables that use or omit these variants and others.\nOptimal betting.\nWhen craps is played in a casino, all bets have a house advantage. That is, it can be shown mathematically that a player will (with 100% probability) lose all their money to the casino in the long run, while in the short run the player is more likely to lose money than make money. There may be players who are lucky and get ahead for a period of time, but in the long run these winning streaks are eroded away. One can slow, but not eliminate, one's average losses by only placing bets with the smallest house advantage.\nThe Pass / Don't Pass line, Come / Don't Come line, place 6, place 8, buy 4 and buy 10 (only under the casino rules where commission is charged only on wins) have the lowest house edge in the casino, and all other bets will, on average, lose money between three and twelve times faster because of the difference in house edges.\nThe place bets and buy bets differ from the Pass line and come line, in that place bets and buy bets can be removed at any time, since, while they are multi-roll bets, their odds of winning do not change from roll to roll, whereas Pass line bets and come line bets are a combination of different odds on their first roll and subsequent rolls. The first roll of a Pass line bet is 2:1 advantage for the player (8 wins, 4 losses), but it is \"paid for\" by subsequent rolls that are at the same disadvantage to the player as the Don't Pass bets were at an advantage. As such, they cannot profitably let the player take down the bet after the first roll. Players can bet or lay odds behind an established point depending on whether it was a Pass / Come or Don't Pass / Don't Come to lower house edge by receiving true odds on the point. Casinos which allow put betting allows players to increase or make new pass / come bets after the come-out roll. This bet generally has a higher house edge than place betting, unless the casino offers high odds.\nConversely, a player can take back (pick up) a Don't Pass or Don't Come bet after the first roll, but this cannot be recommended, because they already endured the disadvantaged part of the combination \u2013 the first roll. On that come-out roll, they win just 3 times (2 and 3), while losing 8 of them (7 and 11) and pushing one (12) out of the 36 possible rolls. On the other 24 rolls that become a point, their Don't Pass bet is now to their advantage by 6:3 (4 and 10), 6:4 (5 and 9) and 6:5 (6 and 8). If a player chooses to remove the initial Don't Come and / or Don't Pass line bet, they can no longer lay odds behind the bet and cannot re-bet the same Don't Pass and / or Don't Come number (players must make a new Don't Pass or come bets if desired). However, players can still make standard lay bets odds on any of the point numbers (4,5,6,8,9,10).\nAmong these, and the remaining numbers and possible bets, there are a myriad of systems and progressions that can be used with many combinations of numbers.\nAn important alternative metric is house advantage per roll (rather than per bet), which may be expressed in loss per hour. The typical pace of rolls varies depending on the number of players, but 102 rolls per hour is a cited rate for a nearly full table. This same reference states that only \"29.6% of total rolls are come out rolls, on average\", so for this alternative metric, needing extra rolls to resolve the Pass line bet, for example, is factored. This number then permits calculation of rate of loss per hour, and per the 4 day / 5 hour per day gambling trip:\nTable rules.\nBesides the rules of the game itself, a number of formal and informal rules are commonly applied in the table form of Craps, especially when played in a casino.\nTo reduce the potential opportunity for switching dice by sleight-of-hand, players are not supposed to handle the dice with more than one hand (such as shaking them in cupped hands before rolling) nor take the dice past the edge of the table. If a player wishes to change shooting hands, they may set the dice on the table, let go, then take them with the other hand.\nWhen throwing the dice, the player is expected to hit the farthest wall at the opposite end of the table (these walls are typically augmented with pyramidal structures to ensure highly unpredictable bouncing after impact). Casinos will sometimes allow a roll that does not hit the opposite wall as long as the dice are thrown past the middle of the table; a very short roll will be nullified as a \"no roll\". The dice may not be slid across the table and must be tossed. These rules are intended to prevent dexterous players from physically influencing the outcome of the roll.\nPlayers are generally asked not to throw the dice above a certain height (such as the eye level of the dealers). This is both for the safety of those around the table, and to eliminate the potential use of such a throw as a distraction device in order to cheat.\nDice are still considered \"in play\" if they land on players' bets on the table, the dealer's working stacks, on the marker puck, or with one die resting on top of the other. The roll is invalid if either or both dice land in the boxman's bank, the stickman's bowl (where the extra three dice are kept between rolls), or in the rails around the top of the table where players chips are kept. If one or both dice hits a player or dealer and rolls back onto the table, the roll counts as long as the person being hit did not intentionally interfere with either of the dice, though some casinos will rule \"no roll\" for this situation. If one or both leave the table, it is also a \"no roll\", and the dice may either be replaced or examined by the boxman and returned to play.\nShooters may wish to \"set\" the dice to a particular starting configuration before throwing (such as showing a particular number or combination, stacking the dice, or spacing them to be picked up between different fingers), but if they do, they are often asked to be quick about it so as not to delay the game. Some casinos disallow such rituals to speed up the pace of the game. Some may also discourage or disallow unsanitary practices such as kissing or spitting on the dice.\nIn most casinos, players are not allowed to hand anything directly to dealers, and vice versa. Items such as cash, checks, and chips are exchanged by laying them down on the table; for example, when \"buying in\" (paying cash for chips), players are expected to place the cash on the layout: the dealer will take it and then place the chips in front of the player. This rule is enforced in order to allow the casino to easily monitor and record all transfers via overhead surveillance cameras, and to reduce the opportunity for cheating via sleight-of-hand.\nMost casinos prohibit \"call bets\", and may have a warning such as \"No Call Bets\" printed on the layout to make this clear. This means a player may not call out a bet without also placing the corresponding chips on the table. Such a rule reduces the potential for misunderstanding in loud environments, as well as disputes over the amount that the player intended to bet after the outcome has been decided. Some casinos choose to allow call bets once players have bought-in. When allowed, they are usually made when a player wishes to bet at the last second, immediately before the dice are thrown, to avoid the risk of obstructing the roll.\nEtiquette.\nCraps is among the most social and most superstitious of all gambling games, which leads to an enormous variety of informal rules of etiquette that players may be expected to follow. An exhaustive list of these is beyond the scope of this article, but the guidelines below are most commonly given.\nTips.\nTipping the dealers is universal and expected in Craps. As in most other casino games, a player may simply place (or toss) chips onto the table and say, \"For the dealers\", \"For the crew\", \"etc.\" In craps, it is also common to place a bet for the dealers. This is usually done one of three ways: by placing an ordinary bet and simply declaring it for the dealers, as a \"two-way\", or \"on top\". A \"Two-Way\" is a bet for both parties: for example, a player may toss in two chips and say \"Two Way Hard Eight\", which will be understood to mean one chip for the player and one chip for the dealers. Players may also place a stack of chips for a bet as usual, but leave the top chip off-center and announce \"on top for the dealers\". The dealer's portion is often called a \"toke\" bet, which comes from the practice of using $1 slot machine tokens to place dealer bets in some casinos.\nIn some cases, players may also tip each other, for example as a show of gratitude to the thrower for a roll on which they win a substantial bet.\nSuperstition.\nCraps players routinely practice a wide range of superstitious behaviors, and may expect or demand these from other players as well.\nMost prominently, it is universally considered bad luck to say the word \"seven\" (after the \"come-out\", a roll of 7 is a loss for \"pass\" bets). Dealers themselves often make significant efforts to avoid calling out the number. When necessary, participants may refer to seven with a \"nickname\" such as \"Big Red\" (or just \"Red\"), \"the S-word\", etc.\nDice setting or dice control.\nAn approach to achieving an advantage is to \"set\" the dice in a particular orientation, and then throw them in such a manner that they do not tumble randomly. The theory is that given exactly the same throw from exactly the same starting configuration, the dice will tumble in the same way and therefore show the same or similar values every time.\nCasinos take steps to prevent this. The dice are usually required to hit the back wall of the table, which is normally faced with a jagged angular texture such as pyramids, making controlled spins more difficult. There has been no independent evidence that such methods can be successfully applied in a real casino.\nVariants.\nBank craps is a variation of the original craps game and is sometimes known as Las Vegas Craps. This variant is quite popular in Nevada gambling houses, and its availability online has now made it a globally played game. Bank craps uses a special table layout and all bets must be made against the house. In Bank Craps, the dice are thrown over a wire or a string that is normally stretched a few inches from the table's surface. The lowest house edge (for the Pass / Don't Pass) in this variation is around 1.4%. Generally, if the word \"craps\" is used without any modifier, it can be inferred to mean this version of the game, to which most of this article refers.\nCrapless craps, also known as bastard craps, is a simple version of the original craps game, and is normally played as an online private game. The biggest difference between crapless craps and original craps is that the shooter (person throwing the dice) is at a far greater disadvantage and has a house edge of 5.38%. Another difference is that this is one of the craps games in which a player can bet on rolling a 2, 3, 11 or 12 before a 7 is thrown. In crapless craps, 2 and 12 have odds of 11:2 and have a house edge of 7.143% while 3 and 11 have odds of 11:4 with a house edge of 6.25%.\nNew York Craps is one of the variations of craps played mostly in the Eastern coast of the US, true to its name. History states that this game was actually found and played in casinos in Yugoslavia, the UK and the Bahamas. In this craps variant, the house edge is greater than Las Vegas Craps or Bank craps. The table layout is also different, and is called a double-end-dealer table. This variation is different from the original craps game in several ways, but the primary difference is that New York craps does not allow Come or Don't Come bets. New York Craps Players bet on box numbers like 4, 5, 6, 8, 9, or 10. The overall house edge in New York craps is 5%.\nCard-based variations.\nIn order to get around Californian laws barring the payout of a game being directly related to the roll of dice, Indian reservations have adapted the game to substitute cards for dice.\nCards replacing dice.\nTo replicate the original dice odds exactly without dice or possibility of card-counting, one scheme uses two shuffle machines each with just one deck of Ace through 6 each. Each machine selects one of the 6 cards at random and this is the roll. The selected cards are replaced and the decks are reshuffled for the next roll.\nIn one variation, two shoes are used, each containing some number of regular card decks that have been stripped down to just the Aces and deuces through sixes. The boxman simply deals one card from each shoe and that is the roll on which bets are settled. Since a card-counting scheme is easily devised to make use of the information of cards that have already been dealt, a relatively small portion (less than 50%) of each shoe is usually dealt in order to protect the house.\nIn a similar variation, cards representing dice are dealt directly from a continuous shuffling machine (CSM). Typically, the CSM will hold approximately 264 cards, or 44 sets of 1 through 6 spot cards. Two cards are dealt from the CSM for each roll. The game is played exactly as regular craps, but the roll distribution of the remaining cards in the CSM is slightly skewed from the normal symmetric distribution of dice.\nEven if the dealer were to shuffle each roll back into the CSM, the effect of buffering a number of cards in the chute of the CSM provides information about the skew of the next roll. Analysis shows this type of game is biased towards the Don't Pass and Don't Come bets. A player betting Don't Pass and Don't Come every roll and laying 10x odds receives a 2% profit on the initial Don't Pass / Don't Come bet each roll. Using a counting system allows the player to attain a similar return at lower variance.\nCards mapping physical dice.\nIn this game variation, one red deck and one blue deck of six cards each (A through 6), and a red die and a blue die are used. Each deck is shuffled separately, usually by machine. Each card is then dealt onto the layout, into the 6 red and 6 blue numbered boxes. The shooter then shoots the dice. The red card in the red-numbered box corresponding to the red die, and the blue card in the blue-numbered box corresponding to the blue die are then turned over to form the roll on which bets are settled.\nAnother variation uses a red and a blue deck of 36 custom playing cards each. Each card has a picture of a two-die roll on it \u2013 from one-one to six-six. The shooter shoots what looks like a red and a blue die, called \"cubes\". They are numbered such that they can never throw a pair, and that the blue one will show a higher value than the red one exactly half the time. One such scheme could be two-two-two-five-five-five on the red die and three-three-three-four-four-four on the blue die.\nOne card is dealt from the red deck and one is dealt from the blue deck. The shooter throws the \"cubes\" and the color of the cube that is higher selects the color of the card to be used to settle bets. On one such table, an additional one-roll prop bet was offered: If the card that was turned over for the \"roll\" was either one-one or six-six, the other card was also turned over. If the other card was the \"opposite\" (six-six or one-one, respectively) of the first card, the bet paid 500:1 for this 647:1 proposition.\nAnd additional variation uses a single set of 6 cards, and regular dice. The roll of the dice maps to the card in that position, and if a pair is rolled, then the mapped card is used twice, as a pair.\nRules of play against other players (\"Street Craps\").\nRecreational or informal playing of craps outside of a casino is referred to as street craps or private craps. The most notable difference between playing street craps and bank craps is that there is no bank or house to cover bets in street craps. Players must bet against each other by covering or fading each other's bets for the game to be played. If money is used instead of chips and depending on the laws of where it is being played, street craps can be an illegal form of gambling.\nThere are many variations of street craps. The simplest way is to either agree on or roll a number as the point, then roll the point again before rolling a seven. Unlike more complex proposition bets offered by casinos, street craps has more simplified betting options. The shooter is required to make either a Pass or a Don't Pass bet if he wants to roll the dice. Another player must choose to cover the shooter to create a stake for the game to continue.\nIf there are several players, the rotation of the player who must cover the shooter may change with the shooter (comparable to a blind in poker). The person covering the shooter will always bet against the shooter. For example, if the shooter made a \"Pass\" bet, the person covering the shooter would make a \"Don't Pass\" bet to win. Once the shooter is covered, other players may make Pass / Don't Pass bets, or any other proposition bets, as long as there is another player willing to cover.\nIn popular culture.\nDue to the random nature of the game, in popular culture a \"crapshoot\" is often used to describe an action with an unpredictable outcome.\nThe prayer or invocation \"Baby needs a new pair of shoes!\" is associated with shooting craps.\nFloating craps.\nFloating craps is an illegal operation of craps. The term \"floating\" refers to the practice of the game's operators using portable tables and equipment to quickly move the game from location to location to stay ahead of the law enforcement authorities. The term may have originated in the 1930s when Benny Binion (later known for founding the downtown Las Vegas hotel Binion's) set up an illegal craps game utilizing tables created from portable crates for the Texas Centennial Exposition.\nThe 1950 Broadway musical \"Guys and Dolls\" features a major plot point revolving around a floating craps game.\nIn the 1950s and 1960s The Sands Hotel in Las Vegas had a craps table that floated in the swimming pool, as a joke reference to the notoriety of the term.\nRecords.\nA Golden Arm is a craps player who rolls the dice for longer than one hour without losing. Likely the first known Golden Arm was Oahu native Stanley Fujitake, who rolled 118 times without sevening out in 3 hours and 6 minutes at the California Hotel and Casino on May 28, 1989.\nThe current record for length of a \"hand\" (successive rounds won by the same shooter) is 154 rolls including 25 passes by Patricia DeMauro of New Jersey, lasting 4 hours and 18 minutes, at the Borgata in Atlantic City, New Jersey, on May 23\u201324, 2009. She bested by over an hour the record held for almost 20 years\u00a0\u2013 that of Fujitake.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6063", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6063", "title": "Cartoonists", "text": ""}
{"id": "6065", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=6065", "title": "Cosine", "text": ""}
{"id": "6066", "revid": "1994682", "url": "https://en.wikipedia.org/wiki?curid=6066", "title": "Carl von Clausewitz", "text": "Prussian general and military theorist (1780\u20131831)\nCarl Philipp Gottlieb von Clausewitz ( ; ; born Carl Philipp Gottlieb Clauswitz; 1 July 1780 \u2013 16 November 1831) was a Prussian general and military theorist who stressed the \"moral\" (in modern terms meaning psychological) and political aspects of waging war. His most notable work, (\"About War\"), though unfinished at his death, is considered a seminal treatise on military strategy and science.\nClausewitz stressed the multiple interactions of diverse factors in war, noting how unexpected developments unfolding under the \"fog of war\" (i.e., in the face of incomplete, dubious, and often erroneous information and great fear, doubt, and excitement) call for rapid decisions by alert commanders. He saw history as a vital check on erudite abstractions that did not accord with experience. In contrast to the early work of Antoine-Henri Jomini, he argued that war could not be quantified or reduced to mapwork, geometry, and graphs. Clausewitz had many aphorisms, of which one of the most famous is, \"War is the continuation of policy with other means.\"87\nName.\nClausewitz's Christian names are variously given in English-language sources as \"Karl\", \"Carl Philipp Gottlieb\", or \"Carl Maria.\" He spelled his own given name with a \"C\" in order to identify with the classical Western tradition; writers who use \"Karl\" are often seeking to emphasize his German (rather than European) identity. \"Carl Philipp Gottfried\" appears on Clausewitz's tombstone. \"Encyclop\u00e6dia Britannica\" continues to use Gottlieb instead of Gottfried based on older sources, such as military historian Peter Paret, and historian Sir Michael Howard originated the use of \"Carl Maria.\" However, more modern scholars like Christopher Bassford (editor of \"ClausewitzStudies.org\") and Vanya Eftimova Bellinger (who wrote the 2016 of Carl's wife Marie von Clausewitz) consider his tombstone a more reliable source than the hand-written birth records used by Paret.\nLife and military career.\nClausewitz was born on 1 July 1780 in Burg bei Magdeburg in the Prussian Duchy of Magdeburg as the fourth and youngest son of a family that made claims to a noble status which Carl accepted. Clausewitz's family claimed descent from the Barons of Clausewitz in Upper Silesia, though scholars question the connection. \nHis grandfather, the son of a Lutheran pastor, had been a professor of theology. Clausewitz's father, once a lieutenant in the army of Frederick the Great, King of Prussia, held a minor post in the Prussian internal-revenue service. Clausewitz entered the Prussian military service at the age of twelve as a lance corporal, eventually attaining the rank of major general.\nClausewitz served in the Rhine campaigns (1793\u20131794) including the siege of Mainz, when the Prussian Army invaded France during the French Revolution, and fought in the Napoleonic Wars from 1806 to 1815. He entered the \"Kriegsakademie\" (also cited as \"The German War School\", the \"Military Academy in Berlin\", and the \"Prussian Military Academy,\" later the \"War College\") in Berlin in 1801 (aged 21), probably studied the writings of the philosophers Immanuel Kant and/or Johann Gottlieb Fichte and Friedrich Schleiermacher and won the regard of General Gerhard von Scharnhorst, the future first chief-of-staff of the newly reformed Prussian Army (appointed 1809). Clausewitz, Hermann von Boyen (1771\u20131848) and Karl von Grolman (1777\u20131843) were among Scharnhorst's primary allies in his efforts to reform the Prussian army between 1807 and 1814.\nClausewitz served during the Jena Campaign as aide-de-camp to Prince August. At the Battle of Jena-Auerstedt on 14 October 1806\u2014when Napoleon invaded Prussia and defeated the Prussian-Saxon army commanded by Karl Wilhelm Ferdinand, Duke of Brunswick\u2014he was captured, one of the 25,000 prisoners taken that day as the Prussian army disintegrated. He was 26. Clausewitz was held prisoner with his prince in France from 1807 to 1808. Returning to Prussia, he assisted in the reform of the Prussian army and state. Johann Gottlieb Fichte wrote \"On Machiavelli, as an Author, and Passages from His Writings\" in June 1807. (\"\u00dcber Machiavell, als Schriftsteller, und Stellen aus seinen Schriften\" ). Carl Clausewitz wrote an interesting and anonymous Letter to Fichte (1809) about his book on \"Machiavelli.\" The letter was published in Fichte's \"Verstreute kleine Schriften\" 157\u2013166. For an English translation of the letter see \"Carl von Clausewitz Historical and Political Writings\" Edited by: Peter Paret and D. Moran (1992).\nOn 10 December 1810, he married the socially prominent Countess Marie von Br\u00fchl, whom he had first met in 1803. She was a member of the noble German Br\u00fchl family originating in Thuringia. The couple moved in the highest circles, socialising with Berlin's political, literary, and intellectual \u00e9lite. Marie was well-educated and politically well-connected\u2014she played an important role in her husband's career progress and intellectual evolution. She also edited, published, and introduced his collected works.\nOpposed to Prussia's enforced alliance with Napoleon, Clausewitz left the Prussian army and served in the Imperial Russian Army from 1812 to 1813 during the Russian campaign, taking part in the Battle of Borodino (1812). Like many Prussian officers serving in Russia, he joined the Russian\u2013German Legion in 1813. In the service of the Russian Empire, Clausewitz helped negotiate the Convention of Tauroggen (1812), which prepared the way for the coalition of Prussia, Russia, and the United Kingdom that ultimately defeated Napoleon and his allies.\nIn 1815 the Russian-German Legion became integrated into the Prussian Army and Clausewitz re-entered Prussian service as a colonel. He was soon appointed chief-of-staff of Johann von Thielmann's III Corps. In that capacity he served at the Battle of Ligny and the Battle of Wavre during the Waterloo campaign in 1815. An army led personally by Napoleon defeated the Prussians at Ligny (south of Mont-Saint-Jean and the village of Waterloo) on 16 June 1815, but they withdrew in good order. Napoleon's failure to destroy the Prussian forces led to his defeat a few days later at the Battle of Waterloo (18 June 1815), when the Prussian forces arrived on his right flank late in the afternoon to support the Anglo-Dutch forces pressing his front. Napoleon had convinced his troops that the field grey uniforms were those of Marshal Grouchy's grenadiers. Clausewitz's unit fought heavily outnumbered at Wavre (18\u201319 June 1815), preventing large reinforcements from reaching Napoleon at Waterloo. After the war, Clausewitz served as the director of the \"Kriegsakademie\", where he served until 1830. In that year he returned to active duty with the army. Soon afterward, the outbreak of several revolutions around Europe and a crisis in Poland appeared to presage another major European war. Clausewitz was appointed chief of staff of the only army Prussia was able to mobilise in this emergency, which was sent to the Polish border. Its commander, Gneisenau, died of cholera (August 1831), and Clausewitz took command of the Prussian army's efforts to construct a to contain the great cholera outbreak (the first time cholera had appeared in modern heartland Europe, causing a continent-wide panic). Clausewitz himself died of the same disease shortly afterwards, on 16 November 1831.\nHis widow edited, published, and wrote the introduction to his \"magnum opus\" on the philosophy of war in 1832. (He had started working on the text in 1816 but had not completed it.) She wrote the preface for \"On War\" and had published most of his collected works by 1835. She died in January 1836.\nTheory of war.\nClausewitz was a professional combat soldier and a staff officer who was involved in numerous military campaigns, but he is famous primarily as a military theorist interested in the examination of war, utilising the campaigns of Frederick the Great and Napoleon as frames of reference for his work. He wrote a careful, systematic, philosophical examination of war in all its aspects. The result was his principal book, \"https://\" (in English, \"On War\"), a major work on the philosophy of war. It was unfinished when Clausewitz died and contains material written at different stages in his intellectual evolution, producing some significant contradictions between different sections. The sequence and precise character of that evolution is a source of much debate as to the exact meaning behind some seemingly contradictory observations in discussions pertinent to the tactical, operational and strategic levels of war, for example (though many of these apparent contradictions are simply the result of his dialectical method). Clausewitz constantly sought to revise the text, particularly between 1827 and his departure on his last field assignments, to include more material on \"people's war\" and forms of war other than high-intensity warfare between states, but relatively little of this material was included in the book. Soldiers before this time had written treatises on various military subjects, but none had undertaken a great philosophical examination of war on the scale of those written by Clausewitz and Leo Tolstoy, both of whom were inspired by the events of the Napoleonic Era.\nClausewitz's work is still studied today, demonstrating its continued relevance. More than sixteen major English-language books that focused specifically on his work were published between 2005 and 2014, whereas his 19th-century rival Jomini has faded from influence. The historian Lynn Montross said that this outcome \"may be explained by the fact that Jomini produced a system of war, Clausewitz a philosophy. The one has been outdated by new weapons, the other still influences the strategy behind those weapons.\" Jomini did not attempt to define war but Clausewitz did, providing (and dialectically comparing) a number of definitions. The first is his dialectical thesis: \"War is thus an act of force to compel our enemy to do our will.\" The second, often treated as Clausewitz's 'bottom line,' is in fact merely his dialectical antithesis: \"War is merely the continuation of policy [or politics\u2014the German original is \"Politik\", which encompasses both of those rather different English words] with other means.\" The synthesis of his dialectical examination of the nature of war\u2014and thus his actual definition of war\u2014is his famous \"trinity,\" saying that war is, \"when regarded as a whole, in relation to the tendencies predominating in it, a strange trinity, composed of the original violence of its essence, the hate and enmity which are to be regarded as a blind, natural impulse; of the play of probabilities and chance, which make it a free activity of the emotions; and of the subordinate character of a political tool, through which it belongs to the province of pure intelligence.\" Christopher Bassford says the best shorthand for Clausewitz's trinity should be something like \"violent emotion/chance/rational calculation.\" However, it is frequently presented as \"people/army/government,\" a misunderstanding based on a later paragraph in the same section. This misrepresentation was popularised by U.S. Army Colonel Harry Summers' Vietnam-era interpretation, facilitated by weaknesses in the 1976 Howard/Paret translation.\nThe degree to which Clausewitz managed to revise his manuscript to reflect that synthesis is the subject of much debate. His final reference to war and \"Politik\", however, goes beyond his widely quoted antithesis: \"War is simply the continuation of political intercourse with the addition of other means. We deliberately use the phrase 'with the addition of other means' because we also want to make it clear that war in itself does not suspend political intercourse or change it into something entirely different. In essentials that intercourse continues, irrespective of the means it employs. The main lines along which military events progress, and to which they are restricted, are political lines that continue throughout the war into the subsequent peace.\"\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A prince or general who knows exactly how to organise his war according to his object and means, who does neither too little nor too much, gives by that the greatest proof of his genius. But the effects of this talent are exhibited not so much by the invention of new modes of action, which might strike the eye immediately, as in the successful final result of the whole. It is the exact fulfilment of silent suppositions, it is the noiseless harmony of the whole action which we should admire, and which only makes itself known in the total result.\u2014\u200a\nClausewitz introduced systematic philosophical contemplation into Western military thinking, with powerful implications not only for historical and analytical writing but also for practical policy, military instruction, and operational planning. He relied on his own experiences, contemporary writings about Napoleon, and on deep historical research. His historiographical approach is evident in his first extended study, written when he was 25, of the Thirty Years' War. In \"On War\", Clausewitz sees all wars as the sum of decisions, actions, and reactions in an uncertain and dangerous context, and also as a socio-political phenomenon. He also stressed the complex nature of war, which encompasses both the socio-political and the operational and stresses the primacy of state policy. (One should be careful not to limit his observations on war to war between states, however, as he certainly discusses other kinds of protagonists). Clausewitz, according to Azar Gat, expressed in the field of military theory the main themes of the Romantic reaction against the worldview of the Enlightenment, rejecting universal principles and stressing historical diversity and the forces of the human spirit. This explains the strength and value of many of his arguments, derived from this great cultural movement, but also his often harsh rhetoric against his predecessors.\nThe word \"strategy\" had only recently come into usage in modern Europe, and Clausewitz's definition is quite narrow: \"the use of engagements for the object of war\" (which many today would call \"the operational level\" of war). Clausewitz conceived of war as a political, social, and military phenomenon which might\u2014depending on circumstances\u2014involve the entire population of a political entity at war. In any case, Clausewitz saw military force as an instrument that states and other political actors use to pursue the ends of their policy, in a dialectic between opposing wills, each with the aim of imposing his policies and will upon his enemy.\nClausewitz's emphasis on the inherent superiority of the defense suggests that habitual aggressors are likely to end up as failures. The inherent superiority of the defense obviously does not mean that the defender will always win, however: there are other asymmetries to be considered. He was interested in co-operation between the regular army and militia or partisan forces, or citizen soldiers, as one possible\u2014sometimes the only\u2014method of defense. In the circumstances of the Wars of the French Revolution and those with Napoleon, which were energised by a rising spirit of nationalism, he emphasised the need for states to involve their entire populations in the conduct of war. This point is especially important, as these wars demonstrated that such energies could be of decisive importance and for a time led to a democratisation of the armed forces much as universal suffrage democratised politics.\nWhile Clausewitz was intensely aware of the value of intelligence at all levels, he was also very skeptical of the accuracy of much military intelligence: \"Many intelligence reports in war are contradictory; even more are false, and most are uncertain... In short, most intelligence is false.\" This circumstance is generally described as part of the fog of war. Such skeptical comments apply only to intelligence at the tactical and operational levels; at the strategic and political levels he constantly stressed the requirement for the best possible understanding of what today would be called strategic and political intelligence. His conclusions were influenced by his experiences in the Prussian Army, which was often in an intelligence fog due partly to the superior abilities of Napoleon's system but even more simply to the nature of war. Clausewitz acknowledges that friction creates enormous difficulties for the realization of any plan, and the \"fog of war\" hinders commanders from knowing what is happening. It is precisely in the context of this challenge that he develops the concept of military genius (), evidenced above all in the execution of operations. 'Military genius' is not simply a matter of intellect, but a combination of qualities of intellect, experience, personality, and temperament (and there are many possible such combinations) that create a very highly developed mental aptitude for the waging of war.\nPrincipal ideas.\nKey ideas discussed in \"On War\" include:\nInterpretation and misinterpretation.\nClausewitz used a dialectical method to construct his argument, leading to frequent misinterpretation of his ideas. British military theorist B. H. Liddell Hart contends that the enthusiastic acceptance by the Prussian military establishment\u2014especially Moltke the Elder, a former student of Clausewitz\u2014of what they believed to be Clausewitz's ideas, and the subsequent widespread adoption of the Prussian military system worldwide, had a deleterious effect on military theory and practice, due to their egregious misinterpretation of his ideas:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;As so often happens, Clausewitz's disciples carried his teaching to an extreme which their master had not intended... [Clausewitz's] theory of war was expounded in a way too abstract and involved for ordinary soldier-minds, essentially concrete, to follow the course of his argument\u2014which often turned back from the direction in which it was apparently leading. Impressed yet befogged, they grasped at his vivid leading phrases, seeing only their surface meaning, and missing the deeper current of his thought.\nAs described by Christopher Bassford, then-professor of strategy at the National War College of the United States:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;One of the main sources of confusion about Clausewitz's approach lies in his dialectical method of presentation. For example, Clausewitz's famous line that \"War is the continuation of policy with other means,\" (\"\") while accurate as far as it goes, was not intended as a statement of fact. It is the antithesis in a dialectical argument whose thesis is the point\u2014made earlier in the analysis\u2014that \"war is nothing but a duel [or wrestling match, the extended metaphor in which that discussion was embedded] on a larger scale.\" His synthesis, which resolves the deficiencies of these two bold statements, says that war is neither \"nothing but\" an act of brute force nor \"merely\" a rational act of politics or policy. This synthesis lies in his \"fascinating trinity\" []: a dynamic, inherently unstable interaction of the forces of violent emotion, chance, and rational calculation.\nAnother example of this confusion is the idea that Clausewitz was a proponent of total war as used in the Third Reich's propaganda in the 1940s. In fact, Clausewitz never used the term \"total war\": rather, he discussed \"absolute war,\" a concept which evolved into the much more abstract notion of \"ideal war\" discussed at the very beginning of \u2014the purely \"logical\" result of the forces underlying a \"pure,\" Platonic \"ideal\" of war. In what he called a \"logical fantasy,\" war cannot be waged in a limited way: the rules of competition will force participants to use all means at their disposal to achieve victory. But in the \"real world,\" he said, such rigid logic is unrealistic and dangerous. As a practical matter, the military objectives in \"real\" war that support political objectives generally fall into two broad types: limited aims or the effective \"disarming\" of the enemy \"to render [him] politically helpless or militarily impotent. Thus, the complete defeat of the enemy may not be necessary, desirable, or even possible.\nAccording to Azar Gat, the opposing interpretations of Clausewitz are rooted in Clausewitz\u2019s own conceptual journey. The centerpiece of Clausewitz\u2019s theory of war throughout his life was his concept of all-out fighting and energetic conduct leading to the great battle of annihilation. He believed such conduct expressed the very \u201cnature\u201d, or \u201clasting spirit\u201d of war. Accordingly, Clausewitz disparaged the significance of the maneuver, surprise, and cunning in war, as distracting from the centrality of battle, and argued that defense was legitimate only if and as long as one was weaker than the enemy. Nevertheless, in the last years of his life, after the first six out of the eight books of \"On War\" had already been drafted, Clausewitz came to recognize that this concept was not universal and did not even apply to the Napoleonic Wars, the supreme model of his theory of war. This was demonstrated by the Spanish and Russian campaigns and by guerrilla warfare, in all of which battle was systematically avoided. Consequently, from 1827 on, Clausewitz recognized the legitimacy of limited war and explained it by the influence of politics that harnessed the unlimited nature of war to serve its objectives. Clausewitz died in 1831 before he completed the revision he planned along these lines. He incorporated his new ideas only into the end of Book VI, Book VIII and the beginning of Book I of \"On War\". As a result, when published, \"On War\" encompassed both his old and new ideas, at odds with each other.\nThus, against common interpretations of \"On War\", Gat points out that Clausewitz's transformed views regarding the relationship between politics and war and the admission of limited war into his theory constituted a U-turn against his own life-long fundamental view of the nature of war. Gat further argues the readers\u2019 miscomprehension of the theory in \"On War\" as complete and dialectical, rather than a draft undergoing a radical change of mind, has thus generated a range of reactions. People of each age have found in \"On War\" the Clausewitz who suited their own views on war and its conduct. Between 1870 and 1914, he was celebrated mainly for his insistence on the clash of forces and the decisive battle, and his emphasis on moral forces. By contrast, after 1945, during the nuclear age, his reputation has reached a second pinnacle for his later acceptance of the primacy of politics and the concept of limited war.\nReferring to much of the current interpretation of \"On War\" as the Emperor's New Clothes syndrome, Gat argues that instead of critically addressing the puzzling contradictions in \"On War,\" Clausewitz has been set in stone and could not be wrong.\nIn modern times the reconstruction of Clausewitzian theory has been a matter of much dispute. One analysis was that of Panagiotis Kondylis, a Greek writer and philosopher, who opposed the interpretations of Raymond Aron in \"Penser la Guerre, Clausewitz,\" and other liberal writers. According to Aron, Clausewitz was one of the first writers to condemn the militarism of the Prussian general staff and its war-proneness, based on Clausewitz's argument that \"war is a continuation of policy by other means.\" In \"Theory of War,\" Kondylis claims that this is inconsistent with Clausewitzian thought. He claims that Clausewitz was morally indifferent to war (though this probably reflects a lack of familiarity with personal letters from Clausewitz, which demonstrate an acute awareness of war's tragic aspects) and that his advice regarding politics' dominance over the conduct of war has nothing to do with pacifist ideas.\nOther notable writers who have studied Clausewitz's texts and translated them into English are historians Peter Paret of the Institute for Advanced Study and Sir Michael Howard. Howard and Paret edited the most widely used edition of \"On War\" (Princeton University Press, 1976/1984) and have produced comparative studies of Clausewitz and other theorists, such as Tolstoy. Bernard Brodie's \"A Guide to the Reading of \"On War,\"\" in the 1976 Princeton translation, expressed his interpretations of the Prussian's theories and provided students with an influential synopsis of this vital work. The 1873 translation by Colonel James John Graham was heavily\u2014and controversially\u2014edited by the philosopher, musician, and game theorist Anatol Rapoport.\nThe British military historian John Keegan attacked Clausewitz's theory in his book \"A History of Warfare\". Keegan argued that Clausewitz assumed the existence of states, yet 'war antedates the state, diplomacy and strategy by many millennia.'\nInfluence.\nClausewitz died without completing \"Vom Kriege,\" but despite this his ideas have been widely influential in military theory and have had a strong influence on German military thought specifically. Later Prussian and German generals, such as Helmuth Graf von Moltke, were clearly influenced by Clausewitz: Moltke's widely quoted statement that \"No operational plan extends with high certainty beyond the first encounter with the main enemy force\" is a classic reflection of Clausewitz's insistence on the roles of chance, friction, \"fog,\" uncertainty, and interactivity in war.\nClausewitz's influence spread to British thinking as well, though at first more as a historian and analyst than as a theorist. See for example Wellington's extended essay discussing Clausewitz's study of the Campaign of 1815\u2014Wellington's only serious written discussion of the battle, which was widely discussed in 19th-century Britain. Clausewitz's broader thinking came to the fore following Britain's military embarrassments in the Boer War (1899\u20131902). One example of a heavy Clausewitzian influence in that era is Spenser Wilkinson, journalist, the first Chichele Professor of Military History at Oxford University, and perhaps the most prominent military analyst in Britain from c.\u20091885 until well into the interwar period. Another is naval historian Julian Corbett (1854\u20131922), whose work reflected a deep if idiosyncratic adherence to Clausewitz's concepts and frequently an emphasis on Clausewitz's ideas about 'limited objectives' and the inherent strengths of the defensive form of war. Corbett's practical strategic views were often in prominent public conflict with Wilkinson's\u2014see, for example, Wilkinson's article \"Strategy at Sea\", \"The Morning Post\", 12 February 1912. Following the First World War, however, the influential British military commentator B. H. Liddell Hart in the 1920s erroneously attributed to him the doctrine of \"total war\" that during the First World War had been embraced by many European general staffs and emulated by the British. More recent scholars typically see that war as so confused in terms of political rationale that it in fact contradicts much of \"On War.\" That view assumes, however, a set of values as to what constitutes \"rational\" political objectives\u2014in this case, values not shaped by the fervid Social Darwinism that was rife in 1914 Europe. One of the most influential British Clausewitzians today is Colin S. Gray; historian Hew Strachan (like Wilkinson also the Chichele Professor of Military History at Oxford University, since 2001) has been an energetic proponent of the \"study\" of Clausewitz, but his own views on Clausewitz's ideas are somewhat ambivalent.\nWith some interesting exceptions (e.g., John McAuley Palmer, Robert M. Johnston, Hoffman Nickerson), Clausewitz had little influence on American military thought before 1945 other than via British writers, though Generals Eisenhower and Patton were avid readers of English translations. He did influence Karl Marx, Friedrich Engels, Vladimir Lenin, Leon Trotsky, V\u00f5 Nguy\u00ean Gi\u00e1p, Ferdinand Foch, and Mao Zedong, and thus the Communist Soviet and Chinese traditions, as Lenin emphasized the inevitability of wars among capitalist states in the age of imperialism and presented the armed struggle of the working class as the only path toward the eventual elimination of war. Because Lenin was an admirer of Clausewitz and called him \"one of the great military writers,\" his influence on the Red Army was immense. The Russian historian A.N. Mertsalov commented that \"It was an irony of fate that the view in the USSR was that it was Lenin who shaped the attitude towards Clausewitz, and that Lenin's dictum that war is a continuation of politics is taken from the work of this [allegedly] anti-humanist anti-revolutionary.\" The American mathematician Anatol Rapoport wrote in 1968 that Clausewitz as interpreted by Lenin formed the basis of all Soviet military thinking since 1917, and quoted the remarks by Marshal V.D. Sokolovsky:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In describing the essence of war, Marxism-Leninism takes as its point of departure the premise that war is not an aim in itself, but rather a tool of politics. In his remarks on Clausewitz's \"On War,\" Lenin stressed that \"Politics is the reason, and war is only the tool, not the other way around. Consequently, it remains only to subordinate the military point of view to the political.\"\nHenry A. Kissinger, however, described Lenin's approach as being that politics is a continuation of war by other means, thus turning Clausewitz's argument \"on its head.\"\nRapoport argued that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;As for Lenin's approval of Clausewitz, it probably stems from his obsession with the struggle for power. The whole Marxist conception of history is that of successive struggles for power, primarily between social classes. This was constantly applied by Lenin in a variety of contexts. Thus the entire history of philosophy appears in Lenin's writings as a vast struggle between \"idealism\" and \"materialism.\" The fate of the socialist movement was to be decided by a struggle between the revolutionists and the reformers. Clausewitz's acceptance of the struggle for power as the essence of international politics must have impressed Lenin as starkly realistic.\nClausewitz directly influenced Mao Zedong, who read \"On War\" in 1938 and organised a seminar on Clausewitz for the Party leadership in Yan'an. Thus the \"Clausewitzian\" content in many of Mao's writings is not merely a regurgitation of Lenin but reflects Mao's own study. The idea that war involves inherent \"friction\" that distorts, to a greater or lesser degree, all prior arrangements, has become common currency in fields such as business strategy and sport. The phrase \"fog of war\" derives from Clausewitz's stress on how confused warfare can seem while one is immersed within it. The term center of gravity, used in a military context derives from Clausewitz's usage, which he took from Newtonian mechanics. In U.S. military doctrine, \"center of gravity\" refers to the basis of an opponent's power at the operational, strategic, or political level, though this is only one aspect of Clausewitz's use of the term.\nLate 20th and early 21st century.\nThe deterrence strategy of the United States in the 1950s was closely inspired by President Dwight Eisenhower's reading of Clausewitz as a young officer in the 1920s. Eisenhower was greatly impressed by Clausewitz's example of a theoretical, idealized \"absolute war\" in \"Vom Kriege\" as a way of demonstrating how absurd it would be to attempt such a strategy in practice. For Eisenhower, the age of nuclear weapons had made what was for Clausewitz in the early-19th century only a theoretical vision an all too real possibility in the mid-20th century. From Eisenhower's viewpoint, the best deterrent to war was to show the world just how appalling and horrific a nuclear \"absolute war\" would be if it should ever occur, hence a series of much-publicized nuclear tests in the Pacific, giving first priority in the defense budget to nuclear weapons and to their delivery-systems over conventional weapons, and making repeated statements in public that the United States was able and willing at all times to use nuclear weapons. In this way, through the massive retaliation doctrine and the closely related foreign-policy concept of brinkmanship, Eisenhower hoped to hold out a credible vision of Clausewitzian nuclear \"absolute war\" in order to deter the Soviet Union and/or China from ever risking a war or even conditions that might lead to a war with the United States.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...Philanthropists may easily imagine there is a skillful method of disarming and overcoming an enemy without causing great bloodshed, and that this is the proper tendency of the art of War. However plausible this may appear, still it is an error which must be extirpated; for in such dangerous things as war, the errors which proceed from a spirit of benevolence are just the worst. As the use of physical power to the utmost extent by no means excludes the co-operation of the intelligence, it follows that he who uses force unsparingly, without reference to the quantity of bloodshed, must obtain a superiority if his adversary does not act likewise. By such means the former dictates the law to the latter, and both proceed to extremities, to which the only limitations are those imposed by the amount of counteracting force on each side.\u2014\u200a\nAfter 1970, some theorists claimed that nuclear proliferation made Clausewitzian concepts obsolete after the 20th-century period in which they dominated the world. John E. Sheppard Jr., argues that by developing nuclear weapons, state-based conventional armies simultaneously both perfected their original purpose, to destroy a mirror image of themselves, and made themselves obsolete. No two powers have used nuclear weapons against each other, instead using diplomacy, conventional means, or proxy wars to settle disputes. If such a conflict did occur, presumably both combatants would be annihilated. Heavily influenced by the war in Vietnam and by antipathy to American strategist Henry Kissinger, the American biologist, musician, and game-theorist Anatol Rapoport argued in 1968 that a Clausewitzian view of war was not only obsolete in the age of nuclear weapons, but also highly dangerous as it promoted a \"zero-sum paradigm\" to international relations and a \"dissolution of rationality\" amongst decision-makers.\nThe end of the 20th century and the beginning of the 21st century have seen many instances of state armies attempting to suppress insurgencies and terrorism, and engaging in other forms of asymmetrical warfare. Clausewitz did not focus solely on wars between countries with well-defined armies. The era of the French Revolution and Napoleon was full of revolutions, rebellions, and violence by \"non-state actors\" - witness the wars in the French Vend\u00e9e and in Spain. Clausewitz wrote a series of \"Lectures on Small War\" and studied the rebellion in the Vend\u00e9e (1793\u20131796) and the Tyrolean uprising of 1809. In his famous \"Bekenntnisdenkschrift\" of 1812 he called for a \"Spanish war in Germany\" and laid out a comprehensive guerrilla strategy to be waged against Napoleon. In \"On War\" he included a famous chapter on \"The People in Arms\".\nOne prominent critic of Clausewitz is the Israeli military historian Martin van Creveld. In his 1991 book \"The Transformation of War\", Creveld argued that Clausewitz's famous \"Trinity\" of people, army, and government was an obsolete socio-political construct based on the state, which was rapidly passing from the scene as the key player in war, and that he (Creveld) had constructed a new \"non-trinitarian\" model for modern warfare. Creveld's work has had great influence. Daniel Moran replied, 'The most egregious misrepresentation of Clausewitz's famous metaphor must be that of Martin van Creveld, who has declared Clausewitz to be an apostle of Trinitarian War, by which he means, incomprehensibly, a war of 'state against state and army against army,' from which the influence of the people is entirely excluded.\" Christopher Bassford went further, noting that one need only \"read\" the paragraph in which Clausewitz defined his Trinity to see \"that the words 'people,' 'army,' and 'government' appear nowhere at all in the list of the Trinity's components... Creveld's and Keegan's assault on Clausewitz's Trinity is not only a classic 'blow into the air,' i.e., an assault on a position Clausewitz doesn't occupy. It is also a pointless attack on a concept that is quite useful in its own right. In any case, their failure to read the actual wording of the theory they so vociferously attack, and to grasp its deep relevance to the phenomena they describe, is hard to credit.\"\nSome have gone further and suggested that Clausewitz's best-known aphorism, that war is a continuation of policy with other means, is not only irrelevant today but also inapplicable historically. For an opposing view see the sixteen essays presented in \"Clausewitz in the Twenty-First Century\" edited by Hew Strachan and Andreas Herberg-Rothe.\nIn military academies, schools, and universities worldwide, Clausewitz's \"Vom Kriege\" is often (usually in translation) mandatory reading.\nSome theorists of management look to Clausewitz\u2014just as some look to Sun Tzu\u2014to bolster ideas on the concept of leadership.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nInformational notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nPrimary sources (including translations).\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6067", "revid": "41419081", "url": "https://en.wikipedia.org/wiki?curid=6067", "title": "Common lisp", "text": ""}
{"id": "6068", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=6068", "title": "Common Lisp", "text": "Programming language standard\nCommon Lisp (CL) is a dialect of the Lisp programming language, published in American National Standards Institute (ANSI) standard document \"ANSI INCITS 226-1994 (S2018)\" (formerly \"X3.226-1994 (R1999)\"). The Common Lisp HyperSpec, a hyperlinked HTML version, has been derived from the ANSI Common Lisp standard.\nThe Common Lisp language was developed as a standardized and improved successor of Maclisp. By the early 1980s several groups were already at work on diverse successors to MacLisp: Lisp Machine Lisp (aka ZetaLisp), Spice Lisp, NIL and S-1 Lisp. Common Lisp sought to unify, standardise, and extend the features of these MacLisp dialects. Common Lisp is not an implementation, but rather a language specification. Several implementations of the Common Lisp standard are available, including free and open-source software and proprietary products.\nCommon Lisp is a general-purpose, multi-paradigm programming language. It supports a combination of procedural, functional, and object-oriented programming paradigms. As a dynamic programming language, it facilitates evolutionary and incremental software development, with iterative compilation into efficient run-time programs. This incremental development is often done interactively without interrupting the running application.\nIt also supports optional type annotation and casting, which can be added as necessary at the later profiling and optimization stages, to permit the compiler to generate more efficient code. For instance, codice_1 can hold an unboxed integer in a range supported by the hardware and implementation, permitting more efficient arithmetic than on big integers or arbitrary precision types. Similarly, the compiler can be told on a per-module or per-function basis which type of safety level is wanted, using \"optimize\" declarations.\nCommon Lisp includes CLOS, an object system that supports multimethods and method combinations. It is often implemented with a Metaobject Protocol.\nCommon Lisp is extensible through standard features such as \"Lisp macros\" (code transformations) and \"reader macros\" (input parsers for characters).\nCommon Lisp provides partial backwards compatibility with Maclisp and John McCarthy's original Lisp. This allows older Lisp software to be ported to Common Lisp.\nHistory.\nWork on Common Lisp started in 1981 after an initiative by ARPA manager Bob Engelmore to develop a single community standard Lisp dialect. Much of the initial language design was done via electronic mail. In 1982, Guy L. Steele Jr. gave the first overview of Common Lisp at the 1982 ACM Symposium on LISP and functional programming.\nThe first language documentation was published in 1984 as Common Lisp the Language (known as CLtL1), first edition. A second edition (known as CLtL2), published in 1990, incorporated many changes to the language, made during the ANSI Common Lisp standardization process: extended LOOP syntax, the Common Lisp Object System, the Condition System for error handling, an interface to the pretty printer and much more. But CLtL2 does not describe the final ANSI Common Lisp standard and thus is not a documentation of ANSI Common Lisp. The final ANSI Common Lisp standard then was published in 1994. Since then no update to the standard has been published. Various extensions and improvements to Common Lisp (examples are Unicode, Concurrency, CLOS-based IO) have been provided by implementations and libraries.\nSyntax.\nCommon Lisp is a dialect of Lisp. It uses S-expressions to denote both code and data structure. Function calls, macro forms and special forms are written as lists, with the name of the operator first, as in these examples:\n (+ 2 2) ; adds 2 and 2, yielding 4. The function's name is '+'. Lisp has no operators as such.\n (defvar *x*) ; Ensures that a variable *x* exists,\n ; without giving it a value. The asterisks are part of\n ; the name, by convention denoting a special (global) variable. \n ; The symbol *x* is also hereby endowed with the property that\n ; subsequent bindings of it are dynamic, rather than lexical.\n (setf *x* 42.1) ; Sets the variable *x* to the floating-point value 42.1\n ;; Define a function that squares a number:\n (defun square (x)\n (* x x))\n ;; Execute the function:\n (square 3) ; Returns 9\n ;; The 'let' construct creates a scope for local variables. Here\n ;; the variable 'a' is bound to 6 and the variable 'b' is bound\n ;; to 4. Inside the 'let' is a 'body', where the last computed value is returned.\n ;; Here the result of adding a and b is returned from the 'let' expression.\n ;; The variables a and b have lexical scope, unless the symbols have been\n ;; marked as special variables (for instance by a prior DEFVAR).\n (let ((a 6)\n (b 4))\n (+ a b)) ; returns 10\nData types.\nCommon Lisp has many data types.\nScalar types.\n\"Number\" types include integers, ratios, floating-point numbers, and complex numbers. Common Lisp uses bignums to represent numerical values of arbitrary size and precision. The ratio type represents fractions exactly, a facility not available in many languages. Common Lisp automatically coerces numeric values among these types as appropriate.\nThe Common Lisp \"character\" type is not limited to ASCII characters. Most modern implementations allow Unicode characters.\nThe \"symbol\" type is common to Lisp languages, but largely unknown outside them. A symbol is a unique, named data object with several parts: name, value, function, property list, and package. Of these, \"value cell\" and \"function cell\" are the most important. Symbols in Lisp are often used similarly to identifiers in other languages: to hold the value of a variable; however there are many other uses. Normally, when a symbol is evaluated, its value is returned. Some symbols evaluate to themselves, for example, all symbols in the keyword package are self-evaluating. Boolean values in Common Lisp are represented by the self-evaluating symbols T and NIL. Common Lisp has namespaces for symbols, called 'packages'.\nA number of functions are available for rounding scalar numeric values in various ways. The function codice_2 rounds the argument to the nearest integer, with halfway cases rounded to the even integer. The functions codice_3, codice_4, and codice_5 round towards zero, down, or up respectively. All these functions return the discarded fractional part as a secondary value. For example, codice_6 yields \u22123, 0.5; codice_7 yields \u22122, \u22120.5; codice_8 yields 2, 0.5; and codice_9 yields 4, \u22120.5.\nData structures.\n\"Sequence\" types in Common Lisp include lists, vectors, bit-vectors, and strings. There are many operations that can work on any sequence type.\nAs in almost all other Lisp dialects, \"lists\" in Common Lisp are composed of \"conses\", sometimes called \"cons cells\" or \"pairs\". A cons is a data structure with two slots, called its \"car\" and \"cdr\". A list is a linked chain of conses or the empty list. Each cons's car refers to a member of the list (possibly another list). Each cons's cdr refers to the next cons\u2014except for the last cons in a list, whose cdr refers to the codice_10 value. Conses can also easily be used to implement trees and other complex data structures; though it is usually advised to use structure or class instances instead. It is also possible to create circular data structures with conses.\nCommon Lisp supports multidimensional \"arrays\", and can dynamically resize \"adjustable\" arrays if required. Multidimensional arrays can be used for matrix mathematics. A \"vector\" is a one-dimensional array. Arrays can carry any type as members (even mixed types in the same array) or can be specialized to contain a specific type of members, as in a vector of bits. Usually, only a few types are supported. Many implementations can optimize array functions when the array used is type-specialized. Two type-specialized array types are standard: a \"string\" is a vector of characters, while a \"bit-vector\" is a vector of bits.\n\"Hash tables\" store associations between data objects. Any object may be used as key or value. Hash tables are automatically resized as needed.\n\"Packages\" are collections of symbols, used chiefly to separate the parts of a program into namespaces. A package may \"export\" some symbols, marking them as part of a public interface. Packages can use other packages.\n\"Structures\", similar in use to C structs and Pascal records, represent arbitrary complex data structures with any number and type of fields (called \"slots\"). Structures allow single-inheritance.\n\"Classes\" are similar to structures, but offer more dynamic features and multiple-inheritance. (See CLOS). Classes have been added late to Common Lisp and there is some conceptual overlap with structures. Objects created of classes are called \"Instances\". A special case is Generic Functions. Generic Functions are both functions and instances.\nFunctions.\nCommon Lisp supports first-class functions. For instance, it is possible to write functions that take other functions as arguments or return functions as well. This makes it possible to describe very general operations.\nThe Common Lisp library relies heavily on such higher-order functions. For example, the codice_11 function takes a relational operator as an argument and key function as an optional keyword argument. This can be used not only to sort any type of data, but also to sort data structures according to a key.\n ;; Sorts the list using the &gt; and &lt; function as the relational operator.\n (sort (list 5 2 6 3 1 4) #'&gt;) ; Returns (6 5 4 3 2 1)\n (sort (list 5 2 6 3 1 4) #'&lt;) ; Returns (1 2 3 4 5 6)\n ;; Sorts the list according to the first element of each sub-list.\n (sort (list '(9 A) '(3 B) '(4 C)) #'&lt; :key #'first) ; Returns ((3 B) (4 C) (9 A))\nThe evaluation model for functions is very simple. When the evaluator encounters a form codice_12 then it presumes that the symbol named f is one of the following:\nIf codice_14 is the name of a function, then the arguments codice_15 are evaluated in left-to-right order, and the function is found and invoked with those values supplied as parameters.\nDefining functions.\nThe macro codice_16 defines functions where a function definition gives the name of the function, the names of any arguments, and a function body:\n (defun square (x)\n (* x x))\nFunction definitions may include compiler directives, known as \"declarations\", which provide hints to the compiler about optimization settings or the data types of arguments. They may also include \"documentation strings\" (docstrings), which the Lisp system may use to provide interactive documentation:\n (defun square (x)\n \"Calculates the square of the single-float x.\"\n (declare (single-float x) (optimize (speed 3) (debug 0) (safety 1)))\n (the single-float (* x x)))\nAnonymous functions (function literals) are defined using codice_13 expressions, e.g. codice_18 for a function that squares its argument. Lisp programming style frequently uses higher-order functions for which it is useful to provide anonymous functions as arguments.\nLocal functions can be defined with codice_19 and codice_20.\n (flet ((square (x)\n (* x x)))\n (square 3))\nThere are several other operators related to the definition and manipulation of functions. For instance, a function may be compiled with the codice_21 operator. (Some Lisp systems run functions using an interpreter by default unless instructed to compile; others compile every function).\nDefining generic functions and methods.\nThe macro codice_22 defines generic functions. Generic functions are a collection of methods.\nThe macro codice_23 defines methods.\nMethods can specialize their parameters over CLOS \"standard classes\", \"system classes\", \"structure classes\" or individual objects. For many types, there are corresponding \"system classes\".\nWhen a generic function is called, multiple-dispatch will determine the effective method to use.\n (defgeneric add (a b))\n (defmethod add ((a number) (b number))\n (+ a b))\n (defmethod add ((a vector) (b number))\n (map 'vector (lambda (n) (+ n b)) a))\n (defmethod add ((a vector) (b vector))\n (map 'vector #'+ a b))\n (concatenate 'string a b))\n (add 2 3) ; returns 5\n (add #(1 2 3 4) 7) ; returns #(8 9 10 11)\n (add #(1 2 3 4) #(4 3 2 1)) ; returns #(5 5 5 5)\n (add \"COMMON \" \"LISP\") ; returns \"COMMON LISP\"\nGeneric Functions are also a first class data type. There are many more features to Generic Functions and Methods than described above.\nThe function namespace.\nThe namespace for function names is separate from the namespace for data variables. This is a key difference between Common Lisp and Scheme. For Common Lisp, operators that define names in the function namespace include codice_16, codice_19, codice_20, codice_23 and codice_22.\nTo pass a function by name as an argument to another function, one must use the codice_29 special operator, commonly abbreviated as codice_30. The first codice_11 example above refers to the function named by the symbol codice_32 in the function namespace, with the code codice_33. Conversely, to call a function passed in such a way, one would use the codice_34 operator on the argument.\nScheme's evaluation model is simpler: there is only one namespace, and all positions in the form are evaluated (in any order) \u2013 not just the arguments. Code written in one dialect is therefore sometimes confusing to programmers more experienced in the other. For instance, many Common Lisp programmers like to use descriptive variable names such as \"list\" or \"string\" which could cause problems in Scheme, as they would locally shadow function names.\nWhether a separate namespace for functions is an advantage is a source of contention in the Lisp community. It is usually referred to as the \"Lisp-1 vs. Lisp-2 debate\". Lisp-1 refers to Scheme's model and Lisp-2 refers to Common Lisp's model. These names were coined in a 1988 paper by Richard P. Gabriel and Kent Pitman, which extensively compares the two approaches.\nMultiple return values.\nCommon Lisp supports the concept of \"multiple values\", where any expression always has a single \"primary value\", but it might also have any number of \"secondary values\", which might be received and inspected by interested callers. This concept is distinct from returning a list value, as the secondary values are fully optional, and passed via a dedicated side channel. This means that callers may remain entirely unaware of the secondary values being there if they have no need for them, and it makes it convenient to use the mechanism for communicating information that is sometimes useful, but not always necessary. For example,\n (y 458))\n (multiple-value-bind (quotient remainder)\n (truncate x y)\n (format nil \"~A divided by ~A is ~A remainder ~A\" x y quotient remainder)))\n (gethash 'answer library 42))\n (format nil \"The answer is ~A\" (get-answer library)))\n (multiple-value-bind (answer sure-p)\n (get-answer library)\n (if (not sure-p)\n \"I don't know\"\n (format nil \"The answer is ~A\" answer))))\nMultiple values are supported by a handful of standard forms, most common of which are the codice_37 special form for accessing secondary values and codice_38 for returning multiple values:\n \"Return an outlook prediction, with the probability as a secondary value\"\n (values \"Outlook good\" (random 1.0)))\nOther types.\nOther data types in Common Lisp include:\nScope.\nLike programs in many other programming languages, Common Lisp programs make use of names to refer to variables, functions, and many other kinds of entities. Named references are subject to scope.\nThe association between a name and the entity which the name refers to is called a binding.\nScope refers to the set of circumstances in which a name is determined to have a particular binding.\nDeterminers of scope.\nThe circumstances which determine scope in Common Lisp include:\nTo understand what a symbol refers to, the Common Lisp programmer must know what kind of reference is being expressed, what kind of scope it uses if it is a variable reference (dynamic versus lexical scope), and also the run-time situation: in what environment is the reference resolved, where was the binding introduced into the environment, et cetera.\nKinds of environment.\nGlobal.\nSome environments in Lisp are globally pervasive. For instance, if a new type is defined, it is known everywhere thereafter. References to that type look it up in this global environment.\nDynamic.\nOne type of environment in Common Lisp is the dynamic environment. Bindings established in this environment have dynamic extent, which means that a binding is established at the start of the execution of some construct, such as a codice_53 block, and disappears when that construct finishes executing: its lifetime is tied to the dynamic activation and deactivation of a block. However, a dynamic binding is not just visible within that block; it is also visible to all functions invoked from that block. This type of visibility is known as indefinite scope. Bindings which exhibit dynamic extent (lifetime tied to the activation and deactivation of a block) and indefinite scope (visible to all functions which are called from that block) are said to have dynamic scope.\nCommon Lisp has support for dynamically scoped variables, which are also called special variables. Certain other kinds of bindings are necessarily dynamically scoped also, such as restarts and catch tags. Function bindings cannot be dynamically scoped using codice_19 (which only provides lexically scoped function bindings), but function objects (a first-level object in Common Lisp) can be assigned to dynamically scoped variables, bound using codice_53 in dynamic scope, then called using codice_34 or codice_59.\nDynamic scope is extremely useful because it adds referential clarity and discipline to global variables. Global variables are frowned upon in computer science as potential sources of error, because they can give rise to ad-hoc, covert channels of communication among modules that lead to unwanted, surprising interactions.\nIn Common Lisp, a special variable which has only a top-level binding behaves just like a global variable in other programming languages. A new value can be stored into it, and that value simply replaces what is in the top-level binding. Careless replacement of the value of a global variable is at the heart of bugs caused by the use of global variables. However, another way to work with a special variable is to give it a new, local binding within an expression. This is sometimes referred to as \"rebinding\" the variable. Binding a dynamically scoped variable temporarily creates a new memory location for that variable, and associates the name with that location. While that binding is in effect, all references to that variable refer to the new binding; the previous binding is hidden. When execution of the binding expression terminates, the temporary memory location is gone, and the old binding is revealed, with the original value intact. Of course, multiple dynamic bindings for the same variable can be nested.\nIn Common Lisp implementations which support multithreading, dynamic scopes are specific to each thread of execution. Thus special variables serve as an abstraction for thread local storage. If one thread rebinds a special variable, this rebinding has no effect on that variable in other threads. The value stored in a binding can only be retrieved by the thread which created that binding. If each thread binds some special variable codice_60, then codice_60 behaves like thread-local storage. Among threads which do not rebind codice_60, it behaves like an ordinary global: all of these threads refer to the same top-level binding of codice_60.\nDynamic variables can be used to extend the execution context with additional context information which is implicitly passed from function to function without having to appear as an extra function parameter. This is especially useful when the control transfer has to pass through layers of unrelated code, which simply cannot be extended with extra parameters to pass the additional data. A situation like this usually calls for a global variable. That global variable must be saved and restored, so that the scheme doesn't break under recursion: dynamic variable rebinding takes care of this. And that variable must be made thread-local (or else a big mutex must be used) so the scheme doesn't break under threads: dynamic scope implementations can take care of this also.\nIn the Common Lisp library, there are many standard special variables. For instance, all standard I/O streams are stored in the top-level bindings of well-known special variables. The standard output stream is stored in *standard-output*.\nSuppose a function foo writes to standard output:\n (defun foo ()\n (format t \"Hello, world\"))\nTo capture its output in a character string, *standard-output* can be bound to a string stream and called:\n (with-output-to-string (*standard-output*)\n (foo))\n -&gt; \"Hello, world\" ; gathered output returned as a string\nLexical.\nCommon Lisp supports lexical environments. Formally, the bindings in a lexical environment have lexical scope and may have either an indefinite extent or dynamic extent, depending on the type of namespace. Lexical scope means that visibility is physically restricted to the block in which the binding is established. References which are not textually (i.e. lexically) embedded in that block simply do not see that binding.\nThe tags in a TAGBODY have lexical scope. The expression (GO X) is erroneous if it is not embedded in a TAGBODY which contains a label X. However, the label bindings disappear when the TAGBODY terminates its execution, because they have dynamic extent. If that block of code is re-entered by the invocation of a lexical closure, it is invalid for the body of that closure to try to transfer control to a tag via GO:\n (defvar *stashed*) ;; will hold a function\n (tagbody\n (setf *stashed* (lambda () (go some-label)))\n (go end-label) ;; skip the (print \"Hello\")\n some-label\n (print \"Hello\")\n end-label)\n -&gt; NIL\nWhen the TAGBODY is executed, it first evaluates the setf form which stores a function in the special variable *stashed*. Then the (go end-label) transfers control to end-label, skipping the code (print \"Hello\"). Since end-label is at the end of the tagbody, the tagbody terminates, yielding NIL. Suppose that the previously remembered function is now called:\n (funcall *stashed*) ;; Error!\nThis situation is erroneous. One implementation's response is an error condition containing the message, \"GO: tagbody for tag SOME-LABEL has already been left\". The function tried to evaluate (go some-label), which is lexically embedded in the tagbody, and resolves to the label. However, the tagbody isn't executing (its extent has ended), and so the control transfer cannot take place.\nLocal function bindings in Lisp have lexical scope, and variable bindings also have lexical scope by default. By contrast with GO labels, both of these have indefinite extent. When a lexical function or variable binding is established, that binding continues to exist for as long as references to it are possible, even after the construct which established that binding has terminated. References to lexical variables and functions after the termination of their establishing construct are possible thanks to lexical closures.\nLexical binding is the default binding mode for Common Lisp variables. For an individual symbol, it can be switched to dynamic scope, either by a local declaration, by a global declaration. The latter may occur implicitly through the use of a construct like DEFVAR or DEFPARAMETER. It is an important convention in Common Lisp programming that special (i.e. dynamically scoped) variables have names which begin and end with an asterisk sigil codice_64 in what is called the \"earmuff convention\". If adhered to, this convention effectively creates a separate namespace for special variables, so that variables intended to be lexical are not accidentally made special.\nLexical scope is useful for several reasons.\nFirstly, references to variables and functions can be compiled to efficient machine code, because the run-time environment structure is relatively simple. In many cases it can be optimized to stack storage, so opening and closing lexical scopes has minimal overhead. Even in cases where full closures must be generated, access to the closure's environment is still efficient; typically each variable becomes an offset into a vector of bindings, and so a variable reference becomes a simple load or store instruction with a base-plus-offset addressing mode.\nSecondly, lexical scope (combined with indefinite extent) gives rise to the lexical closure, which in turn creates a whole paradigm of programming centered around the use of functions being first-class objects, which is at the root of functional programming.\nThirdly, perhaps most importantly, even if lexical closures are not exploited, the use of lexical scope isolates program modules from unwanted interactions. Due to their restricted visibility, lexical variables are private. If one module A binds a lexical variable X, and calls another module B, references to X in B will not accidentally resolve to the X bound in A. B simply has no access to X. For situations in which disciplined interactions through a variable are desirable, Common Lisp provides special variables. Special variables allow for a module A to set up a binding for a variable X which is visible to another module B, called from A. Being able to do this is an advantage, and being able to prevent it from happening is also an advantage; consequently, Common Lisp supports both lexical and dynamic scope.\nMacros.\nA \"macro\" in Lisp superficially resembles a function in usage. However, rather than representing an expression which is evaluated, it represents a transformation of the program source code. The macro gets the source it surrounds as arguments, binds them to its parameters and computes a new source form. This new form can also use a macro. The macro expansion is repeated until the new source form does not use a macro. The final computed form is the source code executed at runtime.\nTypical uses of macros in Lisp:\nVarious standard Common Lisp features also need to be implemented as macros, such as:\nMacros are defined by the \"defmacro\" macro. The special operator \"macrolet\" allows the definition of local (lexically scoped) macros. It is also possible to define macros for symbols using \"define-symbol-macro\" and \"symbol-macrolet\".\nPaul Graham's book On Lisp describes the use of macros in Common Lisp in detail. Doug Hoyte's book Let Over Lambda extends the discussion on macros, claiming \"Macros are the single greatest advantage that lisp has as a programming language and the single greatest advantage of any programming language.\" Hoyte provides several examples of iterative development of macros.\nExample using a macro to define a new control structure.\nMacros allow Lisp programmers to create new syntactic forms in the language. One typical use is to create new control structures. The example macro provides an codice_75 looping construct. The syntax is:\nThe macro definition for \"until\":\n (let ((start-tag (gensym \"START\"))\n (end-tag (gensym \"END\")))\n `(tagbody ,start-tag\n (when ,test (go ,end-tag))\n (progn ,@body)\n (go ,start-tag)\n ,end-tag)))\n\"tagbody\" is a primitive Common Lisp special operator which provides the ability to name tags and use the \"go\" form to jump to those tags. The backquote \"`\" provides a notation that provides code templates, where the value of forms preceded with a comma are filled in. Forms preceded with comma and at-sign are \"spliced\" in. The tagbody form tests the end condition. If the condition is true, it jumps to the end tag. Otherwise, the provided body code is executed and then it jumps to the start tag.\nAn example of using the above \"until\" macro:\n (write-line \"Hello\"))\nThe code can be expanded using the function \"macroexpand-1\". The expansion for the above example looks like this:\n(TAGBODY\n #:START1136\n (WHEN (ZEROP (RANDOM 10))\n (GO #:END1137))\n (PROGN (WRITE-LINE \"hello\"))\n (GO #:START1136)\n #:END1137)\nDuring macro expansion the value of the variable \"test\" is \"(= (random 10) 0)\" and the value of the variable \"body\" is \"((write-line \"Hello\"))\". The body is a list of forms.\nSymbols are usually automatically upcased. The expansion uses the TAGBODY with two labels. The symbols for these labels are computed by GENSYM and are not interned in any package. Two \"go\" forms use these tags to jump to. Since \"tagbody\" is a primitive operator in Common Lisp (and not a macro), it will not be expanded into something else. The expanded form uses the \"when\" macro, which also will be expanded. Fully expanding a source form is called \"code walking\".\nIn the fully expanded (\"walked\") form, the \"when\" form is replaced by the primitive \"if\":\n(TAGBODY\n #:START1136\n (IF (ZEROP (RANDOM 10))\n (PROGN (GO #:END1137))\n NIL)\n (PROGN (WRITE-LINE \"hello\"))\n (GO #:START1136))\n #:END1137)\nAll macros must be expanded before the source code containing them can be evaluated or compiled normally. Macros can be considered functions that accept and return S-expressions \u2013 similar to abstract syntax trees, but not limited to those. These functions are invoked before the evaluator or compiler to produce the final source code. Macros are written in normal Common Lisp, and may use any Common Lisp (or third-party) operator available.\nVariable capture and shadowing.\nCommon Lisp macros are capable of what is commonly called \"variable capture\", where symbols in the macro-expansion body coincide with those in the calling context, allowing the programmer to create macros wherein various symbols have special meaning. The term \"variable capture\" is somewhat misleading, because all namespaces are vulnerable to unwanted capture, including the operator and function namespace, the tagbody label namespace, catch tag, condition handler and restart namespaces.\n\"Variable capture\" can introduce software defects. This happens in one of the following two ways:\nThe Scheme dialect of Lisp provides a macro-writing system which provides the referential transparency that eliminates both types of capture problem. This type of macro system is sometimes called \"hygienic\", in particular by its proponents (who regard macro systems which do not automatically solve this problem as unhygienic). \nIn Common Lisp, macro hygiene is ensured one of two different ways.\nOne approach is to use gensyms: guaranteed-unique symbols which can be used in a macro-expansion without threat of capture. The use of gensyms in a macro definition is a manual chore, but macros can be written which simplify the instantiation and use of gensyms. Gensyms solve type 2 capture easily, but they are not applicable to type 1 capture in the same way, because the macro expansion cannot rename the interfering symbols in the surrounding code which capture its references. Gensyms could be used to provide stable aliases for the global symbols which the macro expansion needs. The macro expansion would use these secret aliases rather than the well-known names, so redefinition of the well-known names would have no ill effect on the macro.\nAnother approach is to use packages. A macro defined in its own package can simply use internal symbols in that package in its expansion. The use of packages deals with type 1 and type 2 capture.\nHowever, packages don't solve the type 1 capture of references to standard Common Lisp functions and operators. The reason is that the use of packages to solve capture problems revolves around the use of private symbols (symbols in one package, which are not imported into, or otherwise made visible in other packages). Whereas the Common Lisp library symbols are external, and frequently imported into or made visible in user-defined packages.\nThe following is an example of unwanted capture in the operator namespace, occurring in the expansion of a macro:\n ;; expansion of UNTIL makes liberal use of DO\n (defmacro until (expression &amp;body body)\n `(do () (,expression) ,@body))\n ;; macrolet establishes lexical operator binding for DO\n (macrolet ((do (...) ... something else ...))\n (until (= (random 10) 0) (write-line \"Hello\")))\nThe codice_75 macro will expand into a form which calls codice_77 which is intended to refer to the standard Common Lisp macro codice_77. However, in this context, codice_77 may have a completely different meaning, so codice_75 may not work properly.\nCommon Lisp solves the problem of the shadowing of standard operators and functions by forbidding their redefinition. Because it redefines the standard operator codice_77, the preceding is actually a fragment of non-conforming Common Lisp, which allows implementations to diagnose and reject it.\nCondition system.\nThe \"condition system\" is responsible for exception handling in Common Lisp. It provides \"conditions\", \"handler\"s and \"restart\"s. \"Condition\"s are objects describing an exceptional situation (for example an error). If a \"condition\" is signaled, the Common Lisp system searches for a \"handler\" for this condition type and calls the handler. The \"handler\" can now search for restarts and use one of these restarts to automatically repair the current problem, using information such as the condition type and any relevant information provided as part of the condition object, and call the appropriate restart function.\nThese restarts, if unhandled by code, can be presented to users (as part of a user interface, that of a debugger for example), so that the user can select and invoke one of the available restarts. Since the condition handler is called in the context of the error (without unwinding the stack), full error recovery is possible in many cases, where other exception handling systems would have already terminated the current routine. The debugger itself can also be customized or replaced using the codice_82 dynamic variable. Code found within \"unwind-protect\" forms such as finalizers will also be executed as appropriate despite the exception.\nIn the following example (using Symbolics Genera) the user tries to open a file in a Lisp function \"test\" called from the Read-Eval-Print-LOOP (REPL), when the file does not exist. The Lisp system presents four restarts. The user selects the \"Retry OPEN using a different pathname\" restart and enters a different pathname (lispm-init.lisp instead of lispm-int.lisp). The user code does not contain any error handling code. The whole error handling and restart code is provided by the Lisp system, which can handle and repair the error without terminating the user code.\nCommand: (test \"&gt;zippy&gt;lispm-int.lisp\")\nError: The file was not found.\n For lispm:&gt;zippy&gt;lispm-int.lisp.newest\nLMFS:OPEN-LOCAL-LMFS-1\n Arg 0: #P\"lispm:&gt;zippy&gt;lispm-int.lisp.newest\"\ns-A, &lt;Resume&gt;: Retry OPEN of lispm:&gt;zippy&gt;lispm-int.lisp.newest\ns-B: Retry OPEN using a different pathname\ns-C, &lt;Abort&gt;: Return to Lisp Top Level in a TELNET server\ns-D: Restart process TELNET terminal\n-&gt; Retry OPEN using a different pathname\nUse what pathname instead [default lispm:&gt;zippy&gt;lispm-int.lisp.newest]:\n lispm:&gt;zippy&gt;lispm-init.lisp.newest\n...the program continues\nCommon Lisp Object System (CLOS).\nCommon Lisp includes a toolkit for object-oriented programming, the Common Lisp Object System or CLOS. Peter Norvig explains how many Design Patterns are simpler to implement in a dynamic language with the features of CLOS (Multiple Inheritance, Mixins, Multimethods, Metaclasses, Method combinations, etc.).\nSeveral extensions to Common Lisp for object-oriented programming have been proposed to be included into the ANSI Common Lisp standard, but eventually CLOS was adopted as the standard object-system for Common Lisp. CLOS is a dynamic object system with multiple dispatch and multiple inheritance, and differs radically from the OOP facilities found in static languages such as C++ or Java. As a dynamic object system, CLOS allows changes at runtime to generic functions and classes. Methods can be added and removed, classes can be added and redefined, objects can be updated for class changes and the class of objects can be changed.\nCLOS has been integrated into ANSI Common Lisp. Generic functions can be used like normal functions and are a first-class data type. Every CLOS class is integrated into the Common Lisp type system. Many Common Lisp types have a corresponding class. There is more potential use of CLOS for Common Lisp. The specification does not say whether conditions are implemented with CLOS. Pathnames and streams could be implemented with CLOS. These further usage possibilities of CLOS for ANSI Common Lisp are not part of the standard. Actual Common Lisp implementations use CLOS for pathnames, streams, input\u2013output, conditions, the implementation of CLOS itself and more.\nCompiler and interpreter.\nA Lisp interpreter directly executes Lisp source code provided as Lisp objects (lists, symbols, numbers, ...) read from s-expressions. A Lisp compiler generates bytecode or machine code from Lisp source code. Common Lisp allows both individual Lisp functions to be compiled in memory and the compilation of whole files to externally stored compiled code (\"fasl\" files).\nSeveral implementations of earlier Lisp dialects provided both an interpreter and a compiler. Unfortunately often the semantics were different. These earlier Lisps implemented lexical scoping in the compiler and dynamic scoping in the interpreter. Common Lisp requires that both the interpreter and compiler use lexical scoping by default. The Common Lisp standard describes both the semantics of the interpreter and a compiler. The compiler can be called using the function \"compile\" for individual functions and using the function \"compile-file\" for files. Common Lisp allows type declarations and provides ways to influence the compiler code generation policy. For the latter various optimization qualities can be given values between 0 (not important) and 3 (most important): \"speed\", \"space\", \"safety\", \"debug\" and \"compilation-speed\".\nThere is also a function to evaluate Lisp code: codice_83. codice_83 takes code as pre-parsed s-expressions and not, like in some other languages, as text strings. This way code can be constructed with the usual Lisp functions for constructing lists and symbols and then this code can be evaluated with the function codice_83. Several Common Lisp implementations (like Clozure CL and SBCL) are implementing codice_83 using their compiler. This way code is compiled, even though it is evaluated using the function codice_83.\nThe file compiler is invoked using the function \"compile-file\". The generated file with compiled code is called a \"fasl\" (from \"fast load\") file. These \"fasl\" files and also source code files can be loaded with the function \"load\" into a running Common Lisp system. Depending on the implementation, the file compiler generates byte-code (for example for the Java Virtual Machine), C language code (which then is compiled with a C compiler) or, directly, native code.\nCommon Lisp implementations can be used interactively, even though the code gets fully compiled. The idea of an Interpreted language thus does not apply for interactive Common Lisp.\nThe language makes a distinction between read-time, compile-time, load-time, and run-time, and allows user code to also make this distinction to perform the wanted type of processing at the wanted step.\nSome special operators are provided to especially suit interactive development; for instance, codice_88 will only assign a value to its provided variable if it wasn't already bound, while codice_89 will always perform the assignment. This distinction is useful when interactively evaluating, compiling and loading code in a live image.\nSome features are also provided to help writing compilers and interpreters. Symbols consist of first-level objects and are directly manipulable by user code. The codice_90 special operator allows to create lexical bindings programmatically, while packages are also manipulable. The Lisp compiler is available at runtime to compile files or individual functions. These make it easy to use Lisp as an intermediate compiler or interpreter for another language.\nCode examples.\nBirthday paradox.\nThe following program calculates the smallest number of people in a room for whom the probability of unique birthdays is less than 50% (the birthday paradox, where for 1 person the probability is obviously 100%, for 2 it is 364/365, etc.). The answer is 23.\nIn Common Lisp, by convention, constants are enclosed with + characters.\n (let ((new-probability (* (/ (- +year-size+ number-of-people)\n +year-size+)\n probability)))\n (if (&lt; new-probability 0.5)\n (1+ number-of-people)\n (birthday-paradox new-probability (1+ number-of-people)))))\nCalling the example function using the REPL (Read Eval Print Loop):\nCL-USER &gt; (birthday-paradox 1.0 1)\n23\nSorting a list of person objects.\nWe define a class codice_91 and a method for displaying the name and age of a person.\nNext we define a group of persons as a list of codice_91 objects.\nThen we iterate over the sorted list.\n ((name :initarg :name :accessor person-name)\n (age :initarg :age :accessor person-age))\n (:documentation \"The class PERSON with slots NAME and AGE.\"))\n \"Displaying a PERSON object to an output stream.\"\n (with-slots (name age) object\n (format stream \"~a (~a)\" name age)))\n(defparameter *group*\n (list (make-instance 'person :name \"Bob\" :age 33)\n (make-instance 'person :name \"Chris\" :age 16)\n (make-instance 'person :name \"Ash\" :age 23))\n \"A list of PERSON objects.\")\n :key #'person-age))\n (display person *standard-output*)\n (terpri))\nIt prints the three names with descending age.\nBob (33)\nAsh (23)\nChris (16)\nExponentiating by squaring.\nUse of the LOOP macro is demonstrated:\n (loop with result = 1\n while (plusp n)\n when (oddp n) do (setf result (* result x))\n do (setf x (* x x)\n n (truncate n 2))\n finally (return result)))\nExample use:\nCL-USER &gt; (power 2 200)\n1606938044258990275541962092341162602522202993782792835301376\nCompare with the built in exponentiation:\nCL-USER &gt; (= (expt 2 200) (power 2 200))\nT\nFind the list of available shells.\nWITH-OPEN-FILE is a macro that opens a file and provides a stream. When the form is returning, the file is automatically closed. FUNCALL calls a function object. The LOOP collects all lines that match the predicate.\n \"Returns a list of lines in file, for which the predicate applied to\n the line returns T.\"\n (with-open-file (stream file)\n (loop for line = (read-line stream nil nil)\n while line\n when (funcall predicate line)\n collect it)))\nThe function AVAILABLE-SHELLS calls the above function LIST-MATCHING-LINES with a pathname and an anonymous function as the predicate. The predicate returns the pathname of a shell or NIL (if the string is not the filename of a shell).\n (list-matching-lines\n file\n (lambda (line)\n (and (plusp (length line))\n (char= (char line 0) #\\/)\n (pathname\n (string-right-trim '(#\\space #\\tab) line))))))\nExample results (on Mac OS X 10.6):\nCL-USER &gt; (available-shells)\nComparison with other Lisps.\nCommon Lisp is most frequently compared with, and contrasted to, Scheme\u2014if only because they are the two most popular Lisp dialects. Scheme predates CL, and comes not only from the same Lisp tradition but from some of the same engineers\u2014Guy Steele, with whom Gerald Jay Sussman designed Scheme, chaired the standards committee for Common Lisp.\nCommon Lisp is a general-purpose programming language, in contrast to Lisp variants such as Emacs Lisp and AutoLISP which are extension languages embedded in particular products (GNU Emacs and AutoCAD, respectively). Unlike many earlier Lisps, Common Lisp (like Scheme) uses lexical variable scope by default for both interpreted and compiled code.\nMost of the Lisp systems whose designs contributed to Common Lisp\u2014such as ZetaLisp and Franz Lisp\u2014used dynamically scoped variables in their interpreters and lexically scoped variables in their compilers. Scheme introduced the sole use of lexically scoped variables to Lisp; an inspiration from ALGOL 68. CL supports dynamically scoped variables as well, but they must be explicitly declared as \"special\". There are no differences in scoping between ANSI CL interpreters and compilers.\nCommon Lisp is sometimes termed a \"Lisp-2\" and Scheme a \"Lisp-1\", referring to CL's use of separate namespaces for functions and variables. (In fact, CL has \"many\" namespaces, such as those for go tags, block names, and codice_74 keywords). There is a long-standing controversy between CL and Scheme advocates over the tradeoffs involved in multiple namespaces. In Scheme, it is (broadly) necessary to avoid giving variables names that clash with functions; Scheme functions frequently have arguments named codice_94, codice_95, or codice_96 so as not to conflict with the system function codice_97. However, in CL it is necessary to explicitly refer to the function namespace when passing a function as an argument\u2014which is also a common occurrence, as in the codice_11 example above.\nCL also differs from Scheme in its handling of Boolean values. Scheme uses the special values #t and #f to represent truth and falsity. CL follows the older Lisp convention of using the symbols T and NIL, with NIL standing also for the empty list. In CL, \"any\" non-NIL value is treated as true by conditionals, such as codice_70, whereas in Scheme all non-#f values are treated as true. These conventions allow some operators in both languages to serve both as predicates (answering a Boolean-valued question) and as returning a useful value for further computation, but in Scheme the value '() which is equivalent to NIL in Common Lisp evaluates to true in a Boolean expression.\nLastly, the Scheme standards documents require tail-call optimization, which the CL standard does not. Most CL implementations do offer tail-call optimization, although often only when the programmer uses an optimization directive. Nonetheless, common CL coding style does not favor the ubiquitous use of recursion that Scheme style prefers\u2014what a Scheme programmer would express with tail recursion, a CL user would usually express with an iterative expression in codice_77, codice_101, codice_74, or (more recently) with the codice_103 package.\nImplementations.\nSee the Category .\nCommon Lisp is defined by a specification (like Ada and C) rather than by one implementation (like Perl). There are many implementations, and the standard details areas in which they may validly differ.\nIn addition, implementations tend to come with extensions, which provide functionality not covered in the standard:\nFree and open-source software libraries have been created to support extensions to Common Lisp in a portable way, and are most notably found in the repositories of the Common-Lisp.net and CLOCC (Common Lisp Open Code Collection) projects.\nCommon Lisp implementations may use any mix of native code compilation, byte code compilation or interpretation. Common Lisp has been designed to support incremental compilers, file compilers and block compilers. Standard declarations to optimize compilation (such as function inlining or type specialization) are proposed in the language specification. Most Common Lisp implementations compile source code to native machine code. Some implementations can create (optimized) stand-alone applications. Others compile to interpreted bytecode, which is less efficient than native code, but eases binary-code portability. Some compilers compile Common Lisp code to C code. The misconception that Lisp is a purely interpreted language is most likely because Lisp environments provide an interactive prompt and that code is compiled one-by-one, in an incremental way. With Common Lisp incremental compilation is widely used.\nSome Unix-based implementations (CLISP, SBCL) can be used as a scripting language; that is, invoked by the system transparently in the way that a Perl or Unix shell interpreter is.\nApplications.\nCommon Lisp is used to develop research applications (often in Artificial Intelligence), for rapid development of prototypes or for deployed applications.\nCommon Lisp is used in many commercial applications, including the Yahoo! Store web-commerce site, which originally involved Paul Graham and was later rewritten in C++ and Perl. Other notable examples include:\nThere also exist open-source applications written in Common Lisp, such as:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\nA chronological list of books published (or about to be published) about Common Lisp (the language) or about programming with Common Lisp (especially AI programming).\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "6069", "revid": "6046731", "url": "https://en.wikipedia.org/wiki?curid=6069", "title": "Color code", "text": "System for displaying information by using different colors\nA color code is a system for encoding and representing non-color information with colors to facilitate communication. This information tends to be categorical (representing unordered/qualitative categories) though may also be sequential (representing an ordered/quantitative variable).\nHistory.\nThe earliest examples of color codes in use are for long-distance communication by use of flags, as in semaphore communication. The United Kingdom adopted a color code scheme for such communication wherein red signified danger and white signified safety, with other colors having similar assignments of meaning.\nAs chemistry and other technologies advanced, it became expedient to use coloration as a signal for telling apart things that would otherwise be confusingly similar, such as wiring in electrical and electronic devices, and pharmaceutical pills.\nEncoded Variable.\nA color code encodes a variable, which may have different representations, where the color code type should match the variable type:\nTypes.\nThe types of color code are:\nCategorical.\nWhen color is the only varied attribute, the color code is \"unidimensional\". When other attributes are varied (e.g. shape, size), the code is \"multidimensional\", where the dimensions can be \"independent\" (each encoding separate variables) or \"redundant\" (encoding the same variable). Partial redundancy sees one variable as a subset of another. For example, playing card suits are multidimensional with color (black, red) and shape (club, diamond, heart, spade), which are partially redundant since clubs and spades are always black and diamonds and hearts are always red. Tasks using categorical color codes can be classified as identification tasks, where a single stimulus is shown and must be identified (connotatively or denotatively), versus search tasks, where a color stimulus must be found within a field of heterogenous stimuli. Performance in these tasks is measured by speed and/or accuracy.\nThe ideal color scheme for a categorical color code depends on whether speed or accuracy is more important. Despite humans being able to distinguish 150 distinct colors along the hue dimension during comparative task, evidence supports that color schemes where colors differ only by hue (equal luminosity and colorfulness) should have a maximum of eight categories with optimized stimulus spacing along the hue dimension, though this would not be color blind accessible. The IALA recommends categorical color codes in seven colors: red, orange, yellow, green, blue, white and black. Adding redundant coding of luminosity and colorfulness adds information and increases speed and accuracy of color decoding tasks. Color codes are superior to others (encoding to letters, shape, size, etc.) in certain types of tasks. Adding color as a redundant attribute to a numeral or letter encoding in search tasks decreased time by 50\u201375%,Fig9 but in unidimensional identification tasks, using alphanumeric or line inclination codes caused less errors than color codes.19\nSeveral studies demonstrate a subjective preference for color codes over achromatic codes (e.g. shapes), even in studies where color coding did not increase performance over achromatic coding.18 Subjects reported the tasks as less monotonous and less inducing of eye strain and fatigue.18\nThe ability to discriminate color differences decreases rapidly as the visual angle subtends less than 12' (0.2\u00b0 or ~2\u00a0mm at a viewing distance of 50\u00a0cm), so color stimulus of at least 3\u00a0mm in diameter or thickness is recommended when the color is on paper or on a screen. Under normal conditions, colored backgrounds do not affect the interpretation of color codes, but chromatic (and/or low) illumination of surface color code can degrade performance.\nCriticism.\nColor codes present some potential problems. On forms and signage, the use of color can distract from black and white text.\nColor codes are often designed without consideration for accessibility to color blind and blind people, and may even be inaccessible for those with normal color vision, since use of many colors to code many variables can lead to use of confusingly similar colors. Only 15\u201340% of the colorblind can correctly name surface color codes with 8\u201310 color categories, most of which test as mildly colorblind. This finding uses ideal illumination; when dimmer illumination is used, performance drops sharply.\nExamples.\nSystems incorporating color-coding include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6070", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=6070", "title": "Color/Orange", "text": ""}
{"id": "6071", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6071", "title": "Color/black", "text": ""}
{"id": "6074", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6074", "title": "Color/orange", "text": ""}
{"id": "6076", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6076", "title": "Color/cyan", "text": ""}
{"id": "6077", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6077", "title": "Color/Black", "text": ""}
{"id": "6078", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=6078", "title": "Color/white", "text": ""}
{"id": "6080", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6080", "title": "CGI", "text": "CGI may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "6082", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6082", "title": "Cortex", "text": "Cortex or cortical may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "6084", "revid": "50354445", "url": "https://en.wikipedia.org/wiki?curid=6084", "title": "Collection", "text": "&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nCollection or Collections may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "6085", "revid": "1307693073", "url": "https://en.wikipedia.org/wiki?curid=6085", "title": "Cauchy sequence", "text": "Sequence of points that get progressively closer to each other\nIn mathematics, a Cauchy sequence is a sequence whose elements become arbitrarily close to each other as the sequence progresses. More precisely, given any small positive distance, all excluding a finite number of elements of the sequence are less than that given distance from each other. Cauchy sequences are named after Augustin-Louis Cauchy; they may occasionally be known as fundamental sequences.\nIt is not sufficient for each term to become arbitrarily close to the preceding term. For instance, in the sequence of square roots of natural numbers:\nformula_1\nthe consecutive terms become arbitrarily close to each other \u2013 their differences\nformula_2\ntend to zero as the index n grows. However, with growing values of n, the terms formula_3 become arbitrarily large. So, for any index n and distance d, there exists an index m big enough such that formula_4 As a result, no matter how far one goes, the remaining terms of the sequence never get close to each other; hence the sequence is not Cauchy.\nThe utility of Cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination. \nGeneralizations of Cauchy sequences in more abstract uniform spaces exist in the form of Cauchy filters and Cauchy nets.\nIn real numbers.\nA sequence\nformula_5\nof real numbers is called a Cauchy sequence if for every positive real number formula_6 there is a positive integer \"N\" such that for all natural numbers formula_7\nformula_8\nwhere the vertical bars denote the absolute value. In a similar way one can define Cauchy sequences of rational or complex numbers. Cauchy formulated such a condition by requiring formula_9 to be infinitesimal for every pair of infinite \"m\", \"n\".\nFor any real number \"r\", the sequence of truncated decimal expansions of \"r\" forms a Cauchy sequence. For example, when formula_10 this sequence is (3, 3.1, 3.14, 3.141, ...). The \"m\"th and \"n\"th terms differ by at most formula_11 when \"m\" &lt; \"n\", and as \"m\" grows this becomes smaller than any fixed positive number formula_12\nModulus of Cauchy convergence.\nIf formula_13 is a sequence in the set formula_14 then a \"modulus of Cauchy convergence\" for the sequence is a function formula_15 from the set of natural numbers to itself, such that for all natural numbers formula_16 and natural numbers formula_17 formula_18\nAny sequence with a modulus of Cauchy convergence is a Cauchy sequence. The existence of a modulus for a Cauchy sequence follows from the well-ordering property of the natural numbers (let formula_19 be the smallest possible formula_20 in the definition of Cauchy sequence, taking formula_21 to be formula_22). The existence of a modulus also follows from the principle of countable choice. \"Regular Cauchy sequences\" are sequences with a given modulus of Cauchy convergence (usually formula_23 or formula_24). Any Cauchy sequence with a modulus of Cauchy convergence is equivalent to a regular Cauchy sequence; this can be proven without using any form of the axiom of choice.\nModuli of Cauchy convergence are used by constructive mathematicians who do not wish to use any form of choice. Using a modulus of Cauchy convergence can simplify both definitions and theorems in constructive analysis. Regular Cauchy sequences were used by and by in constructive mathematics textbooks.\nIn a metric space.\nSince the definition of a Cauchy sequence only involves metric concepts, it is straightforward to generalize it to any metric space \"X\". \nTo do so, the absolute difference formula_25 is replaced by the distance formula_26 (where \"d\" denotes a metric) between formula_27 and formula_28\nFormally, given a metric space formula_29 a sequence of elements of formula_30\nformula_5\nis Cauchy, if for every positive real number formula_32 there is a positive integer formula_20 such that for all positive integers formula_7 the distance\nformula_35\nRoughly speaking, the terms of the sequence are getting closer and closer together in a way that suggests that the sequence ought to have a limit in \"X\". \nNonetheless, such a limit does not always exist within \"X\": the property of a space that every Cauchy sequence converges in the space is called \"completeness\", and is detailed below.\nCompleteness.\nA metric space (\"X\", \"d\") in which every Cauchy sequence converges to an element of \"X\" is called complete. For any metric space \"M\", it is possible to construct a complete metric space \"M\u2032\" that contains \"M\" as a dense subspace; see .\nExamples of complete metric spaces.\nThe real numbers formula_36 are complete under the metric induced by the usual absolute value, and one of the standard constructions of the real numbers involves Cauchy sequences of rational numbers. In this construction, each equivalence class of Cauchy sequences of rational numbers with a certain tail behavior\u2014that is, each class of sequences that get arbitrarily close to one another\u2014 is a real number.\nA rather different type of example is afforded by a metric space \"X\" which has the discrete metric (where any two distinct points are at distance 1 from each other). Any Cauchy sequence of elements of \"X\" must be constant beyond some fixed point, and converges to the eventually repeating term.\nNon-example: rational numbers.\nThe rational numbers formula_37 are not complete (for the usual distance):\nThere are sequences of rationals that converge (in formula_36) to irrational numbers; these are Cauchy sequences having no limit in formula_39 In fact, if a real number \"x\" is irrational, then the sequence (\"x\"\"n\"), whose \"n\"-th term is the truncation to \"n\" decimal places of the decimal expansion of \"x\", gives a Cauchy sequence of rational numbers with irrational limit \"x\". Irrational numbers certainly exist in formula_40 for example:\nCompletion turns formula_37 into formula_48\nNon-example: open interval.\nThe open interval formula_49 in the set of real numbers with an ordinary distance in formula_36 is not a complete space: there is a sequence formula_51 in it, which is Cauchy (for arbitrarily small distance bound formula_52 all terms formula_53 of formula_54 fit in the formula_55 interval), however does not converge in formula_30\u2014its 'limit', number 0, does not belong to the space formula_57\nCompletion turns the open interval formula_49 into the closed interval formula_59\nOther properties.\nThese last two properties, together with the Bolzano\u2013Weierstrass theorem, yield one standard proof of the completeness of the real numbers, closely related to both the Bolzano\u2013Weierstrass theorem and the Heine\u2013Borel theorem. Every Cauchy sequence of real numbers is bounded, hence by Bolzano\u2013Weierstrass has a convergent subsequence, hence is itself convergent. This proof of the completeness of the real numbers implicitly makes use of the least upper bound axiom. The alternative approach, mentioned above, of constructing the real numbers as the completion of the rational numbers, makes the completeness of the real numbers tautological.\nOne of the standard illustrations of the advantage of being able to work with Cauchy sequences and make use of completeness is provided by consideration of the summation of an infinite series of real numbers\n(or, more generally, of elements of any complete normed linear space, or Banach space). Such a series \nformula_67 is considered to be convergent if and only if the sequence of partial sums formula_68 is convergent, where formula_69 It is a routine matter to determine whether the sequence of partial sums is Cauchy or not, since for positive integers formula_70\nformula_71\nIf formula_72 is a uniformly continuous map between the metric spaces \"M\" and \"N\" and (\"x\"\"n\") is a Cauchy sequence in \"M\", then formula_73 is a Cauchy sequence in \"N\". If formula_74 and formula_75 are two Cauchy sequences in the rational, real or complex numbers, then the sum formula_76 and the product formula_77 are also Cauchy sequences.\nGeneralizations.\nIn topological vector spaces.\nThere is also a concept of Cauchy sequence for a topological vector space formula_30: Pick a local base formula_79 for formula_30 about 0; then (formula_81) is a Cauchy sequence if for each member formula_82 there is some number formula_20 such that whenever \nformula_84 is an element of formula_85 If the topology of formula_30 is compatible with a translation-invariant metric formula_87 the two definitions agree.\nIn topological groups.\nSince the topological vector space definition of Cauchy sequence requires only that there be a continuous \"subtraction\" operation, it can just as well be stated in the context of a topological group: A sequence formula_88 in a topological group formula_89 is a Cauchy sequence if for every open neighbourhood formula_90 of the identity in formula_89 there exists some number formula_20 such that whenever formula_93 it follows that formula_94 As above, it is sufficient to check this for the neighbourhoods in any local base of the identity in formula_95\nAs in the construction of the completion of a metric space, one can furthermore define the binary relation on Cauchy sequences in formula_89 that formula_88 and formula_98 are equivalent if for every open neighbourhood formula_90 of the identity in formula_89 there exists some number formula_20 such that whenever formula_93 it follows that formula_103 This relation is an equivalence relation: It is reflexive since the sequences are Cauchy sequences. It is symmetric since formula_104 which by continuity of the inverse is another open neighbourhood of the identity. It is transitive since formula_105 where formula_106 and formula_107 are open neighbourhoods of the identity such that formula_108; such pairs exist by the continuity of the group operation.\nIn groups.\nThere is also a concept of Cauchy sequence in a group formula_89:\nLet formula_110 be a decreasing sequence of normal subgroups of formula_89 of finite index.\nThen a sequence formula_74 in formula_89 is said to be Cauchy (with respect to formula_114) if and only if for any formula_115 there is formula_20 such that for all formula_117\nTechnically, this is the same thing as a topological group Cauchy sequence for a particular choice of topology on formula_118 namely that for which formula_114 is a local base.\nThe set formula_120 of such Cauchy sequences forms a group (for the componentwise product), and the set formula_121 of null sequences (sequences such that formula_122) is a normal subgroup of formula_123 The factor group formula_124 is called the completion of formula_89 with respect to formula_126\nOne can then show that this completion is isomorphic to the inverse limit of the sequence formula_127\nAn example of this construction familiar in number theory and algebraic geometry is the construction of the formula_128-adic completion of the integers with respect to a prime formula_129 In this case, formula_89 is the integers under addition, and formula_131 is the additive subgroup consisting of integer multiples of formula_132 \nIf formula_114 is a cofinal sequence (that is, any normal subgroup of finite index contains some formula_131), then this completion is canonical in the sense that it is isomorphic to the inverse limit of formula_135 where formula_114 varies over all normal subgroups of finite index. For further details, see Ch. I.10 in Lang's \"Algebra\".\nIn a hyperreal continuum.\nA real sequence formula_137 has a natural hyperreal extension, defined for hypernatural values \"H\" of the index \"n\" in addition to the usual natural \"n\". The sequence is Cauchy if and only if for every infinite \"H\" and \"K\", the values formula_138 and formula_139 are infinitely close, or adequal, that is,\nformula_140 \nwhere \"st\" is the standard part function.\nCauchy completion of categories.\n introduced a notion of Cauchy completion of a category. Applied to formula_37 (the category whose objects are rational numbers, and there is a morphism from \"x\" to \"y\" if and only if formula_142), this Cauchy completion yields formula_143 (again interpreted as a category using its natural ordering).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6086", "revid": "1151336260", "url": "https://en.wikipedia.org/wiki?curid=6086", "title": "Cauchy sequences", "text": ""}
{"id": "6087", "revid": "38627444", "url": "https://en.wikipedia.org/wiki?curid=6087", "title": "Copernicus", "text": ""}
{"id": "6088", "revid": "73920", "url": "https://en.wikipedia.org/wiki?curid=6088", "title": "Common Era", "text": "Modern calendar era\nCommon Era (CE) and Before the Common Era (BCE) are year notations for the Gregorian or Julian calendar, and are exactly equivalent to the Anno Domini (AD) and Before Christ (BC) notations. The expressions \"2025 CE\" and \"AD 2025\" each equally describe the current year; \"400 BCE\" and \"400 BC\" are the same year too. BCE/CE are primarily used to avoid religious connotations, by not referring to Jesus as [Lord].\nHistory.\nAnno Domini.\nAround the year 525, the Christian monk Dionysius Exiguus devised the principle of taking the moment that he believed to be the date of the incarnation of Jesus to be the point from which years are numbered (the epoch) of the Christian ecclesiastical calendar. Dionysius labeled the column of the table in which he introduced the new era as \"\" (the years of our Lord Jesus Christ). He did this to replace the Era of the Martyrs system (then used for some Easter tables) because he did not wish to continue the memory of a tyrant who persecuted Christians.\nThis way of numbering years became more widespread in Europe, with its use by Bede in England in 731. Bede also introduced the practice of dating years before 1 backwards, without a year zero though the name \"BC\" for this era did not happen until much later.\nVulgar Era.\nThe first use of the Latin term may be in a 1615 book by Johannes Kepler. to distinguish the Anno Domini era, which was in popular use, from dates of the regnal year (the year of the reign of a sovereign) typically used in national law. The word 'vulgar' originally meant 'of the ordinary people', with no derogatory associations. Kepler uses it again, as , in a 1616 table of ephemerides, and again, as , in 1617. An English edition of that book from 1635 may contain the earliest known use of \"Vulgar Era\" in its title page. A 1701 book edited by John Le Clerc includes the phrase \"Before Christ according to the Vulgar \u00c6ra,6\".\nThe Merriam-Webster Dictionary gives 1716 as the date of first use of the term \"vulgar era\" (which it defines as \"Christian era\").\nChristian Era.\nThe first published use of \"Christian Era\" may be the Latin phrase on the title page of a 1584 theology book, . In 1649, the Latin phrase appeared in the title of an English almanac. A 1652 ephemeris may be the first instance of the English use of \"Christian Era\". It is possible this influenced the wording choice of \"Common Era\" to have the same abbreviation.\nCommon Era.\nThe English phrase \"Common Era\" appears at least as early as 1708, and in a 1715 book on astronomy, it is used interchangeably with \"Christian Era\" and \"Vulgar Era\". A 1759 history book uses \"common \u00e6ra\" in a generic sense to refer to \"the common era of the Jews\". The phrase \"before the common era\" may have first appeared in a 1770 work that also uses \"common era\" and \"vulgar era\" as synonyms in a translation of a book originally written in German. The 1797 edition of the Encyclop\u00e6dia Britannica uses the terms \"vulgar era\" and \"common era\" synonymously. \nIn 1835, in his book \"Living Oracles\", Alexander Campbell wrote: \"The vulgar Era, or Anno Domini; the fourth year of Jesus Christ, the first of which was but eight days\". He refers to the \"common era\" as a synonym for \"vulgar era\": \"the fact that our Lord was born on the 4th year before the vulgar era, called Anno Domini, thus making (for example) the 42d year from his birth to correspond with the 38th of the common era\". The \"Catholic Encyclopedia\" (1909), in at least one article, reports all three terms (Christian, Vulgar, Common Era) being commonly understood by the early 20th century.\nThe phrase \"common era\", in lower case, also appeared in the 19th century in a \"generic\" sense, not necessarily to refer to the Christian Era, but to any system of dates in everyday use throughout a civilization. Thus, \"the common era of the Jews\", \"the common era of the Mahometans\", \"common era of the world\", or \"the common era of the foundation of Rome\". When it did refer to the Christian Era, it was sometimes qualified (e.g., \"common era of the Incarnation\", \"common era of the Nativity\", or \"common era of the birth of Christ\").\nAn adapted translation of \"Common Era\" into Latin as was adopted in the 20th century by some followers of Aleister Crowley, and thus the abbreviation \"e.v.\" or \"EV\" may sometimes be seen as a replacement for AD.\nJudaism.\nAlthough Jews have the Hebrew calendar, they often use the Gregorian calendar without the AD prefix, as Judaism does not recognize Jesus as the Messiah. As early as 1825, the abbreviation VE (for Vulgar Era) was in use among Jews to denote years in the Western calendar. \"Common Era\" has been in use for Hebrew lessons since before 1905. Jews have also used the term \"Current Era\".\nContemporary usage.\nSome academics in the fields of theology, education, archaeology and history have adopted CE and BCE notation despite some disagreement. A study conducted in 2014 found that the BCE/CE notation is not growing at the expense of BC and AD notation in the scholarly literature, and that both notations are used in a relatively stable fashion. \nAustralia.\nIn 2011, media reports suggested that the BC/AD notation in Australian school textbooks would be replaced by BCE/CE notation. The change drew opposition from some politicians and church leaders. Weeks after the story broke, the Australian Curriculum, Assessment and Reporting Authority denied the rumours and stated that the BC/AD notation would remain, with CE and BCE as an optional suggested learning activity.\nCanada.\nIn 2013, the Canadian Museum of Civilization (now the Canadian Museum of History) in Gatineau (opposite Ottawa), which had previously switched to BCE/CE, decided to change back to BC/AD in material intended for the public while retaining BCE/CE in academic content.\nNepal.\nThe notation is in particularly common use in Nepal in order to disambiguate dates from the local (Indian or Hindu) calendar, Bikram or Vikram Sambat. Disambiguation is needed because the era of the Hindu calendar is quite close to the Common Era.\nUnited Kingdom.\nIn 2002, an advisory panel for the religious education syllabus for England and Wales recommended introducing BCE/CE dates to schools, and by 2018 some local education authorities were using them. \nIn 2018, the National Trust said it would continue to use BC/AD as its house style. English Heritage explains its era policy thus: \"It might seem strange to use a Christian calendar system when referring to British prehistory, but the BC/AD labels are widely used and understood.\" Some parts of the BBC use BCE/CE, but some presenters have said they will not. As of October 2019, the BBC News style guide has entries for AD and BC, but not for CE or BCE. The style guide for \"The Guardian\" says, under the entry for CE/BCE: \"some people prefer CE (common era, current era, or Christian era) and BCE (before common era, etc.) to AD and BC, which, however, remain our style\".\nUnited States.\nIn the United States, the use of the BCE/CE notation in textbooks was reported in 2005 to be growing. Some publications have transitioned to using it exclusively. For example, the 2007 World Almanac was the first edition to switch to BCE/CE, ending a period of 138 years in which the traditional BC/AD dating notation was used. BCE/CE is used by the College Board in its history tests, and by the Norton Anthology of English Literature. Others have taken a different approach. The US-based History Channel uses BCE/CE notation in articles on non-Christian religious topics such as Jerusalem and Judaism. The 2006 style guide for the Episcopal Diocese \"Maryland Church News\" says that BCE and CE should be used. The US-based Society of Biblical Literature style guide for academic texts on religion prefers BCE/CE to BC/AD.\nIn June 2006, in the United States, the Kentucky State School Board reversed its decision to use BCE and CE in the state's new Program of Studies, leaving education of students about these concepts a matter of local discretion.\nRationales.\nSupport.\nThe use of CE in Jewish scholarship was historically motivated by the desire to avoid the implicit \"Our Lord\" in the abbreviation \"AD\". Although other aspects of dating systems are based in Christian origins, AD is a direct reference to Jesus as Lord. Proponents of the Common Era notation assert that the use of BCE/CE shows sensitivity to those who use the same year numbering system as the one that originated with and is currently used by Christians, but who are not themselves Christian. Former United Nations Secretary-General Kofi Annan has argued:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[T]he Christian calendar no longer belongs exclusively to Christians. People of all faiths have taken to using it simply as a matter of convenience. There is so much interaction between people of different faiths and cultures \u2013 different civilizations, if you like \u2013 that some shared way of reckoning time is a necessity. And so the Christian Era has become the Common Era.\nAdena K. Berkowitz, in her application to argue before the United States Supreme Court, opted to use BCE and CE because, \"Given the multicultural society that we live in, the traditional Jewish designations\u00a0\u2013 B.C.E. and C.E.\u00a0\u2013 cast a wider net of inclusion.\" In the World History Encyclopedia, Joshua J. Mark wrote \"Non-Christian scholars, especially, embraced [CE and BCE] because they could now communicate more easily with the Christian community. Jewish, Islamic, Hindu and Buddhist scholars could retain their [own] calendar but refer to events using the Gregorian Calendar as BCE and CE without compromising their own beliefs about the divinity of Jesus of Nazareth.\" In \"History Today\", Michael Ostling wrote: \"BC/AD Dating: In the year of whose Lord? The continuing use of AD and BC is not only factually wrong but also offensive to many who are not Christians.\"\nOpposition.\nCritics note the fact that there is no difference in the epoch of the two systems\u2014chosen to be close to the date of birth of Jesus. Since the year numbers are the same, BCE and CE dates should be equally offensive to other religions as BC and AD. Roman Catholic priest and writer on interfaith issues Raimon Panikkar argued that the BCE/CE usage is the less inclusive option since they are still using the Christian calendar numbers and forcing it on other nations. In 1993, the English-language expert Kenneth G. Wilson speculated a slippery slope scenario in his style guide that, \"if we do end by casting aside the AD/BC convention, almost certainly some will argue that we ought to cast aside as well the conventional numbering system [that is, the method of numbering years] itself, given its Christian basis.\"\nSome Christians are offended by the removal of the reference to Jesus.\nConventions in style guides.\nThe abbreviation BCE, just as with BC, always follows the year number. Unlike AD, which still often precedes the year number, CE always follows the year number (if context requires that it be written at all). Thus, the current year is written as 2025 in both notations (or, if further clarity is needed, as 2025 CE, or as AD 2025), and the year that Socrates died is represented as 399 BCE (the same year that is represented by 399 BC in the BC/AD notation). The abbreviations are sometimes written with small capital letters, or with periods (e.g., \"B.C.E.\" or \"C.E.\").\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6089", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6089", "title": "Creationist", "text": ""}
{"id": "6091", "revid": "10503050", "url": "https://en.wikipedia.org/wiki?curid=6091", "title": "Charles Robert Malden", "text": "British naval officer, surveyor and educator\nCharles Robert Malden (9 August 1797 \u2013 23 May 1855) was a nineteenth-century British naval officer, surveyor and educator. He is the discoverer of Malden Island in the central Pacific, which is named in his honour. He also founded Windlesham House School at Brighton, England.\nBiography.\nMalden was born in Putney, Surrey, son of Jonas Malden, a surgeon. He entered British naval service at the age of 11 on 22 June 1809. He served nine years as a volunteer 1st class, midshipman, and shipmate, including one year in the English Channel and Bay of Biscay (1809), four years at the Cape of Good Hope and in the East Indies (1809\u201314), two and a half years on the North American and West Indian stations (1814\u201316), and a year and a half in the Mediterranean (1817\u201318). He was present at the capture of Mauritius and Java, and at the battles of Baltimore and New Orleans.\nHe passed the examination in the elements of mathematics and the theory of navigation at the Royal Naval Academy on 2\u20134 September 1816, and became a 1st Lieutenant on 1 September 1818. In eight years of active service as an officer, he served two and a half years in a surveying ship in the Mediterranean (1818\u201321), one and a half years in a surveying sloop in the English Channel and off the coast of Ireland (1823\u201324), and one and a half years as Surveyor of the frigate during a voyage (1824\u201326) to and from the Hawaiian Islands (then known as the \"Sandwich islands\").\nIn Hawaii he surveyed harbours which, he noted, were \"said not to exist by Captains Cook and Vancouver.\" On the return voyage he discovered and explored uninhabited Malden Island in the central Pacific on 30 July 1825. After his return he left active service but remained at half pay. He served for several years as hydrographer to King William IV.\nHe married Frances Cole, daughter of Rev. William Hodgson Cole, rector of West Clandon and Vicar of Wonersh, near Guildford, Surrey, on 8 April 1828. Malden became the father of seven sons and a daughter.\nFrom 1830 to 1836 he took pupils for the Royal Navy at Ryde, Isle of Wight. He purchased the school of Henry Worsley at Newport, Isle of Wight, in December 1836, reopened it as a preparatory school on 20 February 1837, and moved it to Montpelier Road in Brighton in December 1837. He built the Windlesham House School at Brighton in 1844, and conducted the school until his death there in 1855. He was succeeded as headmaster by his son Henry Charles Malden.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "6094", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=6094", "title": "CPD", "text": "CPD may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
