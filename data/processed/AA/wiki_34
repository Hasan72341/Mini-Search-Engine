{"id": "4179", "revid": "49350947", "url": "https://en.wikipedia.org/wiki?curid=4179", "title": "British English", "text": "Set of varieties of English language\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nBritish English is the set of varieties of the English language native to the United Kingdom, especially Great Britain. More narrowly, it can refer specifically to the English language in England, or, more broadly, to the collective dialects of English throughout the United Kingdom taken as a single umbrella variety, for instance additionally incorporating Scottish English, Welsh English, and Northern Irish English. Tom McArthur in the Oxford Guide to World English acknowledges that British English shares \"all the ambiguities and tensions [with] the word 'British' and as a result can be used and interpreted in two ways, more broadly or more narrowly, within a range of blurring and ambiguity\".\nVariations exist in formal (both written and spoken) English in the United Kingdom. For example, the adjective \"wee\" is almost exclusively used in parts of Scotland, north-east England, Northern Ireland, Ireland, and occasionally Yorkshire, whereas the adjective \"little\" is predominant elsewhere. Nevertheless, there is a meaningful degree of uniformity in written English within the United Kingdom, and this could be described by the term \"British English\". The forms of spoken English, however, vary considerably more than in most other areas of the world where English is spoken and so a uniform concept of British English is more difficult to apply to the spoken language. \nGlobally, countries that are former British colonies or members of the Commonwealth tend to follow British English, as is the case for English used by European Union institutions. The United Nations also uses British English with Oxford spelling. In China, both British English and American English are taught. The UK government actively teaches and promotes English around the world and operates in over 100 countries.\nHistory.\nOrigins.\nEnglish is a West Germanic language that originated from the Anglo-Frisian dialects brought to Britain by Germanic settlers from various parts of what is now northwest Germany and the northern Netherlands. The resident population at this time was generally speaking Common Brittonic\u2014the insular variety of Continental Celtic, which was influenced by the Roman occupation. This group of languages (Welsh, Cornish, Cumbric) cohabited alongside English into the modern period, but due to their remoteness from the Germanic languages, influence on English was notably limited. However, the degree of influence remains debated, and it has recently been argued that its grammatical influence accounts for the substantial innovations noted between English and the other West Germanic languages.\nInitially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon kingdoms of England. One of these dialects, Late West Saxon, eventually came to dominate. The original Old English was then influenced by two waves of invasion: the first was by speakers of the Scandinavian branch of the Germanic family, who settled in parts of Britain in the eighth and ninth centuries; the second was the Normans in the 11th century, who spoke Old Norman and ultimately developed an English variety of this called Anglo-Norman. These two invasions caused English to become \"mixed\" to some degree (though it was never a truly mixed language in the strictest sense of the word; mixed languages arise from the cohabitation of speakers of different languages, who develop a hybrid tongue for basic communication).\nThe more idiomatic, concrete and descriptive English is, the more it is from Anglo-Saxon origins. The more intellectual and abstract English is, the more it contains Latin and French influences, e.g. swine (like the Germanic ) is the animal in the field bred by the occupied Anglo-Saxons and pork (like the French ) is the animal at the table eaten by the occupying Normans. Another example is the Anglo-Saxon meaning cow, and the French meaning beef.\nCohabitation with the Scandinavians resulted in a significant grammatical simplification and lexical enrichment of the Anglo-Frisian core of English; the later Norman occupation led to the grafting onto that Germanic core of a more elaborate layer of words from the Romance branch of the European languages. This Norman influence entered English largely through the courts and government. Thus, English developed into a \"borrowing\" language of great flexibility and with a huge vocabulary.\nDialects.\nDialects and accents vary amongst the four countries of the United Kingdom, as well as within the countries themselves.\nThe major divisions are normally classified as English English (or English as spoken in England (which is itself broadly grouped into Southern English, West Country, East and West Midlands English and Northern English), Northern Irish English (in Northern Ireland), Welsh English (not to be confused with the Welsh language), and Scottish English (not to be confused with the Scots language or Scottish Gaelic). Each group includes a range of dialects, some markedly different from others. The various British dialects also differ in the words that they have borrowed from other languages.\nAround the middle of the 15th century, there were points where within the 5 major dialects there were almost 500 ways to spell the word \"though\".\nResearch.\nFollowing its last major survey of English Dialects (1949\u20131950), the University of Leeds has started work on a new project. In May 2007 the Arts and Humanities Research Council awarded a grant to Leeds to study British regional dialects.\nThe team are sifting through a large collection of examples of regional slang words and phrases turned up by the \"Voices project\" run by the BBC, in which they invited the public to send in examples of English still spoken throughout the country. The BBC Voices project also collected hundreds of news articles about how the British speak English from swearing through to items on language schools. This information will also be collated and analysed by Johnson's team both for content and for where it was reported. \"Perhaps the most remarkable finding in the Voices study is that the English language is as diverse as ever, despite our increased mobility and constant exposure to other accents and dialects through TV and radio\". When discussing the award of the grant in 2007, Leeds University stated:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nEnglish regional.\nMost people in Britain speak with a regional accent or dialect. However, about 2% of Britons speak with an accent called Received Pronunciation (also called \"the King's English\", \"Oxford English\" and \"BBC English\"), that is essentially region-less. It derives from a mixture of the Midlands and Southern dialects spoken in London in the early modern period. It is frequently used as a model for teaching English to foreign learners.\nIn the South East, there are significantly different accents; the Cockney accent spoken by some East Londoners is strikingly different from Received Pronunciation (RP). Cockney rhyming slang can be (and was initially intended to be) difficult for outsiders to understand, although the extent of its use is often somewhat exaggerated.\nLondoners speak with a mixture of accents, depending on ethnicity, neighbourhood, class, age, upbringing, and sundry other factors. Estuary English has been gaining prominence in recent decades: it has some features of RP and some of Cockney. Immigrants to the UK in recent decades have brought many more languages to the country and particularly to London. Surveys started in 1979 by the Inner London Education Authority discovered over 125 languages being spoken domestically by the families of the inner city's schoolchildren. Notably Multicultural London English, a sociolect that emerged in the late 20th century spoken mainly by young, working-class people in multicultural parts of London.\nSince the mass internal migration to Northamptonshire in the 1940s and given its position between several major accent regions, it has become a source of various accent developments. In Northampton the older accent has been influenced by overspill Londoners. There is an accent known locally as the Kettering accent, which is a transitional accent between the East Midlands and East Anglian. It is the last southern Midlands accent to use the broad \"a\" in words like \"bath\" or \"grass\" (i.e. or ). Conversely \"crass\" or \"plastic\" use a slender \"a\". A few miles northwest in Leicestershire the slender \"a\" becomes more widespread generally. In the town of Corby, north, one can find Corbyite which, unlike the Kettering accent, is largely influenced by the West Scottish accent.\nFeatures.\nPhonological features characteristic of British English revolve around the pronunciation of the letter R, as well as the dental plosive T and some diphthongs specific to this dialect.\nT-glottalling.\nOnce regarded as a Cockney feature, in a number of forms of spoken British English, has become commonly realised as a glottal stop when it is in the intervocalic position, in a process called T-glottalisation. National media, being based in London, have seen the glottal stop spreading more widely than it once was in word endings, \"not\" being heard as \"no\" and \"bottle of water\" being heard as \"bole of waer\". It is still stigmatised when used in word-medial positions, such as \"later\". Other consonants subject to this usage in Cockney English are \"p\", as in paer and \"k\" as in baer.\nR-dropping.\nIn most areas of England and Wales, outside the West Country and other near-by counties of the UK, the consonant R is not pronounced if not followed by a vowel, lengthening the preceding vowel instead. This phenomenon is known as non-rhoticity.\nIn these same areas, a tendency exists to insert an R between a word ending in a vowel and a next word beginning with a vowel. This is called the intrusive R. It could be understood as a merger, in that words that once ended in an R and words that did not are no longer treated differently. This is also due to London-centric influences. Examples of R-dropping are \"car\" and \"sugar\", where the R is not pronounced.\nDiphthongisation.\nBritish dialects differ on the extent of diphthongisation of long vowels, with southern varieties extensively turning them into diphthongs, and with northern dialects normally preserving many of them. As a comparison, North American varieties could be said to be in-between.\nNorth.\nLong vowels /i\u02d0/ and /u\u02d0/ are usually preserved, and in several areas also /o\u02d0/ and /e\u02d0/, as in go and say (unlike other varieties of English, that change them to [o\u028a] and [e\u026a] respectively). Some areas go as far as not diphthongising medieval /i\u02d0/ and /u\u02d0/, that give rise to modern /a\u026a/ and /a\u028a/; that is, for example, in the traditional accent of Newcastle upon Tyne, 'out' will sound as 'oot', and in parts of Scotland and North-West England, 'my' will be pronounced as 'me'.\nSouth.\nLong vowels /i\u02d0/ and /u\u02d0/ are diphthongised to [\u026ai] and [\u028au] respectively (or, more technically, [\u028f\u0289], with a raised tongue), so that ee and oo in feed and food are pronounced with a movement. The diphthong [o\u028a] is also pronounced with a greater movement, normally [\u0259\u028a], [\u0259\u0289] or [\u0259\u0268].\nPeople in groups.\nDropping a morphological grammatical number, in collective nouns, is stronger in British English than North American English. This is to treat them as plural when once grammatically singular, a perceived natural number prevails, especially when applying to institutional nouns and groups of people.\nThe noun 'police', for example, undergoes this treatment:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Police are investigating the theft of work tools worth \u00a3500 from a van at the Sprucefield park and ride car park in Lisburn.\nA sports team can be treated likewise:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Arsenal have lost just one of 20 home Premier League matches against Manchester City.\nThis tendency can be observed in texts produced already in the 19th century. For example, Jane Austen, a British author, writes in Chapter 4 of \"Pride and Prejudice\", published in 1813:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;All the world are good and agreeable in your eyes. However, in Chapter 16, the grammatical number is used. &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The world is blinded by his fortune and consequence.\nNegatives.\nSome dialects of British English use negative concords, also known as double negatives. Rather than changing a word or using a positive, words like \"nobody\", \"not\", \"nothing\", and \"never\" would be used in the same sentence. While this does not occur in Standard English, it does occur in non-standard dialects. The double negation follows the idea of two different morphemes, one that causes the double negation, and one that is used for the point or the verb.\nStandard British English.\nStandard English in the United Kingdom, as in other English-speaking nations, is widely enforced in schools and by social norms for formal contexts but not by any singular authority; for instance, there is no institution equivalent to the with French or the Royal Spanish Academy with Spanish. Standard British English differs notably in certain vocabulary, grammar, and pronunciation features from standard American English and certain other standard English varieties around the world. British and American spelling also differ in minor ways.\nThe accent, or pronunciation system, of standard British English, based in southeastern England, has been known for over a century as Received Pronunciation (RP). However, due to language evolution and changing social trends, some linguists argue that RP is losing prestige or has been replaced by another accent, one that the linguist Geoff Lindsey for instance calls Standard Southern British English. Other scholars suggest that more regionally-oriented standard accents are emerging in England. Outside of England, namely in Scotland and Northern Ireland, RP exerts very little influence, particularly in the 21st century. RP, while long established as the standard English accent around the globe due to the spread of the British Empire, is distinct from the standard English pronunciation in some parts of the world; most prominently, RP notably contrasts with standard North American accents.\nAs of the 21st century, dictionaries such as the \"Oxford English Dictionary\", the \"Longman Dictionary of Contemporary English\", the \"Chambers Dictionary\", and the \"Collins Dictionary\" record actual usage rather than attempting to prescribe it. In addition, vocabulary and usage change with time; words are freely borrowed from other languages and other varieties of English, and neologisms are frequent.\nHistory of standardisation.\nFor historical reasons dating back to the rise of London in the ninth century, the form of language spoken in London and the East Midlands became standard English within the Court, and ultimately became the basis for generally accepted use in the law, government, literature and education in Britain. The standardisation of British English is thought to be from both dialect levelling and a thought of social superiority. Speaking in the Standard dialect created class distinctions; those who did not speak the standard English would be considered of a lesser class or social status and often discounted or considered of a low intelligence. Another contribution to the standardisation of British English was the introduction of the printing press to England in the mid-15th century. In doing so, William Caxton enabled a common language and spelling to be dispersed among the entirety of England at a much faster rate.\n\"Samuel Johnson's A Dictionary of the English Language\" (1755) was a large step in the English-language spelling reform, where the purification of language focused on standardising both speech and spelling. By the early 20th century, British authors had produced numerous books intended as guides to English grammar and usage, a few of which achieved sufficient acclaim to have remained in print for long periods and to have been reissued in new editions after some decades. These include, most notably of all, Fowler's \"Modern English Usage\" and \"The Complete Plain Words\" by Sir Ernest Gowers.\nDetailed guidance on many aspects of writing British English for publication is included in style guides issued by various publishers including \"The Times\" newspaper, the \"Oxford University Press\" and the \"Cambridge University Press\". \"The Oxford University Press\" guidelines were originally drafted as a single broadsheet page by Horace Henry Hart, and were at the time (1893) the first guide of their type in English; they were gradually expanded and eventually published, first as \"Hart's Rules\", and in 2002 as part of \"The Oxford Manual of Style\". Comparable in authority and stature to \"The Chicago Manual of Style\" for published American English, the Oxford Manual is a fairly exhaustive standard for published British English that writers can turn to in the absence of specific guidance from their publishing house.\nRelationship with Commonwealth English.\nBritish English is the basis of, and very similar to, Commonwealth English. Commonwealth English is English as spoken and written in the Commonwealth countries, though often with some local variation. This includes English spoken in Australia, Malta, New Zealand, Nigeria, and South Africa. It also includes South Asian English used in South Asia, in English varieties in Southeast Asia, and in parts of Africa. Canadian English is based on British English, but has more influence from American English, often grouped together due to their close proximity. British English, for example, is the closest English to Indian English, but Indian English has extra vocabulary and some English words are assigned different meanings.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4181", "revid": "42664955", "url": "https://en.wikipedia.org/wiki?curid=4181", "title": "Battle", "text": "Military engagement\nA battle is an occurrence of combat in warfare between opposing military units of any number or size. A war usually consists of multiple battles. In general, a battle is a series of military engagements that is well defined in duration, area, and force commitment. An engagement with only limited commitment between the forces and without decisive results is sometimes called a skirmish.\nThe word \"battle\" can also be used infrequently to refer to an entire operational campaign, although this usage greatly diverges from its conventional or customary meaning. Generally, the word \"battle\" is used for such campaigns if referring to a protracted combat encounter in which either one or both of the combatants had the same methods, resources, and strategic objectives throughout the encounter. Some prominent examples of this would be the Battle of the Atlantic, Battle of Britain, and the Battle of France, all in World War II.\nWars and military campaigns are guided by military strategy, whereas battles take place on a level of planning and execution known as operational mobility. German strategist Carl von Clausewitz stated that \"the employment of battles ... to achieve the object of war\" was the essence of strategy.\nEtymology.\nBattle is a loanword from the Old French , first attested in 1297, from Late Latin , meaning \"exercise of soldiers and gladiators in fighting and fencing\", from Late Latin (taken from Germanic) \"beat\", from which the English word battery is also derived via Middle English .\nCharacteristics.\nThe defining characteristic of the fight as a concept in military science has changed with the variations in the organisation, employment and technology of military forces. The English military historian John Keegan suggested an ideal definition of battle as \"something which happens between two armies leading to the moral then physical disintegration of one or the other of them\" but the origins and outcomes of battles can rarely be summarized so neatly. Battle in the 20th and 21st centuries is defined as the combat between large components of the forces in a military campaign, used to achieve military objectives. Where the duration of the battle is longer than a week, it is often for reasons of planning called an operation. Battles can be planned, encountered or forced by one side when the other is unable to withdraw from combat.\nA battle always has as its purpose the reaching of a mission goal by use of military force. A victory in the battle is achieved when one of the opposing sides forces the other to abandon its mission and surrender its forces, routs the other (i.e., forces it to retreat or renders it militarily ineffective for further combat operations) or annihilates the latter, resulting in their deaths or capture. A battle may end in a Pyrrhic victory, which ultimately favors the defeated party. If no resolution is reached in a battle, it can result in a stalemate. A conflict in which one side is unwilling to reach a decision by a direct battle using conventional warfare often becomes an insurgency.\nUntil the 19th century the majority of battles were of short duration, many lasting a part of a day. (The Battle of Preston (1648), the Battle of Nations (1813) and the Battle of Gettysburg (1863) were exceptional in lasting three days.) This was mainly due to the difficulty of supplying armies in the field or conducting night operations. The means of prolonging a battle was typically with siege warfare. Improvements in transport and the sudden evolving of trench warfare, with its siege-like nature during the First World War in the 20th century, lengthened the duration of battles to days and weeks. This created the requirement for unit rotation to prevent combat fatigue, with troops preferably not remaining in a combat area of operations for more than a month.\nThe use of the term \"battle\" in military history has led to its misuse when referring to almost any scale of combat, notably by strategic forces involving hundreds of thousands of troops that may be engaged in either one battle at a time (Battle of Leipzig) or operations (Battle of Wuhan). The space a battle occupies depends on the range of the weapons of the combatants. A \"battle\" in this broader sense may be of long duration and take place over a large area, as in the case of the Battle of Britain or the Battle of the Atlantic. Until the advent of artillery and aircraft, battles were fought with the two sides within sight, if not reach, of each other. The depth of the battlefield has also increased in modern warfare with inclusion of the supporting units in the rear areas; supply, artillery, medical personnel etc. often outnumber the front-line combat troops.\nBattles are made up of a multitude of individual combats, skirmishes and small engagements and the combatants will usually only experience a small part of the battle. To the infantryman, there may be little to distinguish between combat as part of a minor raid or a big offensive, nor is it likely that he anticipates the future course of the battle; few of the British infantry who went over the top on the first day on the Somme, 1 July 1916, would have anticipated that the battle would last five months. Some of the Allied infantry who had just dealt a crushing defeat to the French at the Battle of Waterloo fully expected to have to fight again the next day (at the Battle of Wavre).\nBattlespace.\nBattlespace is a unified strategic concept to integrate and combine armed forces for the military theatre of operations, including air, information, land, sea and space. It includes the environment, factors and conditions that must be understood to apply combat power, protect the force or complete the mission, comprising enemy and friendly armed forces; facilities; weather; terrain; and the electromagnetic spectrum.\nFactors.\nBattles are decided by various factors, the number and quality of combatants and equipment, the skill of commanders and terrain are among the most prominent. Weapons and armour can be decisive; on many occasions armies have achieved victory through more advanced weapons than those of their opponents. An extreme example was in the Battle of Omdurman, in which a large army of Sudanese Mahdists armed in a traditional manner were destroyed by an Anglo-Egyptian force equipped with Maxim machine guns and artillery.\nOn some occasions, simple weapons employed in an unorthodox fashion have proven advantageous; Swiss pikemen gained many victories through their ability to transform a traditionally defensive weapon into an offensive one. Zulus in the early 19th century were victorious in battles against their rivals in part because they adopted a new kind of spear, the iklwa. Forces with inferior weapons have still emerged victorious at times, for example in the Wars of Scottish Independence. Disciplined troops are often of greater importance; at the Battle of Alesia, the Romans were greatly outnumbered but won because of superior training.\nBattles can also be determined by terrain. Capturing high ground has been the main tactic in innumerable battles. An army that holds the high ground forces the enemy to climb and thus wear themselves down. Areas of jungle and forest, with dense vegetation act as force-multipliers, of benefit to inferior armies. Terrain may have lost importance in modern warfare, due to the advent of aircraft, though the terrain is still vital for camouflage, especially for guerrilla warfare.\nGenerals and commanders also play an important role, Hannibal, Julius Caesar, Khalid ibn Walid, Subutai and Napoleon Bonaparte were all skilled generals and their armies were extremely successful at times. An army that can trust the commands of their leaders with conviction in its success invariably has a higher morale than an army that doubts its every move. The British in the naval Battle of Trafalgar owed its success to the reputation of Admiral Lord Nelson.\nTypes.\nBattles can be fought on land, at sea, and in the air. Naval battles have occurred since before the 5th century BC. Air battles have been far less common, due to their late conception, the most prominent being the Battle of Britain in 1940. Since the Second World War, land or sea battles have come to rely on air support. During the Battle of Midway, five aircraft carriers were sunk without either fleet coming into direct contact.\nBattles are usually hybrids of different types listed above.\nA \"decisive battle\" is one with political effects, determining the course of the war such as the Battle of Smolensk or bringing hostilities to an end, such as the Battle of Hastings or the Battle of Hattin. A decisive battle can change the balance of power or boundaries between countries. The concept of the \"decisive battle\" became popular with the publication in 1851 of Edward Creasy's \"The Fifteen Decisive Battles of the World\". British military historians J.F.C. Fuller (\"The Decisive Battles of the Western World\") and B.H. Liddell Hart (\"Decisive Wars of History\"), among many others, have written books in the style of Creasy's work.\nLand.\nThere is an obvious difference in the way battles have been fought. Early battles were probably fought between rival hunting bands as unorganized crowds. During the Battle of Megiddo, the first reliably documented battle in the fifteenth century BC, both armies were organised and disciplined; during the many wars of the Roman Empire, barbarians continued to use mob tactics.\nAs the Age of Enlightenment dawned, armies began to fight in highly disciplined lines. Each would follow the orders from their officers and fight as a unit instead of individuals. Armies were divided into regiments, battalions, companies and platoons. These armies would march, line up and fire in divisions.\nNative Americans, on the other hand, did not fight in lines, using guerrilla tactics. American colonists and European forces continued using disciplined lines into the American Civil War.\nA new style arose from the 1850s to the First World War, known as trench warfare, which also led to tactical radio. Chemical warfare also began in 1915.\nBy the Second World War, the use of the smaller divisions, platoons and companies became much more important as precise operations became vital. Instead of the trench stalemate of 1915\u20131917, in the Second World War, battles developed where small groups encountered other platoons. As a result, elite squads became much more recognized and distinguishable. Maneuver warfare also returned with an astonishing pace with the advent of the tank, replacing the cannon of the Enlightenment Age. Artillery has since gradually replaced the use of frontal troops. Modern battles resemble those of the Second World War, along with indirect combat through the use of aircraft and missiles which has come to constitute a large portion of wars in place of battles, where battles are now mostly reserved for capturing cities.\nNaval.\nOne significant difference of modern naval battles, as opposed to earlier forms of combat is the use of marines, which introduced amphibious warfare. Today, a marine is actually an infantry regiment that sometimes fights solely on land and is no longer tied to the navy. A good example of an ancient naval battle is the Battle of Salamis. Most ancient naval battles were fought by fast ships using the battering ram to sink opposing fleets or steer close enough for boarding in hand-to-hand combat. Troops were often used to storm enemy ships as used by Romans and pirates. This tactic was usually used by civilizations that could not beat the enemy with ranged weaponry. Another invention in the late Middle Ages was the use of Greek fire by the Byzantines, which was used to set enemy fleets on fire. Empty demolition ships utilized the tactic to crash into opposing ships and set it afire with an explosion. After the invention of cannons, naval warfare became useful as support units for land warfare. During the 19th century, the development of mines led to a new type of naval warfare. The ironclad, first used in the American Civil War, resistant to cannons, soon made the wooden ship obsolete. The invention of military submarines, during World War I, brought naval warfare to both above and below the surface. With the development of military aircraft during World War II, battles were fought in the sky as well as below the ocean. Aircraft carriers have since become the central unit in naval warfare, acting as a mobile base for lethal aircraft.\nAerial.\nAlthough the use of aircraft has for the most part always been used as a supplement to land or naval engagements, since their first major military use in World War I aircraft have increasingly taken on larger roles in warfare. During World War I, the primary use was for reconnaissance, and small-scale bombardment. Aircraft began becoming much more prominent in the Spanish Civil War and especially World War II. Aircraft design began specializing, primarily into two types: bombers, which carried explosive payloads to bomb land targets or ships; and fighter-interceptors, which were used to either intercept incoming aircraft or to escort and protect bombers (engagements between fighter aircraft were known as dog fights). Some of the more notable aerial battles in this period include the Battle of Britain and the Battle of Midway. Another important use of aircraft came with the development of the helicopter, which first became heavily used during the Vietnam War, and still continues to be widely used today to transport and augment ground forces. Today, direct engagements between aircraft are rare \u2013 the most modern fighter-interceptors carry much more extensive bombing payloads, and are used to bomb precision land targets, rather than to fight other aircraft. Anti-aircraft batteries are used much more extensively to defend against incoming aircraft than interceptors. Despite this, aircraft today are much more extensively used as the primary tools for both army and navy, as evidenced by the prominent use of helicopters to transport and support troops, the use of aerial bombardment as the \"first strike\" in many engagements, and the replacement of the battleship with the aircraft carrier as the center of most modern navies.\nNaming.\nBattles are usually named after some feature of the battlefield geography, such as a town, forest or river, commonly prefixed \"Battle of...\". Occasionally battles are named after the date on which they took place, such as The Glorious First of June. In the Middle Ages it was considered important to settle on a suitable name for a battle which could be used by the chroniclers. After Henry V of England defeated a French army on October 25, 1415, he met with the senior French herald and they agreed to name the battle after the nearby castle and so it was called the Battle of Agincourt. In other cases, the sides adopted different names for the same battle, such as the Battle of Gallipoli which is known in Turkey as the Battle of \u00c7anakkale. During the American Civil War, the Union tended to name the battles after the nearest watercourse, such as the Battle of Wilsons Creek and the Battle of Stones River, whereas the Confederates favoured the nearby towns, as in the Battles of Chancellorsville and Murfreesboro. Occasionally both names for the same battle entered the popular culture, such as the First Battle of Bull Run and the Second Battle of Bull Run, which are also referred to as the First and Second Battles of Manassas.\nSometimes in desert warfare, there is no nearby town name to use; map coordinates gave the name to the Battle of 73 Easting in the First Gulf War. Some place names have become synonymous with battles, such as the Passchendaele, Pearl Harbor, the Alamo, Thermopylae and Waterloo. Military operations, many of which result in battle, are given codenames, which are not necessarily meaningful or indicative of the type or the location of the battle. Operation Market Garden and Operation Rolling Thunder are examples of battles known by their military codenames. When a battleground is the site of more than one battle in the same conflict, the instances are distinguished by ordinal number, such as the First and Second Battles of Bull Run. An extreme case are the twelve Battles of the Isonzo\u2014First to Twelfth\u2014between Italy and Austria-Hungary during the First World War.\nSome battles are named for the convenience of military historians so that periods of combat can be neatly distinguished from one another. Following the First World War, the British Battles Nomenclature Committee was formed to decide on standard names for all battles and subsidiary actions. To the soldiers who did the fighting, the distinction was usually academic; a soldier fighting at Beaumont Hamel on November 13, 1916, was probably unaware he was taking part in what the committee named the Battle of the Ancre. Many combats are too small to be battles; terms such as \"action\", \"affair\", \"skirmish\", \"firefight\", \"raid\", or \"offensive patrol\" are used to describe small military encounters. These combats often take place within the time and space of a battle and while they may have an objective, they are not necessarily \"decisive\". Sometimes the soldiers are unable to immediately gauge the significance of the combat; in the aftermath of the Battle of Waterloo, some British officers were in doubt as to whether the day's events merited the title of \"battle\" or would be called an \"action\".\nEffects.\nBattles affect the individuals who take part, as well as the political actors. Personal effects of battle range from mild psychological issues to permanent and crippling injuries. Some battle-survivors have nightmares about the conditions they encountered or abnormal reactions to certain sights or sounds and some experience flashbacks. Physical effects of battle can include scars, amputations, lesions, loss of bodily functions, blindness, paralysis and death. Battles affect politics; a decisive battle can cause the losing side to surrender, while a Pyrrhic victory such as the Battle of Asculum can cause the winning side to reconsider its goals. Battles in civil wars have often decided the fate of monarchs or political factions. Famous examples include the Wars of the Roses, as well as the Jacobite risings. Battles affect the commitment of one side or the other to the continuance of a war, for example the Battle of Inchon and the Battle of Hu\u1ebf during the Tet Offensive.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4182", "revid": "1319147978", "url": "https://en.wikipedia.org/wiki?curid=4182", "title": "Berry Berenson", "text": "American actress (1948\u20132001)\nBerinthia \"Berry\" Berenson-Perkins (n\u00e9e Berenson; April 14, 1948 \u2013 September 11, 2001) was an American actress, model and photographer. She was the wife of actor Anthony Perkins. \nShe died in the September 11 attacks, as a passenger on American Airlines Flight 11. It crashed into the North Tower of the World Trade Center in New York City.\nEarly life.\nBerinthia Berenson, nicknamed \"Berry\", was born in Murray Hill, Manhattan, New York City. Her mother was born Maria-Luisa Yvonne Radha de Wendt de Kerlor, better known as Gogo Schiaparelli, a socialite of Italian, Swiss, &amp; French ancestry. Her father, Robert Lawrence Berenson, was an American career diplomat turned shipping executive. He was of Russian-Jewish and Polish-Jewish descent, and his family's original surname was Valvrojenski.\nHer elder sister, Marisa Berenson, became a well-known model and actress. Their maternal grandmother was Italian-born fashion designer Elsa Schiaparelli, and her maternal grandfather was Wilhelm de Wendt de Kerlor, a Theosophist and psychic medium. The Berenson sisters were also great-grandnieces of Giovanni Schiaparelli, an Italian astronomer who believed he had discovered canals on Mars, and a second cousin, once removed, of art expert Bernard Berenson (1865\u20131959), and his sister Senda Berenson (1868\u20131954), an athlete and educator who was one of the first two women elected to the Basketball Hall of Fame.\nCareer.\nFollowing a brief modeling career in the late 1960s, Berenson became a freelance photographer. In 1972, Berenson's fianc\u00e9 Richard Bernstein was hired as the cover artist for Andy Warhol's \"Interview\" magazine. Berenson would recruit models for the cover and photograph them, and Bernstein illustrated the images. By 1973, her photographs had been published in \"Life\", \"Glamour\", \"Vogue\" and \"Newsweek\".\nBerenson studied acting at New York's The American Place Theatre with Wynn Handman along with Richard Gere, Philip Anglim, Penelope Milford, Robert Ozn, Ingrid Boulting and her sister Marisa.\nAs an actress, Berenson starred opposite her husband Anthony Perkins in the 1978 Alan Rudolph film \"Remember My Name\". She also appeared with Jeff Bridges in the 1979 film \"Winter Kills\", and with Malcolm McDowell in \"Cat People\" (1982).\nPersonal life.\nBerenson was engaged to artist Richard Bernstein. In 1972, Berenson had an affair with actor Anthony Perkins and they married on August 9, 1973, in Wellfleet, Massachusetts while she was three months pregnant. The couple raised two sons: actor-director Oz Perkins and folk/rock singer-songwriter Elvis Perkins. They remained married until Perkins died from AIDS-related complications on September 12, 1992.\nDeath.\nBerenson died on September 11, 2001, a day before the ninth anniversary of Perkins\u2019 death, as she was returning home to Los Angeles from a vacation on Cape Cod. She and the other passengers and crew aboard American Airlines Flight 11 died when the plane was hijacked and deliberately crashed into the North Tower of the World Trade Center during the September 11 attacks on the US. Her remains were never found.\nAt the National September 11 Memorial &amp; Museum, Berenson's name is inscribed on Panel N-76 at the North Pool.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "4183", "revid": "50865928", "url": "https://en.wikipedia.org/wiki?curid=4183", "title": "Botany", "text": "Study of plant life\nBotany, also called phytology or plant science, is the branch of natural science and biology that studies plants, especially their anatomy, taxonomy, and ecology. A botanist or plant scientist is a scientist who specialises in this field. \"Plant\" and \"botany\" may be defined more narrowly to include only land plants and their study, which is also known as phytology. Phytologists or botanists (in the strict sense) study approximately 410,000 species of land plants, including some 391,000 species of vascular plants (of which approximately 369,000 are flowering plants) and approximately 20,000 bryophytes.\nBotany originated as prehistoric herbalism to identify and later cultivate plants that were edible, poisonous, and medicinal, making it one of the first endeavours of human investigation. Medieval physic gardens, often attached to monasteries, contained plants that possibly had medicinal benefits. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy and led in 1753 to the binomial system of nomenclature of Carl Linnaeus that remains in use to this day for the naming of all biological species.\nIn the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.\nModern botany is a broad subject with contributions and insights from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st-century plant science are molecular genetics and epigenetics, which study the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.\nEtymology.\nThe term \"botany\" comes from the Ancient Greek word ' () meaning \"pasture\", \"herbs\" \"grass\", or \"fodder\"; ' is in turn derived from \"\" (Greek: ), \"to feed\" or \"to graze\". Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress.\nHistory.\nEarly botany.\nBotany originated as herbalism, the study and use of plants for their possible medicinal properties. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BCE, Ancient Egypt, in archaic Avestan writings, and in works from China purportedly from before 221 BCE.\nModern botany traces its roots back to Ancient Greece specifically to Theophrastus (c.\u2009371\u2013287 BCE), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the \"Father of Botany\". His major works, \"Enquiry into Plants\" and \"On the Causes of Plants\", constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.\nAnother work from Ancient Greece that made an early impact on botany is , a five-volume encyclopaedia about preliminary herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's \"Nabatean Agriculture\", Ab\u016b \u1e24an\u012bfa D\u012bnawar\u012b's (828\u2013896) the \"Book of Plants\", and Ibn Bassal's \"The Classification of Soils\". In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.\nIn the mid-16th century, botanical gardens were founded in a number of Italian universities. The Padua botanical garden in 1545 is usually considered to be the first that is still in its original location. These gardens continued the practical value of earlier \"physic gardens\", often associated with monasteries, in which plants were cultivated for suspected medicinal uses. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621.\nGerman physician Leonhart Fuchs (1501\u20131566) was one of \"the three German fathers of botany\", along with theologian Otto Brunfels (1489\u20131534) and physician Hieronymus Bock (1498\u20131554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.\nPhysician Valerius Cordus (1515\u20131544) authored a botanically and pharmacologically important herbal \"Historia Plantarum\" in 1544 and a pharmacopoeia of lasting importance, the \"Dispensatorium\" in 1546. Naturalist Conrad von Gesner (1516\u20131565) and herbalist John Gerard (1545 \u2013 c.\u20091611) published herbals covering the supposed medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522\u20131605) was considered the \"father of natural history\", which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells (a term he coined) in cork, and a short time later in living plant tissue.\nEarly modern botany.\nDuring the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753, Carl Linnaeus published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's \"Systema Sexuale\" classified plants into 24 groups according to the number of their male sexual organs. The 24th group, \"Cryptogamia\", included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.\nIncreasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later Bentham &amp; Hooker system, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the \"Origin of Species\" in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.\nIn the 19th century botany was a socially acceptable hobby for upper-class women. These women would collect and paint flowers and plants from around the world with scientific accuracy. The paintings were used to record many species that could not be transported or maintained in other environments. Marianne North illustrated over 900 species in extreme detail with watercolour and oil paintings. Her work and many other women's botany work was the beginning of popularising botany to a wider audience.\nBotany was greatly stimulated by the appearance of the first \"modern\" textbook, Matthias Schleiden's \"\", published in English in 1849 as \"Principles of Scientific Botany\". Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831. In 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.\nLate modern botany.\nBuilding upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822\u20131884), August Weismann (1834\u20131914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898\u20131997) on plant anatomy is still a major foundation of modern botany. Her books \"Plant Anatomy\" and \"Anatomy of Seed Plants\" have been key plant structural biology texts for more than half a century.\nThe discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunki\u00e6r whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887\u20131943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.\nParticularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues), the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled a precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled the regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.\n20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown \"in vitro\" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.\nModern morphology recognises a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasises structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for the identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.\nBranches of botany.\nBotany is divided along several axes.\nSome subfields of botany relate to particular groups of organisms. Divisions related to the broader historical sense of botany include \"bacteriology\", \"mycology\" (or \"fungology\"), and \"phycology\" \u2013 respectively, the study of bacteria, fungi, and algae \u2013 with \"lichenology\" as a subfield of mycology. The narrower sense of botany as the study of embryophytes (land plants) is called \"phytology\". \"Bryology\" is the study of mosses (and in the broader sense also liverworts and hornworts). \"Pteridology\" (or \"filicology\") is the study of ferns and allied plants. A number of other taxa of ranks varying from family to subgenus have terms for their study, including \"agrostology\" (or \"graminology\") for the study of grasses, \"synantherology\" for the study of composites, and \"batology\" for the study of brambles.\nStudy can also be divided by guild rather than clade or grade. For example, \"dendrology\" is the study of woody plants.\nMany divisions of biology have botanical subfields. These are commonly denoted by prefixing the word plant (e.g. plant taxonomy, plant ecology, plant anatomy, plant morphology, plant systematics), or prefixing or substituting the prefix phyto- (e.g. phytochemistry, phytogeography). The study of fossil plants is called \"palaeobotany\". Other fields are denoted by adding or substituting the word botany (e.g. systematic botany).\n\"Phytosociology\" is a subfield of plant ecology that classifies and studies communities of plants.\nThe intersection of fields from the above pair of categories gives rise to fields such as \"bryogeography\", the study of the distribution of mosses.\nDifferent parts of plants also give rise to their own subfields, including \"xylology\", \"carpology\" (or \"fructology\"), and \"palynology\", these being the study of wood, fruit and pollen/spores respectively.\nBotany also overlaps on the one hand with agriculture, horticulture and silviculture, and on the other hand with medicine and pharmacology, giving rise to fields such as \"agronomy\", \"horticultural botany\", \"phytopathology\", and \"phytopharmacology\".\nScope and importance.\nThe study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, biochemicals, and products for people, as well as creating and preserving soil.\nHistorically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.\nThe strictest definition of \"plant\" includes only the \"land plants\" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) \u2013 including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.\nPalaeobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.\nAmong the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.\nHuman nutrition.\nVirtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them. Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level. The modern forms of the major staple foods, such as hemp, teff, maize, rice, wheat and other cereal grasses, pulses, bananas and plantains, as well as hemp, flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics.\nBotanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to humanity's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant\u2013people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany. Some of the earliest plant-people relationships arose between the indigenous people of Canada in identifying edible plants from inedible plants. This relationship the indigenous people had with plants was recorded by ethnobotanists.\nPlant biochemistry.\nPlant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialised materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.\nPlants and various other groups of photosynthetic eukaryotes collectively known as \"algae\" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll \"a\". Chlorophyll \"a\" (as well as its plant and green algal-specific cousin chlorophyll \"b\") absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O2) as a by-product.\nThe light energy captured by chlorophyll \"a\" is initially in the form of electrons (and later a proton gradient) that is used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesised. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.\nUnlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesising all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out.\nPlants synthesise a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.\nVascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibres that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.\nThe concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the C4 carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common C3 carbon fixation pathway. These biochemical strategies are unique to land plants.\nMedicine and materials.\nPhytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine). Native Americans have used various plants as ways of treating illness or disease for thousands of years. This knowledge Native Americans have on plants has been recorded by enthnobotanists and then in turn has been used by pharmaceutical companies as a way of drug discovery.\nPlants can synthesise coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.\nSugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel. Sweetgrass was used by Native Americans to ward off bugs like mosquitoes. These bug repelling properties of sweetgrass were later found by the American Chemical Society in the molecules phytol and coumarin.\nPlant ecology.\nPlant ecology is the science of the functional relationships between plants and their habitats\u00a0\u2013 the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. Some ecologists even rely on empirical data from indigenous people that is gathered by ethnobotanists. This information can relay a great deal of information on how the land once was thousands of years ago and how it has changed over that time. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.\nPlants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilise mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.\nHerbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.\nPlants, climate and environmental change.\nPlant responses to climate and other environmental changes can inform one's understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\nGenetics.\n&lt;templatestyles src=\"Plain image with caption/styles.css\"/&gt; \nA Punnett square depicting a cross between two pea plants heterozygous for purple (B) and white (b) blossoms \nInheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in \"Pisum sativum\" (peas). What Mendel learned from studying plants has had far-reaching benefits outside of botany. Similarly, \"jumping genes\" were discovered by Barbara McClintock while she was studying maize. Nevertheless, there are some distinctive genetic differences between plants and other organisms.\nSpecies boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, \"Mentha\" \u00d7 \"piperita\", a sterile hybrid between \"Mentha aquatica\" and spearmint, \"Mentha spicata\". The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.\nCharles Darwin in his 1878 book The Effects of Cross and Self-Fertilization in the Vegetable Kingdom at the start of chapter XII noted \"The first and most important of the conclusions which may be drawn from the observations given in this volume, is that generally cross-fertilisation is beneficial and self-fertilisation often injurious, at least with the plants on which I experimented.\" An important adaptive benefit of outcrossing is that it allows the masking of deleterious mutations in the genome of progeny. This beneficial effect is also known as hybrid vigour or heterosis. Once outcrossing is established, subsequent switching to inbreeding becomes disadvantageous since it allows expression of the previously masked deleterious recessive mutations, commonly referred to as inbreeding depression.\nUnlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.\nMost sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.\nAs in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.\nMolecular genetics.\nA considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, \"Arabidopsis thaliana\", a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. \"Arabidopsis\" was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice (\"Oryza sativa\") and \"Brachypodium distachyon\", has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.\nModel plants such as \"Arabidopsis thaliana\" are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in C4 plants. The single celled green alga \"Chlamydomonas reinhardtii\", while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga \"Cyanidioschyzon merolae\" has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss \"Physcomitrella patens\" are commonly used to study plant cell biology.\n\"Agrobacterium tumefaciens\", a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.\nEpigenetics.\nEpigenetics is the study of heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or \"express themselves\") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.\nEpigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilised egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.\nUnlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodelling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.\nEpigenetic changes can lead to paramutations, which do not follow the Mendelian heritage rules. These epigenetic marks are carried from one generation to the next, with one allele inducing a change on the other.\nPlant evolution.\nThe chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as \"blue-green algae\") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a .\nThe algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.\nNonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved \"megaspory\" \u2013 their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce \"naked seeds\" not fully enclosed in an ovary; modern representatives include conifers, cycads, \"Ginkgo\", and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.\nPlant physiology.\nPlant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.\nMolecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulphur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals. Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.\nPlant hormones.\nPlants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of \"Mimosa pudica\", the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.\nThe hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded \"It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements\". About the same time, the role of auxins (from the Greek , to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.\nCytokinins are a class of plant hormones named for their control of cell division (especially cytokinesis). The natural cytokinin zeatin was discovered in corn, \"Zea mays\", and is a derivative of the purine adenine. Zeatin is produced in roots and transported to shoots in the xylem where it promotes cell division, bud development, and the greening of chloroplasts. The gibberelins, such as gibberelic acid are diterpenes synthesised from acetyl CoA via the mevalonate pathway. They are involved in the promotion of germination and dormancy-breaking in seeds, in regulation of plant height by controlling stem elongation and the control of flowering. Abscisic acid (ABA) occurs in all land plants except liverworts, and is synthesised from carotenoids in the chloroplasts and other plastids. It inhibits cell division, promotes seed maturation, and dormancy, and promotes stomatal closure. It was so named because it was originally thought to control abscission. Ethylene is a gaseous hormone that is produced in all higher plant tissues from methionine. It is now known to be the hormone that stimulates or regulates fruit ripening and abscission, and it, or the synthetic growth regulator ethephon which is rapidly metabolised to produce ethylene, are used on industrial scale to promote ripening of cotton, pineapples and other climacteric crops.\nAnother class of phytohormones is the jasmonates, first isolated from the oil of \"Jasminum grandiflorum\" which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.\nIn addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.\nPlant anatomy and morphology.\nPlant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.\nAll plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.\n&lt;templatestyles src=\"Plain image with caption/styles.css\"/&gt; \nA diagram of a \"typical\" eudicot, the most common type of plant (three-fifths of all plant species). However, no plant actually looks exactly like this. \nThe bodies of vascular plants including clubmosses, ferns and seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.\nThe root system and the shoot system are interdependent \u2013 the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with plants in \"Streptocarpus\" sect. \"Saintpaulia\", or even a single cell \u2013 which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.\nIn vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.\nStems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, \"Ginkgo\", and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.\nAlthough reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.\nSystematic botany.\nSystematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent \u2013 grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.\nKingdom Plantae belongs to Domain Eukaryota and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural \"genera\"); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single worldwide name for each organism. For example, the tiger lily is \"Lilium columbianum\". \"Lilium\" is the genus, and \"columbianum\" the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).\nThe evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of \"Pereskia\" are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an \"Echinocactus\". However, both \"Pereskia\" and \"Echinocactus\" have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.\nJudging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history \u2013 such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) \u2013 and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.\nFrom the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having \"direct access to the genetic basis of evolution.\" As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below \u2013 fungi are more closely related to animals than to plants.\nIn 1998, the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.\nSymbols.\nA few symbols are in current use in botany. A number of others are obsolete; for example, Linnaeus used planetary symbols \u27e8\u2642\u27e9 (Mars) for biennial plants, \u27e8\u2643\u27e9 (Jupiter) for herbaceous perennials and \u27e8\u2644\u27e9 (Saturn) for woody perennials, based on the planets' orbital periods of 2, 12 and 30 years; and Willd used \u27e8\u2644\u27e9 (Saturn) for neuter in addition to \u27e8\u263f\u27e9 (Mercury) for hermaphroditic. The following symbols are still used:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n\u2640 female\n\u2642 male\n\u26a5 hermaphrodite/bisexual\n\u26b2 vegetative (asexual) reproduction\n\u25ca sex unknown\n\u2609 annual\n\u2687 biennial\n\u267e perennial\n\u2620 poisonous\n\ud83d\udec8 further information\n\u00d7 crossbred hybrid\n+ grafted hybrid\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4184", "revid": "51037973", "url": "https://en.wikipedia.org/wiki?curid=4184", "title": "Bacillus thuringiensis", "text": "Species of bacteria used as an insecticide\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nBacillus thuringiensis (or Bt) is a gram-positive, soil-dwelling bacterium, and is the most commonly used biological pesticide worldwide. \"B. thuringiensis\" also occurs naturally in the gut of caterpillars of various types of moths and butterflies, as well as on leaf surfaces, aquatic environments, animal feces, insect-rich environments, flour mills and grain-storage facilities. It has also been observed to parasitize moths such as \"Cadra calidella\"\u2014in laboratory experiments working with \"C. calidella\", many of the moths were diseased due to this parasite.\nDuring sporulation, many Bt strains produce crystal proteins (proteinaceous inclusions), called delta endotoxins, that have insecticidal action. This has led to their use as insecticides, and more recently to genetically modified crops using Bt genes, such as Bt corn. Many crystal-producing Bt strains, though, do not have insecticidal properties. Bacillus thuringiensis israelensis (Bti) was discovered in 1976 by Israeli settler researchers Yoel Margalith and B. Goldberg in the Negev Desert of occupied Palestine. While investigating mosquito breeding sites in the region, they isolated a bacterial strain from a stagnant pond that exhibited potent larvicidal activity against various mosquito species, including \"Anopheles\", \"Culex\", and \"Aedes\". This subspecies, \"israelensis\", is now commonly used for the biological control of mosquitoes and fungus gnats due to its effectiveness and environmental safety.\nAs a toxic mechanism, \"cry\" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells of the targeted pests, resulting in their rupture. Other organisms (including humans, other animals and non-targeted insects) that lack the appropriate receptors in their gut cannot be affected by the \"cry\" protein, and therefore are not affected by Bt.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nTaxonomy and discovery.\nIn 1902, \"B. thuringiensis\" was first discovered in silkworms by Japanese sericultural engineer . He named it \"B. sotto\", using the Japanese word , here referring to bacillary paralysis. In 1911, German microbiologist Ernst Berliner rediscovered it when he isolated it as the cause of a disease called \"\" in flour moth caterpillars in Thuringia (hence the specific name \"thuringiensis\", \"Thuringian\"). \"B. sotto\" would later be reassigned as \"B. thuringiensis\" var. \"sotto\".\nIn 1976, Robert A. Zakharyan reported the presence of a plasmid in a strain of \"B. thuringiensis\" and suggested the plasmid's involvement in endospore and crystal formation. \"B. thuringiensis\" is closely related to \"B. cereus\", a soil bacterium, and \"B. anthracis\", the cause of anthrax; the three organisms differ mainly in their plasmids. Like other members of the genus, all three are capable of producing endospores.\nSpecies group placement.\n\"B. thuringiensis\" is placed in the \"Bacillus cereus\" group which is variously defined as seven closely related species: \"B.\u00a0cereus\" \"sensu stricto\" (\"B.\u00a0cereus\"), \"B.\u00a0anthracis\", \"B.\u00a0thuringiensis\", \"B. mycoides\", \"B. pseudomycoides\", and \"B.\u00a0cytotoxicus\"; or as six species in a \"Bacillus cereus\" sensu lato: \"B. weihenstephanensis\", \"B. mycoides\", \"B. pseudomycoides\", \"B. cereus\", \"B. thuringiensis\", and \"B. anthracis\". Within this grouping \"B.t.\" is more closely related to \"B.ce.\" It is more distantly related to \"B.w.\", \"B.m.\", \"B.p.\", and \"B.cy.\"\nSubspecies.\nThere are several dozen recognized subspecies of \"B. thuringiensis\". Subspecies commonly used as insecticides include \"B. thuringiensis\" subspecies \"kurstaki\" (Btk), subspecies \"israelensis\" (Bti) and &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;subspecies \"aizawai\" (Bta). Some Bti lineages are clonal.\nGenetics.\nSome strains are known to carry the same genes that produce enterotoxins in \"B. cereus\", and so it is possible that the entire \"B. cereus\" sensu lato group may have the potential to be enteropathogens.\nThe proteins that \"B. thuringiensis\" is most known for are encoded by \"cry\" genes. In most strains of \"B. thuringiensis\", these genes are located on a plasmid (in other words \"cry\" is not a chromosomal gene in most strains). If these plasmids are lost it becomes indistinguishable from \"B. cereus\" as \"B. thuringiensis\" has no other species characteristics. Plasmid exchange has been observed both naturally and experimentally both within \"B.t.\" and between \"B.t.\" and two congeners, \"B. cereus\" and \"B. mycoides\".\nplcR is an indispensable transcription regulator of most virulence factors, its absence greatly reducing virulence and toxicity. Some strains do naturally complete their life cycle with an inactivated plcR. It is half of a two-gene operon along with the heptapeptide &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;papR. papR is part of quorum sensing in \"B. thuringiensis\".\nVarious strains including \"Btk\" ATCC 33679 carry plasmids belonging to the wider pXO1-like family. (The pXO1 family being a \"B. cereus\"-common family with members of \u2248330kb length. They differ from pXO1 by replacement of the pXO1 pathogenicity island.) The insect parasite \"Btk\" HD73 carries a pXO2-like plasmid (pBT9727) lacking the 35kb pathogenicity island of pXO2 itself, and in fact having no identifiable virulence factors. (The pXO2 family does not have replacement of the pathogenicity island, instead simply lacking that part of pXO2.)\nThe genomes of the \"B. cereus\" group may contain two types of introns, dubbed group I and group II. \"B.t\" strains have variously 0\u20135 group Is and 0\u201313 group IIs.\nThere is still insufficient information to determine whether chromosome-plasmid coevolution to enable adaptation to particular environmental niches has occurred or is even possible.\nCommon with \"B. cereus\" but so far not found elsewhere \u2013 including in other members of the species group \u2013 are the efflux pump BC3663, the \"N\"-acyl-L-amino-acid amidohydrolase BC3664, and the methyl-accepting chemotaxis protein BC5034.\nMechanism of insecticidal action.\nUpon sporulation, \"B. thuringiensis\" forms crystals of two types of proteinaceous insecticidal delta endotoxins (\u03b4-endotoxins) called crystal proteins or Cry proteins, which are encoded by \"cry\" genes, and Cyt proteins.\nCry toxins have specific activities against insect species of the orders Lepidoptera (moths and butterflies), Diptera (flies and mosquitoes), Coleoptera (beetles) and Hymenoptera (wasps, bees, ants and sawflies), as well as against nematodes. A specific example of \"B. thuringiensis\" use against beetles is the fight against Colorado Potato Beetles in potato crops. Thus, \"B. thuringiensis\" serves as an important reservoir of Cry toxins for production of biological insecticides and insect-resistant genetically modified crops. When insects ingest toxin crystals, their alkaline digestive tracts denature the insoluble crystals, making them soluble and thus amenable to being cut with proteases found in the insect gut, which liberate the toxin from the crystal. The Cry toxin is then inserted into the insect gut cell membrane, paralyzing the digestive tract and forming a pore. The insect stops eating and starves to death; live Bt bacteria may also colonize the insect, which can contribute to death. Death occurs within a few hours or weeks. The midgut bacteria of susceptible larvae may be required for \"B. thuringiensis\" insecticidal activity.\nA \"B. thuringiensis\" small RNA called BtsR1 can silence the Cry5Ba toxin expression when outside the host by binding to the RBS site of the Cry5Ba toxin transcript to avoid nematode behavioral defenses. The silencing results in an increase of the bacteria ingestion by \"C. elegans\". The expression of BtsR1 is then reduced after ingestion, resulting in Cry5Ba toxin production and host death.\nIn 1996 another class of insecticidal proteins in Bt was discovered: the vegetative insecticidal proteins (Vip; InterPro:\u00a0\"https://\"). Vip proteins do not share sequence homology with Cry proteins, in general do not compete for the same receptors, and some kill different insects than do Cry proteins.\nIn 2000, a novel subgroup of Cry protein, designated parasporin, was discovered from non-insecticidal \"B. thuringiensis\" isolates. The proteins of parasporin group are defined as \"B. thuringiensis\" and related bacterial parasporal proteins that are not hemolytic, but capable of preferentially killing cancer cells. As of January 2013, parasporins comprise six subfamilies: PS1 to PS6.\nUse of spores and proteins in pest control.\nSpores and crystalline insecticidal proteins produced by \"B. thuringiensis\" have been used to control insect pests since the 1920s and are often applied as liquid sprays and donut pellets. They are now used as specific insecticides under trade names such as DiPel, Thuricide, and Mosquito Dunks. Because of their specificity, these pesticides are regarded as environmentally friendly, with little or no effect on humans, wildlife, pollinators, and most other beneficial insects, and are used in organic farming; however, the manuals for these products do contain many environmental and human health warnings, and a 2012 European regulatory peer review of five approved strains found, while data exist to support some claims of low toxicity to humans and the environment, the data are insufficient to justify many of these claims.\nNew strains of Bt are developed and introduced over time as insects develop resistance to Bt, or the desire occurs to force mutations to modify organism characteristics, or to use homologous recombinant genetic engineering to improve crystal size and increase pesticidal activity, or broaden the host range of Bt and obtain more effective formulations. Each new strain is given a unique number and registered with the U.S. EPA and allowances may be given for genetic modification depending on \"its parental strains, the proposed pesticide use pattern, and the manner and extent to which the organism has been genetically modified\". Formulations of Bt that are approved for organic farming in the US are listed at the website of the Organic Materials Review Institute (OMRI) and several university extension websites offer advice on how to use Bt spore or protein preparations in organic farming.\nUse of Bt genes in genetic engineering of plants for pest control.\nThe Belgian company Plant Genetic Systems (now part of Bayer CropScience) was the first company (in 1985) to develop genetically modified crops (tobacco) with insect tolerance by expressing \"cry\" genes from \"B. thuringiensis\"; the resulting crops contain delta endotoxin. The Bt tobacco was never commercialized; tobacco plants are used to test genetic modifications since they are easy to manipulate genetically and are not part of the food supply.\nUsage.\nIn 1995, &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;potato plants producing CRY 3A Bt toxin were approved safe by the Environmental Protection Agency, making it the first human-modified pesticide-producing crop to be approved in the US, though many plants produce pesticides naturally, including tobacco, coffee plants, cocoa, cotton and black walnut. This was the 'New Leaf' potato, and it was removed from the market in 2001 due to lack of interest.\nIn 1996, &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;genetically modified maize producing Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.\nThe Bt genes engineered into crops and approved for release include, singly and stacked: Cry1A.105, CryIAb, CryIF, Cry2Ab, Cry3Bb1, Cry34Ab1, Cry35Ab1, mCry3A, and VIP, and the engineered crops include corn and cotton.\nCorn genetically modified to produce VIP was first approved in the US in 2010.\nIn India, by 2014, more than seven million cotton farmers, occupying twenty-six million acres, had adopted &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Bt cotton.\nMonsanto developed a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;soybean expressing Cry1Ac and the glyphosate-resistance gene for the Brazilian market, which completed the Brazilian regulatory process in 2010.\n&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Bt aspen - specifically \"Populus\" hybrids - have been developed. They do suffer lesser leaf damage from insect herbivory. The results have not been entirely positive however: The intended result - better timber yield - was not achieved, with no growth advantage despite that reduction in herbivore damage; one of their major pests still preys upon the transgenic trees; and besides that, their leaf litter decomposes differently due to the transgenic toxins, resulting in alterations to the aquatic insect populations nearby.\nSafety studies.\nThe use of Bt toxins as plant-incorporated protectants prompted the need for extensive evaluation of their safety for use in foods and potential unintended impacts on the environment.\nDietary risk assessment.\nConcerns over the safety of consumption of genetically modified plant materials that contain Cry proteins have been addressed in extensive dietary risk assessment studies. As a toxic mechanism, \"cry\" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells of the targeted pests, resulting in their rupture. While the target pests are exposed to the toxins primarily through leaf and stalk material, Cry proteins are also expressed in other parts of the plant, including trace amounts in maize kernels which are ultimately consumed by both humans and animals. However, other organisms (including humans, other animals and non-targeted insects) that lack the appropriate receptors in their gut cannot be affected by the \"cry\" protein, and therefore are not affected by Bt.\nToxicology studies.\nAnimal models have been used to assess human health risk from consumption of products containing Cry proteins. The United States Environmental Protection Agency recognizes mouse acute oral feeding studies where doses as high as 5,000\u00a0mg/kg body weight resulted in no observed adverse effects. Research on other known toxic proteins suggests that , further suggesting that Bt toxins are not toxic to mammals. The results of toxicology studies are further strengthened by the lack of observed toxicity from decades of use of \"B. thuringiensis\" and its crystalline proteins as an insecticidal spray.\nAllergenicity studies.\nIntroduction of a new protein raised concerns regarding the potential for allergic responses in sensitive individuals. Bioinformatic analysis of known allergens has indicated there is no concern of allergic reactions as a result of consumption of Bt toxins. Additionally, skin prick testing using purified Bt protein resulted in no detectable production of toxin-specific IgE antibodies, even in atopic patients.\nDigestibility studies.\nStudies have been conducted to evaluate the fate of Bt toxins that are ingested in foods. Bt toxin proteins have been shown to digest within minutes of exposure to simulated gastric fluids. The instability of the proteins in digestive fluids is an additional indication that Cry proteins are unlikely to be allergenic, since most known food allergens resist degradation and are ultimately absorbed in the small intestine.\nPersistence in environment.\nConcerns over possible environmental impact from accumulation of Bt toxins from plant tissues, pollen dispersal, and direct secretion from roots have been investigated. Bt toxins may persist in soil for over 200 days, with half-lives between 1.6 and 22 days. Much of the toxin is initially degraded rapidly by microorganisms in the environment, while some is adsorbed by organic matter and persists longer. Some studies, in contrast, claim that the toxins do not persist in the soil. Bt toxins are less likely to accumulate in bodies of water, but pollen shed or soil runoff may deposit them in an aquatic ecosystem. Fish species are not susceptible to Bt toxins if exposed.\nImpact on non-target organisms.\nThe toxic nature of Bt proteins has an adverse impact on many major crop pests, but some ecological risk assessments has been conducted to ensure safety of beneficial non-target organisms that may come into contact with the toxins. Toxicity for the monarch butterfly, has been shown to not reach dangerous levels. Most soil-dwelling organisms, potentially exposed to Bt toxins through root exudates, are probably not impacted by the growth of Bt crops.\nInsect resistance.\nMultiple insects have developed a resistance to \"B. thuringiensis\". In November 2009, Monsanto scientists found the pink bollworm had become resistant to the first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, \"Cry1Ac\". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Monsanto responded by introducing a second-generation cotton with multiple Bt proteins, which was rapidly adopted. Bollworm resistance to first-generation Bt cotton was also identified in Australia, China, Spain, and the United States. Additionally, resistance to Bt was documented in field population of diamondback moth in Hawaii, the continental US, and Asia. Studies in the cabbage looper have suggested that a mutation in the membrane transporter ABCC2 can confer resistance to Bt \"Cry1Ac\".\nSecondary pests.\nSeveral studies have documented surges in \"sucking pests\" (which are not affected by Bt toxins) within a few years of adoption of Bt cotton. In China, the main problem has been with mirids, which have in some cases \"completely eroded all benefits from Bt cotton cultivation\". The increase in sucking pests depended on local temperature and rainfall conditions and increased in half the villages studied. The increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. Another study in five provinces in China found the reduction in pesticide use in Bt cotton cultivars is significantly lower than that reported in research elsewhere, consistent with the hypothesis suggested by recent studies that more pesticide sprayings are needed over time to control emerging secondary pests, such as aphids, spider mites, and lygus bugs.\nSimilar problems have been reported in India, with both mealy bugs and aphids although a survey of small Indian farms between 2002 and 2008 concluded Bt cotton adoption has led to higher yields and lower pesticide use, decreasing over time.\nControversies.\nThe controversies surrounding Bt use are among the many genetically modified food controversies more widely.\nLepidopteran toxicity.\nThe most publicised problem associated with Bt crops is the claim that pollen from Bt maize could kill the monarch butterfly. The paper produced a public uproar and demonstrations against Bt maize; however by 2001 several follow-up studies coordinated by the USDA had asserted that \"the most common types of Bt maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields.\" Similarly, \"B. thuringiensis\" has been widely used for controlling \"Spodoptera littoralis\" larvae growth due to their detrimental pest activities in Africa and Southern Europe. However, \"S. littoralis\" showed resistance to many strains of \"B. thuriginesis\" and were only effectively controlled by a few strains.\nWild maize genetic mixing.\nA study published in \"Nature\" in 2001 reported Bt-containing maize genes were found in maize in its center of origin, Oaxaca, Mexico. Another \"Nature\" paper published in 2002 claimed that the previous paper's conclusion was the result of an artifact caused by an inverse polymerase chain reaction and that \"the evidence available is not sufficient to justify the publication of the original paper.\" A significant controversy happened over the paper and \"Nature\"'s unprecedented notice.\nA subsequent large-scale study in 2005 failed to find any evidence of genetic mixing in Oaxaca. A 2007 study found the \"transgenic proteins expressed in maize were found in two (0.96%) of 208 samples from farmers' fields, located in two (8%) of 25 sampled communities.\" Mexico imports a substantial amount of maize from the U.S., and due to formal and informal seed networks among rural farmers, many potential routes are available for transgenic maize to enter into food and feed webs. One study found small-scale (about 1%) introduction of transgenic sequences in sampled fields in Mexico; it did not find evidence for or against this introduced genetic material being inherited by the next generation of plants. That study was immediately criticized, with the reviewer writing, \"Genetically, any given plant should be either non-transgenic or transgenic, therefore for leaf tissue of a single transgenic plant, a GMO level close to 100% is expected. In their study, the authors chose to classify leaf samples as transgenic despite GMO levels of about 0.1%. We contend that results such as these are incorrectly interpreted as positive and are more likely to be indicative of contamination in the laboratory.\"\nColony collapse disorder.\nAs of 2007, a new phenomenon called colony collapse disorder (CCD) began affecting bee hives all over North America. Initial speculation on possible causes included new parasites, pesticide use, and the use of Bt transgenic crops. The Mid-Atlantic Apiculture Research and Extension Consortium found no evidence that pollen from Bt crops is adversely affecting bees. According to the USDA, \"Genetically modified (GM) crops, most commonly Bt corn, have been offered up as the cause of CCD. But there is no correlation between where GM crops are planted and the pattern of CCD incidents. Also, GM crops have been widely planted since the late 1990s, but CCD did not appear until 2006. In addition, CCD has been reported in countries that do not allow GM crops to be planted, such as Switzerland. German researchers have noted in one study a possible correlation between exposure to Bt pollen and compromised immunity to \"Nosema\".\" The actual cause of CCD was unknown in 2007, and scientists believe it may have multiple exacerbating causes.\nBeta-exotoxins.\nSome isolates of \"B. thuringiensis\" produce a class of insecticidal small molecules called beta-exotoxin, the common name for which is thuringiensin. A consensus document produced by the OECD says: \"Beta-exotoxins are known to be toxic to humans and almost all other forms of life and its presence is prohibited in \"B. thuringiensis\" microbial products\". Thuringiensins are nucleoside analogues. They inhibit RNA polymerase activity, a process common to all forms of life, in rats and bacteria alike.\nOther hosts.\nThis bacterium is an opportunistic pathogen of animals other than insects, causing necrosis, pulmonary infection, and/or food poisoning. It is unknown how common this is, because these infections are always taken to be \"B. cereus\" infections and are rarely tested for the \"Cry\" and \"Cyt\" proteins that are the only factor distinguishing \"B. thuringiensis\" from \"B. cereus\".\nNew nomenclature for pesticidal proteins (Bt toxins).\n\"Bacillus thuringiensis\" is no longer the sole source of pesticidal proteins. The Bacterial Pesticidal Protein Resource Center (BPPRC) provides information on the rapidly expanding field of pesticidal proteins for academics, regulators, and research and development personnel.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4185", "revid": "48060194", "url": "https://en.wikipedia.org/wiki?curid=4185", "title": "Bacteriophage", "text": "Virus that infects bacteria\nA bacteriophage (), also known informally as a phage (), is a virus that infects and replicates within bacteria. The term is derived from grc \" \"\" (phagein)\"\u00a0'to devour' and \" bacteria\". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm.\nBacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined. Viruses are the most abundant biological entity in the water column of the world's oceans, and the second largest component of biomass after prokaryotes, where up to 9x108 virions per millilitre have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by bacteriophages.\nBacteriophages were used from the 1920s as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France and Brazil. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria.\nBacteriophages are known to interact with the immune system both indirectly via bacterial expression of phage-encoded proteins and directly by influencing innate immunity and bacterial clearance. Phage\u2013host interactions are becoming increasingly important areas of research.\nClassification.\nBacterial viruses lack common ancestry and, for that reason, are classified in many unrelated taxa, listed hereafter:\nThe taxonomy of the aforementioned taxa can be visualized as follows, with bacterial virus taxa in bold:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nHistory.\nIn 1896, Ernest Hanbury Hankin reported that something in the waters of the Ganges and Yamuna rivers in India had a marked antibacterial action against cholera and it could pass through a very fine porcelain Chamberland filter. In 1915, British bacteriologist Frederick Twort, superintendent of the Brown Institution of London, discovered a small agent that infected and killed bacteria. He believed the agent must be one of the following:\nTwort's research was interrupted by the onset of World War I, as well as a shortage of funding and the discoveries of antibiotics.\nIndependently, French-Canadian microbiologist F\u00e9lix d'H\u00e9relle, working at the Pasteur Institute in Paris, announced on 3 September 1917 that he had discovered \"an invisible, antagonistic microbe of the dysentery bacillus\". For d'H\u00e9relle, there was no question as to the nature of his discovery: \"In a flash I had understood: what caused my clear spots was in fact an invisible microbe... a virus parasitic on bacteria.\" D'H\u00e9relle called the virus a bacteriophage, a bacterium-eater (from the Greek \"\", meaning \"to devour\"). He also recorded a dramatic account of a man suffering from dysentery who was restored to good health by the bacteriophages. It was d'H\u00e9relle who conducted much research into bacteriophages and introduced the concept of phage therapy. In 1919, in Paris, France, d'H\u00e9relle conducted the first clinical application of a bacteriophage, with the first reported use in the United States being in 1922.\nNobel prizes awarded for phage research.\nIn 1969, Max Delbr\u00fcck, Alfred Hershey, and Salvador Luria were awarded the Nobel Prize in Physiology or Medicine for their discoveries of the replication of viruses and their genetic structure. Specifically the work of Hershey, as contributor to the Hershey\u2013Chase experiment in 1952, provided convincing evidence that DNA, not protein, was the genetic material of life. Delbr\u00fcck and Luria carried out the Luria\u2013Delbr\u00fcck experiment which demonstrated statistically that mutations in bacteria occur randomly and thus follow Darwinian rather than Lamarckian principles.\nIn 2018, George Smith and Gregory Winter were awarded Nobel Prize in Chemistry for the phage display of peptides and antibodies.\nUses.\nPhage therapy.\nPhages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, F\u00e9lix d'H\u00e9relle) during the 1920s and 1930s for treating bacterial infections.\nD'Herelle \"quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients.\"\nThey had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:\nThe use of phages has continued since the end of the Cold War in Russia, Georgia, and elsewhere in Central and Eastern Europe. The first regulated, randomized, double-blind clinical trial was reported in the \"Journal of Wound Care\" in June 2009, which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous ulcers of the leg in human patients. The FDA approved the study as a Phase I clinical trial. The study's results demonstrated the safety of therapeutic application of bacteriophages, but did not show efficacy. The authors explained that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin or silver) may have interfered with bacteriophage viability. Shortly after that, another controlled clinical trial in Western Europe (treatment of ear infections caused by \"Pseudomonas aeruginosa\") was reported in the journal \"Clinical Otolaryngology\" in August 2009. The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis-associated lung infections, among others. On the other hand, phages of \"Inoviridae\" have been shown to complicate biofilms involved in pneumonia and cystic fibrosis and to shelter the bacteria from drugs meant to eradicate disease, thus promoting persistent infection.\nMeanwhile, bacteriophage researchers have been developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes that degrade the biofilm matrix, phage structural proteins, and the enzymes responsible for lysis of the bacterial cell wall. There have been results showing that T4 phages that are small in size and short-tailed can be helpful in detecting \"E. coli\" in the human body.\nTherapeutic efficacy of a phage cocktail was evaluated in a mouse model with nasal infection of multi-drug-resistant (MDR) \"A. baumannii\". Mice treated with the phage cocktail showed a 2.3-fold higher survival rate compared to those untreated at seven days post-infection.\nIn 2017, a 68-year-old diabetic patient with necrotizing pancreatitis complicated by a pseudocyst infected with MDR \"A. baumannii\" strains was being treated with a cocktail of Azithromycin, Rifampicin, and Colistin for 4 months without results and overall rapidly declining health.\nBecause discussion had begun of the clinical futility of further treatment, an Emergency Investigational New Drug (eIND) was filed as a last effort to at the very least gain valuable medical data from the situation, and approved, so he was subjected to phage therapy using a percutaneously (PC) injected cocktail containing nine different phages that had been identified as effective against the primary infection strain by rapid isolation and testing techniques (a process which took under a day). This proved effective for a very brief period, although the patient remained unresponsive and his health continued to worsen; soon isolates of a strain of \"A. baumannii\" were being collected from drainage of the cyst that showed resistance to this cocktail, and a second cocktail which was tested to be effective against this new strain was added, this time by intravenous (IV) injection as it had become clear that the infection was more pervasive than originally thought.\nOnce on the combination of the IV and PC therapy the patient's downward clinical trajectory reversed, and within two days he had awoken from his coma and become responsive. As his immune system began to function he had to be temporarily removed from the cocktail because his fever was spiking to over , but after two days the phage cocktails were re-introduced at levels he was able to tolerate. The original three-antibiotic cocktail was replaced by minocycline after the bacterial strain was found not to be resistant to this and he rapidly regained full lucidity, although he was not discharged from the hospital until roughly 145 days after phage therapy began. Towards the end of the therapy it was discovered that the bacteria had become resistant to both of the original phage cocktails, but they were continued because they seemed to be preventing minocycline resistance from developing in the bacterial samples collected so were having a useful synergistic effect.\nOther.\nFood industry.\nPhages have increasingly been used to safen food products and to forestall spoilage bacteria. Since 2006, the United States Food and Drug Administration (FDA) and United States Department of Agriculture (USDA) have approved several bacteriophage products. LMP-102 (Intralytix) was approved for treating ready-to-eat (RTE) poultry and meat products. In that same year, the FDA approved LISTEX (developed and produced by Micreos) using bacteriophages on cheese to kill \"Listeria monocytogenes\" bacteria, in order to give them generally recognized as safe (GRAS) status. In July 2007, the same bacteriophage were approved for use on all food products. In 2011 USDA confirmed that LISTEX is a clean label processing aid and is included in USDA. Research in the field of food safety is continuing to see if lytic phages are a viable option to control other food-borne pathogens in various food products.\nSwitzerland authorized a phage for use in cheese production in 2016. The European Union has not yet (2025) authorized any. \nWater indicators.\nBacteriophages, including those specific to \"Escherichia coli\", have been employed as indicators of fecal contamination in water sources. Due to their shared structural and biological characteristics, coliphages can serve as proxies for viral fecal contamination and the presence of pathogenic viruses such as rotavirus, norovirus, and HAV. Research conducted on wastewater treatment systems has revealed significant disparities in the behavior of coliphages compared to fecal coliforms, demonstrating a distinct correlation with the recovery of pathogenic viruses at the treatment's conclusion. Establishing a secure discharge threshold, studies have determined that discharges below 3000 PFU/100 mL are considered safe in terms of limiting the release of pathogenic viruses.\nDiagnostics.\nIn 2011, the FDA cleared the first bacteriophage-based product for in vitro diagnostic use. The KeyPath MRSA/MSSA Blood Culture Test uses a cocktail of bacteriophage to detect \"Staphylococcus aureus\" in positive blood cultures and determine methicillin resistance or susceptibility. The test returns results in about five hours, compared to two to three days for standard microbial identification and susceptibility test methods. It was the first accelerated antibiotic-susceptibility test approved by the FDA.\nCounteracting bioweapons and toxins.\nGovernment agencies in the West have for several years been looking to Georgia and the former Soviet Union for help with exploiting phages for counteracting bioweapons and toxins, such as anthrax and botulism. Developments are continuing among research groups in the U.S. Other uses include spray application in horticulture for protecting plants and vegetable produce from decay and the spread of bacterial disease. Other applications for bacteriophages are as biocides for environmental surfaces, e.g., in hospitals, and as preventative treatments for catheters and medical devices before use in clinical settings. The technology for phages to be applied to dry surfaces, e.g., uniforms, curtains, or even sutures for surgery now exists. Clinical trials reported in \"Clinical Otolaryngology\" show success in veterinary treatment of pet dogs with otitis.\nBacterium sensing and identification.\nThe sensing of phage-triggered ion cascades (SEPTIC) bacterium sensing and identification method uses the ion emission and its dynamics during phage infection and offers high specificity and speed for detection.\nPhage display.\nPhage display is a different use of phages involving a library of phages with a variable peptide linked to a surface protein. Each phage genome encodes the variant of the protein displayed on its surface (hence the name), providing a link between the peptide variant and its encoding gene. Variant phages from the library may be selected through their binding affinity to an immobilized molecule (e.g., botulism toxin) to neutralize it. The bound, selected phages can be multiplied by reinfecting a susceptible bacterial strain, thus allowing them to retrieve the peptides encoded in them for further study.\nAntimicrobial drug discovery.\nPhage proteins often have antimicrobial activity and may serve as leads for peptidomimetics, i.e. drugs that mimic peptides. Phage-ligand technology makes use of phage proteins for various applications, such as binding of bacteria and bacterial components (e.g. endotoxin) and lysis of bacteria.\nBasic research.\nBacteriophages are important model organisms for studying principles of evolution and ecology.\nAgriculture.\nPhages can be used to combat bacterial infections such as blackleg. A line of phage-based products is licensed in the United States, and Georgia has long used agricultural phages. Elsewhere, research and pilot testing are still underway. This is notably the case in Switzerland, where research is being conducted by the Fribourg School of Engineering and Architecture in collaboration with the Lausanne University Hospital (CHUV). \nDetriments.\nDairy industry.\nBacteriophages present in the environment can cause cheese to not ferment. In order to avoid this, mixed-strain starter cultures and culture rotation regimes can be used. Genetic engineering of culture microbes \u2013 especially \"Lactococcus lactis\" and \"Streptococcus thermophilus\" \u2013 have been studied for genetic analysis and modification to improve phage resistance. This has especially focused on plasmid and recombinant chromosomal modifications.\nSome research has focused on the potential of bacteriophages as antimicrobial against foodborne pathogens and biofilm formation within the dairy industry. As the spread of antibiotic resistance is a main concern within the dairy industry, phages can serve as a promising alternative.\nReplication.\nThe life cycle of bacteriophages tends to be either a lytic cycle or a lysogenic cycle. In addition, some phages display pseudolysogenic behaviors.\nWith \"lytic phages\" such as the T4 phage, bacterial cells are broken open (lysed) and destroyed after immediate replication of the virion. As soon as the cell is destroyed, the phage progeny can find new hosts to infect. Lytic phages are more suitable for phage therapy. Some lytic phages undergo a phenomenon known as lysis inhibition, where completed phage progeny will not immediately lyse out of the cell if extracellular phage concentrations are high. This mechanism is not identical to that of the temperate phage going dormant and usually is temporary.\nIn contrast, the \"lysogenic cycle\" does not result in immediate lysing of the host cell. Those phages able to undergo lysogeny are known as temperate phages. Their viral genome will integrate with host DNA and replicate along with it, relatively harmlessly, or may even become established as a plasmid. The virus remains dormant until host conditions deteriorate, perhaps due to depletion of nutrients, then, the endogenous phages (known as prophages) become active. At this point they initiate the reproductive cycle, resulting in lysis of the host cell. As the lysogenic cycle allows the host cell to continue to survive and reproduce, the virus is replicated in all offspring of the cell. An example of a bacteriophage known to follow the lysogenic cycle and the lytic cycle is the phage lambda of \"E. coli.\"\nSometimes prophages may provide benefits to the host bacterium while they are dormant by adding new functions to the bacterial genome, in a phenomenon called lysogenic conversion. Examples are the conversion of harmless strains of \"Corynebacterium diphtheriae\" or \"Vibrio cholerae\" by bacteriophages to highly virulent ones that cause diphtheria or cholera, respectively. Strategies to combat certain bacterial infections by targeting these toxin-encoding prophages have been proposed.\nAttachment and penetration.\nBacterial cells are protected by a cell wall of polysaccharides, which are important virulence factors protecting bacterial cells against both immune host defenses and antibiotics. \nHost growth conditions also influence the ability of the phage to attach and invade them. As phage virions do not move independently, they must rely on random encounters with the correct receptors when in solution, such as blood, lymphatic circulation, irrigation, soil water, etc.\nMyovirus bacteriophages use a hypodermic syringe-like motion to inject their genetic material into the cell. After contacting the appropriate receptor, the tail fibers flex to bring the base plate closer to the surface of the cell. This is known as reversible binding. Once attached completely, irreversible binding is initiated and the tail contracts, possibly with the help of ATP present in the tail, injecting genetic material through the bacterial membrane. The injection is accomplished through a sort of bending motion in the shaft by going to the side, contracting closer to the cell and pushing back up. Podoviruses lack an elongated tail sheath like that of a myovirus, so instead, they use their small, tooth-like tail fibers enzymatically to degrade a portion of the cell membrane before inserting their genetic material.\nSynthesis of proteins and nucleic acid.\nWithin minutes, bacterial ribosomes start translating viral mRNA into protein. For RNA-based phages, RNA replicase is synthesized early in the process. Proteins modify the bacterial RNA polymerase so it preferentially transcribes viral mRNA. The host's normal synthesis of proteins and nucleic acids is disrupted, and it is forced to manufacture viral products instead. These products go on to become part of new virions within the cell, helper proteins that contribute to the assemblage of new virions, or proteins involved in cell lysis. In 1972, Walter Fiers (University of Ghent, Belgium) was the first to establish the complete nucleotide sequence of a gene and in 1976, of the viral genome of bacteriophage MS2. Some dsDNA bacteriophages encode ribosomal proteins, which are thought to modulate protein translation during phage infection.\nVirion assembly.\nIn the case of the T4 phage, the construction of new virus particles involves the assistance of helper proteins that act catalytically during phage morphogenesis. The base plates are assembled first, with the tails being built upon them afterward. The head capsids, constructed separately, will spontaneously assemble with the tails. During assembly of the phage T4 virion, the morphogenetic proteins encoded by the phage genes interact with each other in a characteristic sequence. Maintaining an appropriate balance in the amounts of each of these proteins produced during viral infection appears to be critical for normal phage T4 morphogenesis. The DNA is packed efficiently within the heads. The whole process takes about 15 minutes.\nEarly studies of bactioriophage T4 (1962\u20131964) provided an opportunity to gain understanding of virtually all of the genes that are essential for growth of the bacteriophage under laboratory conditions. These studies were made possible by the availability of two classes of conditional lethal mutants. One class of such mutants was referred to as amber mutants. The other class of conditional lethal mutants was referred to as temperature-sensitive mutants Studies of these two classes of mutants led to considerable insight into the functions and interactions of the proteins employed in the machinery of DNA replication, repair and recombination, and on how viruses are assembled from protein and nucleic acid components (molecular morphogenesis).\nRelease of virions.\nPhages may be released via cell lysis, by extrusion, or, in a few cases, by budding. Lysis, by tailed phages, is achieved by an enzyme called endolysin, which attacks and breaks down the cell wall peptidoglycan. An altogether different phage type, the filamentous phage, makes the host cell continually secrete new virus particles. Released virions are described as free, and, unless defective, are capable of infecting a new bacterium. Budding is associated with certain \"Mycoplasma\" phages. In contrast to virion release, phages displaying a lysogenic cycle do not kill the host and instead become long-term residents as prophages.\nCommunication.\nResearch in 2017 revealed that the bacteriophage \u03a63T makes a short viral protein that signals other bacteriophages to lie dormant instead of killing the host bacterium. Arbitrium is the name given to this protein by the researchers who discovered it.\nGenome structure.\nGiven the millions of different phages in the environment, phage genomes come in a variety of forms and sizes. RNA phages such as MS2 have the smallest genomes, with only a few kilobases. However, some DNA phages such as T4 may have large genomes with hundreds of genes; the size and shape of the capsid varies along with the size of the genome. The largest bacteriophage genomes reach a size of 735 kb.Bacteriophage genomes can be highly mosaic, i.e. the genome of many phage species appear to be composed of numerous individual modules. These modules may be found in other phage species in different arrangements. Mycobacteriophages, bacteriophages with mycobacterial hosts, have provided excellent examples of this mosaicism. In these mycobacteriophages, genetic assortment may be the result of repeated instances of site-specific recombination and illegitimate recombination (the result of phage genome acquisition of bacterial host genetic sequences). Evolutionary mechanisms shaping the genomes of bacterial viruses vary between different families and depend upon the type of the nucleic acid, characteristics of the virion structure, as well as the mode of the viral life cycle.\nSome marine roseobacter phages, also known as roseophages, contain deoxyuridine (dU) instead of deoxythymidine (dT) in their genomic DNA. There is some evidence that this unusual component is a mechanism to evade bacterial defense mechanisms such as restriction endonucleases and CRISPR/Cas systems which evolved to recognize and cleave sequences within invading phages, thereby inactivating them. Other phages have long been known to use unusual nucleotides. In 1963, Takahashi and Marmur identified a \"Bacillus\" phage that has dU substituting dT in its genome, and in 1977, Kirnos et al. identified a cyanophage containing 2-aminoadenine (Z) instead of adenine (A).\nSystems biology.\nThe field of systems biology investigates the complex networks of interactions within an organism, usually using computational tools and modeling. For example, a phage genome that enters into a bacterial host cell may express hundreds of phage proteins which will affect the expression of numerous host genes or the host's metabolism. All of these complex interactions can be described and simulated in computer models.\nFor instance, infection of \"Pseudomonas aeruginosa\" by the temperate phage PaP3 changed the expression of 38% (2160/5633) of its host's genes. Many of these effects are probably indirect, hence the challenge becomes to identify the direct interactions among bacteria and phage.\nSeveral attempts have been made to map protein\u2013protein interactions among phage and their host. For instance, bacteriophage lambda was found to interact with its host, \"E. coli\", by dozens of interactions. Again, the significance of many of these interactions remains unclear, but these studies suggest that there most likely are several key interactions and many indirect interactions whose role remains uncharacterized.\nHost resistance and anti-phage defense.\nBacteriophages are a major threat to bacteria and prokaryotes have evolved numerous mechanisms to block infection (host resistance) or to block the replication of bacteriophages within host cells (anti-phage defense). Some examples include\nBacteriophage\u2013host symbiosis.\nTemperate phages are bacteriophages that integrate their genetic material into the host as extrachromosomal episomes or as a prophage during a lysogenic cycle. Some temperate phages can confer fitness advantages to their host in numerous ways, including giving antibiotic resistance through the transfer or introduction of antibiotic resistance genes (ARGs), protecting hosts from phagocytosis, protecting hosts from secondary infection through superinfection exclusion, enhancing host pathogenicity, or enhancing bacterial metabolism or growth. Bacteriophage\u2013host symbiosis may benefit bacteria by providing selective advantages while passively replicating the phage genome.\nIn the environment.\nMetagenomics has allowed the in-water detection of bacteriophages that was not possible previously.\nAlso, bacteriophages have been used in hydrological tracing and modelling in river systems, especially where surface water and groundwater interactions occur. The use of phages is preferred to the more conventional dye marker because they are significantly less absorbed when passing through ground waters and they are readily detected at very low concentrations. Non-polluted water may contain approximately 2\u00d7108 bacteriophages per ml.\nBacteriophages are thought to contribute extensively to horizontal gene transfer in natural environments, principally via transduction, but also via transformation. Metagenomics-based studies also have revealed that viromes from a variety of environments harbor antibiotic-resistance genes, including those that could confer multidrug resistance.\nRecent findings have mapped the complex and intertwined arsenal of anti-phage defense tools in environmental bacteria.\nIn humans.\nAlthough phages do not infect humans, there are countless phage particles in the human body, given the extensive human microbiome. One's phage population has been called the human phageome, including the \"healthy gut phageome\" (HGP) and the \"diseased human phageome\" (DHP). The active phageome of a healthy human (i.e., actively replicating as opposed to nonreplicating, integrated prophage) has been estimated to comprise dozens to thousands of different viruses.\nThere is evidence that bacteriophages and bacteria interact in the human gut microbiome both antagonistically and beneficially.\nPreliminary studies have indicated that common bacteriophages are found in 62% of healthy individuals on average, while their prevalence was reduced by 42% and 54% on average in patients with ulcerative colitis (UC) and Crohn's disease (CD). Abundance of phages may also decline in the elderly.\nThe most common phages in the human intestine, found worldwide, are crAssphages. CrAssphages are transmitted from mother to child soon after birth, and there is some evidence suggesting that they may be transmitted locally. Each person develops their own unique crAssphage clusters. CrAss-like phages also may be present in primates besides humans.\nCommonly studied bacteriophages.\nAmong the countless phages, only a few have been studied in detail, including some historically important phage that were discovered in the early days of microbial genetics. These, especially the T-phage, helped to discover important principles of gene structure and function.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4186", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4186", "title": "Bacteriostat", "text": ""}
{"id": "4187", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=4187", "title": "Bactericide", "text": "Agent which kills bacteria\nA bactericide or bacteriocide, sometimes abbreviated Bcidal, is a substance which kills bacteria. Bactericides are disinfectants, antiseptics, or antibiotics.\nHowever, material surfaces can also have bactericidal properties based solely on their physical surface structure, as for example biomaterials like insect wings.\nDisinfectants.\nThe most used disinfectants are those applying\nAntiseptics.\nAs antiseptics (i.e., germicide agents that can be used on human or animal body, skin, mucosae, wounds and the like), few of the above-mentioned disinfectants can be used, under proper conditions (mainly concentration, pH, temperature and toxicity toward humans and animals). Among them, some important are\nOthers are generally not applicable as safe antiseptics, either because of their corrosive or toxic nature.\nAntibiotics.\nBactericidal antibiotics kill bacteria; bacteriostatic antibiotics slow their growth or reproduction.\nBactericidal antibiotics that inhibit cell wall synthesis: the beta-lactam antibiotics (penicillin derivatives (penams), cephalosporins (cephems), monobactams, and carbapenems) and vancomycin.\nAlso bactericidal are daptomycin, fluoroquinolones, metronidazole, nitrofurantoin, co-trimoxazole, telithromycin.\nAminoglycosidic antibiotics are usually considered bactericidal, although they may be bacteriostatic with some organisms.\nAs of 2004, the distinction between bactericidal and bacteriostatic agents appeared to be clear according to the basic/clinical definition, but this only applies under strict laboratory conditions and it is important to distinguish microbiological and clinical definitions. The distinction is more arbitrary when agents are categorized in clinical situations. The supposed superiority of bactericidal agents over bacteriostatic agents is of little relevance when treating the vast majority of infections with gram-positive bacteria, particularly in patients with uncomplicated infections and noncompromised immune systems. Bacteriostatic agents have been effectively used for treatment that are considered to require bactericidal activity. Furthermore, some broad classes of antibacterial agents considered bacteriostatic can exhibit bactericidal activity against some bacteria on the basis of in vitro determination of MBC/MIC values. At high concentrations, bacteriostatic agents are often bactericidal against some susceptible organisms. The ultimate guide to treatment of any infection must be clinical outcome.\nSurfaces.\nMaterial surfaces can exhibit bactericidal properties because of their crystallographic surface structure.\nSomewhere in the mid-2000s it was shown that metallic nanoparticles can kill bacteria. The effect of a silver nanoparticle for example depends on its size with a preferential diameter of about 1\u201310\u00a0nm to interact with bacteria.\nIn 2013, cicada wings were found to have a selective anti-gram-negative bactericidal effect based on their physical surface structure. Mechanical deformation of the more or less rigid nanopillars found on the wing releases energy, striking and killing bacteria within minutes, hence called a mechano-bactericidal effect.\nIn 2020 researchers combined cationic polymer adsorption and femtosecond laser surface structuring to generate a bactericidal effect against both gram-positive \"Staphylococcus aureus\" and gram-negative \"Escherichia coli\" bacteria on borosilicate glass surfaces, providing a practical platform for the study of the bacteria-surface interaction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4188", "revid": "37371155", "url": "https://en.wikipedia.org/wiki?curid=4188", "title": "Brion Gysin", "text": "British-Canadian painter, writer, sound poet, and performance artist (1916\u20131986)\nBrion Gysin (19 January 1916 \u2013 13 July 1986) was a British-Canadian painter, writer, sound poet, performance artist and inventor of experimental devices.\nHe is best known for his use of the cut-up technique, alongside his close friend, the novelist William S. Burroughs. With the engineer Ian Sommerville he also invented the Dreamachine, a flicker device designed as an art object to be viewed with the eyes closed. It was in painting and drawing, however, that Gysin devoted his greatest efforts, creating calligraphic works inspired by cursive Japanese \"grass\" script and Arabic script. Burroughs later stated that \"Brion Gysin was the only man I ever respected.\"\nBiography.\nEarly years.\nJohn Clifford Brian Gysin was born at the Canadian military hospital in Taplow, Buckinghamshire, England. His mother, Stella Margaret Martin, was a Canadian from Deseronto, Ontario. His father, Leonard Gysin, a captain with the Canadian Expeditionary Force, was killed in action eight months after his son's birth. Stella returned to Canada and settled in Edmonton, Alberta where her son became \"the only Catholic day-boy at an Anglican boarding school\". Leaving that school at the age of fifteen, Gysin was sent next to Downside School in Stratton-on-the-Fosse, near Bath in England, a prestigious school for boys run by Benedictine monks. Despite attending both Anglican and Roman Catholic schools, Gysin was already an atheist when he left St Joseph's.\nSurrealism.\nIn 1934, he moved to Paris to study \"La Civilisation Fran\u00e7aise\", an open course given at the Sorbonne where he made literary and artistic contacts through Marie Berthe Aurenche, Max Ernst's second wife. He joined the Surrealist Group and began associating with Valentine Hugo, Leonor Fini, Salvador Dal\u00ed, Picasso and Dora Maar. A year later, he had his first exhibition at the \"Gal\u00e9rie Quatre Chemins\" in Paris with Ernst, Picasso, Hans Arp, Hans Bellmer, Victor Brauner, Giorgio de Chirico, Dal\u00ed, Marcel Duchamp, Ren\u00e9 Magritte, Man Ray and Yves Tanguy. On the day of the preview, however, he was expelled from the Surrealist Group by Andr\u00e9 Breton, who ordered the poet Paul \u00c9luard to take down his pictures. Gysin was 19 years old. His biographer, John Geiger, suggests the arbitrary expulsion \"had the effect of a curse. Years later, he blamed other failures on the Breton incident. It gave rise to conspiracy theories about the powerful interests who seek control of the art world. He gave various explanations for the expulsion, the more elaborate involving 'insubordination' or \"l\u00e8se majest\u00e9\" towards Breton\".\nAfter World War II.\nAfter serving in the U.S. Army during World War II, Gysin published a biography of Josiah \"Uncle Tom\" Henson titled, \"To Master, a Long Goodnight: The History of Slavery in Canada\" (1946). A gifted draughtsman, he took an 18-month course learning the Japanese language (including calligraphy) that would greatly influence his artwork. In 1949, he was among the first Fulbright Fellows. His goal was to research, at the University of Bordeaux and in the Archivo de Indias in Seville, Spain, the history of slavery, a project that he later abandoned. He moved to Tangier, Morocco, after visiting the city with novelist and composer Paul Bowles in 1950. In 1952/3 he met the travel writer and sexual adventurer Anne Cumming and they remained friends until his death.\nMorocco and the Beat Hotel.\nIn 1954 in Tangier, Gysin opened a restaurant called The 1001 Nights, with his friend Mohamed Hamri, who was the cook. Gysin hired the Master Musicians of Jajouka from the village of Jajouka to perform alongside entertainment that included acrobats, a dancing boy and fire eaters. The musicians performed there for an international clientele that included William S. Burroughs. Gysin lost the business in 1958, and the restaurant closed permanently. That same year, Gysin returned to Paris, taking lodgings in a flophouse located at 9 rue G\u00eet-le-C\u0153ur that would become famous as the Beat Hotel. Working on a drawing, he discovered a Dada technique by accident:\nWilliam Burroughs and I first went into techniques of writing, together, back in room No. 15 of the Beat Hotel during the cold Paris spring of 1958... Burroughs was more intent on Scotch-taping his photos together into one great continuum on the wall, where scenes faded and slipped into one another, than occupied with editing the monster manuscript... \"Naked Lunch\" appeared and Burroughs disappeared. He kicked his habit with Apomorphine and flew off to London to see Dr Dent, who had first turned him on to the cure. While cutting a mount for a drawing in room No. 15, I sliced through a pile of newspapers with my Stanley blade and thought of what I had said to Burroughs some six months earlier about the necessity for turning painters' techniques directly into writing. I picked up the raw words and began to piece together texts that later appeared as \"First Cut-Ups\" in \"Minutes to Go\" (Two Cities, Paris 1960).\nWhen Burroughs returned from London in September 1959, Gysin not only shared his discovery with his friend but the new techniques he had developed for it. Burroughs then put the techniques to use while completing \"Naked Lunch\" and the experiment dramatically changed the landscape of American literature. Gysin helped Burroughs with the editing of several of his novels including \"Interzone\", and wrote a script for a film version of \"Naked Lunch\", which was never produced. The pair collaborated on a large manuscript for Grove Press titled \"The Third Mind\", but it was determined that it would be impractical to publish it as originally envisioned. The book later published under that title incorporates little of this material. Interviewed for \"The Guardian\" in 1997, Burroughs explained that Gysin was \"the only man that I've ever respected in my life. I've admired people, I've liked them, but he's the only man I've ever respected.\" In 1969, Gysin completed his finest novel, \"The Process\", a work judged by critic Robert Palmer as \"a classic of 20th century modernism\".\nA consummate innovator, Gysin altered the cut-up technique to produce what he called permutation poems in which a single phrase was repeated several times with the words rearranged in a different order with each reiteration. An example of this is \"I don't dig work, man / Man, work I don't dig.\" Many of these permutations were derived using a random sequence generator in an early computer program written by Ian Sommerville. Commissioned by the BBC in 1960 to produce material for broadcast, Gysin's results included \"Pistol Poem\", which was created by recording a gun firing at different distances and then splicing the sounds. That year, the piece was subsequently used as a theme for the Paris performance of Le Domaine Poetique, a showcase for experimental works by people like Gysin, Fran\u00e7ois Dufr\u00eane, Bernard Heidsieck, and Henri Chopin.\nWith Sommerville, he built the Dreamachine in 1961. Described as \"the first art object to be seen with the eyes closed\", the flicker device uses alpha waves in the 8\u201316 Hz range to produce a change of consciousness in receptive viewers.\nLater years.\nIn April 1974, while sitting at a social engagement, Gysin had a very noticeable rectal bleeding. In May he wrote to Burroughs complaining he was not feeling well. A short time later he was diagnosed with colon cancer and began to receive cobalt treatment. Between December 1974 and April 1975, Gysin had to undergo several surgeries, among them a very traumatic colostomy, that drove him to extreme depression and to a suicide attempt. Later, in \"Fire: Words by Day \u2013 Images by Night\" (1975), a crudely lucid text, he would describe the horrendous ordeal he went through.\nIn 1985 Gysin was made an American Commander of the French Ordre des Arts et des Lettres. He'd begun to work extensively with noted jazz soprano saxophonist Steve Lacy. They recorded an album in 1986 with French musician Ramuntcho Matta, featuring Gysin singing/rapping his own texts, with performances by Lacy, Don Cherry, Elli Medeiros, Lizzy Mercier Descloux and more. The album was reissued on CD in 1993 by Crammed Discs, under the title \"Self-Portrait Jumping\".\nDeath.\nOn 13 July 1986 Brion Gysin died of lung cancer. Anne Cumming arranged his funeral and for his ashes to be scattered at the Caves of Hercules in Morocco. An obituary by Robert Palmer published in \"The New York Times\" described him as a man who \"threw off the sort of ideas that ordinary artists would parlay into a lifetime career, great clumps of ideas, as casually as a locomotive throws off sparks\". Later that year a heavily edited version of his novel, \"The Last Museum\", was published posthumously by Faber &amp; Faber (London) and by Grove Press (New York).\nAs a joke, Gysin had contributed a recipe for marijuana fudge to a cookbook by Alice B. Toklas; it was included for publication, becoming famous under the name Alice B. Toklas brownies.\nBurroughs on the Gysin cut-up.\nIn a 1966 interview by Conrad Knickerbocker for \"The Paris Review\", William S. Burroughs explained that Brion Gysin was, to his knowledge, \"the first to create cut-ups\":\nA friend, Brion Gysin, an American poet and painter, who has lived in Europe for thirty years, was, as far as I know, the first to create cut-ups. His cut-up poem, \"Minutes to Go\", was broadcast by the BBC and later published in a pamphlet. I was in Paris in the summer of 1960; this was after the publication there of \"Naked Lunch\". I became interested in the possibilities of this technique, and I began experimenting myself. Of course, when you think of it, \"The Waste Land\" was the first great cut-up collage, and Tristan Tzara had done a bit along the same lines. Dos Passos used the same idea in 'The Camera Eye' sequences in \"USA\". I felt I had been working toward the same goal; thus it was a major revelation to me when I actually saw it being done.\nInfluence.\nAccording to Jos\u00e9 F\u00e9rez Kuri, author of \"Brion Gysin: Tuning in to the Multimedia Age\" (2003) and co-curator of a major retrospective of the artist's work at The Edmonton Art Gallery in 1998, Gysin's wide range of \"radical ideas would become a source of inspiration for artists of the Beat Generation, as well as for their successors (among them David Bowie, Mick Jagger, Keith Haring, and Laurie Anderson)\". Other artists include Genesis P-Orridge, John Zorn (as displayed on the 2013's Dreamachines album) and Brian Jones.\nSelected bibliography.\nGysin is the subject of John Geiger's biography, \"Nothing Is True Everything Is Permitted: The Life of Brion Gysin\", and features in \"Chapel of Extreme Experience: A Short History of Stroboscopic Light and the Dream Machine\", also by Geiger. \"Man From Nowhere: Storming the Citadels of Enlightenment with William Burroughs and Brion Gysin\", a biographical study of Burroughs and Gysin with a collection of homages to Gysin, was authored by Joe Ambrose, Frank Rynne, and Terry Wilson with contributions by Marianne Faithfull, John Cale, William S. Burroughs, John Giorno, Stanley Booth, Bill Laswell, Mohamed Hamri, Keith Haring and Paul Bowles. A monograph on Gysin was published in 2003 by Thames and Hudson.\nWorks.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4190", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=4190", "title": "Bulgarian", "text": "Bulgarian may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "4191", "revid": "1839637", "url": "https://en.wikipedia.org/wiki?curid=4191", "title": "BCG vaccine", "text": "Vaccine primarily used against tuberculosis\n&lt;templatestyles src=\"Infobox drug/styles.css\"/&gt;\nThe Bacillus Calmette\u2013Gu\u00e9rin (BCG) vaccine is a vaccine primarily used against tuberculosis (TB). It is named after its inventors Albert Calmette and Camille Gu\u00e9rin. In countries where tuberculosis or leprosy is common, one dose is recommended in healthy babies as soon after birth as possible. In areas where tuberculosis is not common, only children at high risk are typically immunized, while suspected cases of tuberculosis are individually tested for and treated. Adults who are frequently exposed to tuberculosis may also be immunized. BCG has some effectiveness against Buruli ulcer infection and other nontuberculous mycobacterial infections. It is also often used as part of the treatment of bladder cancer.\nRates of protection against tuberculosis infection vary widely and protection lasts up to 20 years. Among children, it prevents about 20% from getting infected and among those who do get infected, it protects half from developing disease. The vaccine is injected into the skin. No evidence shows that additional doses are beneficial.\nSerious side effects are rare. Redness, swelling, and mild pain often occur at the injection site. A small ulcer may also form with some scarring after healing. Side effects are more common and potentially more severe in those with immunosuppression. Although no harmful effects on the fetus have been observed, there is insufficient evidence about the safety of BCG vaccination during pregnancy. Therefore, the vaccine is not recommended for use during pregnancy. The vaccine was originally developed from \"Mycobacterium bovis\", which is commonly found in cattle. Although it has been weakened, it is still live.\nThe BCG vaccine was first used medically in 1921. It is on the World Health Organization's List of Essential Medicines. As of 2004[ [update]], the vaccine is given to about 100 million children per year globally. However, it is not commonly administered in the United States.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nMedical uses.\nTuberculosis.\nThe main use of BCG is for vaccination against tuberculosis. BCG vaccine can be administered after birth intradermally. BCG vaccination can cause a false positive Mantoux test.\nThe most controversial aspect of BCG is the variable efficacy found in different clinical trials, which appears to depend on geography. Trials in the UK consistently show a 60 to 80% protective effect. Still, those trials conducted elsewhere have shown no protective effect, and efficacy appears to fall the closer one gets to the equator.\nA 1994 systematic review found that BCG reduces the risk of getting tuberculosis by about 50%. Differences in effectiveness depend on region, due to factors such as genetic differences in the populations, changes in environment, exposure to other bacterial infections, and conditions in the laboratory where the vaccine is grown, including genetic differences between the strains being cultured and the choice of growth medium.\nA systematic review and meta-analysis conducted in 2014 demonstrated that the BCG vaccine reduced infections by 19\u201327% and reduced progression to active tuberculosis by 71%. The studies included in this review were limited to those that used interferon gamma release assay.\nThe duration of protection of BCG is not clearly known. In those studies showing a protective effect, the data are inconsistent. The MRC study showed protection waned to 59% after 15 years and to zero after 20 years; however, a study looking at Native Americans immunized in the 1930s found evidence of protection even 60 years after immunization, with only slightly waning in efficacy.\nBCG seems to have its greatest effect in preventing miliary tuberculosis or tuberculosis meningitis, so it is still extensively used even in countries where efficacy against pulmonary tuberculosis is negligible.\nThe 100th anniversary of the BCG vaccine was in 2021. It remains the only vaccine licensed against tuberculosis, which is an ongoing pandemic. Tuberculosis elimination is a goal of the World Health Organization (WHO). The development of new vaccines with greater efficacy against adult pulmonary tuberculosis may be needed to make substantial progress.\nEfficacy.\nSeveral possible reasons for the variable efficacy of BCG in different countries have been proposed. None has been proven, some have been disproved, and none can explain the lack of efficacy in low tuberculosis-burden countries (US) and high tuberculosis-burden countries (India). The reasons for variable efficacy have been discussed at length in a WHO document on BCG.\nMycobacteria.\nBCG has protective effects against some nontuberculosis mycobacteria.\nCancer.\nBCG has been one of the most successful immunotherapies. BCG vaccine has been the \"standard of care for patients with bladder cancer (NMIBC)\" since 1977. By 2014, more than eight different considered biosimilar agents or strains used to treat nonmuscle-invasive bladder cancer.\nMethod of administration.\nA pre-injection tuberculin skin test is usually carried out before administering the BCG vaccine. A reactive tuberculin skin test is a contraindication to BCG due to the risk of severe local inflammation and scarring; it does not indicate immunity. BCG is also contraindicated in certain people who have IL-12 receptor pathway defects.\nBCG is given as a single intradermal injection at the insertion of the deltoid. If BCG is accidentally given subcutaneously, then a local abscess may form (a \"BCG-oma\") that can sometimes ulcerate, and may require treatment with antibiotics immediately, otherwise without treatment it could spread the infection, causing severe damage to vital organs. An abscess is not always associated with incorrect administration, and it is one of the more common complications that can occur with the vaccination. Numerous medical studies on the treatment of these abscesses with antibiotics have been done with varying results, but the consensus is once pus is aspirated and analysed, provided no unusual bacilli are present, the abscess will generally heal on its own in a matter of weeks.\nThe characteristic raised scar that BCG immunization leaves is often used as proof of prior immunization. This scar must be distinguished from that of smallpox vaccination, which it may resemble.\nWhen given for bladder cancer, the vaccine is not injected through the skin but is instilled into the bladder through the urethra using a soft catheter.\nAdverse effects.\nBCG immunization generally causes some pain and scarring at the site of injection during infancy. The mechanism of this side effect is not fully understood, however it can be mitigated by performing the vaccination later in life. Usually wealthier regions have lower scar rate among the population, such as Western Europe, unlike its Eastern counterpart, where the BCG scar is considered widespread. The insertion to the deltoid muscle is typically used because the local complication rate is smallest for that site. In most countries it is the left shoulder, although some, like Brazil, administer the vaccination to the right. Nonetheless, the buttock is an alternative site of administration because it provides better cosmetic outcomes.\nBCG vaccine should be given intradermally. If given subcutaneously, it may induce local infection and spread to the regional lymph nodes, causing either suppurative (production of pus) or nonsuppurative lymphadenitis. Conservative management is usually adequate for nonsuppurative lymphadenitis. If suppuration occurs, it may need needle aspiration. For unresolved suppuration, surgical excision may be required. Evidence for the treatment of these complications is scarce.\nUncommonly, breast and gluteal abscesses can occur due to haematogenous (carried by the blood) and lymphangiomatous spread. Regional bone infection (BCG osteomyelitis or osteitis) and disseminated BCG infection are rare complications of BCG vaccination, but potentially life-threatening. Systemic antituberculous therapy may be helpful in severe complications.\nWhen BCG is used for bladder cancer, around 2.9% of treated patients discontinue immunotherapy due to a genitourinary or systemic BCG-related infection, however while symptomatic bladder BCG infection is frequent, the involvement of other organs is very uncommon. When systemic involvement occurs, liver and lungs are the first organs to be affected (1 week [median] after the last BCG instillation).\nIf BCG is accidentally given to an immunocompromised patient (e.g., an infant with severe combined immune deficiency), it can cause disseminated or life-threatening infection. The documented incidence of this happening is less than one per million immunizations given. In 2007, the WHO stopped recommending BCG for infants with HIV, even if the risk of exposure to tuberculosis is high, because of the risk of disseminated BCG infection (which is roughly 400 per 100,000 in that higher risk context).\nUsage.\nThe person's age and the frequency with which BCG is given have always varied from country to country. The WHO recommends childhood BCG for all countries with a high incidence of tuberculosis and/or high leprosy burden. This is a partial list of historic and active BCG practices around the globe. A complete atlas of past and present practice has been generated. As of 2022, 155 countries offer the BCG vaccine in their schedule.\nManufacture.\nBCG is prepared from a strain of the attenuated (virulence-reduced) live bovine tuberculosis bacillus, \"Mycobacterium bovis\", that has lost its ability to cause disease in humans. It is specially subcultured in a culture medium, usually Middlebrook 7H9. Because the living bacilli evolve to make the best use of available nutrients, they become less well-adapted to human blood and can no longer induce disease when introduced into a human host. Still, they are similar enough to their wild ancestors to provide some immunity against human tuberculosis. The BCG vaccine can be anywhere from 0 to 80% effective in preventing tuberculosis for 15 years; however, its protective effect appears to vary according to geography and the lab in which the vaccine strain was grown.\nSeveral companies make BCG, sometimes using different genetic strains of the bacterium. This may result in different product characteristics. OncoTICE, used for bladder instillation for bladder cancer, was developed by Organon Laboratories (since acquired by Schering-Plough, and in turn acquired by Merck &amp; Co.). A similar application is the product of Onko BCG of the Polish company Biomed-Lublin, which owns the Brazilian substrain M. bovis BCG Moreau which is less reactogenic than vaccines including other BCG strains. Pacis BCG, made from the Montr\u00e9al (Institut Armand-Frappier) strain, was first marketed by Urocor in about 2002. Urocor was since acquired by Dianon Systems. Evans Vaccines (a subsidiary of PowderJect Pharmaceuticals). Statens Serum Institut in Denmark has marketed a BCG vaccine prepared using Danish strain 1331. The production of BCG Danish strain 1331 and its distribution was later undertaken by AJVaccines company since the ownership transfer of SSI's vaccine production business to AJ Vaccines Holding A/S which took place on 16 January 2017. Japan BCG Laboratory markets its vaccine, based on the Tokyo 172 substrain of Pasteur BCG, in 50 countries worldwide.\nAccording to a UNICEF report published in December 2015, on BCG vaccine supply security, global demand increased in 2015 from 123 to 152.2\u00a0million doses. To improve security and to [diversify] sources of affordable and flexible supply,\" UNICEF awarded seven new manufacturers contracts to produce BCG. Along with supply availability from existing manufacturers, and a \"new WHO prequalified vaccine\" the total supply will be \"sufficient to meet both suppressed 2015 demand carried over to 2016, as well as total forecast demand through 2016\u20132018.\"\nSupply shortage.\nIn 2011, the Sanofi Pasteur plant flooded, causing problems with mold. The facility, located in Toronto, Ontario, Canada, produced BCG vaccine products made with substrain Connaught such as a tuberculosis vaccine and ImmuCYST, a BCG immunotherapeutic and bladder cancer drug. By April 2012 the FDA had found dozens of documented problems with sterility at the plant including mold, nesting birds and rusted electrical conduits. The resulting closure of the plant for over two years caused shortages of bladder cancer and tuberculosis vaccines. On 29 October 2014 Health Canada gave the permission for Sanofi to resume production of BCG. A 2018 analysis of the global supply concluded that the supplies are adequate to meet forecast BCG vaccine demand, but that risks of shortages remain, mainly due to dependence of 75 percent of WHO pre-qualified supply on just two suppliers.\nDried.\nSome BCG vaccines are freeze dried and become fine powder. Sometimes the powder is sealed with vacuum in a glass ampoule. Such a glass ampoule has to be opened slowly to prevent the airflow from blowing out the powder. Then the powder has to be diluted with saline water before injecting.\nHistory.\nThe history of BCG is tied to that of smallpox. By 1865 Jean Antoine Villemin had demonstrated that rabbits could be infected with tuberculosis from humans; by 1868 he had found that rabbits could be infected with tuberculosis from cows and that rabbits could be infected with tuberculosis from other rabbits. Thus, he concluded that tuberculosis was transmitted via some unidentified microorganism (or \"virus\", as he called it). In 1882 Robert Koch regarded human and bovine tuberculosis as identical. But in 1895, Theobald Smith presented differences between human and bovine tuberculosis, which he reported to Koch. By 1901 Koch distinguished \"Mycobacterium bovis\" from \"Mycobacterium tuberculosis\". Following the success of vaccination in preventing smallpox, established during the 18th century, scientists thought to find a corollary in tuberculosis by drawing a parallel between bovine tuberculosis and cowpox: it was hypothesized that infection with bovine tuberculosis might protect against infection with human tuberculosis. In the late 19th century, clinical trials using \"M. bovis\" were conducted in Italy with disastrous results, because \"M. bovis\" was found to be just as virulent as \"M. tuberculosis\".\nAlbert Calmette, a French physician and bacteriologist, and his assistant and later colleague, Camille Gu\u00e9rin, a veterinarian, were working at the Institut Pasteur de Lille (Lille, France) in 1908. Their work included subculturing virulent strains of the tuberculosis bacillus and testing different culture media. They noted a glycerin-bile-potato mixture grew bacilli that seemed less virulent and changed the course of their research to see if repeated subculturing would produce a strain that was attenuated enough to be considered for use as a vaccine. The BCG strain was isolated after subculturing 239 times during 13 years from a virulent strain on glycerine potato medium. The research continued throughout World War I until 1919 when the now avirulent bacilli were unable to cause tuberculosis disease in research animals. Calmette and Guerin transferred to the Paris Pasteur Institute in 1919. The BCG vaccine was first used in humans in 1921.\nPublic acceptance was slow, and the L\u00fcbeck disaster, in particular, did much to harm it. Between 1929 and 1933 in L\u00fcbeck, 251 infants were vaccinated in the first 10 days of life; 173 developed tuberculosis and 72 died. It was subsequently discovered that the BCG administered there had been contaminated with a virulent strain that was being stored in the same incubator, which led to legal action against the vaccine's manufacturers.\nDr. R. G. Ferguson, working at the Fort Qu'Appelle Sanatorium in Saskatchewan, was among the pioneers in developing the practice of vaccination against tuberculosis. In Canada, more than 600 children from residential schools were used as involuntary participants in BCG vaccine trials between 1933 and 1945. In 1928, the BCG vaccine was adopted by the Health Committee of the League of Nations (predecessor to the World Health Organization (WHO)). Because of opposition, however, it only became widely used after World War II. From 1945 to 1948, relief organizations (International Tuberculosis Campaign or Joint Enterprises) vaccinated over eight million babies in Eastern Europe and prevented the predicted typical increase of tuberculosis after a major war.\nThe BCG vaccine is very efficacious against tuberculous meningitis in the pediatric age group, but its efficacy against pulmonary tuberculosis appears variable. Some countries have removed the BCG vaccine from routine vaccination. Two countries that have never used it routinely are the United States and the Netherlands (in both countries, it is felt that having a reliable Mantoux test and therefore being able to accurately detect active disease is more beneficial to society than vaccinating against a relatively rare condition).\nOther names include \"Vaccin Bili\u00e9 de Calmette et Gu\u00e9rin vaccine\" and \"Bacille de Calmette et Gu\u00e9rin vaccine\".\nResearch.\nTentative evidence exists for a beneficial non-specific effect of BCG vaccination on overall mortality in low-income countries, or for its reducing other health problems including sepsis and respiratory infections when given early, with greater benefit the earlier it is used.\nIn rhesus macaques, BCG shows improved rates of protection when given intravenously. Some risks must be evaluated before it can be translated to humans.\nThe University of Oxford Jenner Institute is conducting a study comparing the efficacy of injected versus inhaled BCG vaccine in already-vaccinated adults.\nType 1 diabetes.\nAs of 2017[ [update]], BCG vaccine is in the early stages of being studied in type 1 diabetes (T1D).\nCOVID-19.\nUse of the BCG vaccine may provide protection against COVID-19. However, epidemiologic observations in this respect are ambiguous. The WHO does not recommend its use for prevention as of 2021[ [update]].\nAs of \u00a02021[ [update]], 20 BCG trials are in various clinical stages. As of \u00a02022[ [update]], the results are extremely mixed. A 15-month trial involving people thrice-vaccinated over the two years before the pandemic shows positive results in preventing infection in BCG-naive people with type 1 diabetes. On the other hand, a 5-month trial shows that re-vaccinating with BCG does not help prevent infection in healthcare workers. Both of these trials were double-blind randomized controlled trials.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4192", "revid": "1450053", "url": "https://en.wikipedia.org/wiki?curid=4192", "title": "Bunsen", "text": "Bunsen may refer to:\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Surname listThis page lists people with the surname . "}
{"id": "4193", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4193", "title": "Common buzzard", "text": "Species of bird of prey\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nThe common buzzard (Buteo buteo) is a medium-to-large bird of prey which has a large range. It is a member of the genus \"Buteo\" in the family Accipitridae. The species lives in most of Europe and extends its breeding range across much of the Palearctic as far as northwestern China (Tian Shan), far western Siberia and northwestern Mongolia. Over much of its range, it is a year-round resident. However, buzzards from the colder parts of the Northern Hemisphere as well as those that breed in the eastern part of their range typically migrate south for the northern winter, many journeying as far as South Africa.\nThe common buzzard is an opportunistic predator that can take a wide variety of prey, but it feeds mostly on small mammals, especially rodents such as voles. It typically hunts from a perch. Like most accipitrid birds of prey, it builds a nest, typically in trees in this species, and is a devoted parent to a relatively small brood of young. The common buzzard appears to be the most common diurnal raptor in Europe, as estimates of its total global population run well into the millions.\nTaxonomy.\nThe first formal description of the common buzzard was by the Swedish naturalist Carl Linnaeus in 1758 in the tenth edition of his \"Systema Naturae\" under the binomial name \"Falco buteo\". The genus \"Buteo\" was introduced by the French naturalist Bernard Germain de Lac\u00e9p\u00e8de in 1799 by tautonymy with the specific name of this species. The word \"buteo\" is Latin for a buzzard. It should not be confused with the Turkey vulture, which is sometimes called a buzzard in American English.\nThe Buteoninae subfamily originated from and is most diversified in the Americas, with occasional broader radiations that led to common buzzards and other Eurasian and African buzzards. The common buzzard is a member of the genus \"Buteo\", a group of medium-sized raptors with robust bodies and broad wings. The \"Buteo\" species of Eurasia and Africa are usually commonly referred to as \"buzzards\" while those in the Americas are called hawks. Under current classification, the genus includes approximately 28 species, the second most diverse of all extant accipitrid genera behind only \"Accipiter\". DNA testing shows that the common buzzard is fairly closely related to the red-tailed hawk (\"Buteo jamaicensis\") of North America, which occupies a similar ecological niche to the buzzard in that continent. The two species may belong to the same species complex. Three buzzards in Africa are likely closely related to the common buzzard based on genetic materials, the Mountain buzzard (\"Buteo oreophilus\"), Forest buzzards (\"Buteo trizonatus\") and the Madagascar buzzard (\"Buteo brachypterus\"), to the point where it has been questioned whether they are sufficiently distinct to qualify as full species. However, the distinctiveness of these African buzzards has generally been supported. Genetic studies have further indicated that the modern buzzards of Eurasia and Africa are a relatively young group, showing that they diverged at about 300,000 years ago. Nonetheless, fossils dating earlier than 5 million year old (the late Miocene period) showed \"Buteo\" species were present in Europe much earlier than that would imply, although it cannot be stated to a certainty that these would have been related to the extant buzzards.\nSubspecies and species splits.\nSome 16 subspecies have been described in the past and up to 11 are often considered valid, although some authorities accept as few as seven. Common buzzard subspecies fall into two groups.\nThe western \"buteo\" group is mainly resident or short-distance migrants and includes:\nThe eastern \"vulpinus\" group includes:\nAt one time, races of the common buzzard were thought to range as far in Asia as a breeding bird well into the Himalayas and as far east as northeastern China, Russia to the Sea of Okhotsk, and all the islands of the Kurile Islands and of Japan, despite both the Himalayan and eastern birds showing a natural gap in distribution from the next nearest breeding common buzzard. However, DNA testing has revealed that the buzzards of these populations probably belong to different species. Most authorities now accept these buzzards as full species: the eastern buzzard (\"Buteo japonicus\"; with three subspecies of its own) and the Himalayan buzzard (\"Buteo refectus\"). Buzzards found on the islands of Cape Verde off of the coast of western Africa, once referred to as the subspecies \"B. b. bannermani\", and Socotra Island off of the northern peninsula of Arabia, once referred to as the rarely recognized subspecies \"B. b. socotrae\", are now generally thought not to belong to the common buzzard. DNA testing has indicated that these insular buzzards are actually more closely related to the long-legged buzzard (\"Buteo rufinus\") than to the common buzzard. Subsequently, some researchers have advocated full species status for the Cape Verde population, but the placement of these buzzards is generally deemed unclear.\nDescription.\nThe common buzzard is a medium to large sized raptor that is highly variable in plumage. Most buzzards are distinctly round headed with a somewhat slender bill, relatively long wings that either reach or fall slightly short of the tail tip when perched, a fairly short tail, and somewhat short and mainly bare tarsi. They can appear fairly compact in overall appearance but may also appear large relative to other more common raptorial birds such as kestrels and sparrowhawks. The common buzzard measures between in length with a wingspan. Females average about 2\u20137% larger than males linearly and weigh about 15% more. Body mass can show considerable variation. Buzzards from Great Britain alone can vary from in males, while females there can range from .\nIn Europe, most typical buzzards are dark brown above and on the upperside of the head and mantle, but can become paler and warmer brown with worn plumage. The flight feathers on perched European buzzards are always brown in the nominate subspecies (\"B. b. buteo\"). Usually the tail will usually be narrowly barred grey-brown and dark brown with a pale tip and a broad dark subterminal band but the tail in palest birds can show a varying amount a white and reduced subterminal band or even appear almost all white. In European buzzards, the underside coloring can be variable but most typically show a brown-streaked white throat with a somewhat darker chest. A pale U across breast is often present; followed by a pale line running down the belly which separates the dark areas on breast-side and flanks. These pale areas tend to have highly variable markings that tend to form irregular bars. Juvenile buzzards are quite similar to adult in the nominate race, being best told apart by having a paler eye, a narrower subterminal band on the tail and underside markings that appear as streaks rather than bars. Furthermore, juveniles may show variable creamy to rufous fringes to upperwing coverts but these also may not be present. Seen from below in flight, buzzards in Europe typically have a dark trailing edge to the wings. If seen from above, one of the best marks is their broad dark subterminal tail band. Flight feathers of typical European buzzards are largely greyish, the aforementioned dark wing linings at front with contrasting paler band along the median coverts. In flight, paler individuals tend to show dark carpal patches that can appears as blackish arches or commas but these may be indistinct in darker individuals or can appear light brownish or faded in paler individuals. Juvenile nominate buzzards are best told apart from adults in flight by the lack of a distinct subterminal band (instead showing fairly even barring throughout) and below by having less sharp and brownish rather than blackish trailing wing edge. Juvenile buzzards show streaking paler parts of under wing and body showing rather than barring as do adults. Beyond the typical mid-range brownish buzzard, birds in Europe can range from almost uniform black-brown above to mainly white. Extreme dark individuals may range from chocolate brown to blackish with almost no pale showing but a variable, faded U on the breast and with or without faint lighter brown throat streaks. Extreme pale birds are largely whitish with variable widely spaced streaks or arrowheads of light brown about the mid-chest and flanks and may or may not show dark feather-centres on the head, wing-coverts and sometimes all but part of mantle. Individuals can show nearly endless variation of colours and hues in between these extremes and the common buzzard is counted among the most variably plumage diurnal raptors for this reason. One study showed that this variation may actually be the result of diminished single-locus genetic diversity.\nBeyond the nominate form (\"B. b. buteo\") that occupies most of the common buzzard's European range, a second main, widely distributed subspecies is known as the steppe buzzard (\"B. b. vulpinus\"). The steppe buzzard race shows three main colour morphs, each of which can be predominant in a region of breeding range. It is more distinctly polymorphic rather than just individually very variable like the nominate race. This may be because, unlike the nominate buzzard, the steppe buzzard is highly migratory. Polymorphism has been linked with migratory behaviour. The most common type of steppe buzzard is the rufous morph which gives this subspecies its scientific name (\"vulpes\" is Latin for \"fox\"). This morph comprises a majority of birds seen in passage east of the Mediterranean. Rufous morph buzzards are a paler grey-brown above than most nominate \"B. b. buteo\". Compared to the nominate race, rufous \"vulpinus\" show a patterning not dissimilar but generally far more rufous-toned on head, the fringes to mantle wing coverts and, especially, on the tail and the underside. The head is grey-brown with rufous tinges usually while the tail is rufous and can vary from almost unmarked to thinly dark-barred with a subterminal band. The underside can be uniformly pale to dark rufous, barred heavily or lightly with rufous or with dusky barring, usually with darker individuals showing the U as in nominate but with a rufous hue. The pale morph of the steppe buzzard is commonest in the west of its subspecies range, predominantly seen in winter and migration at the various land bridge of the Mediterranean. As in the rufous morph, the pale morph \"vulpinus\" is grey-brown above but the tail is generally marked with thin dark bars and a subterminal band, only showing rufous near the tip. The underside in the pale morph is greyish-white with dark grey-brown or somewhat streaked head to chest and barred belly and chest, occasionally showing darker flanks that can be somewhat rufous. Dark morph \"vulpinus\" tend to be found in the east and southeast of the subspecies range and are easily outnumbered by rufous morph while largely using similar migration points. Dark morph individuals vary from grey-brown to much darker blackish-brown, and have a tail that is dark grey or somewhat mixed grey and rufous, is distinctly marked with dark barring and has a broad, black subterminal band. Dark morph \"vulpinus\" have a head and underside that is mostly uniform dark, from dark brown to blackish-brown to almost pure black. Rufous morph juveniles are often distinctly paler in ground colour (ranging even to creamy-grey) than adults with distinct barring below actually increased in pale morph type juvenile. Pale and rufous morph juveniles can only be distinguished from each other in extreme cases. Dark morph juveniles are more similar to adult dark morph \"vulpinus\" but often show a little whitish streaking below, and like all other races have lighter coloured eyes and more evenly barred tails than adults. Steppe buzzards tend to appear smaller and more agile in flight than nominate whose wing beats can look slower and clumsier. In flight, rufous morph \"vulpinus\" have their whole body and underwing varying from uniform to patterned rufous (if patterning present, it is variable, but can be on chest and often thighs, sometimes flanks, pale band across median coverts), while the under-tail usually paler rufous than above. Whitish flight feathers are more prominent than in nominate and more marked contrast with the bold dark brown band along the trailing edges. Markings of pale \"vulpinus\" as seen in flight are similar to rufous morph (such as paler wing markings) but more greyish both on wings and body. In dark morph \"vulpinus\" the broad black trailing edges and colour of body make whitish areas of inner wing stand out further with an often bolder and blacker carpal patch than in other morphs. As in nominate, juvenile \"vulpinus\" (rufous/pale) tend to have much less distinct trailing edges, general streaking on body and along median underwing coverts. Dark morph \"vulpinus\" resemble adult in flight more so than other morphs.\nSimilar species.\nThe common buzzard is often confused with other raptors especially in flight or at a distance. Inexperienced and over-enthusiastic observers have even mistaken darker birds for the far larger and differently proportioned golden eagle (\"Aquila chrysaetos\") and also dark birds for western marsh harrier (\"Circus aeruginosus\") which also flies in a dihedral but is obviously relatively much longer and slenderer winged and tailed and with far different flying methods. Also buzzards may possibly be confused with dark or light morph booted eagles (\"Hieraeetus pennatus\"), which are similar in size, but the eagle flies on level, parallel-edged wings which usually appear broader, has a longer squarer tail, with no carpal patch in pale birds and all dark flight feathers but for whitish wedge on inner primaries in dark morph ones. Pale individuals are sometimes also mistaken with pale morph short-toed eagles (\"Circaetus gallicus\") which are much larger with a considerably bigger head, longer wings (which are usually held evenly in flight rather than in a dihedral) and paler underwing lacking any carpal patch or dark wing lining. More serious identification concerns lie in other \"Buteo\" species and in flight with honey buzzards, which are quite different looking when seen perched at close range. The European honey buzzard (\"Pernis apivorus\") is thought to engage in mimicry of more powerful raptors, in particular, juveniles may mimic the plumage of the more powerful common buzzard. While less individually variable in Europe, the honey buzzard is more extensively polymorphic on underparts than even the common buzzard. The most common morph of the adult European honey buzzard is heavily barred and rufous on the underside, quite different from the common buzzard, however the brownish juvenile much more resembles an intermediate common buzzard. Honey buzzards flap with distinctively slower and more even wing beats than common buzzards. The wings are also lifted higher on each upstroke, creating a more regular and mechanical effect, furthermore their wings are held slightly arched when soaring but not in a V-shape. On the honey buzzard, the head appears smaller, the body thinner, the tail longer and the wings narrower and more parallel-edged. The steppe buzzard race is particularly often mistaken for juvenile European honey buzzards, to the point where early observers of raptor migration in Israel considered distant individuals indistinguishable. However, when compared to a steppe buzzard, the honey buzzard has distinctly darker secondaries on the underwing with fewer and broader bars and more extensive black wing-tips (whole fingers) contrasting with a less extensively pale hand. Found in the same range as the steppe buzzard in some parts of southern Siberia as well as (with wintering steppes) in southwestern India, the Oriental honey buzzard (\"Pernis ptilorhynchus\") is larger than both the European honey buzzard and the common buzzard. The Oriental species is more similar in body plan to common buzzards, being relatively broader winged, shorter tailed and more amply-headed (though the head is still relatively small) relative to the European honey buzzard, but all plumages lack carpal patches.\nIn much of Europe, the common buzzard is the only species of \"Buteo\" present. However, the subarctic breeding rough-legged buzzard (\"Buteo lagopus\") comes down to occupy much of the northern part of the continent during winter in the same haunts as the common buzzard. However, the rough-legged buzzard is typically larger and distinctly longer-winged with feathered legs, as well as having a white based tail with a broad subterminal band. Rough-legged buzzards have slower wing beats and hover far more frequently than do common buzzards. The carpal patch marking on the under-wing are also bolder and blacker on all paler forms of the rough-legged buzzard. Many pale morph rough-legged buzzards have a bold, blackish band across the belly against contrasting paler feathers, a feature which rarely appears in individual common buzzard. Usually the face also appears somewhat whitish in most pale morphs of rough-legged buzzards, which is true of only extremely pale common buzzards. Dark morph rough-legged buzzards are usually distinctly darker (ranging to almost blackish) than even extreme dark individuals of common buzzards in Europe and still have the distinct white-based tail and broad subterminal band of other roughlegs. In eastern Europe and much of the Asian range of common buzzards, the long-legged buzzard (\"Buteo rufinus\") may live alongside the common species. As in the steppe buzzard race, the long-legged buzzard has three main colour morphs that are more or less similar in hue. In both the steppe buzzard race and long-legged buzzard, the main colour is overall fairly rufous. More so than steppe buzzards, long-legged buzzards tend to have a distinctly paler head and neck compared to other feathers, and, more distinctly, a normally unbarred tail. Furthermore, the long-legged buzzard is usually a rather larger bird, often considered fairly eagle-like in appearance (although it does appear gracile and small-billed even compared to smaller true eagles), an effect enhanced by its longer tarsi, somewhat longer neck and relatively elongated wings. The flight style of the latter species is deeper, slower and more aquiline, with much more frequent hovering, showing a more protruding head and a slightly higher V held in a soar. The smaller North African and Arabian race of long-legged buzzard (\"B. r. cirtensis\") is more similar in size and nearly all colour characteristics to steppe buzzard, extending to the heavily streaked juvenile plumage, in some cases such birds can be distinguished only by their proportions and flight patterns which remain unchanged. Hybridization with the latter race (\"B. r. cirtensis\") and nominate common buzzards has been observed in the Strait of Gibraltar, a few such birds have been reported potentially in the southern Mediterranean due to mutually encroaching ranges, which are blurring possibly due to climate change.\nWintering steppe buzzards may live alongside mountain buzzards and especially with forest buzzard while wintering in Africa. The juveniles of steppe and forest buzzards are more or less indistinguishable and only told apart by proportions and flight style, the latter species being smaller, more compact, having a smaller bill, shorter legs and shorter and thinner wings than a steppe buzzard. However, size is not diagnostic unless side by side as the two buzzards overlap in this regard. Most reliable are the species wing proportions and their flight actions. Forest buzzard have more flexible wing beats interspersed with glides, additionally soaring on flatter wings and apparently never engage in hovering. Adult forest buzzards compared to the typical adult steppe buzzard (rufous morph) are also similar, but the forest typically has a whiter underside, sometimes mostly plain white, usually with heavy blotches or drop-shaped marks on abdomen, with barring on thighs, more narrow tear-shaped on chest and more spotted on leading edges of underwing, usually lacking marking on the white U across chest (which is otherwise similar but usually broader than that of \"vulpinus\"). In comparison, the mountain buzzard, which is more similar in size to the steppe buzzard and slightly larger than the forest buzzard, is usually duller brown above than a steppe buzzard and is more whitish below with distinctive heavy brown blotches from breasts to the belly, flanks and wing linings while juvenile mountain buzzard is buffy below with smaller and streakier markings. The steppe buzzard when compared to another African species, the red-necked buzzard (\"Buteo auguralis\"), which has red tail similar to \"vulpinus\", is distinct in all other plumage aspects despite their similar size. The latter buzzard has a streaky rufous head and is white below with a contrasting bold dark chest in adult plumage and, in juvenile plumage, has heavy, dark blotches on the chest and flanks with pale wing-linings. Jackal and augur buzzards (\"Buteo rufofuscus\" &amp; \"augur\"), also both rufous on the tail, are larger and bulkier than steppe buzzards and have several distinctive plumage characteristics, most notably both having their own striking, contrasting patterns of black-brown, rufous and cream.\nDistribution and habitat.\nThe common buzzard is found throughout several islands in the eastern Atlantic islands, including the Canary Islands and Azores and almost throughout Europe. It is today found in Ireland and in nearly every part of Scotland, Wales and England. In mainland Europe, remarkably, there are no substantial gaps without breeding common buzzards from Portugal and Spain to Greece, Estonia, Belarus and Ukraine, though are present mainly only in the breeding season in much of the eastern half of the latter three countries. They are also present in all larger Mediterranean islands such as Corsica, Sardinia, Sicily and Crete. Further north in Scandinavia, they are found mainly in southeastern Norway (though also some points in southwestern Norway close to the coast and one section north of Trondheim), just over the southern half of Sweden and hugging over the Gulf of Bothnia to Finland where they live as a breeding species over nearly two-thirds of the land.\nThe common buzzard reaches its northern limits as a breeder in far eastern Finland and over the border to European Russia, continuing as a breeder over to the narrowest straits of the White Sea and nearly to the Kola Peninsula. In these northern quarters, the common buzzard is present typically only in summer but is a year-around resident of a hearty bit of southern Sweden and some of southern Norway. Outside of Europe, it is a resident of northern Turkey (largely close to the Black Sea) otherwise occurring mainly as a passage migrant or winter visitor in the remainder of Turkey, Georgia, sporadically but not rarely in Azerbaijan and Armenia, northern Iran (largely hugging the Caspian Sea) to northern Turkmenistan. Further north though its absent from either side of the northern Caspian Sea, the common buzzard is found in much of western Russia (though exclusively as a breeder) including all of the Central Federal District and the Volga Federal District, all but the northernmost parts of the Northwestern and Ural Federal Districts and nearly the southern half of the Siberian Federal District, its farthest easterly occurrence as a breeder. It also found in northern Kazakhstan, Kyrgyzstan, far northwestern China (Tien Shan) and northwestern Mongolia.\nNon-breeding populations occur, either as migrants or wintering birds, in southwestern India, Israel, Lebanon, Syria, Egypt (northeastern), northern Tunisia (and far northwestern Algeria), northern Morocco, near the coasts of The Gambia, Senegal and far southwestern Mauritania and Ivory Coast (and bordering Burkina Faso). In eastern and central Africa, it is found in winter from southeastern Sudan, Eritrea, about two-thirds of Ethiopia, much of Kenya (though apparently absent from the northeast and northwest), Uganda, southern and eastern Democratic Republic of the Congo, and more or less the entirety of southern Africa from Angola across to Tanzania down the remainder of the continent (but for an apparent gap along the coast from southwestern Angola to northwestern South Africa).\nHabitat.\nThe common buzzard generally inhabits the interface of woodlands and open grounds; most typically the species lives in forest edge, small woods or shelterbelts with adjacent grassland, arables or other farmland. It acquits to open moorland as long as there is some trees for perch hunting and nesting use. The woods they inhabit may be coniferous, temperate broadleaf and mixed forests and temperate deciduous forest with occasional preferences for the local dominant tree. It is absent from treeless tundra, as well as the Subarctic where the species almost entirely gives way to the rough-legged buzzard. The common buzzard is sporadic or rare in treeless steppe but can occasionally migrate through it (despite its name, the steppe buzzard subspecies breeds primarily in the wooded fringes of the steppe). The species may be found to some extent in both in mountainous or flat country. Although adaptable to and sometimes seen in wetlands and in coastal areas, buzzards are often considered more of an upland species and neither appear to be regularly attracted to or to strongly avoid bodies of waters in non-migratory times. Buzzards in well-wooded areas of eastern Poland largely used large, mature stands of trees that were more humid, richer and denser than prevalent in surrounding area, but showed preference for those within of openings. Mostly resident buzzards live in lowlands and foothills, but they can live in timbered ridges and uplands as well as rocky coasts, sometimes nesting on cliff ledges rather than trees. Buzzards may live from sea level to elevations of , breeding mostly below but they can winter to an elevation of and migrates easily to . In the mountainous Italian Apennines, buzzard nests were at a mean elevation of and were, relative to the surrounding area, further from human developed areas (i.e. roads) and nearer to valley bottoms in rugged, irregularly topographed places, especially ones that faced northeast. Common buzzards are fairly adaptable to agricultural lands but will show can show regional declines in apparent response to agriculture. Changes to more extensive agricultural practices were shown to reduce buzzard populations in western France where reduction of \"hedgerows, woodlots and grasslands areas\" caused a decline of buzzards and in Hampshire, England where more extensive grazing by free-range cattle and horses led to declines of buzzards, probably largely due to the seeming reduction of small mammal populations there. On the contrary, buzzards in central Poland adapted to removal of pine trees and reduction of rodent prey by changing nest sites and prey for a time with no strong change in their local numbers. Extensive urbanization seems to negatively affect buzzards, this species being generally less adaptable to urban areas than their New World counterparts, the red-tailed hawk. Although peri-urban areas can actually increase potential prey populations in a location at times, individual buzzard mortality, nest disturbances and nest site habitat degradation rises significantly in such areas. Common buzzards are fairly adaptive to rural areas as well as suburban areas with parks and large gardens, in addition to such areas if they're near farms.\nBehaviour.\nThe common buzzard is a typical \"Buteo\" in much of its behaviour. It is most often seen either soaring at varying heights or perched prominently on tree tops, bare branches, telegraph poles, fence posts, rocks or ledges, or alternately well inside tree canopies. Buzzards will also stand and forage on the ground. In resident populations, it may spend more than half of its day inactively perched. Furthermore, it has been described a \"sluggish and not very bold\" bird of prey. It is a gifted soarer once aloft and can do so for extended periods but can appear laborious and heavy in level flight, more so nominate buzzards than steppe buzzards. Particularly in migration, as was recorded in the case of steppe buzzards' movement over Israel, buzzards readily adjust their direction, tail and wing placement and flying height to adjust for the surrounding environment and wind conditions. In Israel, migrant buzzards rarely soar all that high (maximum above ground) due to the lack of mountain ridges that in other areas typically produce flyways; however tail-winds are significant and allow birds to cover a mean of .\nMigration.\nThe common buzzard is aptly described as a partial migrant. The autumn and spring movements of buzzards are subject to extensive variation, even down to the individual level, based on a region's food resources, competition (both from other buzzards and other predators), extent of human disturbance and weather conditions. Short-distance movements are the norm for juveniles and some adults in autumn and winter, but more adults in central Europe and the British Isles remain on their year-around residence than do not. Even for first year juvenile buzzards dispersal may not take them very far. In England, 96% of first-years moved in winter to less than from their natal site. Southwestern Poland was recorded to be a fairly important wintering grounds for central European buzzards in early spring that apparently travelled from somewhat farther north, in winter average density was a locally high 2.12 individual per square kilometer. Habitat and prey availability seemed to be the primary drivers of habitat selection in fall for European buzzards. In northern Germany, buzzards were recorded to show preferences in fall for areas fairly distant from nesting site, with a large quantity of vole-holes and more widely dispersed perches. In Bulgaria, the mean wintering density was 0.34 individual per square kilometer, and buzzards showed a preference for agricultural over forested areas. Similar habitat preferences were recorded in northeastern Romania, where buzzard density was 0.334\u20130.539 individuals per square kilometer. The nominate buzzards of Scandinavia are somewhat more strongly migratory than most central European populations. However, birds from Sweden show some variation in migratory behaviours. A maximum of 41,000 individuals have been recorded at one of the main migration sites within southern Sweden in Falsterbo. In southern Sweden, winter movements and migration was studied via observation of buzzard colour. White individuals were substantially more common in southern Sweden rather than further north in their Swedish range. The southern population migrates earlier than intermediate to dark buzzards, in both adults and juveniles. A larger proportion of juveniles than of adults migrate in the southern population. Especially adults in the southern population are resident to a higher degree than more northerly breeders.\nThe entire population of the steppe buzzard is strongly migratory, covering substantial distances during migration. In no part of the range do steppe buzzards use the same summering and wintering grounds. Steppe buzzards are slightly gregarious in migration, and travel in variously sized flocks. This race migrates in September to October often from Asia Minor to the Cape of Africa in about a month but does not cross water, following around the Winam Gulf of Lake Victoria rather than crossing the several kilometer wide gulf. Similarly, they will funnel along both sides of the Black Sea. Migratory behavior of steppe buzzards mirrors those of broad-winged &amp; Swainson's hawks (\"Buteo platypterus\" &amp; \"swainsoni\") in every significant way as similar long-distance migrating \"Buteos\", including trans-equatorial movements, avoidance of large bodies of waters and flocking behaviour. Migrating steppe buzzards will rise up with the morning thermals and can cover an average of hundreds of miles a day using the available currents along mountain ridges and other topographic features. The spring migration for steppe buzzards peaks around March\u2013April, but the latest \"vulpinus\" arrive in their breeding grounds by late April or early May. Distances covered by migrating steppe buzzards in one way flights from northern Europe (i.e. Finland or Sweden) to southern Africa have ranged over within a season . For the steppe buzzards from eastern and northern Europe and western Russia (which compromise a majority of all steppe buzzards), peak migratory numbers occur in differing areas in autumn, when the largest recorded movements occurs through Asia Minor such as Turkey, than in spring, when the largest recorded movement are to the south in the Middle East, especially Israel. The two migratory movements barely differ overall until they reach the Middle East and east Africa, where the largest volume of migrants in autumn occurs at the southern part of the Red Sea, around Djibouti and Yemen, while the main volume in spring is in the northernmost strait, around Egypt and Israel. In autumn, numbers of steppe buzzards recorded in migration have ranged up to 32,000 (recorded 1971) in northwestern Turkey (Bosporus) and in northeastern Turkey (Black Sea) up to 205,000 (recorded 1976). Further down in migration, autumn numbers of up to 98,000 have been recorded in passage in Djibouti. Between 150,000 and nearly 466,000 Steppe Buzzard have been recorded migrating through Israel during spring, making this not only the most abundant migratory raptor here but one of the largest raptor migrations anywhere in the world. Migratory movements of southern Africa buzzards largely occur along the major mountain ranges, such as the Drakensberg and Lebombo Mountains. Wintering steppe buzzards occur far more irregularly in Transvaal than Cape region in winter. The onset of migratory movement for steppe buzzards back to the breeding grounds in southern Africa is mainly in March, peaking in the second week. Steppe buzzard molt their feathers rapidly upon arrival at wintering grounds and seems to split their flight feather molt between breeding ground in Eurasia and wintering ground in southern Africa, the molt pausing during migration. In last 50 years, it was recorded that nominate buzzards are typically migrating shorter distances and wintering further north, possibly in response to climate change, resulting in relatively smaller numbers of them at migration sites. They are also extending their breeding range possibly reducing/supplanting steppe buzzards.\nVocalizations.\nResident populations of common buzzards tend to vocalize all year around, whereas migrants tend to vocalize only during the breeding season. Both nominate buzzards and steppe buzzards (and their numerous related subspecies within their types) tend to have similar voices. The main call of the species is a plaintive, far-carrying \"pee-yow\" or \"peee-oo\", used as both contact call and more excitedly in aerial displays. Their call is sharper, more ringing when used in aggression, tends to be more drawn-out and wavering when chasing intruders, sharper, more yelping when as warning when approaching the nest or shorter and more explosive when called in alarm. Other variations of their vocal performances include a cat-like \"mew\", uttered repeatedly on the wing or when perched, especially in display; a repeated \"mah\" has been recorded as uttered by pairs answering each other, further chuckles and croaks have also been recorded at nests. Juveniles can usually be distinguished by the discordant nature of their calls compared to those of adults.\nDietary biology.\nThe common buzzard is a generalist predator which hunts a wide variety of prey given the opportunity. Their prey spectrum extents to a wide variety of vertebrates including mammals, birds (from any age from eggs to adult birds), reptiles, amphibians and, rarely, fish, as well as to various invertebrates, mostly insects. Young animals are often attacked, largely the nidifugous young of various vertebrates. In total well over 300 prey species are known to be taken by common buzzards. Furthermore, prey size can vary from tiny beetles, caterpillars and ants to large adult grouse and rabbits up to nearly twice their body mass. Mean body mass of vertebrate prey was estimated at in Belarus. At times, they will also subsist partially on carrion, usually of dead mammals or fish. However, dietary studies have shown that they mostly prey upon small mammals, largely small rodents. Like many temperate zone raptorial birds of varied lineages, voles are an essential part of the common buzzard's diet. This bird's preference for the interface between woods and open areas frequently puts them in ideal vole habitat. Hunting in relatively open areas has been found to increase hunting success whereas more complete shrub cover lowered success. A majority of prey is taken by dropping from perch, and is normally taken on ground. Alternately, prey may be hunted in a low flight. This species tends not to hunt in a spectacular stoop but generally drops gently then gradually accelerate at bottom with wings held above the back. Sometimes, the buzzard also forages by random glides or soars over open country, wood edges or clearings. Perch hunting may be done preferentially but buzzards fairly regularly also hunt from a ground position when the habitat demands it. Outside the breeding season, as many 15\u201330 buzzards have been recorded foraging on ground in a single large field, especially juveniles. Normally the rarest foraging type is hovering. A study from Great Britain indicated that hovering does not seem to increase hunting success.\nMammals.\nA high diversity of rodents may be taken given the chance, as around 60 species of rodent have been recorded in the foods of common buzzards. It seems clear that voles are the most significant prey type for European buzzards. Nearly every study from the continent makes reference to the importance, in particular, of the two most numerous and widely distributed European voles: the common vole (\"Microtus arvalis\") and the somewhat more northerly ranging field vole (\"Microtus agrestis\"). In southern Scotland, field voles were the best-represented species in pellets, accounting for 32.1% of 581 pellets. In southern Norway, field voles were again the main food in years with peak vole numbers, accounting for 40.8% of 179 prey items in 1985 and 24.7% of 332 prey items in 1994. Altogether, rodents amount to 67.6% and 58.4% of the foods in these respective peak vole years. However, in low vole population years, the contribution of rodents to the diet was minor. As far west as the Netherlands, common voles were the most regular prey, amounting to 19.6% of 6624 prey items in a very large study. Common voles were the main foods recorded in central Slovakia, accounting for 26.5% of 606 prey items. The common vole, or other related vole species at times, were the main foods as well in Ukraine (17.2% of 146 prey items) ranging east to Russia in the Privolshky Steppe Nature Reserve (41.8% of 74 prey items) and in Samara (21.4% of 183 prey items). Other records from Russia and Ukraine show voles ranging from slightly secondary prey to as much as 42.2% of the diet. In Belarus, voles, including \"Microtus\" species and bank voles (\"Myodes glareolus\"), accounted for 34.8% of the biomass on average in 1065 prey items from different study areas over 4 years. At least 12 species of the genus \"Microtus\" are known to be hunted by common buzzards and even this is probably conservative, moreover similar species like lemmings will be taken if available.\nOther rodents are taken largely opportunistically rather than by preference. Several wood mice (\"Apodemus ssp.\") are known to be taken quite frequently but given their preference for activity in deeper woods than the field-forest interfaces preferred, they are rarely more than secondary food items. An exception was in Samara where the yellow-necked mouse (\"Apodemus flavicollis\"), one of the largest of its genus at , made up 20.9%, putting it just behind the common vole in importance. Similarly, tree squirrels are readily taken but rarely important in the foods of buzzards in Europe, as buzzards apparently prefer to avoid taking prey from trees nor do they possess the agility typically necessary to capture significant quantities of tree squirrels. All four ground squirrels that range (mostly) into eastern Europe are also known to be common buzzard prey but little quantitative analysis has gone into how significant such predator-prey relations are. Rodent prey taken have ranged in size from the Eurasian harvest mouse (\"Micromys minutus\") to the non-native, muskrat (\"Ondatra zibethicus\"). Other rodents taken either seldom or in areas where the food habits of buzzards are spottily known include flying squirrels, marmots (presumably very young if taken alive), chipmunks, spiny rats, hamsters, mole-rats, gerbils, jirds and jerboas and occasionally hearty numbers of dormice, although these are nocturnal. Surprisingly little research has gone into the diets of wintering steppe buzzards in southern Africa, considering their numerous status there. However, it has been indicated that the main prey remains consist of rodents such as the four-striped grass mouse (\"Rhabdomys pumilio\") and Cape mole-rats (\"Georychus capensis\").\nOther than rodents, two other groups of mammals can be counted as significant to the diet of common buzzards. One of these main prey types of import in the diets of common buzzards are leporids or lagomorphs, especially the European rabbit (\"Oryctolagus cuniculus\") where it is found in numbers in a wild or feral state. In all dietary studies from Scotland, rabbits were highly important to the buzzard's diet. In southern Scotland, rabbits constituted 40.8% of remains at nests and 21.6% of pellet contents, while lagomorphs (mainly rabbits but also some young hares) were present in 99% of remains in Moray, Scotland. The nutritional richness relative to the commonest prey elsewhere, such as voles, might account for the high productivity of buzzards here. For example, clutch sizes were twice as large on average where rabbits were common (Moray) than were where they were rare (Glen Urquhart). In northern Ireland, an area of interest because it is devoid of any native vole species, rabbits were again the main prey. Here, lagomorphs constituted 22.5% of prey items by number and 43.7% by biomass. While rabbits are non-native, albeit long-established, in the British Isles, in their native area of the Iberian peninsula, rabbits are similarly significant to the buzzard's diet. In Murcia, Spain, rabbits were the most common mammal in the diet, making up 16.8% of 167 prey items. In a large study from northeastern Spain, rabbits were dominant in the buzzard's foods, making up 66.5% of 598 prey items. In the Netherlands, European rabbits were second in number (19.1% of 6624 prey items) only to common voles and the largest contributor of biomass to nests (36.7%). Outside of these (at least historically) rabbit-rich areas, leverets of the common hare species found in Europe can be important supplemental prey. European hare (\"Lepus europaeus\") were the fourth most important prey species in central Poland and the third most significant prey species in Stavropol Krai, Russia. Buzzards normally attack the young of European rabbits and hares. Most of the rabbits taken by buzzard variously been estimated from , and infrequently up to in weight. Similarly, in different areas and the mean weight of brown hares taken in Finland was around . One young mountain hares (\"Lepus timidus\") taken in Norway was estimated to about . However, common buzzards are known to kill adult rabbits at times. This can be supported by remains of relatively large-sized tarsus bones of the rabbit, up to 64mm in length.\nThe other significant mammalian prey type is insectivores, among which more than 20 species are known to be taken by this species, including nearly all the species of shrew, mole and hedgehog found in Europe. Moles are taken particularly often among this order, since as is the case with \"vole-holes\", buzzards probably tend to watch molehills in fields for activity and dive quickly from their perch when one of the subterranean mammals pops up. The most widely found mole in the buzzard's northern range is the European mole (\"Talpa europaea\") and this is one of the more important non-rodent prey items for the species. This species was present in 55% of 101 remains in Glen Urquhart, Scotland and was the second most common prey species (18.6%) in 606 prey items in Slovakia. In Bari, Italy, the Roman mole (\"Talpa romana\"), of similar size to the European species, was the leading identified mammalian prey, making up 10.7% of the diet. The full-size range of insectivores may be taken by buzzards, ranging from the world's smallest mammal (by weight), the Etruscan shrew (\"Suncus etruscus\") to arguably the heaviest insectivore, the European hedgehog (\"Erinaceus europaeus\"). Mammalian prey for common buzzards other than rodents, insectivores, and lagomorphs is rarely taken. Occasionally, some weasels such as least weasel (\"Mustela nivalis\") and stoat (\"Mustela erminea\") are taken, and remains of young pine martens (\"Martes martes\") and adult european polecats (\"Mustela putorius\") was found in buzzard nest. Numerous larger mammals, including medium-sized carnivores such as dogs, cats and foxes and various ungulates, are sometimes eaten as carrion by buzzards, mainly during lean winter months. Still-borns of deer are also visited with some frequency.\nBirds.\nWhen attacking birds, common buzzards chiefly prey on nestlings and fledglings of small to medium-sized birds, largely passerines but also a variety of gamebirds, but sometimes also injured, sickly or unwary but healthy adults. While capable of overpowering birds larger than itself, the common buzzard is usually considered to lack the agility necessary to capture many adult birds, even gamebirds which would presumably be weaker fliers considering their relatively heavy bodies and small wings. The amount of fledgling and younger birds preyed upon relative to adults is variable, however. For example, in the Italian Alps, 72% of birds taken were fledglings or recently fledged juveniles, 19% were nestlings and 8% were adults. On the contrary, in southern Scotland, even though the buzzards were taking relatively large bird prey, largely red grouse (\"Lagopus lagopus scotica\"), 87% of birds taken were reportedly adults. In total, as in many raptorial birds that are far from bird-hunting specialists, birds are the most diverse group in the buzzard's prey spectrum due to the sheer number and diversity of birds, few raptors do not hunt them at least occasionally. Nearly 150 species of bird have been identified in the common buzzard's diet. In general, despite many that are taken, birds usually take a secondary position in the diet after mammals. In northern Scotland, birds were fairly numerous in the foods of buzzards. The most often recorded avian prey and 2nd and 3rd most frequent prey species (after only field voles) in Glen Urquhart, were chaffinch (\"Fringilla coelebs\") and meadow pipits (\"Anthus pratensis\"), with the buzzards taking 195 fledglings of these species against only 90 adults. This differed from Moray where the most frequent avian prey and 2nd most frequent prey species behind the rabbit was the common wood pigeon (\"Columba palumbus\") and the buzzards took four times as many adults relative to fledglings.\nBirds were the primary food for common buzzards in the Italian Alps, where they made up 46% of the diet against mammal which accounted for 29% in 146 prey items. The leading prey species here were Eurasian blackbirds (\"Turdus merula\") and Eurasian jays (\"Garrulus glandarius\"), albeit largely fledglings were taken of both. Birds could also take the leading position in years with low vole populations in southern Norway, in particular thrushes, namely the blackbird, the song thrush (\"Turdus philomelos\") and the redwing (\"Turdus iliacus\"), which were collectively 22.1% of 244 prey items in 1993. In southern Spain, birds were equal in number to mammals in the diet, both at 38.3%, but most remains were classified as \"unidentified medium-sized birds\", although the most often identified species of those that apparently could be determined were Eurasian jays and red-legged partridges (\"Alectoris rufa\"). Similarly, in northern Ireland, birds were roughly equal in import to mammals but most were unidentified corvids. In Seversky Donets, Ukraine, birds and mammals both made up 39.3% of the foods of buzzards. Common buzzards may hunt nearly 80 species passerines and nearly all available gamebirds. Like many other largish raptors, gamebirds are attractive to hunt for buzzards due to their ground-dwelling habits. Buzzards were the most frequent predator in a study of juvenile pheasants in England, accounting for 4.3% of 725 deaths (against 3.2% by foxes, 0.7% by owls and 0.5% by other mammals). They also prey on a wide size range of birds, ranging down to Europe's smallest bird, the goldcrest (\"Regulus regulus\"). Very few individual birds hunted by buzzards weigh more than . However, there have been some particularly large avian kills by buzzards, including any that weigh more or , or about the largest average size of a buzzard, have including adults of mallard (\"Anas platyrhynchos\"), black grouse (\"Tetrao tetrix\"), ring-necked pheasant (\"Phasianus colchicus\"), common raven (\"Corvus corax\") and some of the larger gulls if ambushed on their nests. The largest avian kill by a buzzard, and possibly largest known overall for the species, was an adult female western capercaillie (\"Tetrao urogallus\") that weighed an estimated . At times, buzzards will hunt the young of large birds such as herons and cranes. Other assorted avian prey has included a few species of waterfowl, most available pigeons and doves, cuckoos, swifts, grebes, rails, nearly 20 assorted shorebirds, tubenoses, hoopoes, bee-eaters and several types of woodpecker. Birds with more conspicuous or open nesting areas or habits are more likely to have fledglings or nestlings attacked, such as water birds, while those with more secluded or inaccessible nests, such as pigeons/doves and woodpeckers, adults are more likely to be hunted.\nReptiles and amphibians.\nThe common buzzard may be the most regular avian predator of reptiles and amphibians in Europe apart from the sections where they are sympatric with the largely snake-eating short-toed eagle. In total, the prey spectrum of common buzzards include nearly 50 herpetological prey species. In studies from northern and southern Spain, the leading prey numerically were both reptilian, although in Biscay (northern Spain) the leading prey (19%) was classified as \"unidentified snakes\". In Murcia, the most numerous prey was the ocellated lizard (\"Timon lepidus\"), at 32.9%. In total, at Biscay and Murcia, reptiles accounted for 30.4% and 35.9% of the prey items, respectively. Findings were similar in a separate study from northeastern Spain, where reptiles amounted to 35.9% of prey. In Bari, Italy, reptiles were the main prey, making up almost exactly half of the biomass, led by the large green whip snake (\"Hierophis viridiflavus\"), at 24.2% of food mass. In Stavropol Krai, Russia, the sand lizard (\"Lacerta agilis\") was the main prey at 23.7% of 55 prey items. The slowworm (\"Anguis fragilis\"), a legless lizard, became the most numerous prey for the buzzards of southern Norway in low vole years, amounting to 21.3% of 244 prey items in 1993 and were also common even in the peak vole year of 1994 (19% of 332 prey items). More or less any snake in Europe is potential prey and the buzzard has been known to be uncharacteristically bold in going after and overpowering large snakes such as rat snakes, ranging up to nearly in length, and healthy, large vipers despite the danger of being struck by such prey. However, in at least one case, the corpse of a female buzzard was found envenomed over the body of an adder that it had killed. In some parts of range, the common buzzard acquires the habit of taking many frogs and toads. This was the case in the Mogilev Region of Belarus where the moor frog (\"Rana arvalis\") was the major prey (28.5%) over several years, followed by other frogs and toads amounting to 39.4% of the diet over the years. In central Scotland, the common toad (\"Bufo bufo\") was the most numerous prey species, accounting for 21.7% of 263 prey items, while the common frog (\"Rana temporaria\") made up a further 14.7% of the diet. Frogs made up about 10% of the diet in central Poland as well.\nInvertebrates and other prey.\nWhen common buzzards feed on invertebrates, these are chiefly earthworms, beetles and caterpillars in Europe and largely seemed to be preyed on by juvenile buzzards with less refined hunting skills or in areas with mild winters and ample swarming or social insects. In most dietary studies, invertebrates are at best a minor supplemental contributor to the buzzard's diet. Nonetheless, roughly a dozen beetle species have found in the foods of buzzards from Ukraine alone. In winter in northeastern Spain, it was found that the buzzards switched largely from the vertebrate prey typically taken during spring and summer to a largely insect-based diet. Most of this prey was unidentified but the most frequently identified were European mantis (\"Mantis religiosa\") and European mole cricket (\"Gryllotalpa gryllotalpa\"). In Ukraine, 30.8% of the food by number was found to be insects. Especially in winter quarters such as southern Africa, common buzzards are often attracted to swarming locusts and other orthopterans. In this way the steppe buzzard may mirror a similar long-distance migrant from the Americas, the Swainson's hawk, which feeds its young largely on nutritious vertebrates but switches to a largely insect-based once the reach their distant wintering grounds in South America. In Eritrea, 18 returning migrant steppe buzzards were seen to feed together on swarms of grasshoppers. For wintering steppe buzzards in Zimbabwe, one source went so far as to refer to them as primarily insectivorous, apparently being somewhat locally specialized to feeding on termites. Stomach contents in buzzards from Malawi apparently consisted largely of grasshoppers (alternately with lizards). Fish tend to be the rarest class of prey found in the common buzzard's foods. There are a couple cases of predation of fish detected in the Netherlands, while elsewhere they have been known to have fed upon eels and carp.\nInterspecies predatory relationships.\nCommon buzzards co-occur with dozens of other raptorial birds through their breeding, resident and wintering grounds. There may be many other birds that broadly overlap in prey selection to some extent. Furthermore, their preference for interfaces of forest and field is used heavily by many birds of prey. Some of the most similar species by diet are the common kestrel (\"Falco tinniculus\"), hen harrier (\"Circus cyaenus\") and lesser spotted eagle (\"Clanga clanga\"), not to mention nearly every European species of owl, as all but two may locally prefer rodents such as voles in their diets. Diet overlap was found to be extensive between buzzards and red foxes (\"Vulpes vulpes\") in Poland, with 61.9% of prey selection overlapping by species although the dietary breadth of the fox was broader and more opportunistic. Both fox dens and buzzard roosts were found to be significantly closer to high vole areas relative to the overall environment here. The only other widely found European \"Buteo\", the rough-legged buzzard, comes to winter extensively with common buzzards. It was found in southern Sweden, habitat, hunting and prey selection often overlapped considerably. Rough-legged buzzards appear to prefer slightly more open habitat and took slightly fewer wood mice than common buzzard. Roughlegs also hover much more frequently and are more given to hunting in high winds. The two buzzards are aggressive towards one another and excluded each other from winter feeding territories in similar ways to the way they exclude conspecifics. In northern Germany, the buffer of their habitat preferences apparently accounted for the lack of effect on each other's occupancy between the two buzzard species. Despite a broad range of overlap, very little is known about the ecology of common and long-legged buzzards where they co-exist. However, it can be inferred from the long-legged species preference for predation on differing prey, such as blind mole-rats, ground squirrels, hamsters and gerbils, from the voles usually preferred by the common species, that serious competition for food is unlikely.\nA more direct negative effect has been found in buzzard's co-existence with northern goshawk (\"Accipiter gentilis\"). Despite the considerable discrepancy of the two species dietary habits, habitat selection in Europe is largely similar between buzzards and goshawks. Goshawks are slightly larger than buzzards and are more powerful, agile and generally more aggressive birds, and so they are considered dominant. In studies from Germany and Sweden, buzzards were found to be less disturbance sensitive than goshawks but were probably displaced into inferior nesting spots by the dominant goshawks. The exposure of buzzards to a dummy goshawk was found to decrease breeding success whereas there was no effect on breeding goshawks when they were exposed to a dummy buzzard. In many cases, in Germany and Sweden, goshawks displaced buzzards from their nests to take them over for themselves. In Poland, buzzards productivity was correlated to prey population variations, particularly voles which could vary from 10 to 80 per hectare, whereas goshawks were seemingly unaffected by prey variations; buzzards were found here to number 1.73 pair per against goshawk 1.63 pair per . In contrast, the slightly larger counterpart of buzzards in North America, the red-tailed hawk (which is also slightly larger than American goshawks, the latter averaging smaller than European ones) are more similar in diet to goshawks there. Redtails are not invariably dominated by goshawks and are frequently able to outcompete them by virtue of greater dietary and habitat flexibility. Furthermore, red-tailed hawks are apparently equally capable of killing goshawks as goshawks are of killing them (killings are more one-sided in buzzard-goshawk interactions in favour of the latter). Other raptorial birds, including many of similar or mildly larger size than common buzzards themselves, may dominate or displace the buzzard, especially with aims to take over their nests. Species such as the black kite (\"Milvus migrans\"), booted eagle (\"Hieraeetus pennatus\") and the lesser spotted eagle have been known to displace actively nesting buzzards, although in some cases the buzzards may attempt to defend themselves. The broad range of accipitrids that take over buzzard nests is somewhat unusual. More typically, common buzzards are victims of nest parasitism to owls and falcons, as neither of these other kinds of raptorial birds builds their own nests, but these may regularly take up occupancy on already abandoned or alternate nests rather than ones the buzzards are actively using. Even with birds not traditionally considered raptorial, such as common ravens, may compete for nesting sites with buzzards. In urban vicinities of southwestern England, it was found that peregrine falcons (\"Falco peregrinus\") were harassing buzzards so persistently, in many cases resulting in injury or death for the buzzards, the attacks tending to peak during the falcon's breeding seasons and tend to be focused on subadult buzzards. Despite often being dominated in nesting site confrontations by even similarly sized raptors, buzzards appear to be bolder in direct competition over food with other raptors outside of the context of breeding, and has even been known to displace larger birds of prey such as red kites (\"Milvus milvus\") and female buzzards may also dominate male goshawks (which are much smaller than the female goshawk) at disputed kills.\nCommon buzzards are occasionally threatened by predation by other raptorial birds. Northern goshawks have been known to have preyed upon buzzards in a few cases. Much larger raptors are known to have killed a few buzzards as well, including steppe eagles (\"Aquila nipalensis\") on migrating steppe buzzards in Israel. Further instances of predation on buzzards have involved golden, eastern imperial (\"Aquila heliaca\"), Bonelli's (\"Aquila fasciata\") and white-tailed eagles (\"Haliaeetus albicilla\") in Europe. Besides preying on adult buzzard, white-tailed eagles have been known to raise buzzards with their own young. These are most likely cases of eagles carrying off young buzzard nestlings with the intention of predation but, for unclear reasons, not killing them. Instead the mother eagle comes to brood the young buzzard. Despite the difference of the two species diets, white-tailed eagles are surprisingly successful at raising young buzzards (which are conspicuously much smaller than their own nestlings) to fledging. Studies in Lithuania of white-tailed eagle diets found that predation on common buzzards was more frequent than anticipated, with 36 buzzard remains found in 11 years of study of the summer diet of the white-tailed eagles. While nestling buzzards were multiple times more vulnerable to predation than adult buzzards in the Lithuanian data, the region's buzzards expelled considerable time and energy during the late nesting period trying to protect their nests. The most serious predator of common buzzards, however, is almost certainly the Eurasian eagle-owl (\"Bubo bubo\"). This is a very large owl with a mean body mass about three to four times greater than that of a buzzard. The eagle-owl, despite often taking small mammals that broadly overlap with those selected by buzzards, is considered a \"super-predator\" that is a major threat to nearly all co-existing raptorial birds, capably destroying whole broods of other raptorial birds and dispatching adult raptors even as large as eagles. Due to their large numbers in edge habitats, common buzzards frequently feature heavily in the eagle-owl's diet. Eagle-owls, as will some other large owls, also readily expropriate the nests of buzzards. In the Czech Republic and in Luxembourg, the buzzard was the third and fifth most frequent prey species for eagle-owls, respectively. The reintroduction of eagle-owls to sections of Germany has been found to have a slight deleterious effect on the local occupancy of common buzzards. The only sparing factor is the temporal difference (the buzzard nesting later in the year than the eagle-owl) and buzzards may locally be able to avoid nesting near an active eagle-owl family. As the ecology of the wintering population is relatively little studied, a similar very large owl at the top of the avian food chain, the Verreaux's eagle-owl (\"Bubo lacteus\"), is the only known predator of wintering steppe buzzards in southern Africa. Despite not being known predators of buzzards, other large, vole-eating owls are known to displace or to be avoided by nesting buzzards, such as great grey owls (\"Strix nebulosa\") and Ural owls (\"Strix uralensis\"). Unlike with large birds of prey, next to nothing is known of mammalian predators of common buzzards, despite up to several nestlings and fledglings being likely depredated by mammals.\nCommon buzzards themselves rarely present a threat to other raptorial birds but may occasionally kill a few of those of smaller size. The buzzard is a known predator of Eurasian sparrowhawks (\"Accipiter nisus\"), common kestrel and lesser kestrel (\"Falco naumanni\") . Perhaps surprisingly, given the nocturnal habits of this prey, the group of raptorial birds the buzzard is known to hunt most extensively is owls. Known owl prey has included Western barn owls (\"Tyto alba\"), European scops owls (\"Otus scops\"), tawny owls (\"Strix aluco\"), little owls (\"Athene noctua\"), boreal owls (\"Aegolius funereus\"), long-eared owls (\"Asio otus\") and short-eared owls (\"Asio flammeus\"). Despite their relatively large size, tawny owls are known to avoid buzzards as there are several records of them preying upon the owls.\nBreeding.\nNesting territories and density.\nHome ranges of common buzzards are generally . The size of breeding territory seem to be generally correlated with food supply. In a German study, the range was with an average of . Some of the lowest pair densities of common buzzards seem to come from Russia. For instance, in Kerzhenets Nature Reserve, the recorded density was 0.6 pairs per and the average distance of nearest neighbors was . The Snowdonia region of northern Wales held a pair per with a mean nearest neighbor distance of ; in adjacent Migneint, pair occurrence was , with a mean distance of . In the Teno massif of the Canary Islands, the average density was estimated as 23 pairs per , similar to that of a middling continental population. On another set of islands, on Crete the density of pairs was lower at 5.7 pairs per ; here buzzards tend to have an irregular distribution, some in lower intensity harvest olive groves but their occurrence actually more common in agricultural than natural areas. In the Italian Alps, it was recorded in 1993\u201396 that there were from 28 to 30 pairs per . In central Italy, density average was lower at 19.74 pairs per . Higher density areas are known than those above. Two areas of the Midlands of England showed occupancies of 81 and 22 territorial pairs per . High buzzard densities there were associated with high proportions of unimproved pasture and mature woodland within the estimated territories. Similarly high densities of common buzzards were estimated in central Slovakia using two different methods, here indicating densities of 96 to 129 pairs per . Despite claims from the study of the English midlands were the highest known territory density for the species, a number ranging from 32 to 51 pairs in wooded area of merely in Czech Republic seems to surely exceed even those densities. The Czech study hypothesized that fragmentation of forest in human management of lands for wild sheep and deer, creating exceptional concentrations of prey such as voles, and lack of appropriate habitat in surrounding regions for the exceptionally high density.\nIn the North-Estonian Neeruti landscape reserve (area 1250 ha), Marek Vahula found 9 populated nests in 1989 and 1990. One nest was found in 1982 and is apparently the oldest known nest that is still populated today.\nCommon buzzards maintain their territories through flight displays. In Europe, territorial behaviour generally starts in February. However, displays are not uncommon throughout year in resident pairs, especially by males, and can elicit similar displays by neighbors. In them, common buzzards generally engage in high circling, spiraling upward on slightly raised wings. Mutual high circling by pairs sometimes go on at length, especially during the period prior to or during breeding season. In mutual displays, a pair may follow each other at in level flight. During the mutual displays, the male may engage in exaggerated deep flapping or zig-zag tumbling, apparently in response to the female being too distant. Two or three pairs may circle together at times and as many as 14 individual adults have been recorded over established display sites. Sky-dancing by common buzzards have been recorded in spring and autumn, typically by male but sometimes by female, nearly always with much calling. Their sky-dances are of the rollercoaster type, with upward sweep until they start to stall, but sometimes embellished with loops or rolls at the top. Next in the sky-dance, they dive on more or less closed wings before spreading them and shooting up again, upward sweeps of up to , with dive drops of up to at least . These dances may be repeated in series of 10 to 20. In the climax of the sky dance, the undulations become progressive shallower, often slowing and terminating directly onto a perch. Various other aerial displays include low contour flight or weaving among trees, frequently with deep beats and exaggerated upstrokes which show underwing pattern to rivals perched below. Talon grappling and occasionally cartwheeling downward with feet interlocked has been recorded in buzzards and, as in many raptors, is likely the physical culmination of the aggressive territorial display, especially between males. Despite the highly territorial nature of buzzards and their devotion to a single mate and breeding ground each summer, there is one case of a polyandrous trio of buzzards nesting in the Canary Islands.\nNests.\nCommon buzzards tend to build a bulky nest of sticks, twigs and often heather. Commonly, nests are up to across and deep. With reuse over years, the diameter can reach or exceed and weight of nests can reach over . Active nests tend to be lined with greenery, most often this consists of broad-leafed foliage but sometimes also includes rush or seaweed locally. Nest height in trees is commonly , usually by main trunk or main crutch of the tree. In Germany, trees used for nesting consisted mostly of red beeches (\"Fagus sylvatica\") (in 337 cases), whereas a further 84 were in assorted oaks. Buzzards were recorded to nest almost exclusively in pines in Spain at a mean height of . Trees are generally used for a nesting location but they will also utilize crags or bluffs if trees are unavailable. Buzzards in one English study were surprisingly partial to nesting on well-vegetated banks and due to the rich surrounding environment habitat and prey population, were actually more productive than nests located in other locations here. Furthermore, a few ground nests were recorded in high prey-level agricultural areas in the Netherlands. In the Italian Alps, 81% of 108 nests were on cliffs. The common buzzard generally lacks the propensity of its Nearctic counterpart, the red-tailed hawk, to occasionally nest on or near manmade structures (often in heavily urbanized areas) but in Spain some pairs recorded nesting along the perimeter of abandoned buildings. Pairs often have several nests but some pairs may use one over several consecutive years. Two to four alternate nests in a territory is typical for common buzzards, especially those breeding further north in their range.\nReproduction and eggs.\nThe breeding season commences at differing times based on latitude. Common buzzard breeding seasons may fall as early as January to April but typically the breeding season is March to July in much of Palearctic. In the northern stretches of the range the breeding season may last into May\u2013August. Mating usually occurs on or near the nest and lasts about 15 seconds, typically occurring several times a day. Eggs are usually laid in 2 to 3-day intervals. The clutch size can range from to 2 to 6, a relatively large clutch for an accipitrid. More northerly and westerly buzzard usually bear larger clutches, which average nearer 3, than those further east and south. In Spain, the average clutch size is about 2 to 2.3. From 4 locations in different parts of Europe, 43% had clutch size of 2, 41% had size of 3, clutches of 1 and 4 each constituted about 8%. Laying dates are remarkably constant throughout Great Britain. There are, however, highly significant differences in clutch size between British study areas. These do not follow any latitudinal gradient and it is likely that local factors such as habitat and prey availability are more important determinants of clutch size. The eggs are white in ground colour, rather round in shape with sporadic red to brown markings sometimes lightly showing. In the nominate race, egg size is in height by in diameter with an average of in 600 eggs. In the race of \"vulpinus\", egg height is by with an average of in 303 eggs. Eggs are generally laid in late March to early April in extreme south, sometime in April in most of Europe, into May and possibly even early June in the extreme north. If eggs are lost to a predator (including humans) or fail in some other way, common buzzards do not usually lay replacement clutches but they have been recorded, even with 3 attempts of clutches by a single female. The female does most but not all of the incubating, doing so for a total of 33\u201335 days. The female remains at the nest brooding the young in the early stages with the male bringing all prey. At about 8\u201312 days, both the male and female will bring prey but the female continues to do all feeding until the young can tear up their own prey.\nDevelopment of young.\nOnce hatching commences, it may take 48 hours for the chick to chip out. Hatching may take place over 3\u20137 days, with new hatchlings averaging about in body mass. Often the youngest nestling dies from starvation, especially in broods of three or more. In nestlings, the first down replaces by longer, coarser down at about 7 days of age with the first proper feathers appearing at 12 to 15 days. The young are nearly fully feathered rather than downy at about a month of age and can start to feed themselves as well. The first attempts to leave the nest are often at about 40\u201350 days, averaging usually 40\u201345 in nominate buzzards in Europe, but more quickly on average at 40\u201342 in \"vulpinus\". Fledging occurs typically at 43\u201354 days but in extreme cases at as late 62 days. Sexual dimorphism is apparent in European fledglings, as females often scale about against in males. After leaving the nest, buzzards generally stay close by, but with migratory ones there is more definitive movement generally southbound. Full independence is generally sought 6 to 8 weeks after fledging. 1st year birds generally remain in wintering area for following summer but then return to near area of origin but then migrate south again without breeding. Radio-tracking suggests that most dispersal, even relatively early dispersals, by juvenile buzzards is undertaken independently rather than via exile by parents, as has been recorded in some other birds of prey. In common buzzards, generally speaking, siblings stay quite close to each other after dispersal from their parents and form something of a social group, although parents usually tolerate their presence on their territory until they are laying another clutch. However, the social group of siblings disbands at about a year of age. Juvenile buzzards are subordinate to adults during most encounters and tend to avoid direct confrontations and actively defended territories until they are of appropriate age (usually at least 2 years of age). This was the case as well for steppe buzzard juveniles wintering in southern Africa, although in some cases juveniles were able to successfully steal prey from adults there.\nBreeding success rates.\nNumerous factors may weigh into the breeding success of common buzzards. Chiefly among these are prey populations, habitat, disturbance and persecution levels and interspecies competition. In Germany, intra- and interspecific competition, plumage morph, laying date, precipitation levels and anthropogenic disturbances in the breeding territory, in declining order, were deemed to be the most significant bearers of breeding success. In an accompanying study, it was found that a mere 17% of adult birds of both sexes present in a German study area produced 50% of offspring, so breeding success may be lower than perceived and many adult buzzards for unknown causes may not attempt to breed at all. High breeding success was detected in Argyll, Scotland, due likely to hearty prey populations (rabbits) but also probably a lower local rate of persecution than elsewhere in the British isles. Here, the mean number of fledglings were 1.75 against 0.82\u20131.41 in other parts of Britain. It was found in the English Midlands that breeding success both by measure of clutch size and mean number of fledglings, was relatively high thanks again to high prey populations. Breeding success was lower farther from significant stands of trees in the Midlands and most nesting failures that could be determined occurred in the incubation stage, possibly in correlation with predation of eggs by corvids. More significant than even prey, late winter-early spring was found to be likely the primary driver of breeding success in buzzards from southern Norway. Here, even in peak vole years, nesting success could be considerably hampered by heavy snow at this crucial stage. In Norway, large clutches of 3+ were expected only in years with minimal snow cover, high vole populations and lighter rains in May\u2013June. In the Italian Alps, the mean number of fledglings per pair was 1.07. 33.4% of nesting attempts were failures per a study in southwestern Germany, with an average of 1.06 of all nesting attempts and 1.61 for all successful attempt. In Germany, weather conditions and rodent populations seemed to be the primary drivers of nesting success. In Murcia part of Spain contrasted with Biscay to the north, higher levels of interspecific competition from booted eagles and northern goshawks did not appear to negatively affect breeding success due to more ample prey populations (rabbits again) in Murcia than in Biscay.\nIn the Westphalia area of Germany, it was found that intermediate colour morphs were more productive than those that were darker or lighter. For reasons that are not entirely clear, apparently fewer parasites were found to afflict broods of intermediate plumaged buzzard less so than dark and light phenotypes, in particular higher melanin levels somehow were found to be more inviting to parasitic organism that effect the health of the buzzard's offspring. The composition of habitat and its relation to human disturbance were important variables for the dark and light phenotypes but were less important to intermediate individuals. Thus selection pressures resulting from different factors did not vary much between sexes but varied between the three phenotypes in the population. Breeding success in areas with wild European rabbits was considerably effected by rabbit myxomatosis and rabbit haemorrhagic disease, both of which have heavily depleted wild rabbit population. Breeding success in formerly rabbit-rich areas were recorded to decrease from as much as 2.6 to as little as 0.9 young per pair. Age of first breeding in several radio-tagged buzzards showed only a single male breeding as early as his 2nd summer (at about a year of age). Significantly more buzzards were found to start breeding at the 3 summer but breeding attempts can be individually erratic given the availability of habitat, food and mates. The mean life expectancy was estimated at 6.3 years in the late 1950s, but this was at a time of high persecution when humans were causing 50\u201380% of buzzard deaths. In a more modern context with regionally reduced persecution rates, the lifespan expected can be higher (possibly in excess of 10 years at times) but is still widely variable due to a wide variety of factors.\nStatus.\nThe common buzzard is one of the most numerous birds of prey in its range. Almost certainly, it is the most numerous diurnal bird of prey throughout Europe. Conservative estimates put the total population at no fewer than 700,000 pairs in Europe, which are more than twice the total estimates for the next four birds of prey estimated as most common: the Eurasian sparrowhawk (more than 340,000 pairs), the common kestrel (more than 330,000 pairs) and the northern goshawk (more than 160,000 pairs). Ferguson-Lees et al. roughly estimated that the total population of the common buzzard ranges to nearly 5 million pairs but at time was including the now split-off species of eastern and Himalayan buzzards in those numbers. These numbers may be excessive but the total population of common buzzards is certain to total well over seven figures. More recently, the IUCN estimated the common buzzard (sans the Himalayan and eastern subspecies) to number somewhere between 2.1 and 3.7 million birds, which would put this buzzard one of the most numerous of all accipitrid family members (estimates for Eurasian sparrowhawks, red-tailed hawks and northern goshawks also may range over 2 million). In 1991, other than their absence in Iceland, after having been extent as breeder by 1910, buzzards recolonized Ireland sometime in the 1950s and has increased by the 1990s to 26 pairs. Supplemental feeding has reportedly helped the Irish buzzard population to rebound, especially where rabbits have decreased. Most other countries have at least four figures of breeding pairs. As of the 1990s, other countries such as Great Britain, France, Switzerland, Czech Republic, Poland, Sweden, Belarus and Ukraine all numbered pairs well into five figures, while Germany had an estimated 140,000 pairs and European Russian may have held 500,000 pairs. Between 44,000 and 61,000 pairs nested in Great Britain by 2001 with numbers gradually increasing after past persecution, habitat alteration and prey reductions, making it by far the most abundant diurnal raptor there. In Westphalia, Germany, population of buzzards was shown to nearly triple over the last few decades. The Westphalian buzzards are possibly benefiting from increasingly warmer mean climate, which in turn is increasing vulnerability of voles. However, the rate of increase was significantly greater in males than in females, in part because of reintroduced Eurasian eagle-owls to the region preying on nests (including the brooding mother), which may in turn put undue pressure on the local buzzard population.\nAt least 238 common buzzards killed through persecution were recovered in England from 1975 to 1989, largely through poisoning. Persecution did not significantly differ at any time due this span of years nor did the persecution rates decrease, nor did it when compared to rates of last survey of this in 1981. While some persecution persists in England, it is probably slightly less common today. The buzzard was found to be the most vulnerable raptor to power-line collision fatalities in Spain probably as it is one of the most common largish birds, and together with the common raven, it accounted for nearly a third of recorded electrocutions. Given its relative abundance, the common buzzard is held as an ideal bioindicator, as they are effected by a range of pesticide and metal contamination through pollution like other raptors but are largely resilient to these at the population levels. In turn, this allows biologists to study (and harvest if needed) the buzzards intensively and their environments without affecting their overall population. The lack of affect may be due to the buzzard's adaptability as well as its relatively short, terrestrially-based food chain, which exposes them to less risk of contamination and population depletions than raptors that prey more heavily on water-based prey (such as some large eagles) or other birds (such as falcons). Common buzzards are seldom vulnerable to egg-shell thinning from DDT as are other raptors but egg-shell thinning has been recorded. Other factors that negatively effect raptors have been studied in common buzzards are helminths, avipoxvirus and assorted other viruses.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4194", "revid": "46967642", "url": "https://en.wikipedia.org/wiki?curid=4194", "title": "Bohrium", "text": "element with atomic number 107 (Bh)\nBohrium is a synthetic chemical element; it has symbol Bh and atomic number 107. It is named after Danish physicist Niels Bohr. As a synthetic element, it can be created in particle accelerators but is not found in nature. All known isotopes of bohrium are highly radioactive; the most stable known isotope is 270Bh with a half-life of approximately 2.4 minutes, though the unconfirmed 278Bh may have a longer half-life of about 11.5 minutes.\nIn the periodic table, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements as the fifth member of the 6d series of transition metals. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.\nHistory.\nDiscovery.\nTwo groups claimed discovery of the element. Evidence of bohrium was first reported in 1976 by a Soviet research team led by Yuri Oganessian, in which targets of bismuth-209 and lead-208 were bombarded with accelerated nuclei of chromium-54 and manganese-55, respectively. Two activities, one with a half-life of one to two milliseconds, and the other with an approximately five-second half-life, were seen. Since the ratio of the intensities of these two activities was constant throughout the experiment, it was proposed that the first was from the isotope bohrium-261 and that the second was from its daughter dubnium-257. Later, the dubnium isotope was corrected to dubnium-258, which indeed has a five-second half-life (dubnium-257 has a one-second half-life); however, the half-life observed for its parent is much shorter than the half-lives later observed in the definitive discovery of bohrium at Darmstadt in 1981. The IUPAC/IUPAP Transfermium Working Group (TWG) concluded that while dubnium-258 was probably seen in this experiment, the evidence for the production of its parent bohrium-262 was not convincing enough.\nIn 1981, a German research team led by Peter Armbruster and Gottfried M\u00fcnzenberg at the GSI Helmholtz Centre for Heavy Ion Research (GSI Helmholtzzentrum f\u00fcr Schwerionenforschung) in Darmstadt bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:\n[&lt;noinclude /&gt;[bismuth-209|Bi]&lt;noinclude /&gt;] + [&lt;noinclude /&gt;[chromium-54|Cr]&lt;noinclude /&gt;] \u2192 [&lt;noinclude /&gt;[bohrium-262|Bh]&lt;noinclude /&gt;] + \nThis discovery was further substantiated by their detailed measurements of the alpha decay chain of the produced bohrium atoms to previously known isotopes of fermium and californium. The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.\nProposed names.\nIn September 1992, the German group suggested the name \"nielsbohrium\" with symbol \"Ns\" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction, and simultaneously help to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.\nThere was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted \"unnilseptium\" (symbol \"Uns\") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named \"bohrium\", not \"nielsbohrium\", since there was no precedent for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, \"bohrate\" and \"borate\". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name \"bohrium\", and thus the name \"bohrium\" for element 107 was recognized internationally in 1997; the names of the respective oxyanions of boron and bohrium remain unchanged despite their homophony.\nIsotopes.\nBohrium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes of bohrium have been reported with atomic masses 260\u2013262, 264\u2013267, 270\u2013272, 274, and 278, one of which, bohrium-262, has a known metastable state. All of these but the unconfirmed 278Bh decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.\nThe lighter isotopes usually have shorter half-lives; half-lives of under 100\u00a0ms for 260Bh, 261Bh, 262Bh, and 262mBh were observed. 264Bh, 265Bh, 266Bh, and 271Bh are more stable at around 1\u00a0s, and 267Bh and 272Bh have half-lives of about 10\u00a0s. The heaviest isotopes are the most stable, with 270Bh and 274Bh having measured half-lives of about 2.4\u00a0min and 40\u00a0s respectively, and the even heavier unconfirmed isotope 278Bh appearing to have an even longer half-life of about 11.5 minutes.\nThe most proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The five most neutron-rich ones with masses 270, 271, 272, 274, and 278 (unconfirmed) appear in the decay chains of 282Nh, 287Mc, 288Mc, 294Ts, and 290Fl respectively. The half-lives of bohrium isotopes range from about ten\u00a0milliseconds for 262mBh to about one\u00a0minute for 270Bh and 274Bh, extending to about 11.5 minutes for the unconfirmed 278Bh, which may have one of the longest half-lives among reported superheavy nuclides.\nPredicted properties.\nVery few properties of bohrium or its compounds have been measured; this is due to its extremely limited and expensive production and the fact that bohrium (and its parents) decays very quickly. A few singular chemistry-related properties have been measured, but properties of bohrium metal remain unknown and only predictions are available.\nChemical.\nBohrium is the fifth member of the 6d series of transition metals and the heaviest member of group 7 in the periodic table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well. The higher +7 oxidation state is more likely to exist in oxyanions, such as perbohrate, BhO4-, analogous to the lighter permanganate, pertechnetate, and perrhenate. Nevertheless, bohrium(VII) is likely to be unstable in aqueous solution, and would probably be easily reduced to the more stable bohrium(IV).\nThe lighter group 7 elements are known to form volatile heptoxides (M = Mn, Tc, Re), so bohrium should also form the volatile oxide . The oxide should dissolve in water to form perbohric acid, .\nRhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MO3Cl, so BhO3Cl should be formed in this reaction. Fluorination results in and for the heavier elements in addition to the rhenium compounds ReOF5 and ReF7. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties. Since the oxychlorides are asymmetrical, and they should have increasingly large dipole moments going down the group, they should become less volatile in the order TcO3Cl &gt; ReO3Cl &gt; BhO3Cl: this was experimentally confirmed in 2000 by measuring the enthalpies of adsorption of these three compounds. The values are for TcO3Cl and ReO3Cl are \u221251\u00a0kJ/mol and \u221261\u00a0kJ/mol respectively; the experimental value for BhO3Cl is \u221277.8\u00a0kJ/mol, very close to the theoretically expected value of \u221278.5\u00a0kJ/mol.\nPhysical and atomic.\nBohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (\"c\"/\"a\"\u00a0=\u00a01.62), similar to its lighter congener rhenium. Early predictions by Fricke estimated its density at 37.1\u00a0g/cm3, but newer calculations predict a somewhat lower value of 26\u201327\u00a0g/cm3.\nThe atomic radius of bohrium is expected to be around 128\u00a0pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Bh+ ion is predicted to have an electron configuration of [Rn] 5f14 6d4 7s2, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues manganese and technetium. Rhenium, on the other hand, follows its heavier congener bohrium in giving up a 5d electron before a 6s electron, as relativistic effects have become significant by the sixth period, where they cause among other things the yellow color of gold and the low melting point of mercury. The Bh2+ ion is expected to have an electron configuration of [Rn] 5f14 6d3 7s2; in contrast, the Re2+ ion is expected to have a [Xe] 4f14 5d5 configuration, this time analogous to manganese and technetium. The ionic radius of hexacoordinate heptavalent bohrium is expected to be 58\u00a0pm (heptavalent manganese, technetium, and rhenium having values of 46, 57, and 53\u00a0pm respectively). Pentavalent bohrium should have a larger ionic radius of 83\u00a0pm.\nExperimental chemistry.\nIn 1995, the first report on attempted isolation of the element was unsuccessful, prompting new theoretical studies to investigate how best to investigate bohrium (using its lighter homologs technetium and rhenium for comparison) and removing unwanted contaminating elements such as the trivalent actinides, the group 5 elements, and polonium.\nIn 2000, it was confirmed that although relativistic effects are important, bohrium behaves like a typical group 7 element. A team at the Paul Scherrer Institute (PSI) conducted a chemistry reaction using six atoms of 267Bh produced in the reaction between 249Bk and 22Ne ions. The resulting atoms were thermalised and reacted with a HCl/O2 mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as 108Tc) and rhenium (as 169Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7. The adsorption enthalpies of the oxychlorides of technetium, rhenium, and bohrium were measured in this experiment, agreeing very well with the theoretical predictions and implying a sequence of decreasing oxychloride volatility down group 7 of TcO3Cl &gt; ReO3Cl &gt; BhO3Cl.\n2 Bh + 3 O2 + 2 HCl \u2192 2 BhO3Cl + H2\nThe longer-lived heavy isotopes of bohrium, produced as the daughters of heavier elements, offer advantages for future radiochemical experiments. Although the heavy isotope 274Bh requires a rare and highly radioactive berkelium target for its production, the isotopes 272Bh, 271Bh, and 270Bh can be readily produced as daughters of more easily produced moscovium and nihonium isotopes.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4195", "revid": "1320154338", "url": "https://en.wikipedia.org/wiki?curid=4195", "title": "Barbara Olson", "text": "American lawyer (1955\u20132001)\nBarbara Kay Olson (n\u00e9e Bracher; December 27, 1955\u00a0\u2013 September 11, 2001) was an American lawyer and conservative television commentator who worked for CNN, Fox News Channel, and several other outlets. She was a passenger on American Airlines Flight 77 en route to a taping of Bill Maher's television show \"Politically Incorrect\" when it was flown into the Pentagon in the September 11 attacks.\nEarly life.\nOlson was born Barbara Kay Bracher in Houston, Texas, on December 27, 1955. Her older sister, Toni Bracher-Lawrence, was a member of the Houston City Council from 2004 to 2010. She graduated from Waltrip High School.\nPersonal life.\nShe married Theodore Olson in 1996, becoming his third wife.\nOlson was a frequent critic of the Bill Clinton administration and wrote a book about then\u2013First Lady Hillary Clinton, \"Hell to Pay: The Unfolding Story of Hillary Rodham Clinton\" (1999). Olson's second book, \"The Final Days: The Last, Desperate Abuses of Power by the Clinton White House\" was published posthumously.\nDeath and legacy.\nOlson was a passenger on American Airlines Flight 77, on her way to a taping of \"Politically Incorrect\" in Los Angeles, when it was flown into the Pentagon in the September 11 attacks.\nHer original plan had been to fly to California on September 10, but she waited until the next day so that she could wake up with her husband on his birthday, September 11. At the National September 11 Memorial, Olson's name is located on Panel S-70 of the South Pool, along with those of other passengers of Flight 77.\nThree months after the attacks, Olson's remains were identified. She was buried at her family's retreat in Wisconsin.\nSince November of 2001, the Federalist Society has given a lecture honoring her at their annual National Lawyers Convention.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4196", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=4196", "title": "Barnard's Star", "text": "Red dwarf star about six light-years from Earth in the constellation Ophiuchus\n&lt;/td&gt;\n! style=\"text-align: center; background-color: #FFFFC0;\" colspan=\"2\" | Observation dataEpoch J2000.0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Equinox J2000.0\n! style=\"text-align:left\" | Constellation\n! style=\"text-align:left\" | Right ascension\n! style=\"text-align:left\" | Declination\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(V)\n! style=\"background-color: #FFFFC0; text-align: center;\" colspan=\"2\"| Characteristics\n! style=\"text-align:left\" | Spectral\u00a0type\n! style=\"text-align:left\" | Apparent magnitude\u00a0(U)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(B)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(R)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(I)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(J)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(H)\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(K)\n! style=\"text-align:left\" | U\u2212B \n! style=\"text-align:left\" | B\u2212V \n! style=\"text-align:left\" | V\u2212R \n! style=\"text-align:left\" | R\u2212I \n! style=\"text-align:left\" | Variable\u00a0type\n&lt;/th&gt;&lt;/tr&gt;\n&lt;/th&gt;&lt;/tr&gt;\n&lt;/td&gt;\nBarnard's Star is a small red dwarf star in the constellation of Ophiuchus. At a distance of from Earth, it is the fourth-nearest-known individual star to the Sun after the three components of the Alpha Centauri system, and is the closest star in the northern celestial hemisphere. Its stellar mass is about 16% of the Sun's, and it has 19% of the Sun's diameter. Despite its proximity, the star has a dim apparent visual magnitude of +9.5 and is invisible to the unaided eye; it is much brighter in the infrared than in visible light.\nBarnard's Star is among the most studied red dwarfs because of its proximity and favorable location for observation near the celestial equator. Historically, research on Barnard's Star has focused on measuring its stellar characteristics, its astrometry, and also refining the limits of possible extrasolar planets. Although Barnard's Star is ancient, it still experiences stellar flare events, one being observed in 1998.\nBarnard's Star hosts a system of four close-orbiting, sub-Earth-mass planets (Barnard's Star b, c, d &amp; e). Multiple claims for a planetary system had been proposed since the beginning of the twentieth century, notably by Peter van de Kamp in the 1960s, but none were supported by follow-up studies, until the now known four-planet system was discovered by two independent teams of astronomers in 2024-2025.\nDiscovery and naming.\nThe star is named after Edward Emerson Barnard, an American astronomer who in 1916 measured its proper motion as 10.3 arcseconds per year relative to the Sun, the highest known for any star. The star had previously appeared on Harvard University photographic plates in 1888 and 1890.\nIn 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalogue and standardize proper names for stars. The WGSN approved the name \"Barnard's Star\" for this star on 1 February 2017 and it is now included in the List of IAU-approved Star Names.\nDescription.\nBarnard's Star is a red dwarf of the dim spectral type M4 and is too faint to see without a telescope; its apparent magnitude is 9.5.\nAt 7\u201312 billion years of age, Barnard's Star is considerably older than the Sun, which is 4.5 billion years old, and it might be among the oldest stars in the Milky Way galaxy. Barnard's Star has lost a great deal of rotational energy; the periodic slight changes in its brightness indicate that it rotates once in 130 days (the Sun rotates in 25). Given its age, Barnard's Star was long assumed to be quiescent in terms of stellar activity. In 1998, astronomers observed an intense stellar flare, showing that Barnard's Star is a flare star. Barnard's Star has the variable star designation V2500 Ophiuchi. In 2003, Barnard's Star presented the first detectable change in the radial velocity of a star caused by its motion. Further variability in the radial velocity of Barnard's Star was attributed to its stellar activity.\nThe proper motion of Barnard's Star corresponds to a relative lateral speed of 90km/s. The 10.3 arcseconds it travels in a year amount to a quarter of a degree in a human lifetime, roughly half the angular diameter of the full Moon.\nThe radial velocity of Barnard's Star is , as measured from the blueshift due to its motion toward the Sun. Combined with its proper motion and distance, this gives a \"space velocity\" (actual speed relative to the Sun) of . Barnard's Star will make its closest approach to the Sun around 11,800 CE, when it will approach to within about 3.75 light-years.\nProxima Centauri is the closest star to the Sun at a position currently 4.24 light-years distant from it. However, despite Barnard's Star's even closer pass to the Sun in 11,800 CE, it will still not then be the nearest star, since by that time Proxima Centauri will have moved to a yet-nearer proximity to the Sun. At the time of the star's closest pass by the Sun, Barnard's Star will still be too dim to be seen with the naked eye, since its apparent magnitude will only have increased by one magnitude to about 8.5 by then, still being 2.5 magnitudes short of visibility to the naked eye.\nBarnard's Star has a mass of about 0.16 solar masses (M\u2609), and a radius about 0.2 times that of the Sun. Thus, although Barnard's Star has roughly 150 times the mass of Jupiter (MJ), its radius is only roughly 2 times larger, due to its much higher density. Its effective temperature is about 3,220 kelvin, and it has a luminosity of only 0.0034 solar luminosities. Barnard's Star is so faint that if it were at the same distance from Earth as the Sun is, it would appear only 100 times brighter than a full moon, comparable to the brightness of the Sun at 80 astronomical units.\nBarnard's Star has 10\u201332% of the solar metallicity. Metallicity is the proportion of stellar mass made up of elements heavier than helium and helps classify stars relative to the galactic population. Barnard's Star seems to be typical of the old, red dwarf population II stars, yet these are also generally metal-poor halo stars. While sub-solar, Barnard's Star's metallicity is higher than that of a halo star and is in keeping with the low end of the metal-rich disk star range; this, plus its high space motion, have led to the designation \"intermediate population II star\", between a halo and disk star. However, some recently published scientific papers have given much higher estimates for the metallicity of the star, very close to the Sun's level, between 75 and 125% of the solar metallicity.\nPlanetary system.\nIn August 2024, by using data from ESPRESSO spectrograph of the Very Large Telescope, the existence of an exoplanet with a minimum mass of and orbital period of 3.15 days was confirmed. This constituted the first convincing evidence for a planet orbiting Barnard's Star. Additionally, three other candidate low-mass planets were proposed in this study. All of these planets orbit closer to the star than the habitable zone. The confirmed planet is designated Barnard's Star b (or Barnard b), a re-use of the designation originally used for the refuted super-Earth candidate. An examination of TESS photometry revealed no planetary transits, implying that the system is not viewed edge-on.\nIn March 2025, an independent follow-up study confirmed all four planets. The data ruled out planets with masses greater than in the habitable zone of Barnard's Star with 99% confidence. With a minimum mass of only , Barnard's Star e is the least massive exoplanet yet detected by the radial velocity method. The best-fit orbital solution implies the planets have slightly eccentric orbits, but simulations suggest that these orbits would be unstable while circular orbits remain stable, so the eccentricities may be overestimated.\n! align=center| Companion\n! align=center| Mass\n! align=center| Semimajor axis\n! align=center| Orbital period\n! align=center| Eccentricity\n! align=center| Inclination\n! align=center| Radius\nPrevious planetary claims.\nBarnard's Star has been subject to multiple claims of planets that were later disproven. From the early 1960s to the early 1970s, Peter van de Kamp argued that planets orbited Barnard's Star. His specific claims of large gas giants were refuted in the mid-1970s after much debate. In November 2018, a candidate super-Earth planetary companion was reported to orbit Barnard's Star. It was believed to have a minimum mass of 3.2\u00a0M\ud83d\udf28 and orbit at . However, work presented in July 2021 refuted the existence of this planet.\nAstrometric planetary claims.\nFor a decade from 1963 to about 1973, a substantial number of astronomers accepted a claim by Peter van de Kamp that he had detected, by using astrometry, a perturbation in the proper motion of Barnard's Star consistent with its having one or more planets comparable in mass with Jupiter. Van de Kamp had been observing the star from 1938, attempting, with colleagues at the Sproul Observatory at Swarthmore College, to find minuscule variations of one micrometre in its position on photographic plates consistent with orbital perturbations that would indicate a planetary companion; this involved as many as ten people averaging their results in looking at plates, to avoid systemic individual errors. \nVan de Kamp's initial suggestion was a planet having about 1.6\u00a0MJ at a distance of 4.4AU in a slightly eccentric orbit, and these measurements were apparently refined in a 1969 paper. Later that year, Van de Kamp suggested that there were two planets of 1.1 and 0.8\u00a0MJ.\nOther astronomers subsequently repeated Van de Kamp's measurements, and two papers in 1973 undermined the claim of a planet or planets. George Gatewood and Heinrich Eichhorn, at a different observatory and using newer plate measuring techniques, failed to verify the planetary companion. Another paper published by John L. Hershey four months earlier, also using the Swarthmore observatory, found that changes in the astrometric field of various stars correlated to the timing of adjustments and modifications that had been carried out on the refractor telescope's objective lens; the claimed planet was attributed to an artifact of maintenance and upgrade work. The affair has been discussed as part of a broader scientific review.\nVan de Kamp never acknowledged any error and published a further claim of two planets' existence as late as 1982; he died in 1995. Wulff Heintz, Van de Kamp's successor at Swarthmore and an expert on double stars, questioned his findings and began publishing criticisms from 1976 onwards. The two men were reported to have become estranged because of this.\nRefuted 2018 planetary claim.\nIn November 2018, an international team of astronomers announced the detection by radial velocity of a candidate super-Earth orbiting in relatively close proximity to Barnard's Star. Led by Ignasi Ribas of Spain their work, conducted over two decades of observation, provided strong evidence of the planet's existence. However, the existence of the planet was refuted in 2021, when the radial velocity signal was found to originate from long-term activity on the star itself, related to its rotation. Further studies in the following years confirmed this result.\nDubbed Barnard's Star b, the planet was thought to be near the stellar system's snow line, which is an ideal spot for the icy accretion of proto-planetary material. It was thought to orbit at 0.4AU every 233 days and had a proposed minimum mass of 3.2\u00a0M\ud83d\udf28. The planet would have most likely been frigid, with an estimated surface temperature of about , and lie outside Barnard Star's presumed habitable zone. Direct imaging of the planet and its tell-tale light signature would have been possible in the decade after its discovery. Further faint and unaccounted-for perturbations in the system suggested there may be a second planetary companion even farther out.\nRefining planetary boundaries.\nFor the more than four decades between van de Kamp's rejected claim and the eventual announcement of a planet candidate, Barnard's Star was carefully studied and the mass and orbital boundaries for possible planets were slowly tightened. M dwarfs such as Barnard's Star are more easily studied than larger stars in this regard because their lower masses render perturbations more obvious.\nNull results for planetary companions continued throughout the 1980s and 1990s, including interferometric work with the Hubble Space Telescope in 1999. Gatewood was able to show in 1995 that planets with 10\u00a0MJ were impossible around Barnard's Star, in a paper which helped refine the negative certainty regarding planetary objects in general. In 1999, the Hubble work further excluded planetary companions of 0.8\u00a0MJ with an orbital period of less than 1,000 days (Jupiter's orbital period is 4,332 days), while Kuerster determined in 2003 that within the habitable zone around Barnard's Star, planets are not possible with an \"\"M\" sin \"i\" value greater than 7.5 times the mass of the Earth (M\ud83d\udf28), or with a mass greater than 3.1 times the mass of Neptune (much lower than van de Kamp's smallest suggested value).\nIn 2013, a research paper was published that further refined planet mass boundaries for the star. Using radial velocity measurements, taken over a period of 25 years, from the Lick and Keck Observatories and applying Monte Carlo analysis for both circular and eccentric orbits, upper masses for planets out to 1,000-day orbits were determined. Planets above two Earth masses in orbits of less than 10 days were excluded, and planets of more than ten Earth masses out to a two-year orbit were also confidently ruled out. It was also discovered that the habitable zone of the star seemed to be devoid of roughly Earth-mass planets or larger, save for face-on orbits.\nEven though this research greatly restricted the possible properties of planets around Barnard's Star, it did not rule them out completely as terrestrial planets were always going to be difficult to detect. NASA's Space Interferometry Mission, which was to begin searching for extrasolar Earth-like planets, was reported to have chosen Barnard's Star as an early search target, however the mission was shut down in 2010. ESA's similar Darwin interferometry mission had the same goal, but was stripped of funding in 2007.\nThe analysis of radial velocities that eventually led to the announcement of a candidate super-Earth orbiting Barnard's Star was also used to set more precise upper mass limits for possible planets, up to and within the habitable zone: a maximum of 0.7\u00a0M\ud83d\udf28 up to the inner edge and 1.2\u00a0M\ud83d\udf28 on the outer edge of the optimistic habitable zone, corresponding to orbital periods of up to 10 and 40 days respectively. Therefore, it appears that Barnard's Star indeed does not host Earth-mass planets or larger, in hot and temperate orbits, unlike other M-dwarf stars that commonly have these types of planets in close-in orbits.\nStellar flares.\n1998.\nIn 1998 a stellar flare on Barnard's Star was detected based on changes in the spectral emissions on 17 July during an unrelated search for variations in the proper motion. Four years passed before the flare was fully analyzed, at which point it was suggested that the flare's temperature was 8,000K, more than twice the normal temperature of the star. Given the essentially random nature of flares, Diane Paulson, one of the authors of that study, noted that \"the star would be fantastic for amateurs to observe\".\nThe flare was surprising because intense stellar activity is not expected in stars of such age. Flares are not completely understood, but are believed to be caused by strong magnetic fields, which suppress plasma convection and lead to sudden outbursts: strong magnetic fields occur in rapidly rotating stars, while old stars tend to rotate slowly. For Barnard's Star to undergo an event of such magnitude is thus presumed to be a rarity. Research on the star's periodicity, or changes in stellar activity over a given timescale, also suggest it ought to be quiescent; 1998 research showed weak evidence for periodic variation in the star's brightness, noting only one possible starspot over 130 days.\nStellar activity of this sort has created interest in using Barnard's Star as a proxy to understand similar stars. It is hoped that photometric studies of its X-ray and UV emissions will shed light on the large population of old M dwarfs in the galaxy. Such research has astrobiological implications: given that the habitable zones of M dwarfs are close to the star, any planet located therein would be strongly affected by solar flares, stellar winds, and plasma ejection events.\n2019.\nIn 2019, two additional ultraviolet stellar flares were detected, each with far-ultraviolet energy of 3\u00d71022 joules, together with one X-ray stellar flare with energy 1.6\u00d71022 joules. The flare rate observed to date is enough to cause loss of 87 Earth atmospheres per billion years through thermal processes and \u22483 Earth atmospheres per billion years through ion loss processes on Barnard's Star b.\nEnvironment.\nBarnard's Star shares much the same neighborhood as the Sun. The neighbors of Barnard's Star are generally of red dwarf size, the smallest and most common star type. Its closest neighbor is currently the red dwarf Ross 154, at a distance of 1.66 parsecs (5.41 light-years). The Sun (5.98 light-years) and Alpha Centauri (6.47 light-years) are, respectively, the next closest systems. From Barnard's Star, the Sun would appear on the diametrically opposite side of the sky at coordinates RA=5h 57m 48.5s, Dec=\u00b0\u00a041\u2032\u00a036\u2033, in the westernmost part of the constellation Monoceros. The absolute magnitude of the Sun is 4.83, and at a distance of 1.834 parsecs, it would be a first-magnitude star, as Pollux is from the Earth.\nProposed exploration.\nProject Daedalus.\nBarnard's Star was studied as part of Project Daedalus. Undertaken between 1973 and 1978, the study suggested that rapid, uncrewed travel to another star system was possible with existing or near-future technology. Barnard's Star was chosen as a target partly because it was believed to have planets.\nThe theoretical model suggested that a nuclear pulse rocket employing nuclear fusion (specifically, electron bombardment of deuterium and helium-3) and accelerating for four years could achieve a velocity of 12% of the speed of light. The star could then be reached in 50 years, within a human lifetime. Along with detailed investigation of the star and any companions, the interstellar medium would be examined and baseline astrometric readings performed.\nThe initial Project Daedalus model sparked further theoretical research. In 1980, Robert Freitas suggested a more ambitious plan: a self-replicating spacecraft intended to search for and make contact with extraterrestrial life. Built and launched in Jupiter's orbit, it would reach Barnard's Star in 47 years under parameters similar to those of the original Project Daedalus. Once at the star, it would begin automated self-replication, constructing a factory, initially to manufacture exploratory probes and eventually to create a copy of the original spacecraft after 1,000 years.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4199", "revid": "1265255280", "url": "https://en.wikipedia.org/wiki?curid=4199", "title": "Bayer designation", "text": "Star naming system\nA Bayer designation is a stellar designation in which a specific star is identified by a Greek or Latin letter followed by the genitive form of its parent constellation's Latin name. The original list of Bayer designations contained 1564 stars. The brighter stars were assigned their first systematic names by the German astronomer Johann Bayer in 1603, in his star atlas \"Uranometria\". Bayer catalogued only a few stars too far south to be seen from Germany, but later astronomers (including Nicolas-Louis de Lacaille and Benjamin Apthorp Gould) supplemented Bayer's catalog with entries for southern constellations.\nScheme.\nBayer assigned a lowercase Greek letter (alpha (\u03b1), beta (\u03b2), gamma (\u03b3), etc.) or a Latin letter (A, b, c, etc.) to each star he catalogued, combined with the Latin name of the star's parent constellation in genitive (possessive) form. The constellation name is frequently abbreviated to a standard three-letter form. For example, Aldebaran in the constellation Taurus (the Bull) is designated \"\u03b1\u00a0Tauri\" (abbreviated \"\u03b1\u00a0Tau\", pronounced \"Alpha Tauri\"), which means \"Alpha of the Bull\".\nBayer used Greek letters for the brighter stars, but the Greek alphabet has only twenty-four letters, while a single constellation may contain fifty or more stars visible to the naked eye. When the Greek letters ran out, Bayer continued with Latin letters: uppercase \"A\", followed by lowercase \"b\" through \"z\" (omitting \"j\" and \"v\", but \"o\" was included), for a total of another 24\u00a0letters.\nBayer did not label \"permanent\" stars with uppercase letters (except for \"A\", which he used instead of \"a\" to avoid confusion with \"\u03b1\"). However, a number of stars in southern constellations have uppercase letter designations, like B Centauri and G Scorpii. These letters were assigned by later astronomers, notably Lacaille in his \"Coelum Australe Stelliferum\" and Gould in his \"Uranometria Argentina\". Lacaille followed Bayer's use of Greek letters, but this was insufficient for many constellations. He used first the lowercase letters, starting with \"a\", and if needed the uppercase letters, starting with \"A\", thus deviating somewhat from Bayer's practice. Lacaille used the Latin alphabet three times over in the large constellation Argo Navis, once for each of the three areas that are now the constellations of Carina, Puppis and Vela. That was still insufficient for the number of stars, so he also used uppercase Latin letters such as N\u00a0Velorum and Q Puppis. Lacaille assigned uppercase letters between R and Z in several constellations, but these have either been dropped to allow the assignment of those letters to variable stars or have actually turned out to be variable.\nOrder by magnitude class.\nIn most constellations, Bayer assigned Greek and Latin letters to stars within a constellation in rough order of apparent brightness, from brightest to dimmest. The order is not necessarily a precise labeling from brightest to dimmest: in Bayer's day stellar brightness could not be measured precisely. Instead, stars were traditionally assigned to one of six magnitude classes (the brightest to first magnitude, the dimmest to sixth), and Bayer typically ordered stars within a constellation by class: all the first-magnitude stars (in some order), followed by all the second-magnitude stars, and so on. Within each magnitude class, Bayer made no attempt to arrange stars by relative brightness. As a result, the brightest star in each class did not always get listed first in Bayer's order\u2014and the brightest star overall did not necessarily get the designation \"Alpha\". A good example is the constellation Gemini, where Pollux is Beta Geminorum and the slightly dimmer Castor is Alpha Geminorum.\nIn addition, Bayer did not always follow the magnitude class rule; he sometimes assigned letters to stars according to their location within a constellation, or the order of their rising, or to historical or mythological details. Occasionally the order looks quite arbitrary.\nOf the 88 modern constellations, there are at least 30 in which Alpha is not the brightest star, and four of those lack a star labeled \"Alpha\" altogether. The constellations with no Alpha-designated star include Vela and Puppis\u2014both formerly part of Argo Navis, whose Greek-letter stars were split among three constellations. Canopus, the former \u03b1 Argus, is now \u03b1 Carinae in the modern constellation Carina. Norma's Alpha and Beta were reassigned to Scorpius and re-designated N and H Scorpii respectively, leaving Norma with no Alpha. Francis Baily died before designating an Alpha in Leo Minor, so it also has no Alpha. (The star 46 Leonis Minoris would have been the obvious candidate.)\nOrion as an example.\nIn Orion, Bayer first designated Betelgeuse and Rigel, the two 1st-magnitude stars (those of magnitude 1.5 or less), as Alpha and Beta from north to south, with Betelgeuse (the shoulder) coming ahead of Rigel (the foot), even though the latter is usually the brighter. (Betelgeuse is a variable star and can at its maximum occasionally outshine Rigel.) Bayer then repeated the procedure for the stars of the 2nd magnitude, labeling them from \"gamma\" through \"zeta\" in \"top-down\" (north-to-south) order. Letters as far as Latin \"p\" were used for stars of the sixth magnitude.\nBayer's miscellaneous labels.\nAlthough Bayer did not use uppercase Latin letters (except \"A\") for \"fixed stars\", he did use them to label other items shown on his charts, such as neighboring constellations, \"temporary stars\", miscellaneous astronomical objects, or reference lines like the Tropic of Cancer. In Cygnus, for example, Bayer's fixed stars run through \"g\", and on this chart Bayer employs \"H\" through \"P\" as miscellaneous labels, mostly for neighboring constellations. Bayer did not intend such labels as catalog designations, but some have survived to refer to astronomical objects: P Cygni for example is still used as a designation for Nova Cyg 1600. Tycho's Star (SN 1572), another \"temporary star\", appears as B Cassiopeiae. In charts for constellations that did not exhaust the Greek letters, Bayer sometimes used the leftover Greek letters for miscellaneous labels as well.\nRevised designations.\nPtolemy designated four stars as \"border stars\", each shared by two constellations: Alpheratz (in Andromeda and Pegasus), Elnath (in Taurus and Auriga), Nu Bo\u00f6tis (Nu1 and Nu2)(in Bo\u00f6tes and Hercules) and Fomalhaut (in Piscis Austrinus and Aquarius). Bayer assigned the first three of these stars a Greek letter from both constellations: Alpha Andromedae \n Delta Pegasi, Beta Tauri \n Gamma Aurigae, and Nu Bo\u00f6tis \n Psi Herculis. (He catalogued Fomalhaut only once, as Alpha Piscis Austrini.) When the International Astronomical Union (IAU) assigned definite boundaries to the constellations in 1930, it declared that stars and other celestial objects can belong to only one constellation. Consequently, the redundant second designation in each pair above has dropped out of use.\nBayer assigned two stars duplicate names by mistake: Xi Arietis (duplicated as Psi Ceti) and Kappa Ceti (Kappa1 and Kappa2) (duplicated as g Tauri). He corrected these in a later atlas, and the duplicate names were no longer used.\nOther cases of multiple Bayer designations arose when stars named by Bayer in one constellation were transferred by later astronomers to a different constellation. Bayer's Gamma and Omicron Scorpii, for example, were later reassigned from Scorpius to Libra and given the new names Sigma and Upsilon Librae. (To add to the confusion, the star now known as Omicron Scorpii was not named by Bayer but was assigned the designation o Scorpii (Latin lowercase 'o') by Lacaille\u2014which later astronomers misinterpreted as omicron once Bayer's omicron had been reassigned to Libra.)\nA few stars no longer lie (according to the modern constellation boundaries) within the constellation for which they are named. The proper motion of Rho Aquilae, for example, carried it across the boundary into Delphinus in 1992.\nA further complication is the use of numeric superscripts to distinguish neighboring stars that Bayer (or a later astronomer) labeled with a common letter. Usually these are double stars (mostly optical doubles rather than true binary stars), but there are some exceptions such as the chain of stars \u03c01, \u03c02, \u03c03, \u03c04, \u03c05 and \u03c06 Orionis. The most stars given the same Bayer designation but with an extra number attached to it is Psi Aurigae. (\u03c81, \u03c82, \u03c83, \u03c84, \u03c85, \u03c86, \u03c87, \u03c88, \u03c89, \u03c810, although according to the modern IAU constellation boundaries, \u03c810 lies in Lynx).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4200", "revid": "34017952", "url": "https://en.wikipedia.org/wiki?curid=4200", "title": "Bo\u00f6tes", "text": "Constellation in the northern celestial hemisphere\nBo\u00f6tes ( ) is a constellation in the northern sky, located between 0\u00b0 and +60\u00b0 declination, and 13 and 16 hours of right ascension on the celestial sphere. The name comes from , which comes from 'herdsman' or 'plowman' (literally, 'ox-driver'; from \"bo\u00fbs\" 'cow').\nOne of the 48 constellations described by the 2nd-century astronomer Ptolemy, Bo\u00f6tes is now one of the 88 modern constellations. It contains the fourth-brightest star in the night sky, the orange giant Arcturus. Epsilon Bo\u00f6tis, or Izar, is a colourful multiple star popular with amateur astronomers. Bo\u00f6tes is home to many other bright stars, including eight above the fourth magnitude and an additional 21 above the fifth magnitude, making a total of 29 stars easily visible to the naked eye.\nHistory and mythology.\nIn ancient Babylon, the stars of Bo\u00f6tes were known as SHU.PA. They were apparently depicted as the god Enlil, who was the leader of the Babylonian pantheon and special patron of farmers. Bo\u00f6tes may have been represented by the animal foreleg constellation in ancient Egypt, resembling that of an ox sufficiently to have been originally proposed as the \"foreleg of ox\" by Berio. \nHomer mentions Bo\u00f6tes in the \"Odyssey\" as a celestial reference for navigation, describing it as \"late-setting\" or \"slow to set\". Exactly whom Bo\u00f6tes is supposed to represent in Greek mythology is not clear. According to one version, he was a son of Demeter, Philomenus, twin brother of Plutus, a plowman who drove the oxen in the constellation Ursa Major. This agrees with the constellation's name. The ancient Greeks saw the asterism now called the \"Big Dipper\" or \"Plough\" as a cart with oxen. Some myths say that Bo\u00f6tes invented the plow and was memorialized for his ingenuity as a constellation.\nAnother myth associated with Bo\u00f6tes by Hyginus is that of Icarius, who was schooled as a grape farmer and winemaker by Dionysus. Icarius made wine so strong that those who drank it appeared poisoned, which caused shepherds to avenge their supposedly poisoned friends by killing Icarius. Maera, Icarius' dog, brought his daughter Erigone to her father's body, whereupon both she and the dog died by suicide. Zeus then chose to honor all three by placing them in the sky as constellations: Icarius as Bo\u00f6tes, Erigone as Virgo, and Maera as Canis Major or Canis Minor.\nFollowing another reading, the constellation is identified with Arcas and also referred to as Arcas and Arcturus, son of Zeus and Callisto. Arcas was brought up by his maternal grandfather Lycaon, to whom one day Zeus went and had a meal. To verify that the guest was really the king of the gods, Lycaon killed his grandson and prepared a meal made from his flesh. Zeus noticed and became very angry, transforming Lycaon into a wolf and giving life back to his son. In the meantime Callisto had been transformed into a she-bear by Zeus's wife Hera, who was angry at Zeus's infidelity. This is corroborated by the Greek name for Bo\u00f6tes, \"Arctophylax\", which means \"Bear Watcher\".\nCallisto, in the form of a bear was almost killed by her son, who was out hunting. Zeus rescued her, taking her into the sky where she became Ursa Major, \"the Great Bear\". Arcturus, the name of the constellation's brightest star, comes from the Greek word meaning \"guardian of the bear\". Sometimes Arcturus is depicted as leading the hunting dogs of nearby Canes Venatici and driving the bears of Ursa Major and Ursa Minor.\nSeveral former constellations were formed from stars now included in Bo\u00f6tes. Quadrans Muralis, the Quadrant, was a constellation created near Beta Bo\u00f6tis from faint stars. It was designated in 1795 by J\u00e9r\u00f4me Lalande, an astronomer who used a quadrant to perform detailed astronometric measurements. Lalande worked with Nicole-Reine Lepaute and others to predict the 1758 return of Halley's Comet. Quadrans Muralis was formed from the stars of eastern Bo\u00f6tes, western Hercules and Draco. It was originally called \"Le Mural\" by Jean Fortin in his 1795 \"Atlas C\u00e9leste\"; it was not given the name \"Quadrans Muralis\" until Johann Bode's 1801 \"Uranographia\". The constellation was quite faint, with its brightest stars reaching the 5th magnitude. Mons Maenalus, representing the Maenalus mountains, was created by Johannes Hevelius in 1687 at the foot of the constellation's figure. The mountain was named for the son of Lycaon, Maenalus. The mountain, one of Diana's hunting grounds, was also holy to Pan.\nNon-Western astronomy.\nThe stars of Bo\u00f6tes were incorporated into many different Chinese constellations. Arcturus was part of the most prominent of these, variously designated as the celestial king's throne (\"Tian Wang\") or the Blue Dragon's horn (\"Daijiao\"); the name \"Daijiao\", meaning \"great horn\", is more common. Arcturus was given such importance in Chinese celestial mythology because of its status marking the beginning of the lunar calendar, as well as its status as the brightest star in the northern night sky.\nTwo constellations flanked \"Daijiao\": \"Yousheti\" to the right and \"Zuosheti\" to the left; they represented companions that orchestrated the seasons. \"Zuosheti\" was formed from modern Zeta, Omicron and Pi Bo\u00f6tis, while \"Yousheti\" was formed from modern Eta, Tau and Upsilon Bo\u00f6tis. \"Dixi\", the Emperor's ceremonial banquet mat, was north of Arcturus, consisting of the stars 12, 11 and 9 Bo\u00f6tis. Another northern constellation was \"Qigong\", the Seven Dukes, which mostly straddled the Bo\u00f6tes-Hercules border. It included either Delta Bo\u00f6tis or Beta Bo\u00f6tis as its terminus.\nThe other Chinese constellations made up of the stars of Bo\u00f6tes existed in the modern constellation's north; they are all representations of weapons. \"Tianqiang\", the spear, was formed from Iota, Kappa and Theta Bo\u00f6tis; \"Genghe\", variously representing a lance or shield, was formed from Epsilon, Rho and Sigma Bo\u00f6tis.\nThere were also two weapons made up of a singular star. \"Xuange\", the halberd, was represented by Lambda Bo\u00f6tis, and \"Zhaoyao\", either the sword or the spear, was represented by Gamma Bo\u00f6tis.\nTwo Chinese constellations have an uncertain placement in Bo\u00f6tes. \"Kangchi\", the lake, was placed south of Arcturus, though its specific location is disputed. It may have been placed entirely in Bo\u00f6tes, on either side of the Bo\u00f6tes-Virgo border, or on either side of the Virgo-Libra border. The constellation \"Zhouding\", a bronze tripod-mounted container used for food, was sometimes cited as the stars 1, 2 and 6 Bo\u00f6tis. However, it has also been associated with three stars in Coma Berenices.\nBo\u00f6tes is also known to Native American cultures. In Yup'ik language, Bo\u00f6tes is \"Taluyaq\", literally \"fish trap,\" and the funnel-shaped part of the fish trap is known as \"Ilulirat.\"\nCharacteristics.\nBo\u00f6tes is a constellation bordered by Virgo to the south, Coma Berenices and Canes Venatici to the west, Ursa Major to the northwest, Draco to the northeast, and Hercules, Corona Borealis and Serpens Caput to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is \"Boo\". The official constellation boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of 16 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between 13h 36.1m and 15h 49.3m, while the declination coordinates stretch from +7.36\u00b0 to +55.1\u00b0. Covering 907 square degrees, Bo\u00f6tes culminates at midnight around 2 May and ranks 13th in area.\nColloquially, its pattern of stars has been likened to a kite or ice cream cone. However, depictions of Bo\u00f6tes have varied historically. Aratus described him circling the north pole, herding the two bears. Later ancient Greek depictions, described by Ptolemy, have him holding the reins of his hunting dogs (Canes Venatici) in his left hand, with a spear, club, or staff in his right hand. After Hevelius introduced Mons Maenalus in 1681, Bo\u00f6tes was often depicted standing on the Peloponnese mountain. By 1801, when Johann Bode published his \"Uranographia\", Bo\u00f6tes had acquired a sickle, which was also held in his left hand.\nThe placement of Arcturus has also been mutable through the centuries. Traditionally, Arcturus lay between his thighs, as Ptolemy depicted him. However, Germanicus Caesar deviated from this tradition by placing Arcturus \"where his garment is fastened by a knot\".\nFeatures.\nStars.\nIn his \"Uranometria\", Johann Bayer used the Greek letters alpha through to omega and then A to k to label what he saw as the most prominent 35 stars in the constellation, with subsequent astronomers splitting Kappa, Mu, Nu and Pi as two stars each. Nu is also the same star as Psi Herculis. John Flamsteed numbered 54 stars for the constellation.\nLocated 36.7 light-years from Earth, Arcturus, or Alpha Bo\u00f6tis, is the brightest star in Bo\u00f6tes and the fourth-brightest star in the sky at an apparent magnitude of \u22120.05; It is also the brightest star north of the celestial equator, just shading out Vega and Capella. Its name comes from the Greek for \"bear-keeper\". An orange giant of spectral class K1.5III, Arcturus is an ageing star that has exhausted its core supply of hydrogen and cooled and expanded to a diameter of 27 solar diameters, equivalent to approximately 32 million kilometers. Though its mass is approximately one solar mass (M\u2609), Arcturus shines with 133 times the luminosity of the Sun (L\u2609).\nBayer located Arcturus above the Herdman's left knee in his \"Uranometria\". Nearby Eta Bo\u00f6tis, or Muphrid, is the uppermost star denoting the left leg. It is a 2.68-magnitude star 37 light-years distant with a spectral class of G0IV, indicating it has just exhausted its core hydrogen and is beginning to expand and cool. It is 9 times as luminous as the Sun and has 2.7 times its diameter. Analysis of its spectrum reveals that it is a spectroscopic binary. Muphrid and Arcturus lie only 3.3 light-years away from each other. Viewed from Arcturus, Muphrid would have a visual magnitude of \u22122\u00bd, while Arcturus would be around visual magnitude \u22124\u00bd when seen from Muphrid.\nMarking the herdsman's head is Beta Bo\u00f6tis, or Nekkar, a yellow giant of magnitude 3.5 and spectral type G8IIIa. Like Arcturus, it has expanded and cooled off the main sequence\u2014likely to have lived most of its stellar life as a blue-white B-type main sequence star. Its common name comes from the Arabic phrase for \"ox-driver\". It is 219 light-years away and has a luminosity of 58\u00a0L\u2609.\nLocated 86 light-years distant, Gamma Bo\u00f6tis, or Seginus, is a white giant star of spectral class A7III, with a luminosity 34 times and diameter 3.5 times that of the Sun. It is a Delta Scuti variable, ranging between magnitudes 3.02 and 3.07 every 7 hours. These stars are short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study asteroseismology.\nDelta Bo\u00f6tis is a wide double star with a primary of magnitude 3.5 and a secondary of magnitude 7.8. The primary is a yellow giant that has cooled and expanded to 10.4 times the diameter of the Sun. Of spectral class G8IV, it is around 121 light-years away, while the secondary is a yellow main sequence star of spectral type G0V. The two are thought to take 120,000 years to orbit each other.\nMu Bo\u00f6tis, known as Alkalurops, is a triple star popular with amateur astronomers. It has an overall magnitude of 4.3 and is 121 light-years away. Its name is from the Arabic phrase for \"club\" or \"staff\". The primary appears to be of magnitude 4.3 and is blue-white. The secondary appears to be of magnitude 6.5, but is actually a close double star itself with a primary of magnitude 7.0 and a secondary of magnitude 7.6. The secondary and tertiary stars have an orbital period of 260 years. The primary has an absolute magnitude of 2.6 and is of spectral class F0. The secondary and tertiary stars are separated by 2 arcseconds; the primary and secondary are separated by 109.1 arcseconds at an angle of 171 degrees.\nNu Bo\u00f6tis is an optical double star. The primary is an orange giant of magnitude 5.0 and the secondary is a white star of magnitude 5.0. The primary is 870 light-years away and the secondary is 430 light-years.\nEpsilon Bo\u00f6tis, also known as \"Izar\" or \"Pulcherrima\", is a close triple star popular with amateur astronomers and the most prominent binary star in Bo\u00f6tes. The primary is a yellow- or orange-hued magnitude 2.5 giant star, the secondary is a magnitude 4.6 blue-hued main-sequence star, and the tertiary is a magnitude 12.0 star. The system is 210 light-years away. The name \"Izar\" comes from the Arabic word for \"girdle\" or \"loincloth\", referring to its location in the constellation. The name \"Pulcherrima\" comes from the Latin phrase for \"most beautiful\", referring to its contrasting colors in a telescope. The primary and secondary stars are separated by 2.9 arcseconds at an angle of 341 degrees; the primary's spectral class is K0 and it has a luminosity of 200\u00a0L\u2609. To the naked eye, Izar has a magnitude of 2.37.\nNearby Rho and Sigma Bo\u00f6tis denote the herdsman's waist. Rho is an orange giant of spectral type K3III located around 160 light-years from Earth. It is ever so slightly variable, wavering by 0.003 of a magnitude from its average of 3.57. Sigma, a yellow-white main-sequence star of spectral type F3V, is suspected of varying in brightness from 4.45 to 4.49. It is around 52 light-years distant.\nTraditionally known as \"Aul\u0101d al Dhi\u02bcbah\" (\u0623\u0648\u0644\u0627\u062f \u0627\u0644\u0636\u0628\u0627\u0639 \u2013 \"aul\u0101d al dhi\u02bcb\"), \"the Whelps of the Hyenas\", Theta, Iota, Kappa and Lambda Bo\u00f6tis (or Xuange) are a small group of stars in the far north of the constellation. The magnitude 4.05 Theta Bo\u00f6tis has a spectral type of F7 and an absolute magnitude of 3.8. Iota Bo\u00f6tis is a triple star with a primary of magnitude 4.8 and spectral class of A7, a secondary of magnitude 7.5, and a tertiary of magnitude 12.6. The primary is 97 light-years away. The primary and secondary stars are separated by 38.5 arcseconds, at an angle of 33 degrees. The primary and tertiary stars are separated by 86.7 arcseconds at an angle of 194 degrees. Both the primary and tertiary appear white in a telescope, but the secondary appears yellow-hued.\nKappa Bo\u00f6tis is another wide double star. The primary is 155 light-years away and has a magnitude of 4.5. The secondary is 196 light-years away and has a magnitude of 6.6. The two components are separated by 13.4 arcseconds, at an angle of 236 degrees. The primary, with spectral class A7, appears white and the secondary appears bluish.\nAn apparent magnitude 4.18 type A0p star, Lambda Bo\u00f6tis is the prototype of a class of chemically peculiar stars, only some of which pulsate as Delta Scuti-type stars. The distinction between the Lambda Bo\u00f6tis stars as a class of stars with peculiar spectra, and the Delta Scuti stars whose class describes pulsation in low-overtone pressure modes, is an important one. While many Lambda Bo\u00f6tis stars pulsate and are Delta Scuti stars, not many Delta Scuti stars have Lambda Bo\u00f6tis peculiarities, since the Lambda Bo\u00f6tis stars are a much rarer class whose members can be found both inside and outside the Delta Scuti instability strip. Lambda Bo\u00f6tis stars are dwarf stars that can be either spectral class A or F. Like BL Bo\u00f6tis-type stars they are metal-poor. Scientists have had difficulty explaining the characteristics of Lambda Bo\u00f6tis stars, partly because only around 60 confirmed members exist, but also due to heterogeneity in the literature. Lambda has an absolute magnitude of 1.8.\nThere are two dimmer F-type stars, magnitude 4.83 12 Bo\u00f6tis, class F8; and magnitude 4.93 45 Bo\u00f6tis, class F5. Xi Bo\u00f6tis is a G8 yellow dwarf of magnitude 4.55, and absolute magnitude is 5.5. Two dimmer G-type stars are magnitude 4.86 31 Bo\u00f6tis, class G8, and magnitude 4.76 44 Bo\u00f6tis, class G0.\nOf apparent magnitude 4.06, Upsilon Bo\u00f6tis has a spectral class of K5 and an absolute magnitude of \u22120.3. Dimmer than Upsilon Bo\u00f6tis is magnitude 4.54 Phi Bo\u00f6tis, with a spectral class of K2 and an absolute magnitude of \u22120.1. Just slightly dimmer than Phi at magnitude 4.60 is O Bo\u00f6tis, which, like Izar, has a spectral class of K0. O Bo\u00f6tis has an absolute magnitude of 0.2. The other four dim stars are magnitude 4.91 6 Bo\u00f6tis, class K4; magnitude 4.86 20 Bo\u00f6tis, class K3; magnitude 4.81 Omega Bo\u00f6tis, class K4; and magnitude 4.83 A Bo\u00f6tis, class K1.\nThere is one bright B-class star in Bo\u00f6tes; magnitude 4.93 Pi1 Bo\u00f6tis, also called Alazal. It has a spectral class of B9 and is 40 parsecs from Earth. There is also one M-type star, magnitude 4.81 34 Bo\u00f6tis. It is of class gM0.\nMultiple stars.\nBesides Pulcherrima and Alkalurops, there are several other binary stars in Bo\u00f6tes:\n44 Bo\u00f6tis (i Bo\u00f6tis) is a double variable star 42 light-years away. It has an overall magnitude of 4.8 and appears yellow to the naked eye. The primary is of magnitude 5.3 and the secondary is of magnitude 6.1; their orbital period is 220 years. The secondary is itself an eclipsing variable star with a range of 0.6 magnitudes; its orbital period is 6.4 hours. It is a W Ursae Majoris variable that ranges in magnitude from a minimum of 7.1 to a maximum of 6.5 every 0.27 days. Both stars are G-type stars. Another eclipsing binary star is ZZ Bo\u00f6tis, which has two F2-type components of almost equal mass, and ranges in magnitude from a minimum of 6.79 to a maximum of 7.44 over a period of 5.0 days.\nVariable stars.\nTwo of the brighter Mira-type variable stars in the constellation are R and S Bo\u00f6tis. Both are red giants that range greatly in magnitude\u2014from 6.2 to 13.1 over 223.4 days, and 7.8 to 13.8 over a period of 270.7 days, respectively. Also red giants, V and W Bo\u00f6tis are semi-regular variable stars that range in magnitude from 7.0 to 12.0 over a period of 258 days, and magnitude 4.7 to 5.4 over 450 days, respectively.\nBL Bo\u00f6tis is the prototype of its class of pulsating variable stars, the anomalous Cepheids. These stars are somewhat similar to Cepheid variables, but they do not have the same relationship between their period and luminosity. Their periods are similar to RRAB variables; however, they are far brighter than these stars. BL Bo\u00f6tis is a member of the cluster NGC 5466. Anomalous Cepheids are metal poor and have masses not much larger than the Sun's, on average, 1.5\u00a0M\u2609. BL Bo\u00f6tis type stars are a subtype of RR Lyrae variables.\nT Bo\u00f6tis was a nova observed in April 1860 at a magnitude of 9.7. It has never been observed since, but that does not preclude the possibility of it being a highly irregular variable star or a recurrent nova.\nStars with planetary systems.\nExtrasolar planets have been discovered encircling ten stars in Bo\u00f6tes as of 2012. Tau Bo\u00f6tis is orbited by a large planet, discovered in 1999. The host star itself is a magnitude 4.5 star of type F7V, 15.6 parsecs from Earth. It has a mass of 1.3\u00a0M\u2609 and a radius of 1.331 solar radii (R\u2609); a companion, GJ527B, orbits at a distance of 240 AU. Tau Bo\u00f6tis b, the sole planet discovered in the system, orbits at a distance of 0.046 AU every 3.31 days. Discovered through radial velocity measurements, it has a mass of 5.95 Jupiter masses (MJ). This makes it a hot Jupiter. The host star and planet are tidally locked, meaning that the planet's orbit and the star's particularly high rotation are synchronized. Furthermore, a slight variability in the host star's light may be caused by magnetic interactions with the planet. Carbon monoxide is present in the planet's atmosphere. Tau Bo\u00f6tis b does not transit its star, rather, its orbit is inclined 46 degrees.\nLike Tau Bo\u00f6tis b, HAT-P-4b is also a hot Jupiter. It is noted for orbiting a particularly metal-rich host star and being of low density. Discovered in 2007, HAT-P-4 b has a mass of 0.68\u00a0MJ and a radius of 1.27\u00a0RJ. It orbits every 3.05 days at a distance of 0.04 AU. HAT-P-4, the host star, is an F-type star of magnitude 11.2, 310 parsecs from Earth. It is larger than the Sun, with a mass of 1.26\u00a0M\u2609 and a radius of 1.59\u00a0R\u2609.\nBo\u00f6tes is also home to multiple-planet systems. HD 128311 is the host star for a two-planet system, consisting of HD 128311 b and HD 128311 c, discovered in 2002 and 2005, respectively. HD 128311 b is the smaller planet, with a mass of 2.18\u00a0MJ; it was discovered through radial velocity observations. It orbits at almost the same distance as Earth, at 1.099 AU; however, its orbital period is significantly longer at 448.6 days.\nThe larger of the two, HD 128311 c, has a mass of 3.21\u00a0MJ and was discovered in the same manner. It orbits every 919 days inclined at 50\u00b0, and is 1.76 AU from the host star. The host star, HD 128311, is a K0V-type star located 16.6 parsecs from Earth. It is smaller than the Sun, with a mass of 0.84\u00a0M\u2609 and a radius of 0.73\u00a0R\u2609; it also appears below the threshold of naked-eye visibility at an apparent magnitude of 7.51.\nThere are several single-planet systems in Bo\u00f6tes. HD 132406 is a Sun-like star of spectral type G0V with an apparent magnitude of 8.45, 231.5 light-years from Earth. It has a mass of 1.09\u00a0M\u2609 and a radius of 1\u00a0R\u2609. The star is orbited by a gas giant, HD 132406 b, discovered in 2007. HD 132406 orbits 1.98 AU from its host star with a period of 974 days and has a mass of 5.61\u00a0MJ. The planet was discovered by the radial velocity method.\nHD 131496 is also encircled by one planet, HD 131496 b. The star is of type K0 and is located 110 parsecs from Earth; it appears at a visual magnitude of 7.96. It is significantly larger than the Sun, with a mass of 1.61\u00a0M\u2609 and a radius of 4.6 solar radii. Its one planet, discovered in 2011 by the radial velocity method, has a mass of 2.2\u00a0MJ; its radius is as yet undetermined. HD 131496 b orbits at a distance of 2.09 AU with a period of 883 days.\nAnother single planetary system in Bo\u00f6tes is the HD 132563 system, a triple star system. The parent star, technically HD 132563B, is a star of magnitude 9.47, 96 parsecs from Earth. It is almost exactly the size of the Sun, with the same radius and a mass only 1% greater. Its planet, HD 132563B b, was discovered in 2011 by the radial velocity method. 1.49\u00a0MJ, it orbits 2.62 AU from its star with a period of 1544 days. Its orbit is somewhat elliptical, with an eccentricity of 0.22. HD 132563B b is one of very few planets found in triple star systems; it orbits the isolated member of the system, which is separated from the other components, a spectroscopic binary, by 400 AU.\nAlso discovered through the radial velocity method, albeit a year earlier, is HD 136418 b, a two-Jupiter-mass planet that orbits the star HD 136418 at a distance of 1.32 AU with a period of 464.3 days. Its host star is a magnitude 7.88 G5-type star, 98.2 parsecs from Earth. It has a radius of 3.4\u00a0R\u2609 and a mass of 1.33\u00a0M\u2609.\nWASP-14 b is one of the most massive and dense exoplanets known, with a mass of 7.341\u00a0MJ and a radius of 1.281\u00a0RJ. Discovered via the transit method, it orbits 0.036 AU from its host star with a period of 2.24 days. WASP-14 b has a density of 4.6 grams per cubic centimeter, making it one of the densest exoplanets known. Its host star, WASP-14, is an F5V-type star of magnitude 9.75, 160 parsecs from Earth. It has a radius of 1.306\u00a0R\u2609 and a mass of 1.211\u00a0M\u2609. It also has a very high proportion of lithium.\nDeep-sky objects.\nBo\u00f6tes is in a part of the celestial sphere facing away from the plane of our home Milky Way galaxy, and so does not have open clusters or nebulae. Instead, it has one bright globular cluster and many faint galaxies. The globular cluster NGC 5466 has an overall magnitude of 9.1 and a diameter of 11 arcminutes. It is a very loose globular cluster with fairly few stars and may appear as a rich, concentrated open cluster in a telescope. NGC 5466 is classified as a Shapley\u2013Sawyer Concentration Class 12 cluster, reflecting its sparsity. Its fairly large diameter means that it has a low surface brightness, so it appears far dimmer than the catalogued magnitude of 9.1 and requires a large amateur telescope to view. Only approximately 12 stars are resolved by an amateur instrument.\nBo\u00f6tes has two bright galaxies. NGC 5248 (Caldwell 45) is a type Sc galaxy (a variety of spiral galaxy) of magnitude 10.2. It measures 6.5 by 4.9 arcminutes. Fifty million light-years from Earth, NGC 5248 is a member of the Virgo Cluster of galaxies; it has dim outer arms and obvious H II regions, dust lanes and young star clusters. NGC 5676 is another type Sc galaxy of magnitude 10.9. It measures 3.9 by 2.0 arcminutes. Other galaxies include NGC 5008, a type Sc emission-line galaxy, NGC 5548, a type S Seyfert galaxy, NGC 5653, a type S HII galaxy, NGC 5778 (also classified as NGC 5825), a type E galaxy that is the brightest of its cluster, NGC 5886, and NGC 5888, a type SBb galaxy. NGC 5698 is a barred spiral galaxy, notable for being the host of the 2005 supernova SN 2005bc, which peaked at magnitude 15.3.\nFurther away lies the 250-million-light-year-diameter Bo\u00f6tes Void, a huge space largely empty of galaxies. Discovered by Robert Kirshner and colleagues in 1981, it is roughly 700 million light-years from Earth. Beyond it and within the bounds of the constellation, lie two superclusters at around 830 million and 1 billion light-years distant.\nThe Hercules\u2013Corona Borealis Great Wall, the largest-known structure in the Universe, covers a significant part of Bo\u00f6tes.\nMeteor showers.\nBo\u00f6tes is home to the Quadrantid meteor shower, the most prolific annual meteor shower. It was discovered in January 1835 and named in 1864 by Alexander Herschel. The radiant is located in northern Bo\u00f6tes near Kappa Bo\u00f6tis, in its namesake former constellation of Quadrans Muralis. Quadrantid meteors are dim, but have a peak visible hourly rate of approximately 100 per hour on January 3\u20134. The zenithal hourly rate of the Quadrantids is approximately 130 meteors per hour at their peak; it is also a very narrow shower.\nThe Quadrantids are notoriously difficult to observe because of a low radiant and often inclement weather. The parent body of the meteor shower has been disputed for decades; however, Peter Jenniskens has proposed 2003 EH1, a minor planet, as the parent. 2003 EH1 may be linked to C/1490 Y1, a comet previously thought to be a potential parent body for the Quadrantids.\n2003 EH1 is a short-period comet of the Jupiter family; 500 years ago, it experienced a catastrophic breakup event. It is now dormant. The Quadrantids had notable displays in 1982, 1985 and 2004. Meteors from this shower often appear to have a blue hue and travel at a moderate speed of 41.5\u201343 kilometers per second.\nOn April 28, 1984, a remarkable outburst of the normally placid Alpha Bootids was observed by visual observer Frank Witte from 00:00 to 2:30 UTC. In a 6\u00a0cm telescope, he observed 433 meteors in a field of view near Arcturus with a diameter of less than 1\u00b0. Peter Jenniskens comments that this outburst resembled a \"typical dust trail crossing\". The Alpha Bootids normally begin on April 14, peaking on April 27 and 28, and finishing on May 12. Its meteors are slow-moving, with a velocity of 20.9 kilometers per second. They may be related to Comet 73P/Schwassmann\u2013Wachmann 3, but this connection is only theorized.\nThe June Bootids, also known as the Iota Draconids, is a meteor shower associated with the comet 7P/Pons\u2013Winnecke, first recognized on May 27, 1916, by William F. Denning. The shower, with its slow meteors, was not observed prior to 1916 because Earth did not cross the comet's dust trail until Jupiter perturbed Pons\u2013Winnecke's orbit, causing it to come within of Earth's orbit the first year the June Bootids were observed.\nIn 1982, E. A. Reznikov discovered that the 1916 outburst was caused by material released from the comet in 1819. Another outburst of the June Bootids was not observed until 1998, because Comet Pons\u2013Winnecke's orbit was not in a favorable position. However, on June 27, 1998, an outburst of meteors radiating from Bo\u00f6tes, later confirmed to be associated with Pons-Winnecke, was observed. They were incredibly long-lived, with trails of the brightest meteors lasting several seconds at times. Many fireballs, green-hued trails, and even some meteors that cast shadows were observed throughout the outburst, which had a maximum zenithal hourly rate of 200\u2013300 meteors per hour.\nTwo Russian astronomers determined in 2002 that material ejected from the comet in 1825 was responsible for the 1998 outburst. Ejecta from the comet dating to 1819, 1825 and 1830 was predicted to enter Earth's atmosphere on June 23, 2004. The predictions of a shower less spectacular than the 1998 showing were borne out in a display that had a maximum zenithal hourly rate of 16\u201320 meteors per hour that night. The June Bootids are not expected to have another outburst in the next 50 years.\nTypically, only 1\u20132 dim, very slow meteors are visible per hour; the average June Bootid has a magnitude of 5.0. It is related to the Alpha Draconids and the Bootids-Draconids. The shower lasts from June 27 to July 5, with a peak on the night of June 28. The June Bootids are classified as a class III shower (variable), and has an average entry velocity of 18 kilometers per second. Its radiant is located 7 degrees north of Beta Bo\u00f6tis.\nThe Beta Bootids is a weak shower that begins on January 5, peaks on January 16, and ends on January 18. Its meteors travel at 43\u00a0km/s. The January Bootids is a short, young meteor shower that begins on January 9, peaks from January 16 to January 18, and ends on January 18.\nThe Phi Bootids is another weak shower radiating from Bo\u00f6tes. It begins on April 16, peaks on April 30 and May 1, and ends on May 12. Its meteors are slow-moving, with a velocity of 15.1\u00a0km/s. They were discovered in 2006. The shower's peak hourly rate can be as high as six meteors per hour. Though named for a star in Bo\u00f6tes, the Phi Bootid radiant has moved into Hercules. The meteor stream is associated with three different asteroids: 1620 Geographos, 2062 Aten and 1978 CA.\nThe Lambda Bootids, part of the Bootid-Coronae Borealid Complex, are a weak annual shower with moderately fast meteors; 41.75\u00a0km/s. The complex includes the Lambda Bootids, as well as the Theta Coronae Borealids and Xi Coronae Borealids. All of the Bootid-Coronae Borealid showers are Jupiter family comet showers; the streams in the complex have highly inclined orbits.\nThere are several minor showers in Bo\u00f6tes, some of whose existence is yet to be verified. The Rho Bootids radiate from near the namesake star, and were hypothesized in 2010. The average Rho Bootid has an entry velocity of 43\u00a0km/s. It peaks in November and lasts for three days.\nThe Rho Bootid shower is part of the SMA complex, a group of meteor showers related to the Taurids, which is in turn linked to the comet 2P/Encke. However, the link to the Taurid shower remains unconfirmed and may be a chance correlation. Another such shower is the Gamma Bootids, which were hypothesized in 2006. Gamma Bootids have an entry velocity of 50.3\u00a0km/s. The Nu Bootids, hypothesized in 2012, have faster meteors, with an entry velocity of 62.8\u00a0km/s.\nReferences.\nCitations\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4201", "revid": "4928500", "url": "https://en.wikipedia.org/wiki?curid=4201", "title": "Borromini, Francesco", "text": ""}
{"id": "4203", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=4203", "title": "Bernardino Ochino", "text": "Italian theologian (1487\u20131564)\nBernardino Ochino (1487\u20131564) was an Italian, who was raised a Roman Catholic and later turned to Protestantism and became a Protestant reformer.\nBiography.\nBernardino Ochino was born in Siena, the son of the barber Domenico Ochino, and at the age of 7 or 8, in around 1504, was entrusted to the order of Franciscan Friars. From 1510 he studied medicine at Perugia.\nTransfer to the Capuchins.\nAt the age of 38, Ochino transferred himself in 1534 to the newly founded Order of Friars Minor Capuchin. By then he was the close friend of Juan de Vald\u00e9s, Pietro Bembo, Vittoria Colonna, Pietro Martire, and Carnesecchi. In 1538 he was elected vicar-general of his order. In 1539, urged by Bembo, he visited Venice and delivered a course of sermons showing a sympathy with justification by faith, which appeared more clearly in his \"Dialogues\" published the same year. He was suspected and denounced, but nothing ensued until the establishment of the Inquisition in Rome in June 1542, at the instigation of Cardinal Giovanni Pietro Carafa. Ochino received a citation to Rome, and set out to obey it about the middle of August. According to his own statement, he was deterred from presenting himself at Rome by the warnings of Cardinal Contarini, whom he found at Bologna, allegedly dying of poison administered by the reactionary party.\nEscape to Geneva.\nOchino turned aside to Florence, and after some hesitation went across the Alps to Geneva. He was cordially received by John Calvin, and published within two years several volumes of \"Prediche\", controversial tracts rationalizing his change of religion. He also addressed replies to marchioness Vittoria Colonna, Claudio Tolomei, and other Italian sympathizers who were reluctant to go to the same length as himself. His own breach with the Roman Catholic Church was final.\nAugsburg and England.\nIn 1545 Ochino became minister of the Italian Protestant congregation at Augsburg. From this time dates his contact with Caspar Schwenckfeld. In 1546 he participated in the anti-Trinitarian Collegia Vicentina. He was compelled to flee from Augsburg when, in January 1547, the city was occupied by the imperial forces for the Diet of Augsburg.\nOchino found asylum in England, where he was made a prebendary of Canterbury Cathedral, received a pension from Edward VI's privy purse, and composed his major work, the \"Tragoedie or Dialoge of the unjuste usurped primacie of the Bishop of Rome\". This text, originally written in Latin, is extant only in the 1549 translation of Bishop John Ponet. The form is a series of dialogues. Lucifer, enraged at the spread of Jesus's kingdom, convokes the fiends in council, and resolves to set up the pope as antichrist. The state, represented by the emperor Phocas, is persuaded to connive at the pope's assumption of spiritual authority; the other churches are intimidated into acquiescence; Lucifer's projects seem fully accomplished, when Heaven raises up Henry VIII of England and his son for their overthrow.\nSeveral of Ochino's \"Prediche\" were translated into English by Anna Cooke; and he published numerous controversial treatises on the Continent. Ochino's \"Che Cosa \u00e8 Christo\" was translated into Latin and English by the future Queen Elizabeth I of England in 1547.\nZ\u00fcrich.\nIn 1553 the accession of Mary I drove Ochino from England. He went to Basel, where Lelio Sozzini and the lawyer Martino Muralto were sent to secure Ochino as pastor of the Italian church at Zurich, which Ochino accepted. The Italian congregation there was composed mainly of refugees from Locarno. There for 10 years Ochino wrote books which gave increasing evidence of his alienation from the orthodoxy around him. The most important of these was the \"Labyrinth\", a discussion of the freedom of the will, covertly undermining the Calvinistic doctrine of predestination.\nIn 1563 a long simmering storm burst on Ochino with the publication of his \"Thirty Dialogues\", in one of which his adversaries maintained that he had justified polygamy under the disguise of a pretended refutation. His dialogues on divorce and against the Trinity were also considered heretical.\nPoland, and death.\nOchino was not given opportunity to defend himself, and was banished from Z\u00fcrich. After being refused admission by other Protestant cities, he directed his steps towards Poland, at that time the most tolerant state in Europe. He had not resided there long when an edict appeared (August 8, 1564) banishing all foreign dissidents. Fleeing the country, he encountered the plague at Pi\u0144cz\u00f3w; three of his four children were carried off; and he himself, worn out by misfortune, died in solitude and obscurity at Slavkov in Moravia, about the end of 1564.\nLegacy.\nOchino's reputation among Protestants was low. He was charged by Thomas Browne in 1643 with the authorship of the legendary-apocryphal heretical treatise \"De tribus Impostoribus\", as well as with having carried his alleged approval of polygamy into practice.\nHis biographer Karl Benrath justified him, representing him as a fervent evangelist and at the same time as a speculative thinker with a passion for free inquiry. The picture is of Ochino always learning and unlearning and arguing out difficult questions with himself in his dialogues, frequently without attaining to any absolute conviction.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n\u00a0\"This article incorporates text from the 1902 Encyclop\u00e6dia Britannica, which is in the public domain\"."}
{"id": "4204", "revid": "13720955", "url": "https://en.wikipedia.org/wiki?curid=4204", "title": "Bay of Quinte", "text": "Bay in Ontario, Canada\nThe Bay of Quinte () is a long, narrow bay shaped like the letter \"Z\" on the northern shore of Lake Ontario in the province of Ontario, Canada. It is just west of the head of the Saint Lawrence River that drains the Great Lakes into the Gulf of Saint Lawrence. It is located about east of Toronto and west of Montreal.\nThe name \"Quinte\" is derived from \"Kent\u00e9\" or Kentio, an Iroquoian village located near the south shore of the Bay. Later on, an early French Catholic mission was built at Kent\u00e9, located on the north shore of what is now Prince Edward County, leading to the Bay being named after the Mission. Officially, in the Mohawk language, the community is called , which means \"the place of the bay\". The Cayuga name is or , \"land of two logs.\"\nThe Bay, as it is known locally, provides some of the best trophy walleye angling in North America as well as most sport fish common to the great lakes. The bay is subject to algal blooms in late summer. Zebra mussels as well as the other invasive species found in the Great Lakes are present.\nThe Quinte area played a vital role in bootlegging during prohibition in the United States, with large volumes of liquor being produced in the area, and shipped via boat on the bay to Lake Ontario finally arriving in New York State where it was distributed. Prohibition-era illegal sales of liquor accounted for many fortunes made in and around Belleville.\nTourism in the area is significant, especially in the summer months due to the Bay of Quinte and its fishing, local golf courses, provincial parks, and wineries.\nGeography.\nThe northern side of the bay is defined by Ontario's mainland, while the southern side follows the shore of the Prince Edward County headland. Beginning in the east with the outlet to Lake Ontario, the bay runs west-southwest for to Picton (although this section is also called Adolphus Reach), where it turns north-northwest for another as far as Deseronto. From there it turns south-southwest again for another , running past Big Island on the south and Belleville on the north. The width of the bay rarely exceeds . The bay ends at Trenton (Quinte West) and the Trent River, both also on the north side. The Murray Canal has been cut through the \"Carrying Place\", the few kilometres separating the end of the bay and Lake Ontario on the west side. The Trent River is part of the Trent-Severn Waterway, a canal connecting Lake Ontario to Lake Simcoe and then Georgian Bay on Lake Huron.\nThere are several sub-bays off the Bay of Quinte, including Hay Bay, Big Bay, and Muscote Bay.\nBay of Quinte Region.\nQuinte is also a region comprising several communities situated along the Bay of Quinte, including Quinte West, Brighton and the City of Belleville, which is the largest city in the Quinte Region, and represents a midpoint between Montreal, Ottawa, and Toronto.\nThe Greater Bay of Quinte area includes the municipalities of Brighton, Quinte West, Belleville, Prince Edward County, and Greater Napanee as well as the Native Tyendinaga Mohawk Territory. Overall population of the area exceeds 200,000.\nMohawks of the Bay of Quinte.\nThe Mohawks of the Bay of Quinte (Kenht\u00e8:ke Kanyen'keh\u00e1:ka) live on traditional Tyendinaga Mohawk Territory. Their reserve Band number 244, their current land base, is on the Bay of Quinte in southeastern Ontario east of Belleville and immediately to the west of Deseronto.\nThe community takes its name from a variant spelling of Mohawk leader Joseph Brant's traditional Mohawk name, Thayendanegea (standardized spelling Thayentin\u00e9:ken), which means 'two pieces of fire wood beside each other'. Officially, in the Mohawk language, the community is called \"Kenht\u00e8:ke\" (Tyendinaga), which means \"on the bay\", and was the birthplace of Tekanaw\u00ed:ta. The Cayuga name is Tyendinaga, \"Tay\u0119da:ne:g\u0119\u02c0 or Detgay\u0119:da:neg\u0119\u02c0\", \"land of two logs.\"\nEducation.\nThe Quinte Region, specifically the City of Belleville, is home to Loyalist College of Applied Arts and Technology. Other post-secondary schools in the region include Maxwell College of Advanced Technology, CDI College, and Quinte Literacy. Secondary schools in the region include Albert College (private school) and Sir James Whitney (a school for the deaf and severely hearing-impaired).\nIndustry and employment.\nThe Bay of Quinte region is a hub for industry in eastern Ontario. The region is home to a diverse cluster of domestic and multi-national manufacturing and logistics companies. Sectors include; food processing, auto-parts, plastics and packaging, consumer goods, and more. The region's close proximity to North American markets, strong labour force and start-up and operating costs have attracted attention and new investment from companies all over the globe. Industry in the Bay of Quinte region is supported by a workforce of over 11,000.\nInvestment attraction and industrial retention are supported regionally by the Quinte Economic Development Commission.\nJust a few of over 350 industries located in the Bay of Quinte Region include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4207", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=4207", "title": "Bassoon", "text": "Double-reed woodwind instrument\nThe bassoon is a musical instrument in the woodwind family, which plays in the tenor and bass ranges. It is composed of six pieces, and is usually made of wood. It is known for its distinctive tone color, wide range, versatility, and virtuosity. It is a non-transposing instrument and typically its music is written in the bass and tenor clefs, and sometimes in the treble. There are two forms of modern bassoon: the Buffet (or French) and Heckel (or German) systems. It is typically played while sitting using a seat strap, but can be played while standing if the player has a harness to hold the instrument. Sound is produced by rolling both lips over the reed and blowing direct air pressure to cause the reed to vibrate. Its fingering system can be quite complex when compared to those of other instruments. Appearing in its modern form in the 19th century, the bassoon figures prominently in orchestral, concert band, and chamber music literature, and is occasionally heard in pop, rock, and jazz settings as well. One who plays a bassoon is called a bassoonist.\nEtymology.\nThe word bassoon comes from French and from Italian ( with the augmentative suffix ), both terms that refer to the version of any instrument in a bass register. The term for bassoon used in classical music scores and parts is often its Italian name (plural ), which referred initially to the dulcian. This word, adopted in many other European languages as , , or in German, , comes from an Old French word meaning a bundle of sticks.\nCharacteristics.\nRange.\nThe range of the bassoon begins at B\u266d1 (the first one below the bass staff) and extends upward over three octaves, roughly to the G above the treble staff (G5). However, most writing for bassoon rarely calls for notes above C5 or D5; even Stravinsky's opening solo in \"The Rite of Spring\" only ascends to D5. Notes higher than this are possible, but seldom written, as they are difficult to produce (often requiring specific reed design features to ensure reliability), and at any rate are quite homogeneous in timbre to the same pitches on the cor anglais, which can produce them with relative ease. French bassoon has greater facility in the extreme high register, and so repertoire written for it is somewhat likelier to include very high notes, although repertoire for French system can be executed on German system without alterations and vice versa.\nThe extensive high register of the bassoon and its frequent role as a lyric tenor have meant that tenor clef is very commonly employed in its literature after the Baroque, partly to avoid excessive ledger lines, and, beginning in the 20th century, treble clef is also seen for similar reasons.\nLike other woodwind instruments, the lowest note is fixed, but A1 is possible with a special extension to the instrument\u2014see \"Extended techniques\" below.\nAlthough the primary tone hole pitches are a pitched perfect 5th lower than other non-transposing Western woodwinds (effectively an octave beneath English horn) the bassoon is non-transposing, meaning that notes sounded match the written pitch.\nConstruction.\nThe bassoon disassembles into six main pieces, including the reed. The bell (6), extending upward; the bass joint (or long joint) (5), connecting the bell and the boot; the boot (or butt) (4), at the bottom of the instrument and folding over on itself; the wing joint (or tenor joint) (3), which extends from boot to bocal; and the bocal (or crook) (2), a crooked metal tube that attaches the wing joint to a reed (1) ().\nStructure.\nThe bore of the bassoon is conical, like that of the oboe and the saxophone, and the two adjoining bores of the boot joint are connected at the bottom of the instrument with a U-shaped metal connector. Both bore and tone holes are precision-machined, and each instrument is finished by hand for proper tuning. The walls of the bassoon are thicker at various points along the bore; here, the tone holes are drilled at an angle to the axis of the bore, which reduces the distance between the holes on the exterior. This ensures coverage by the fingers of the average adult hand. Playing is facilitated by closing the distance between the widely spaced holes with a complex system of key work, which extends throughout nearly the entire length of the instrument. The overall height of the bassoon stretches to tall, but the total sounding length is considering that the tube is doubled back on itself. There are also short-reach bassoons made for the benefit of young or petite players.\nMaterials.\nA modern beginner's bassoon is generally made of maple, with medium-hardness types such as sycamore maple and sugar maple preferred. Less-expensive models are also made of materials such as polypropylene and ebonite, primarily for student and outdoor use. Metal bassoons were made in the past but have not been produced by any major manufacturer since 1889.\nDouble reeds.\nThe art of reed-making has been practiced for several hundred years, some of the earliest known reeds having been made for the dulcian, a predecessor of the bassoon. Current methods of reed-making consist of a set of basic methods; however, individual bassoonists' playing styles vary greatly and thus require that reeds be customized to best suit their respective bassoonist. Advanced players usually make their own reeds to this end. With regards to commercially made reeds, many companies and individuals offer pre-made reeds for sale, but players often find that such reeds still require adjustments to suit their particular playing style.\nModern bassoon reeds, made of \"Arundo donax\" cane, are often made by the players themselves, although beginner bassoonists tend to buy their reeds from professional reed makers or use reeds made by their teachers. Reeds begin with a length of tube cane that is split into three or four pieces using a tool called a cane splitter. The cane is then trimmed and \"gouged\" to the desired thickness, leaving the bark attached. After soaking, the gouged cane is cut to the proper shape and milled to the desired thickness, or \"profiled\", by removing material from the bark side. This can be done by hand with a file; more frequently it is done with a machine or tool designed for the purpose. After the profiled cane has soaked once again it is folded over in the middle. Prior to soaking, the reed maker will have lightly scored the bark with parallel lines with a knife; this ensures that the cane will assume a cylindrical shape during the forming stage.\nOn the bark portion, the reed maker binds on one, two, or three coils or loops of brass wire to aid in the final forming process. The exact placement of these loops can vary somewhat depending on the reed maker. The bound reed blank is then wrapped with thick cotton or linen thread to protect it, and a conical steel mandrel (which sometimes has been heated in a flame) is quickly inserted in between the blades. Using a special pair of pliers, the reed maker presses down the cane, making it conform to the shape of the mandrel. (The steam generated by the heated mandrel causes the cane to permanently assume the shape of the mandrel.) The upper portion of the cavity thus created is called the \"throat\", and its shape has an influence on the final playing characteristics of the reed. The lower, mostly cylindrical portion will be reamed out with a special tool called a reamer, allowing the reed to fit on the bocal.\nAfter the reed has dried, the wires are tightened around the reed, which has shrunk after drying, or replaced completely. The lower part is sealed (a nitrocellulose-based cement such as Duco may be used) and then wrapped with thread to ensure both that no air leaks out through the bottom of the reed and that the reed maintains its shape. The wrapping itself is often sealed with Duco, beeswax, or clear nail varnish (polish). Electrical tape can also be used as a wrapping for amateur reed makers. The bulge in the wrapping is sometimes referred to as the \"Turk's head\"\u2014it serves as a convenient handle when inserting the reed on the bocal. Alternatively, hot glue, epoxy, or heat shrink wrap may be used to seal the tube of the reed. The thread wrapping (commonly known as a \"Turban\" due to the criss-crossing fabric) is still more common in commercially sold reeds.\nTo finish the reed, the end of the reed blank, originally at the center of the unfolded piece of cane, is cut off, creating an opening. The blades above the first wire are now roughly long. For the reed to play, a slight bevel must be created at the tip with a knife, although there is also a machine that can perform this function. Other adjustments with the reed knife may be necessary, depending on the hardness, the profile of the cane, and the requirements of the player. The reed opening may also need to be adjusted by squeezing either the first or second wire with the pliers. Additional material may be removed from the sides (the \"channels\") or tip to balance the reed. Additionally, if the \"e\" in the bass clef staff is sagging in pitch, it may be necessary to \"clip\" the reed by removing from its length using a pair of very sharp scissors or the equivalent.\nHistory.\nOrigin.\nMusic historians generally consider the dulcian to be the forerunner of the modern bassoon, as the two instruments share many characteristics: a double reed fitted to a metal crook, obliquely drilled tone holes and a conical bore that doubles back on itself. The origins of the dulcian are obscure, but by the mid-16th century it was available in as many as eight different sizes, from soprano to great bass. A full consort of dulcians was a rarity; its primary function seems to have been to provide the bass in the typical wind band of the time, either loud (shawms) or soft (recorders), indicating a remarkable ability to vary dynamics to suit the need. Otherwise, dulcian technique was rather primitive, with eight finger holes and two keys, indicating that it could play in only a limited number of key signatures.\nCircumstantial evidence indicates that the baroque bassoon was a newly invented instrument, rather than a simple modification of the old dulcian. The dulcian was not immediately supplanted, but continued to be used well into the 18th century by Bach and others; and, presumably for reasons of interchangeability, repertoire from this time is very unlikely to go beyond the smaller compass of the dulcian. The man most likely responsible for developing the true bassoon was Martin Hotteterre (d.1712), who may also have invented the three-piece \"fl\u00fbte traversi\u00e8re\" (transverse flute) and the \"hautbois\" (baroque oboe). Some historians believe that sometime in the 1650s, Hotteterre conceived the bassoon in four sections (bell, bass joint, boot and wing joint), an arrangement that allowed greater accuracy in machining the bore compared to the one-piece dulcian. He also extended the compass down to B\u266d by adding two keys. An alternate view maintains Hotteterre was one of several craftsmen responsible for the development of the early bassoon. These may have included additional members of the Hotteterre family, as well as other French makers active around the same time. No original French bassoon from this period survives, but if it did, it would most likely resemble the earliest extant bassoons of Johann Christoph Denner and Richard Haka from the 1680s. Sometime around 1700, a fourth key (G\u266f) was added, and it was for this type of instrument that composers such as Antonio Vivaldi, Bach, and Georg Philipp Telemann wrote their demanding music. A fifth key, for the low E\u266d, was added during the first half of the 18th century. Notable makers of the 4-key and 5-key baroque bassoon include J.H. Eichentopf (c.\u20091678\u20131769), J. Poerschmann (1680\u20131757), Thomas Stanesby Jr. (1668\u20131734), G.H. Scherer (1703\u20131778), and Prudent Thieriot (1732\u20131786).\nModern configuration.\nIncreasing demands on capabilities of instruments and players in the 19th century\u2014particularly larger concert halls requiring greater volume and the rise of virtuoso composer-performers\u2014spurred further refinement. Increased sophistication, both in manufacturing techniques and acoustical knowledge, made possible great improvements in the instrument's playability.\nThe modern bassoon exists in two distinct primary forms, the Buffet (or \"French\") system and the Heckel (\"German\") system. Most of the world plays the Heckel system, while the Buffet system is primarily played in France, Belgium, and parts of Latin America. A number of other types of bassoons have been constructed by various instrument makers, such as the rare Galandronome. Owing to the ubiquity of the Heckel system in English-speaking countries, references in English to the contemporary bassoon always mean the Heckel system, with the Buffet system being explicitly qualified where it appears.\nHeckel (German) system.\nThe design of the modern bassoon owes a great deal to the performer, teacher, and composer Carl Almenr\u00e4der. Assisted by the German acoustic researcher Gottfried Weber, he developed the 17-key bassoon with a range spanning four octaves. Almenr\u00e4der's improvements to the bassoon began with an 1823 treatise describing ways of improving intonation, response, and technical ease of playing by augmenting and rearranging the keywork. Subsequent articles further developed his ideas. His employment at Schott gave him the freedom to construct and test instruments according to these new designs, and he published the results in \"Caecilia\", Schott's house journal. Almenr\u00e4der continued publishing and building instruments until his death in 1846, and Ludwig van Beethoven himself requested one of the newly made instruments after hearing of the papers. In 1831, Almenr\u00e4der left Schott to start his own factory with a partner, Johann Adam Heckel.\nHeckel and two generations of descendants continued to refine the bassoon, and their instruments became the standard, with other makers following. Because of their superior singing tone quality (an improvement upon one of the main drawbacks of the Almenr\u00e4der instruments), the Heckel instruments competed for prominence with the reformed Wiener system, a Boehm-style bassoon, and a completely keyed instrument devised by Charles-Joseph Sax, father of Adolphe Sax. F.W. Kruspe implemented a latecomer attempt in 1893 to reform the fingering system, but it failed to catch on. Other attempts to improve the instrument included a 24-keyed model and a single-reed mouthpiece, but both these had adverse effects on tone and were abandoned.\nComing into the 20th century, the Heckel-style German model of bassoon dominated the field. Heckel himself had made over 1,100 instruments by the turn of the 20th century (serial numbers begin at 3,000), and the British makers' instruments were no longer desirable for the changing pitch requirements of the symphony orchestra, remaining primarily in military band use.\nExcept for a brief 1940s wartime conversion to ball bearing manufacture, the Heckel concern has produced instruments continuously to the present day. Heckel bassoons are considered by many to be the best, although a range of Heckel-style instruments is available from several other manufacturers, all with slightly different playing characteristics.\nBecause its mechanism is primitive compared to most modern woodwinds, makers have occasionally attempted to \"reinvent\" the bassoon. In the 1960s, Giles Brindley began to develop what he called the \"logical bassoon\", which aimed to improve intonation and evenness of tone through use of an electrically activated mechanism, making possible key combinations too complex for the human hand to manage. Brindley's logical bassoon was never marketed.\nBuffet (French) system.\nThe Buffet system bassoon achieved its basic acoustical properties somewhat earlier than the Heckel. Thereafter, it continued to develop in a more conservative manner. While the early history of the Heckel bassoon included a complete overhaul of the instrument in both acoustics and key work, the development of the Buffet system consisted primarily of incremental improvements to the key work. This minimalist approach of the Buffet deprived it of improved consistency of intonation, ease of operation, and increased power, which is found in Heckel bassoons, but the Buffet is considered by some to have a more vocal and expressive quality. The conductor John Foulds lamented in 1934 the dominance of the Heckel-style bassoon, considering them too homogeneous in sound with the horn. The modern Buffet system has 22 keys with its range being the same as the Heckel; although Buffet instruments have greater facility in the upper registers, reaching E5 and F5 with far greater ease and less air resistance.\nCompared to the Heckel bassoon, Buffet system bassoons have a narrower bore and simpler mechanism, requiring different, and often more complex fingerings for many notes. Switching between Heckel and Buffet, or vice versa, requires extensive retraining. French woodwind instruments' tone in general exhibits a certain amount of \"edge\", with more of a vocal quality than is usual elsewhere, and the Buffet bassoon is no exception. This sound has been utilised effectively in writing for Buffet bassoon, but is less inclined to blend than the tone of the Heckel bassoon. As with all bassoons, the tone varies considerably, depending on individual instrument, reed, and performer. In the hands of a lesser player, the Heckel bassoon can sound flat and woody, but good players succeed in producing a vibrant, singing tone. Conversely, a poorly played Buffet can sound buzzy and nasal, but good players succeed in producing a warm, expressive sound.\nThough the United Kingdom once favored the French system, Buffet-system instruments are no longer made there and the last prominent British player of the French system retired in the 1980s. However, with continued use in some regions and its distinctive tone, the Buffet continues to have a place in modern bassoon playing, particularly in France, where it originated. Buffet-model bassoons are currently made in Paris by Buffet Crampon and the atelier Ducasse (Romainville, France). The Selmer Company stopped fabrication of French system bassoons around the year 2012. Some players, for example the late Gerald Corey in Canada, have learned to play both types and will alternate between them depending on the repertoire.\nUse in ensembles.\nEnsembles prior to the 20th century.\nPre-1760.\nPrior to 1760, the early ancestor of the bassoon was the dulcian. It was used to reinforce the bass line in wind ensembles called consorts. However, its use in concert orchestras was sporadic until the late 17th century when double reeds began to make their way into standard instrumentation. Increasing use of the dulcian as a \"basso continuo\" instrument meant that it began to be included in opera orchestras, in works such as those by Reinhard Keiser and Jean-Baptiste Lully. Meanwhile, as the dulcian advanced technologically and was able to achieve more virtuosity, composers such as Joseph Bodin de Boismortier, Johann Ernst Galliard, Johann Friedrich Fasch and Georg Philipp Telemann wrote demanding solo and ensemble music for the instrument. Antonio Vivaldi brought it to prominence by featuring it in thirty-nine concerti.\nc. 1760\u20131830.\nWhile the bassoon was still often used to give clarity to the bassline due to its sonorous low register, the capabilities of wind instruments grew as technology advanced during the Classical era. This allowed the instrument to play in more keys than the dulcian. Joseph Haydn took advantage of this in his Symphony No. 45 (\"Farewell Symphony\"), in which the bassoon plays in F-sharp minor. Following with these advances, composers also began to exploit the bassoon for its unique color, flexibility, and virtuosic ability, rather than for its perfunctory ability to double the bass line. Those who did this include Ludwig van Beethoven in his three Duos for Clarinet and Bassoon (WoO 27) for clarinet and bassoon and Niccolo Paganini in his duets for violin and bassoon. In his Bassoon Concerto in B-flat major, K. 191, W. A. Mozart utilized all aspects of the bassoon's expressiveness with its contrasts in register, staccato playing, and expressive sound, and was especially noted for its singing quality in the second movement. This concerto is often considered one of the most important works in all of the bassoon's repertoire, even today.\nThe bassoon's similarity to the human voice, in addition to its newfound virtuosic ability, was another quality many composers took advantage of during the classical era. After 1730, the German bassoon's range expended up to B\u266d4, and much higher with the French instrument. Technological advances also caused the bassoon's tenor register sound to become more resonant, and playing in this register grew in popularity, especially in the Austro-Germanic musical world. Pedagogues such as Josef Frohlich instructed students to practice scales, thirds, and fourths as vocal students would. In 1829, he wrote that the bassoon was capable of expressing \"the worthy, the virile, the solemn, the great, the sublime, composure, mildness, intimacy, emotion, longing, heartfulness, reverence, and soulful ardour.\" In G.F. Brandt's performance of Carl Maria von Weber's Concerto for Bassoon in F Major, Op. 75 (J. 127) it was also likened to the human voice. In France, Pierre Cugnier described the bassoon's role as encompassing not only the bass part, but also to accompany the voice and harp, play in pairs with clarinets and horns in Harmonie, and to play in \"nearly all types of music,\" including concerti, which were much more common than the sonatas of the previous era. Both Cugnier and \u00c9tienne Ozi emphasized the importance of the bassoon's similarity to the singing voice.\nThe role of the bassoon in the orchestra varied depending on the country. In the Viennese orchestra the instrument offered a three-dimensional sound to the ensemble by doubling other instruments such as violins, as heard in Mozart's overture to \"The Marriage of Figaro\", K 492. where it plays a rather technical part alongside the strings. He also wrote for the bassoon to change its timbre depending on which instrument it was paired with; warmer with clarinets, hollow with flutes, and dark and dignified with violins. In Germany and Scandinavian countries, orchestras typically featured only two bassoons. But in France, orchestras increased the number to four in the latter half of the nineteenth century. In England, the bassoonist's role varied depending on the ensemble. Johann Christian Bach wrote two concertos for solo bassoon, and it also appeared in more supportive roles such as accompanying church choirs after the Puritan revolution destroyed most church organs. In the American colonies, the bassoon was typically seen in a chamber setting. After the Revolutionary War, bassoonists were found in wind bands that gave public performances. By 1800, there was at least one bassoon in the United States Marine Band. In South America, the bassoon also appeared in small orchestras, bands, and military musique (similar to Harmonie ensembles).\nc. 1830\u20131900.\nThe role of the bassoon during the Romantic era varied between a role as a supportive bass instrument and a role as a virtuosic, expressive, solo instrument. In fact, it was very much considered an instrument that could be used in almost any circumstance. The comparison of the bassoon's sound to the human voice continued on during this time, as much of the pedagogy surrounded emulating this sound. Giuseppe Verdi used the instrument's lyrical, singing voice to evoke emotion in pieces such as his \"Messa da Requiem\". Eug\u00e8ne Jancourt compared the use of vibrato on the bassoon to that of singers, and Luigi Orselli wrote that the bassoon blended well with human voice. He also noted the function of the bassoon in the French orchestra at the time, which served to support the sound of the viola, reinforce staccato sound, and double the bass, clarinet, flute, and oboe. Emphasis also began to be placed on the unique sound of the bassoon's staccato, which might be described as quite short and aggressive, such as in Hector Berlioz's \"Symphonie fantastique, Op. 14\" in the fifth movement. Paul Dukas utilized the staccato to depict the image of two brooms coming to life in \"The Sorcerer's Apprentice.\"\nIt was common for there to be only two bassoons in German orchestras. Austrian and British military bands also only carried two bassoons, and were mainly used for accompaniment and offbeat playing. In France, Hector Berlioz also made it fashionable to use more than two bassoons; he often scored for three or four, and at time wrote for up to eight such as in his \"l'Imp\u00e9riale\".\nAt this point, composers expected bassoons to be as virtuosic as the other wind instruments, as they often wrote solos challenging the range and technique of the instrument. Examples of this include Nikolai Rimsky-Korsakov's bassoon solo and cadenza following the clarinet in \"Sheherazade,\" Op. 35 and in Richard Wagner's \"Tannh\u00e4user\", which required the bassoonist to triple tongue and also play up to the top of its range at an E5. Wagner also used the bassoon for its staccato ability in his work, and often wrote his three bassoon parts in thirds to evoke a darker sound with noticeable tone color. In Modest Mussorgsky's \"Night on Bald Mountain\", the bassoons play fortissimo alongside other bass instruments in order to evoke \"the voice of the Devil.\"\n20th and 21st century ensembles.\nAt this point in time, the development of the bassoon slowed. Rather than making large leaps in technological improvements, tiny imperfections in the instrument's function were corrected. The instrument became quite versatile throughout the twentieth century; the instrument was at this point able to play three octaves, a variety of different trills, and maintained stable intonation across all registers and dynamic levels. The pedagogy among bassoonists varied among different countries, and so the overall instrument itself played a variety of roles. As was a common theme in previous eras, the bassoon was valued by composers for its unique voice, and its use rose higher in pitch. A famous example of this is the beginning of Igor Stravinsky's \"Rite of Spring\" in which the bassoon plays in its highest register in order to mimic the Ukrainian Dentsivka. Composers also wrote for the bassoon's middle register, such as in Stravinsky's \"Berceuse\" in The \"Firebird\" and Symphony No. 5 in E-flat major, Op. 82 by Jean Sibelius. They also continued to highlight the staccato sound of the bassoon, as heard in Sergei Prokofiev's \"Humorous Scherzo\". In Sergei Prokofiev's Peter and the Wolf, the part of the grandfather is played by the bassoon.\nIn orchestral settings, most orchestras from the beginning of the twentieth century to the present have three or four bassoonists, with the fourth typically covering contrabassoon as well. Greater emphasis on the use of timbre, vibrato, and phrasing began to appear in bassoon pedagogy, and many followed Marcel Tabuteau's philosophy on musical phrasing. Vibrato began to be used in ensemble playing, depending on the phrasing of the music. The bassoon was, and currently is, expected to be fluent with other woodwinds in terms of virtuosity and technique. Examples of this include the cadenza for bassoons in Maurice Ravel's \"Rapsodie espagnole\" and the multi-finger trills used in Stravinsky's Octet.\nIn the twentieth century, the bassoon was less of a concerto soloist, and when it was, the accompanying ensemble was made softer and quieter. In addition, it was no longer used in marching bands, though still existed in concert bands with one or two of them. Orchestral repertoire remained very much the same Austro-Germanic tradition throughout most Western countries. It mostly appeared in solo, chamber, and symphonic settings. By the mid-1900s, broadcasting and recording grew in popularity, allowing for new opportunities for bassoonists, and leading to a slow decline of live performances. Much of the new music for bassoon in the late twentieth and early twenty-first centuries, often included extended techniques and was written for solo or chamber settings. One piece that included extended techniques was Luciano Berio's \"Sequenza XII\", which called for microtonal fingerings, glissandos, and timbral trills. Double and triple tonguing, flutter tonguing, multiphonics, quarter-tones, and singing are all utilized in Bruno Bartolozzi's \"Concertazioni.\" There were also a variety of concerti and bassoon and piano pieces written, such as John Williams's \"Five Sacred Trees\" and Andr\u00e9 Previn's \"Sonata for bassoon and piano\". There were also \"performance\" pieces such as Peter Schickele's \"Sonata Abassoonata\", which required the bassoonist to be both a musician and an actor. The bassoon quartet became prominent at this time, with pieces such as Daniel Dorff's \"It Takes Four to Tango\".\nJazz.\nThe bassoon is infrequently used as a jazz instrument and rarely seen in a jazz ensemble. It first began appearing in the 1920s, when Garvin Bushell began incorporating the bassoon in his performances. Specific calls for its use occurred in Paul Whiteman's group, the unusual octets of Alec Wilder, and a few other session appearances. The next few decades saw the instrument used only sporadically, as symphonic jazz fell out of favor, but the 1960s saw artists such as Yusef Lateef and Chick Corea incorporate bassoon into their recordings. Lateef's diverse and eclectic instrumentation saw the bassoon as a natural addition (see, e.g., \"The Centaur and the Phoenix\" (1960) which features bassoon as part of a 6-man horn section, including a few solos) while Corea employed the bassoon in combination with flautist Hubert Laws.\nMore recently, Illinois Jacquet, Ray Pizzi, Frank Tiberi, and Marshall Allen have both doubled on bassoon in addition to their saxophone performances. Bassoonist Karen Borca, a performer of free jazz, is one of the few jazz musicians to play only bassoon; Michael Rabinowitz, the Spanish bassoonist Javier Abad, and James Lassen, an American resident in Bergen, Norway, are others. Katherine Young plays the bassoon in the ensembles of Anthony Braxton. Lindsay Cooper, Paul Hanson, the Brazilian bassoonist Alexandre Silv\u00e9rio, Trent Jacobs and Daniel Smith are also currently using the bassoon in jazz. French bassoonists Jean-Jacques Decreux and Alexandre Ouzounoff have both recorded jazz, exploiting the flexibility of the Buffet system instrument to good effect.\nPopular music.\nIn conjunction with the use of electronic pickups and amplification, the instrument began to be used more somewhat in jazz and rock settings. However, the bassoon is still quite rare as a regular member of rock bands. Several 1960s pop music hits feature the bassoon, including \"The Tears of a Clown\" by Smokey Robinson and the Miracles (the bassoonist was Charles R. Sirard), \"Jennifer Juniper\" by Donovan, \"59th Street Bridge Song\" by Harpers Bizarre, and the oompah bassoon underlying The New Vaudeville Band's \"Winchester Cathedral\". From 1974 to 1978, the bassoon was played by Lindsay Cooper in the British avant-garde band Henry Cow. The Leonard Nimoy song \"The Ballad of Bilbo Baggins\" features the bassoon. In the 1970s it was played, in the British medieval/progressive rock band Gryphon, by Brian Gulland, as well as by the American band Ambrosia, where it was played by drummer Burleigh Drummond. The Belgian Rock in Opposition-band Univers Zero is also known for its use of the bassoon.\nMore recently, These New Puritans's 2010 album \"Hidden\" makes heavy use of the instrument throughout; their principal songwriter, Jack Barnett, claimed repeatedly to be \"writing a lot of music for bassoon\" in the run-up to its recording. The rock band Better Than Ezra took their name from a passage in Ernest Hemingway's \"A Moveable Feast\" in which the author comments that listening to an annoyingly talkative person is still \"better than Ezra learning how to play the bassoon\", referring to Ezra Pound.\nBritish psychedelic/progressive rock band Knifeworld features the bassoon playing of Chloe Herrington, who also plays for experimental chamber rock orchestra Chrome Hoof.\nFiona Apple featured the bassoon in the opening track of her 2004 album \"Extraordinary Machine\".\nIn 2016, the bassoon was featured on the album \"Gang Signs and Prayers\" by UK \"grime\" artist Stormzy. Played by UK bassoonist Louise Watson, the bassoon is heard in the tracks \"Cold\" and \"Mr Skeng\" as a complement to the electronic synthesizer bass lines typically found in this genre.\nAppearance in television.\nThe Cartoon Network animated series \"Over the Garden Wall\" features a bassoon in episode 6 entitled \"Lullaby in Frogland\", where the main character is encouraged to play the bassoon to impress a group of frogs.\nThe character Jan Bellows in the Hulu series \"Only Murders in the Building\" is a professional bassoonist.\nTechnique.\nThe bassoon is held diagonally in front of the player, but unlike the flute, oboe and clarinet, it cannot be easily supported by the player's hands alone. Some means of additional support is usually required; the most common ones are a seat strap attached to the base of the boot joint, which is laid across the chair seat prior to sitting down, or a neck strap or shoulder harness attached to the top of the boot joint. Occasionally a spike similar to those used for the cello or the bass clarinet is attached to the bottom of the boot joint and rests on the floor. It is possible to play while standing up if the player uses a neck strap or similar harness, or if the seat strap is tied to the belt. Sometimes a device called a \"balance hanger\" is used when playing in a standing position. This is installed between the instrument and the neck strap, and shifts the point of support closer to the center of gravity, adjusting the distribution of weight between the two hands.\nThe bassoon is played with both hands in a stationary position, the left above the right, with five main finger holes on the front of the instrument (nearest the audience) plus a sixth that is activated by an open-standing key. Five additional keys on the front are controlled by the little fingers of each hand. The back of the instrument (nearest the player) has twelve or more keys to be controlled by the thumbs, the exact number varying depending on model.\nTo stabilize the right hand, many bassoonists use an adjustable comma-shaped apparatus called a \"crutch\", or a hand rest, which mounts to the boot joint. The crutch is secured with a thumb screw, which also allows the distance that it protrudes from the bassoon to be adjusted. Players rest the curve of the right hand where the thumb joins the palm against the crutch. The crutch also keeps the right hand from tiring and enables the player to keep the finger pads flat on the finger holes and keys.\nAn aspect of bassoon technique not found on any other woodwind is called \"flicking\". It involves the left hand thumb momentarily pressing, or \"flicking\" the high A, C and D keys at the beginning of certain notes in the middle octave to achieve a clean slur from a lower note. This eliminates cracking, or brief multiphonics that happens without the use of this technique. Alternatively, a similar method is called \"venting\", which requires that the register key be used as part of the full fingering as opposed to being open momentarily at the start of the note. This is sometimes called the \"European style\"; venting raises the intonation of the notes slightly, and it can be advantageous when tuning to higher frequencies. Some bassoonists flick A and B\u266d when tongued, for clarity of articulation, but flicking (or venting) is practically ubiquitous for slurs.\nWhile flicking is used to slur up to higher notes, the whisper key is used for lower notes. From the A\u266d right below middle C and lower, the whisper key is pressed with the left thumb and held for the duration of the note. This prevents cracking, as low notes can sometimes crack into a higher octave. Both flicking and using the whisper key is especially important to ensure notes speak properly during slurring between high and low registers.\nWhile bassoons are usually critically tuned at the factory, the player nonetheless has a great degree of flexibility of pitch control through the use of breath support, embouchure, and reed profile. Players can also use alternate fingerings to adjust the pitch of many notes. Similar to other woodwind instruments, the length of the bassoon can be increased to lower pitch or decreased to raise pitch. On the bassoon, this is done preferably by changing the bocal to one of a different length, (lengths are denoted by a number on the bocal, usually starting at 0 for the shortest length, and 3 for the longest, but there are some manufacturers who will use other numbers) but it is possible to push the bocal in or out slightly to grossly adjust the pitch.\nEmbouchure and sound production.\nThe bassoon embouchure is a very important aspect of producing a full, round, and rich sound on the instrument. The lips are both rolled over the teeth, often with the upper lip further along in an \"overbite\". The lips provide micromuscular pressure on the entire circumference of the reed, which grossly controls intonation and harmonic excitement, and thus must be constantly modulated with every change of note. How far along the reed the lips are placed affects both tone (with less reed in the mouth making the sound more edged or \"reedy\", and more reed making it smooth and less projectile) and the way the reed will respond to pressure.\nThe musculature employed in a bassoon embouchure is primarily around the lips, which pressure the reed into the shapes needed for the desired sound. The jaw is raised or lowered to adjust the oral cavity for better reed control, but the jaw muscles are used much less for upward vertical pressure than in single reeds, only being substantially employed in the very high register. However, double reed students often \"bite\" the reed with these muscles because the control and tone of the labial and other muscles is still developing, but this generally makes the sound sharp and \"choked\" as it contracts the aperture of the reed and stifles the vibration of its blades.\nApart from the embouchure proper, students must also develop substantial muscle tone and control in the diaphragm, throat, neck and upper chest, which are all employed to increase and direct air pressure. Air pressure is a very important aspect of the tone, intonation and projection of double reed instruments, affecting these qualities as much, or more than the embouchure does.\nAttacking a note on the bassoon with imprecise amounts of muscle or air pressure for the desired pitch will result in poor intonation, cracking or multiphonics, accidentally producing the incorrect partial, or the reed not speaking at all. These problems are compounded by the individual qualities of reeds, which are categorically inconsistent in behaviour for inherent and exherent reasons.\nThe muscle requirements and variability of reeds mean it takes some time for bassoonists (and oboists) to develop an embouchure that exhibits consistent control across all reeds, dynamics and playing environments.\nModern fingering.\nThe fingering technique of the bassoon varies more between players, by a wide margin, than that of any other orchestral woodwind. The complex mechanism and acoustics mean the bassoon lacks simple fingerings of good sound quality or intonation for some notes (especially in the higher range), but, conversely, there is a great variety of superior, but generally more complicated, fingerings for them. Typically, the simpler fingerings for such notes are used as alternate or trill fingerings, and the bassoonist will use as \"full fingering\" one or several of the more complex executions possible, for optimal sound quality. The fingerings used are at the discretion of the bassoonist, and, for particular passages, he or she may experiment to find new alternate fingerings that are thus idiomatic to the player.\nThese elements have resulted in both \"full\" and alternate fingerings differing extensively between bassoonists, and are further informed by factors such as cultural difference in what sound is sought, how reeds are made, and regional variation in tuning frequencies (necessitating sharper or flatter fingerings). Regional enclaves of bassoonists tend to have some uniformity in technique, but on a global scale, technique differs such that two given bassoonists may share no fingerings for certain notes. Owing to these factors, ubiquitous bassoon technique can only be partially notated.\nThe left thumb operates nine keys: B\u266d1, B1, C2, D2, D5, C5 (also B4), two keys when combined create A4, and the whisper key. The whisper key should be held down for notes between and including F2 and G\u266f3 and certain other notes; it can be omitted, but the pitch will destabilise. Additional notes can be created with the left thumb keys; the D2 and bottom key above the whisper key on the tenor joint (C\u266f key) together create both C\u266f3 and C\u266f4. The same bottom tenor-joint key is also used, with additional fingering, to create E5 and F5. D5 and C5 together create C\u266f5. When the two keys on the tenor joint to create A4 are used with slightly altered fingering on the boot joint, B\u266d4 is created. The whisper key may also be used at certain points throughout the instrument's high register, along with other fingerings, to alter sound quality as desired.\nThe right thumb operates four keys. The uppermost key is used to produce B\u266d2 and B\u266d3, and may be used in B4,F\u266f4, C5, D5, F5, and E\u266d5. The large circular key, otherwise known as the \"pancake key\", is held down for all the lowest notes from E2 down to B\u266d1. It is also used, like the whisper key, in additional fingerings for muting the sound. For example, in Ravel's \"Bol\u00e9ro\", the bassoon is asked to play the ostinato on G4. This is easy to perform with the normal fingering for G4, but Ravel directs that the player should also depress the E2 key (pancake key) to mute the sound (this being written with Buffet system in mind; the G fingering on which involves the Bb key\u00a0\u2013 sometimes called \"French\" G on Heckel). The next key operated by the right thumb is known as the \"spatula key\": its primary use is to produce F\u266f2 and F\u266f3. The lowermost key is used less often: it is used to produce A\u266d2 (G\u266f2) and A\u266d3 (G\u266f3), in a manner that avoids sliding the right fourth finger from another note.\nThe four fingers of the left hand can each be used in two different positions. The key normally operated by the index finger is primarily used for E5, also serving for trills in the lower register. Its main assignment is the upper tone hole. This hole can be closed fully, or partially by rolling down the finger. This half-holing technique is used to overblow F\u266f3, G3 and G\u266f3. The middle finger typically stays on the centre hole on the tenor joint. It can also move to a lever used for E\u266d5, also a trill key. The ring finger operates, on most models, one key. Some bassoons have an alternate E\u266d key above the tone hole, predominantly for trills, but many do not. The smallest finger operates two side keys on the bass joint. The lower key is typically used for C\u266f2, but can be used for muting or flattening notes in the tenor register. The upper key is used for E\u266d2, E4, F4, F\u266f4, A4, B\u266d4, B4, C5, C\u266f5, and D5; it flattens G3 and is the standard fingering for it in many places that tune to lower Hertz levels such as A440.\nThe four fingers of the right hand have at least one assignment each. The index finger stays over one hole, except that when E\u266d5 is played a side key at the top of the boot is used (this key also provides a C\u266f3 trill, albeit sharp on D). The middle finger remains stationary over the hole with a ring around it, and this ring and other pads are lifted when the smallest finger on the right hand pushes a lever. The ring finger typically remains stationary on the lower ring-finger key. However, the upper ring-finger key can be used, typically for B\u266d2 and B\u266d3, in place of the top thumb key on the front of the boot joint; this key comes from the oboe, and some bassoons do not have it because the thumb fingering is practically universal. The smallest finger operates three keys. The backmost one, closest to the bassoonist, is held down throughout most of the bass register. F\u266f4 may be created with this key, as well as G4, B\u266d4, B4, and C5 (the latter three employing solely it to flatten and stabilise the pitch). The lowest key for the smallest finger on the right hand is primarily used for A\u266d2 (G\u266f2) and A\u266d3 (G\u266f3) but can be used to improve D5, E\u266d5, and F5. The frontmost key is used, in addition to the thumb key, to create G\u266d2 and G\u266d3; on many bassoons this key operates a different tone hole to the thumb key and produces a slightly flatter F\u266f (\"duplicated F\u266f\"); some techniques use one as standard for both octaves and the other for utility, but others use the thumb key for the lower and the fourth finger for the higher.\nExtended techniques.\nMany extended techniques can be performed on the bassoon, such as multiphonics, flutter-tonguing, circular breathing, double tonguing, and harmonics. In the case of the bassoon, flutter-tonguing may be accomplished by \"gargling\" in the back of the throat as well as by the conventional method of rolling Rs. Multiphonics on the bassoon are plentiful, and can be achieved by using particular alternative fingerings, but are generally heavily influenced by embouchure position. Also, again using certain fingerings, notes may be produced on the instrument that sound lower pitches than the actual range of the instrument. These notes tend to sound very gravelly and out of tune, but technically sound below the low B\u266d.\nThe bassoonist may also produce lower notes than the bottom B\u266d by extending the length of bell. This can be achieved by inserting a specially made \"low A extension\" into the bell, but may also be achieved with a small paper or rubber tube or a clarinet/cor anglais bell sitting inside the bassoon bell (although the note may tend sharp). The effect of this is to convert the lower B\u266d into a lower note, almost always A natural; this broadly lowers the pitch of the instrument (most noticeably in the lower register) and will often accordingly convert the lowest B to B\u266d (and render the neighbouring C very flat). The idea of using low A was begun by Richard Wagner, who wanted to extend the range of the bassoon. Many passages in his later operas require the low A as well as the B-flat immediately above it; this is possible on a normal bassoon using an extension which also flattens low B to B\u266d, but all extensions to the bell have significant effects on intonation and sound quality in the bottom register of the instrument, and passages such as this are more often realised with comparative ease by the contrabassoon.\nSome bassoons have been specially made to allow bassoonists to realize similar passages. These bassoons are made with a \"Wagner bell\" which is an extended bell with a key for both the low A and the low B-flat, but they are not widespread; bassoons with Wagner bells suffer similar intonational problems as a bassoon with an ordinary A extension, and a bassoon must be constructed specifically to accommodate one, making the extension option far less complicated. Extending the bassoon's range even lower than the A, though possible, would have even stronger effects on pitch and make the instrument effectively unusable.\nDespite the logistic difficulties of the note, Wagner was not the only composer to write the low A. Another composer who has required the bassoon to be chromatic down to low A is Gustav Mahler. Richard Strauss also calls for the low A in his opera \"Intermezzo\". Some works have optional low As, as in Carl Nielsen's Wind Quintet, op. 43, which includes an optional low A for the final cadence of the work.\nLearning the bassoon.\nThe complex fingering system and the expense and lack of access to quality bassoon reeds can make the bassoon more of a challenge to learn than some of the other woodwind instruments. Cost is another factor in a person's decision to pursue the bassoon. Prices may range from US$7,000 to over $45,000 for a high-quality instrument. In North America, schoolchildren may take up bassoon only after starting on another reed instrument, such as clarinet or saxophone.\nStudents in America often begin to pursue the study of bassoon performance and technique in the middle years of their music education, often in association with their school band program. Students are often provided with a school instrument and encouraged to pursue lessons with private instructors. Students typically receive instruction in proper posture, hand position, embouchure, repertoire, and tone production.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4210", "revid": "50692984", "url": "https://en.wikipedia.org/wiki?curid=4210", "title": "Bipedalism", "text": "Terrestrial locomotion using two limbs\nBipedalism is a form of terrestrial locomotion where an animal moves by means of its two rear (or lower) limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped , meaning 'two feet' (from Latin \"bis\" 'double' and \"pes\" 'foot'). Types of bipedal movement include walking or running (a bipedal gait) and hopping.\nSeveral groups of modern species are habitual bipeds whose normal method of locomotion is two-legged. In the Triassic period some groups of archosaurs, a group that includes crocodiles and dinosaurs, developed bipedalism; among the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds are members of a clade of exclusively bipedal dinosaurs, the theropods. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare, hopping mice, pangolins and hominin apes such as australopithecines, including humans, as well as many other extinct groups evolving the trait independently.\nA larger number of modern species intermittently or briefly use a bipedal gait. Several lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment, though there are a few cases where they walk on their hind limbs only. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs while fighting or copulating. Some animals commonly stand on their hind legs to reach food, keep watch, threaten a competitor or predator, or pose in courtship, but do not move bipedally.\nEtymology.\nThe word is derived from the Latin words \"bi(s)\" 'two' and \"ped-\" 'foot', as contrasted with quadruped 'four feet'.\nAdvantages.\nLimited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in the giant pangolin), combat (in bears, great apes and the large monitor lizard) or camouflage.\nThe maximum bipedal speed appears slower than the maximum speed of quadrupedal movement with a flexible backbone \u2013 both the ostrich and the red kangaroo can reach speeds of , while the cheetah can exceed . Even though bipedalism is slower at first, over long distances, it has allowed humans to outrun most other animals according to the endurance running hypothesis. Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, which could aid in escaping from predators.\nFacultative and obligate bipedalism.\nZoologists often label behaviors, including bipedalism, as \"facultative\" (i.e. optional) or \"obligate\" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut \u2014 for example, humans other than infants normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome \u2014 very rare genetic neurological disorders rather than normal behavior. Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that \"normal\" humans can crawl on hands and knees. This article therefore avoids the terms \"facultative\" and \"obligate\", and focuses on the range of styles of locomotion \"normally\" used by various groups of animals. Normal humans may be considered \"obligate\" bipeds because the alternatives are very uncomfortable and usually only resorted to when walking is impossible.\nMovement.\nThere are a number of states of movement commonly associated with bipedalism.\nBipedal animals.\nThe great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand, most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to walk or hop, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.\nExtant reptiles.\nMany species of lizards become bipedal during high-speed, sprint locomotion, including the world's fastest lizard, the spiny-tailed iguana (genus \"Ctenosaura\").\nEarly reptiles and lizards.\nThe first known biped is the bolosaurid \"Eudibamus\" whose fossils date from 290 million years ago. Its long hind-legs, short forelegs, and distinctive joints all suggest bipedalism. The species became extinct in the early Permian.\nArchosaurs (includes crocodilians and dinosaurs).\nBirds.\nAll birds are bipeds, as is the case for all theropod dinosaurs. However, hoatzin chicks have claws on their wings which they use for climbing.\nOther archosaurs.\nBipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians. All dinosaurs are thought to be descended from a fully bipedal ancestor, perhaps similar to \"Eoraptor\".\nDinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95 percent of all life on Earth. Radiometric dating of fossils from the early dinosaur genus \"Eoraptor\" establishes its presence in the fossil record at this time. Paleontologists suspect \"Eoraptor\" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as \"Marasuchus\" and \"Lagerpeton\" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.\nBipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodonts. Some extinct members of Pseudosuchia, a sister group to the avemetatarsalians (the group including dinosaurs and relatives), also evolved bipedal forms \u2013 a poposauroid from the Triassic, \"Effigia okeeffeae\", is thought to have been bipedal. Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion.\nMammals.\nA number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion\u00a0\u2013 for example, humans, ground pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied, are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.\nBipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few non-primate mammals commonly move bipedally with an alternating leg gait. Exceptions are the ground pangolin and in some circumstances the tree kangaroo. One black bear, Pedals, became famous locally and on the internet for having a frequent bipedal gait, although this is attributed to injuries on the bear's front paws. A two-legged fox was filmed in a Derbyshire garden in 2023, most likely having been born that way.\nPrimates.\nMost bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans), and the tail may be absent entirely. Many primates can stand upright on their hind legs without any support. \nChimpanzees, bonobos, gorillas, gibbons and baboons exhibit forms of bipedalism. On the ground sifakas move like all indrids with bipedal sideways hopping movements of the hind legs, holding their forelimbs up for balance. Geladas, although usually quadrupedal, will sometimes move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion. However, they can only do so for brief amounts, as their bodies are not adapted for constant bipedal locomotion.\nHumans are the only primates who are normally biped, due to an extra curve in the spine (i.e. the lumbar lordosis) which shifts the center of gravity more dorsally and thus stabilizes the upright position, as well as shorter arms relative to the legs than is the case for the nonhuman great apes. The evolution of human bipedalism began in primates about four million years ago, or as early as seven million years ago with \"Sahelanthropus\" or about 12 million years ago with \"Danuvius guggenmosi\". One hypothesis for human bipedalism is that it evolved as a result of differentially successful survival from carrying food to share with group members, although there are alternative hypotheses.\nInjured chimpanzees and bonobos have been capable of sustained bipedalism.\nThree captive primates, one macaque Natasha and two chimps, Oliver and Poko (chimpanzee), were found to move bipedally. Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage. Oliver reverted to knuckle-walking after developing arthritis. Non-human primates often use bipedal locomotion when carrying food, or while moving through shallow water.\nLimited bipedalism.\nLimited bipedalism in mammals.\nOther mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver will move bipedally if transporting wood for their dams, as will the raccoon when holding food). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs (e.g. Faith) can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will walk on its front legs when threatened, rearing up on its front legs while facing the attacker so that its anal glands, capable of spraying an offensive oil, face its attacker.\nLimited bipedalism in non-mammals (and non-birds).\nBipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the \"reared-up\" running of lizards such as agamids and monitor lizards. Many reptile species will also temporarily adopt bipedalism while fighting. One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds. Bipedalism is rarely found outside terrestrial animals, though at least two species of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.\nEvolution of human bipedalism.\nThere are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools. Bipedal specializations are found in \"Australopithecus\" fossils from 4.2 to 3.9 million years ago and recent studies have suggested that obligate bipedal hominid species were present as early as 7 million years ago. Nonetheless, the evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium. Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as \"Australopithecus africanus\". This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make. Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward. The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.\nThe form and function of modern-day humans' upper bodies appear to have evolved from living in a more forested setting. Living in this kind of environment would have made it so that being able to travel arboreally would have been advantageous at the time. Although different to human walking, bipedal locomotion in trees was thought to be advantageous. It has also been proposed that, like some modern-day apes, early hominins had undergone a knuckle-walking stage prior to adapting the back limbs for bipedality while retaining forearms capable of grasping. Numerous causes for the evolution of human bipedalism involve freeing the hands for carrying and using tools, sexual dimorphism in provisioning, changes in climate and environment (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun. It is possible that bipedalism provided a variety of benefits to the hominin species, and scientists have suggested multiple reasons for evolution of human bipedalism. There is also not only the question of why the earliest hominins were partially bipedal but also why hominins became more bipedal over time. For example, the postural feeding hypothesis describes how the earliest hominins became bipedal for the benefit of reaching food in trees while the savanna-based theory describes how the late hominins that started to settle on the ground became increasingly bipedal.\nMultiple factors.\nNapier (1963) argued that it is unlikely that a single factor drove the evolution of bipedalism. He stated \"It seems unlikely that any single factor was responsible for such a dramatic change in behaviour. In addition to the advantages of accruing from ability to carry objects \u2013 food or otherwise \u2013 the improvement of the visual range and the freeing of the hands for purposes of defence and offence may equally have played their part as catalysts.\" Sigmon (1971) demonstrated that chimpanzees exhibit bipedalism in different contexts, and one single factor should be used to explain bipedalism: preadaptation for human bipedalism. Day (1986) emphasized three major pressures that drove evolution of bipedalism: food acquisition, predator avoidance, and reproductive success. Ko (2015) stated that there are two main questions regarding bipedalism 1. Why were the earliest hominins partially bipedal? and 2. Why did hominins become more bipedal over time? He argued that these questions can be answered with combination of prominent theories such as Savanna-based, Postural feeding, and Provisioning.\nSavannah-based theory.\nAccording to the Savanna-based theory, hominines came down from the tree's branches and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. One of the proposed mechanisms was the knuckle-walking hypothesis, which states that human ancestors used quadrupedal locomotion on the savanna, as evidenced by morphological characteristics found in \"Australopithecus anamensis\" and \"Australopithecus afarensis\" forelimbs, and that it is less parsimonious to assume that knuckle walking developed twice in genera \"Pan\" and \"Gorilla\" instead of evolving it once as synapomorphy for \"Pan\" and \"Gorilla\" before losing it in Australopithecus. The evolution of an orthograde posture would have been very helpful on a savanna as it would allow the ability to look over tall grasses in order to watch out for predators, or terrestrially hunt and sneak up on prey. It was also suggested in P. E. Wheeler's \"The evolution of bipedality and loss of functional body hair in hominids\", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature. In fact, Elizabeth Vrba's turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.\nOthers state hominines had already achieved the bipedal adaptation that was used in the savanna. The fossil evidence reveals that early bipedal hominins were still adapted to climbing trees at the time they were also walking upright. It is possible that bipedalism evolved in the trees, and was later applied to the savanna as a vestigial trait. Humans and orangutans are both unique to a bipedal reactive adaptation when climbing on thin branches, in which they have increased hip and knee extension in relation to the diameter of the branch, which can increase an arboreal feeding range and can be attributed to a convergent evolution of bipedalism evolving in arboreal environments. Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence\u2014the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.\nSome of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous \"Australopithecus afarensis\", found in Hadar in Ethiopia, which may have been forested at the time of Lucy's death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. \"Little Foot\", a nearly-complete specimen of \"Australopithecus africanus\", has a divergent big toe as well as the ankle strength to walk upright. \"Little Foot\" could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be much more wet and covered in thick vegetation and has only recently become the arid desert it is now.\nTraveling efficiency hypothesis.\nAn alternative explanation is that the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism. In an experiment monitoring chimpanzee metabolic rate via oxygen consumption, it was found that the quadrupedal and bipedal energy costs were very similar, implying that this transition in early ape-like ancestors would not have been very difficult or energetically costing. This increased travel efficiency is likely to have been selected for as it assisted foraging across widely dispersed resources.\nPostural feeding hypothesis.\nThe postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University. This hypothesis asserts that chimpanzees were only bipedal when they eat. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was used to reach up to grab for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt's hypotheses states that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, \"Australopithecus afarensis\" has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the \"Australopithecus\" hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.\nA related study conducted by University of Birmingham, Professor Susannah Thorpe examined the most arboreal great ape, the orangutan, holding onto supporting branches in order to navigate branches that were too flexible or unstable otherwise. In more than 75 percent of observations, the orangutans used their forelimbs to stabilize themselves while navigating thinner branches. Increased fragmentation of forests where A. afarensis as well as other ancestors of modern humans and other apes resided could have contributed to this increase of bipedalism in order to navigate the diminishing forests. Findings also could shed light on discrepancies observed in the anatomy of A. afarensis, such as the ankle joint, which allowed it to \"wobble\" and long, highly flexible forelimbs. If bipedalism started from upright navigation in trees, it could explain both increased flexibility in the ankle as well as long forelimbs which grab hold of branches.\nProvisioning model.\nOne theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as \"male provisioning\". Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction (\"feminization\") of the male canine teeth in early hominids such as \"Sahelanthropus tchadensis\" and \"Ardipithecus ramidus\", which along with low body size dimorphism in \"Ardipithecus\" and \"Australopithecus\", suggests a reduction in inter-male antagonistic behavior in early hominids. In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.\nHowever, this model has been debated, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that \"Australopithecus afarensis\" males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk. Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother\u00a0\u2013 hampering the mother's freedom and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that \"Australopithecus afarensis\" lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a pair-bonded though group living primate.\nEarly bipedalism in homininae model.\nRecent studies of 4.4 million years old \"Ardipithecus ramidus\" suggest bipedalism. It is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. Other recent studies of the foot structure of \"Ardipithecus ramidus\" suggest that the species was closely related to African-ape ancestors. This possibly provides a species close to the true connection between fully bipedal hominins and quadruped apes. According to Richard Dawkins in his book \"The Ancestor's Tale\", chimps and bonobos are descended from \"Australopithecus\" gracile type species while gorillas are descended from \"Paranthropus\". These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines from whom eventually evolved hominins. Early hominines such as \"Ardipithecus ramidus\" may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.\nWarning display (aposematic) model.\nJoseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects. Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.\nOther behavioural models.\nThere are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski &amp; Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others (e.g. Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins (e.g. 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested (e.g. Tanner 1981:165) that male phallic display could have been the initial incentive, as well as increased sexual signaling in upright female posture.\nThermoregulatory model.\nThe thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation. When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus. A feedback mechanism from the advantages of bipedality in hot and open habitats would then in turn make a forest preadaptation solidify as a permanent state.\nCarrying models.\nCharles Darwin wrote that \"Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will\". Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat \"over considerable distances\" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea, as indeed did Lovejoy (1981) with his \"provisioning model\" described above. Others, such as Nancy Tanner (1981), have suggested that infant carrying was key, while others again have suggested stone tools and weapons drove the change. This stone-tools theory is very unlikely, as though ancient humans were known to hunt, the discovery of tools was not discovered for thousands of years after the origin of bipedalism, chronologically precluding it from being a driving force of evolution. (Wooden tools and spears fossilize poorly and therefore it is difficult to make a judgment about their potential usage.)\nWading models.\nThe observation that large primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea, labelled \"the wading hypothesis\", was originally suggested by the Oxford marine biologist Alister Hardy who said: \"It seems to me likely that Man learnt to stand erect first in water and then, as his balance improved, he found he became better equipped for standing up on the shore when he came out, and indeed also for running.\" It was then promoted by Elaine Morgan, as part of the aquatic ape hypothesis, who cited bipedalism among a cluster of other human traits unique among primates, including voluntary control of breathing, hairlessness and subcutaneous fat. The \"aquatic ape hypothesis\", as originally formulated, has not been accepted or considered a serious theory within the anthropological scholarly community. Others, however, have sought to promote wading as a factor in the origin of human bipedalism without referring to further (\"aquatic ape\" related) factors. Since 2000 Carsten Niemitz has published a series of papers and a book on a variant of the wading hypothesis, which he calls the \"amphibian generalist theory\" ().\nOther theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution or critical fallback foods) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism. It has also been thought that consistent water-based food sources had developed early hominid dependency and facilitated dispersal along seas and rivers.\nVocal learning model.\nAn essay by Albert Roca suggests that bipedalism was adopted as a consequence of the acquisition of vocal learning in the human lineage, which led to a longer brain development period, which led to an increased altriciality at birth, which positively selected the offspring of the more efficient bipedal females, thus allowing the extrauterine increase of the brain size and ultimately the development of language. The study highlights the fact that all species with vocal learning exhibit peculiar locomotor adaptations.\nConsequences.\nPrehistoric fossil records show that early hominins first developed bipedalism before being followed by an increase in brain size. The consequences of these two changes in particular resulted in painful and difficult labor due to the increased favor of a narrow pelvis for bipedalism being countered by larger heads passing through the constricted birth canal. This phenomenon is commonly known as the obstetrical dilemma.\nNon-human primates habitually deliver their young on their own, but the same cannot be said for modern-day humans. Isolated birth appears to be rare and actively avoided cross-culturally, even if birthing methods may differ between said cultures. This is due to the fact that the narrowing of the hips and the change in the pelvic angle caused a discrepancy in the ratio of the size of the head to the birth canal. The result of this is that there is greater difficulty in birthing for hominins in general, let alone to be doing it by oneself.\nPhysiology.\nBipedal movement occurs in a number of ways and requires many mechanical and neurological adaptations. Some of these are described below.\nBiomechanics.\nStanding.\nEnergy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.\nShoulder stability.\nShoulder stability would decrease with the evolution of bipedalism. Shoulder mobility would increase because the need for a stable shoulder is only present in arboreal habitats. Shoulder mobility would support suspensory locomotion behaviors which are present in human bipedalism. The forelimbs are freed from weight-bearing requirements, which makes the shoulder a place of evidence for the evolution of bipedalism.\nWalking.\nUnlike non-human apes that are able to practice bipedality such as \"Pan\" and \"Gorilla\", hominins have the ability to move bipedally without the utilization of a bent-hip-bent-knee (BHBK) gait, which requires the engagement of both the hip and the knee joints. This human ability to walk is made possible by the spinal curvature humans have that non-human apes do not. Rather, walking is characterized by an \"inverted pendulum\" movement in which the center of gravity vaults over a stiff leg with each step. Force plates can be used to quantify the whole-body kinetic &amp; potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two. This model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.\nIn humans, walking is composed of several separate processes:\nRunning.\nEarly hominins underwent post-cranial changes in order to better adapt to bipedality, especially running. One of these changes is having longer hindlimbs proportional to the forelimbs and their effects. As previously mentioned, longer hindlimbs assist in thermoregulation by reducing the total surface area exposed to direct sunlight while simultaneously allowing for more space for cooling winds. Additionally, having longer limbs is more energy-efficient, since longer limbs mean that overall muscle strain is lessened. Better energy efficiency, in turn, means higher endurance, particularly when running long distances.\nRunning is characterized by a spring-mass movement. Kinetic and potential energy are in phase, and the energy is stored &amp; released from a spring-like limb during foot contact, achieved by the plantar arch and the Achilles tendon in the foot and leg, respectively. Again, the whole-body kinetics are similar to animals with more limbs.\nMusculature.\nBipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms. In addition to the leg muscles, the increased size of the gluteus maximus in humans is an important adaptation as it provides support and stability to the trunk and lessens the amount of stress on the joints when running.\nRespiration.\nQuadrupeds, have more restrictive breathing respire while moving than do bipedal humans. \"Quadrupedal species normally synchronize the locomotor and respiratory cycles at a constant ratio of 1:1 (strides per breath) in both the trot and gallop. Human runners differ from quadrupeds in that while running they employ several phase-locked patterns (4:1, 3:1, 2:1, 1:1, 5:2, and 3:2), although a 2:1 coupling ratio appears to be favored. Even though the evolution of bipedal gait has reduced the mechanical constraints on respiration in man, thereby permitting greater flexibility in breathing pattern, it has seemingly not eliminated the need for the synchronization of respiration and body motion during sustained running.\"\nRespiration through bipedality means that there is better breath control in bipeds, which can be associated with brain growth. The modern brain utilizes approximately 20% of energy input gained through breathing and eating, as opposed to species like chimpanzees who use up twice as much energy as humans for the same amount of movement. This excess energy, leading to brain growth, also leads to the development of verbal communication. This is because breath control means that the muscles associated with breathing can be manipulated into creating sounds. This means that the onset of bipedality, leading to more efficient breathing, may be related to the origin of verbal language.\nBipedal robots.\nFor nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO, MABEL and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot, those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4211", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=4211", "title": "Bootstrapping", "text": "Self-starting process that is supposed to proceed without external input\nIn general, bootstrapping usually refers to a self-starting process that is supposed to continue or grow without external input. Many analytical techniques are often called \"bootstrap methods\" in reference to their self-starting or self-supporting implementation, such as \"bootstrapping\" in statistics, in finance, or in linguistics.\nEtymology.\nTall boots may have a tab, loop or handle at the top known as a \"bootstrap\", allowing one to use fingers or a boot hook tool to help pull the boots on. The saying \"pull oneself up by one's bootstraps\" was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the \"Workingman's Advocate\": \"It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots.\" In 1860 it appeared in a comment about philosophy of mind: \"The attempt of the mind to analyze itself [is] an effort analogous to one who would lift himself by his own bootstraps.\" Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.\nThe term is sometimes attributed to a story in Rudolf Erich Raspe's \"\", but in that story Baron Munchausen pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), not by his bootstraps\u00a0\u2013 and no explicit reference to bootstraps has been found elsewhere in the various versions of the Munchausen tales.\nOriginally meant to attempt something ludicrously far-fetched or even impossible, the phrase \"Pull yourself up by your bootstraps!\" has since been utilized as a narrative for economic mobility or a cure for depression. That idea is believed to have been popularized by American writer Horatio Alger in the 19th century. To request that someone \"bootstrap\" is to suggest that they might overcome great difficulty by sheer force of will.\nCritics have observed that the phrase is used to portray unfair situations as far more meritocratic than they really are. A 2009 study found that 77% of Americans believe that wealth is often the result of hard work. Various studies have found that the main predictor of future wealth is not IQ or hard work, but initial wealth.\nApplications.\nComputing.\nIn computer technology, the term bootstrapping is a process for creating self-compiling compilers. For example, the Rust compiler was bootstrapped in OCaml.15:34 Also, booting usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, the kernel will load the operating system which will then take care of loading other device drivers and software as needed.\nSoftware loading and execution.\nBooting is the process of starting a computer, specifically with regard to starting its software. The process involves a chain of stages, in which at each stage, a relatively small and simple program loads and then executes the larger, more complicated program of the next stage. It is in this sense that the computer \"pulls itself up by its bootstraps\"; i.e., it improves itself by its own efforts. Booting is a chain of events that starts with execution of hardware-based procedures and may then hand off to firmware and software which is loaded into main memory. Booting often involves processes such as performing self-tests, loading configuration settings, loading a BIOS, resident monitors, a hypervisor, an operating system, or utility software.\nThe computer term bootstrap began as a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an input unit. The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.\nSoftware development.\nBootstrapping can also refer to the development of successively more complex, faster programming environments. The simplest environment will be, perhaps, a very basic text editor (\"e.g.\", ed) and an assembler program. Using these tools, one can write a more complex text editor, and a simple compiler for a higher-level language and so on, until one can have a graphical IDE and an extremely high-level programming language.\nHistorically, bootstrapping also refers to an early technique for computer program development on new hardware. The technique described in this paragraph has been replaced by the use of a cross compiler executed by a pre-existing computer. Bootstrapping in program development began during the 1950s when each program was constructed on paper in decimal code or in binary code, bit by bit (1s and 0s), because there was no high-level computer language, no compiler, no assembler, and no linker. A tiny assembler program was hand-coded for a new computer (for example the IBM 650) which converted a few instructions into binary or decimal code: A1. This simple assembler program was then rewritten in its just-defined assembly language but with extensions that would enable the use of some additional mnemonics for more complex operation codes. The enhanced assembler's source program was then assembled by its predecessor's executable (A1) into binary or decimal code to give A2, and the cycle repeated (now with those enhancements available), until the entire instruction set was coded, branch addresses were automatically calculated, and other conveniences (such as conditional assembly, macros, optimisations, etc.) established. This was how the early Symbolic Optimal Assembly Program (SOAP) was developed. Compilers, linkers, loaders, and utilities were then coded in assembly language, further continuing the bootstrapping process of developing complex software systems by using simpler software.\nThe term was also championed by Doug Engelbart to refer to his belief that organizations could better evolve by improving the process they use for improvement (thus obtaining a compounding effect over time). His SRI team that developed the NLS hypertext system applied this strategy by using the tool they had developed to improve the tool.\nCompilers.\nThe development of compilers for new programming languages first developed in an existing language but then rewritten in the new language and compiled by itself, is another example of the bootstrapping notion.\nInstallers.\nDuring the installation of computer programs, it is sometimes necessary to update the installer or package manager itself. The common pattern for this is to use a small executable bootstrapper file (\"e.g.,\" setup.exe) which updates the installer and starts the real installation after the update. Sometimes the bootstrapper also installs other prerequisites for the software during the bootstrapping process.\nOverlay networks.\nA bootstrapping node, also known as a rendezvous host, is a node in an overlay network that provides initial configuration information to newly joining nodes so that they may successfully join the overlay network.\nDiscrete-event simulation.\nA type of computer simulation called discrete-event simulation represents the operation of a system as a chronological sequence of events. A technique called \"bootstrapping the simulation model\" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state\u2014the bootstrapping behavior is overwhelmed by steady-state behavior.\nArtificial intelligence and machine learning.\nBootstrapping is a technique used to iteratively improve a classifier's performance. Typically, multiple classifiers will be trained on different sets of the input data, and on prediction tasks the output of the different classifiers will be combined.\nSeed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading to an exponential increase in intelligence. No such AI is known to exist, but it remains an active field of research. Seed AI is a significant part of some theories about the technological singularity: proponents believe that the development of seed AI will rapidly yield ever-smarter intelligence (via bootstrapping) and thus a new era.\nStatistics.\nBootstrapping is a resampling technique used to obtain estimates of summary statistics.\nBusiness.\nBootstrapping in business means starting a business without external help or working capital. Entrepreneurs in the startup development phase of their company survive through internal cash flow and are very cautious with their expenses. Generally at the start of a venture, a small amount of money will be set aside for the bootstrap process. Bootstrapping can also be a supplement for econometric models. Bootstrapping was also expanded upon in the book \"Bootstrap Business\" by Richard Christiansen, the Harvard Business Review article \"The Art of Bootstrapping\" and the follow-up book \"The Origin and Evolution of New Businesses\" by Amar Bhide. There is also an entire bible written on how to properly bootstrap by Seth Godin.\nExperts have noted that several common stages exist for bootstrapping a business venture:\nThere are many types of companies that are eligible for bootstrapping. Early-stage companies that do not necessarily require large influxes of capital (particularly from outside sources) qualify. This would specifically allow for flexibility for the business and time to grow. Serial entrepreneur companies could also possibly reap the benefits of bootstrapping. These are organizations whereby the founder has money from the sale of a previous companies they can use to invest.\nThere are different methods of bootstrapping. Future business owners aspiring to use bootstrapping as way of launching their product or service often use the following methods:\nBootstrapping is often considered successful. When taking into account statistics provided by Fundera, approximately 77% of small business rely on some sort of personal investment and or savings in order to fund their startup ventures. The average small business venture requires approximately $10,000 in startup capital with a third of small business launching with less than $5,000 bootstrapped.\nBased on startup data presented by Entrepreneur.com, in comparison other methods of funding, bootstrapping is more commonly used than others. \"0.91% of startups are funded by angel investors, while 0.05% are funded by VCs. In contrast, 57 percent of startups are funded by personal loans and credit, while 38 percent receive funding from family and friends.\"\nSome examples of successful entrepreneurs that have used bootstrapping in order to finance their businesses include serial entrepreneur Mark Cuban. He has publicly endorsed bootstrapping claiming that \"If you can start on your own \u2026 do it by [yourself] without having to go out and raise money.\" When asked why he believed this approach was most necessary, he replied, \"I think the biggest mistake people make is once they have an idea and the goal of starting a business, they think they have to raise money. And once you raise money, that's not an accomplishment, that's an obligation\" because \"now, you're reporting to whoever you raised money from.\"\nBootstrapped companies such as Apple Inc. (APPL), eBay Inc. (EBAY) and Coca-Cola Co. have also claimed that they attribute some of their success to the fact that this method of funding enables them to remain highly focused on a specific array of profitable product.\nStartups can grow by reinvesting profits in its own growth if bootstrapping costs are low and return on investment is high. This financing approach allows owners to maintain control of their business and forces them to spend with discipline. In addition, bootstrapping allows startups to focus on customers rather than investors, thereby increasing the likelihood of creating a profitable business. This leaves startups with a better exit strategy with greater returns.\nLeveraged buyouts, or highly leveraged or \"bootstrap\" transactions, occur when an investor acquires a controlling interest in a company's equity and where a significant percentage of the purchase price is financed through leverage, i.e.\u00a0borrowing by the acquired company.\nOperation Bootstrap (\"Operaci\u00f3n Manos a la Obra\") refers to the ambitious projects that industrialized Puerto Rico in the mid-20th century.\nBiology.\nRichard Dawkins in his book \"River Out of Eden\" used the computer bootstrapping concept to explain how biological cells differentiate: \"Different cells receive different combinations of chemicals, which switch on different combinations of genes, and some genes work to switch other genes on or off. And so the bootstrapping continues, until we have the full repertoire of different kinds of cells.\"\nPhylogenetics.\nBootstrapping analysis gives a way to judge the strength of support for clades on phylogenetic trees. A number is written by a node, which reflects the percentage of bootstrap trees which also resolve the clade at the endpoints of that branch (the classical Felsenstein bootstrap). Repeating the creation of trees can be computationally expensive and there are newer methods such as the RAxML rapid bootstrap (RBS), the Shimodaira\u2013Hasegawa-like approximate likelihood ratio test (SH-aLRT), and ultrafast bootstrap (UFBoot). The classical/standard bootstrap tends to underestimate the likelihood of a clade being correct.\nBootstrapping (classical or otherwise) does not work on well on trees made from multi-gene concatenation, where all splits tend towards 100%. The concordance factor should be used instead.\nLaw.\nBootstrapping is a rule preventing the admission of hearsay evidence in conspiracy cases.\nLinguistics.\nBootstrapping is a theory of language acquisition.\nPhysics.\nFlatness.\nWhitworth's three plates method does not rely other flat reference surfaces or other precision instruments, and thus solves the problem of how to create a better precise flat surface.\nQuantum theory.\nBootstrapping is using very general consistency criteria to determine the form of a quantum theory from some assumptions on the spectrum of particles or operators.\nMagnetically confined fusion plasmas.\nIn tokamak fusion devices, bootstrapping refers to the process in which a bootstrap current is self-generated by the plasma, which reduces or eliminates the need for an external current driver. Maximising the bootstrap current is a major goal of advanced tokamak designs.\nInertially confined fusion plasmas.\nBootstrapping in inertial confinement fusion refers to the alpha particles produced in the fusion reaction providing further heating to the plasma. This heating leads to ignition and an overall energy gain.\nElectronics.\nBootstrapping is a form of positive feedback in analog circuit design.\nElectric power grid.\nAn electric power grid is almost never brought down intentionally. Generators and power stations are started and shut down as necessary. A typical power station requires power for start up prior to being able to generate power. This power is obtained from the grid, so if the entire grid is down these stations cannot be started.\nTherefore, to get a grid started, there must be at least a small number of power stations that can start entirely on their own. A black start is the process of restoring a power station to operation without relying on external power. In the absence of grid power, one or more black starts are used to bootstrap the grid.\nNuclear power.\nA nuclear power plant always needs to have a way to remove decay heat, which is usually done with electrical cooling pumps. But in the rare case of a complete loss of electrical power, this can still be achieved by booting a turbine generator. As steam builds up in the steam generator, it can be used to power the turbine generator (initially with no oil pumps, circ water pumps, or condensation pumps). Once the turbine generator is producing electricity, the auxiliary pumps can be powered on, and the reactor cooling pumps can be run momentarily. Eventually the steam pressure will become insufficient to power the turbine generator, and the process can be shut down in reverse order. The process can be repeated until no longer needed. This can cause great damage to the turbine generator, but more importantly, it saves the nuclear reactor.\nCellular networks.\nA Bootstrapping Server Function (BSF) is an intermediary element in cellular networks which provides application independent functions for mutual authentication of user equipment and servers unknown to each other and for 'bootstrapping' the exchange of secret session keys afterwards. The term 'bootstrapping' is related to building a security relation with a previously unknown device first and to allow installing security elements (keys) in the device and the BSF afterwards.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4212", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=4212", "title": "Bolshevik", "text": ""}
{"id": "4213", "revid": "31967890", "url": "https://en.wikipedia.org/wiki?curid=4213", "title": "Baltic languages", "text": "Branch of the Indo-European language family\nThe Baltic languages are a branch of the Indo-European language family spoken natively or as a second language by a population of about 6.5\u20137.0 million people mainly in areas extending east and southeast of the Baltic Sea in Europe. Together with the Slavic languages, they form the Balto-Slavic branch of the Indo-European family.\nScholars usually regard them as a single subgroup divided into two branches: West Baltic (containing only extinct languages) and East Baltic (containing at least two living languages, Lithuanian, Latvian, and by some counts including Latgalian and Samogitian as separate languages rather than dialects of those two). In addition, the existence of the Dnieper-Oka language is hypothesized, with the extinct Golyad language being the only known member. The range of the East Baltic linguistic influence once possibly reached as far as the Ural Mountains, but this hypothesis has been questioned.\nOld Prussian, a Western Baltic language that became extinct in the 18th century, had possibly conserved the greatest number of properties from Proto-Baltic.\nAlthough related, Lithuanian, Latvian, and particularly Old Prussian have lexicons that differ substantially from one another and so the languages are not mutually intelligible. Relatively low mutual interaction for neighbouring languages historically led to gradual erosion of mutual intelligibility, and development of their respective linguistic innovations that did not exist in shared Proto-Baltic. The substantial number of false friends and various uses and sources of loanwords from their surrounding languages are considered to be the major reasons for poor mutual intelligibility today.\nBranches.\nWithin Indo-European, the Baltic languages are generally classified as forming a single family with two branches: Eastern and Western Baltic. But these two branches are sometimes classified as independent branches of Balto-Slavic itself.\nHistory.\nIt is believed that the Baltic languages are among the most conservative of the currently remaining Indo-European languages, despite their late attestation.\nAlthough the Baltic Aesti tribe was mentioned by ancient historians such as Tacitus as early as 98 CE, the first attestation of a Baltic language was c. 1369, in a Basel epigram of two lines written in Old Prussian. Lithuanian was first attested in a printed book, which is a Catechism by Martynas Ma\u017evydas published in 1547. Latvian appeared in a printed Catechism in 1585.\nOne reason for the late attestation is that the Baltic peoples resisted Christianization longer than any other Europeans, which delayed the introduction of writing and isolated their languages from outside influence.\nWith the establishment of a German state in Prussia, and the mass influx of Germanic (and to a lesser degree Slavic-speaking) settlers, the Prussians began to be assimilated, and by the end of the 17th century, the Prussian language had become extinct.\nAfter the Partitions of Polish-Lithuanian Commonwealth, most of the Baltic lands were under the rule of the Russian Empire, where the native languages or alphabets were sometimes prohibited from being written down or used publicly in a Russification effort (see Lithuanian press ban for the ban in force from 1864 to 1904).\nGeographic distribution.\nSpeakers of modern Baltic languages are generally concentrated within the borders of Lithuania and Latvia, and in emigrant communities in the United States, Canada, Australia and the countries within the former borders of the Soviet Union.\nHistorically the languages were spoken over a larger area: west to the mouth of the Vistula river in present-day Poland, at least as far east as the Dniepr river in present-day Belarus, perhaps even to Moscow, and perhaps as far south as Kyiv. Key evidence of Baltic language presence in these regions is found in hydronyms (names of bodies of water) that are characteristically Baltic. The use of hydronyms is generally accepted to determine the extent of a culture's influence, but \"not\" the date of such influence.\nThe eventual expansion of the use of Slavic languages in the south and east, and Germanic languages in the west, reduced the geographic distribution of Baltic languages to a fraction of the area that they formerly covered. The Russian geneticist Oleg Balanovsky speculated that there is a predominance of the assimilated pre-Slavic substrate in the genetics of East and West Slavic populations, according to him the common genetic structure which contrasts East Slavs and Balts from other populations may suggest that the pre-Slavic substrate of the East Slavs consists most significantly of Baltic-speakers, which predated the Slavs in the cultures of the Eurasian steppe according to archaeological references he cites.\nContact with Uralic languages.\nThough Estonia is geopolitically included among the Baltic states due to its location, Estonian is a Finnic language of the Uralic language family and is not related to the Baltic languages, which are Indo-European.\nThe Mordvinic languages, spoken mainly along western tributaries of the Volga, show several dozen loanwords from one or more Baltic languages. These may have been mediated by contacts with the Eastern Balts along the river Oka. In regards to the same geographical location, Asko Parpola, in a 2013 article, suggested that the Baltic presence in this area, dated to c.\u2009200\u2013600 CE, is due to an \"elite superstratum\". However, linguist Petri Kallio argued that the Volga-Oka is a \"secondary\" Baltic-speaking area, expanding from East Baltic, due to a large number of Baltic loanwords in Finnic and Saami.\nFinnish scholars also indicate that Latvian had extensive contacts with Livonian, and, to a lesser extent, to Estonian and South Estonian. Therefore, this contact accounts for the number of Finnic hydronyms in Lithuania and Latvia that increase in a northwards direction.\nParpola, in the same article, supposed the existence of a Baltic substratum for Finnic, in Estonia and coastal Finland. In the same vein, Kallio argues for the existence of a lost \"North Baltic language\" that would account for loanwords during the evolution of the Finnic branch.\nComparative linguistics.\nGenetic relatedness.\nThe Baltic languages are of particular interest to linguists because they retain many archaic features, which are thought to have been present in the early stages of the Proto-Indo-European language. However, linguists have had a hard time establishing the precise relationship of the Baltic languages to other languages in the Indo-European family. Several of the extinct Baltic languages have a limited or nonexistent written record, their existence being known only from the records of ancient historians and personal or place names. All of the languages in the Baltic group (including the living ones) were first written down relatively late in their probable existence as distinct languages. These two factors combined with others have obscured the history of the Baltic languages, leading to a number of theories regarding their position in the Indo-European family.\nThe Baltic languages show a close relationship with the Slavic languages, and are grouped with them in a Balto-Slavic family by most scholars. This family is considered to have developed from a common ancestor, Proto-Balto-Slavic. Later on, several lexical, phonological and morphological dialectisms developed, separating the various Balto-Slavic languages from each other. Although it is generally agreed that the Slavic languages developed from a single more-or-less unified dialect (Proto-Slavic) that split off from common Balto-Slavic, there is more disagreement about the relationship between the Baltic languages.\nThe traditional view is that the Balto-Slavic languages split into two branches, Baltic and Slavic, with each branch developing as a single common language (Proto-Baltic and Proto-Slavic) for some time afterwards. Proto-Baltic is then thought to have split into East Baltic and West Baltic branches. However, more recent scholarship has suggested that there was no unified Proto-Baltic stage, but that Proto-Balto-Slavic split directly into three groups: Slavic, East Baltic and West Baltic. Under this view, the Baltic family is paraphyletic, and consists of all Balto-Slavic languages that are not Slavic. In the 1960s Vladimir Toporov and Vyacheslav Ivanov made the following conclusions about the relationship between the Baltic and Slavic languages: \nThese scholars' theses do not contradict the close relationship between Baltic and Slavic languages and, from a historical perspective, specify the Baltic-Slavic languages' evolution \u2013 the terms 'Baltic' and 'Slavic' are relevant only from the point of view of the present time, meaning diachronic changes, and the oldest stage of the language development could be called both Baltic and Slavic; this concept does not contradict the traditional thesis that the Proto-Slavic and Proto-Baltic languages coexisted for a long time after their formation \u2013 between the 2nd millennium BC and circa the 5th century BC \u2013 the Proto-Slavic language was a continuum of the Proto-Baltic dialects, more rather, the Proto-Slavic language should have been localized in the peripheral circle of Proto-Baltic dialects.\nFinally, a minority of scholars argue that Baltic descended directly from Proto-Indo-European, without an intermediate common Balto-Slavic stage. They argue that the many similarities and shared innovations between Baltic and Slavic are caused by several millennia of contact between the groups, rather than a shared heritage.\nThracian hypothesis.\nThe Baltic-speaking peoples likely encompassed an area in eastern Europe much larger than their modern range. As in the case of the Celtic languages of Western Europe, they were reduced by invasion, extermination and assimilation. Studies in comparative linguistics point to genetic relationship between the languages of the Baltic family and the following extinct languages:\nThe Baltic classification of Dacian and Thracian has been proposed by the Lithuanian scientist Jonas Basanavi\u010dius, who insisted this is the most important work of his life and listed 600 identical words of Balts and Thracians. His theory included Phrygian in the related group, but this did not find support and was disapproved among other authors, such as , whose own analysis found Phrygian completely lacking parallels in either Thracian or Baltic languages.\nThe Bulgarian linguist Ivan Duridanov, who improved the most extensive list of toponyms, in his first publication claimed that Thracian is genetically linked to the Baltic languages and in the next one he made the following classification: \"The Thracian language formed a close group with the Baltic, the Dacian and the \"Pelasgian\" languages. More distant were its relations with the other Indo-European languages, and especially with Greek, the Italic and Celtic languages, which exhibit only isolated phonetic similarities with Thracian; the Tokharian and the Hittite were also distant. \" Of about 200 https:// by Duridanov most cognates (138) appear in the Baltic languages, mostly in Lithuanian, followed by Germanic (61), Indo-Aryan (41), Greek (36), Bulgarian (23), Latin (10) and Albanian (8). The cognates of the reconstructed Dacian words in his publication are found mostly in the Baltic languages, followed by Albanian. Parallels have enabled linguists, using the techniques of comparative linguistics, to decipher the meanings of several Dacian and Thracian placenames with, they claim, a high degree of probability. Of 74 Dacian placenames attested in primary sources and considered by Duridanov, a total of 62 have Baltic cognates, most of which were rated \"certain\" by Duridanov. For a big number of 300 Thracian geographic names most parallels were found between Thracian and Baltic geographic names in the study of Duridanov. According to him the most important impression make the geographic cognates of Baltic and Thracian \"the similarity of these parallels stretching frequently on the main element and the suffix simultaneously, which makes a strong impression\".\nRomanian linguist Sorin Paliga, analysing and criticizing Harvey Mayer's study, did admit \"great likeness\" between Thracian, the substrate of Romanian, and \"some Baltic forms\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4214", "revid": "50865928", "url": "https://en.wikipedia.org/wiki?curid=4214", "title": "Bioinformatics", "text": "Computational analysis of large, complex sets of biological data\nBioinformatics () is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. Bioinformatics uses biology, chemistry, physics, computer science, data science, computer programming, information engineering, mathematics and statistics to analyze and interpret biological data. This process can sometimes be referred to as computational biology, however the distinction between the two terms is often disputed. To some, the term \"computational biology\" refers to building and using models of biological systems.\nComputational, statistical, and computer programming techniques have been used for computer simulation analyses of biological queries. They include reused specific analysis \"pipelines\", particularly in the field of genomics, such as by the identification of genes and single nucleotide polymorphisms (SNPs). These pipelines are used to better understand the genetic basis of disease, unique adaptations, desirable properties (especially in agricultural species), or differences between populations. Bioinformatics also includes proteomics, which aims to understand the organizational principles within nucleic acid and protein sequences.\nImage and signal processing allow the extraction of useful results from large amounts of raw data. It aids in sequencing and annotating genomes and their observed mutations. Bioinformatics includes text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatic tools aid in comparing, analyzing, and interpreting genetic and genomic data and in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.\nHistory.\nThe first definition of the term \"bioinformatics\" was coined by Paulien Hogeweg and Ben Hesper in 1970, to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biochemistry (the study of chemical processes in biological systems).\nBioinformatics and computational biology involved the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.\nAnalyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.\nSequences.\nThere has been a tremendous advance in speed and cost reduction since the completion of the Human Genome Project, with some labs able to sequence over 100,000 billion bases each year, and a full genome can be sequenced for $1,000 or less.\nComputers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. Margaret Oakley Dayhoff, a pioneer in the field, compiled one of the first protein sequence databases, initially published as books as well as methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released online with Tai Te Wu between 1980 and 1991.\nIn the 1970s, new techniques for sequencing DNA were applied to bacteriophage MS2 and \u00f8X174, and the extended nucleotide sequences were then parsed with informational and statistical algorithms. These studies illustrated that well known features, such as the coding segments and the triplet code, are revealed in straightforward statistical analyses and were the proof of the concept that bioinformatics would be insightful.\nGoals.\nIn order to study how normal cellular activities are altered in different disease states, raw biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This also includes nucleotide and amino acid sequences, protein domains, and protein structures.\nImportant sub-disciplines within bioinformatics and computational biology include:\nThe primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\u2013protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.\nBioinformatics entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.\nOver the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.\nCommon activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.\nSequence analysis.\nSince the bacteriophage Phage \u03a6-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Computer programs such as BLAST are used routinely to search sequences\u2014as of 2008, from more than 260,000 organisms, containing over 190 billion nucleotides.\nDNA sequencing.\nBefore sequences can be analyzed, they are obtained from a data storage bank, such as GenBank. DNA sequencing is still a non-trivial problem as the raw data may be noisy or affected by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.\nSequence assembly.\nMost DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The shotgun sequencing technique (used by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, \"Haemophilus influenzae\") generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced (rather than chain-termination or chemical degradation methods), and genome assembly algorithms are a critical area of bioinformatics research.\nGenome annotation.\nIn genomics, annotation refers to the process of marking the stop and start regions of genes and other biological features in a sequenced DNA sequence. Many genomes are too large to be annotated by hand. As the rate of sequencing exceeds the rate of genome annotation, genome annotation has become the new bottleneck in bioinformatics.\nGenome annotation can be classified into three levels: the nucleotide, protein, and process levels.\nGene finding is a chief aspect of nucleotide-level annotation. For complex genomes, a combination of ab initio gene prediction and sequence comparison with expressed sequence databases and other organisms can be successful. Nucleotide-level annotation also allows the integration of genome sequence with other genetic and physical maps of the genome.\nThe principal aim of protein-level annotation is to assign function to the protein products of the genome. Databases of protein sequences and functional domains and motifs are used for this type of annotation. About half of the predicted proteins in a new genome sequence tend to have no obvious function.\nUnderstanding the function of genes and their products in the context of cellular and organismal physiology is the goal of process-level annotation. An obstacle of process-level annotation has been the inconsistency of terms used by different model systems. The Gene Ontology Consortium is helping to solve this problem.\nThe first description of a comprehensive annotation system was published in 1995 by The Institute for Genomic Research, which performed the first complete sequencing and analysis of the genome of a free-living (non-symbiotic) organism, the bacterium \"Haemophilus influenzae\". The system identifies the genes encoding all proteins, transfer RNAs, ribosomal RNAs, in order to make initial functional assignments. The GeneMark program trained to find protein-coding genes in \"Haemophilus influenzae\" is constantly changing and improving.\nFollowing the goals that the Human Genome Project left to achieve after its closure in 2003, the ENCODE project was developed by the National Human Genome Research Institute. This project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).\nGene function prediction.\nWhile genome annotation is primarily based on sequence similarity (and thus homology), other properties of sequences can be used to predict the function of genes. In fact, most \"gene\" function prediction methods focus on \"protein\" sequences as they are more informative and more feature-rich. For instance, the distribution of hydrophobic amino acids predicts transmembrane segments in proteins. However, protein function prediction can also use external information such as gene (or protein) expression data, protein structure, or protein\u2013protein interactions.\nComputational evolutionary biology.\nEvolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:\nComparative genomics.\nThe core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. Intergenomic maps are made to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Entire genomes are involved in processes of hybridization, polyploidization and endosymbiosis that lead to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.\nMany of these studies are based on the detection of sequence homology to assign sequences to protein families.\nPan genomics.\nPan genomics is a concept introduced in 2005 by Tettelin and Medini. Pan genome is the complete gene repertoire of a particular monophyletic taxonomic group. Although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum, etc. It is divided in two parts: the Core genome, a set of genes common to all the genomes under study (often housekeeping genes vital for survival), and the Dispensable/Flexible genome: a set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.\nGenetics of disease.\nAs of 2013, the existence of efficient high-throughput next-generation sequencing technology allows for the identification of cause many different human disorders. Simple Mendelian inheritance has been observed for over 3,000 disorders that have been identified at the Online Mendelian Inheritance in Man database, but complex diseases are more difficult. Association studies have found many individual genetic regions that individually are weakly associated with complex diseases (such as infertility, breast cancer and Alzheimer's disease), rather than a single cause. There are currently many challenges to using genes for diagnosis and treatment, such as how we don't know which genes are important, or how stable the choices an algorithm provides.\nGenome-wide association studies have successfully identified thousands of common genetic variants for complex diseases and traits; however, these common variants only explain a small fraction of heritability. Rare variants may account for some of the missing heritability. Large-scale whole genome sequencing studies have rapidly sequenced millions of whole genomes, and such studies have identified hundreds of millions of rare variants. Functional annotations predict the effect or function of a genetic variant and help to prioritize rare functional variants, and incorporating these annotations can effectively boost the power of genetic association of rare variants analysis of whole genome sequencing studies. Some tools have been developed to provide all-in-one rare variant association analysis for whole-genome sequencing data, including integration of genotype data and their functional annotations, association analysis, result summary and visualization. Meta-analysis of whole genome sequencing studies provides an attractive solution to the problem of collecting large sample sizes for discovering rare variants associated with complex phenotypes.\nAnalysis of mutations in cancer.\nIn cancer, the genomes of affected cells are rearranged in complex or unpredictable ways. In addition to single-nucleotide polymorphism arrays identifying point mutations that cause cancer, oligonucleotide microarrays can be used to identify chromosomal gains and losses (called comparative genomic hybridization). These detection methods generate terabytes of data per experiment. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.\nTwo important principles can be used to identify cancer by mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second, cancer contains driver mutations which need to be distinguished from passengers.\nFurther improvements in bioinformatics could allow for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples. Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.\nGene and protein expression.\nAnalysis of gene expression.\nThe expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.\nAnalysis of protein expression.\nProtein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. The former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples when multiple incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.\nAnalysis of regulation.\nGene regulation is a complex process where a signal, such as an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.\nFor example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the protein-coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.\nExpression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). Clustering algorithms can be then applied to expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.\nAnalysis of cellular organization.\nSeveral approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. A gene ontology category, \"cellular component\", has been devised to capture subcellular localization in many biological databases.\nMicroscopy and image analysis.\nMicroscopic pictures allow for the location of organelles as well as molecules, which may be the source of abnormalities in diseases.\nProtein localization.\nFinding the location of proteins allows us to predict what they do. This is called protein function prediction. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. There are well developed protein subcellular localization prediction resources available, including protein subcellular location databases, and prediction tools.\nNuclear organization of chromatin.\nData from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.\nStructural bioinformatics.\nFinding the structure of proteins is an important application of bioinformatics. The Critical Assessment of Protein Structure Prediction (CASP) is an open competition where worldwide research groups submit protein models for evaluating unknown protein models.\nAmino acid sequence.\nThe linear amino acid sequence of a protein is called the primary structure. The primary structure can be easily determined from the sequence of codons on the DNA gene that codes for it. In most proteins, the primary structure uniquely determines the 3-dimensional structure of a protein in its native environment. An exception is the misfolded prion protein involved in bovine spongiform encephalopathy. This structure is linked to the function of the protein. Additional structural information includes the \"secondary\", \"tertiary\" and \"quaternary\" structure. A viable general solution to the prediction of the function of a protein remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.\nHomology.\nIn the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene \"A\", whose function is known, is homologous to the sequence of gene \"B,\" whose function is unknown, one could infer that B may share A's function. In structural bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. Homology modeling is used to predict the structure of an unknown protein from existing homologous proteins.\nOne example of this is hemoglobin in humans and the hemoglobin in legumes (leghemoglobin), which are distant relatives from the same protein superfamily. Both serve the same purpose of transporting oxygen in the organism. Although both of these proteins have very different amino acid sequences, their protein structures are very similar, reflecting their shared function and shared ancestor.\nOther techniques for predicting protein structure include protein threading and \"de novo\" (from scratch) physics-based modeling.\nAnother aspect of structural bioinformatics include the use of protein structures for Virtual Screening models such as Quantitative Structure-Activity Relationship models and proteochemometric models (PCM). Furthermore, a protein's crystal structure can be used in simulation of for example ligand-binding studies and \"in silico\" mutagenesis studies.\nA 2021 deep-learning algorithms-based software called AlphaFold, developed by Google's DeepMind, greatly outperforms all other prediction software methods, and has released predicted structures for hundreds of millions of proteins in the AlphaFold protein structure database.\nNetwork and systems biology.\n\"Network analysis\" seeks to understand the relationships within biological networks such as metabolic or protein\u2013protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.\n\"Systems biology\" involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.\nMolecular interaction networks.\nTens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\u2013protein interactions only based on these 3D shapes, without performing protein\u2013protein interaction experiments. A variety of methods have been developed to tackle the protein\u2013protein docking problem, though it seems that there is still much work to be done in this field.\nOther interactions encountered in the field include Protein\u2013ligand (including drug) and protein\u2013peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.\nBiodiversity informatics.\nBiodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools. A growing area is also macro-ecology, i.e. the study of how biodiversity is connected to ecology and human impact, such as climate change.\nOthers.\nLiterature analysis.\nThe enormous number of published literature makes it virtually impossible for individuals to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:\nThe area of research draws from statistics and computational linguistics.\nHigh-throughput image analysis.\nComputational technologies are used to automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems can improve an observer's accuracy, objectivity, or speed. Image analysis is important for both diagnostics and research. Some examples are:\nHigh-throughput single cell data analysis.\nComputational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.\nOntologies and data integration.\nBiological ontologies are directed acyclic graphs of controlled vocabularies. They create categories for biological concepts and descriptions so they can be easily analyzed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.\nThe OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.\nDatabases.\nDatabases are essential for bioinformatics research and applications. Databases exist for many different information types, including DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases can contain both empirical data (obtained directly from experiments) and predicted data (obtained from analysis of existing data). They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. Databases can have different formats, access mechanisms, and be public or private.\nSome of the most commonly used databases are listed below:\nSoftware and tools.\nSoftware tools for bioinformatics include simple command-line tools, more complex graphical programs, and standalone web-services. They are made by bioinformatics companies or by public institutions.\nOpen-source bioinformatics software.\nMany free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative \"in silico\" experiments, and freely available open code bases have created opportunities for research groups to contribute to both bioinformatics regardless of funding. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide \"de facto\" standards and shared object models for assisting with the challenge of bioinformation integration.\nOpen-source bioinformatics software includes Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD.\nThe non-profit Open Bioinformatics Foundation and the annual Bioinformatics Open Source Conference promote open-source bioinformatics software.\nWeb services in bioinformatics.\nSOAP- and REST-based interfaces have been developed to allow client computers to use algorithms, data and computing resources from servers in other parts of the world. The main advantage are that end users do not have to deal with software and database maintenance overheads.\nBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.\nBioinformatics workflow management systems.\nA bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to\nSome of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.\nBioCompute and BioCompute Objects.\nIn 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics. Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm. These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.\nIt was decided that the BioCompute paradigm would be in the form of digital 'lab notebooks' which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.\nIn 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as both a \"standard trial use\" document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.\nEducation platforms.\nWhile bioinformatics is taught as an in-person master's degree at many universities, there are many other methods and technologies available to learn and obtain certification in the subject. The computational nature of bioinformatics lends it to computer-aided and online learning. Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\u03c0 project or 4273pi project also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils. 4273 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\u03c0 operating system.\nMOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization at the University of California, San Diego, Genomic Data Science Specialization at Johns Hopkins University, and EdX's Data Analysis for Life Sciences XSeries at Harvard University.\nConferences.\nThere are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4215", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4215", "title": "Brian de Palma", "text": ""}
{"id": "4216", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=4216", "title": "Brian De Palma", "text": "American film director (born 1940)\nBrian Russell De Palma (; born September 11, 1940) is an American film director and screenwriter. With a career spanning over 50 years, he is best known for work in the suspense, crime, and psychological thriller genres. De Palma was a leading member of the New Hollywood generation.\n\"Carrie\" (1976), his adaptation of Stephen King's novel of the same name, gained him prominence as a young filmmaker. He enjoyed commercial success with \"Dressed to Kill\" (1980), \"The Untouchables\" (1987) and (1996) and made cult classics such as \"Greetings\" (1968), \"Hi, Mom!\" (1970), \"Sisters\" (1972), \"Phantom of the Paradise\" (1974), and \"The Fury\" (1978).\nAs a young director, De Palma dreamed of being the \"American Godard\". His style is allusive; he paid homage to Alfred Hitchcock in \"Obsession\" (1976) and \"Body Double\" (1984); \"Blow Out\" (1981) is based on Michelangelo Antonioni's \"Blowup\" (1966), and \"Scarface\" (1983), his remake of Howard Hawks's 1932 film, is dedicated to Hawks and Ben Hecht. His work has been criticized for its violence and sexual content but has also been championed by American critics such as Roger Ebert and Pauline Kael. In 2015, he was interviewed about his work in a well-received documentary by Noah Baumbach.\nEarly life and education.\nDe Palma was born on September 11, 1940, in Newark, New Jersey, the youngest of three boys. His Italian-American parents were Vivienne DePalma (n\u00e9e Muti), and Anthony F. DePalma, an orthopedic surgeon who was the son of immigrants from Alberona, Province of Foggia. He was raised in Philadelphia, Pennsylvania and New Hampshire, and attended various Protestant and Quaker schools, eventually graduating from Friends' Central School. He had a poor relationship with his father, and would secretly follow him to record his adulterous behavior; this would eventually inspire the teenage character in De Palma's \"Dressed to Kill\" (1980). When he was in high school, he built computers. He won a regional science-fair prize for his project \"An Analog Computer to Solve Differential Equations\".\nEnrolled at Columbia University as a physics student, De Palma became enraptured with filmmaking after seeing Orson Welles's \"Citizen Kane\" (1941) and Alfred Hitchcock's \"Vertigo\" (1958). After receiving his undergraduate degree in 1962, De Palma enrolled at the newly mixed-gender Sarah Lawrence College as a graduate student in their theater department, earning an M.A. in the discipline in 1964 and becoming one of the first male students in a predominantly female school. Once there, influences as various as drama teacher Wilford Leach, the Maysles brothers, Michelangelo Antonioni, Andy Warhol and Jean-Luc Godard, impressed upon De Palma the many styles and themes that would shape his work in the coming decades.\nCareer.\n1963\u20131976: Rise to prominence.\nAn early association with a young Robert De Niro resulted in \"The Wedding Party\". The film, co-directed with Wilford Leach and producer Cynthia Munroe, had been shot in 1963 but remained unreleased until 1969, when De Palma's star had risen sufficiently in the Greenwich Village filmmaking scene. De Niro was unknown at the time; the credits mistakenly display his name as \"Robert Denero\". The film is noteworthy for its invocation of silent film techniques and use of the jump-cut. De Palma followed this style with various small films for the NAACP and the Treasury Department.\nDuring the 1960s, De Palma began making a living producing documentaries, notably \"The Responsive Eye\" (1966), about \"The Responsive Eye\" op-art exhibit curated by William Seitz for MoMA in 1965. In an interview with Joseph Gelmis from 1969, De Palma described the film as \"very good and very successful. It's distributed by Pathe Contemporary and makes lots of money. I shot it in four hours, with synched sound. I had two other guys shooting people's reactions to the paintings, and the paintings themselves.\"\n\"Dionysus in '69\" (1969) was De Palma's other major documentary from this period. The film records the Performance Group's performance of Euripides's \"The Bacchae\", starring, amongst others, De Palma regular William Finley. The play is noted for breaking traditional barriers between performers and audience. The film's most striking quality is its extensive use of the split-screen. De Palma recalls that he was \"floored\" by this performance upon first sight, and in 1973 recounts how he \"began to try and figure out a way to capture it on film. I came up with the idea of split-screen, to be able to show the actual audience involvement, to trace the life of the audience and that of the play as they merge in and out of each other.\"\nDe Palma's most significant features from this decade are \"Greetings\" (1968) and \"Hi, Mom!\" (1970). Both films star De Niro and espouse a leftist revolutionary viewpoint in the spirit of the time. \"Greetings\" was entered into the 19th Berlin International Film Festival, where it won a Silver Bear award. His other major film from this period is the slasher comedy \"Murder a la Mod\" (1968). Each of these films experiments with narrative and intertextuality, reflecting De Palma's stated intention to become the \"American Godard\".\nIn 1970, De Palma left New York for Hollywood at age thirty to make \"Get to Know Your Rabbit\" (1972), starring Orson Welles and Tommy Smothers. Making the film was a crushing experience for De Palma, as Smothers did not like many of De Palma's ideas. Here he made several small, studio and independently released films. Among them were the horror film \"Sisters\" (1972), the rock musical \"Phantom of the Paradise\" (1974) and \"Obsession\" (1976), a variation on theme of Alfred Hitchcock's \"Vertigo\" (1958) scored by Hitchcock's frequent collaborator Bernard Herrmann.\n1976\u20131979: Breakthrough.\nIn November 1976, De Palma released an adaptation of Stephen King's novel \"Carrie\". Though some see the psychic thriller as De Palma's bid for a blockbuster, the project was in fact small, underfunded by United Artists, and well under the cultural radar during the early months of production, as King's novel was not yet a bestseller. De Palma gravitated toward the project and changed crucial plot elements based upon his own predilections. The cast was mostly young and relatively new, though Sissy Spacek and John Travolta had gained attention for previous work in, respectively, film and sitcoms. \"Carrie\" became De Palma's first genuine box-office success, garnering Spacek and Piper Laurie Oscar nominations for their performances. Pre-production for the film had coincided with the casting process for George Lucas's \"Star Wars\", and many of the actors cast in De Palma's film had been earmarked as contenders for Lucas's movie, and vice versa. Its suspense sequences are buttressed by teen comedy tropes, and its use of split-screen, split-diopter and slow motion shots tell the story visually rather than through dialogue. As for Lucas's project, De Palma complained in an early viewing of \"Star Wars\" that the opening text crawl was poorly written and volunteered to help edit the text to a more concise and engaging form.\nThe financial and critical success of \"Carrie\" allowed De Palma to pursue more personal material. Alfred Bester's novel \"The Demolished Man\" had fascinated De Palma since the late 1950s and appealed to his background in mathematics and avant-garde storytelling. Its unconventional unfolding of plot (exemplified in its mathematical layout of dialogue) and its stress on perception have analogs in De Palma's filmmaking. He sought to adapt it numerous times, though the project would carry a substantial price tag, and has yet to appear on-screen (Steven Spielberg's 2002 adaptation of Philip K. Dick's \"Minority Report\" bears striking similarities to De Palma's visual style and some of the themes of \"The Demolished Man\"). The result of his experience with adapting \"The Demolished Man\" was the 1978 science fiction psychic thriller \"The Fury\", starring Kirk Douglas, Carrie Snodgress, John Cassavetes and Amy Irving. The film was admired by Jean-Luc Godard, who featured a clip in his mammoth \"Histoire(s) du cin\u00e9ma\", and Pauline Kael, who championed both \"The Fury\" and De Palma. The film boasted a larger budget than \"Carrie\", though the consensus view at the time was that De Palma was repeating himself, with diminishing returns.\n1980\u20131996: Established career.\nThe 1980s were marked by some of De Palma's best known films, including the erotic thriller \"Dressed to Kill\" (1980) starring Michael Caine and Angie Dickinson. Although the film received critical acclaim, it caused controversy for its negative depiction of the transgender community. The following year he directed \"Blow Out\" (1981), a variation on Michelangelo Antonioni's \"Blow-Up\" (1966) and Francis Ford Coppola's \"The Conversation\" (1974). \"Blow Out\" starred John Travolta, Nancy Allen and John Lithgow and received critical acclaim. Kael wrote: \"De Palma has sprung to the place that Robert Altman achieved with films such as \"McCabe &amp; Mrs. Miller\" and \"Nashville\" and that Francis Ford Coppola reached with \"The Godfather\" films\u2014that is, to the place where genre is transcended and what we're moved by is an artist's vision. It's a great movie.\" \nDe Palma directed \"Scarface\" (1983), a remake of Howard Hawks's 1932 film, starring Al Pacino and Michelle Pfeiffer with a screenplay by Oliver Stone. The film received mixed reviews with its negative depictions of ethnic stereotypes, as well as its violence and profanity. It has since been re-evaluated and is now considered a cult classic. The following year he made another erotic thriller, \"Body Double\" (1984), starring Craig Wasson and Melanie Griffith. The film also received mixed reviews but has since had a reassessment and found acclaim. De Palma directed the music video for Bruce Springsteen's single \"Dancing in the Dark\" the same year.\nIn 1987, De Palma directed the crime film \"The Untouchables\", loosely based on the book of the same name and adapted by David Mamet. The film stars Kevin Costner, Andy Garcia, Robert De Niro and Sean Connery, the last of whom won the Academy Award for Best Supporting Actor for the film. It received critical acclaim and box-office success. De Palma's Vietnam War film \"Casualties of War\" (1989) won critical praise but performed poorly in theatres and \"The Bonfire of the Vanities\" (1990) was a notorious failure with both critics and audiences. De Palma then had subsequent successes with \"Raising Cain\" (1992) and \"Carlito's Way\" (1993). \"\" (1996) was his highest-grossing film and started \".\"\n1998\u2013present: Career slump.\nDe Palma's work after \"Mission: Impossible\" has been less well received. His ensuing films \"Snake Eyes\" (1998), \"Mission to Mars\" (2000), and \"Femme Fatale\" (2002) all failed at the box office and received generally poor reviews, though \"Femme Fatale\" has since been revived in the eyes of many film critics and became a cult classic. His 2006 adaptation of \"The Black Dahlia\" was also unsuccessful and is currently the last movie De Palma has directed with backing from Hollywood.\nA political controversy erupted over the portrayal of US soldiers in De Palma's 2007 film \"Redacted\". Loosely based on the 2006 Mahmudiyah killings by American soldiers in Iraq, the film echoes themes that appeared in \"Casualties of War\". \"Redacted\" received a limited release in the United States and grossed less than $1 million against a $5 million budget.\nDe Palma's output has slowed since the release of \"Redacted\", with subsequent projects often falling into development hell, due mostly to creative differences. In 2012, his film \"Passion\" starring Rachel McAdams and Noomi Rapace was selected to compete for the Golden Lion at the 69th Venice International Film Festival but received mixed reviews and was financially unsuccessful.\nDe Palma's next project was the thriller \"Domino\" (2019), released two years after the film began production. It received generally negative reviews and was released direct-to-VOD in the United States, grossing less than half a million dollars internationally. De Palma has also expressed dissatisfaction with both the production of the film and the final result; \"I never experienced such a horrible movie set.\"\nIn 2018, De Palma published his debut novel in France, \"Les serpents sont-ils n\u00e9cessaires?\" (English translation: \"Are Snakes Necessary?\"), co-written with Susan Lehman. It was published in the U.S. in 2020. De Palma and Lehman also wrote a second book, currently unpublished, called \"Terry\", based on one of De Palma's passion projects about a French film production making an adaptation of \"Th\u00e9r\u00e8se Raquin\".\nIt was announced in 2018 that De Palma would write and direct a horror film titled \"Predator\", inspired by the Harvey Weinstein sexual abuse cases, and would direct Wagner Moura in a film titled \"Sweet Vengeance\", based on two real-life murder cases. Filming on the latter was to have begun in early 2019 in Montevideo. In a 2020 interview with the \"Associated Press\", De Palma confirmed that \"Predator\" was retitled \"Catch and Kill\" and added that he was to have started filming in August that same year.\nIn September 2024, De Palma revealed to \"Vulture\" that he had \"one other\" undisclosed film he was planning to make, and that he was in the process of trying to cast it.\nFilmmaking style, techniques and trademarks.\nDe Palma's films can fall into two categories: his thriller films (\"Sisters\", \"Body Double\", \"Obsession\", \"Dressed to Kill\", \"Blow Out\", \"Raising Cain\") and his mainly commercial films (\"The Untouchables\", \"Carlito's Way\", and \"Mission: Impossible\"). He has often produced \"De Palma\" films one after the other before going on to direct a different genre, but would always return to his familiar territory. Because of the subject matter and graphic violence of some of De Palma's films, such as \"Dressed to Kill\", \"Scarface\" and \"Body Double\", they are often at the center of controversy with the Motion Picture Association of America, film critics and the viewing public.\nInspirations.\nDe Palma frequently quotes and refers to other directors' work. His early work was inspired by the films of Jean-Luc Godard. Michelangelo Antonioni's \"Blowup\" and Francis Ford Coppola's \"The Conversation\" plots were used for the basis of \"Blow Out\". \"The Untouchables\"' finale shoot out in the train station is a clear borrowing from the Odessa Steps sequence in Sergei Eisenstein's \"The Battleship Potemkin\". The main plot from \"Rear Window\" was used for \"Body Double\", while it also used elements of \"Vertigo\". \"Vertigo\" was also the basis for \"Obsession\". \"Dressed to Kill\" was a note-for-note homage to Hitchcock's \"Psycho\", including such moments as the surprise death of the lead actress and the exposition scene by the psychiatrist at the end.\nCamera shots.\nFilm critics have often noted De Palma's penchant for unusual camera angles and compositions. He often frames characters against the background using a canted angle shot. Split-screen techniques have been used to show two separate events happening simultaneously. To emphasize the dramatic effect of a certain scene De Palma has employed a 360-degree camera pan. Slow sweeping, panning, and tracking shots are often used throughout his films, often through precisely-choreographed long takes lasting for minutes without cutting. Split focus shots, often referred to as \"di-opt\", are used by De Palma to emphasize the foreground person/object while simultaneously keeping a background person/object in focus. Slow-motion is frequently used in his films to increase suspense.\nPersonal life.\nDe Palma has been married and divorced three times, to actress Nancy Allen (1979\u20131983), producer Gale Anne Hurd (1991\u20131993), and Darnell Gregorio (1995\u20131997). He has one daughter from his marriage to Hurd, and one daughter from his marriage to Gregorio. He resides in Manhattan, New York.\nReception and legacy.\nDe Palma is often cited as a leading member of the New Hollywood generation of film directors, a distinct pedigree who either emerged from film schools or are overtly cine-literate. His contemporaries include Martin Scorsese, Paul Schrader, John Milius, George Lucas, Francis Ford Coppola, Steven Spielberg, John Carpenter, and Ridley Scott. His artistry in directing and use of cinematography and suspense in several of his films has often been compared to the work of Alfred Hitchcock. Psychologists have been intrigued by De Palma's fascination with pathology, by the aberrant behavior aroused in characters who find themselves manipulated by others.\nDe Palma has encouraged and fostered the filmmaking careers of directors such as Mark Romanek and Keith Gordon, the latter of whom collaborated with him twice as an actor, both in 1979's \"Home Movies\" and 1980's \"Dressed to Kill\". Filmmakers influenced by De Palma include Terrence Malick, Quentin Tarantino, Ronny Yu, Don Mancini, Nacho Vigalondo, and Jack Thomas Smith. During an interview with De Palma, Quentin Tarantino said that \"Blow Out\" is one of his all-time favorite films, and that after watching \"Scarface\" he knew how to make his own film. John Travolta's performance as Jack Terry in \"Blow Out\" even resulted in Tarantino casting him as Vincent Vega in his 1994 film \"Pulp Fiction\", which would go on to reinvigorate Travolta's then-declining career. Tarantino also placed \"Carrie\" at number eight in a list of his favorite films.\nCritics who frequently admire De Palma's work include Pauline Kael and Roger Ebert. Kael wrote in her review of \"Blow Out\", \"At forty, Brian De Palma has more than twenty years of moviemaking behind him, and he has been growing better and better. Each time a new film of his opens, everything he has done before seems to have been preparation for it.\" In his review of \"Femme Fatale\", Roger Ebert wrote about the director: \"De Palma deserves more honor as a director. Consider also these titles: \"Sisters\", \"Blow Out\", \"The Fury\", \"Dressed to Kill\", \"Carrie\", \"Scarface\", \"Wise Guys\", \"Casualties of War\", \"Carlito's Way\", \"Mission: Impossible\". Yes, there are a few failures along the way (\"Snake Eyes\", \"Mission to Mars\", \"The Bonfire of the Vanities\"), but look at the range here, and reflect that these movies contain treasure for those who admire the craft as well as the story, who sense the glee with which De Palma manipulates images and characters for the simple joy of being good at it. It's not just that he sometimes works in the style of Hitchcock, but that he has the nerve to.\"\nThe influential French film magazine \"Cahiers du Cin\u00e9ma\" has placed five of De Palma's films (\"Carlito's Way\", \"\", \"Snake Eyes\", \"Mission to Mars\", and \"Redacted\") on their annual top ten list, with \"Redacted\" placing first on the 2008 list. The magazine also listed \"Carlito's Way\" as the greatest film of the 1990s.\nJulie Salamon has written that critics have accused De Palma of being \"a perverse misogynist\", to which De Palma has responded with, \"I'm always attacked for having an erotic, sexist approach\u00a0\u2013 chopping up women, putting women in peril. I'm making suspense movies! What else is going to happen to them?\"\nHis films have also been interpreted as feminist and examined for their perceived queer affinities. In \"Film Comment\"'s \"Queer and Now and Then\" column on \"Femme Fatale\", film critic Michael Koresky writes that \"De Palma's films radiate an undeniable queer energy\" and notes the \"intense appeal\" De Palma's films have for gay critics. In her book \"The Erotic Thriller in Contemporary Cinema\", Linda Ruth Williams writes that \"De Palma understood the cinematic potency of dangerous fucking, perhaps earlier than his feminist detractors\".\nRobin Wood considered \"Sisters\" an overtly feminist film, writing that \"one can define the monster of \"Sisters\" as women's liberation; adding only that the film follows the time-honored horror film tradition of making the monster emerge as the most sympathetic character and its emotional center.\" Pauline Kael's review of \"Casualties of War\", \"A Wounded Apparition\", describes the film as \"feminist\" and notes that \"De Palma was always involved in examining (and sometimes satirizing) victimization, but he was often accused of being a victimizer\". Helen Grace, in a piece for \"Lola\", writes that upon seeing \"Dressed to Kill\" amidst calls for a boycott from feminist groups Women Against Violence Against Women and Women Against Pornography, that the film \"seemed to say more about masculine anxiety than about the fears that women were expressing in relation to the film\". De Palma has also expressed contrition for the depiction of a transgender murderer in the film, saying in a 2016 interview \"I don't know what the transgender community would think [of the film now]... Obviously I realize that it's not good for their image to be transgender and also be a psychopathic murderer. But I think that [perception] passes with time. We're in a different time.\" In the same interview, he said he was \"glad\" that the film had become a \"a favorite of the gay community\".\nDavid Thomson wrote in his entry for De Palma, \"There is a self-conscious cunning in De Palma's work, ready to control everything except his own cruelty and indifference.\" Matt Zoller Seitz objected to this characterisation, writing that there are films from the director which can be seen as \"straightforwardly empathetic and/or moralistic\".\nHis life and career in his own words was the subject of the 2015 documentary \"De Palma,\" directed by Noah Baumbach and Jake Paltrow.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4218", "revid": "29278485", "url": "https://en.wikipedia.org/wiki?curid=4218", "title": "North American B-25 Mitchell", "text": "American WWII medium bomber\nThe North American B-25 Mitchell is an American medium bomber that was introduced in 1941 and named in honor of Brigadier General William \"Billy\" Mitchell, a pioneer of U.S. military aviation. Used by many Allied air forces, the B-25 served in every theater of World War II, and after the war ended, many remained in service, operating across four decades. Produced in numerous variants, nearly 10,000 B-25s were built. It was the most-produced American medium bomber and the third-most-produced American bomber overall. These included several limited models such as the F-10 reconnaissance aircraft, the AT-24 crew trainer, and the United States Marine Corps' PBJ-1 patrol bomber.\nDesign and development.\nOn 11 March 1939, the US Army Air Corps issued Proposal No. 39-640 specifying a medium bomber capable of carrying a bombload over a range of at top speed in excess of . North American Aviation (NAA) used its NA-40B design to develop the NA-62 proposal. More state of the art compared to the competing Martin No. 179 proposal, the North American team included easy field maintenance and repair features, and according to Avery, \"It promised to be an easy airplane to fly and placed no special requirements on pilot training programs.\" On 20 September. the Air Corps issued North American contract No. W353-ac-13258 for 184 B-25s powered by the Wright R-2600. The plane used the NACA 23017 airfoil at the wing root changing to a NACA 4409-R at the wingtip. On 19 August 1940, Vance Breese and NAA test engineer Roy Ferren flew the first flight test, when Ferren noted a severe roll-yaw condition.\nPreliminary flights by the Air Corps noted the Dutch roll characteristic, accentuated by wind and gusts and demanded a fix. NAA's first nine aircraft had a constant-dihedral, the wing having a consistent upward angle from the fuselage to the wingtip. \"Flattening\", or changing the outer wing panels dihedral to zero degrees, was a simple solution that solved the aerodynamic problem. This gave the B-25 its gull wing configuration. The 25 February 1941 flight test confirmed the change resulted in optimum flight characteristics. The vertical tail also went through five variations before being finalized. By the time of the Attack on Pearl Harbor, 130 B-25s had been delivered.\nSpecial variations were made to accommodate photo reconnaissance, armament, long range ferry, anti-submarine patrol, winterizing, and use in a desert environment. By February 1941, the first 24 B-25s were configured with three .30 cal guns and a single .50 cal tail gun. The B-25A had self-sealing fuel cells. The B-25B had top and bottom turrets with twin .50 cal guns each, though the tail gun was removed. By December 1941, the B-25C had additional self-sealing fuel cells outboard the wing center section. By February 1942, the first B-25D, and then in May 1943, the 75mm cannon-armed B-25G series were accepted by the Air Corps. By August 1943, the B-25H had a lighter 75mm cannon, 4 nose guns instead of 2, two waist guns. two in the tail turret, four blister gun packs, and eliminated the co-pilot after Jimmy Doolittle questioned the need. In December 1943, the B-25J was introduced, the final variant and the most produced, reincorporated the co-pilot position and included a bombardier.\nNAA manufactured the greatest number of aircraft in World War II, the first time a company had produced trainers, bombers, and fighters simultaneously (the AT-6/SNJ Texan/Harvard, B-25 Mitchell, and the P-51 Mustang). It produced B-25s at both its Inglewood main plant and an additional 6,608 aircraft at its Kansas City, Kansas, plant at Fairfax Airport.\nAfter the war, the USAF placed a contract for the TB-25L trainer in 1952. This was a modification program by Hayes of Birmingham, Alabama. Its primary role was reciprocating engine pilot training.\nA development of the B-25 was the North American XB-28 Dragon, designed as a high-altitude bomber. Two prototypes were built with the second prototype, the XB-28A, evaluated as a photo-reconnaissance platform, but the aircraft did not enter production.\nFlight characteristics.\nThe B-25 was a safe and forgiving aircraft to fly. With one engine out, 60\u00b0 banking turns into the dead engine were possible, and control could be easily maintained down to 145\u00a0mph (230\u00a0km/h). The pilot had to remember to maintain engine-out directional control at low speeds after takeoff with rudder; if this maneuver were attempted with ailerons, the aircraft could snap out of control. The tricycle landing gear made for excellent visibility while taxiing. The only significant complaint about the B-25 was its extremely noisy engines; as a result, many pilots eventually suffered from some degree of hearing loss. A Clayton S stack, introduced to quench the exhaust flame, was introduced in the B-25C series. These stacks protruded through the cowling, and though they weighed less than the replaced collector ring, they reduced aircraft speed by 9 mph due to the required aircraft fairings. According to Avery, \"The increase in noise as compared to collector rings ported on the outboard side of the nacelles was a general crew complaint.\"\nDurability.\nThe Mitchell was exceptionally sturdy and could withstand tremendous punishment. One B-25C of the 321st Bomb Group was nicknamed \"Patches\" because its crew chief painted all the aircraft's flak hole patches with bright yellow zinc chromate primer. By the end of the war, this aircraft had completed over 300 missions, had been belly-landed six times, and had over 400 patched holes. The airframe of \"Patches\" was so distorted from battle damage that straight-and-level flight required 8\u00b0 of left aileron trim and 6\u00b0 of right rudder, causing the aircraft to \"crab\" sideways across the sky.\nOperational history.\nAsia-Pacific.\nMost B-25s in American service were used in the war against Japan in Asia and the Pacific. The Mitchell fought from the North to the South Pacific and the Far East. These areas included the campaigns in the Aleutian Islands, Papua New Guinea, the Solomon Islands, New Britain, China, Burma, and the island hopping campaign in the Central Pacific, as well as in the Doolittle Raid. The aircraft's potential as a ground-attack aircraft emerged during the Pacific war. The jungle environment reduced the usefulness of medium-level bombing, and made low-level attack the best tactic. Using similar mast height level tactics and skip bombing, the B-25 proved itself to be a capable anti-shipping weapon and sank many enemy sea vessels. An ever-increasing number of forward firing guns made the B-25 a formidable strafing aircraft for island warfare. The Paul Gunn and Jack Fox modified strafer models with four .50 caliber guns were the B-25C1/D1, while the factory B-25J was equipped with a factory made eight gun strafer nose.\nIn Burma, bridge busting was a primary target of the Tenth Air Force 341st Bomb Group operating B-25C and D airplanes. A glide and skip technique, called \"glip\" bombing, was most the effective for the Burma Bridge Busters. The 341st ranged as far as the Formosa Strait, the East China coast and French Indochina.\nMiddle East and Italy.\nThe first B-25s arrived in Egypt and were carrying out independent operations by October 1942. Operations there against Axis airfields and motorized-vehicle columns supported the ground actions of the Second Battle of El Alamein. Thereafter, the aircraft took part in the rest of the campaign in North Africa, the invasion of Sicily, and the advance up Italy. In the Strait of Messina to the Aegean Sea, the B-25 conducted sea sweeps as part of the coastal air forces. In Italy, the B-25 was used in the ground attack role, concentrating on attacks against road and rail links in Italy, Austria, and the Balkans. The B-25 had a longer range than the Douglas A-20 Havoc and Douglas A-26 Invader, allowing it to reach further into occupied Europe. The five bombardment groups \u2013 20 squadrons \u2013 of the Ninth and Twelfth Air Forces that used the B-25 in the Mediterranean Theater of Operations were the only U.S. units to employ the B-25 in Europe.\nEurope.\nIn October 1943, the Ninth Air Force 340th was transferred from the African and Mediterranean theater to England in support of the assault on Germany. In November 1944 the medium bombers eliminated the use of electric locomotives along Brenner Pass.\nUse as a gunship.\nIn antishipping operations, the USAAF had an urgent need for hard-hitting aircraft, and North American responded with the B-25G. In this series, the transparent nose and bombardier/navigator position was changed for a shorter, hatched nose with two fixed .50\u00a0in (12.7\u00a0mm) machine guns and a manually loaded 75\u00a0mm (2.95\u00a0in) M4 cannon.\nThe B-25H series continued the development of the gunship version. NAA Inglewood produced 1000. The H had even more firepower; most replaced the M4 gun with the lighter T13E1, designed specifically for the aircraft, but 20-odd H-1 block aircraft completed by the Republic Aviation modification center at Evansville had the M4 and two-machine-gun nose armament. The 75\u00a0mm (2.95\u00a0in) gun fired at a muzzle velocity of . Due to its slow rate of fire (about four rounds could be fired in a single strafing run), relative ineffectiveness against ground targets, and the substantial recoil, the 75\u00a0mm gun was sometimes removed from both G and H models and replaced with two additional .50\u00a0in (12.7\u00a0mm) machine guns as a field modification.\nThe H series normally came from the factory mounting four fixed, forward-firing .50\u00a0in (12.7\u00a0mm) machine guns in the nose; four in a pair of under-cockpit conformal flank-mount gun pod packages (two guns per side); two more in the manned dorsal turret, relocated forward to a position just behind the cockpit (which became standard for the J-model); one each in a pair of new waist positions, introduced simultaneously with the forward-relocated dorsal turret; and lastly, a pair of guns in a new tail-gunner's position. Company promotional material bragged that the B-25H could \"bring to bear 10 machine guns coming and four going, in addition to the 75\u00a0mm cannon, eight rockets, and 3,000\u00a0lb (1,360\u00a0kg) of bombs.\"\nThe H had a modified cockpit with single flight controls operated by the pilot. The co-pilot's station and controls were removed and replaced by a smaller seat used by the navigator/cannoneer, The radio operator crew position was aft of the bomb bay with access to the waist guns. Factory production totals were 405 B-25Gs and 1,000 B-25Hs, with 248 of the latter being used by the Navy as PBJ-1Hs. Elimination of the co-pilot saved weight, and moving the dorsal turret forward partially counterbalanced the waist guns and the manned rear turret.\nReturn to medium bomber.\nThe final, and most numerous, series of the Mitchell, the B-25J, looked less like earlier series apart from the well-glazed bombardier's nose of nearly identical appearance to the earliest B-25 subtypes. Instead, the J followed the overall configuration of the H series from the cockpit aft. It had the forward dorsal turret and other armament and airframe advancements. All J models included four .50\u00a0in (12.7\u00a0mm) light-barrel Browning AN/M2 guns in a pair of \"fuselage packages\", conformal gun pods each flanking the lower cockpit, each pod containing two Browning M2s. By 1945, however, combat squadrons removed these. The J series restored the co-pilot's seat and dual flight controls. The factory-made kits available to the Air Depot system to create the strafer-nose B-25J-2. This configuration carried a total of 18 .50\u00a0in (12.7\u00a0mm) light-barrel AN/M2 Browning M2 machine guns: eight in the nose, four in the flank-mount conformal gun pod packages, two in the dorsal turret, one each in the pair of waist positions, and a pair in the tail \u2013 with 14 of the guns either aimed directly forward or aimed to fire directly forward for strafing missions. Some aircraft had eight 5-inch (130\u00a0mm) high-velocity aircraft rockets.\nPostwar (USAF) use.\nIn 1947, legislation created an independent United States Air Force (USAF) and by that time, the B-25 inventory numbered only a few hundred. Some B-25s continued in service into the 1950s in training, reconnaissance, and support roles. The principal use during this period was as pilot trainers, radar control trainers, weather reconnaissance, and transports. Others were assigned to units of the Air National Guard in training roles in support of Northrop F-89 Scorpion and Lockheed F-94 Starfire operations.\nDuring its USAF tenure, many B-25s received the so-called \"Hayes modification\" and as a result, surviving B-25s often have exhaust systems with a semicollector ring that splits emissions into two different systems. The upper seven cylinders are collected by a ring, while the other cylinders remain directed to individual ports.\nTB-25J-25-NC Mitchell, \"44-30854\", the last B-25 in the USAF inventory, assigned at March AFB, California, as of March 1960, was flown to Eglin AFB, Florida, from Turner Air Force Base, Georgia, on 21 May 1960, the last flight by a USAF B-25. It was presented by Brigadier General A. J. Russell, Commander of SAC's 822d Air Division at Turner AFB, to the Air Proving Ground Center Commander, Brigadier General Robert H. Warren. He in turn presented the bomber to Valparaiso, Florida, Mayor Randall Roberts on behalf of the Niceville-Valparaiso Chamber of Commerce. Four of the original Tokyo Raiders were present for the ceremony, Colonel (later Major General) David Jones, Colonel Jack Simms, Lieutenant Colonel Joseph Manske, and retired Master Sergeant Edwin W. Horton. It was donated back to the Air Force Armament Museum around 1974 and marked as Doolittle's \"40-2344\".\nU.S. Navy and USMC.\nThe U.S. Navy designation for the Mitchell B-25 was the PBJ-1, similarly the PBJ-1C and PBJ-1D reflected their AAF counterparts. Night search PBJs incorporated a retractable APS-3 radome scope. Under the pre-1962 USN/USMC/USCG aircraft designation system, PBJ-1 stood for Patrol (P) Bomber (B) built by North American Aviation (J), first variant (-1) under the existing American naval aircraft designation system of the era. In early 1943, the Navy took delivery of an initial 706 B-25s, assigned to the Marine Corps for patrol and anti-submarine duties initially, but then transitioning into an attack aircraft with bombs, torpedoes and radar directed rockets. The PBJ had its origin in an inter-service agreement of mid-1942 between the Navy and the USAAF exchanging the Boeing Renton plant for the Kansas plant for B-29 Superfortress production. The Boeing XPBB Sea Ranger flying boat, competing for B-29 engines, was cancelled in exchange for part of the Kansas City Mitchell production. On 1 March 1943, VMB-413 was the first of sixteen USMC squadrons equipped with PBJs, all commissioned at MCAS Cherry Point. The large quantities of B-25H and J series became known as PBJ-1H and PBJ-1J, respectively.\nFrom 1944 onwards, the Marine PBJs flew from the Philippines, Saipan, Iwo Jima, and Okinawa. Their primary mission was radar directed night strikes against enemy shipping. Weapons included the five-inch HVAR rocket, and the 11.75 inch \"Tiny Tim\" rocket. Long range night operations meant more fuel, with weight reductions achieved removing the top turret and slide blisters.\nDuring the war, the Navy tested the cannon-armed G series and conducted carrier trials with an H equipped with arresting gear. After World War II, some PBJs stationed at the Navy's rocket laboratory in Inyokern, California, site of the present-day Naval Air Weapons Station China Lake, tested air-to-ground rockets and arrangements. One arrangement was a twin-barrel nose that could fire 10 spin-stabilized five-inch rockets in one salvo.\nRoyal Air Force.\nGreat Britain received 910 B-25s during WWII, but many were returned afterwards.\nThe Royal Air Force (RAF) was an early customer for the B-25 via Lend-Lease. The first Mitchells were given the service name Mitchell I by the RAF and were delivered in August 1941, to No. 111 Operational Training Unit based in the Bahamas. These bombers were used exclusively for training and familiarization and never became operational. The B-25Cs and Ds were designated Mitchell II. Altogether, 167 B-25Cs and 371 B-25Ds were delivered to the RAF. The RAF tested the cannon-armed G series but did not adopt the series nor the follow-on H series.\nBy the end of 1942, the RAF had taken delivery of 93 Mitchells, marks I and II. Some served with squadrons of No. 2 Group RAF, the RAF's tactical medium-bomber force, including No. 139 Wing RAF at RAF Dunsfold. The first RAF operation with the Mitchell II took place on 22 January 1943, when six aircraft from No. 180 Squadron RAF attacked oil installations at Ghent. After the invasion of Europe (by which point 2 Group was part of Second Tactical Air Force), all four Mitchell squadrons moved to bases in France and Belgium (Melsbroek) to support Allied ground forces. The British Mitchell squadrons were joined by No. 342 (Lorraine) Squadron of the French Air Force in April 1945.\nAs part of its move from Bomber Command, No 305 (Polish) Squadron flew Mitchell IIs from September to December 1943 before converting to the de Havilland Mosquito. In addition to No. 2 Group, the B-25 was used by various second-line RAF units in the UK and abroad. In the Far East, No. 3 PRU, which consisted of Nos. 681 and 684 Squadrons, flew the Mitchell (primarily Mk IIs) on photographic reconnaissance sorties.\nRoyal Canadian Air Force.\nThe Royal Canadian Air Force (RCAF) used the B-25 Mitchell for training during the war. Postwar use continued operations with most of the 162 Mitchells received. The first B-25s had been diverted to Canada from RAF orders. These included one Mitchell I, 42 Mitchell IIs, and 19 Mitchell IIIs. No 13 (P) Squadron was formed unofficially at RCAF Rockcliffe in May 1944 and used Mitchell IIs on high-altitude aerial photography sorties. No. 5 Operational Training Unit at Boundary Bay, British Columbia and Abbotsford, British Columbia, operated the B-25D Mitchell in the training role together with B-24 Liberators for Heavy Conversion as part of the BCATP. The RCAF retained the Mitchell until October 1963.\nNo 418 (Auxiliary) Squadron received its first Mitchell IIs in January 1947. It was followed by No 406 (auxiliary), which flew Mitchell IIs and IIIs from April 1947 to June 1958. No 418 operated a mix of IIs and IIIs until March 1958. No 12 Squadron of Air Transport Command also flew Mitchell IIIs along with other types from September 1956 to November 1960. In 1951, the RCAF received an additional 75 B-25Js from USAF stocks to make up for attrition and to equip various second-line units.\nRoyal Australian Air Force.\nThe Australians received Mitchells by the spring of 1944. The joint Australian-Dutch No. 18 (Netherlands East Indies) Squadron RAAF had more than enough Mitchells for one squadron, so the surplus went to re-equip the RAAF's No. 2 Squadron, replacing their Beauforts.\nDutch Air Force.\nDuring World War II, the Mitchell served in fairly large numbers with the Air Force of the Dutch government-in-exile. They participated in combat in the East Indies, as well as on the European front. On 30 June 1941, the Netherlands Purchasing Commission, acting on behalf of the Dutch government-in-exile in London, signed a contract with North American Aviation for 162 B-25C aircraft. The bombers were to be delivered to the Netherlands East Indies to help deter any Japanese threatened expansion into the region.\nIn February 1942, the British Overseas Airways Corporation agreed to ferry 20 Dutch B-25s from Florida to Australia travelling via Africa and India, and an additional 10 via the South Pacific route from California. During March, five of the bombers on the Dutch order had reached Bangalore, India, and 12 had reached Archerfield in Australia. The B-25s in Australia were used as the nucleus of a new squadron, No. 18 (Netherlands East Indies) Squadron RAAF.\nIn June 1940, No. 320 (Netherlands) Squadron RAF had been formed from personnel formerly serving with the Royal Dutch Naval Air Service, who had escaped to England after the German occupation of the Netherlands. Equipped with various British aircraft, No. 320 Squadron flew antisubmarine patrols, convoy escort missions, and performed air-sea rescue duties. In March 1943, they acquired the B-25 Mark II nd III Mitchells. In October 1944, they deployed to Belgium, but then disbanded in August 1945.\nSoviet Air Force.\nThe USSR received 862 B-25s (B, C, D, G, and J types) from the United States under Lend-Lease during World War II via the Alaska\u2013Siberia ALSIB ferry route. A total of 870 B-25s were sent to the Soviets, meaning that 8 aircraft were lost during transportation.\nOther damaged B-25s arrived or crashed in the Far East of Russia, and one Doolittle Raid aircraft landed there short of fuel after attacking Japan. This lone airworthy Doolittle Raid aircraft to reach the Soviet Union was lost in a hangar fire in the early 1950s while undergoing routine maintenance. In general, the B-25 was operated as a ground-support and tactical day bomber (as similar Douglas A-20 Havocs were used). It saw action in fights from Stalingrad (with B/C/D models) to the German surrender during May 1945 (with G/J types).\nThe B-25s that remained in Soviet Air Force service after the war were assigned the NATO reporting name \"Bank\".\nChina.\nWell over 100 B-25Cs and Ds were supplied to the Nationalist Chinese during the Second Sino-Japanese War. An unknown number were abandoned with the retreat to Formosa.\nBrazilian Air Force.\nDuring the war and after WWII, Brazil received 80 B-25s, with the first delivery prior to December 1941.\nFree French.\nThe Royal Air Force issued at least 21 Mitchell IIIs to No 342 Squadron, which was made up primarily of Free French aircrews. Following the liberation of France, this squadron transferred to the newly formed French Air Force (\"Arm\u00e9e de l'Air\") as GB I/20 Lorraine. The aircraft continued in operation after the war, with some being converted into fast VIP transports. They were struck off charge in June 1947.\nBiafra.\nIn October 1967, during the Nigerian Civil War, Biafra bought two Mitchells. After a few bombings in November, they were put out of action in December.\nIndonesia.\nIndonesian Air Force received 25 ex-Dutch B-25 Mitchells after the end of Indonesian National Revolution in 1950, consisting of 5 B-25C photo-reconnaissance, 1 B-25C transport, 10 B-25J bombers and 9 B-25J gunship/strafer variants. A pair of B-25J were used to attack a radio station in Ambon during South Maluku rebellion in August 1950. They were used to bomb rebel targets during the PRRI and Permesta rebellions in 1958, where one was hit by anti-aircraft fire and three were damaged by strafing run from rebel-flown B-26 Invader. \nTo extend its service life, the B-25s were sent to Hong Kong for major overhaul in 1959\u20131960. Indonesian B-25s once again saw combat during the Operation Trikora against the Dutch in 1962, where one was used for strafing runs against a Dutch warship, while two others were used in Maluku. The last Indonesian B-25s were retired in 1974.\nThe initial production version of B-25s, they were powered by R-2600-9 engines. and carried up to 3,600\u00a0lb (1,600\u00a0kg) of bombs and defensive armament of three .30 machine guns in nose, waist, and ventral positions, with one .50 machine gun in the tail. The first nine aircraft were built with constant dihedral angle. Due to low stability, the wing was redesigned so that the dihedral was eliminated on the outboard section (number made: 24).\nThis version of the B-25 was modified to make it combat ready; additions included self-sealing fuel tanks, crew armor, and an improved tail-gunner station. No changes were made in the armament. It was redesignated obsolete (\"RB-25A\") in 1942 (number made: 40).\nThe tail and gun position were removed and replaced by a manned dorsal turret on the rear fuselage and retractable, remotely operated ventral turret, each with a pair of .50\u00a0in (12.7\u00a0mm) machine guns. A total of 120 were built (this version was used in the Doolittle Raid). A total of 23 were supplied to the Royal Air Force as the Mitchell Mk I.\nAn improved version of the B-25B, its powerplants were upgraded from Wright R-2600-9 radials to R-2600-13s; de-icing and anti-icing equipment were added; the navigator received a sighting blister; and nose armament was increased to two .50\u00a0in (12.7\u00a0mm) machine guns, one fixed and one flexible. The B-25C model was the first mass-produced B-25 version; it was also used in the United Kingdom (as the Mitchell Mk II), in Canada, China, the Netherlands, and the Soviet Union (number made: 1,625).\nThrough block 20, the series was near identical to the B-25C. The series designation differed in that the B-25D was made in Kansas City, Kansas, whereas the B-25C was made in Inglewood, California. Later blocks with interim armament upgrades, the D2s, first flew on 3 January 1942 (number made: 2,290).\nThe F-10 designation distinguished 45 B-25Ds modified for photographic reconnaissance. All armament, armor, and bombing equipment were stripped. Three K.17 cameras were installed, one pointing down and two more mounted at oblique angles within blisters on each side of the nose. Optionally, a second downward-pointing camera could also be installed in the aft fuselage. Although designed for combat operations, these aircraft were mainly used for ground mapping.\nIn 1944, four B-25Ds were converted for weather reconnaissance. One later user was the 53d Weather Reconnaissance Squadron, originally called the Army Hurricane Reconnaissance Unit, now called the \"Hurricane Hunters\". Weather reconnaissance first started in 1943 with the 1st Weather Reconnaissance Squadron, with flights on the North Atlantic ferry routes.\nA single B-25C was modified to test de-icing and anti-icing equipment that circulated exhaust from the engines in chambers in the leading and trailing edges and empennage. The aircraft was tested for almost two years, beginning in 1942; while the system proved extremely effective, no production models were built that used it before the end of World War II. Many surviving warbird-flown B-25 aircraft today use the de-icing system from the XB-25E (number made: 1, converted).\nA modified B-25C, it used insulated electrical coils mounted inside the wing and empennage leading edges to test the effectiveness as a de-icing system. The hot air de-icing system tested on the XB-25E was determined to be the more practical of the two (number made: 1, converted).\nThis modified B-25C had the transparent nose replaced to create a short-nosed gunship carrying two fixed .50\u00a0in (12.7\u00a0mm) machine guns and a 75\u00a0mm (2.95\u00a0in) M4 cannon, then the largest weapon ever carried on an American bomber (number made: 1, converted).\nThe B-25G followed the success of the prototype XB-25G and production was a continuation of the NA96. The production model featured increased armor and a greater fuel supply than the XB-25G. One B-25G was passed to the British, who gave it the name Mitchell II that had been used for the B-25C. The USSR also tested the G (number made: 463; five converted Cs, 58 modified Cs, 400 production).\nAn improved version of the B-25G, this version relocated the manned dorsal turret to a more forward location on the fuselage just aft of the flight deck. It also featured two additional fixed .50\u00a0in (12.7\u00a0mm) machine guns in the nose and in the H-5 onward, four in fuselage-mounted pods. The T13E1 light weight cannon replaced the heavy M4 cannon 75\u00a0mm (2.95\u00a0in). Single controls were installed from the factory with navigator in the right seat (number made: 1000; two airworthy as of 2015[ [update]]).\n Follow-on production at Kansas City, the B-25J could be called a cross between the B-25D and the B-25H. It had a transparent nose, but many of the delivered aircraft were modified to have a strafer nose (J2). Most of its 14\u201318 machine guns were forward-facing for strafing missions, including the two guns of the forward-located dorsal turret. The RAF received 316 aircraft, which were known as the Mitchell III. The J series was the last factory series production of the B-25 (number made: 4,318).\nUtility transport version\nA number of B-25s were converted for use as staff and VIP transports. Henry H. Arnold and Dwight D. Eisenhower both used converted B-25Js as their personal transports. The last VB-25J in active service was retired in May 1960 at the Eglin Air Force Base in Florida.\nVariants.\nTrainer variants.\nMost models of the B-25 were used at some point as training aircraft.\nOriginally designated AT-24A (Advanced Trainer, Model 24, Version A), trainer modification of B-25D often with the dorsal turret omitted, in total, 60 AT-24s were built.\nOriginally designated AT-24B, trainer modification of B-25G\nOriginally designated AT-24C, trainer modification of B-25C\nOriginally designated AT-24D, trainer modification of B-25J, another 600 B-25Js were modified after the war.\nHughes E1 fire-control radar trainer (Hughes) (number made: 117)\nHayes pilot-trainer conversion (number made: 90)\nHughes E5 fire-control radar trainer (number made: 40)\nHayes navigator-trainer conversion (number made: 47)\nSimilar to the B-25C for the U.S. Navy, it was often fitted with airborne search radar and used in the antisubmarine role.\nSimilar to the B-25D for the U.S. Navy and U.S. Marine Corps, it differed in having a single .50\u00a0in (12.7\u00a0mm) machine gun in the tail turret and waist gun positions similar to the B-25H. Often it was fitted with airborne search radar and used in the antisubmarine role.\nU.S. Navy/U.S. Marine Corps designation for the B-25G, trials only\nU.S. Navy/U.S. Marine Corps designation for the B-25H\nOne PBJ-1H was modified with carrier takeoff and landing equipment and successfully tested on the USS \"Shangri-La\", but the Navy did not continue development.\nU.S. Navy designation for the B-25J (Blocks \u22121 through \u221235), it had improvements in radio and other equipment. Beside the standard armament package, the Marines often fitted it with 5-inch underwing rockets and search radar for the antishipping/antisubmarine role. The 11.75 inch Tiny Tim rocket-powered warhead was used in 1945 on PBJ-1H.\n see B-25 Mitchell units of the United States Army Air Forces\nAccidents and incidents.\nTraining mission incident.\nOn 1 November 1941, a B-25 on a training mission flying out of Wright-Patterson Air Force Base, crashed near Benton Ridge, Ohio.\nWest Chester B-25 crash.\nOn 7 May 1944, a B-25C crashed and exploded around north of West Chester, Pennsylvania, killing all seven military passengers and crew members on board. Caught in stormy weather, the plane nose-dived into the woods at Oaklands Cemetery and burst into flames.\nEmpire State Building crash.\nAt 9:40 on 28 July 1945, a USAAF B-25D crashed in thick fog into the north side of the Empire State Building between the 79th and 80th floors. Fourteen people died\u00a0\u2014 11 in the then world\u2019s tallest building and the three occupants of the aircraft, including the pilot, Colonel William F. Smith. Betty Lou Oliver, an elevator attendant, survived the impact and the subsequent fall of the elevator cage 75 stories to the basement.\nGeneral Leclerc's aviation accident.\nFrench general Philippe Leclerc was aboard his North American B-25 Mitchell, \"Tailly II\", when it crashed near Colomb-B\u00e9char in French Algeria on 28 November 1947, killing everyone on board.\nLake Erie skydiving disaster.\nA bit after 16:00 on 27 August 1967, a converted civilian B-25 mistakenly dropped eighteen skydivers over Lake Erie, four or five nautical miles (7.5\u20139.3\u00a0km) from Huron, Ohio. The air traffic controller had confused the B-25 with a Cessna 180 Skywagon that was trailing it to take photographs, causing the B-25 pilot to think he was over the intended drop site at Ortner Airport. Sixteen of the jumpers drowned, while two were rescued. A National Transportation Safety Board report faulted the pilot, and to a lesser extent the skydivers, for executing a jump when they could not see the ground, and faulted the controller for the misidentification. The United States was subsequently held liable for the controller's negligence.\nSurviving aircraft.\nMany B-25s are currently kept in airworthy condition by air museums and collectors.\nSpecifications (B-25H).\n\"Data from\" United States Military Aircraft since 1909General characteristics* Crew: 5 (one pilot, navigator/bombardier, turret gunner/engineer, radio operator/waist gunner, tail gunner)* Aspect ratio: * Airfoil: root: NACA 23017; tip: NACA 4409R* Propellers: -bladed\nPerformance* Endurance: * g limits: * Roll rate: \nArmament\n * Rockets: racks for eight high velocity aircraft rockets (HVAR)* Bombs: 3,000\u00a0lb (1,360\u00a0kg) bombs\nSee also.\nRelated development\nAircraft of comparable role, configuration, and era\nRelated lists\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4219", "revid": "14493457", "url": "https://en.wikipedia.org/wiki?curid=4219", "title": "British Open (disambiguation)", "text": "The British Open often refers to the Open Championship men's golf tournament.\nBritish Open may also refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "4221", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=4221", "title": "B-25", "text": ""}
{"id": "4222", "revid": "15902509", "url": "https://en.wikipedia.org/wiki?curid=4222", "title": "Bernthia Perkins", "text": ""}
{"id": "4224", "revid": "39361517", "url": "https://en.wikipedia.org/wiki?curid=4224", "title": "Bobby Charlton", "text": "English footballer and manager (1937\u20132023)\nSir Robert Charlton (11 October 1937 \u2013 21 October 2023) was an English professional footballer who played as an attacking midfielder, left winger or centre-forward. Widely considered one of the greatest players of all time, he was a member of the England team that won the 1966 FIFA World Cup, the year he also won the Ballon d'Or. He finished second in the Ballon d'Or voting in 1967 and 1968. He played almost all of his club football at Manchester United, where he became renowned for his attacking instincts, passing abilities from midfield, ferocious long-range shooting from both left and right foot, fitness, and stamina. He was cautioned only twice in his career: once against Argentina in the 1966 World Cup, and once in a league match against Chelsea. With success at club and international level, he was one of ten players to have won the FIFA World Cup, the European Cup and the Ballon d'Or. His elder brother Jack, who was also in the World Cup\u2013winning team, was a defender for Leeds United and also for ten years was the manager of the Republic of Ireland.\nBorn in Ashington, Northumberland, Charlton made his debut for the Manchester United first-team in 1956, aged 18, and soon gained a regular place in the team, during which time he became a Football League First Division champion in 1957 then survived the Munich air disaster of February 1958 after being rescued by teammate Harry Gregg; Charlton was the last survivor of the plane crash from the club. After helping United to win the FA Cup in 1963 and the Football League in 1965 and 1967, he captained the team that won the European Cup in 1968, scoring two goals in the final to help them become the first English club to win the competition. Charlton left Manchester United to become manager of Preston North End for the 1973\u201374 season. He changed to player-manager the following season. He next accepted a post as a director with Wigan Athletic, then became a member of Manchester United's board of directors in 1984.\nAt international level, Charlton was named in the England squad for four World Cups (1958, 1962, 1966, and 1970), though he did not play in the first. At the time of his retirement from the England team in 1970, he was the nation's most capped player, having turned out 106 times at the highest level; Bobby Moore overtook this in 1973. Charlton was the long-time record goalscorer for both Manchester United and England, and United's long-time record appearance maker \u2013 his total of 758 matches for United took until 2008 to be beaten, when Ryan Giggs did so in that year's Champions League final. With 249 goals, he was the club's highest all-time goalscorer for more than 40 years, until his record was surpassed by Wayne Rooney in 2017. He is also the third-highest goalscorer for England; his record of 49 goals was beaten in 2015 by Rooney, and again by Harry Kane in 2022.\nEarly life.\nRobert Charlton was born on 11 October 1937 in Ashington, Northumberland, England, to coal miner Robert \"Bob\" Charlton (24 May 1909 \u2013 April 1982) and Elizabeth Ellen \"Cissie\" Charlton (\"n\u00e9e\" Milburn; 11 November 1912 \u2013 25 March 1996). He was related to several professional footballers on his mother's side of the family: his uncles were Jack Milburn (Leeds United and Bradford City), George Milburn (Leeds United and Chesterfield), Jim Milburn (Leeds United and Bradford Park Avenue) and Stan Milburn (Chesterfield, Leicester City and Rochdale), and legendary Newcastle United and England footballer Jackie Milburn was his mother's cousin. However, Charlton credited much of the early development of his career to his grandfather Tanner and his mother Cissie. His elder brother, Jack, initially worked as a miner before applying to the police, only to also become a professional footballer with Leeds United.\nClub career.\nOn 9 February 1953, then a Bedlington Grammar School pupil, Charlton was spotted playing for East Northumberland schools by Manchester United chief scout Joe Armstrong. Charlton went on to play for England Schoolboys and the 15-year-old signed amateur forms with United on 1 January 1953 along with Wilf McGuinness, also aged 15. Initially his mother was reluctant to let him commit to an insecure football career, so he began an apprenticeship as an electrical engineer; however, he went on to turn professional in October 1954.\nCharlton became one of the famed Busby Babes, the collection of talented footballers who emerged through the system at Old Trafford in the 1940s, 1950s and 1960s as Matt Busby set about a long-term plan of rebuilding the club after the Second World War. He worked his way through the pecking order of teams, scoring regularly for the youth and reserve sides before he was handed his first team debut against Charlton Athletic in October 1956 where he scored two goals in a game that finished 4\u20132. At the same time, he was doing his National service with the Royal Army Ordnance Corps in Shrewsbury, where Busby had advised him to apply as it meant he could still play for Manchester United at the weekend. Also doing his army service in Shrewsbury at the same time was his United teammate Duncan Edwards.\nCharlton played 17 times for United in that first season, scoring twice on his debut and managing a total of 12 goals in all competitions, and including a hat-trick in a 5\u20131 away win over Charlton Athletic in February. United won the league championship but were denied the 20th century's first \"double\" when they controversially lost the 1957 FA Cup Final to Aston Villa. Charlton, still only 19, was selected for the game, which saw United goalkeeper Ray Wood carried off with a broken cheekbone after a clash with Villa centre forward Peter McParland. Charlton was a candidate to go in goal to replace Wood (in the days before substitutes, and certainly before goalkeeping substitutes), but it was teammate Jackie Blanchflower who ended up playing in goal.\nCharlton was an established player by the time the next season was fully underway, which saw United, as current League champions, become the first English team to compete in the European Cup. Previously, the Football Association had scorned the competition, but United made progress, reaching the semi-finals where they lost to holders Real Madrid. Their reputation was further enhanced the next season in the 1957\u201358 European Cup as they reached the quarter-finals to play Red Star Belgrade. In the first leg at home, United won 2\u20131. The return in Yugoslavia saw Charlton score twice as United stormed 3\u20130 ahead, although the hosts came back to earn a 3\u20133 draw. However, United maintained their aggregate lead to reach the last four and were in jubilant mood as they left to catch their flight home, thinking of an important League game against Wolves at the weekend.\n1958 Munich air disaster.\nOn 6 February 1958, Charlton was returning to England with the Manchester United Team after a European Cup match in Belgrade, Yugoslavia (now Serbia), having eliminated Red Star Belgrade to advance to the semi-finals of the competition. The aeroplane which took the United players and staff home from Zemun Airport needed to stop in Munich to refuel. This was carried out in worsening weather, and by the time the refuelling was complete and the call was made for the passengers to re-board the aircraft, the wintry showers had taken hold and snow had settled heavily on the runway and around the airport. There were two aborted take-offs which led to concern on board, and the passengers were advised by a stewardess to disembark again while a minor technical error was fixed.\nThe team were back in the airport terminal for barely ten minutes when the call came to reconvene on the plane, and a number of passengers began to feel nervous. Charlton and teammate Dennis Viollet swapped places with Tommy Taylor and David Pegg, who had decided they would be safer at the back of the plane.\nThe plane clipped the fence at the end of the runway on its next take-off attempt and a wing tore through a nearby house, setting it alight. The wing and part of the tail came off and hit a tree and a wooden hut, the plane spinning along the snow until coming to a halt. It had been cut in half.\nCharlton, strapped into his seat, had fallen out of the cabin; when United goalkeeper Harry Gregg (who had somehow got through a hole in the plane unscathed and begun a one-man rescue mission) found him, he thought he was dead. Nevertheless, he grabbed both Charlton and Viollet by their trouser waistbands and dragged them away from the plane, in constant fear that it would explode. Gregg returned to the plane to try to help the appallingly injured Busby and Blanchflower, and when he turned around again, he was relieved to see that Charlton and Viollet, both of whom he had presumed to be dead, had got out of their detached seats and were looking into the wreckage.\nCharlton suffered cuts to his head and severe shock, and was in hospital for a week. Seven of his teammates had perished at the scene, including Taylor and Pegg, with whom he and Viollet had swapped seats prior to the fatal take-off attempt. Club captain Roger Byrne was also killed, along with Mark Jones, Billy Whelan, Eddie Colman and Geoff Bent. Duncan Edwards died a fortnight later from the injuries he had sustained. In total, the crash claimed 23 lives. Initially, ice on the wings was blamed, but a later inquiry declared that slush on the runway had made a safe take-off almost impossible.\nOf the 44 passengers and crew (including the 17-strong Manchester United squad), 23 people (eight of them Manchester United players) died as a result of their injuries in the crash. Charlton survived with minor injuries. Of the eight other players who survived, two of them were injured so badly that they never played again.\nCharlton was the first injured survivor to leave hospital. Harry Gregg and Bill Foulkes were not hospitalised, for they escaped uninjured. He arrived back in England on 14 February 1958, eight days after the crash. As he convalesced with family in Ashington, he spent some time kicking a ball around with local youths, and a famous photograph of him was taken. He was still only 20 years old, yet now there was an expectation that he would help with the rebuilding of the club as Busby's aides tried to piece together what remained of the season.\nBetween Harry Gregg's death in 2020 and his own in 2023, Charlton was the last living survivor of the crash.\nResuming his career.\nCharlton returned to playing in a kickabout with local youths first and then in a practice match on 25 February. He initially said \"I felt as slow as an old cart horse\" but then his vigour returned and he then said \"I feel fine and would like to play in the cup game at West Brom\". This was an FA Cup tie against West Bromwich Albion on 1 March; the game was a draw and United won the replay 1\u20130. Not unexpectedly, United went out of the European Cup to A.C. Milan in the semi-finals to a 5\u20132 aggregate defeat and fell behind in the League. Yet somehow they reached their second consecutive FA Cup final, and the big day at Wembley coincided with Busby's return to work. However, Nat Lofthouse scored twice to give Bolton Wanderers a 2\u20130 win.\nFurther success with Manchester United came at last when they beat Leicester City 3\u20131 in the FA Cup final of 1963, with Charlton finally earning a winners' medal in his third final. Busby's post-Munich rebuilding programme continued to progress, with two League championships within three seasons, in 1965 and 1967. A successful (though trophyless) season with Manchester United saw him take the honours of Football Writers' Association Footballer of the Year and European Footballer of the Year into the competition.\nManchester United reached the 1968 European Cup Final, ten seasons after Munich. Even though other clubs had taken part in the competition in the intervening decade, the team which got to this final was still the first English side to do so. On a highly emotional night at Wembley, Charlton scored twice in a 4\u20131 win after extra time against Benfica and, as United captain, lifted the trophy.\nDuring the early 1970s, Manchester United were no longer competing among the top teams in England, and at several stages were battling against relegation. At times, Charlton was not on speaking terms with United's other superstars, George Best and Denis Law, and Best refused to play in Charlton's testimonial match against Celtic, saying that \"to do so would be hypocritical\". Charlton left Manchester United at the end of the 1972\u201373 season, having scored 249 goals and set a club record of 758 appearances, a record which Ryan Giggs broke in the 2008 UEFA Champions League Final.\nCharlton's last game for Manchester United was against Chelsea at Stamford Bridge on 28 April 1973. Chelsea won the match 1\u20130. Coincidentally, this day also marked his brother Jackie's last appearance as well (for Leeds). Charlton's final goal for the club came a month earlier, on 31 March, in a 2\u20130 win at Southampton, also in the First Division.\nCharlton was the subject of an episode of \"This Is Your Life\" in 1969 when he was surprised by Eamonn Andrews at The Sportsman's Club in central London.\nInternational career.\nCharlton's emergence as the country's leading young football talent was completed when he was called up to join the England squad for a British Home Championship game against Scotland at Hampden Park on 19 April 1958, just over two months after he had survived the Munich air disaster.\nCharlton was handed his debut as England romped home 4\u20130, with the new player gaining even more admirers after scoring a magnificent thumping volley dispatched with authority after a cross by the left winger Tom Finney. He scored both goals in his second game as England beat Portugal 2\u20131 in a friendly at Wembley, and overcame obvious nerves on a return to Belgrade to play his third match against Yugoslavia; England lost that game 5\u20130 and Charlton played poorly.\nCharlton was selected for the squad which competed at the 1958 World Cup in Sweden, but he did not play.\nIn 1959, Charlton scored a hat-trick as England demolished the US 8\u20131; and his second England hat-trick came in 1961 in an 8\u20130 thrashing of Mexico. He also managed to score in every British Home Championship tournament he played in except 1963 in an association with the tournament that lasted from 1958 to 1970 and included 16 goals and 10 tournament victories (five shared).\n1962 World Cup.\nCharlton played in qualifiers for the 1962 World Cup in Chile against Luxembourg and Portugal and was named in the squad for the finals themselves. His goal in the 3\u20131 group win over Argentina was his 25th for England in just 38 appearances, and he was still only 24 years old; but his individual success could not be replicated by that of the team, which was eliminated in the quarter-final by Brazil, who went on to win the tournament.\nBy now, England were coached by Alf Ramsey, who had managed to gain sole control of the recruitment and team selection procedure from the committee-based call-up system which had lasted up to the previous World Cup. Ramsey had already cleared out some of the older players who had been reliant on the loyalty of the committee for their continued selection. A hat-trick in the 8\u20131 rout of Switzerland in June 1963 took Charlton's England goal tally to 30, equalling the record jointly held by Tom Finney and Nat Lofthouse; Charlton's 31st goal, against Wales in October the same year, gave him the record alone.\nCharlton's role was developing from traditional inside-forward to what today would be termed an attacking midfield player, with Ramsey planning to build the team for the 1966 World Cup around him. When England beat the USA 10\u20130 in a friendly on 27 May 1964, he scored one goal, his 33rd at senior level for England.\nHis goals became a little less frequent, and indeed Jimmy Greaves, playing purely as a striker, overtook his England tally in October 1964. Nevertheless, Charlton was still scoring and creating freely, and as the tournament was about to start he was expected to become one of its stars and galvanise his established reputation as one of the world's best footballers.\n1966 World Cup.\nEngland drew the opening game of the tournament 0\u20130 with Uruguay. Charlton scored the first goal in the 2\u20130 win over Mexico. This was followed by an identical scoreline against France, allowing England to qualify for the quarter-finals, where they defeated Argentina 1\u20130. The game was the only international match in which Charlton received a caution.\nThey faced Portugal in the semi-finals. This turned out to be one of Charlton's most important games for England. Charlton opened the scoring with a crisp side-footed finish after a run by Roger Hunt had forced the Portuguese goalkeeper out of his net; his second was a sweetly struck shot after a run and pull-back from Geoff Hurst. Charlton and Hunt were now England's joint-highest scorers in the tournament with three each, and a final against West Germany beckoned.\nThe final turned out to be one of Charlton's quieter days; he and a young Franz Beckenbauer effectively marked each other out of the game. England won 4\u20132 after extra time, with the scores tied at 2\u20132 after 90 minutes, and England lifted the World Cup trophy for the first time.\nEuro 1968.\nCharlton's next England game was his 75th, as England beat Northern Ireland; after two more appearances he became England's second most-capped player, behind the veteran Billy Wright, who was approaching his 100th match when Charlton was starting out and ended with 105 caps.\nWeeks later he scored his 45th England goal in a friendly against Sweden, breaking the record of 44 set the previous year by Jimmy Greaves. He was then in the England team which made it to the semi-finals of the 1968 European Championships, where they were knocked out by Yugoslavia in Florence. During the match Charlton struck a Yugoslav post. England defeated the Soviet Union 2\u20130 in the third place match.\nIn 1969, Charlton was appointed an OBE for services to football. More milestones followed as he won his 100th England cap on 21 April 1970 against Northern Ireland, and was made captain by Ramsey for the occasion. Inevitably, he scored; this was his 48th goal for his country \u2013 his 49th and final goal followed a month later in a 4\u20130 win over Colombia during a warm-up tour for the 1970 World Cup, designed to get the players adapted to altitude conditions. Charlton's inevitable selection by Ramsey for the tournament made him the first \u2013 and still, to date, only \u2013 England player to feature in four World Cup squads.\n1970 World Cup.\nShortly before the World Cup, Charlton was involved in the Bogot\u00e1 Bracelet incident in which he and Bobby Moore were accused of stealing a bracelet from a jewellery store. Moore was later arrested and detained for four days before being granted a conditional release, while Charlton was not arrested.\nEngland began the tournament with two victories in the group stages, plus a memorable defeat against Brazil. Charlton played in all three, though was substituted for Alan Ball in the final game of the group against Czechoslovakia. Ramsey, confident of victory and progress to the quarter-final, wanted Charlton to rest.\nEngland reached the last eight where they again faced West Germany. With England leading 2\u20131, Ramsey replaced Charlton with Colin Bell in the 69th minute: Germany went on to win 3\u20132 after extra time. England were eliminated and, after a record 106 caps and 49 goals, Charlton decided to end his international career at the age of 32. On the flight home from Mexico, he asked Ramsey not to consider him again. His brother Jack, two years his senior but 71 caps his junior, did likewise.\nCharlton's caps record lasted until 1973, when Bobby Moore overtook him; as of October 2023, he lies seventh in the all-time England appearances list behind Moore, Wayne Rooney, Ashley Cole, Steven Gerrard, David Beckham and Peter Shilton, whose own England career began in the first game after Charlton's had ended. Charlton's goalscoring record was surpassed by Wayne Rooney on 8 September 2015, when Rooney scored a penalty in a 2\u20130 win over Switzerland in a qualifying match for UEFA Euro 2016.\nStyle of play.\nIn his early years as a winger who played on the outside left, Charlton possessed great speed and agility. As he matured, Charlton was placed in an offensive midfield role where he flourished as a player. In his prime, Charlton was considered to be one of the greatest players in the world, being able to dictate a game with his accurate passing, and possessing a powerful shot with either foot, often scoring goals from a distance. Charlton also stood out for his stamina, mentality, leadership and modesty, never arguing with referees or opponents.\nLongtime Manchester United manager Matt Busby said of Charlton: \"There has never been a more popular footballer. He was as near perfection as man and player as it is possible to be.\" England national team coach Alf Ramsey remarked: \"He was one of the greatest players I have ever seen, very much the linchpin of the 1966 team. Early in my management, I knew I had to find a role suitable to Bobby's unique talents. He wasn't just a great goalscorer, with a blistering shot using either foot. Bobby was a player who could also do his share of hard work.\"\nManagement career and directorships.\nCharlton became the manager of Preston North End in 1973, signing his former United and England teammate Nobby Stiles as player-coach. His first season ended in relegation, and although he began playing again, he left Preston early in the 1975\u201376 season after a disagreement with the board over the transfer of John Bird to Newcastle United. He was appointed a CBE that year and began a casual association with BBC for punditry on matches, which continued for many years. In early 1976, he scored once in three league appearances for Waterford United. He also made a handful of appearances for Australian clubs Newcastle KB United, Perth Azzurri and Blacktown City.\nCharlton joined Wigan Athletic as a director, and was briefly caretaker manager there in 1983. He then spent some time playing in South Africa. He also built up several businesses in areas such as travel, jewellery and hampers, and ran soccer schools in the UK, the US, Canada, Australia and China. In 1984, he was invited to become member of the board of directors at Manchester United, partly because of his football knowledge and partly because it was felt that the club needed a \"name\" on the board after the resignation of Sir Matt Busby. In June 2005, when the American Glazer family bought Manchester United amidst fan opposition, Charlton apologised to the new owners: \"I tried to explain they couldn't ignore the fans, who are so emotionally involved in the club, but who sometimes do go a bit too far.\"\nPersonal life and retirement.\nCharlton met his wife, Norma Ball, at an ice rink in Manchester in 1959 and they married in 1961. They had two daughters, Suzanne and Andrea. Suzanne was a weather forecaster for the BBC during the 1990s. They went on to have grandchildren, including Suzanne's son Robert, who is named in honour of his grandfather.\nIn 2007, while publicising his forthcoming autobiography, Charlton revealed that he had a long-running feud with his brother Jack. They rarely spoke to each other after a falling-out between his wife Norma and his mother Cissie (who died in 1996 at the age of 83). Bobby Charlton did not see his mother after 1992 as a result of the feud.\nJack presented him with his BBC Sports Personality of the Year Lifetime Achievement Award on 14 December 2008. He said that he was \"knocked out\" as he was presented the award by his brother. He received a standing ovation as he stood waiting for his prize.\nCharlton helped to promote Manchester's bids for the 1996 and 2000 Olympic Games and the 2002 Commonwealth Games, England's bid for the 2006 World Cup and London's successful bid for the 2012 Summer Olympics. He received a knighthood in 1994 and was an Inaugural Inductee to the English Football Hall of Fame in 2002. On accepting his award, he commented: \"I'm really proud to be included in the National Football Museum's Hall of Fame. It's a great honour. If you look at the names included I have to say I couldn't argue with them. They are all great players and people I would love to have played with.\" He was also the (honorary) president of the National Football Museum, an organisation about which he said: \"I can't think of a better museum anywhere in the world.\"\nOn 2 March 2009, Charlton was given the freedom of the city of Manchester. He stated: \"I'm just so proud, it's fantastic. It's a great city. I have always been very proud of it.\"\nCharlton was involved in a number of charitable activities, including fund raising for cancer hospitals. After visits to Bosnia and Cambodia, Charlton became involved in the cause of land mine clearance, and supported the Mines Advisory Group as well as founding his own charity, The Sir Bobby Charlton Foundation (formerly Find a Better Way), which funds research into improved civilian landmine clearance.\nIn January 2011, Charlton was voted the fourth-greatest Manchester United player of all time by the readers of \"Inside United\" and ManUtd.com, behind Ryan Giggs (who topped the poll), Eric Cantona and George Best.\nHe was a member of the Laureus World Sports Academy. On 6 February 2012, Charlton was taken to hospital after falling ill, and subsequently had a gallstone removed. This prevented him from collecting a Lifetime Achievement Award at the Laureus World Sports Awards.\nOn 15 February 2016, Manchester United announced the South Stand of Old Trafford would be renamed in honour of Sir Bobby Charlton. The unveiling took place at the home game against Everton on 3 April 2016.\nIn 2015, he received a Honorary Doctorate from the University of Bath.\nIn October 2017, Charlton had a pitch named after him at St George's Park National Football Centre in Burton-upon-Trent.\nIn November 2020, it was revealed that Charlton had been diagnosed with dementia and as a result, he withdrew from public life.\nDeath.\nCharlton died at Macclesfield District General Hospital in Macclesfield, Cheshire, on 21 October 2023, at the age of 86, from complications of a fall he sustained at the nursing home where he resided; at the subsequent inquest, the coroner recorded his cause of death as \"trauma in the lungs, a fall and dementia\". His death leaves Sir Geoff Hurst as the last surviving English player of the 1966 World Cup final.\nManchester United paid tribute to Charlton at their Champions League match against Copenhagen at Old Trafford three days later in a number of ways. First, United's players wore black armbands, and manager Erik ten Hag was flanked by Alex Stepney and U-21 captain Dan Gore before ten Hag laid a wreath and a minute's silence was observed before the match began. Another wreath was also laid in Charlton's seat in the director's box. In addition, the cover of United's match programme, the \"United Review\", featured Charlton on the front, and supporters laid flowers and scarves at the United Trinity.\nThe funeral took place on 13 November. The main ceremony was at Manchester Cathedral but the procession called first at the Old Trafford stadium before going to the city centre. Crowds lined the streets and there were about a thousand guests at the service, including Prince William, Alex Ferguson and many others associated with the club. Elegies were read by David Gill and Charlton's grandson William Balderston and there was a musical tribute of \"How Great Thou Art\" sung by Russell Watson.\nHonours.\nManchester United Youth\nManchester United\nEngland\nIndividual\nOrders and special awards\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4226", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4226", "title": "Brewsters angle", "text": ""}
{"id": "4227", "revid": "31711350", "url": "https://en.wikipedia.org/wiki?curid=4227", "title": "Barry Lyndon", "text": "1975 film by Stanley Kubrick\nBarry Lyndon is a 1975 historical drama film written, directed, and produced by Stanley Kubrick, based on the 1844 novel \"The Luck of Barry Lyndon\" by William Makepeace Thackeray. Narrated by Michael Hordern, and starring Ryan O'Neal, Marisa Berenson, Patrick Magee, Leonard Rossiter and Hardy Kr\u00fcger, the film recounts the early exploits and later unravelling of an 18th-century Irish rogue and gold digger who marries a rich widow in order to attempt to climb the social ladder and assume her late husband's aristocratic position.\nKubrick began production on \"Barry Lyndon\" after his 1971 film \"A Clockwork Orange\". He had originally intended to direct a biopic on Napoleon, but lost his financing because of the commercial failure of the similar 1970 Dino De Laurentiis\u2013produced \"Waterloo\". Kubrick eventually directed \"Barry Lyndon\", set partially during the Seven Years' War, utilising his research from the Napoleon project. Filming began in December 1973 and lasted roughly eight months, taking place in England, Ireland, and Germany.\nThe film's cinematography has been described as ground-breaking. Especially notable are the long double shots, usually ended with a slow backwards zoom, the scenes shot entirely in candlelight, and the settings based on William Hogarth paintings. The exteriors were filmed on location in England, Ireland, and Germany, with the interiors shot mainly in London. The production had problems related to logistics, weather, and politics (Kubrick feared that he might be an IRA hostage target).\n\"Barry Lyndon\" won four Oscars at the 48th Academy Awards: Best Scoring: Original Song Score and Adaptation or Scoring: Adaptation, Best Costume Design, Best Art Direction, and Best Cinematography. Although some critics took issue with the film's slow pace and restrained emotion, its reputation, like that of many of Kubrick's works, has grown over time, and it is widely considered one of the greatest films of all time. In the 2022 \"Sight &amp; Sound\" Greatest Films of All Time poll, \"Barry Lyndon\" placed 12th in the directors' poll and 45th in the critics' poll.\nPlot.\nPart I: \"By What Means Redmond Barry Acquired the Style and Title of Barry Lyndon\".\nIn the Kingdom of Ireland, 1750, Redmond Barry's father is killed in a duel. Barry, of a genteel but impoverished Irish Protestant family, becomes infatuated with his cousin Nora Brady, and shoots her suitor, Army captain John Quin, in a duel. He flees but is robbed by highwaymen on his way to Dublin. Penniless, Barry enlists in the Army. Family friend Captain Grogan informs him that Quin is not dead: the duel was staged so that Nora's family could get rid of Barry and improve their finances through her marriage to Quin.\nBarry serves with his regiment in Germany during the Seven Years' War, but deserts after Grogan dies in combat against the French. Absconding with a lieutenant's horse and uniform, Barry has a brief affair with Frau Lieschen, a married German peasant woman. On his way to Bremen, he encounters Captain Potzdorf, who sees through the ruse and impresses him into the Prussian Army. Barry later saves Potzdorf's life and receives a commendation from Frederick the Great.\nAt the end of the war, Barry is recruited by Captain Potzdorf's uncle into the Prussian Ministry of Police. The Prussians suspect that the Chevalier de Balibari, an Austrian diplomat and professional gambler, is in fact an Irishman and a spy for Empress Maria Theresa, and assign Barry to become his manservant. An emotional Barry confides everything to the Chevalier and they become confederates. After they win an enormous sum from the Prince of T\u00fcbingen at cards, the Prince accuses the Chevalier of cheating (without proof) and refuses to pay his debt. The Chevalier threatens to demand satisfaction. To avoid a scandal, the Ministry of Police pays off the debt and quietly escorts the Chevalier outside Prussian borders, which allows Barry, disguised as the Chevalier, to leave the country as well, the Chevalier himself having uneventfully crossed the frontier the night before.\nBarry and the Chevalier travel across Europe, perpetrating similar gambling scams, with Barry forcing payment from debtors with sword duels. In Spa, he encounters the beautiful, wealthy, and visibly depressed Lady Lyndon. He seduces her, and goads her elderly husband Sir Charles Lyndon to death with verbal repartee.\nPart II: \"Containing an Account of the Misfortunes and Disasters Which Befell Barry Lyndon\".\nIn 1773, Barry marries Lady Lyndon, takes her last name and settles in England. The Countess bears Barry a son, Bryan Patrick, whom Barry spoils. The marriage is unhappy: Barry is openly unfaithful and squanders his wife's wealth while keeping her in seclusion. Lord Bullingdon, Lady Lyndon's son by Sir Charles, still grieves for his father and sees Barry as a gold digger who does not love his mother. Barry responds with years of escalating emotional and physical abuse.\nBarry's mother comes to live with him and warns him that if Lady Lyndon dies, Bullingdon will inherit everything. She advises her son to obtain a title. To this end, Barry cultivates the influential Lord Wendover and spends even larger sums of Lady Lyndon's money to ingratiate himself with high society. Bullingdon, now a young adult, interrupts a society party Barry throws. He publicly accuses his stepfather of infidelity, abuse, and financial mismanagement, and announces that he will leave the Lyndon estate for as long as Barry remains there. Barry responds by brutally assaulting Bullingdon until they are physically separated by the party attendees. In the aftermath, Barry is ostracised by high society and plunges further into financial ruin.\nBarry arranges to give Bryan a full-grown horse for his ninth birthday. An impatient Bryan rides the horse unaccompanied and dies in a riding accident. Barry sinks into alcoholism, while Lady Lyndon seeks religious counseling from clergyman Samuel Runt, who had been tutor to Bullingdon and Bryan and a longtime companion of Lady Lyndon. When Barry's mother dismisses Runt to keep her son from losing control, Lady Lyndon attempts suicide. Runt and Graham, the family's steward, write to Bullingdon, who returns to the estate and challenges his stepfather to a duel.\nWhen the duel takes place, Bullingdon accidentally misfires the first shot, but Barry then deliberately fires into the ground, refusing to exploit his stepson's mistake. Bullingdon refuses to accept this as satisfaction and fires again, shooting Barry in the leg. This wound forces Barry to have an amputation below the knee. While Barry is recovering, Bullingdon takes control of the Lyndon estate. Through Graham, he reminds Barry that his credit is exhausted and offers him 500 guineas a year to leave Lady Lyndon, her estates, and England forever. Barry grudgingly accepts and resumes his former gambling profession, but without any real success. In December 1789, Lady Lyndon signs Barry's annuity cheque as her son looks on.\nCast.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;* Michael Hordern (voice) as Narrator\nCritic Tim Robey suggests that the film \"makes you realise that the most undervalued aspect of Kubrick's genius could well be his way with actors.\" He adds that the supporting cast is a \"glittering procession of cameos, not from star names but from vital character players.\"\nThe cast featured Leon Vitali as the older Lord Bullingdon, who then became Kubrick's personal assistant, working as the casting director on his following films, and supervising film-to-video transfers for Kubrick. Their relationship lasted until Kubrick's death. The film's cinematographer, John Alcott, appears at the men's club in the non-speaking role of the man asleep in a chair near the title character when Lord Bullingdon challenges Barry to a duel. Kubrick's daughter Vivian also appears (in an uncredited role) as a guest at Bryan's birthday party.\nOther Kubrick featured regulars were Leonard Rossiter (\"\"), Steven Berkoff, Patrick Magee, Godfrey Quigley, Anthony Sharp, and Philip Stone (\"A Clockwork Orange\"). Stone went on to feature in \"The Shining\".\nThematic analysis.\nA main theme explored in Barry Lyndon is one of fate and destiny. Barry is pushed through life by a series of key events, some of which seem unavoidable. As critic Roger Ebert says, \"He is a man to whom things happen.\" He declines to eat with the highwayman Captain Feeney, where he would most likely have been robbed, but is robbed anyway farther down the road. The narrator repeatedly emphasizes the role of fate as he announces events before they unfold on screen, like Bryan's death and Bullingdon seeking satisfaction. This theme of fate is also developed in the recurring motif of the painting. Just like the events featured in the paintings, Barry is participating in events which always were.\nAnother major theme is between father and son. Barry lost his father at a young age and throughout the film he seeks and attaches himself to father-figures. Examples include his uncle, Grogan, and the Chevalier. When given the chance to be a father, Barry loves his son to the point of spoiling him. This contrasts with his role as a (step)father to Lord Bullingdon, whom he disregards and punishes.\nProduction.\nDevelopment.\nAfter completing post production on \"\", Kubrick resumed planning a film about Napoleon. During pre-production, Sergei Bondarchuk and Dino De Laurentiis's \"Waterloo\" was released, and failed at the box office. Reconsidering, Kubrick's financiers pulled funding, and he turned his attention towards a film adaptation of Anthony Burgess's 1962 novel \"A Clockwork Orange\". Subsequently, Kubrick showed an interest in Thackeray's \"Vanity Fair\" but dropped the project when a serialised version for television was produced. He told an interviewer, \"At one time, \"Vanity Fair\" interested me as a possible film but, in the end, I decided the story could not be successfully compressed into the relatively short time-span of a feature film ... as soon as I read \"Barry Lyndon\" I became very excited about it.\"\nHaving earned Oscar nominations for \"Dr. Strangelove\", \"2001: A Space Odyssey\" and \"A Clockwork Orange\", Kubrick's reputation in the early 1970s was that of \"a perfectionist auteur who loomed larger over his movies than any concept or star\". His studio\u2014Warner Bros.\u2014was therefore \"eager to bankroll\" his next project, which Kubrick kept \"shrouded in secrecy\" from the press partly due to the furor surrounding the controversially violent \"A Clockwork Orange\" (particularly in the UK) and partly due to his \"long-standing paranoia about the tabloid press\". Kubrick was initially rumored to be developing an adaptation of Arthur Schnitzler's 1926 novella \"Dream Story\", which would serve as the source material for his later film \"Eyes Wide Shut\" (1999).\nIn 1972 Kubrick finally set his sights on Thackeray's 1844 \"satirical picaresque about the fortune-hunting of an Irish rogue\", \"The Luck of\" \"Barry Lyndon\", the setting of which allowed Kubrick to take advantage of the copious period research he had done for the now-aborted \"Napoleon\". At the time, Kubrick merely announced that his next film would star Ryan O'Neal (deemed \"a seemingly un-Kubricky choice of leading man\") and Marisa Berenson, a former \"Vogue\" and \"Time\" magazine cover model, and be shot largely in Ireland. So heightened was the secrecy surrounding the film that \"Even Berenson, when Kubrick first approached her, was told only that it was to be an 18th-century costume piece [and] she was instructed to keep out of the sun in the months before production, to achieve the period-specific pallor he required.\"\nScreenplay.\nKubrick based his adapted screenplay on William Makepeace Thackeray's \"The Luck of Barry Lyndon\" (republished as the novel \"Memoirs of Barry Lyndon, Esq.),\" a picaresque tale written and published in serial form in 1844.\nThe film departs from the novel in several ways. In Thackeray's writings, events are related in the first person by Barry himself. A comic tone pervades the work, as Barry proves both a raconteur and an unreliable narrator. Kubrick's film, by contrast, presents the story objectively. Though the film contains voice-over (by actor Michael Hordern), the comments expressed are not Barry's, but those of an omniscient narrator. Kubrick felt that using a first-person narrative would not be useful in a film adaptation:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I believe Thackeray used Redmond Barry to tell his own story in a deliberately distorted way because it made it more interesting. Instead of the omniscient author, Thackeray used the imperfect observer, or perhaps it would be more accurate to say the dishonest observer, thus allowing the reader to judge for himself, with little difficulty, the probable truth in Redmond Barry's view of his life. This technique worked extremely well in the novel but, of course, in a film you have objective reality in front of you all of the time, so the effect of Thackeray's first-person story-teller could not be repeated on the screen. It might have worked as comedy by the juxtaposition of Barry's version of the truth with the reality on the screen, but I don't think that Barry Lyndon should have been done as a comedy.\nKubrick made several changes to the plot, including the addition of the final duel.\nPrincipal photography.\nPrincipal photography lasted 300 days, from spring 1973 through to early 1974, with a break for Christmas. Kubrick initially wished to film the entire production near his home in Borehamwood, but Ken Adam convinced him to relocate the shoot to Ireland. The crew arrived in Dublin in May 1973. Jan Harlan recalls that Kubrick \"loved his time in Ireland \u2013 he rented a lovely house west of Dublin, he loved the scenery and the culture and the people\".\nMany of the exteriors were shot in Ireland, playing \"itself, England, and Prussia during the Seven Years' War\". Kubrick and cinematographer Alcott drew inspiration from \"the landscapes of Watteau and Gainsborough\", and also relied on the art direction of Ken Adam and Roy Walker. Alcott, Adam and Walker were among those who would win Oscars for their work on the film.\nSeveral of the interior scenes were filmed in Powerscourt House, an 18th-century mansion in County Wicklow. The house was destroyed in an accidental fire several months after filming (November 1974), so the film serves as a record of the lost interiors, particularly the \"Saloon\" which was used for more than one scene. The Wicklow Mountains are visible, for example, through the window of the saloon during a scene set in Berlin. Other locations included Kells Priory, County Kilkenny (the English Redcoat encampment); Huntington Castle, County Carlow (exterior) and Dublin Castle, County Dublin (the chevalier's home). Some exterior shots were also filmed at Waterford Castle, County Waterford (now a luxury hotel and golf course) and Little Island, Waterford. Moorstown Castle in County Tipperary also featured. Several scenes were filmed at Castletown House in Celbridge, County Kildare; outside Carrick-on-Suir, County Tipperary, and at Youghal, County Cork.\nThe filming took place in the backdrop of some of the most intense years of the Troubles in Ireland, during which the Provisional Irish Republican Army (Provisional IRA) was waging an armed campaign in order to unite the island. On 30 January 1974, while filming in Dublin City's Phoenix Park, shooting had to be cancelled due to the chaos caused by 14 bomb threats. One day a phone call was received and Kubrick was given 24 hours to leave the country; he left within 12 hours. The phone call alleged that the Provisional IRA had him on a hit list and Harlan recalls: \"Whether the threat was a hoax or it was real, almost doesn't matter ... Stanley was not willing to take the risk. He was threatened, and he packed his bag and went home.\" Production of the film was one-third completed when this occurred, and it was rumored that the film would be abandoned. Nonetheless, Kubrick continued shooting the remainder of the film at locations in England, mainly southern England, and in Scotland, West Germany, and East Germany.\nLocations in England include Blenheim Palace, Oxfordshire; Castle Howard, North Yorkshire (exteriors of the Lyndon estate, \"Castle Hickham\"); Corsham Court, Wiltshire (various interiors and the music room scene); Petworth House, West Sussex (chapel); Stourhead, Wiltshire (lake and temple); Longleat, Wiltshire; Wilton House, Wiltshire (interior and exterior) and Lavenham Guildhall at Lavenham in Suffolk (amputation scene). Filming took place at Dunrobin Castle (exterior and garden as Spa) in Sutherland, Scotland. Locations in West Germany include Ludwigsburg Palace in Ludwigsburg and Hohenzollern Castle in Hechingen, both near Stuttgart. Frederick II of Prussia's Neues Palais at Potsdam near Berlin, at the time East Germany, was also used as a location (suggesting Berlin's main street Unter den Linden as construction in Potsdam had just begun in 1763).\nCinematography.\nThe film, as with \"almost every Kubrick film\", is a \"showcase for [a] major innovation in technique\". While \"2001: A Space Odyssey\" had featured \"revolutionary effects\", and \"The Shining\" would later feature heavy use of the Steadicam, \"Barry Lyndon\" saw a considerable number of sequences shot \"without recourse to electric light\". The film's cinematography was overseen by director of photography John Alcott (who won an Oscar for his work), and is particularly noted for the technical innovations that made some of its most spectacular images possible. To achieve photography without electric lighting \"[f]or the many densely furnished interior scenes\u00a0\u2026 meant shooting by candlelight\", which is known to be difficult in still photography, \"let alone with moving images\".\nKubrick was \"determined not to reproduce the set-bound, artificially lit look of other costume dramas from that time\". After \"tinker[ing] with different combinations of lenses and film stock,\" the production obtained three super-fast 50mm lenses (Carl Zeiss Planar 50mm \"f\"/0.7) developed by Zeiss for use by NASA in the Apollo Moon landings, which Kubrick had discovered. These super-fast lenses \"with their huge aperture (the film actually features the lowest f-stop in film history) and fixed focal length\" were problematic to mount, and were extensively modified into three versions by Cinema Products Corporation for Kubrick to gain a wider angle of view, with input from optics expert Richard Vetter of Todd-AO. The rear element of the lens had to be 2.5\u00a0mm away from the film plane, requiring special modification to the rotating camera shutter. This allowed Kubrick and Alcott to shoot scenes lit in candlelight to an average lighting volume of only three candela, \"recreating the huddle and glow of a pre-electrical age\". In addition, Kubrick had the entire film push-developed by one stop.\nAlthough Kubrick and Alcott sought to avoid electric lighting where possible, most shots were achieved with conventional lenses and lighting, but were lit to deliberately mimic natural light rather than for compositional reasons. In addition to potentially seeming more realistic, these methods also gave a particular period look to the film which has often been likened to 18th-century paintings (which of course depict a world devoid of electric lighting), in particular owing \"a lot to William Hogarth, with whom Thackeray had always been fascinated\".\nThe film is widely regarded as having a stately, static, painterly quality, mostly due to its lengthy, wide-angle long shots. To illuminate the more notable interior scenes, artificial lights called \"Mini-Brutes\" were placed outside and aimed through the windows, which were covered in a diffuse material to scatter the light evenly through the room rather than being placed inside for maximum use as most conventional films do. In some instances, the natural daylight was allowed to come through, which when recorded on the film stock used by Kubrick showed up as blue-tinted compared to the incandescent electric light.\nDespite such slight tinting effects, this method of lighting not only gave the look of natural daylight coming in through the windows, but it also protected the historic locations from the damage caused by mounting the lights on walls or ceilings and the heat from the lights. This helped the film \"fit\u00a0... perfectly with Kubrick's gilded-cage aesthetic \u2013 the film is consciously a museum piece, its characters pinned to the frame like butterflies\".\nMusic.\nThe film's period setting allowed Kubrick to indulge his penchant for using classical music, and the film score includes pieces by Vivaldi, Bach, Handel, Paisiello, Mozart, and Schubert. The piece most associated with the film, however, is the main title music, Handel's Sarabande from the Keyboard suite in D minor (HWV 437). Originally for solo harpsichord, the versions for the main and end titles are performed with strings, timpani, and continuo. The score also includes Irish folk music, including Se\u00e1n \u00d3 Riada's song \"Women of Ireland\", arranged by Paddy Moloney and performed by The Chieftains. \"The British Grenadiers\" also features in scenes with Redcoats marching.\nReception.\nContemporaneous.\nThe film \"was not the commercial success Warner Bros. had been hoping for\" within the United States, although it fared better in Europe. In the US it earned $9.1 million. \nThe film grossed a worldwide total of $20.2 million against a production budget of $12 million.\nThis mixed reaction saw the film (in the words of one retrospective review) \"greeted, on its release, with dutiful admiration \u2013 but not love. Critics\u00a0... rail[ed] against the perceived coldness of Kubrick's style, the film's self-conscious artistry and slow pace. Audiences, on the whole, rather agreed\".\nRoger Ebert gave the film three and a half stars out of four and wrote that it \"is almost aggressive in its cool detachment. It defies us to care, it forces us to remain detached about its stately elegance.\" He added, \"This must be one of the most beautiful films ever made.\" Vincent Canby of \"The New York Times\" called the film \"another fascinating challenge from one of our most remarkable, independent-minded directors.\" Gene Siskel of the \"Chicago Tribune\" gave the film three and a half stars out of four and wrote \"I found \"Barry Lyndon\" to be quite obvious about its intentions and thoroughly successful in achieving them. Kubrick has taken a novel about a social class and has turned it into an utterly comfortable story that conveys the stunning emptiness of upper-class life only 200 years past.\" He ranked the film fifth on his year-end list of the best films of 1975.\nCharles Champlin of the \"Los Angeles Times\" called it \"the motion picture equivalent of one of those very large, very heavy, very expensive, very elegant and very dull books that exist solely to be seen on coffee tables. It is ravishingly beautiful and incredibly tedious in about equal doses, a succession of salon quality still photographs\u2014as often as not very still indeed.\" \"The Washington Post\" wrote, \"It's not inaccurate to describe 'Barry Lyndon' as a masterpiece, but it's a deadend masterpiece, an objet d'art rather than a movie. It would be more at home, and perhaps easier to like, on the bookshelf, next to something like 'The Age of the Grand Tour,' than on the silver screen.\" Pauline Kael of \"The New Yorker\" wrote that \"Kubrick has taken a quick-witted story\" and \"controlled it so meticulously that he's drained the blood out of it,\" adding, \"It's a coffee-table movie; we might as well be at a three-hour slide show for art-history majors.\"\nThis \"air of disappointment\" affected Kubrick's choice for his next film, an adaption of Stephen King's \"The Shining\", a project that would not only please him artistically, but was more likely to succeed financially.\nRe-evaluation.\nOver time, the film has gained a more positive reaction. On review aggregator Rotten Tomatoes, the film holds an approval rating of 77% based on 151 reviews. The website's critical consensus reads, \"Visually astonishing and placid as a pond in the English countryside, Stanley Kubrick's maddening and masterful \"Barry Lyndon\" renders a hollow life with painterly poise.\" On Metacritic, the film has a weighted average score of 89 out of 100 based on reviews from 21 critics, indicating \"universal acclaim\". Roger Ebert added the film to his 'Great Movies' list on 9 September 2009 and increased his original rating from three and a half stars to four, writing, \"Stanley Kubrick's \"Barry Lyndon\", received indifferently in 1975, has grown in stature in the years since and is now widely regarded as one of the master's best. It is certainly in every frame a Kubrick film: technically awesome, emotionally distant, remorseless in its doubt of human goodness.\"\n\"The Village Voice\" ranked the film at number 46 in its Top 250 \"Best Films of the Century\" list in 1999, based on a poll of critics. Director Martin Scorsese has named \"Barry Lyndon\" as his favourite Kubrick film, and it is also one of Lars von Trier's favourite films. \"Barry Lyndon\" was included on \"Time\"'s All-Time 100 best movies list. In the 2012 \"Sight &amp; Sound\" Greatest Films of All Time poll, \"Barry Lyndon\" placed 19th in the directors' poll and 59th in the critics' poll. The film ranked 27th in BBC's 2015 list of the 100 greatest American films. In the 2022 \"Sight &amp; Sound\" Greatest Films of All Time poll, \"Barry Lyndon\" placed 12th in the directors' poll and 45th in the critics' poll.\nIn a list compiled by \"The Irish Times\" critics Tara Brady and Donald Clarke in 2020, \"Barry Lyndon\" was named the greatest Irish film of all time.\nThe Japanese filmmaker Akira Kurosawa cited the movie as one of his 100 favorite films.\nHome media.\n\"Barry Lyndon\" was released by Warner Bros. on DVD in 1999. The film was released on Blu-ray by Warner Bros. in 2011. It was released on Blu-ray and DVD by The Criterion Collection in October 2017, featuring a new remaster in the film's original 1.66:1 aspect ratio (compared to the Warner Blu-ray's 16:9 aspect ratio). The film was released on Ultra HD Blu-ray by Criterion in July 2025.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4230", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=4230", "title": "Cell (biology)", "text": "Basic unit of life forms\nThe cell is the basic structural and functional unit of all forms of life or organisms. The term comes from the Latin word meaning 'small room'. A biological cell basically consists of a semipermeable cell membrane enclosing cytoplasm that contains genetic material. Most cells are only visible under a microscope. Except for highly-differentiated cell types (examples include red blood cells and gametes) most cells are capable of replication, and protein synthesis. Some types of cell are motile. Cells emerged on Earth about four billion years ago. \nAll organisms are grouped into prokaryotes, and eukaryotes. Prokaryotes are single-celled, and include archaea, and bacteria. Eukaryotes can be single-celled or multicellular, and include protists, plants, animals, most types of fungi, and some species of algae. All multicellular organisms are made up of many different types of cell. The diploid cells that make up the body of a plant or animal are known as somatic cells, and in animals excludes the haploid gametes.\nProkaryotic cells lack the membrane-bound nucleus present in eukaryotic cells, and instead have a nucleoid region. In eukaryotic cells the nucleus is enclosed in the nuclear membrane. Eukaryotic cells contain other membrane-bound organelles such as mitochondria, which provide energy for cell functions, and chloroplasts, in plants that create sugars by photosynthesis. Other non-membrane-bound organelles may be proteinaceous such as the ribosomes present (though different) in both groups. A unique membrane-bound prokaryotic organelle the magnetosome has been discovered in magnetotactic bacteria.\nCells were discovered by Robert Hooke in 1665, who named them after their resemblance to cells in a monastery. Cell theory, developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all organisms, and that all cells come from pre-existing cells.\nTypes.\nOrganisms are broadly grouped into eukaryotes, and prokaryotes. Eukaryotic cells possess a membrane-bound nucleus, and prokaryotic cells lack a nucleus but have a nucleoid region. Prokaryotes are single-celled organisms, whereas eukaryotes can be either single-celled or multicellular. Single-celled eukaryotes include microalgae such as diatoms. Multicellular eukaryotes include all animals, and plants, most fungi, and some species of algae.\nProkaryotes.\nAll prokaryotes are single-celled and include bacteria and archaea, two of the three domains of life. Prokaryotic cells were likely the first form of life on Earth, characterized by having vital biological processes including cell signaling. They are simpler and smaller than eukaryotic cells, lack a nucleus, and the other usually present membrane-bound organelles. Prokaryotic organelles are less complex, and are typically non-membrane-bound. All prokaryotic cells secrete different substances from their membranes, including exoenzymes, and extracellular polymeric substances.\nMost prokaryotes are the smallest of all organisms, ranging from 0.5 to 2.0\u00a0\u03bcm in diameter. The largest bacterium known, \"Thiomargarita magnifica\", is visible to the naked eye with an average length of , but can be as much as \nBacteria.\nBacteria are enclosed in a cell envelope, that protects the interior from the exterior. It generally consists of a plasma membrane covered by a cell wall which, for some bacteria, is covered by a third gelatinous layer called a bacterial capsule. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as \"Bacillus anthracis\" or hyaluronic acid as in streptococci. \"Mycoplasma\" only possess the cell membrane. The cell envelope gives rigidity to the cell and separates the interior of the cell from its environment, serving as a protective mechanical and chemical filter. The cell wall consists of peptidoglycan and acts as an additional barrier against exterior forces. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. It also prevents the cell from expanding and bursting (cytolysis) from osmotic pressure due to a hypotonic environment. \nThe DNA of a bacterium typically consists of a single circular chromosome that is in direct contact with the cytoplasm in a region called the nucleoid. Some bacteria contain multiple circular or even linear chromosomes. The cytoplasm also contains ribosomes and various inclusions where transcription takes place alongside translation. Extrachromosomal DNA as plasmids, are usually circular and encode additional genes, such as those of antibiotic resistance. Linear bacterial plasmids have been identified in several species of spirochete bacteria, including species of \"Borrelia\" which causes Lyme disease. The prokaryotic cytoskeleton in bacteria is involved in the maintenance of cell shape, polarity and cytokinesis.\nCompartmentalization is a feature of eukaryotic cells but some species of bacteria, have protein-based organelle-like microcompartments such as gas vesicles, and carboxysomes, and encapsulin nanocompartments. Certain membrane-bound prokaryotic organelles have also been discovered. They include the magnetosome of magnetotactic bacteria, and the anammoxosome of anammox bacteria. \nCell-surface appendages can include flagella, and pili, protein structures that facilitate movement and communication between cells. The flagellum stretches from the cytoplasm through the cell membrane and extrudes through the cell wall. Fimbriae are short attachment pili, the other type of pilus is the longer conjugative type. Fimbriae are formed of an antigenic protein called pilin, and are responsible for the attachment of bacteria to specific receptors on host cells. \nArchaea.\nArchaea are enclosed in a cell envelope consisting of a plasma membrane and a cell wall.\nAn exception to this is the \"Thermoplasma\" that only has the cell membrane. The cell membranes of archaea are unique, consisting of ether-linked lipids. The prokaryotic cytoskeleton has homologues of eukaryotic actin and tubulin. A unique form of metabolism in the archaean is methanogenesis. Their cell-surface appendage equivalent of the flagella is the differently structured and unique archaellum. The DNA is contained in a circular chromosome in direct contact with the cytoplasm, in a region known as the nucleoid. Ribosomes are also found freely in the cytoplasm, or attached to the cell membrane where DNA processing takes place.\nThe archaea are noted for their extremophile species, and many are selectively evolved to thrive in extreme heat, cold, acidic, alkaline, or high salt conditions. There are no known archaean pathogens.\nEukaryotes.\nEukaryotes can be single-celled, as in diatoms (microscopic algae), or multicellular, as in animals, plants, most fungi, and some algae. Multicellular organisms are made up of many different types of cell known overall as somatic cells. Eukaryotes are distinguished by the presence of a membrane-bound nucleus. The nucleus gives the eukaryote its name, which means \"true nut\" or \"true kernel\", where \"nut\" means the nucleus. A eukaryotic cell can be 2 to 100 times larger in diameter than a typical prokaryotic cell. \nEukaryotic cells have a cell membrane that surrounds a gel-like cytoplasm. The cytoplasm contains the cytoskeleton, and surrounds the cell nucleus, the endoplasmic reticulum, ribosomes, the Golgi apparatus, mitochondria, lysosomes, peroxisomes, endosomes, vacuoles and vesicles, and may have a cell wall, chloroplasts, vaults, and cell-surface appendages. There are many cell variations among the different eukaryote groups. \nThe membranes of most of the organelles including the cell membrane are sometimes referred to as the endomembrane system. All of these membranes are involved in the secretory and endocytic pathways, modifying, packaging, and transporting proteins and lipids to and from the trans-Golgi network. In mammalian cells, endocytosis includes early,late and recycling endosomes. \nMost distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell\u2013cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).\nEukaryotic cell types include those that make up animals, plants, fungi, algae, and protists. All of which have many different species and cell differences.\nAnimal cells.\nAll the cells in an animal body develop from one totipotent diploid cell called a zygote. During the embryonic development of an animal, the cells differentiate into the specialised tissues and organs of the organism. Different groups of cells differentiate from the germ layers. The sponge has only one layer. Some other animals known as diploblasts have two germ layers the ectoderm, and the endoderm. More advanced animals have an extra layer, the middle mesodermal layer, and are known as triploblastic. Triploblastic animals make up the large clade of Bilateria. Differentiation results in structural or functional changes to stem cells, and progenitor cells. There are an estimated 200 different cell types in the human body. The estimated cell count in a typical adult human body is around 30 trillion cells, 36 trillion in an adult male, and 28 trillion in a female.\nDifferent types of epithelial cells make up many of an animal's tissues. Epithelial tissues includes the skin, glands, the lining of many organs, and inner cavities. Epithelial cells are joined together in sheets by way of cell junctions; adherens junctions, and desmosomes bind the cells together, and hemidesmosomes bind the cells to the basement membrane. All three types are linked to the cell cytoskeleton.\nStructure.\nAn animal cell has a cell membrane that surrounds a gel-like cytoplasm. The cytoplasm contains the cytoskeleton, the cell nucleus, the endoplasmic reticulum, ribosomes, the Golgi apparatus, mitochondria, lysosomes, peroxisomes, endosomes, vacuoles and vesicles, and vaults. An animal cell structure, as other eukaryotes, includes an endomembrane system encompassing all the membranes of the organelles and the cell membrane, excluding the mitochondria.The whole system cooperates in the modification, packaging, and transport of proteins and lipids.\nCell membrane.\nThe cell membrane, or plasma membrane, is a selectively permeable membrane as an outer boundary of the cell that encloses the cytoplasm. The membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a lipid bilayer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). It has been best described in the fluid mosaic model. Embedded within the cell membrane is a macromolecular structure called the porosome the universal secretory portal in cells and a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is semi-permeable, and selectively permeable, in that it can either let a substance (molecule or ion) pass through freely, to a limited extent or not at all. Cell surface receptors embedded in the membrane allow cells to detect external signaling molecules such as hormones.\nUnderlying, and attached to the cell membrane is the cell cortex, the outermost part of the actin cytoskeleton. \nCytoplasm.\nThe cell membrane encloses the cytoplasm of the cell that surrounds all of the cell's organelles. It is made up of two main components, the protein filaments that make up the cytoskeleton, and the cytosol. The network of filaments and microtubules of the cytoskeleton gives shape and support to the cell, and has a part in organising the cell components. The cytosol is the main site of protein synthesis, and degradation.\nThe cytosol is a gel-like substance made up of water, ions, and non-essential biomolecules. The acidity (pH) of the cytosol is near neutral, and transporters in the cell membrane regulate this. Different proteins in the cytoplasm operate optimally at different pHs. The cytosol forms of the cell's volume.\nCytoskeleton.\nThe cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, and in the uptake of external materials by a cell.The cytoskeleton is composed of microtubules, intermediate filaments and microfilaments. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The outermost part of the cytoskeleton is the cell cortex, or actin cortex, a thin layer of cross-linked actomyosins. Its thickness varies with cell type and physiology. It directs the transport through the ER and the Golgi apparatus. The cytoskeleton in the animal cell also plays a part in cytokinesis, in the formation of the spindle apparatus during cell division, the separation of daughter cells. \nOrganelles.\nOrganelles are compartments of the cell that are specialized for carrying out one or more functions, analogous to the organs, such as the heart, and lungs. There are several types of organelles held in the cytoplasm. Most organelles are membrane-bounded, and vary in size and number based on the growth of the host cell. Organelles include the nucleus, mitochondria, endoplasmic reticulum, Golgi apparatus, vesicles, and vacuoles. Non membrane-bounded organelles include the centrosome, and typically the ribosome.\nNucleus.\nThe cell nucleus is the largest organelle in the animal cell. It houses the cell's chromosomes, and is the place where almost all DNA replication and RNA synthesis (transcription) occur. The nucleus is spherical and separated from the cytoplasm by a double-membraned nuclear envelope. A space between the membranes is called the perinuclear space. The nuclear envelope isolates and protects a cell's DNA from various molecules that could accidentally damage its structure or interfere with its processing. During processing, DNA is transcribed, or copied into a special RNA, called messenger RNA (mRNA). This mRNA is then transported out of the nucleus, where it is translated into a specific protein molecule. The nucleolus is a specialized region within the nucleus where ribosome subunits are assembled. Cells use DNA for their long-term information storage that is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.\nThe DNA of each cell is its genetic material, and is organized in multiple linear molecules, called chromosomes, that are coiled around histone proteins and housed in the cell nucleus. In humans, the nuclear genome is divided into 46 linear chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. \nThe nucleus is a membrane-bound organelle. Other organelles in the cell have specific functions such as mitochondria which provide the cell's energy.\nEndoplasmic reticulum.\nThe endoplasmic reticulum (ER) is a transport network for molecules targeted for certain modifications and specific destinations, as compared to molecules that float freely in the cytoplasm. The ER has two forms: the rough endoplasmic reticulum (RER), which has ribosomes on its surface that secrete proteins into the ER, and the smooth endoplasmic reticulum (SER), which lacks ribosomes. The smooth ER plays a role in calcium sequestration and release, and helps in synthesis of lipid.\nGolgi apparatus.\nThe Golgi apparatus processes and packages proteins, and lipids, that are synthesized by the cell. It is organized as a stack of plate-like structures known as cisternae.\nMitochondria.\nMitochondria generate energy for the cell. Mitochondria are self-replicating double membrane-bound organelles that occur in various numbers, shapes, and sizes in the cytoplasm of the cell. Respiration occurs in the cell mitochondria, which generate the cell's energy by oxidative phosphorylation, using oxygen to release energy stored in cellular nutrients (typically pertaining to glucose) to generate ATP (aerobic respiration). Mitochondria multiply by binary fission. Mitochondria have their own DNA (mitochondrial DNA). The mitochondrial genome is a circular DNA molecule distinct from nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.\nLysosomes.\nLysosomes contain over 60 different hydrolytic enzymes. They digest excess or worn-out organelles, food particles, and engulfed viruses or bacteria. Lysosomes are optimally active in an acidic environment. The cell could not house these destructive enzymes if they were not contained in a membrane-bound system.\nPeroxisomes.\nPeroxisomes, are microbodies bounded by a single membrane. A peroxisome has no DNA or ribosomes and the proteins that it needs are encoded in the nucleus, and selectively imported from the cytosol. Some proteins enter via the endomembrane reticulum. They have enzymes that rid the cell of toxic peroxides. The enzymatic content of the peroxisomes varies widely across the species, as it can in an individual organism. The peroxisomes in animal cells are concentrated in the liver cells and adipocytes.\nVacuoles.\nVacuoles sequester waste products. Some cells, most notably \"Amoeba\", have contractile vacuoles, which can pump water out of the cell if there is too much water. \nCentrosome.\nThe centrosome is a non membrane-bounded organelle composed of pericentriolar material and the two centrioles. The centrosome is the main microtubule organizing center in the animal cell that produces the microtubules key components of the cytoskeleton. Centrosomes are composed of two centrioles which lie perpendicular to each other in which each has an organization like a cartwheel, which separate during cell division and help in the formation of the mitotic spindle.\nRibosomes.\nA ribosome is a large complex of RNA and protein molecules often considered as a non-membrane-bound organelle. They each consist of two subunits, one larger than the other, and act as an assembly line where RNA from the nucleus is used to synthesise proteins from amino acids. Ribosomes can be found either floating freely or bound to a membrane of the rough endoplasmatic reticulum.\nVaults.\nA vault is a large ribonuclear protein particle, a non-membrane-bound organelle, three times the size of a ribosome but with only three proteins in contrast to the near hundred in the ribosome. Most human cells have around 10,000 vaults, and in some types of immune cell there may be up to 100,000. Macrophages have the greatest number of vaults of any human cell. Vaults are largely overlooked because their functions are purely speculative. They may play a role in transport from the nucleus to the cytoplasm, and may serve as scaffolds for signal transduction proteins. They are present in normal tissues, and more so in secretory and excretory epithelial cells.\nAnimal cell types.\nSome types of specialised cell are localised to a particular animal group. Vertebrates for example have specialised, structurally changed cells including muscle cells. The cell membrane of a skeletal muscle cell or of a cardiac muscle cell is termed the sarcolemma. And the cytoplasm is termed the sarcoplasm. Skeletal muscle cells also become multinucleated. Populations of animal groups evolve to become distinct species, where sexual reproduction is isolated. The many species of vertebrates for example have other unique characteristics by way of additional specialised cells. In some species of electric fish for example modified muscle cells or nerve cells have specialised to become electerocytes capable of creating and storing electrical energy for future release, as in stunning prey, or use in electrolocation. These are large flat cells in the electric eel, and electric ray in which thousands are stacked into an electric organ comparable to a voltaic pile.\nMany animal cells are ciliated and most cells except red blood cells have primary cilia. Primary cilia play important roles in chemosensation and mechanosensation. \nEach cilium may be \"viewed as a sensory cellular antennae that coordinates a large number of cellular signaling pathways, sometimes coupling the signaling to ciliary motility or alternatively to cell division and differentiation.\" The cilia in other cells are motile organelles, and in the respiratory epithelium play an important role in the movement of mucus. In the reproductive system ciliated epithelium in the fallopian tubes move the egg from the uterus to the ovary. Motile cilia also known as flagella, drive the sperm cells. Invertebrate planarians have ciliated excretory flame cells. Other excretory cells also found in planarians are solenocytes that are long and flagellated.\nPlant cells.\nOther types of organelle specific to plant cells, are pigment-containing plastids, especially chloroplasts that contain chlorophyll. Chloroplasts capture the sun's energy to make carbohydrates through photosynthesis. Chromoplasts contain fat-soluble carotenoid pigments such as orange carotene and yellow xanthophylls which helps in synthesis and storage. Leucoplasts are non-pigmented plastids and helps in storage of nutrients. \nPlastids divide by binary fission. Vacuoles in plant cells store water, and are surrounded by a membrane. The vacuoles of plant cells are usually larger than those of animal cells. The vacuole membrane transports ions against concentration gradients. \nThe plant cytoskeleton is a dynamic structure that has a scaffold of microtubules and microfilaments, but not the intermediate filaments. The microtubule organizing center in plant cells is often sited underneath the cell membrane where nucleated microtubules often form sheet-like semi-parallel arrays.\nThere are two types of peroxisomes in plants. One type is in the leaves where it takes part in photorespiration. The other type is in germinating seeds where they take part in the conversion of fatty acids into sugars for the plant's growth. In this peroxisome type the enzymatic content is so different than in other groups that it has an alternative name of glyoxysome, their enzymes are of the glyoxylate cycle.\nAlgal cells.\nAlgae members are photoautotrophs able to use photosynthesis to produce energy. Photosynthesis is made possible by the use of plastids, organelles in the cytoplasm known as chloroplasts. Algal photoautotrophs include red algae.\nAlginate is a polysaccharide found in the matrix of the cell walls of brown algae, and has many important uses in the food industry, and in pharmacology.\nFungal cells.\nThe cells of fungi have in addition to the shared eukaryotic organelles a spitzenk\u00f6rper in their endomembrane system, associated with hyphal tip growth. It is a phase-dark body that is composed of an aggregation of membrane-bound vesicles containing cell wall components, serving as a point of assemblage and release of such components intermediate between the Golgi and the cell membrane. The spitzenk\u00f6rper is motile and generates new hyphal tip growth as it moves forward.\nThe cell walls of fungi are uniquely made of a chitin-glucan complex.\nProtist cells.\nThe cells of protists may be bounded only by a cell membrane, or may in addition have a cell wall, or may be covered by a pellicle (in ciliates), a test (in testate amoebae), or a frustule (in diatoms).\nSome protists such as amoebae may feed on other organisms and ingest food by phagocytosis. Vacuoles known as phagosomes in the cytoplasm may be used to draw in and incorporate the captured particles. Other types of protists are photoautotrophs, providing themselves with energy by photosynthesis. Most single-celled protists are motile, and generate movement with cilia, flagella, or pseudopodia.\nCiliates have two different sorts of nuclei: a tiny, diploid micronucleus (the \"generative nucleus\", which carries the germline of the cell), and a large, ampliploid macronucleus (the \"vegetative nucleus\", which takes care of general cell regulation.\nPhysiology.\nReplication.\nDuring cell division, a single cell, the \"mother cell\" divides into two daughter cells. This leads to the growth of tissue in multicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.\nDNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S (synthesis) phase of the cell cycle.\nIn meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins.\nSignaling.\nCell signaling is the process by which a cell interacts with itself, other cells, and the environment. Typically, the signaling process involves three components: the first messenger (the ligand), the receptor, and the signal itself. Most cell signaling is chemical in nature, and can occur with neighboring cells or more distant targets. Signal receptors are complex proteins or tightly bound multimer of proteins, located in the plasma membrane or within the interior.\nEach cell is programmed to respond to specific extracellular signal molecules, and this process is the basis of development, tissue repair, immunity, and homeostasis. Individual cells are able to manage receptor sensitivity including turning them off, and receptors can become less sensitive when they are occupied for long durations. Errors in signaling interactions may cause diseases such as cancer, autoimmunity, and diabetes.\nProtein targeting.\nProtein targeting or protein sorting is the biological mechanism by which proteins are transported to their appropriate destinations within or outside the cell.\nProteins can be targeted to the inner space of an organelle, different intracellular membranes, the plasma membrane, or to the exterior of the cell via secretion. Information contained in the protein itself directs this delivery process. Correct sorting is crucial for the cell; errors or dysfunction in sorting have been linked to multiple diseases.\nDNA repair.\nAll cells contain enzyme systems that scan for DNA damage and carry out repair. Diverse repair processes have evolved in all organisms. Repair is vital to maintain DNA integrity, avoid cell death and errors of replication that could lead to mutation. Repair processes include nucleotide excision repair, DNA mismatch repair, non-homologous end joining of double-strand breaks, recombinational repair and light-dependent repair (photoreactivation).\nGrowth and metabolism.\nBetween successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.\nComplex sugars can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a molecule that possesses readily available energy, through two different pathways. In plant cells, chloroplasts create sugars by photosynthesis, using the energy of light to join molecules of water and carbon dioxide.\nProtein synthesis.\nCells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.\nTranscription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate into the cytoplasm. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.\nMotility.\nUnicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.\nIn multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps: protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.\nNavigation, control and communication.\nIn August 2020, scientists described one way cells\u2014in particular cells of a slime mold and mouse pancreatic cancer-derived cells\u2014are able to navigate efficiently through a body and identify the best routes through complex mazes: generating gradients after breaking down diffused chemoattractants which enable them to sense upcoming maze junctions before reaching them, including around corners.\nCell death.\nCell death occurs when a cell ceases to carry out its functions, as a result of ageing, or types of cell injury (necrosis). Programmed cell death, including apoptosis, and autophagy is a natural process of replacing dead cells with new ones.\nA separate mode of cellular death is known as a mitotic catastrophe, which occurs during mitosis, following the improper progression of, or entrance to the cell cycle. This mechanism operates to prevent genomic instability. \nOther cell death pathways are described, and include anoikis, pyroptosis, mitoptosis, parthanatos, and necroptosis.\nOrigins.\nThe origin of cells has to do with the origin of life, which began the history of life on Earth. Small molecules needed for life may have been carried to Earth on meteorites, created at deep-sea vents, or synthesized by lightning in a reducing atmosphere. There is little experimental data defining what the first self-replicating forms were. RNA may have been the earliest self-replicating molecule, as it can both store genetic information and catalyze chemical reactions. This process required an enzyme to catalyze the RNA reactions, which may have been the early peptides that formed in hydrothermal vents.\nCells emerged around 4 billion years ago. The first cells were most likely heterotrophs. The early cell membranes were probably simpler and more permeable than later ones, with only a single fatty acid chain per lipid. Lipids spontaneously form bilayered vesicles in water, and could have preceded RNA.\nEukaryotic cells were created some 2.2 billion years ago in a process called eukaryogenesis. This is widely agreed to have involved symbiogenesis, in which an archaean and a bacterium came together to create the first eukaryotic common ancestor. It evolved into a population of single-celled organisms that included the last eukaryotic common ancestor, gaining capabilities along the way.\nThis cell had a new level of complexity, with a nucleus and facultatively aerobic mitochondria. It featured at least one centriole and cilium, sex (meiosis and syngamy), peroxisomes, and a dormant cyst with a cell wall of chitin and/or cellulose. The last eukaryotic common ancestor gave rise to the eukaryotes' crown group, containing the ancestors of animals, fungi, plants, and a diverse range of single-celled organisms. The green plants were created around 1.6 billion years ago with a second episode of symbiogenesis that added chloroplasts, derived from cyanobacteria.\nMulticellularity.\nMulticellular behavior is demonstrated by microorganisms that are cloned from a single cell and form visible microbial colonies. A microbial consortium of two or more species can form a biofilm by the secretion of extracellular polymeric substances (EPSs). Slime molds consist of different groups of microorganisms grouped together in a multicellular-like fashion.\nThe first evidence of multicellularity in an organism comes from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Cyanobacteria are variable in morphology, filamentous forms exhibit functional cell differentiation such as heterocysts (for nitrogen fixation), akinetes (resting stage cells), and hormogonia (reproductive, motile filaments). These, together with the intercellular connections they possess, are considered the first signs of multicellularity.\nMulticellularity was made possible by the development of the extracellular matrix (ECM) similar in function to the bacterial ECM that consists of extracellular polymeric substances. EPS enables microbial cell adhesion, and is believed to be the first evolutionary step toward multicellular organisms. Basement membranes are a type of specialized extracellular matrix that surrounds most animal tissues, and are essential in their formation. Their emergence coincided with the origin of multicellularity. \nThe evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.\nHistory of research.\nIn 1665, Robert Hooke examined a thin slice of cork under his microscope, and saw a structure of small enclosures. He wrote \"I could exceeding plainly perceive it to be all perforated and porous, much like a honeycomb, but that the pores of it were not regular\". To further support his theory, Matthias Schleiden and Theodor Schwann studied cells of both animal and plants. What they discovered were significant differences between the two types of cells. This put forth the idea that cells were fundamental to both plants and animals.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4231", "revid": "50146653", "url": "https://en.wikipedia.org/wiki?curid=4231", "title": "Buffy the Vampire Slayer (film)", "text": "1992 American film by Fran Rubel Kuzui\nBuffy the Vampire Slayer is a 1992 American comedy vampire film directed by Fran Rubel Kuzui and written by Joss Whedon. It stars Kristy Swanson as the eponymous Buffy Summers, a Valley Girl cheerleader who learns it is her fate to hunt vampires. Donald Sutherland, Paul Reubens, Rutger Hauer and Luke Perry appear in supporting roles, this is also the film debut of Hilary Swank.\nThe film received mixed reviews from critics and grossed $16.6 million on a $7 million budget. It also took a different direction from what Whedon intended, leading him to create a television series of the same name with his original plan to critical acclaim.\nPlot.\nBuffy Summers is a cheerleader at Hemery High School in Los Angeles. Her main concerns are shopping and spending time with her rich, snooty friends and her boyfriend, Jeffrey. While at the mall one day, she is approached by a man who calls himself Merrick. He informs her that she is The Slayer, or Chosen One, destined to kill vampires and his duty as her \"Watcher\" is to guide and train her. Buffy initially rejects his claim, calls him homeless, and leaves. \nSoon after, he approaches her at school and convinces Buffy of her birth-right when he vividly describes a recurring dream of hers. She begins exhibiting abilities not known to her, including heightened agility, senses, and strength. These abilities are realized when she tries to reject her new responsibilities by punching Merrick's nose much harder than expected. While he stands by his duty as \"Watcher\", Buffy repeatedly tries Merrick's patience with her frivolous nature, indifference to slaying, and sharp-tongued remarks. Merrick informs Buffy that her duty is to slay the elder vampire Lothos, who is plotting to expand his brood in Los Angeles with the aid of his loyal acolyte, Amilyn. \nMeanwhile, fellow Hemery High seniors, Pike and Benny, are out drinking when they're attacked by Amilyn. Benny is turned, but Pike is saved by Merrick. As a vampire, Benny visits his friend Pike and unsuccessfully tries to turn him. Amilyn also abducts Cassandra, a girl from Buffy's class, and sacrifices her to Lothos.\nPike decides to leave town when he realizes he is no longer safe. His plan is thwarted when he encounters Amilyn and his gang of vampires, and Buffy and Merrick rescue him. After this encounter, Pike insists on helping Buffy and they become friends despite a previous dislike of each other.\nBuffy discovers her friend Grueller is a vampire. Shortly after he is dispatched, Buffy encounters Lothos and Amilyn. Lothos easily overcomes Buffy by placing her in a hypnotic trance. Merrick intervenes and breaks the trance but is swiftly killed by Lothos when he attempts to stake him. As Merrick dies, he tells Buffy to do things her own way rather than live by the rules of others. \nAt school, Buffy attempts to explain things to her friends, but they refuse to listen, being more concerned with an upcoming school dance. Buffy tries to reason with them, but realizes they're too immature and selfish to understand the consequences of a vampire infestation.\nTrying to grapple with the responsibilities that have been thrust upon her, Buffy goes shopping to find a dress for the upcoming dance; attempting to reclaim her own ability to be immature and selfish. Pike finds her and tries to convince her that she's can't ignore her newfound responsibilities. She tries to maintain her carefree youth, denying her role as Slayer one last time. While Buffy wants to go to the dance, Pike insists that he won't go because it's frivolous.\nAt the senior dance, Buffy is dismayed to find Jeffrey has dumped her (via a message on her answering machine) and had come to the dance with her friend Jenny. While she grieves, Pike enters the dance. He asks Buffy to dance and they enjoy a romantic moment. Lothos summons his vampire followers to attack the dance (among which is Cassandra, though it is unknown what became of her later). During the attack, students and attendants try to fight off the vampires in the gym. Pike fatally electrocutes Benny after they fight, while Buffy confronts Amilyn and Lothos in the school's basement. She kills Amilyn, but Lothos starts to hypnotize her again. The trance is broken when Buffy is reminded of Merrick's last words, and she defends herself against Lothos.\nBuffy returns to the gym, and Lothos suddenly emerges with a sword; the two duel, and Buffy manages to defeat him. The survivors leave; Buffy and Pike share another dance and the couple ride away on a motorcycle.\nCast.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;* Kristy Swanson as Buffy Summers\nAppearing in uncredited roles are Ben Affleck as a basketball player, Ricki Lake as Charlotte, Seth Green as a vampire, and Alexis Arquette as the vampire DJ.\nProduction.\nWriter Whedon sold the film to country singer Dolly Parton's production company, Sandollar, in the fall of 1991. Production was limited to five weeks to accommodate Luke Perry's \"Beverly Hills, 90210\" filming schedule. Whedon was inspired by \"Night of the Comet\" for the film's story.\nWhedon was involved in an advisory role early in the production but departed after becoming dissatisfied with the direction the film was taking. Executives at 20th Century Fox removed many of Whedon's jokes, believing the humor to be too abstract for audiences. They also disliked the darker elements in Whedon's original script, wanting to make it a lighter comedy. Merrick's suicide was replaced with his being killed by Lothos, and Buffy's burning down her high school gym to kill all the vampires was eliminated altogether.\nAll this led Whedon to finally abandon the production. He has been highly critical of actor Donald Sutherland's behavior on set, describing him as entitled and difficult to work with. Sutherland had a penchant for improvising or altering his lines in the script, which director Rubel Kuzui allowed him to do freely because he was the film's most high-profile star. Whedon, by contrast, felt that this made Merrick's dialogue in the film disjointed and unintelligible.\nFilming in Los Angeles included the ballroom of the Park Plaza Hotel, where Merrick lives and trains Buffy, John Marshall High School in Los Feliz, and the gymnasium of University High School in West Los Angeles, where the high school dance and vampire attack was filmed.\nReception.\nBox office.\nThe film debuted at #5 at the North American box office and eventually grossed $16,624,456 against a $7 million production budget.\nCritical reception.\nOn review aggregator website Rotten Tomatoes, the film has an approval rating of 36%, based on 56 reviews, with an average rating of 4.4/10. The consensus reads, \"\"Buffy the Vampire Slayer\"'s supernatural coming of age tale is let down by poor directing and even poorer plotting\u2014though Kristy Swanson's and Paul Reubens' game performances still manage to slay.\" On Metacritic, the film has a weighted average score of 48 out of 100, based on 17 critics, indicating \"mixed or average\" reviews.\nHome media.\nThe film was released on VHS and LaserDisc in the United States in November 1992 and in the United Kingdom in April 1993 by Fox Video and re-released in 1995 under the 20th Century Fox Selections banner. It was released on DVD in the United States in 2001 and on Blu-ray in 2011.\nTelevision.\nThe film was taken in a different direction from what one of its writers Joss Whedon intended, and, five years later, he created the darker and acclaimed television series of the same name.\nMany of the details given in the film differ from those of the later television series. For example, Buffy's age and history are dissimilar; she is a senior in high school in the film, but the series starts with her as a sophomore. The film does portray who the Buffy of the television series was before she learned of her destiny as the Slayer: a popular but selfish and superficial cheerleader. In the film, her parents are wealthy but negligent socialites who care little for her and spend their time at parties and golf tournaments; in the television series, Buffy has a caring, newly divorced mother named Joyce. The supernatural abilities of both vampires and the Slayer are depicted differently. The vampires in the film die like humans; in the television series, they turn to dust. Unlike the television series, their faces remain human albeit pale, fanged, and with notched ears, whereas in the television series, they take on a demonic aspect, especially when newly raised. The television series suggests that new vampires must consciously learn to maintain a human appearance. In the film, Merrick has been reincarnated many times, to train many Slayers; in the television series, Watchers are mortal and specially trained for their role and mission. Merrick's British accent and the manner of his death are different when he appears in flashbacks in the television series.\nJoss Whedon has expressed his dissatisfaction with the film's interpretation of the script, stating, \"I finally sat down and had written it and somebody had made it into a movie, and I felt like\u2014well, that's not quite her. It's a start, but it's not quite the girl.\"\nAccording to the \"Official Buffy Watcher's Guide\", Whedon wrote the pilot to the television series as a sequel to his original script, which is why the television series makes references to events that did not occur in the film. In 1999, Dark Horse Comics released a graphic novel adaptation of Whedon's original script under the title \"The Origin\". Whedon stated: \"The \"Origin\" comic, though I have issues with it, CAN pretty much be accepted as canonical. They did a cool job of combining the movie script with the series, that was nice, and using the series Merrick and not a certain OTHER thespian who shall remain hated.\"\nSoundtrack.\nOther songs featured in the film but not the soundtrack album include: \"Everybody Hurts\" by R.E.M., \"In the Wind\" by War Babies, and \"Inner Mind\" by Eon.\nPossible remake.\nOn May 25, 2009, \"The Hollywood Reporter\" reported that Roy Lee and Doug Davison of Vertigo Entertainment were working with Fran Rubel Kuzui and Kaz Kuzui on a re-envisioning or relaunch of the \"Buffy\" film for the big screen. The film would not be a sequel nor prequel to the existing film or television franchise, and Joss Whedon would have no involvement in the project. None of the characters, cast, or crew from the television series would be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Whedon into taking the reins of the project. On November 22, 2010, \"The Hollywood Reporter\" confirmed that Warner Bros. had picked up the movie rights to the remake. The film was set for release sometime in 2012. 20th Century Fox, which usually holds the rights to both the \"Buffy\" and \"Angel\" television series, would retain merchandising and some distribution rights.\nThe idea of the remake caused wrath among fans of the TV series, since Whedon was not involved. The project did not have any connection with the show and would not conform to the continuity maintained with the \"Buffy the Vampire Slayer Season Eight\" and \"Season Nine\" comic book titles. Proposed shooting locations included Black Wood and other areas in rural England, due to budgetary constraints and the potential setting being outside of the city, an unusual change for the franchise.\nIn December 2011, more than a year after the official reboot announcement, the \"Los Angeles Times\" site reported that Whit Anderson, the writer picked for the new \"Buffy\" movie, had her script rejected by the producers behind the project, and that a new writer was being sought. Sources also stated that \"If you're going to bring it back, you have to do it right. [Anderson] came in with some great ideas and she had reinvented some of the lore and it was pretty cool but in the end there just wasn't enough on the page.\"\nAs of July 2018, Joss Whedon announced at San Diego Comic-Con that he was working on a sequel of the TV series and that it might feature a slayer of color.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4232", "revid": "50372922", "url": "https://en.wikipedia.org/wiki?curid=4232", "title": "Barter", "text": "Direct reciprocal exchange of goods or services without the use of money\nIn trade, barter (derived from \"bareter\") is a system of exchange in which participants in a transaction directly exchange goods or services for other goods or services without using a medium of exchange, such as money. Barter is considered one of the earliest systems of economic exchange, used before the invention of money. Economists usually distinguish barter from gift economies in many ways; barter, for example, features immediate reciprocal exchange, not one delayed in time. Barter usually takes place on a bilateral basis, but may be multilateral (if it is mediated through a trade exchange). In most developed countries, barter usually exists parallel to monetary systems only to a very limited extent. Market actors use barter as a replacement for money as the method of exchange in times of monetary crisis, such as when currency becomes unstable (such as hyperinflation or a deflationary spiral) or simply unavailable for conducting commerce.\nNo ethnographic studies have shown that any present or past society has used barter without any other medium of exchange or measurement, and anthropologists have found no evidence that money emerged from barter. Nevertheless, economists since the times of Adam Smith (1723\u20131790) often imagined pre-modern societies for the sake of showing how the inefficiency of barter explains the emergence of money and the economy, and hence the discipline of economics itself.\nEconomic theory.\nAdam Smith on the origin of money.\nAdam Smith sought to demonstrate that markets (and economies) pre-existed the state. He argued that money was not the creation of governments. Markets emerged, in his view, out of the division of labour, by which individuals began to specialize in specific crafts and hence had to depend on others for subsistence goods. These goods were first exchanged by barter. Specialization depended on trade but was hindered by the \"double coincidence of wants\" which barter requires, i.e., for the exchange to occur, each participant must want what the other has. To complete this hypothetical history, craftsmen would stockpile one particular good, be it salt or metal, that they thought no one would refuse. This is the origin of money according to Smith. Money, as a universally desired medium of exchange, allows each half of the transaction to be separated.\nBarter is characterized in Adam Smith's \"The Wealth of Nations\" by a disparaging vocabulary: \"haggling, swapping, dickering\". It has also been characterized as negative reciprocity, or \"selfish profiteering\".\nDavid Graeber's theory.\nAnthropologists such as David Graeber have argued, in contrast, \"that when something resembling barter \"does\" occur in stateless societies it is almost always between strangers.\" Barter occurred between strangers, not fellow villagers, and hence cannot be used to naturalistically explain the origin of money without the state. Since most people who engaged in trade knew each other, exchange was fostered through the extension of credit. Marcel Mauss, author of 'The Gift', argued that the first economic contracts were not to act in one's economic self-interest, and that before money, exchange was fostered through the processes of reciprocity and redistribution, not barter. Everyday exchange relations in such societies are characterized by generalized reciprocity, or a non-calculative familial \"communism\" where each takes according to their needs, and gives as they have.\nFeatures of bartering.\nOften the following features are associated with barter transactions:\nThere is a demand for things of a different kind.\nMost often, parties trade goods and services for goods or services that differ from what they are willing to forego.\nThe parties of the barter transaction are both equal and free.\nNeither party has advantage over the other, and both are free to leave the trade at any point in time.\nThe transaction happens simultaneously.\nThe goods are normally traded at the same point in time. Nonetheless delayed barter in goods may rarely occur as well. In the case of services being traded however, the two parts of the trade may be separated.\nThe transaction is transformative.\nA barter transaction \"moves objects between the regimes of value\", meaning that a good or service that is being traded may take up a new meaning or value under its recipient than that of its original owner.\nThere is no criterion of value.\nThere is no real way to value each side of the trade. There is bargaining taking place, not to do with the value of each party's good or service, but because each player in the transaction wants what is offered by the other.\nAdvantages.\nSince direct barter does not require payment in money, it can be utilized when money is in short supply, when there is little information about the credit worthiness of trade partners, or when there is a lack of trust between those trading.\nBarter is an option for those who cannot afford to store their small supply of wealth in money, especially in hyperinflation situations where money devalues quickly.\nBarter economies are usually free from interest and usury. The German-Argentine economist Silvio Gesell created a Robinson Crusoe economy thought experiment in Part V of \"The Natural Economic Order\" which showed that interest rates are a purely monetary phenomenon that tends to be absent from barter economies.\nLimitations.\nThe limitations of barter are often explained in terms of its inefficiencies in facilitating exchange in comparison to money.\nIt is said that barter is 'inefficient' because:\n For barter to occur between two parties, both parties need to have what the other wants.\n In a monetary economy, money plays the role of a measure of the value of all goods, so their values can be assessed against each other; this role may be absent in a barter economy.\n If a person wants to buy a certain amount of another's goods, but only has for payment one indivisible unit of another good which is worth more than what the person wants to obtain, a barter transaction cannot occur.\n This is related to the absence of a common measure of value, although if the debt is denominated in units of the good that will eventually be used in payment, it is not a problem.\n If a society relies exclusively on perishable goods, storing wealth for the future may be impractical. However, some barter economies rely on durable goods like sheep or cattle for this purpose.\nHistory.\nSilent trade.\nOther anthropologists have questioned whether barter is typically between \"total\" strangers, a form of barter known as \"silent trade\". Silent trade, also called silent barter, dumb barter (\"dumb\" here used in its old meaning of \"mute\"), or depot trade, is a method by which traders who cannot speak each other's language can trade without talking. However, Benjamin Orlove has shown that while barter occurs through \"silent trade\" (between strangers), it occurs in commercial markets as well. \"Because barter is a difficult way of conducting trade, it will occur only where there are strong institutional constraints on the use of money or where the barter symbolically denotes a special social relationship and is used in well-defined conditions. To sum up, multipurpose money in markets is like lubrication for machines - necessary for the most efficient function, but not necessary for the existence of the market itself.\"\nIn his analysis of barter between coastal and inland villages in the Trobriand Islands, Keith Hart highlighted the difference between highly ceremonial gift exchange between community leaders, and the barter that occurs between individual households. The haggling that takes place between strangers is possible because of the larger temporary political order established by the gift exchanges of leaders. From this, he concludes that barter is \"an atomized interaction predicated upon the presence of society\" (i.e. that social order established by gift exchange), and not typical between strangers. In rural parts of India, informal barter systems still exist, particularly in agriculture and tribal communities.\nTimes of monetary crisis.\nAs Orlove noted, barter may occur in commercial economies, usually during periods of monetary crisis. During such a crisis, currency may be in short supply, or highly devalued through hyperinflation. In such cases, money ceases to be the universal medium of exchange or standard of value. Money may be in such short supply that it becomes an item of barter itself rather than the means of exchange. Barter may also occur when people cannot afford to keep money (as when hyperinflation quickly devalues it).\nAn example of this would be during the Crisis in Bolivarian Venezuela, when Venezuelans resorted to bartering as a result of hyperinflation. The increasingly low value of bank notes, and their lack of circulation in suburban areas, meant that many Venezuelans, especially those living outside of larger cities, took to trading over their own goods for even the most basic of transactions.\nAdditionally, in the wake of the 2008 financial crisis, barter exchanges reported a double-digit increase in membership, due to the scarcity of fiat money, and the degradation of monetary system sentiment.\nExchanges.\nEconomic historian Karl Polanyi has argued that where barter is widespread, and cash supplies limited, barter is aided by the use of credit, brokerage, and money as a unit of account (i.e. used to price items). All of these strategies are found in ancient economies including Ptolemaic Egypt. They are also the basis for more recent barter exchange systems.\nWhile one-to-one bartering is practised between individuals and businesses on an informal basis, organized barter exchanges have developed to conduct third party bartering which helps overcome some of the limitations of barter. A barter exchange operates as a broker and bank in which each participating member has an account that is debited when purchases are made, and credited when sales are made.\nModern barter and trade has evolved considerably to become an effective method of increasing sales, conserving cash, moving inventory, and making use of excess production capacity for businesses around the world. Businesses in a barter earn trade credits (instead of cash) that are deposited into their account. They then have the ability to purchase goods and services from other members utilizing their trade credits \u2013 they are not obligated to purchase from those whom they sold to, and vice versa. The exchange plays an important role because they provide the record-keeping, brokering expertise and monthly statements to each member. Commercial exchanges make money by charging a commission on each transaction either all on the buy side, all on the sell side, or a combination of both. Transaction fees typically run between 8 and 15%. A successful example is International Monetary Systems, which was founded in 1985 and is one of the first exchanges in North America opened after the TEFRA Act of 1982.\nOrganized barter (retail barter).\nSince the 1930s, organized barter has been a common type of barter where company's join a barter organization (barter company) which serves as a hub to exchange goods and services without money as a medium of exchange. Similarly to brokerage houses, barter company facilitates the exchange of goods and services between member companies, allowing members to acquire goods and services by providing their own as payment. Member companies are required to sign a barter agreement with the barter company as a condition of their membership. In turn, the barter company provides each member with the current levels of supply and demand for each good and service which can be purchased or sold in the system. These transactions are mediated by barter authorities of the member companies. The barter member companies can then acquire their desired goods or services from another member company within a predetermined time. Failure to deliver the good or service within the fixed time period results in the debt being settled in cash. Each member company pays an annual membership fee and purchase and sales commission outlined in the contract. Organized barter increases liquidity for member companies as it mitigates the requirement of cash to settle transactions, enabling sales and purchases to be made with excess capacity or surplus inventory. Additionally, organized barter facilitates competitive advantage within industries and sectors. Considering the quantity of transactions depending on the supply-demand balance of the goods and services within the barter organization, member companies tend to face minimal competition within their own operating sector.\nCorporate barter.\nProducers, wholesalers and distributors tend to engage in corporate barter as a method of exchanging goods and services with companies they are in business with. These bilateral barter transactions are targeted towards companies aiming to convert stagnant inventories into receivable goods or services, to increase market share without cash investments, and to protect liquidity. However, issues arise as to the imbalance of supply and demand of desired goods and services and the inability to efficiently match the value of goods and services exchanged in these transactions.\nLabour notes.\nThe Owenite socialists in Britain and the United States in the 1830s were the first to attempt to organize barter exchanges. Owenism developed a \"theory of equitable exchange\" as a critique of the exploitative wage relationship between capitalist and labourer, by which all profit accrued to the capitalist. To counteract the uneven playing field between employers and employed, they proposed \"schemes of labour notes based on labour time, thus institutionalizing Owen's demand that human labour, not money, be made the standard of value.\" This alternate currency eliminated price variability between markets, as well as the role of merchants who bought low and sold high. The system arose in a period where paper currency was an innovation. Paper currency was an IOU circulated by a bank (a promise to pay, not a payment in itself). Both merchants and an unstable paper currency created difficulties for direct producers.\nAn alternate currency, denominated in labour time, would prevent profit taking by middlemen; all goods exchanged would be priced only in terms of the amount of labour that went into them as expressed in the maxim 'Cost the limit of price'. It became the basis of exchanges in London, and in America, where the idea was implemented at the New Harmony communal settlement by Josiah Warren in 1826, and in his Cincinnati 'Time store' in 1827. Warren ideas were adopted by other Owenites and currency reformers, even though the labour exchanges were relatively short lived.\nIn England, about 30 to 40 cooperative societies sent their surplus goods to an \"exchange bazaar\" for direct barter in London, which later adopted a similar labour note. The British Association for Promoting Cooperative Knowledge established an \"equitable labour exchange\" in 1830. This was expanded as the National Equitable Labour Exchange in 1832 on Grays Inn Road in London. These efforts became the basis of the British cooperative movement of the 1840s. In 1848, the socialist and first self-designated anarchist Pierre-Joseph Proudhon postulated a system of \"time chits\".\nMichael Linton coined the term \"local exchange trading system\" (LETS) in 1983 and for a time ran the Comox Valley LETSystems in Courtenay, British Columbia. LETS networks use interest-free local credit so direct swaps do not need to be made. For instance, a member may earn credit by doing childcare for one person and spend it later on carpentry with another person in the same network. In LETS, unlike other local currencies, no scrip is issued, but rather transactions are recorded in a central location open to all members. As credit is issued by the network members, for the benefit of the members themselves, LETS are considered mutual credit systems.\nLocal currencies.\nThe first exchange system was the Swiss WIR Bank. It was founded in 1934 as a result of currency shortages after the stock market crash of 1929. \"WIR\" is both an abbreviation of Wirtschaftsring (economic circle) and the word for \"we\" in German, reminding participants that the economic circle is also a community.\nIn Australia and New Zealand, the largest barter exchange is Bartercard, founded in 1991, with offices in the United Kingdom, United States, Cyprus, UAE, Thailand, and most recently, South Africa. Other than its name suggests, it uses an electronic local currency, the trade dollar. Since its inception, Bartercard has amassed a trading value of over US$10 billion, and increased its customer network to 35,000 cardholders.\nThe Cr\u00e9dito was a local currency started on 1 May 1995 in Bernal, Argentina, at a garage sale, which was the first of many neighbourhood barter markets () and barter clubs () that emerged in Argentina during the 1998\u20132002 Argentine great depression.\nAt the barter clubs, people could exchange goods and services, often using cr\u00e9ditos.\nAn estimated 2.5 million Argentinians used the Cr\u00e9dito between 2001 and 2003.\nThe currency started as a local exchange trading system (LETS), but was soon replaced by a number of printed currencies.\nAfter further experimentation with a LETS called \"nodine\" (from \"no dinero\", \"not money\"), it finally became the \"Cr\u00e9dito\" (Spanish for \"credit\"), a printed currency again.\nBartering in business.\nIn business, barter has the benefit that one gets to know each other, one discourages investments for rent (which is inefficient) and one can impose trade sanctions on dishonest partners.\nAccording to the International Reciprocal Trade Association, the industry trade body, more than 450,000 businesses transacted $10 billion globally in 2008 \u2013 and officials expect trade volume to grow by 15% in 2009.\nIt is estimated that over 450,000 businesses in the United States were involved in barter exchange activities in 2010. There are approximately 400 commercial and corporate barter companies serving all parts of the world. There are many opportunities for entrepreneurs to start a barter exchange. Several major cities in the U.S. and Canada do not currently have a local barter exchange. There are two industry groups in the United States, the National Association of Trade Exchanges (NATE) and the International Reciprocal Trade Association (IRTA). Both offer training and promote high ethical standards among their members. Moreover, each has created its own currency through which its member barter companies can trade. NATE's currency is known as the BANC and IRTA's currency is called Universal Currency (UC).\nIn Canada, barter continues to thrive. The largest b2b barter exchange is International Monetary Systems (IMS Barter), founded in 1985. P2P bartering has seen a renaissance in major Canadian cities through Bunz - built as a network of Facebook groups that went on to become a stand-alone bartering based app in January 2016. Within the first year, Bunz accumulated over 75,000 users in over 200 cities worldwide.\nCorporate barter focuses on larger transactions, which is different from a traditional, retail oriented barter exchange. Corporate barter exchanges typically use media and advertising as leverage for their larger transactions. It entails the use of a currency unit called a \"trade-credit\". The trade-credit must not only be known and guaranteed but also be valued in an amount the media and advertising could have been purchased for had the \"client\" bought it themselves (contract to eliminate ambiguity and risk).\nSoviet bilateral trade is occasionally called \"barter trade\", because although the purchases were denominated in U.S. dollars, the transactions were credited to an international clearing account, avoiding the use of hard cash.\nTax implications.\nIn the United States, Karl Hess used bartering to make it harder for the IRS to seize his wages and as a form of tax resistance. Hess explained how he turned to barter in an op-ed for \"The New York Times\" in 1975. However the IRS now requires barter exchanges to be reported as per the Tax Equity and Fiscal Responsibility Act of 1982. Barter exchanges are considered taxable revenue by the IRS and must be reported on a 1099-B form. According to the IRS, \"The fair market value of goods and services exchanged must be included in the income of both parties.\"\nOther countries, though, do not have the reporting requirement that the U.S. does concerning proceeds from barter transactions, but taxation is handled the same way as a cash transaction. If one barters for a profit, one pays the appropriate tax; if one generates a loss in the transaction, they have a loss. Bartering for business is also taxed accordingly as business income or business expense. Many barter exchanges require that one register as a business.\nIn countries like Australia and New Zealand, barter transactions require the appropriate tax invoices declaring the value of the transaction and its reciprocal GST component. All records of barter transactions must also be kept for a minimum of five years after the transaction is made. The Australian Taxation Office additionally considers sponsorships between businesses and influencers that are paid in non-monetary goods to be a form of taxable barter transaction.\nRecent developments.\nIn Spain (particularly the Catalonia region) there is a growing number of exchange markets. These barter markets or swap meets work without money. Participants bring things they do not need and exchange them for the unwanted goods of another participant. Swapping among three parties often helps satisfy tastes when trying to get around the rule that money is not allowed.\nOther examples are El Cambalache in San Cristobal de las Casas, Chiapas, Mexico and post-Soviet societies.\nThe recent blockchain technologies are making it possible to implement decentralized and autonomous barter exchanges that can be used by crowds on a massive scale. BarterMachine is an Ethereum smart contract based system that allows direct exchange of multiple types and quantities of tokens with others. It also provides a solution miner that allows users to compute direct bartering solutions in their browsers. Bartering solutions can be submitted to BarterMachine which will perform collective transfer of tokens among the blockchain addresses that belong to the users. If there are excess tokens left after the requirements of the users are satisfied, the leftover tokens will be given as reward to the solution miner.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4233", "revid": "51049068", "url": "https://en.wikipedia.org/wiki?curid=4233", "title": "Berthe Morisot", "text": "French artist (1841\u20131895)\nBerthe Marie Pauline Morisot (; 14 January 1841\u00a0\u2013 2 March 1895) was a French painter, printmaker and a member of the circle of painters in Paris who became known as the Impressionists.\nIn 1864, Morisot exhibited for the first time in the highly esteemed Salon de Paris, listed as a student of Joseph Guichard and Achille-Francois Oudinot. Her work was selected for exhibition in six subsequent Salons until, in 1874, she joined the \"rejected\" Impressionists in the first of their own exhibitions (15 April \u2013 15 May 1874), which included Paul C\u00e9zanne, Edgar Degas, Claude Monet, Camille Pissarro, Pierre-Auguste Renoir and Alfred Sisley. It was held at the studio of the photographer Nadar. Morisot went on to participate in all but one of the following eight impressionist exhibitions, between 1874 and 1886.\nMorisot was married to Eug\u00e8ne Manet, the brother of her friend and colleague \u00c9douard Manet.\nShe was described by art critic Gustave Geffroy in 1894 as one of \"les trois grandes dames\" (The three great ladies) of Impressionism alongside Marie Bracquemond and Mary Cassatt.\nEarly life.\nMorisot was born on 14 January 1841, in Bourges, France, into an affluent bourgeois family. Her father, Edm\u00e9 Tiburce Morisot, was the prefect (senior administrator) of the department of Cher. He also studied architecture at \u00c9cole des Beaux Arts. Her mother, Marie-Jos\u00e9phine-Corn\u00e9lie Thomas, was the great-niece of Jean-Honor\u00e9 Fragonard, one of the most prolific Rococo painters of the ancien r\u00e9gime. She had two older sisters, Yves (1838\u20131893) and Edma (1839\u20131921), plus a younger brother, Tiburce, born in 1848. The family moved to Paris in 1852, when Morisot was a child.\nIt was commonplace for daughters of bourgeois families to receive art education, so Berthe and her sisters Yves and Edma were taught privately by Geoffroy-Alphonse Chocarne and Joseph Guichard. Morisot and her sisters initially started taking lessons so that they could each make a drawing for their father for his birthday. In 1857 Guichard, who ran a school for girls in Rue des Moulins, introduced Berthe and Edma to the Louvre gallery where from 1858 they learned by copying paintings. The Morisots were not only forbidden to work at the museum unchaperoned, but they were also totally barred from formal training. Guichard also introduced them to the works of Gavarni.\nAs art students, Berthe and Edma worked closely together until 1869, when Edma married Adolphe Pontillon, a naval officer, moved to Cherbourg, and had less time to paint. Letters between the sisters show a loving relationship, underscored by Berthe's regret at the distance between them and Edma's withdrawal from painting. Edma wholeheartedly supported Berthe's continued work and their families always remained close. Edma wrote \"... I am often with you in thought, dear Berthe. I'm in your studio and I like to slip away, if only for a quarter of an hour, to breathe that atmosphere that we shared for many years...\".\nHer sister Yves married Th\u00e9odore Gobillard, a tax inspector, in 1866 and was painted by Edgar Degas as \"Madame Th\u00e9odore Gobillard\" (Metropolitan Museum of Art, New York City).\nAs a copyist at the Louvre, Morisot met and befriended other artists such as Manet and Monet. In 1861 she was introduced to Jean-Baptiste-Camille Corot, the pivotal landscape painter of the Barbizon school who also excelled in figure painting. Under Corot's influence, she took up the plein air (outdoors) method of working. By 1863 she was studying under Achille Oudinot, another Barbizon painter. In the winter of 1863\u201364 she studied sculpture under Aim\u00e9 Millet, but none of her sculptures is known to survive.\nMain periods of Morisot's work.\nTraining, 1857\u20131870.\nIt is hard to trace the stages of Morisot's training and to tell the exact influence of her teachers because she was never pleased with her work and she destroyed nearly all of the artworks she produced before 1869. Morisot began her first art lessons in 1857, and her first teacher, Geoffroy-Alphonse Chocarne, taught her the basics of drawing. After several months, Morisot began to take issue with the dull and monotonic nature of Chocarne's teaching, requesting for a new teacher. She subsequently began to take classes taught by Guichard. During this period, she drew mostly ancient classical figures. When Morisot expressed her interests in plein-air painting, Guichard sent her to follow Corot and Oudinot. Painting outdoors, she used watercolors which are easy to carry. At that time, Morisot also became interested in pastel.\nWatercolorist, 1870\u20131874.\nDuring this period, Morisot still found oil painting difficult, and worked mostly in watercolor. Her choice of colors is rather restrained; however, the delicate repetition of hues renders a balanced effect. Due to specific characteristics of watercolors as a medium, Morisot was able to create a translucent atmosphere and feathery touch, which contribute to the freshness in her paintings.\nImpressionism, 1875\u20131885.\nHaving become more confident about oil painting, Morisot worked in oil, watercolor and pastel at the same time, as Degas did. She painted very quickly but did much sketching as preparation, so she could paint \"a mouth, eyes, and a nose with a single brushstroke.\" She made countless studies of her subjects, which were drawn from her life so she became quite familiar with them. When it became inconvenient to paint outdoors, the highly finished watercolors done in the preparatory stages allowed her to continue painting indoors later. In 1874, Berthe's submission to the Salon is rejected; it would be the last time she would submit a piece to the exhibition. That same year, Berthe shows ten works at the First Impressionist Exhibition, notably being the only woman who exhibits. She exhibited with the Impressionists from 1874 onwards, only missing the exhibition in 1879 when her daughter Julie was born.\nImpressionism's alleged attachment to brilliant color, sensual surface effects, and fleeting sensory perceptions led a number of critics to assert in retrospect that this style, once primarily the battlefield of insouciant, combative males, was inherently feminine and best suited to women's weaker temperaments, lesser intellectual capabilities, and greater sensibility.\nDuring Morisot's 1874 exhibition with the Impressionists, such as Monet and Manet, Le Figaro critic Albert Wolff noted that the Impressionists consisted of \"five or six lunatics of which one is a woman...[whose] feminine grace is maintained amid the outpourings of a delirious mind.\"\nMorisot's mature career began in 1872. She found an audience for her work with Durand-Ruel, the private dealer, who bought twenty-two paintings. In 1877, she was described by the critic for \"Le Temps\" as the \"one real Impressionist in this group.\" She chose to exhibit under her full maiden name instead of using a pseudonym or her married name. As her skill and style improved, many began to rethink their opinion toward Morisot. In the 1880 exhibition, many reviews judged Morisot among the best, even including \"Le Figaro\" critic Albert Wolff.\nTurning, 1885\u20131887.\nAfter 1885, drawing began to dominate in Morisot's works. Morisot actively experimented with charcoals and color pencils. Her reviving interest in drawing was motivated by her Impressionist friends, who are known for blurring forms. Morisot put her emphasis on the clarification of the form and lines during this period. In addition, she was influenced by photography and Japonisme. She adopted the style of placing objects away from the center of the composition from Japanese prints of the time.\nSynthesis, 1887\u20131895.\nMorisot started to use the technique of squaring and the medium of tracing paper to transcribe her drawing to the canvas exactly. By employing this new method, Morisot was able to create compositions with more complicated interaction between figures. She stressed the composition and the forms while her Impressionist brushstrokes still remained. Her original synthesis of the Impressionist touch with broad strokes and light reflections, and the graphic approach featured by clear lines, made her late works distinctive.\nStyle and technique.\nBecause she was a female artist, Morisot's paintings were often labeled as being full of \"feminine charm\" by male critics, for their elegance and lightness. In 1890, Morisot wrote in a notebook about her struggles to be taken seriously as an artist: \"I don't think there has ever been a man who treated a woman as an equal and that's all I would have asked for, for I know I'm worth as much as they.\" \nHer light brushstrokes often led to critics using the verb \"effleurer\" (to touch lightly, brush against) to describe her technique. In her early life, Morisot painted in the open air as other Impressionists to look for truths in observation. Around 1880 she began painting on unprimed canvases\u2014a technique Manet and Eva Gonzal\u00e8s also experimented with at the time\u2014and her brushwork became looser. In 1888\u201389, her brushstrokes transitioned from short, rapid strokes to long, sinuous ones that define form. The outer edges of her paintings were often left unfinished, allowing the canvas to show through and increasing the sense of spontaneity. After 1885, she worked mostly from preliminary drawings before beginning her oil paintings. She often worked in oil paint, watercolors, and pastel simultaneously, and sketched using various drawing media. Morisot's works are almost always small in scale.\nMorisot creates a sense of space and depth through the use of color. Although her color palette was somewhat limited, her fellow impressionists regarded her as a \"virtuoso colorist\". She typically made expansive use of white to create a sense of transparency, whether used as a pure white or mixed with other colors. In her large painting \"The Cherry Tree\", the colors are more vivid but still emphasize the form.\nInspired by Manet's drawings, she kept the use of color to a minimum when constructing a motif. Responding to the experiments conducted by Manet and Edgar Degas, Morisot used barely tinted whites to harmonize the paintings. Like Degas, she played with three media simultaneously in one painting: watercolor, pastel, and oil paints. In the second half of her career, she learned from Renoir by mimicking his motifs. She also shared with Renoir an interest in keeping a balance between the density of figures and the atmospheric traits of light in her later works.\nSubjects.\nMorisot painted what she experienced on a daily basis. Most of her paintings include domestic scenes of family, children, ladies, and flowers, depicting what women's life was like in the late nineteenth century. Instead of portraying the public space and society, Morisot preferred private, intimate scenes. This reflects the cultural restrictions of her class and gender at that time. Like her fellow Impressionist Mary Cassatt, she focused on domestic life and portraits in which she could use family and personal friends as models, including her daughter Julie and sister Edma. The stenographic presentation of her daily life conveys a strong hope to stop the fleeting passage of time. By portraying flowers, she used metaphors to celebrate womanhood. Prior to the 1860s, Morisot painted subjects in line with the Barbizon school before turning to scenes of contemporary femininity. Paintings like \"The Cradle\" (1872), in which she depicted current trends for nursery furniture, reflect her sensitivity to fashion and advertising, both of which would have been apparent to her female audience. Her works also include landscapes, garden settings, boating scenes, and themes of boredom or ennui. Later in her career Morisot worked with more ambitious themes, such as nudes. In her late works, she often referred to the past to recall a memory from her earlier life and youth, and her departed companions.\nPersonal life.\nMorisot came from an eminent family, the daughter of a senior government official and the great-niece of Rococo artist Jean-Honor\u00e9 Fragonard. Henri Fantin-Latour, a fellow artist, introduced Morisot to Edouard Manet in 1868. She became his longtime friend and colleague, and she married his brother, Eug\u00e8ne Manet, in 1874. On 14 November 1878, she gave birth to her only child, Julie, later a painter and art collector, who posed frequently for her mother and other Impressionist artists, including Renoir and her uncle \u00c9douard.\nCorrespondence between Morisot and \u00c9douard Manet shows warm affection, and Manet gave her an easel as a Christmas present. Morisot often posed for Manet and there are several portrait paintings of Morisot such as \"Repose (Portrait of Berthe Morisot)\" and \"Berthe Morisot with a Bouquet of Violets\". Morisot died on 2 March 1895, in Paris, of pneumonia contracted while attending to her daughter Julie's similar illness, thus making Julie an orphan at the age of 16. The day before she died, Berthe wrote to Julie: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;My little Julie, I love you as I die; I shall still love you when I am dead; I beg you not to cry, this parting was inevitable. I hoped to live until you were married... Work and be good as you have always been; you have not caused me one sorrow in your little life. You have beauty, money; make good use of them... Please give a remembrance to your Aunt Edma and to your cousins... Berthe Morisot was interred in the Cimeti\u00e8re de Passy.\nIt has been speculated that there was a repressed love between Manet and Morisot, exemplified by the numerous portraits he did of her before she married his brother.\n\"This list is incomplete, you can help by expanding it with certified entries.\"\nWorks.\nSelection of works.\nThis limited selection is based in part on the book \"Berthe Morisot, Impressionist\", by Charles F. Stuckey and William P. Scott, with the assistance of Suzanne G. Lindsay, which is in turn drawn from the 1961 catalogue by Marie-Louise Bataille, Denis Rouart, and Georges Wildenstein. There are variations between the dates of execution, first showing, and purchase. Titles may vary between sources.\nArt market.\nMorisot's work sold comparatively well. She achieved the two highest prices at a H\u00f4tel Drouot auction in 1875, the \"Interior (Young Woman with Mirror)\" sold for 480 francs, and her pastel \"On the Lawn\" sold for 320 francs. Her works averaged 250 francs, the best relative prices at the auction.\nIn February 2013, Morisot became the highest priced female artist, when \"After Lunch\" (1881), a portrait of a young redhead in a straw hat and purple dress, sold for $10.9\u00a0million at a Christie's auction. The painting achieved roughly three times its upper estimate, and it exceeded the 2012 record of $10.7\u00a0million for a sculpture by Louise Bourgeois.\nLegacy.\nShe was portrayed by actress Marine Delterme in a 2012 French biographical TV film directed by Caroline Champetier. The character of Beatrice de Clerval in Elizabeth Kostova's \"The Swan Thieves\" is largely based on Morisot.\nShe was featured as the \"A First Impressionist\" in an article written by Anne Truitt in the \"New York Times\" on 3 June 1990.\nFrom Melissa Burdick Harmon, an editor at \"Biography\" magazine, \"While some of Morisot's work may seem to us today like sweet depictions of babies in cradles, at the time these images were considered extremely intimate, as objects related to infants belonged exclusively to the world of women.\"\nIn 2019, the Mus\u00e9e d'Orsay devoted a temporary exhibition to Berthe Morisot to pay tribute to her work.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4237", "revid": "50718490", "url": "https://en.wikipedia.org/wiki?curid=4237", "title": "Barnard College", "text": "Private women's college in New York City\nBarnard College, Columbia University is a private women's liberal arts college affiliated with Columbia University, located in New York City.\nBarnard College was founded in 1889 by a group of women led by young student activist Annie Nathan Meyer, who petitioned Columbia University's trustees to create an affiliated college named after Columbia's 10th president, Frederick A. P. Barnard. The college is one of the original Seven Sisters\u2014seven liberal arts colleges in the Northeastern United States that were historically women's colleges.\nBarnard College has independent admission, curricula, and finances separate from Columbia. It shares sports teams with Columbia through the Columbia\u2013Barnard Athletic Consortium, an agreement that makes Barnard the only women's college to compete in NCAA Division I athletics.\nBarnard offers bachelor of arts degree programs in about 50 areas of study. In addition to Columbia, students may also pursue elements of their education at the Juilliard School, the Manhattan School of Music, and the Jewish Theological Seminary which are also based in New York City. Its campus is located in the Upper Manhattan neighborhood of Morningside Heights, stretching along Broadway between 116th and 120th Streets. It is directly across from Columbia's main campus.\nBarnard College alumnae include leaders in science, religion, politics, the Peace Corps, medicine, law, education, communications, theater, and business. Barnard graduates have been recipients of Emmy, Tony, Grammy, Academy, and Peabody awards, Guggenheim Fellowships, MacArthur Fellowships, the Presidential Medal of Freedom, the National Medal of Science, and the Pulitzer Prize.\nHistory.\nFounding.\nFrom its founding in 1754 until the mid-1980s, Columbia College of Columbia University admitted only men for undergraduate study. Barnard College was founded in 1889 as a response to Columbia's refusal to admit women. Classes took place in a rented brownstone at 343 Madison Avenue, where a faculty of six offered instruction to 36 students.\nThe college was named after Frederick Augustus Porter Barnard, a deaf American educator and mathematician who later served as Columbia's president for over twenty years. He advocated for coeducational settings and proposed in 1879 that Columbia admit women. Columbia's Board of Trustees repeatedly rejected Barnard's suggestion, but in 1883 agreed to create a syllabus that would allow the college's students to receive degrees. The first such graduate received her bachelor's degree in 1887. A former student of the program, Annie Meyer, and other prominent New York women persuaded the board in 1889 to create a women's college connected to Columbia. Men and women were evenly represented among the founding trustees of Barnard College.\nMorningside campus.\nWhen Columbia University announced in 1892 its impending move to Morningside Heights, Barnard built a new campus nearby with gifts from Mary E. Brinckerhoff, Elizabeth Milbank Anderson and Martha Fiske. Two of these gifts were made with several stipulations attached. Brinckerhoff insisted that Barnard acquire land within 1,000 feet of the Columbia campus within the next four years. The Barnard trustees purchased land between 119th\u2013120th Streets after receiving funds for that purpose in 1895. Anderson requested that Charles A. Rich be hired. Rich designed the Milbank, Brinckerhoff, and Fiske Halls, built in 1897\u20131898; these were listed on the National Register of Historic Places in 2003. The first classes at the new campus were held in 1897. Despite Brinckerhoff's, Anderson's, and Fiske's gifts, Barnard remained in debt.\nElla Weed supervised the college in its first four years; Emily James Smith succeeded her as Barnard's first dean. Jessica Finch is credited with coining the phrase \"current events\" while teaching at Barnard College in the 1890s.\nThe college received the three blocks south of 119th Street from Anderson in 1903. Rich provided a master plan for the campus, but only Brooks Hall was built, being constructed between 1906 and 1908. None of Rich's other plans was carried out. Students' Hall, now known as Barnard Hall, was built in 1916 to a design by Arnold Brunner. Hewitt Hall was the last structure to be erected, in 1926\u20131927. All three buildings were listed on the National Register of Historic Places in 2003.\nBy the mid-20th century, Barnard had succeeded in its original goal of providing a top-tier education to women. Between 1920 and 1974, only the much larger Hunter College and University of California, Berkeley produced more women graduates who later received doctorates. In the 1970s, Barnard faced considerable pressure to merge with male only Columbia College, which was fiercely resisted by its president, Jacquelyn Mattfeld.\nPresidents.\nList of presidents and deans of Barnard College from 1889 to present:\nTable notes:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAcademics.\nBarnard students are able to pursue a bachelor of arts degree in about 50 areas of study. Joint programs for the bachelor of science and other degrees exist with Columbia University, Juilliard School, and the Jewish Theological Seminary. The most popular majors at the college by 2021 graduates were:\nEconometrics and Quantitative Economics (62)\nResearch and Experimental Psychology (56)\nHistory (43)\nEnglish Language and Literature (39)\nPolitical Science and Government (36)\nNeuroscience (33)\nArt History, Criticism and Conservation (33)\nThe liberal arts general education requirements are collectively called Foundations. Students must take two courses in the sciences (one of which must be accompanied by a laboratory course), study a single foreign language for two semesters, and take two courses in the arts/humanities as well as two in the social sciences. In addition, students must complete at least one three-credit course in the so-called \"Modes of Thinking\" series, and fulfill other requirements.\nAdmissions.\nAdmissions to Barnard are considered \"most selective\" by \"U.S. News &amp; World Report\". It is the most selective women's college in the nation; in 2017, Barnard had the lowest acceptance rate of the five Seven Sisters that remain single-sex in admissions.\nThe class of 2026's admission rate was 8% of the 12,009 applicants, the lowest acceptance rate in the institution's history. The median SAT composite score of enrolled students was 1440, with median subscores of 720 in Math and 715 in Evidence-Based Reading and Writing. The median ACT Composite score was 33.\nIn 2015, Barnard announced that it would admit transgender women who \"consistently live and identify as women, regardless of the gender assigned to them at birth\" and would continue to support and enroll those students who transitioned to male after they had already been admitted.\nThe college practices need-blind admission for domestic first-year applicants.\nRankings.\nIn 2025, \"U.S. News &amp; World Report\" ranked Barnard as tied at 14th of 211 U.S. liberal arts colleges overall. Barnard was tied for 30th for \"Best Undergraduate Teaching,\" among U.S. liberal arts colleges by \"U.S. News &amp; World Report\". \"Forbes\" ranked Barnard 73rd of 500 colleges in 2023. In 2024, \"Washington Monthly\" ranked Barnard 63rd among 194 liberal arts colleges in the U.S. based on its contribution to the public good, as measured by social mobility, research, and promoting public service.\nCampus.\nLibrary.\nWhile Barnard students have access to the libraries at Columbia University, the college has always maintained a library of its own. The Barnard Library also encompasses the Archives and Special Collections, with material that documents Barnard's history from its founding to the present day. Among the collections are the Ntozake Shange papers.\nZine collection.\nThe Barnard Zine Library is a unit of the Barnard Library and Academic Information Systems (BLAIS). Zine collections target primarily female, default queer, intentionally of color, and gender expansive topics. In 2004, it became the first zine library in the United States to be fully cataloged in the OCLC. It opened for circulation in 2008, and holds roughly 5,000 processed zines as of 2018. The library supports the student-run Barnard Zine Club.\nStudent life.\nStudent organizations.\nEvery Barnard student is part of the Student Government Association (SGA), which elects a representative student government. SGA aims to facilitate the expression of opinions on matters that directly affect the Barnard community.\nStudent groups include theater and vocal music groups, language clubs, literary magazines, a freeform radio station called WBAR, a biweekly magazine called the \"Barnard Bulletin\", Club Q, community service groups, and others.\nBarnard students can join extracurricular activities or organizations at Columbia University, while Columbia University students are allowed in most, but not all, Barnard organizations. Barnard's McIntosh Activities Council organizes various community focused events on campus, such as Big Sub and Midnight Breakfast. There are sub-committees focussed on cultural events (Mosaic), health and wellness (Wellness), networking (Network), event-planning (Community), and service (Action).\nSororities.\nBarnard students participate in various sororities. As of 2010[ [update]], Barnard does not fully recognize the National Panhellenic Conference sororities at Columbia, despite it being home to the Alpha chapter of Alpha Omicron Pi which was founded at Barnard in 1897, but it does provide some funding to account for Barnard students living in Columbia housing through these organizations.\nTraditions.\nBarnard Greek Games: One of Barnard's oldest traditions, the Barnard Greek Games were first held in 1903, and occurred annually until the Columbia University protests in 1968. Since then they have been sporadically revived. The games consist of competitions between each graduating class at Barnard, and events have traditionally included Greek poetry recitation, dance, chariot racing, and a torch race.\nTake Back the Night: Each April, Barnard and Columbia students participate in the Take Back the Night march and speak-out. This annual event grew out of a 1988 Seven Sisters conference. The march grew from less than 200 participants in 1988 to more than 2,500 in 2007.\nMidnight breakfast marks the beginning of finals week. As a highly popular event and long-standing college tradition, Midnight Breakfast is hosted by the student-run activities council, McAC (McIntosh Activities Council). In addition to providing standard breakfast foods, each year's theme is also incorporated into the menu. Past themes have included \"I YUMM the 90s,\" \"Grease,\" and \"Take Me Out to the Ballgame.\" The event is a school-wide affair as college deans, trustees and the president serve food to about a thousand students. It takes place the night before finals begin every semester.\nBig Sub: Toward the beginning of each fall semester, Barnard College supplies a 700+ feet long subway sandwich. Students from the college can take as much of the sub as they can carry. The sub has kosher, dairy free, vegetarian, and vegan sections. This event is organized by the student-run activities council, McAC.\nAcademic affiliations.\nRelationship with Columbia University.\nThe \"Barnard Bulletin\" in 1976 described the relationship between the college and Columbia University as \"intricate and ambiguous\". Barnard president Debora Spar said in 2012 that \"the relationship is admittedly a complicated one, a unique one and one that may take a few sentences to explain to the outside community\".\nOutside sources often describe Barnard as part of Columbia; \"The New York Times\" in 2013, for example, called Barnard \"an undergraduate women's college of Columbia University\". Its front gates read \"Barnard College of Columbia University.\"\nBarnard describes itself as \"both an independently incorporated educational institution and an official college of Columbia University\" that is \"one of the University's four colleges, but we're largely autonomous, with our own leadership and purse strings\", and advises students to state \"Barnard College, Columbia University\" or \"Barnard College of Columbia University\" on r\u00e9sum\u00e9s.\nColumbia refers to Barnard as one of its schools \nand an affiliated institution that is a faculty of the university. Both the college and Columbia evaluate Barnard faculty for tenure; in other words, a Barnard tenured professor is a Columbia tenured professor. \nBarnard graduates receive Columbia diplomas signed by the Barnard and the Columbia presidents; both diplomas are titled \"Cvratores Vniversitatis Colvmbiae\" ('Trustees of Columbia University'). According to the university, a Barnard College degree holds the same value as a Columbia College degree. Additionally, both\u2014Barnard College and Columbia College\u2014Columbia University diplomas are written in Latin. \nBarnard graduates are also considered members of the Columbia Alumni Association (CAA), and are eligible to receive honors such as the annual Columbia Alumni Medal.\nBarnard students wear the same graduation gown as undergraduates from Columbia College, the School of Engineering and Applied Science, and the School of General Studies, and their degrees are conferred during the University Commencement ceremony. \nBefore coeducation at Columbia.\nSmith and Columbia president Seth Low worked to open Columbia classes to Barnard students. By 1900 they could attend Columbia classes in philosophy, political science, and several scientific fields. That year, Barnard formalized an affiliation with the university that made available to its students the instruction and facilities of Columbia. Franz Boas, who taught at both Columbia and Barnard in the early 1900s, was among those faculty members who reportedly found Barnard students superior to their male Columbia counterparts. From 1955, Columbia and Barnard students could register for the other school's classes with the permission of the instructor; from 1973 no permission was needed.\nExcept for Columbia College, by the 1940s, other undergraduate and graduate divisions of Columbia University admitted women. Columbia president William J. McGill predicted in 1970, that Barnard College and Columbia College would merge within five years. In 1973, Columbia and Barnard signed a three-year agreement to increase sharing classrooms, facilities, and housing, and cooperation in faculty appointments, which they described as \"integration without assimilation\"; by the mid-1970s, most Columbia dormitories were coed. The university's financial difficulties during the decade increased its desire to merge to end what Columbia described as the \"anachronism\" of single-sex education, but Barnard resisted doing so because of Columbia's large debt, rejecting in 1975 Columbia dean Peter Pouncey's proposal to merge Barnard and the three Columbia undergraduate schools. The 1973\u20131976 chairwoman of the board at Barnard, Eleanor Thomas Elliott, led the resistance to the takeover. The college's marketing emphasized the Columbia relationship, however; the \"Bulletin\" in 1976 said that Barnard described it as identical to the one between Harvard College and Radcliffe College (\"who are merged in practically everything but name at this point\").\nAfter Barnard rejected later merger proposals from Columbia and a one-year extension to the 1973 agreement expired, in 1977, the two schools began discussing their future relationship. By 1979, the relationship had so deteriorated that Barnard officials stopped attending meetings. Because of an expected decline in enrollment, in 1980 a Columbia committee recommended that Columbia College begin admitting women without Barnard's cooperation. A 1981 committee found that Columbia was no longer competitive with other Ivy League universities without women, and that admitting women would not affect Barnard's applicant pool. That year Columbia president Michael Sovern agreed for the two schools to cooperate in admitting women to Columbia, but Barnard faculty's opposition caused president Ellen Futter to reject the agreement.\nA decade of negotiations for a Columbia-Barnard merger akin to Harvard and Radcliffe had failed. In January 1982, the two schools instead announced that Columbia College would begin admitting women in 1983, and Barnard's control over tenure for its faculty would increase; previously, a committee on which Columbia faculty outnumbered Barnard's three to two controlled the latter's tenure. Applications to Columbia rose 56% that year, making admission more selective, and nine Barnard students transferred to Columbia. Eight students admitted to both Columbia and Barnard chose Barnard, while 78 chose Columbia. Within a few years, however, selectivity rose at both schools as they received more women applicants than expected.\nAfter coeducation.\nThe Columbia-Barnard affiliation continued. As of 2012[ [update]], Barnard paid Columbia about $5\u00a0million a year under the terms of the \"interoperate relationship\", which the two schools renegotiate every 15 years. Despite the affiliation, Barnard is legally and financially separate from Columbia with an independent faculty and board of trustees. It is responsible for its own separate admissions, health, security, guidance and placement services, and has its own alumnae association. Nonetheless, Barnard students participate in the academic, social, athletic and extracurricular life of the broader university community on a reciprocal basis. The affiliation permits the two schools to share some academic resources; for example, only Barnard has an urban studies and dance department and only Columbia has a computer science department. Most Columbia classes are open to Barnard students and vice versa. Barnard students and faculty are represented in the University Senate, and student organizations such as the \"Columbia Daily Spectator\" are open to all students. Barnard students play on Columbia athletics teams, including the Ivy League Consortium, and Barnard uses Columbia email, telephone, and network services.\nBarnard athletes compete in the Ivy League (NCAA Division I) through the Columbia-Barnard Athletic Consortium, which was established in 1983. Through the arrangement, Barnard is the only women's college offering DivisionI athletics. There are 15 intercollegiate teams, and students also compete at the intramural and club levels. From 1975 to 1983, before the establishment of the Columbia-Barnard Athletic Consortium, Barnard students competed as the \"Barnard Bears\". Prior to 1975, students referred to themselves as the \"Barnard honeybears\".\nControversies.\nIn the spring of 1960, Columbia University president Grayson Kirk complained to the president of Barnard that Barnard students were wearing inappropriate clothing. The garments in question were pants and Bermuda shorts. The administration forced the student council to institute a dress code. Students would be allowed to wear shorts and pants only at Barnard and only if the shorts were no more than two inches above the knee and the pants were not tight. Barnard women crossing the street to enter the Columbia campus wearing shorts or pants were required to cover themselves with a long coat.\nIn March 1968, \"The New York Times\" ran an article on students who cohabited, identifying one of the persons they interviewed as a student at Barnard College from New Hampshire named \"Susan\". Barnard officials searched their records for women from New Hampshire and were able to determine that \"Susan\" was the pseudonym of a student (Linda LeClair) who was living with her boyfriend, a student at Columbia University. She was called before Barnard's student-faculty administration judicial committee, where she faced the possibility of expulsion. A student protest included a petition signed by 300 other Barnard women, admitting that they too had broken the regulations against cohabitating. The judicial committee reached a compromise and the student was allowed to remain in school, but was denied use of the college cafeteria and barred from all social activities. The student briefly became a focus of intense national attention. Barnard president Martha Peterson overruled the committee and expelled LeClair.\nIn February 2025, Barnard College expelled two students following their disruption of a \"History of Modern Israel\" class at Columbia University on January 21, 2025. The students interrupted the lecture taught by Professor Avi Shilon, a lecturer with Columbia University's Institute for Israel and Jewish Studies, and distributed materials which condemned the course as \"Zionist propaganda\". In response to these expulsions, on February 26, 2025, several dozen pro-Palestinian student protesters staged a sit-in at Barnard's Milbank Hall, outside the office of Dean Leslie Grinage. The protesters demanded the reversal of the expulsions, amnesty for students disciplined for pro-Palestinian actions, and a public meeting with college administrators. During the protest, a Barnard employee was physically assaulted and required hospitalization. Barnard's faculty members Nara Milanich, professor of history, and Severin Fowles, professor of anthropology and American studies, served as mediators between the protesters and the administration. The sit-in lasted for over six hours before an agreement was reached to disperse, with a private meeting between the protesters and administrators scheduled for the following day. In July, Barnard College settled its lawsuit \"Students Against Antisemitism, Inc. et al v. The Trustees of Columbia University in the City of New York et al\" which accused Barnard of not taking enough measures to combat campus antisemitism. \nNotable people.\nBarnard College has graduated many prominent leaders in science, religion, politics, the Peace Corps, medicine, law, education, communications, theater, and business as well as acclaimed actors, architects, artists, astronauts, engineers, human rights activists, inventors, musicians, philanthropists, and writers. Graduates include academic Louise Holland (1914), author Zora Neale Hurston (1928), Grace Lee Boggs, author and political activist (1935), author Patricia Highsmith (1942), television host Ronnie Eldridge (1952), Phyllis E. Grann, CEO of Penguin Putnam, U.S. Representative Helen Gahagan (1924), author Erica Jong (1963), Helene D. Gayle, Spelman College's 11th President and former chair of the Presidential Advisory Council on HIV/AIDS (1970), Susan N. Herman, president of the American Civil Liberties Union (1968), Judith Kaye, Chief Judge of the New York Court of Appeals (1958), Wilma B. Liebman, chair of the National Labor Relations Board (1971), Laurie Anderson, musician and performance artist (1969), Cynthia Nixon, actress, activist, and gubernatorial candidate (1988), author Jhumpa Lahiri (1989), Ann Brashares, author of \"The Sisterhood of the Traveling Pants\" (1989), Amy Hwang, \"The New Yorker\" cartoonist (2000), Kelly McCreary, actress from \"Grey's Anatomy\" (2003), Greta Gerwig, writer and director (2004), and Christy Carlson Romano, Disney Channel actress (2015).\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4238", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=4238", "title": "Bronx, New York", "text": ""}
{"id": "4240", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=4240", "title": "Benedictines", "text": "Roman Catholic monastic order\nThe Benedictines, officially the Order of Saint Benedict (, abbreviated as O.S.B. or OSB), are a mainly contemplative monastic order of the Catholic Church for men and for women who follow the Rule of Saint Benedict. Initiated in 529, they are the oldest of all the religious orders in the Latin Church. The male religious are also sometimes called the Black Monks, especially in English speaking countries, after the colour of their habits, although some, like the Olivetans, wear white. They were founded by Benedict of Nursia, a 6th-century Italian monk who laid the foundations of Benedictine monasticism through the formulation of his Rule. Benedict's sister Scholastica, possibly his twin, also became religious from an early age, but chose to live as a hermit. They retained a close relationship until her death.\nDespite being called an order, the Benedictines do not operate under a single hierarchy. They are instead organized as a collection of autonomous monasteries and convents, some known as abbeys. The order is represented internationally by the Benedictine Confederation, an organization set up in 1893 to represent the order's shared interests. They do not have a superior general or motherhouse with universal jurisdiction but elect an Abbot Primate to represent themselves to the Vatican and to the world.\nIn some regions, Benedictine nuns are given the title Dame in preference to Sister.\nHistorical development.\nThe monastery at Subiaco in Italy, established by Benedict of Nursia c. 529, was the first of the dozen monasteries he founded. He later founded the Abbey of Monte Cassino. There is no evidence, however, that he intended to found an order and the Rule of Saint Benedict presupposes the autonomy of each community. When Monte Cassino was sacked by the Lombards about the year 580, the monks fled to Rome, and it seems probable that this constituted an important factor in the diffusion of a knowledge of Benedictine monasticism.\nCopies of Benedict's Rule survived; around 594 Pope Gregory I spoke favorably of it. The rule is subsequently found in some monasteries in southern Gaul along with other rules used by abbots. Gregory of Tours says that at Ainay Abbey, in the sixth century, the monks \"followed the rules of Basil, Cassian, Caesarius, and other fathers, taking and using whatever seemed proper to the conditions of time and place\", and doubtless the same liberty was taken with the Benedictine Rule when it reached them. In Gaul and Switzerland, it gradually supplemented the much stricter Irish or Celtic Rule introduced by Columbanus and others. In many monasteries it eventually entirely displaced the earlier codes.\nBy the ninth century, however, the Benedictine had become the standard form of monastic life throughout the whole of Western Europe, excepting Scotland, Wales, and Ireland, where the Celtic observance still prevailed for another century or two. Largely through the work of Benedict of Aniane, it became the rule of choice for monasteries throughout the Carolingian empire.\nMonastic scriptoria flourished from the ninth through the twelfth centuries. Sacred Scripture was always at the heart of every monastic scriptorium. As a general rule those of the monks who possessed skill as writers made this their chief, if not their sole, active work. An anonymous writer of the ninth or tenth century speaks of six hours a day as the usual task of a scribe, which would absorb almost all the time available for active work in the day of a medieval monk.\nIn the Middle Ages monasteries were often founded by the nobility. Cluny Abbey was founded by William I, Duke of Aquitaine, in 910. The abbey was noted for its strict adherence to the Rule of Saint Benedict. The abbot of Cluny was the superior of all the daughter houses, through appointed priors.\nOne of the earliest reforms of Benedictine practice was that initiated in 980 by Romuald, who founded the Camaldolese community. The Cistercians branched off from the Benedictines in 1098; they are often called the \"White monks\".\nThe dominance of the Benedictine monastic way of life began to decline towards the end of the twelfth century, which saw the rise of the mendicant Franciscans and nomadic Dominicans. Benedictines by contrast, took a vow of \"stability\", which professed loyalty to a particular foundation in a particular location. Not being bound by location, the mendicants were better able to respond to an increasingly \"urban\" environment. This decline was further exacerbated by the practice of appointing a commendatory abbot, a lay person, appointed by a noble to oversee and to protect the assets of the monastery. Often, however, this resulted in the appropriation of the assets of monasteries at the expense of the community which they were intended to support.\nAustria and Germany.\nSaint Blaise Abbey in the Black Forest of Baden-W\u00fcrttemberg is believed to have been founded around the latter part of the tenth century. Between 1070 and 1073 there seem to have been contacts between St. Blaise and the Cluniac Abbey of Fruttuaria in Italy, which led to St. Blaise following the Fruttuarian reforms. The Empress Agnes was a patron of Fruttuaria, and retired there in 1065 before moving to Rome. The Empress was instrumental in introducing Fruttuaria's Benedictine customs, as practiced at Cluny, to Saint Blaise Abbey in Baden-W\u00fcrttemberg. Other houses either reformed by, or founded as priories of, St. Blasien were Muri Abbey (1082), Ochsenhausen Abbey (1093), G\u00f6ttweig Abbey (1094), Stein am Rhein Abbey (before 1123) and Pr\u00fcm Abbey (1132). It also had significant influence on the abbeys of Alpirsbach (1099), Ettenheimm\u00fcnster (1124) and Sulzburg (c.\u20091125), and the priories of Weitenau (now part of Steinen, c.\u20091100), B\u00fcrgel (before 1130) and Sitzenkirch (c.\u20091130).\nFrance.\nFleury Abbey in Saint-Beno\u00eet-sur-Loire, Loiret was founded in about 640. It is one of the most celebrated Benedictine monasteries of Western Europe, and possesses the relics of St. Benedict. Like many Benedictine abbeys it was located on the banks of a river, here the Loire. Ainey Abbey is a ninth century foundation on the Lyon peninsula. In the twelfth century on the current site there was a romanesque monastery, subsequently rebuilt.\nThe seventeenth century saw a number of Benedictine foundations for women, some dedicated to the indigent to save them from a life of exploitation, others dedicated to the Perpetual Adoration of the Blessed Sacrament such as the one established by Catherine de Bar (1614\u20131698). In 1688 Dame Mechtilde de Bar assisted Marie Casimire Louise de La Grange d'Arquien, queen consort of Poland, to establish a Benedictine foundation in Warsaw.\nAbbeys were among the institutions of the Catholic Church swept away during the French Revolution. Monasteries and convents were again allowed to form in the 19th century under the Bourbon Restoration. Later that century, under the Third French Republic, laws were enacted preventing religious teaching. The original intent was to allow secular schools. Thus in 1880 and 1882, Benedictine teaching monks were effectively exiled; this was not completed until 1901.\nIn 1898 Marie-Ad\u00e8le Garnier, in religion, Mother Marie de Saint-Pierre, founded in Montmartre (\"Mount of the Martyr\"), Paris a Benedictine house. However, the Waldeck-Rousseau's \"Law of Associations\", passed in 1901, placed severe restrictions on religious bodies which were obliged to leave France. Garnier and her community relocated to another place associated with executions, this time it was in London, near the site of Tyburn tree where 105 Catholic martyrs\u2014including Saint Oliver Plunkett and Saint Edmund Campion had been executed during the English Reformation. A stone's throw from Marble Arch, the Tyburn Convent is now the Mother House of the Congregation.\nPoland and Lithuania.\nBenedictines are thought to have arrived in the Kingdom of Poland in the 11th-century. One of the earliest foundations is Tyniec Abbey on a promontory by the Vistula river. The Tyniec monks led the translation of the Bible into Polish vernacular. Other surviving Benedictine houses can be found in Stary Krak\u00f3w Village, Biskup\u00f3w, Lubi\u0144. Older foundations are in Mogilno, Trzemeszno, \u0141\u0119czyca, \u0141ysa G\u00f3ra and in Opactwo, among others. In the Middle Ages the city of P\u0142ock, also on the Vistula, had a successful monastery, which played a significant role in the local economy. In the 18th-century benedictine convents were opened for women, notably in Warsaw's New Town.\nA 15th-century Benedictine foundation can be found in Senieji Trakai, a village in Eastern Lithuania.\nSwitzerland.\nKloster Rheinau was a Benedictine monastery in Rheinau in the Canton of Z\u00fcrich, Switzerland, founded in about 778. \nEinsiedeln Abbey is a Benedictine monastery in Einsiedeln. The abbey of Our Lady of the Angels was founded in 1120.\nUnited Kingdom.\nThe English Benedictine Congregation is the oldest of the nineteen Benedictine congregations. Through the influence of Wilfrid, Benedict Biscop, and Dunstan, the Benedictine Rule spread rapidly, and in the North it was adopted in most of the monasteries that had been founded by the Celtic missionaries from Iona. Many of the episcopal sees of England were founded and governed by the Benedictines, and no fewer than nine of the old cathedrals were served by the black monks of the priories attached to them. Monasteries served as hospitals and places of refuge for the weak and homeless. The monks studied the healing properties of plants and minerals to alleviate the sufferings of the sick.\nDuring the English Reformation, all monasteries were dissolved and their lands confiscated by the Crown, forcing those who wished to continue in the monastic life to flee into exile on the Continent. During the 19th century English members of these communities were able to return to England.\nSt. Mildred's Priory, on the Isle of Thanet, Kent, was built in 1027 on the site of an abbey founded in 670 by the daughter of the first Christian King of Kent. Currently the priory is home to a community of Benedictine nuns. Five of the most notable English abbeys are the Basilica of St Gregory the Great at Downside, commonly known as Downside Abbey, The Abbey of St Edmund, King and Martyr commonly known as Douai Abbey in Upper Woolhampton, Reading, Berkshire, Ealing Abbey in Ealing, West London, and Worth Abbey. Prinknash Abbey, used by Henry VIII as a hunting lodge, was officially returned to the Benedictines four hundred years later, in 1928. During the next few years, so-called Prinknash Park was used as a home until it was returned to the order.\nSt. Lawrence's Abbey in Ampleforth, Yorkshire was founded in 1802. In 1955, Ampleforth set up a daughter house, a priory at St. Louis, Missouri which became independent in 1973 and became Saint Louis Abbey in its own right in 1989.\nAs of 2015, the English Congregation consists of three abbeys of nuns and ten abbeys of monks. Members of the congregation are found in England, Wales, the United States of America, Peru and Zimbabwe.\nIn England there are also houses of the Subiaco Cassinese Congregation: Farnborough, Prinknash, and Chilworth: the Solesmes Congregation, Quarr and St Cecilia's on the Isle of Wight, as well as a diocesan monastery following the Rule of Saint Benedict: The Community of Our Lady of Glastonbury.\nSince the Oxford Movement, there has also been a modest flourishing of Benedictine monasticism in the Anglican Church and Protestant Churches. Anglican Benedictine Abbots are invited guests of the Benedictine Abbot Primate in Rome at Abbatial gatherings at Sant'Anselmo.\nIn 1168 local Benedictine monks instigated the anti-semitic blood libel of Harold of Gloucester as a template for explaining child deaths. According to historian Joe Hillaby, the blood libel of Harold was crucially important because for the first time an unexplained child death occurring near the Easter festival was arbitrarily linked to Jews in the vicinity by local Christian churchmen: \"they established a pattern quickly taken up elsewhere. Within three years the first ritual murder charge was made in France.\"\nMonastic libraries in England.\nThe forty-eighth Rule of Saint Benedict prescribes extensive and habitual \"holy reading\" for the brethren. Three primary types of reading were done by the monks in medieval times. Monks would read privately during their personal time, as well as publicly during services and at mealtimes. In addition to these three mentioned in the Rule, monks would also read in the infirmary. Monasteries were thriving centers of education, with monks and nuns actively encouraged to learn and pray according to the Benedictine Rule. Rule 38 states that 'these brothers' meals should usually be accompanied by reading, and that they were to eat and drink in silence while one read out loud.\nBenedictine monks were not allowed worldly possessions, thus necessitating the preservation and collection of sacred texts in monastic libraries for communal use. For the sake of convenience, the books in the monastery were housed in a few different places, namely the sacristy, which contained books for the choir and other liturgical books, the rectory, which housed books for public reading such as sermons and lives of the saints, and the library, which contained the largest collection of books and was typically in the cloister.\nThe first record of a monastic library in England is in Canterbury. To assist with Augustine of Canterbury's English mission, Pope Gregory the Great gave him nine books which included the Gregorian Bible in two volumes, the Psalter of Augustine, two copies of the Gospels, two martyrologies, an Exposition of the Gospels and Epistles, and a Psalter. Theodore of Tarsus brought Greek books to Canterbury more than seventy years later, when he founded a school for the study of Greek.\nUnited States.\nThe first Benedictine to live in the United States was Pierre-Joseph Didier. He came to the United States in 1790 from Paris and served in the Ohio and St. Louis areas until his death. The first actual Benedictine monastery founded was Saint Vincent Archabbey, located in Latrobe, Pennsylvania. It was founded in 1832 by Boniface Wimmer, a German monk, who sought to serve German immigrants in America. In 1856, Wimmer started to lay the foundations for St. John's Abbey in Minnesota. In 1876, Herman Wolfe, of Saint Vincent Archabbey established Belmont Abbey in North Carolina. By the time of his death in 1887, Wimmer had sent Benedictine monks to Kansas, New Jersey, North Carolina, Georgia, Florida, Alabama, Illinois, and Colorado.\nWimmer also asked for Benedictine sisters to be sent to America by St. Walburg Convent in Eichst\u00e4tt, Bavaria. In 1852, Sister Benedicta Riepp and two other sisters founded St. Marys, Pennsylvania. Soon they would send sisters to Michigan, New Jersey, and Minnesota.\nBy 1854, Swiss monks began to arrive and founded St. Meinrad Abbey in Indiana, and they soon spread to Arkansas and Louisiana. They were soon followed by Swiss sisters.\nThere are now over 100 Benedictine houses across America. Most Benedictine houses are part of one of four large Congregations: American-Cassinese, Swiss-American, St. Scholastica, and St. Benedict. The congregations mostly are made up of monasteries that share the same lineage. For instance the American-Cassinese congregation included the 22 monasteries descended from Boniface Wimmer.\nBenedictine vows and life.\nA sense of community has been the defining characteristic of the order since the beginning. To that end, section 17 in chapter 58 of the Rule of Saint Benedict specifies the solemn vows candidates joining a Benedictine community are required to make: a vow of stability (to remain in the same community), and to adopt a \"conversion of habits\", in Latin, \"conversatio morum\" and obedience to the community's superior. The \"Benedictine vows\" are equivalent to the evangelical counsels accepted by all candidates entering a religious order. The interpretation of \"conversatio morum\" understood as \"conversion of the habits of life\" has generally been replaced by notions such as adoption of a monastic manner of life, drawing on the Vulgate's use of \"conversatio\" as indicating \"citizenship\" or \"local customs\", see Philippians 3:20. The Rule enjoins monks and nuns \"to live in this place as a religious, in obedience to its rule and to the abbot or abbess.\"\nBenedictine abbots and abbesses have jurisdiction over their abbey and thus canonical authority over the monks or nuns who are resident. This authority includes the power to assign duties, to decide which books may or may not be read, to regulate comings and goings, and to punish and to excommunicate, in the sense of an enforced isolation from the monastic community.\nA tight communal timetable\u00a0\u2013 the horarium\u00a0\u2013 is meant to ensure that the time given by God is not wasted but used in God's service, whether for prayer, work, meals, spiritual reading or sleep. The order's motto is \"Ora et Labora\" \"pray and work\".\nAlthough Benedictines do not take a vow of silence, hours of strict silence are set, and at other times silence is maintained as much as is practically possible. Social conversations tend to be limited to communal recreation times. Such details, like other aspects of the daily routine of a Benedictine house are left to the discretion of the superior, and are set out in its \"customary\", the code adopted by a particular Benedictine house by adapting the Rule to local conditions.\nAccording to the norms of the 1983 Code of Canon Law, a Benedictine abbey is a \"religious institute\" and its members therefore participate in consecrated life which Canon 588 \u00a71 explains is intrinsically \"neither clerical nor lay.\" Males in consecrated life, however, may be ordained.\nBenedictines' rules contain a reference to ritual purification, which is inspired by Benedict's encouragement of bathing. Benedictine monks have played a role in the development and promotion of spas.\nOrganization.\nBenedictine monasticism differs from other Christian religious orders in that as congregations sometimes with several houses, some of them in other countries, they are not bound into a unified religious order headed by a \"Superior General\". Each Benedictine congregation is autonomous and governed by an abbot or abbess.\nThe autonomous houses are characterised by their chosen charism or specific dedication to a particular devotion. For example, In 1313 Bernardo Tolomei established the Order of Our Lady of Mount Olivet. The community adopted the Rule of Saint Benedict and received canonical approval in 1344. The Olivetans are part of the Benedictine Confederation. Other specialisms, such as Gregorian chant as at Solesmes in France, or Perpetual Adoration of the \"Holy Sacrament\" have been adopted by different houses, as at the Warsaw Convent, or the Adorers of the Sacred Heart of Montmartre at Tyburn Convent in London. Other houses have dedicated themselves to books, reading, writing and printing them as at Stanbrook Abbey in England. Others still are associated with the places where they were founded or their founders centuries ago, hence Cassinese, Subiaco, Camaldolese or Sylvestrines.\nAll Benedictine houses became federated in the Benedictine Confederation brought into existence by Pope Leo XIII's Apostolic Brief \"Summum semper\" on 12 July 1893. Pope Leo also established the office of Abbot Primate as the abbot elected to represent this Confederation at the Vatican and to the world. The headquarters of the Benedictine Confederation and the Abbot Primate is the Primatial Abbey of Sant'Anselmo built by Pope Leo XIII in Rome.\nOther orders.\nThe Rule of Saint Benedict is also used by a number of religious orders that began as reforms of the Benedictine tradition such as the Cistercians and Trappists. These groups are separate congregations and not members of the Benedictine Confederation.\nAlthough most Benedictines are Roman Catholic, there are also other communities that follow the Rule of Saint Benedict. For example, of an estimated 2,400 celibate Anglican religious (1,080 men and 1,320 women) in the Anglican Communion as a whole, some have adopted the Rule of Benedict. Likewise, such communities can be found in the Eastern Orthodox Church, and the Lutheran Church.\nNotable Benedictines.\nIndividuals are arranged in chronological order by date of death if deceased, and by date of birth if alive.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4241", "revid": "50764492", "url": "https://en.wikipedia.org/wiki?curid=4241", "title": "Bayezid I", "text": "Sultan of the Ottoman Empire from 1389 to 1402\nBayezid I (; ), also known as Bayezid the Thunderbolt (; ; c.\u20091360 \u2013 8 March 1403), was the sultan of the Ottoman Empire from 1389 to 1402. He adopted the title of \"Sultan-i R\u00fbm\", \"R\u00fbm\" being the Arabic name for the Eastern Roman Empire. In 1394, Bayezid unsuccessfully besieged Constantinople. Bayezid vanquished all the Beyliks and proceeded to conquer and vassalize the entirety of Anatolia. In 1402, he once more besieged Constantinople, appearing to find success, but he ultimately withdrew due to the invasion of the Turco-Mongol conqueror Timur. He defeated the crusaders at the Battle of Nicopolis in what is now Bulgaria in 1396. He was later defeated and captured by Timur at the Battle of Ankara in 1402 and died in captivity in March 1403, which triggered the Ottoman Interregnum.\nBiography.\nBayezid was the son of Murad I and his Greek wife, G\u00fcl\u00e7i\u00e7ek Hatun. His first major role was as governor of K\u00fctahya, a city that he earned by marrying the daughter of a Germiyanid ruler, Devlet\u015fah. He was an impetuous soldier, earning the nickname \"Thunderbolt\" in a battle against the Karamanids.\nBayezid ascended to the throne following the death of his father, Murad I, who was killed by Serbian knight Milo\u0161 Obili\u0107 during (15 June), or immediately after (16 June), the Battle of Kosovo in 1389, soon after which Serbia became a vassal of the Ottoman Sultanate. Immediately after obtaining the throne, he had his younger brother strangled to avoid a plot. In 1390, Bayezid took as a wife Princess Olivera Despina, the daughter of Prince Lazar of Serbia, who also lost his life in Kosovo. Bayezid recognized Stefan Lazarevi\u0107, the son of Lazar, as the new Serbian leader - later despot - with considerable autonomy.\n Upper Serbia resisted the Ottomans until Bayezid captured Skopje in 1391, converting the city into an important base of operations.\nEfforts to unify Anatolia.\nMeanwhile, Bayezid began unifying Anatolia under his rule. Forcible expansion into Muslim territories could have endangered the Ottoman relationship with the gazis, who were an important source of warriors for this ruling house on the European frontier. Thus Bayezid began the practice of first securing \"fatwas\", or legal rulings from Islamic scholars, to justify wars against these Muslim states. However, Bayezid doubted the loyalty of his Muslim Turkish followers, so he relied heavily on his Serbian and Byzantine vassal troops in these conquests.\nIn a single campaign over the summer and fall of 1390, Bayezid conquered the beyliks of Aydin, Saruhan and Menteshe. His major rival Sulayman, the emir of Karaman, responded by allying himself with the ruler of Sivas, Kadi Burhan al-Din and the remaining Turkish beyliks. Nevertheless, Bayezid pushed on and overwhelmed the remaining beyliks (Hamid, Teke, and Germiyan), as well as taking the cities of Ak\u015fehir and Ni\u011fde, as well as their capital Konya from the Karaman. At this point, Bayezid accepted peace proposals from Karaman (1391), concerned that further advances would antagonize his Turkoman followers and lead them to ally with Kadi Burhan al-Din. Once peace had been made with Karaman, Bayezid moved north against Kastamonu which had given refuge to many fleeing from his forces, and conquered both that city as well as Sinop. However, his subsequent campaign was stopped by Burhan al-Din at the Battle of K\u0131rkdilim.\nFrom 1389 to 1395 he conquered Bulgaria and Northern Greece. In 1394 Bayezid crossed the River Danube to attack Wallachia, ruled at that time by Mircea the Elder. The Ottomans were superior in number, but on 10 October 1394 (or 17 May 1395), in the Battle of Rovine, on forested and swampy terrain, the Wallachians won the fierce battle and prevented Bayezid's army from advancing beyond the Danube.\nIn 1394, Bayezid laid siege to Constantinople, the capital of the Byzantine Empire. Anadoluhisar\u0131 fortress was built between 1393 and 1394 as part of preparations for the second Ottoman siege of Constantinople, which took place in 1395. On the urgings of the Byzantine emperor Manuel II Palaeologus, a new crusade was organized to defeat him. This proved unsuccessful: in 1396 the Christian allies, under the leadership of the King of Hungary and future Holy Roman Emperor (in 1433) Sigismund, were defeated in the Battle of Nicopolis. Bayezid built the magnificent Ulu Cami in Bursa, to celebrate this victory.\nThus the siege of Constantinople continued, lasting until 1402. The beleaguered Byzantines had their reprieve when Bayezid fought the Timurid Empire in the east. At this time, the empire of Bayezid included Thrace (except Constantinople), Macedonia, Bulgaria, and parts of Serbia in Europe. In Asia, his domains extended to the Taurus Mountains. His army was considered one of the best in the Islamic world.\nClash with Timur.\nIn 1397, Bayezid defeated the emir of Karaman in Ak\u00e7ay, killing him and annexing his territory. In 1398, the sultan conquered the Djanik emirate and the territory of Burhan al-Din, violating the accord with the Turco-Mongol emir Timur. Finally, Bayezid occupied Elbistan and Malatya.\nIn 1400, Timur succeeded in rousing the local Turkic beyliks who had been vassals of the Ottomans to join him in his attack on Bayezid, who was also considered one of the most powerful rulers in the Muslim world during that period. Years of insulting letters had passed between Timur and Bayezid. Both rulers insulted each other in their own way while Timur preferred to undermine Bayezid's position as a ruler and play down the significance of his military successes.\nThis is the excerpt from one of Timur's letters addressed to the Ottoman sultan:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Believe me, you are but pismire ant: don't seek to fight the elephants for they'll crush you under their feet. Shall a petty prince such as you are contend with us? But your rodomontades [braggadocio] are not extraordinary; for a Turcoman never spoke with judgement. If you don't follow our counsels you will regret it.\nIn the fateful Battle of Ankara, on 20 July 1402, the Ottoman army was defeated. Bayazid tried to escape, but was captured and taken to Timur. Historians describe their first meeting as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;When Timur saw Bayezid, he laughed. Bayezid, offended by this laugh, told Timur that it was indecent to laugh at misfortune; to which Timur replied: \"It is clear then that fate does not value power and possession of vast lands if it distributes them to cripples: to you, the crooked, and to me, the lame.\"\nMany writers claim that Bayezid was mistreated by the Timurids. However, writers and historians from Timur's own court reported that Bayezid was treated well, and that Timur even mourned his death. One of Bayezid's sons, Mustafa \u00c7elebi, was captured with him and held captive in Samarkand until 1405.\nFour of Bayezid's sons, specifically S\u00fcleyman \u00c7elebi, \u0130sa \u00c7elebi, Mehmed \u00c7elebi, and Musa \u00c7elebi, however, escaped from the battlefield and later started a civil war for the Ottoman throne known as the Ottoman Interregnum. After Mehmed's victory, his coronation as Mehmed I, and the deaths of the other three, Bayezid's other son Mustafa \u00c7elebi emerged from hiding and began two failed rebellions against his brother Mehmed and, after Mehmed's death, his nephew Murad II.\nBayezid in captivity.\nIn Europe, the legend of Bayezid's humiliation in captivity was very popular. He was allegedly chained, and forced to watch how his beloved wife, Olivera, served Timur at dinner. According to a legend, Timur took Bayezid with himself everywhere in a barred palanquin or cage, humiliating him in various ways, used Bayezid as a support under his legs, and at dinner had him placed under the table where bones were thrown at him.\nDifferent versions on Bayezid's death existed, too. One of them mentioned the suicide of Bayezid. Allegedly, the Sultan committed suicide through hitting his head against the bars of his cell or taking poison. The version was promoted by Ottoman historians: Lutfi Pasha, Ashik Pasha-Zade. There was also a version where Bayezid was supposedly poisoned on Timur's order. This is considered unlikely, because there is evidence that the Turco-Mongol ruler entrusted the care of Bayezid to his personal doctors.\nIn the descriptions of contemporaries and witnesses of the events, neither a cell nor humiliation is mentioned.\nGerman traveller and writer Johann Schiltberger did not write anything about the cell, bars or violent death. Another contemporary, Jean II Le Maingre, who witnessed Bayezid's captivity, wrote nothing about the cell or poisoning either. Clavijo, who came to Timur's court in 1404 as part of the embassy and visited Constantinople on his return trip, also did not mention the cell. All Greek sources of the first decade of the 15th century are equally silent about the cell. Sharafaddin Yazdi (d.\u20091454) in \"Zafar-nama\" wrote that Bayezid was treated with respect, and at his request, Turco-Mongols found his son among the captives and brought him to his father. Regarding Bayezid's wife, Sharafaddin wrote that Timur sent her and his daughters to her husband. Olivera allegedly became a Muslim under the influence of Timur.\nFirst references to a disrespectful attitude towards Bayazid appear in the works of ibn Arabshah (1389\u20131450) and Constantine of Ostrovica. Ibn Arabshah wrote that \"Bayezid's heart was broken to pieces\" when he saw that his wives and concubines were serving at a banquet.\nIbn Arabshah wrote the following about the captivity of Bayezid:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Ibn Usman became a prey and was locked up like a bird in a cage.\nHowever, this is just a \"flowery style\", and not a real cell. According to literary historian H.A.R. Gibb, \"the flowery elegance of style has also affected historiography. Most of the authors of the Timurid era succumbed to its influence .\"\nConstantine of Ostrovica wrote neither about the cell, nor about the nudity of Bayezid's wife; though he did write that Bayezid committed suicide. In the story of Constantine, just like in that of ibn Arabshah, the sultan was so struck by the fact that his wife carried wine to a feast that he poisoned himself with a poison from his ring.\nOttoman historian Mehmed Neshri (1450\u20131520) described Bayezid's imprisonment and mentioned the cell twice. According to him, Timur asked Bayezid what he would do in Timur's place with regard to the captive. \"I would have planted him in an iron cage,\" Bayezid answered. To which Timur replied: \"This is a bad answer.\" He ordered to prepare the cage and the Sultan was put into it.\nThe complete set of legends may perhaps be found in the work of Pope Pius II \"Asiae Europaeque elegantissima descriptio\", written in 1450\u20131460 (published in 1509): Bayezid is kept in a cage, fed with garbage under the table, Timur uses Bayezid as a support to get on or off a horse. Further development can be found in later authors, such as Theodore Spandounes. The first version of his story was written in Italian and completed in 1509, and a French translation was published in 1519. In these versions of the text, Spandounes wrote only about the golden chains and that the sultan was used as a stand. Spandounes added the cell only in later versions of the text. Later versions of the text also include a description of the public humiliation of Bayezid's wife:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He had a wife of Ildrim [Y\u0131ld\u0131r\u0131m, i.e., Bayezid], who was also a captive. They ripped off her clothes to the navel, exposing shameful areas. And he (Timur) made her serve food to him and his guests like that.\nFamily.\nConsorts.\nBayezid I had at least nine consorts:\nSons.\nBayezid I had at least twelve sons: \nDaughters.\nBayezid I had at least five daughters:\nPersonality.\nAccording to the British orientalist, Lord Kinross, Bayezid was distinguished by haste, impulsivity, unpredictability and imprudence. He cared little for state affairs, which he entrusted to his governors. As Kinross writes, between campaigns Bayezid was often engaged in pleasures: gluttony, drunkenness and debauchery. The court of the sultan was famous for its luxury and was comparable to the Byzantine court during its heyday.\nAt the same time, the sultan was a talented commander. Despite his lust for earthly pleasures, Bayezid was a religious man and used to spend hours in his personal mosque in Bursa. He also kept Islamic theologians in his circle.\nIn the words of the contemporary Greek historian Doukas:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[Bayezid] was a feared man, precipitate in deeds of war, a persecutor of Christians as no other around him, and in the religion of the Arabs a most ardent disciple of Muhammad, whose unlawful commandments were observed to the utmost, never sleeping, spending his nights contriving intrigues and machinations against the rational flock of Christ... His purpose was to increase the nation of the Prophet and to decrease that of the Romans. Many cities and provinces did he add to the dominion of the Muslims.\nEvaluation of rule.\nBayezid managed to expand the territory of the Ottoman Empire to the Danube and the Euphrates. However, his reign culminated with a humiliating defeat at Ankara, whereby the empire was reduced to the size of a beylik from the time of Orhan. This small territory was divided between Bayezid's two sons by Timur and many beyliks regained their independence. The defeat at Ankara marked the beginning of the Ottoman interregnum, which lasted 10 years.\nIn fiction.\nThe defeat of Bayezid became a popular subject for later Western European writers, composers, and painters. They embellished the legend that he was taken by Timur to Samarkand with a cast of characters to create an oriental fantasy that has maintained its appeal over the years. Christopher Marlowe's play \"Tamburlaine the Great\" was first performed in London in 1587, three years after the formal opening of English-Ottoman trade relations when William Harborne sailed for Constantinople as an agent of the Levant Company.\nIn 1648, the play \"Le Gran Tamerlan et Bejezet\" by Jean Magnon appeared in London, and in 1725, Handel's \"Tamerlano\" was first performed and published in London; Vivaldi's version of the story, \"Bajazet\", was written in 1735. Magnon had given Bayezid an intriguing wife and daughter; the Handel and Vivaldi renditions included, as well as Tamerlane and Bayezid and his daughter, a prince of Byzantium and a princess of Trebizond (Trabzon) in a passionate love story. A cycle of paintings in Schloss Eggenberg, near Graz in Austria, translated the theme to a different medium; this was completed in the 1670s shortly before the Ottoman army attacked the Habsburgs in central Europe.\nThe historical novel \"The Grand Cham\" (1921) by Harold Lamb focuses on the quest of its European hero to gain the assistance of Tamerlane in defeating Bayezid. Bayezid (spelled Bayazid) is a central character in the Robert E. Howard story \"Lord of Samarcand,\" where he commits suicide at Tamerlane's victory banquet. Bayazid is a main character in the novel \"The Walls of Byzantium\" (2013) by James Heneage.\nIn popular culture.\nSultan Bayezid was portrayed in the Serbian 1989 historical drama film \"Battle of Kosovo\", as a participant of the Battle of Kosovo by actor Branislav Le\u010di\u0107, and in the Romanian historical drama \"Mircea (Proud heritage)\" by Ion Ritiu as a young Sultan who fought in the battles of Rovine, Nicopolis and Angora.\nIn the 29th Degree of the Scottish Rite, Northern Masonic Jurisdiction, Bayezid appears as a central figure in a drama that is historical fiction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4242", "revid": "43623777", "url": "https://en.wikipedia.org/wiki?curid=4242", "title": "Bayezid II", "text": "Sultan of the Ottoman Empire from 1481 to 1512\nBayezid II (; ; 3 December 1447 \u2013 26 May 1512) was the sultan of the Ottoman Empire from 1481 to 1512. During his reign, Bayezid consolidated the Ottoman Empire, thwarted a pro-Safavid rebellion and finally abdicated his throne to his son, Selim I. Bayezid evacuated Sephardi Jews from Spain following the fall of the Nasrid Kingdom of Granada and the proclamation of the Alhambra Decree and resettled them throughout Ottoman lands, especially in Salonica.\nEarly life.\nBayezid II was the son of \u015eehzade Mehmed (later Mehmed II) and G\u00fclbahar Hatun, an Albanian concubine. At the time he was born, his grandfather Murad II was Sultan. When his grandfather died in 1451, his father became Sultan.\nThere are sources that claim that Bayezid was the son of Sitti\u015fah Hatun, due to the two women's common middle name, M\u00fckrime. This would make Ay\u015fe Hatun, one of Bayezid's consorts, a first cousin of Bayezid II. However, the marriage of Sitti\u015fah Hatun took place two years after Bayezid was born and the whole arrangement was not to Mehmed's liking.\nBorn in Demotika, Bayezid II was educated in Amasya and later served there as a bey for 27 years. In 1473, he fought in the Battle of Otlukbeli against the Aq Qoyunlu.\nFight for the throne.\nBayezid II's overriding concern was the quarrel with his brother Cem Sultan, who claimed the throne and sought military backing from the Mamluks in Egypt. Karamani Mehmed Pasha, latest grand vizier of Mehmed II, informed him of the death of the Sultan and invited Bayezid to ascend the throne. Having been defeated by his brother's armies, Cem sought protection from the Knights of St. John in Rhodes. Eventually, the Knights handed Cem over to Pope Innocent VIII (1484\u20131492). The Pope thought of using Cem as a tool to drive the Turks out of Europe, but as the papal crusade failed to come to fruition, Cem died in Naples.\nReign.\nBayezid II ascended the Ottoman throne in 1481. Like his father, Bayezid II was a patron of western and eastern culture. Unlike many other sultans, he worked hard to ensure a smooth running of domestic politics, which earned him the epithet of \"the Just\". Throughout his reign, Bayezid II engaged in numerous campaigns to conquer the Venetian possessions in Morea, accurately defining this region as the key to future Ottoman naval power in the Eastern Mediterranean. In 1497, he went to war with Poland and decisively defeated the 80,000 strong Polish army during the Moldavian campaign. The last of these wars ended in 1501 with Bayezid II in control of the whole Peloponnese. Rebellions in the east, such as that of the Qizilbash, plagued much of Bayezid II's reign and were often backed by the shah of Iran, Ismail I, who was eager to promote Shi'ism to undermine the authority of the Ottoman state. Ottoman authority in Anatolia was indeed seriously threatened during this period and at one point Bayezid II's vizier, Had\u0131m Ali Pasha, was killed in battle against the \u015eahkulu rebellion. Had\u0131m Ali Pasha's death prompted a power vacuum. As a result, many important statesmen secretly pledged allegiance to Kinsman Karab\u0153cu Pasha (Turkish: \"Karab\u00f6c\u00fc Kuzen Pa\u015fa\") who made his reputation in conducting espionage operations during the Fall of Constantinople in his youth.\nJewish and Muslim immigration.\nIn July 1492, the new state of Spain expelled its Jewish and Muslim populations as part of the Spanish Inquisition. Bayezid II sent out the Ottoman Navy under the command of admiral Kemal Reis to Spain in 1492 in order to evacuate them safely to Ottoman lands. He sent out proclamations throughout the empire that the refugees were to be welcomed. He granted the refugees the permission to settle in the Ottoman Empire and become Ottoman citizens. He ridiculed the conduct of Ferdinand II of Aragon and Isabella I of Castile in expelling a class of people so useful to their subjects. \"You venture to call Ferdinand a wise ruler,\" he said to his courtiers, \"he who has impoverished his own country and enriched mine!\" Bayezid addressed a firman to all the governors of his European provinces, ordering them not only to refrain from repelling the Spanish refugees, but to give them a friendly and welcome reception. He threatened with death all those who treated the Jews harshly or refused them admission into the empire. Moses Capsali, who probably helped to arouse the sultan's friendship for the Jews, was most energetic in his assistance to the exiles. He made a tour of the communities and was instrumental in imposing a tax upon the rich, to ransom the Jewish victims of the persecution.\nThe Muslims and Jews of al-Andalus contributed much to the rising power of the Ottoman Empire by introducing new ideas, methods and craftsmanship. The first printing press in Constantinople (now Istanbul) was established by the Sephardic Jews in 1493. It is reported that under Bayezid's reign, Jews enjoyed a period of cultural flourishing, with the presence of such scholars as the Talmudist and scientist Mordecai Comtino; astronomer and poet Solomon ben Elijah Sharbi\u1e6d ha-Zahab; Shabbethai ben Malkiel Cohen, and the liturgical poet Menahem Tamar.\nSuccession.\nDuring Bayezid II's final years, on 14 September 1509, Constantinople was devastated by an earthquake, and a succession battle developed between his sons Selim and Ahmet. Ahmet unexpectedly captured Karaman, and began marching to Constantinople to exploit his triumph. Fearing for his safety, Selim staged a revolt in Thrace but was defeated by Bayezid and forced to flee back to the Crimean peninsula. Bayezid II developed fears that Ahmet might in turn kill him to gain the throne, so he refused to allow his son to enter Constantinople.Selim returned from Crimea and, with support from the Janissaries, he forced his father to abdicate the throne on 25 April 1512. Bayezid departed for retirement in his native Dimetoka, but he died on 26 May 1512 at Havsa, before reaching his destination and only a month after his abdication. He was buried next to the Bayezid Mosque in Istanbul.\nLegacy.\nBayezid was praised in a ghazal-style poem of Abd\u00fcrrezzak Bah\u015f\u0131, a scribe who came to Constantinople from Samarkand in the second half of the 15th century that worked at the courts of Mehmed II and Bayezid II, and wrote in Chagatai with the Old Uyghur alphabet:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I had a pleasant time in your reign my Padishah.\nI was without fear of all fears and dangers.\nThe fame of your justice and fairness reached to China and Hotan.\nThanks to God that there exist a merciful person like my Padishah.\nSultan Bayezid Khan ascended the throne.\nThis country had been his fate since past eternity.\nAny enemy that denied the country of my master:\nThat enemy's neck had been in rope and gallows.\nYour believing servants' faces smile like Bah\u015f\u0131's.\nThe place of those who walk unbelieving is hellfire.\nBayezid II ordered al-\u02bfAtufi, the librarian of Topkap\u0131 Palace, to prepare a register. The library's diverse holdings reflect a cosmopolitanism that was encyclopaedic in scope.\nFamily.\nConsorts.\nBayezid had ten known consorts:\nSons.\nBayezid had at least eight sons:\nDaughters.\nBayezid II, once ascended to the throne, granted his daughters and granddaughters in the male line the title of \"Sultan\" and his granddaughters in the female line that of \"Han\u0131msultan\", which replaced the simple honorific \"Hatun\" in use until then. His grandsons in female line obtained instead the title of \"Sultanzade\"\". Bayezid's reform of female titles remains in effect today among the surviving members of the Ottoman dynasty.\nBayezid had at least sixteen daughters:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4243", "revid": "49959671", "url": "https://en.wikipedia.org/wiki?curid=4243", "title": "Boxing", "text": "Combat sport and martial art\nBoxing is a combat sport and martial art. Taking place in a boxing ring, it involves two opponents throwing punches at each other for a predetermined amount of time, it is usually done wearing protective equipment, such as protective gloves, hand wraps, and mouthguards. \nAlthough the term 'boxing' commonly refers to the Western style, where only the fists are used, it has evolved differently in various regions and cultures across the world. Today the term, \"boxing\" is also used to refer to any kind of combat sport focused on striking, where two opponents fight each other using their fists, and could possibly involve kicks, elbow strikes, knee strikes, and headbutts, depending on the rules. These include bare-knuckle boxing, kickboxing, Muay Thai, Lethwei, savate, and sanda. Boxing techniques have been incorporated into many martial arts, military systems, as well as other combat sports.\nHumans have engaged in hand-to-hand combat since the beginning of human history. It is unclear when boxing became a sport, but some sources suggest prehistoric origins, dating back to as early as the 6th millennium BC in what is now Ethiopia. It is believed that when the Egyptians invaded Nubia, they adopted boxing from the local populace, subsequently popularizing it in Egypt. From there, the sport of boxing spread to various regions, including Greece, eastward to Mesopotamia, and northward to Rome.\nThe earliest visual evidence of boxing comes from Egypt and Sumer, both from the 3rd millennium, and are found in Sumerian carvings dating to the 3rd and 2nd millennium BC. The earliest evidence of boxing rules dates back to Ancient Greece, when boxing was added to the Olympic games in 688 BC. Boxing evolved through the prizefights of the 16th - 18th-centuries, largely in Great Britain, to its modern forerunner in the mid-19th century, with the introduction of the Marquess of Queensberry Rules in 1867.\nBoxing is overseen by a referee and consists of a series of 1 - 3 minute intervals called \"rounds\". A winner can be decided before the rounds are complete if a referee determines that an opponent is unable to continue, disqualifies an opponent, or if the opponent is knocked out or quits. When the fight reaches the end of its final round and both opponents are still standing, the winner is determined by the judges' scorecards. In case both fighters gain equal scores from the judges, it is considered a draw. In Olympic boxing, because a winner must be declared, judges award the contest to one fighter based on technical criteria. Amateur boxing is part of both the Olympics and Commonwealth Games, and is a standard feature in most international games. Boxing also has its own world championships, which are governed by the WBA, WBC, IBF and WBO. \n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nAncient history.\nHitting with different extremities of the body, such as kicks and punches, as an act of human aggression, has existed across the world throughout human history, being a combat system as old as wrestling. However, in terms of sports competition, due to the lack of writing in the prehistoric times and the lack of references, it is not possible to determine rules of any kind of boxing in prehistory, and in ancient times only can be inferred from the few intact sources and references to the sport.\nThe origin of the sport of boxing is unknown, however according to some sources boxing in any of its forms has prehistoric origins in present-day Ethiopia, where it appeared in the sixth millennium BC. When the Egyptians invaded Nubia they learned the art of boxing from the local population, and they took the sport to Egypt where it became popular. From Egypt, boxing spread to other countries including Greece, eastward to Mesopotamia, and northward to Rome.\nThe earliest visual evidence of any type of boxing comes from Egypt and Sumer both from the third millennium BC. A relief sculpture from Egyptian Thebes (c.\u20091350 BC) shows both boxers and spectators. These early Middle-Eastern and Egyptian depictions showed contests where fighters were either bare-fisted or had a band supporting the wrist. The earliest evidence of use of gloves can be found in Minoan Crete (c.\u20091500\u20131400 BC).\nVarious types of boxing existed in ancient India. The earliest references to \"musti-yuddha\" come from classical Vedic epics such as the \"Rig Veda\" (c. 1500\u20131000 BCE) and \"Ramayana\" (c. 700\u2013400 BCE). The \"Mahabharata\" describes two combatants boxing with clenched fists and fighting with kicks, finger strikes, knee strikes and headbutts during the time of King Virata. Duels (\"niyuddham\") were often fought to the death. During the period of the Western Satraps, the ruler Rudradaman \u2013 in addition to being well-versed in \"the great sciences\" which included Indian classical music, Sanskrit grammar, and logic \u2013 was said to be an excellent horseman, charioteer, elephant rider, swordsman and boxer. The \"Gurbilas Shemi\", an 18th-century Sikh text, gives numerous references to \"musti-yuddha\". The martial art is related to other forms of martial arts found in other parts of the Indian cultural sphere including Muay Thai in Thailand, Muay Lao in Laos, Pradal Serey in Cambodia and Lethwei in Myanmar.\nIn Ancient Greece boxing was a well developed sport called \"pygmachia\", and enjoyed consistent popularity. In Olympic terms, it was first introduced in the 23rd Olympiad, 688 BC. The boxers would wind leather thongs around their hands in order to protect them. There were no rounds and boxers fought until one of them acknowledged defeat or could not continue. Weight categories were not used, which meant heavier fighters had a tendency to dominate. The style of boxing practiced typically featured an advanced left leg stance, with the left arm semi-extended as a guard, in addition to being used for striking, and with the right arm drawn back ready to strike. It was the head of the opponent which was primarily targeted, and there is little evidence to suggest that targeting the body or the use of kicks was common, in which it resembled modern western boxing.\nBoxing was a popular spectator sport in Ancient Rome. Fighters protected their knuckles with leather strips wrapped around their fists. Eventually harder leather was used and the strips became a weapon. Metal studs were introduced to the strips to make the cestus. Fighting events were held at Roman amphitheatres.\nEarly London prize ring rules.\nRecords of boxing activity disappeared in the west after the fall of the Western Roman Empire when the wearing of weapons became common once again and interest in fighting with the fists waned. However, there are detailed records of various fist-fighting sports that were maintained in different cities and provinces of Italy between the 12th and 17th centuries. There was also a sport in ancient Rus called \"kulachniy boy\" or 'fist fighting'.\nAs the wearing of swords became less common, there was renewed interest in fencing with the fists. The sport later resurfaced in England during the early 16th century in the form of bare-knuckle boxing, sometimes referred to as \"prizefighting\". The first documented account of a bare-knuckle fight in England appeared in 1681 in the \"London Protestant Mercury\", and the first English bare-knuckle champion was James Figg in 1719. This is also the time when the word \"boxing\" first came to be used. This earliest form of modern boxing was very different. Contests in Mr. Figg's time, in addition to fist fighting, also contained fencing and cudgeling. On 6 January 1681, the first recorded boxing match took place in Britain when Christopher Monck, 2nd Duke of Albemarle (and later Lieutenant Governor of Jamaica), engineered a bout between his butler and his butcher with the latter winning the prize.\nEarly fighting had no written rules, weight divisions, round limits, or referees. As a result, it was an extremely chaotic and brutal affair. An early article on boxing was published in Nottingham in 1713, by Sir Thomas Parkyns, 2nd Baronet, a wrestling patron from Bunny, Nottinghamshire, who had practised the techniques he described. The article, a single page in his manual of wrestling and fencing, \"Progymnasmata: The inn-play, or Cornish-hugg wrestler\", described a system of headbutting, punching, eye-gouging, chokes, and hard throws, not recognized in boxing today.\nThe first boxing rules, called the Broughton Rules, were introduced by champion Jack Broughton in 1743 to protect fighters in the ring where deaths sometimes occurred. Under these rules, if a man went down and could not continue after a count of 30 seconds, the fight was over. Hitting a downed fighter and grasping below the waist were prohibited. Broughton encouraged the use of \"mufflers\", a form of padded bandage or mitten, to be used in \"jousting\" or sparring sessions in training, and in exhibition matches.\nThese rules did allow the fighters an advantage not enjoyed by today's boxers; they permitted the fighter to drop to one knee to end the round and begin the 30-second count at any time. Thus a fighter realizing he was in trouble had an opportunity to recover. However, this was considered \"unmanly\" and was frequently disallowed by additional rules negotiated by the seconds of the boxers. In modern boxing, there is a three-minute limit to rounds (unlike the downed fighter ends the round rule). Intentionally going down in modern boxing will cause the recovering fighter to lose points in the scoring system. Furthermore, as the contestants did not have heavy leather gloves and wristwraps to protect their hands, they used different punching technique to preserve their hands because the head was a common target to hit full out. Almost all period manuals have powerful straight punches with the whole body behind them to the face (including forehead) as the basic blows.\nThe British sportswriter Pierce Egan coined the term \"the sweet science\" as an epithet for prizefighting \u2013 or more fully \"the sweet science of bruising\" as a description of England's bare-knuckle fight scene in the early nineteenth century.\nBoxing could also be used to settle disputes even by females. In 1790 in Waddington, Lincolnshire Mary Farmery and Susanna Locker both laid claim to the affections of a young man; this produced a challenge from the former to fight for the prize, which was accepted by the latter. Proper sidesmen were chosen, and every matter conducted in form. After several knock-down blows on both sides, the battle ended in favour of Mary Farmery.\nThe London Prize Ring Rules introduced measures that remain in effect for professional boxing to this day, such as outlawing butting, gouging, scratching, kicking, hitting a man while down, holding the ropes, and using resin, stones or hard objects in the hands, and biting.\nMarquess of Queensberry rules (1867).\nIn 1867, the Marquess of Queensberry rules were drafted by John Chambers for amateur championships held at Lillie Bridge in London for lightweights, middleweights and heavyweights. The rules were published under the patronage of the Marquess of Queensberry, whose name has always been associated with them.\nThere were twelve rules in all, and they specified that fights should be \"a fair stand-up boxing match\" in a 24-foot-square or similar ring. Rounds were three minutes with one-minute rest intervals between rounds. Each fighter was given a ten-second count if he was knocked down, and wrestling was banned.\nThe introduction of gloves of \"fair-size\" also changed the nature of the bouts. An average pair of boxing gloves resembles a bloated pair of mittens and are laced up around the wrists.\nThe gloves can be used to block an opponent's blows. As a result of their introduction, bouts became longer and more strategic with greater importance attached to defensive maneuvers such as slipping, bobbing, countering and angling. Because less defensive emphasis was placed on the use of the forearms and more on the gloves, the classical forearms outwards, torso leaning back stance of the bare knuckle boxer was modified to a more modern stance in which the torso is tilted forward and the hands are held closer to the face.\nLate 19th and early 20th centuries.\nThrough the late 19th century, the martial art of boxing or prizefighting was primarily a sport of dubious legitimacy. Outlawed in England and much of the United States, prizefights were often held at gambling venues and broken up by police. Brawling and wrestling tactics continued, and riots at prizefights were common occurrences. Still, throughout this period, there arose some notable bare knuckle champions who developed fairly sophisticated fighting tactics.\nThe English case of \"R v. Coney\" in 1882 found that a bare-knuckle fight was an assault occasioning actual bodily harm, despite the consent of the participants. This marked the end of widespread public bare-knuckle contests in England.\nThe first world heavyweight champion under the Queensberry Rules was \"Gentleman Jim\" Corbett, who defeated John L. Sullivan in 1892 at the Pelican Athletic Club in New Orleans.\nThe first instance of film censorship in the United States occurred in 1897 when several states banned the showing of prize fighting films from the state of Nevada, where it was legal at the time.\nThroughout the early 20th century, boxers struggled to achieve legitimacy. They were aided by the influence of promoters like Tex Rickard and the popularity of great champions such as John L. Sullivan.\nModern boxing.\nThe modern sport arose from illegal venues and outlawed prizefighting and has become a multibillion-dollar commercial enterprise. A majority of young talent still comes from poverty-stricken areas around the world. Places like Mexico, Africa, South America, and Eastern Europe prove to be filled with young aspiring athletes who wish to become the future of boxing. Even in the U.S., places like the inner cities of New York, and Chicago have given rise to promising young talent. According to Rubin, \"boxing lost its appeal with the American middle class, and most of who boxes in modern America come from the streets and are street fighters\".\nRules.\nThe \"Marquess of Queensberry Rules\" have been the general rules governing modern boxing since their publication in 1867.\nA boxing match typically consists of a determined number of three-minute rounds, a total of up to 9 to 12 rounds with a minute spent between each round with the fighters resting in their assigned corners and receiving advice and attention from their coach and staff. The fight is controlled by a referee who works within the ring to judge and control the conduct of the fighters, rule on their ability to fight safely, count knocked-down fighters, and rule on fouls.\nUp to three judges are typically present at ringside to score the bout and assign points to the boxers, based on punches and elbows that connect, defense, knockdowns, hugging and other, more subjective, measures. Because of the open-ended style of boxing judging, many fights have controversial results, in which one or both fighters believe they have been \"robbed\" or unfairly denied a victory. Each fighter has an assigned corner of the ring, where their coach, as well as one or more \"seconds\" may administer to the fighter at the beginning of the fight and between rounds. Each boxer enters into the ring from their assigned corners at the beginning of each round and must cease fighting and return to their corner at the signalled end of each round.\nA bout in which the predetermined number of rounds passes is decided by the judges, and is said to \"go the distance\". The fighter with the higher score at the end of the fight is ruled the winner. With three judges, unanimous and split decisions are possible, as are draws. A boxer may win the bout before a decision is reached through a knock-out; such bouts are said to have ended \"inside the distance\". If a fighter is knocked down during the fight, determined by whether the boxer touches the canvas floor of the ring with any part of their body other than the feet as a result of the opponent's punch and not a slip, as determined by the referee, the referee begins counting until the fighter returns to their feet and can continue. Some jurisdictions require the referee to count to eight regardless of if the fighter gets up before.\nShould the referee count to ten, then the knocked-down boxer is ruled \"knocked out\" (whether unconscious or not) and the other boxer is ruled the winner by knockout (KO). A \"technical knock-out\" (TKO) is possible as well, and is ruled by the referee, fight doctor, or a fighter's corner if a fighter is unable to safely continue to fight, based upon injuries or being judged unable to effectively defend themselves. Many jurisdictions and sanctioning agencies also have a \"three-knockdown rule\", in which three knockdowns in a given round result in a TKO. A TKO is considered a knockout in a fighter's record. A \"standing eight\" count rule may also be in effect. This gives the referee the right to step in and administer a count of eight to a fighter that the referee feels may be in danger, even if no knockdown has taken place. After counting the referee will observe the fighter, and decide if the fighter is fit to continue. For scoring purposes, a standing eight count is treated as a knockdown.\nIn general, boxers are prohibited from hitting below the belt, holding, tripping, pushing, biting, or spitting. The boxer's shorts are raised so the opponent is not allowed to hit to the groin area with intent to cause pain or injury. Failure to abide by the former may result in a foul. They also are prohibited from kicking, head-butting, or hitting with any part of the arm other than the knuckles of a closed fist (including hitting with the elbow, shoulder or forearm, as well as with open gloves, the wrist, the inside, back or side of the hand). They are prohibited as well from hitting the back, back of the head or neck (called a \"rabbit-punch\") or the kidneys. They are prohibited from holding the ropes for support when punching, holding an opponent while punching, or ducking below the belt of their opponent (dropping below the waist of your opponent, no matter the distance between).\nIf a \"clinch\" \u2013 a defensive move in which a boxer wraps their opponent's arms and holds on to create a pause \u2013 is broken by the referee, each fighter must take a full step back before punching again (alternatively, the referee may direct the fighters to \"punch out\" of the clinch). When a boxer is knocked down, the other boxer must immediately cease fighting and move to the furthest neutral corner of the ring until the referee has either ruled a knockout or called for the fight to continue.\nViolations of these rules may be ruled \"fouls\" by the referee, who may issue warnings, deduct points, or disqualify an offending boxer, causing an automatic loss, depending on the seriousness and intentionality of the foul. An intentional foul that causes injury that prevents a fight from continuing usually causes the boxer who committed it to be disqualified. A fighter who suffers an accidental low-blow may be given up to five minutes to recover, after which they may be ruled knocked out if they are unable to continue. Accidental fouls that cause injury ending a bout may lead to a \"no contest\" result, or else cause the fight to go to a decision if enough rounds (typically four or more, or at least three in a four-round fight) have passed.\nUnheard of in the modern era, but common during the early 20th Century in North America, a \"newspaper decision (NWS)\" might be made after a no decision bout had ended. A \"no decision\" bout occurred when, by law or by pre-arrangement of the fighters, if both boxers were still standing at the fight's conclusion and there was no knockout, no official decision was rendered and neither boxer was declared the winner. But this did not prevent the pool of ringside newspaper reporters from declaring a consensus result among themselves and printing a newspaper decision in their publications. Officially, however, a \"no decision\" bout resulted in neither boxer winning or losing. Boxing historians sometimes use these unofficial newspaper decisions in compiling fight records for illustrative purposes only. Often, media outlets covering a match will personally score the match, and post their scores as an independent sentence in their report.\nProfessional vs. amateur boxing.\nThroughout the 17th to 19th centuries, boxing bouts were motivated by money, as the fighters competed for prize money, promoters controlled the gate, and spectators bet on the result.\nThe modern Olympic movement revived interest in amateur sports, and amateur boxing became an Olympic sport in 1908. In their current form, Olympic and other amateur bouts are typically limited to three or four rounds, scoring is computed by points based on the number of clean blows landed, regardless of impact, and fighters wear protective headgear, reducing the number of injuries, knockdowns, and knockouts. Currently scoring blows in amateur boxing are subjectively counted by ringside judges, but the Australian Institute for Sport has demonstrated a prototype of an Automated Boxing Scoring System, which introduces scoring objectivity, improves safety, and arguably makes the sport more interesting to spectators. Professional boxing remains by far the most popular form of the sport globally, though amateur boxing is dominant in Cuba and some former Soviet republics. For most fighters, an amateur career, especially at the Olympics, serves to develop skills and gain experience in preparation for a professional career. Western boxers typically participate in one Olympics and then turn pro, while Cubans and boxers from other socialist countries have an opportunity to collect multiple medals. In 2016, professional boxers were admitted in the Olympic Games and other tournaments sanctioned by AIBA. This was done in part to level the playing field and give all of the athletes the same opportunities government-sponsored boxers from socialist countries and post-Soviet republics have. However, professional organizations strongly opposed that decision.\nAmateur boxing.\nAmateur boxing may be found at the collegiate level, at the Olympic Games, Commonwealth Games, Asian Games, etc. In many other venues sanctioned by amateur boxing associations. Amateur boxing has a point scoring system that measures the number of clean blows landed rather than physical damage. Bouts consist of three rounds of three minutes in the Olympic and Commonwealth Games, and three rounds of three minutes in a national ABA (Amateur Boxing Association) bout, each with a one-minute interval between rounds.\nCompetitors wear protective headgear and gloves with a white strip or circle across the knuckle. There are cases however, where white ended gloves are not required but any solid color may be worn. The white end is just a way to make it easier for judges to score clean hits. Each competitor must have their hands properly wrapped, pre-fight, for added protection on their hands and for added cushion under the gloves. Gloves worn by the fighters must be twelve ounces in weight unless the fighters weigh under , thus allowing them to wear ten ounce gloves. A punch is considered a scoring punch only when the boxers connect with the white portion of the gloves. Each punch that lands cleanly on the head or torso with sufficient force is awarded a point. A referee monitors the fight to ensure that competitors use only legal blows. A belt worn over the torso represents the lower limit of punches \u2013 any boxer repeatedly landing low blows below the belt is disqualified. Referees also ensure that the boxers don't use holding tactics to prevent the opponent from swinging. If this occurs, the referee separates the opponents and orders them to continue boxing. Repeated holding can result in a boxer being penalized or ultimately disqualified. Referees will stop the bout if a boxer is seriously injured, if one boxer is significantly dominating the other or if the score is severely imbalanced. Amateur bouts which end this way may be noted as \"RSC\" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).\nProfessional boxing.\nProfessional bouts are usually much longer than amateur bouts, typically ranging from ten to twelve rounds, though four-round fights are common for less experienced fighters or club fighters. There are also some two- and three-round professional bouts, especially in Australia. Through the early 20th century, it was common for fights to have unlimited rounds, ending only when one fighter quit, benefiting high-energy fighters like Jack Dempsey. Fifteen rounds remained the internationally recognized limit for championship fights for most of the 20th century until the early 1980s, when the death of boxer Kim Duk-koo eventually prompted the World Boxing Council and other organizations sanctioning professional boxing to reduce the limit to twelve rounds.\nHeadgear is not permitted in professional bouts, and boxers are generally allowed to take much more damage before a fight is halted. At any time, the referee may stop the contest if he believes that one participant cannot defend himself due to injury. In that case, the other participant is awarded a technical knockout win. A technical knockout would also be awarded if a fighter lands a punch that opens a cut on the opponent, and the opponent is later deemed not fit to continue by a doctor because of the cut. For this reason, fighters often employ cutmen, whose job is to treat cuts between rounds so that the boxer is able to continue despite the cut. If a boxer simply quits fighting, or if his corner stops the fight, then the winning boxer is also awarded a technical knockout victory. In contrast with amateur boxing, professional male boxers have to be bare-chested.\nBoxing styles.\nDefinition of style.\n\"Style\" is often defined as the strategic approach a fighter takes during a bout. No two fighters' styles are alike, as each is determined by that individual's physical and mental attributes. Four main styles exist in boxing: In-Fighter, Out-Boxer, Slugger and Boxer-Puncher. These styles may be divided into several special subgroups, such as counter puncher, etc. The main philosophy of the styles is, that each style has an advantage over one, but disadvantage over the other one. It follows the rock paper scissors scenario \u2013 boxer beats brawler, brawler beats swarmer, and swarmer beats boxer.\nBoxer/out-fighter.\nA classic \"boxer\" or stylist (also known as an \"out-fighter\") seeks to maintain distance between himself and his opponent, fighting with faster, longer range punches, most notably the jab, and gradually wearing his opponent down. Due to this reliance on weaker punches, out-fighters tend to win by point decisions rather than by knockout, though some out-fighters have notable knockout records. They are often regarded as the best boxing strategists due to their ability to control the pace of the fight and lead their opponent, methodically wearing him down and exhibiting more skill and finesse than a brawler. Out-fighters need reach, hand speed, reflexes, and footwork.\nNotable out-fighters include Muhammad Ali, Larry Holmes, Joe Calzaghe, Wilfredo G\u00f3mez, Salvador S\u00e1nchez, Cecilia Br\u00e6khus, Gene Tunney, Ezzard Charles, Willie Pep, Meldrick Taylor, Ricardo \"Finito\" L\u00f3pez, Floyd Mayweather Jr., Roy Jones Jr., Sugar Ray Leonard, Miguel V\u00e1zquez, Sergio \"Maravilla\" Mart\u00ednez, Wladimir Klitschko and Guillermo Rigondeaux. This style was also used by fictional boxer Apollo Creed.\nBoxer-puncher.\nA boxer-puncher is a well-rounded boxer who is able to fight at close range with a combination of technique and power, often with the ability to knock opponents out with a combination and in some instances a single shot. Their movement and tactics are similar to that of an out-fighter (although they are generally not as mobile as an out-fighter), but instead of winning by decision, they tend to wear their opponents down using combinations and then move in to score the knockout. A boxer must be well rounded to be effective using this style.\nNotable boxer-punchers include Muhammad Ali, Canelo \u00c1lvarez, Sugar Ray Leonard, Roy Jones Jr., Wladimir Klitschko, Vasyl Lomachenko, Lennox Lewis, Joe Louis, Wilfredo G\u00f3mez, Oscar De La Hoya, Archie Moore, Miguel Cotto, Nonito Donaire, Sam Langford, Henry Armstrong, Sugar Ray Robinson, Tony Zale, Carlos Monz\u00f3n, Alexis Arg\u00fcello, \u00c9rik Morales, Terry Norris, Marco Antonio Barrera, Naseem Hamed, Thomas Hearns, Julian Jackson and Gennady Golovkin.\nBrawler/slugger.\nA brawler is a fighter who generally lacks finesse and footwork in the ring, but makes up for it through sheer punching power. Many brawlers tend to lack mobility, preferring a less mobile, more stable platform and have difficulty pursuing fighters who are fast on their feet. They may also have a tendency to ignore combination punching in favor of continuous beat-downs with one hand and by throwing slower, more powerful single punches (such as hooks and uppercuts). Their slowness and predictable punching pattern (single punches with obvious leads) often leaves them open to counter punches, so successful brawlers must be able to absorb a substantial amount of punishment. However, not all brawler/slugger fighters are not mobile; some can move around and switch styles if needed but still have the brawler/slugger style such as Wilfredo G\u00f3mez, Prince Naseem Hamed and Danny Garc\u00eda.\nA brawler's most important assets are power and chin (the ability to absorb punishment while remaining able to continue boxing). Examples of this style include George Foreman, Rocky Marciano, Jack Dempsey, Riddick Bowe, Danny Garc\u00eda, Wilfredo G\u00f3mez, Sonny Liston, John L. Sullivan, Max Baer, Prince Naseem Hamed, Ray Mancini, David Tua, Arturo Gatti, Micky Ward, Brandon R\u00edos, Ruslan Provodnikov, Michael Katsidis, James Kirkland, Marcos Maidana, Vitali Klitschko, Jake LaMotta, Manny Pacquiao, and Ireland's John Duddy. This style of boxing was also used by fictional boxers Rocky Balboa and James \"Clubber\" Lang.\nBrawlers tend to be more predictable and easy to hit but usually fare well enough against other fighting styles because they train to take punches very well. They often have a higher chance than other fighting styles to score a knockout against their opponents because they focus on landing big, powerful hits, instead of smaller, faster attacks. Oftentimes they place focus on training on their upper body instead of their entire body, to increase power and endurance. They also aim to intimidate their opponents because of their power, stature and ability to take a punch.\nSwarmer/in-fighter.\nIn-fighters/swarmers (sometimes called \"pressure fighters\") attempt to stay close to an opponent, throwing intense flurries and combinations of hooks and uppercuts. Mainly Mexican, Irish, Irish-American, Puerto Rican, and Mexican-American boxers popularized this style. A successful in-fighter often needs a good \"chin\" because swarming usually involves being hit with many jabs before they can maneuver inside where they are more effective. In-fighters operate best at close range because they are generally shorter and have less reach than their opponents and thus are more effective at a short distance where the longer arms of their opponents make punching awkward. However, several fighters tall for their division have been relatively adept at in-fighting as well as out-fighting.\nThe essence of a swarmer is non-stop aggression. Many short in-fighters use their stature to their advantage, employing a bob-and-weave defense by bending at the waist to slip underneath or to the sides of incoming punches. Unlike blocking, causing an opponent to miss a punch disrupts his balance, this permits forward movement past the opponent's extended arm and keeps the hands free to counter. A distinct advantage that in-fighters have is when throwing uppercuts, they can channel their entire bodyweight behind the punch; Mike Tyson was famous for throwing devastating uppercuts. Marvin Hagler was known for his hard \"chin\", punching power, body attack and the stalking of his opponents. Some in-fighters, like Mike Tyson, have been known for being notoriously hard to hit. The key to a swarmer is aggression, endurance, chin, and bobbing-and-weaving.\nNotable in-fighters include Henry Armstrong, Aaron Pryor, Julio C\u00e9sar Ch\u00e1vez, Jack Dempsey, Shawn Porter, Miguel Cotto, Gennady Golovkin, Joe Frazier, Danny Garc\u00eda, Mike Tyson, Manny Pacquiao, Rocky Marciano, Wayne McCullough, James Braddock, Gerry Penalosa, Harry Greb, David Tua, James Toney and Ricky Hatton.\nCounter puncher.\nCounter punchers are slippery, defensive style fighters who often rely on their opponent's mistakes in order to gain the advantage, whether it be on the score cards or more preferably a knockout. They use their well-rounded defense to avoid or block shots and then immediately catch the opponent off guard with a well placed and timed punch. A fight with a skilled counter-puncher can turn into a war of attrition, where each shot landed is a battle in itself. Thus, fighting against counter punchers requires constant feinting and the ability to avoid telegraphing one's attacks. To be truly successful using this style they must have good reflexes, a high level of prediction and awareness, pinpoint accuracy and speed, both in striking and in footwork.\nNotable counter punchers include Muhammad Ali, Joe Calzaghe, Vitali Klitschko, Evander Holyfield, Max Schmeling, Chris Byrd, Jim Corbett, Jack Johnson, Bernard Hopkins, Laszlo Papp, Jerry Quarry, Anselmo Moreno, James Toney, Marvin Hagler, Juan Manuel M\u00e1rquez, Humberto Soto, Floyd Mayweather Jr., Roger Mayweather, Pernell Whitaker, Sergio Mart\u00ednez and Guillermo Rigondeaux. This style of boxing is also used by fictional boxer Little Mac.\nCounter punchers usually wear their opponents down by causing them to miss their punches. The more the opponent misses, the faster they tire, and the psychological effects of being unable to land a hit will start to sink in. The counter puncher often tries to outplay their opponent entirely, not just in a physical sense, but also in a mental and emotional sense. This style can be incredibly difficult, especially against seasoned fighters, but winning a fight without getting hit is often worth the pay-off. They usually try to stay away from the center of the ring, in order to outmaneuver and chip away at their opponents. A large advantage in counter-hitting is the forward momentum of the attacker, which drives them further into your return strike. As such, knockouts are more common than one would expect from a defensive style.\nCombinations of styles.\nAll fighters have primary skills with which they feel most comfortable, but truly elite fighters are often able to incorporate auxiliary styles when presented with a particular challenge. For example, an out-fighter will sometimes plant his feet and counter punch, or a slugger may have the stamina to pressure fight with his power punches.\nOld history of the development of boxing and its prevalence contribute to fusion of various types of martial arts and the emergence of new ones that are based on them. For example, a combination of boxing and sportive sambo techniques gave rise to a combat sambo.\nStyle matchups.\nThere is a generally accepted rule of thumb about the success each of these boxing styles has against the others. In general, an in-fighter has an advantage over an out-fighter, an out-fighter has an advantage over a brawler, and a brawler has an advantage over an in-fighter; these form a cycle with each style being stronger relative to one, and weaker relative to another, with none dominating, as in rock paper scissors. Naturally, many other factors, such as the skill level and training of the combatants, determine the outcome of a fight, but the widely held belief in this relationship among the styles is embodied in the clich\u00e9 amongst boxing fans and writers that \"styles make fights\".\nBrawlers tend to overcome swarmers or in-fighters because, in trying to get close to the slugger, the in-fighter will invariably have to walk straight into the guns of the much harder-hitting brawler, so, unless the former has a very good chin and the latter's stamina is poor, the brawler's superior power will carry the day. A famous example of this type of match-up advantage would be George Foreman's knockout victory over Joe Frazier in their original bout \"The Sunshine Showdown\".\nAlthough in-fighters struggle against heavy sluggers, they typically enjoy more success against out-fighters or boxers. Out-fighters prefer a slower fight, with some distance between themselves and the opponent. The in-fighter tries to close that gap and unleash furious flurries. On the inside, the out-fighter loses a lot of his combat effectiveness, because he cannot throw the hard punches. The in-fighter is generally successful in this case, due to his intensity in advancing on his opponent and his good agility, which makes him difficult to evade. For example, the swarming Joe Frazier, though easily dominated by the slugger George Foreman, was able to create many more problems for the boxer Muhammad Ali in their three fights. Joe Louis, after retirement, admitted that he hated being crowded, and that swarmers like untied/undefeated champ Rocky Marciano would have caused him style problems even in his prime.\nThe boxer or out-fighter tends to be most successful against a brawler, whose slow speed (both hand and foot) and poor technique make him an easy target for the faster out-fighter. The out-fighter's main concern is to stay alert, as the brawler only needs to land one good punch to finish the fight. If the out-fighter can avoid those power punches, he can often wear the brawler down with fast jabs, tiring him out. If he is successful enough, he may even apply extra pressure in the later rounds in an attempt to achieve a knockout. Most classic boxers, such as Muhammad Ali, enjoyed their best successes against sluggers.\nAn example of a style matchup was the historical fight of Julio C\u00e9sar Ch\u00e1vez, a swarmer or in-fighter, against Meldrick Taylor, the boxer or out-fighter (see Julio C\u00e9sar Ch\u00e1vez vs. Meldrick Taylor). The match was nicknamed \"Thunder Meets Lightning\" as an allusion to punching power of Ch\u00e1vez and blinding speed of Taylor. Ch\u00e1vez was the epitome of the \"Mexican\" style of boxing. Taylor's hand and foot speed and boxing abilities gave him the early advantage, allowing him to begin building a large lead on points. Ch\u00e1vez remained relentless in his pursuit of Taylor and due to his greater punching power Ch\u00e1vez slowly punished Taylor. Coming into the later rounds, Taylor was bleeding from the mouth, his entire face was swollen, the bones around his eye socket had been broken, he had swallowed a considerable amount of his own blood, and as he grew tired, Taylor was increasingly forced into exchanging blows with Ch\u00e1vez, which only gave Ch\u00e1vez a greater chance to cause damage. While there was little doubt that Taylor had solidly won the first three quarters of the fight, the question at hand was whether he would survive the final quarter. Going into the final round, Taylor held a secure lead on the scorecards of two of the three judges. Ch\u00e1vez would have to knock Taylor out to claim a victory, whereas Taylor merely needed to stay away from the Mexican legend. However, Taylor did not stay away, but continued to trade blows with Ch\u00e1vez. As he did so, Taylor showed signs of extreme exhaustion, and every tick of the clock brought Taylor closer to victory unless Ch\u00e1vez could knock him out.\nWith about a minute left in the round, Ch\u00e1vez hit Taylor squarely with several hard punches and stayed on the attack, continuing to hit Taylor with well-placed shots. Finally, with about 25 seconds to go, Ch\u00e1vez landed a hard right hand that caused Taylor to stagger forward towards a corner, forcing Ch\u00e1vez back ahead of him. Suddenly Ch\u00e1vez stepped around Taylor, positioning him so that Taylor was trapped in the corner, with no way to escape from Ch\u00e1vez' desperate final flurry. Ch\u00e1vez then nailed Taylor with a tremendous right hand that dropped the younger man. By using the ring ropes to pull himself up, Taylor managed to return to his feet and was given the mandatory 8-count. Referee Richard Steele asked Taylor twice if he was able to continue fighting, but Taylor failed to answer. Steele then concluded that Taylor was unfit to continue and signaled that he was ending the fight, resulting in a TKO victory for Ch\u00e1vez with only two seconds to go in the bout.\nEquipment.\nSince boxing involves forceful, repetitive punching, precautions must be taken to prevent damage to bones in the hand. Most trainers do not allow boxers to train and spar without wrist wraps and boxing gloves. Hand wraps are used to secure the bones in the hand, and the gloves are used to protect the hands from blunt injury, allowing boxers to throw punches with more force than if they did not use them. \nGloves have been required in competition since the late nineteenth century, though modern boxing gloves are much heavier than those worn by early twentieth-century fighters. Prior to a bout, both boxers agree upon the weight of gloves to be used in the bout, with the understanding that lighter gloves allow heavy punchers to inflict more damage. The brand of gloves can also affect the impact of punches, so this too is usually stipulated before a bout. Both sides are allowed to inspect the wraps and gloves of the opponent to help ensure both are within agreed upon specifications and no tampering has taken place.\nA mouthguard is important to protect the teeth and gums from injury, and to cushion the jaw, resulting in a decreased chance of knockout. Both fighters must wear soft soled shoes to reduce the damage from accidental (or intentional) stepping on feet. While older boxing boots more commonly resembled those of a professional wrestler, modern boxing shoes and boots tend to be quite similar to their amateur wrestling counterparts.\nBoxers practice their skills on several types of punching bags. A small, tear-drop-shaped \"speed bag\" is used to hone reflexes and repetitive punching skills, while a large cylindrical \"heavy bag\" filled with sand, a synthetic substitute, or water is used to practice power punching and body blows. The double-end bag is usually connected by elastic on the top and bottom and moves randomly upon getting struck and helps the fighter work on accuracy and reflexes. In addition to these distinctive pieces of equipment, boxers also use sport-nonspecific training equipment to build strength, speed, agility, and stamina. Common training equipment includes free weights, rowing machines, jump rope, and medicine balls.\nBoxers also use punch/focus mitts in which a trainer calls out certain combinations and the fighter strikes the mitts accordingly. This is a great exercise for stamina as the boxer isn't allowed to go at his own pace but that of the trainer, typically forcing the fighter to endure a higher output and volume than usual. In addition, they also allow trainers to make boxers utilize footwork and distances more accurately. Recently boxing clubs have started using something called music boxing machines to train newbies in a more musical way to gain rhythm.\nBoxing matches typically take place in a boxing ring, a raised platform surrounded by ropes attached to posts rising in each corner. The term \"ring\" has come to be used as a metaphor for many aspects of prize fighting in general.\nTechnique.\nStance.\nThe modern boxing stance differs substantially from the typical boxing stances of the 19th and early 20th centuries. The modern stance has a more upright vertical-armed guard, as opposed to the more horizontal, knuckles-facing-forward guard adopted by early 20th century hook users such as Jack Johnson.\nUpright stance \u2013 In a fully upright stance, the boxer stands with the legs shoulder-width apart and the rear foot a half-step in front of the lead man. Right-handed or orthodox boxers lead with the left foot and fist (for most penetration power). Both feet are parallel, and the right heel is off the ground. The lead (left) fist is held vertically about six inches in front of the face at eye level. The rear (right) fist is held beside the chin and the elbow tucked against the ribcage to protect the body. The chin is tucked into the chest to avoid punches to the jaw which commonly cause knock-outs and is often kept slightly off-center. Wrists are slightly bent to avoid damage when punching and the elbows are kept tucked in to protect the ribcage. \nCrouching stance \u2013 Some boxers fight from a crouch, leaning forward and keeping their feet closer together. The stance described is considered the \"textbook\" stance and fighters are encouraged to change it around once it's been mastered as a base. Case in point, many fast fighters have their hands down and have almost exaggerated footwork, while brawlers or bully fighters tend to slowly stalk their opponents. In order to retain their stance boxers take 'the first step in any direction with the foot already leading in that direction.'\nDifferent stances allow boxers to position and distribute their bodyweight differently; this alteration can affect the power and explosiveness with which a punch is delivered. For instance, a crouched stance allows for the bodyweight to be positioned further forward over the lead left leg. If a lead left hook is thrown from this position, it will produce a powerful springing action in the lead leg and produce a more explosive punch. This springing action could not be generated effectively, for this punch, if an upright stance was used or if the bodyweight was positioned predominantly over the back leg. Mike Tyson was a keen practitioner of a crouched stance and this style of power punching. The preparatory positioning of the bodyweight over the bent lead leg is also known as an isometric preload.\nOrthodox stance refers to a stance where the left leg, and usually the left arm, is forward.\nSouthpaw stance \u2013 refers to a stance where the right leg, and usually the right arm, is forward. Left-handed or southpaw fighters use a mirror image of the orthodox stance, which can create problems for orthodox fighters unaccustomed to receiving jabs, hooks, or crosses from the opposite side. The southpaw stance, conversely, is vulnerable to a straight right hand.\nOpen stance - refers to when one fighter is in an orthodox stance and the other is in a southpaw stance. \nClosed stance - refers to when both fighters are in orthodox stances or both fighters are in southpaw stances.\nSquare stance \u2013 North American fighters tend to favor a more balanced stance, facing the opponent almost squarely.\nBladed stance \u2013 many European fighters stand with their torso turned more to the side. The positioning of the hands may also vary, as some fighters prefer to have both hands raised in front of the face, risking exposure to body shots.\nFootwork.\nFootwork in boxing refers to the boxer's movement around the ring and how they set their feet in order to punch, block and dodge. They must be ready to switch between stepping and striking very quickly. How a boxer uses their feet is related to the boxing stance which they are in. For an orthodox left-handed boxer who has a left foot forward stance, they will standardly step forward first with their left foot and then follow with their right. When they move backwards, they will first step backwards with their right foot and then their left foot. And vice versa for unorthodox south-paw fighters. For leftwards movement, both orthodox and unorthodox fighters will typically move their left foot first and then their right foot. And for rightwards movement their right foot first and then their left foot.\nBoxers always strive to be very light-footed in their movement around the ring. This enables them to move quickly in and out of range and position themselves to attack from different angles. The importance of being light-footed, and the extent to which some fighters achieve it, is demonstrated by Muhammad Ali and Sugar Ray Robinson who were said to be so light-footed that they floated around the ring.\nWhen a boxer strikes they set themselves to do so. This involves planting their feet which means pushing firmly into the ground to ensure that they are in a stable stance. This additional surety of foot placement gives them a stronger base of support to strike from. The feet are not just planted before a punch but also during it. This is especially the case with power punches. The very fast planting of the feet as a part of the punch makes it more powerful and explosive. The boxer must therefore decide how much they want to plant their feet before the punch in order to be in a strong posture to strike from, and how much they want to plant their feet as a concurrent part of the punch. The ability to move very lightly on their feet one moment and then suddenly plant them in order to strike the next is one of the main skills a boxer needs to develop.\nPunches.\nThere are eight basic punches in boxing, with six of them: the jab, cross, lead hook, rear hook, lead uppercut and rear uppercut, being the most used. The lead overhand and rear overhand are the remaining basic punches. Any punch other than a jab is considered a power punch. If a boxer is right-handed (orthodox), their left hand is the lead hand and his right hand is the rear hand. For a left-handed boxer or southpaw, the hand positions are reversed. When using these punches in combinations they are often referred to as numbers, with the jab being the number 1, cross being 2, lead hook 3, rear hook 4, lead uppercut 5 and rear uppercut 6. For example, a jab and cross combination would be referred to as a 1-2 combination.\nFor clarity, the following assumes a right-handed boxer.\nThese different punch types can be thrown in rapid succession to form combinations or \"combos\". The most common is the jab and cross combination, nicknamed the \"one-two combo\". This is usually an effective combination, because the jab blocks the opponent's view of the cross, making it easier to land cleanly and forcefully.\nA large, swinging circular punch starting from a cocked-back position with the arm at a longer extension than the hook and all of the fighter's weight behind it is sometimes referred to as a \"roundhouse\", \"haymaker\", or sucker-punch. A haymaker is a wide-angle punch similar to a hook, but instead of getting power from body rotation, it gets its power from its large loop. It is considered an unsophisticated punch, and leaves one open to a counter. Relying on body weight and centripetal force within a wide arc, the roundhouse can be a powerful blow, but it is often a wild and uncontrolled punch that leaves the fighter delivering it off balance and with an open guard.\nWide, looping punches have the further disadvantage of taking more time to deliver, giving the opponent ample warning to react and counter. For this reason, the haymaker or roundhouse is not a conventional punch, and is regarded by trainers as a mark of poor technique or desperation. Sometimes it has been used, because of its immense potential power, to finish off an already staggering opponent who seems unable or unlikely to take advantage of the poor position it leaves the puncher in.\nAnother unconventional punch is the rarely used bolo punch, in which the opponent swings an arm out several times in a wide arc, usually as a distraction, before delivering with either that or the other arm.\nAn illegal punch to the back of the head or neck is known as a rabbit punch.\nDefense.\nDefense in boxing refers to actions taken by a boxer to avoid being hit, redirect an opponents attack or reduce the impact of punches to vital areas such as the head. Defensive techniques generally fall into 4 categories of evading, blocking, covering and clinching.\nEvading.\nEvading refers to actions a boxer takes to try to avoid strikes entirely by making their opponents miss.\nBlocking.\nBlocking refers to actions a boxer takes to absorb, redirect, intercept or slow the momentum of an opponents strikes preventing blows from impacting vital areas such as the head and midsection.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"If, however, his right lead is thrown at you when you are out of normal position-when, for example, you have permitted your left hand to drop down in an overzealous feint to the body-you must block with your left shoulder. You give your left shoulder a frantic, whirling hunch to protect\nyour already snuggled chin. Thus, the blow thuds into your shoulder instead of into your face (Figure 53). You'll be tempted to use your right hand to help your left shoulder in that block. You'll be tempted to make a \"shell defense\" with shoulder and hand. But don't do it. You've got to keep that right hand in its normal position, ready to (1) guard against the possibility of a following left hook, and (2) smash a straight right counter to your opponent's solar plexus or chin.\" - Jack Dempsey's Championship Boxing Explosive Punching and Aggressive Defense.\nCovering.\nCovering refers to action a boxer takes to reduce the impact of strikes to vital areas such as the head and midsection. Unlike blocking, covering puts the gloves on the opponents head or body directly. Some damage is still done to the boxer while covering, but the goal is to reduce the damage by using the gloves or arms as shock absorbers lessening the severity of blows.\nClinching.\nClinching refers to grappling techniques a boxer uses to tie up an opponent's arms to prevent them from striking, or lessen the impact of strikes. Clinching techniques can also be used to move an opponent to a position where they are unable to effectively strike from.\nGuards.\nThere are 4 main defensive positions (guards or styles) used in boxing:\nAll fighters have their own variations to these styles. Some fighters may have their guard higher for more head protection while others have their guard lower to provide better protection against body punches. Many fighters don't strictly use a single position, but rather adapt to the situation when choosing a certain position to protect them.\nPeek-a-Boo \u2014 a defensive style often used by a fighter where the hands are placed in front of the boxer's face, like in the babies' game of the same name. It offers extra protection to the face and makes it easier to jab the opponent's face. Peek-a-Boo boxing was developed by legendary trainer Cus D'Amato. Peek-a-Boo boxing utilizes relaxed hands with the forearms in front of the face and the fist at nose-eye level. Other unique features includes side to side head movements, bobbing, weaving and blind siding your opponent. The number system e.g. 3-2-3-Body-head-body or 3-3-2 Body-Body-head is drilled with a stationary dummy called the \"Willie bag\", named by Cus after boxer Willie Pastrano, until the fighter is able to punch rapid combinations with what D'Amato called \"bad intentions.\" The theory behind the style is that when combined with effective bobbing and weaving head movement, the fighter has a very strong defense and becomes more elusive, able to throw hooks and uppercuts with great effectiveness. Also it allows swift neck movements as well quick duckings and bad returning damage, usually by rising uppercuts or even rising hooks. Since it is a defense designed for close range fighting, it is mainly used by in-fighters. Bobo Olson was the first known champion to use this as a defense.\nCrab Style Guards: Work at all ranges, allowing fighters to defend while countering\u2014such as using a lead arm to block jabs while keeping the rear hand free to punch. The style adapts to different boxing approaches: infighters use it to advance safely, out-boxers rely on one-handed defense to strike while evading, and sluggers use it to cover up after missed power shots. Its flexibility makes it effective for both offense and defense. The many variations of this defense include:\nLong guards also knows as Extended Guard: In boxing these guards are often used by taller fighters or fighters with longer reach to keep opponents out of punching range, but shorter fighters or fighters with shorter reach often use them intermittently. Variations include:\nClassic Guards or Basic Guards: The modern Classic Guards are often the first Guards taught to boxers as the initial guard position is easy to learn, and they are effective against haymakers, which is the type of punch many untrained fighters and beginners use often. Guards fitting into this category include:\nRing corner.\nIn boxing, each fighter is given a corner of the ring where they rest in between rounds for one minute and where their trainers stand. Typically, three individuals stand in the corner besides the boxer; these are the trainer, the assistant trainer and the cutman. The trainer and assistant typically give advice to the boxer on what they are doing wrong as well as encouraging them if they are losing. The cutman is a cutaneous doctor responsible for keeping the boxer's face and eyes free of cuts, blood and excessive swelling. This is of particular importance because many fights are stopped because of cuts or swelling that threaten the boxer's eyes.\nIn addition, the corner is responsible for stopping the fight if they feel their fighter is in grave danger of permanent injury. The corner will occasionally throw in a white towel to signify a boxer's surrender (the idiomatic phrase \"to throw in the towel\", meaning to give up, derives from this practice). This can be seen in the fight between Diego Corrales and Floyd Mayweather. In that fight, Corrales' corner surrendered despite Corrales' steadfast refusal.\nHealth concerns.\nParticipating in boxing causes physical injuries. Injuries to the head are most commonly experienced by participants. Deaths of boxers during or after a bout from injuries received in the ring do occur. A 2011 study of bouts from 1890 and 2011 calculated an average death rate of 13 participants per year, for the years studied. An Australian study from 2022 found that efforts passed to improve safety in the sport in 2011 were unsuccessful at preventing deaths of participants in the sport.\nKnocking a person unconscious or even causing a concussion may cause permanent brain damage. There is no clear division between the force required to knock a person out and the force likely to kill a person. Additionally, contact sports, especially combat sports, are directly related to a brain disease called chronic traumatic encephalopathy, abbreviated as CTE. This disease begins to develop during the life of the athlete, and continues to develop even after sports activity has ceased.\nIn March 1981, neurosurgeon Fred Sonstein sought to use CAT scans in an attempt to track the degeneration of boxers' cognitive functions after seeing the decline of Bennie Briscoe. From 1980 to 2007, more than 200 amateur boxers, professional boxers and Toughman fighters died due to ring or training injuries. In 1983, editorials in the \"Journal of the American Medical Association\" called for a ban on boxing. The editor, George Lundberg, called boxing an \"obscenity\" that \"should not be sanctioned by any civilized society\". Since then, the British, Canadian and Australian Medical Associations have called for bans on boxing.\nSupporters of the ban state that boxing is the only sport where hurting the other athlete is the goal. Bill O'Neill, boxing spokesman for the British Medical Association, has supported the BMA's proposed ban on boxing: \"It is the only sport where the intention is to inflict serious injury on your opponent, and we feel that we must have a total ban on boxing.\" Opponents respond that such a position is misguided opinion, stating that amateur boxing is scored solely according to total connecting blows with no award for \"injury\". They observe that many skilled professional boxers have had rewarding careers without inflicting injury on opponents by accumulating scoring blows and avoiding punches winning rounds scored 10\u20139 by the 10-point must system, and they note that there are many other sports where concussions are much more prevalent. However, the data shows that the concussion rate in boxing is the highest of all contact sports. In addition, repetitive and subconcussive blows to the head, and not just concussions, cause CTE, and the evidence indicates that brain damage and the effects of CTE are more severe in boxing.\nIn 2007, one study of amateur boxers showed that protective headgear did not prevent brain damage, and another found that amateur boxers faced a high risk of brain damage. The Gothenburg study analyzed temporary levels of neurofilament light in cerebral spinal fluid which they conclude is evidence of damage, even though the levels soon subside. More comprehensive studies of neurological function on larger samples performed by Johns Hopkins University in 1994 and accident rates analyzed by National Safety Council in 2017 show amateur boxing is a comparatively safe sport due to the regulations of amateur boxing and a greater control of the athletes, although the studies did not focus on CTE or its long-term effects. In addition, a good training methodology and short career can reduce the effects of brain damage.\nIn 1997, the American Association of Professional Ringside Physicians was established to create medical protocols through research and education to prevent injuries in boxing.\nProfessional boxing is forbidden in Iceland, Iran and North Korea. It was banned in Sweden until 2007 when the ban was lifted but strict restrictions, including four three-minute rounds for fights, were imposed. Boxing was banned in Albania from 1965 until the fall of Communism in 1991. Norway legalized professional boxing in December 2014.\nThe International Boxing Association (AIBA) restricted the use of head guards for senior males at the World Championships and Olympics after 2013. A literature review study analyses present knowledge about protecting headgear and injury prevention in boxing to determine if injury risks associated with not head guard usage increased. The research of the reviewed literature indicates that head guards cover well against lacerations and skull fractures. Therefore, AIBA's decision to terminate the head guard must be considered cautiously, and injury rates among (male) boxers should be continuously evaluated.\nPossible health benefits.\nLike other active and dynamic sports, boxing may be argued to provide some general health benefits, such as fat burning, increased muscle tone, strong bones and ligaments, cardiovascular fitness, muscular endurance, improved core stability, co-ordination and body awareness, strength and power, stress relief and self-esteem, though it's unlikely these offset the much greater risks.\nBoxing Halls of Fame.\nThe sport of boxing has two internationally recognized boxing halls of fame; the International Boxing Hall of Fame (IBHOF) and the Boxing Hall of Fame Las Vegas. The latter opened in Las Vegas, Nevada in 2013 and was founded by Steve Lott, former assistant manager for Mike Tyson.\nThe International Boxing Hall of Fame opened in Canastota, New York in 1989. The first inductees in 1990 included Jack Johnson, Benny Leonard, Jack Dempsey, Henry Armstrong, Sugar Ray Robinson, Archie Moore, and Muhammad Ali. Other world-class figures include Salvador Sanchez, Jose Napoles, Roberto \"Manos de Piedra\" Dur\u00e1n, Ricardo Lopez, Gabriel \"Flash\" Elorde, Vicente Saldivar, Ismael Laguna, Eusebio Pedroza, Carlos Monz\u00f3n, Azumah Nelson, Rocky Marciano, Pipino Cuevas, Wilfred Benitez, Wilfredo Gomez, Felix Trinidad and Ken Buchanan. The Hall of Fame's induction ceremony is held every June as part of a four-day event. The fans who come to Canastota for the Induction Weekend are treated to a number of events, including scheduled autograph sessions, boxing exhibitions, a parade featuring past and present inductees, and the induction ceremony itself.\nThe Boxing Hall of Fame Las Vegas features the $75 million ESPN Classic Sports fight film and tape library and radio broadcast collection. The collection includes the fights of many great champions, including: Muhammad Ali, Mike Tyson, George Foreman, Roberto Dur\u00e1n, Marvin Hagler, Jack Dempsey, Joe Louis, Joe Frazier, Rocky Marciano and Sugar Ray Robinson. It is this exclusive fight film library that will separate the Boxing Hall of Fame Las Vegas from the other halls of fame which do not have rights to any video of their sports. The inaugural inductees included Muhammad Ali, Henry Armstrong, Tony Canzoneri, Ezzard Charles, Julio C\u00e9sar Ch\u00e1vez Sr., Jack Dempsey, Roberto Dur\u00e1n, Joe Louis, and Sugar Ray Robinson.\nBoxing rankings.\nThere are various organization and websites, that rank boxers in both weight class and pound-for-pound manner.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4246", "revid": "48877251", "url": "https://en.wikipedia.org/wiki?curid=4246", "title": "Hindi cinema", "text": " \nHindi cinema, popularly known as Bollywood and formerly as Bombay cinema, refers to India's Hindi-language film industry, based in Mumbai. The popular term Bollywood is a portmanteau of \"Bombay\" (former name of Mumbai) and \"Hollywood\". The industry, producing films in the Hindi language, is a part of the larger Indian cinema industry, which also includes South Indian cinema and other smaller film industries. The term 'Bollywood', often mistakenly used to refer to Indian cinema as a whole, only refers to Hindi-language films, with Indian cinema being an umbrella term that includes all the film industries in the country, each offering films in diverse languages and styles.\nIn 2017, Indian cinema produced 1,986 feature films, of which the largest number, 364, have been in Hindi. In 2022, Hindi cinema represented 33% of box office revenue, followed by Telugu and Tamil representing 20% and 16% respectively. Mumbai is one of the largest centres for film production in the world. Hindi films sold an estimated 341 million tickets in India in 2019. Earlier Hindi films tended to use vernacular Hindustani, mutually intelligible by speakers of either Hindi or Urdu, while modern Hindi productions increasingly incorporate elements of Hinglish.\nThe most popular commercial genre in Hindi cinema since the 1970s has been the masala film, which freely mixes different genres including action, comedy, romance, drama and melodrama along with musical numbers. Masala films generally fall under the musical film genre, of which Indian cinema has been the largest producer since the 1960s when it exceeded the American film industry's total musical output after musical films declined in the West. The first Indian talkie, \"Alam Ara\" (1931), was produced in the Hindustani language, four years after Hollywood's first sound film, \"The Jazz Singer\" (1927).\nAlongside commercial masala films, a distinctive genre of art films known as parallel cinema has also existed, presenting realistic content and avoidance of musical numbers. In more recent years, the distinction between commercial masala and parallel cinema has been gradually blurring, with an increasing number of mainstream films adopting the conventions which were once strictly associated with parallel cinema.\nEtymology.\n\"Bollywood\" is a portmanteau derived from Bombay (the former name of Mumbai) and \"Hollywood\", a shorthand reference for the American film industry which is based in Hollywood, California.\nThe term \"Tollywood\", for the Tollygunge-based cinema of West Bengal, predated \"Bollywood\". It was used in a 1932 \"American Cinematographer\" article by Wilford E. Deming, an American engineer who helped produce the first Indian sound picture.\n\"Bollywood\" was probably invented in Bombay-based film trade journals in the 1960s or 1970s, though the exact inventor varies by account. Film journalist Bevinda Collaco claims she coined the term for the title of her column in \"Screen\" magazine. Her column entitled \"On the Bollywood Beat\" covered studio news and celebrity gossip. Other sources state that lyricist, filmmaker and scholar Amit Khanna was its creator. It is unknown if it was derived from \"Hollywood\" through \"Tollywood\", or was inspired directly by \"Hollywood\".\nThe term has been criticised by some film journalists and critics, who believe it implies that the industry is a poor cousin of Hollywood.\nHistory.\nEarly history (1890s\u20131930s).\nIn 1897, a film presentation by Professor Stevenson featured a stage show at Calcutta's Star Theatre. With Stevenson's encouragement and camera, Hiralal Sen, an Indian photographer, made a film of scenes from that show, \"The Flower of Persia\" (1898). \"The Wrestlers\" (1899) by H. S. Bhatavdekar showed a wrestling match at the Hanging Gardens in Bombay.\nDadasaheb Phalke's silent film \"Raja Harishchandra\" (1913) is the first feature-length film made in India. The film, being silent, had English, Marathi, and Hindi-language intertitles. By the 1930s, the Indian film industry as a whole was producing over 200 films per year. The first Indian sound film, Ardeshir Irani's \"Alam Ara\" (1931), made in Hindustani language, was commercially successful. With a great demand for talkies and musicals, Hindustani cinema (as Hindi cinema was then known as) and the other language film industries quickly switched to sound films.\nChallenges and market expansion (1930s\u20131940s).\nThe 1930s and 1940s were tumultuous times; India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Although most early Bombay films were unabashedly escapist, a number of filmmakers tackled tough social issues or used the struggle for Indian independence as a backdrop for their films. Irani made the first Hindi colour film, \"Kisan Kanya\", in 1937. The following year, he made a colour version of \"Mother India\". However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were cinematic staples.\nThe decade of the 1940s saw an expansion of Bombay cinema's commercial market and its presence in the national consciousness. The year 1943 saw the arrival of Indian cinema's first 'blockbuster' offering, the movie \"Kismet\", which grossed in excess of the important barrier of one crore (10 million) rupees, made on a budget of only two lakh (200,000) rupees. The film tackled contemporary issues, especially those arising from the Indian Independence movement, and went on to become \"the longest running hit of Indian cinema\", a title it held till the 1970s. Film personalities like Bimal Roy, Sahir Ludhianvi and Prithviraj Kapoor participated in the creation of a national movement against colonial rule in India, while simultaneously leveraging the popular political movement to increase their own visibility and popularity. Themes from the Independence Movement deeply influenced Bombay film directors, screen-play writers, and lyricists, who saw their films in the context of social reform and the problems of the common people.\nBefore the Partition, the Bombay film industry was closely linked to the Lahore film industry (known as \"Lollywood\"; now part of the Pakistani film industry); both produced films in Hindustani (also known as Hindi-Urdu), the \"lingua franca\" of northern and central India. Another centre of Hindustani-language film production was the Bengal film industry in Calcutta, Bengal Presidency (now Kolkata, West Bengal), which produced Hindustani-language films and local Bengali language films. Around the same time, filmmakers and actors from the Calcutta film industry began migrating to Bombay; as a result, Bombay became the center of Hindustani-language film production.\nGolden age (late 1940s\u20131960s).\nThe period from the late 1940s to the early 1960s, after India's independence, is regarded by film historians as the Golden Age of Hindi cinema. Some of the most critically acclaimed Hindi films of all time were produced during this time. Examples include \"Pyaasa\" (1957) and \"Kaagaz Ke Phool\" (1959), directed by Guru Dutt; \"Awaara\" (1951) and \"Shree 420\" (1955), directed by Raj Kapoor, and \"Aan\" (1952), directed by Mehboob Khan and starring Dilip Kumar. The films explored social themes, primarily dealing with working-class life in India (particularly urban life) in the first two examples. \"Awaara\" presented the city as both nightmare and dream, and \"Pyaasa\" critiqued the unreality of urban life.\nGuru Dutt was lauded for his artistry, notably his usage of close-up shots, lighting, and depictions of melancholia. He directed a total of Hindi films, several of which have gained a cult following internationally. This includes \"Pyaasa\" (1957), which made its way onto \"Time\" magazine's 100 Greatest Movies list, as well as \"Kaagaz Ke Phool\" (1959), all of which are frequently listed among the greatest films in Hindi cinema. He was included among CNN's \"Top 25 Asian Actors\" in 2012.\nMehboob Khan's \"Mother India\" (1957), a remake of his earlier \"Aurat\" (1940), was the first Indian film nominated for the Academy Award for Best Foreign Language Film; it lost by a single vote. \"Mother India\" defined conventional Hindi cinema for decades. It spawned a genre of dacoit films, in turn defined by \"Gunga Jumna\" (1961). Written and produced by Dilip Kumar, \"Gunga Jumna\" was a dacoit crime drama about two brothers on opposite sides of the law (a theme which became common in Indian films during the 1970s). Some of the best-known epic films of Hindi cinema were also produced at this time, such as K. Asif's \"Mughal-e-Azam\" (1960). Other acclaimed mainstream Hindi filmmakers during this period included Kamal Amrohi and Vijay Bhatt.\nThe three most popular male Indian actors of the 1950s and 1960s were Dilip Kumar, Raj Kapoor, and Dev Anand, each with a unique acting style. Kapoor adopted Charlie Chaplin's tramp persona; Anand modeled himself on suave Hollywood stars like Gregory Peck and Cary Grant, and Kumar pioneered a form of method acting which predated Hollywood method actors such as Marlon Brando. Kumar, who was described as \"the ultimate method actor\" by Satyajit Ray, inspired future generations of Indian actors. Much like Brando's influence on Robert De Niro and Al Pacino, Kumar had a similar influence on Amitabh Bachchan, Naseeruddin Shah, Shah Rukh Khan and Nawazuddin Siddiqui. Veteran actresses such as Suraiya, Nargis, Sumitra Devi, Vyjayanthimala, Madhubala, Meena Kumari, Waheeda Rehman, Nutan, Sadhana and Mala Sinha have had their share of influence on Hindi cinema.\nWhile commercial Hindi cinema was thriving, the 1950s also saw the emergence of a parallel cinema movement. Although the movement (emphasising social realism) was led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of parallel cinema include (1946), directed by Khwaja Ahmad Abbas and based on the Bengal famine of 1943, (1946) directed by Chetan Anand and written by Khwaja Ahmad Abbas, and Bimal Roy's \"Do Bigha Zamin\" (1953). Their critical acclaim and the latter's commercial success paved the way for Indian neorealism and the Indian New Wave (synonymous with parallel cinema). Internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal, and Vijaya Mehta.\nAfter the social-realist film received the Palme d'Or at the inaugural 1946 Cannes Film Festival, Hindi films were frequently in competition for Cannes' top prize during the 1950s and early 1960s and some won major prizes at the festival. Guru Dutt, overlooked during his lifetime, received belated international recognition during the 1980s. Film critics polled by the British magazine \"Sight &amp; Sound\" included several of Dutt's films in a 2002 list of greatest films, and Time's All-Time 100 Movies lists \"Pyaasa\" as one of the greatest films of all time.\nDuring the late 1960s and early 1970s, the industry was dominated by musical romance films with romantic-hero leads.\nClassic Hindi cinema (1970s\u20131980s).\nBy 1970, Hindi cinema was thematically stagnant and dominated by musical romance films. The arrival of screenwriting duo Salim\u2013Javed (Salim Khan and Javed Akhtar) was a paradigm shift, revitalising the industry. They began the genre of gritty, violent, Bombay underworld crime films early in the decade with films such as \"Zanjeer\" (1973) and \"Deewaar\" (1975). Salim-Javed reinterpreted the rural themes of Mehboob Khan's \"Mother India\" (1957) and Dilip Kumar's \"Gunga Jumna\" (1961) in a contemporary urban context, reflecting the socio-economic and socio-political climate of 1970s India and channeling mass discontent, disillusionment and the unprecedented growth of slums with anti-establishment themes and those involving urban poverty, corruption and crime. Their \"angry young man\", personified by Amitabh Bachchan, reinterpreted Dilip Kumar's performance in \"Gunga Jumna\" in a contemporary urban context and anguished urban poor.\nBy the mid-1970s, romantic confections had given way to gritty, violent crime films and action films about gangsters (the Bombay underworld) and bandits (dacoits). Salim-Javed's writing and Amitabh Bachchan's acting popularised the trend with films such as \"Zanjeer\" and (particularly) \"Deewaar\", a crime film inspired by \"Gunga Jumna\" which pitted \"a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan\" (Bachchan); according to Danny Boyle, \"Deewaar\" was \"absolutely key to Indian cinema\". In addition to Bachchan, several other actors followed by riding the crest of the trend (which lasted into the early 1990s). Actresses from the era include Hema Malini, Jaya Bachchan, Raakhee, Shabana Azmi, Zeenat Aman, Parveen Babi, Rekha, Dimple Kapadia, Smita Patil, Jaya Prada and Padmini Kolhapure.\nThe name \"Bollywood\" was coined during the 1970s, when the conventions of commercial Hindi films were defined. Key to this was the masala film, which combines a number of genres (action, comedy, romance, drama, melodrama, and musical). The masala film was pioneered early in the decade by filmmaker Manmohan Desai, Prakash Mehra and Nasir Hussain, and the Salim-Javed screenwriting duo, pioneering the Bollywood-blockbuster format. \"Yaadon Ki Baarat\" (1973), directed by Hussain and written by Salim-Javed, has been identified as the first masala film and the first quintessentially \"Bollywood\" film. Salim-Javed wrote more successful masala films during the 1970s and 1980s. Masala films made Amitabh Bachchan the biggest star of the period. A landmark of the genre was \"Amar Akbar Anthony\" (1977), directed by Manmohan Desai and written by Kader Khan. Desai was considered the pioneer of making Masala films in the 1970's.\nBoth genres (masala and violent-crime films) are represented by the blockbuster \"Sholay\" (1975), written by Salim-Javed and starring Dharmendra and Amitabh Bachchan. It combined the dacoit film conventions of \"Mother India\" and \"Gunga Jumna\" with spaghetti Westerns, spawning the Dacoit Western (also known as the curry Western) which was popular during the 1970s.\nSome Hindi filmmakers, such as Shyam Benegal, Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta, continued to produce realistic parallel cinema throughout the 1970s. Although the art film bent of the Film Finance Corporation was criticised during a 1976 Committee on Public Undertakings investigation which accused the corporation of not doing enough to encourage commercial cinema, the decade saw the rise of commercial cinema with films such as \"Sholay\" (1975) which consolidated Amitabh Bachchan's position as a star. The devotional classic \"Jai Santoshi Ma\" was also released that year.\nBy 1983, the Bombay film industry was generating an estimated annual revenue of \u20b9 (\u20b9 7 billion, $), equivalent to $ (\u20b9, \u20b9 111.33 billion) when adjusted for inflation. By 1986, India's annual film output had increased from 741 films produced annually to 833 films annually, making India the world's largest film producer. The most internationally acclaimed Hindi film of the 1980s was Mira Nair's \"Salaam Bombay!\" (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.\nNew Hindi cinema (1990s onwards).\nHindi cinema experienced another period of box-office decline during the late 1980s with due to concerns by audiences over increasing violence and a decline in musical quality, and a rise in video piracy. One of the turning points came with such films as \"Qayamat Se Qayamat Tak\" (1988), presenting a blend of youthfulness, family entertainment, emotional intelligence and strong melodies, all of which lured audiences back to the big screen. It brought back the template for Bollywood musical romance films which went on to define 1990s Hindi cinema.\nKnown since the 1990s as \"New Bollywood\", contemporary Bollywood is linked to economic liberalization in India during the early 1990s. Early in the decade, the pendulum swung back toward family-centered romantic musicals, introducing a new generation of popular actors, including Aamir Khan, Ajay Devgan, Akshay Kumar, Hrithik Roshan, Shah Rukh Khan, and Salman Khan, who have starred in some of the most of the highest-grossing Bollywood films between 1990's and 2010's.\nThe 1990's also marked the entrance of new performers in art and independent films, some of which were commercially successful. The most influential example was \"Satya\" (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. Its critical and commercial success led to the emergence of a genre known as Mumbai noir: urban films reflecting the city's social problems. This led to a resurgence of parallel cinema by the end of the decade. The films featured actors whose performances were often praised by critics.\nThe 2000s saw increased Bollywood recognition worldwide due to growing (and prospering) NRI and South Asian diaspora communities overseas. Aditya Chopra and Karan Johar are considered to have started the \"NRI phase\" in Hindi cinema, which catered to the overseas population of Indians. The growth of the Indian economy and a demand for quality entertainment in this era led the country's film industry to new heights in production values, cinematography and screenwriting as well as technical advances in areas such as special effects and animation. Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films. Some popular films of the decade were \"Kaho Naa... Pyaar Hai\" (2000), \"Kabhi Khushi Kabhie Gham\" (2001), \"\" (2001), \"Lagaan\" (2001), \"Koi... Mil Gaya\" (2003), \"Kal Ho Naa Ho\" (2003), \"Munnabhai M.B.B.S.\" (2003), \"Veer-Zaara\" (2004), \"Rang De Basanti\" (2006), \"Lage Raho Munna Bhai\" (2006), \"Dhoom 2\" (2006), \"Krrish\" (2006), and \"Jab We Met\" (2007), among others, showing the rise of new movie stars.\nDuring the 2010s, the industry saw established stars such as making big-budget masala films like \"Dabangg\" (2010), \"Singham\" (2011)\",\" \"Ek Tha Tiger\" (2012), \"Son of Sardaar\" (2012), \"Rowdy Rathore\" (2012), \"Chennai Express\" (2013), \"Kick\" (2014) and \"Happy New Year\" (2014) with much-younger actresses. Although the films were often not praised by critics, they were commercially successful.\nMost stars from the 2000s continued successful careers into the next decade, and the 2000s-2010s saw a new generation of popular actors in different films, including Hrithik Roshan, Ranbir Kapoor and Ranveer Singh among others. Among new conventions, female-centred films such as \"The Dirty Picture\" (2011), \"Kahaani\" (2012), and \"Queen\" (2014), \"Pink\" (2016), \"Raazi\" (2018), \"Gangubai Kathiawadi\" (2022) and \"Crew\" started gaining wide financial success.\nInfluences on Hindi cinema.\nMoti Gokulsing and Wimal Dissanayake identify six major influences which have shaped Indian popular cinema:\nSharmistha Gooptu identifies Indo-Persian-Islamic culture as a major influence. During the early 20th century, Urdu was the lingua franca of popular cultural performance across northern India and established in popular performance art traditions such as nautch dancing, Urdu poetry, and Parsi theater. Urdu and related Hindi dialects were the most widely understood across northern India, and Hindustani became the standard language of early Indian talkies. Films based on \"Persianate adventure-romances\" led to a popular genre of \"\"Arabian Nights\" cinema\".\nScholars Chaudhuri Diptakirti and Rachel Dwyer and screenwriter Javed Akhtar identify Urdu literature as a major influence on Hindi cinema. Most of the screenwriters and scriptwriters of classic Hindi cinema came from Urdu literary backgrounds, from Khwaja Ahmad Abbas and Akhtar ul Iman to Salim\u2013Javed and Rahi Masoom Raza; a handful came from other Indian literary traditions, such as Bengali and Hindi literature. Most of Hindi cinema's classic scriptwriters wrote primarily in Urdu, including Salim-Javed, Gulzar, Rajinder Singh Bedi, Inder Raj Anand, Rahi Masoom Raza and Wajahat Mirza. Urdu poetry and the ghazal tradition strongly influenced filmi (Bollywood lyrics). Javed Akhtar was also greatly influenced by Urdu novels by Pakistani author Ibn-e-Safi, such as the \"Jasoosi Dunya\" and \"Imran\" series of detective novels; they inspired, for example, famous Bollywood characters such as Gabbar Singh in \"Sholay\" (1975) and Mogambo in \"Mr. India\" (1987).\nTodd Stadtman identifies several foreign influences on 1970s commercial Bollywood masala films, including New Hollywood, Italian exploitation films, and Hong Kong martial arts cinema. After the success of Bruce Lee films (such as \"Enter the Dragon\") in India, \"Deewaar\" (1975) and other Bollywood films incorporated fight scenes inspired by 1970s martial arts films from Hong Kong cinema until the 1990s. Bollywood action scenes emulated Hong Kong rather than Hollywood, emphasising acrobatics and stunts and combining kung fu (as perceived by Indians) with Indian martial arts such as pehlwani.\nInfluence of Hindi cinema.\nIndia.\nPerhaps Hindi cinema's greatest influence has been on India's national identity, where (with the rest of Indian cinema) it has become part of the \"Indian story\". In India, Bollywood is often associated with India's national identity. According to economist and Bollywood biographer Meghnad Desai, \"Cinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence\".\nScholar Brigitte Schulze has written that Indian films, most notably Mehboob Khan's \"Mother India\" (1957), played a key role in shaping the Republic of India's national identity in the early years after independence from the British Raj; the film conveyed a sense of Indian nationalism to urban and rural citizens alike. Bollywood has long influenced Indian society and culture as the biggest entertainment industry; many of the country's musical, dancing, wedding and fashion trends are Bollywood-inspired. Bollywood fashion trendsetters have included Madhubala in \"Mughal-e-Azam\" (1960) and Madhuri Dixit in \"Hum Aapke Hain Koun..!\" (1994).\nHindi films have also had a socio-political impact on Indian society, reflecting Indian politics. In classic 1970s Bollywood films, Bombay underworld crime films written by Salim\u2013Javed and starring Amitabh Bachchan such as \"Zanjeer\" (1973) and \"Deewaar\" (1975) reflected the socio-economic and socio-political realities of contemporary India. They channeled growing popular discontent and disillusionment and state failure to ensure welfare and well-being at a time of inflation, shortages, loss of confidence in public institutions, increasing crime and the unprecedented growth of slums. Salim-Javed and Bachchan's films dealt with urban poverty, corruption and organised crime; they were perceived by audiences as anti-establishment, often with an \"angry young man\" protagonist presented as a vigilante or anti-hero whose suppressed rage voiced the anguish of the urban poor.\nOverseas.\nHindi films have been a significant form of soft power for India, increasing its influence and changing overseas perceptions of India. In Germany, Indian stereotypes included bullock carts, beggars, sacred cows, corrupt politicians, and catastrophes before Bollywood and the IT industry transformed global perceptions of India. According to author Roopa Swaminathan, \"Bollywood cinema is one of the strongest global cultural ambassadors of a new India.\" Its role in expanding India's global influence is comparable to Hollywood's similar role with American influence. Monroe Township, Middlesex County, New Jersey, in the New York metropolitan area, has been profoundly impacted by Bollywood; this U.S. township has displayed one of the fastest growth rates of its Indian population in the Western Hemisphere, increasing from 256 (0.9%) as of the 2000 Census to an estimated 5,943 (13.6%) as of 2017, representing a 2,221.5% (a multiple of 23) numerical increase over that period, including many affluent professionals and senior citizens as well as charitable benefactors to the COVID-19 relief efforts in India in official coordination with Monroe Township, as well as actors with second homes.\nDuring the 2000s, Hindi cinema began influencing musical films in the Western world and was instrumental role in reviving the American musical film. Baz Luhrmann said that his musical film, \"Moulin Rouge!\" (2001), was inspired by Bollywood musicals; the film incorporated a Bollywood-style dance scene with a song from the film \"China Gate\". The critical and financial success of \"Moulin Rouge!\" began a renaissance of Western musical films such as \"Chicago\", \"Rent\", and \"Dreamgirls\".\nIndian film composer A. R. Rahman wrote the music for Andrew Lloyd Webber's \"Bombay Dreams\", and a musical version of \"Hum Aapke Hain Koun\" was staged in London's West End. The sports film \"Lagaan\" (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Hindi films (2002's \"Devdas\" and 2006's \"Rang De Basanti\") were nominated for the BAFTA Award for Best Film Not in the English Language.\nDanny Boyle's \"Slumdog Millionaire\" (2008), which won four Golden Globes and eight Academy Awards, was inspired by mainstream Hindi films and is considered an \"homage to Hindi commercial cinema\". It was also inspired by Mumbai-underworld crime films, such as \"Deewaar\" (1975), \"Satya\" (1998), \"Company\" (2002) and \"Black Friday\" (2007). \"Deewaar\" had a Hong Kong remake, \"The Brothers\" (1979), which inspired John Woo's internationally acclaimed breakthrough \"A Better Tomorrow\" (1986); the latter was a template for Hong Kong action cinema's heroic bloodshed genre. \"Angry young man\" 1970s epics such as \"Deewaar\" and \"Amar Akbar Anthony\" (1977) also resemble the heroic-bloodshed genre of 1980s Hong Kong action cinema.\nThe influence of \"filmi\" may be seen in popular music worldwide. Technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced a 1978 electronic album, \"Cochin Moon\", based on an experimental fusion of electronic music and Bollywood-inspired Indian music. Truth Hurts' 2002 song \"Addictive\", produced by DJ Quik and Dr. Dre, was lifted from Lata Mangeshkar's \"Thoda Resham Lagta Hai\" in \"Jyoti\" (1981). The Black Eyed Peas' Grammy Award winning 2005 song \"Don't Phunk with My Heart\" was inspired by two 1970s Bollywood songs: \"Ye Mera Dil Yaar Ka Diwana\" from \"Don\" (1978) and \"Ae Nujawan Hai Sub\" from \"Apradh\" (1972). Both songs were composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen.\nThe Kronos Quartet re-recorded several R. D. Burman compositions sung by Asha Bhosle for their 2005 album, \"You've Stolen My Heart: Songs from R.D. Burman's Bollywood\", which was nominated for Best Contemporary World Music Album at the 2006 Grammy Awards. \"Filmi\" music composed by A. R. Rahman (who received two Academy Awards for the \"Slumdog Millionaire\" soundtrack) has frequently been sampled by other musicians, including the Singaporean artist Kelly Poon, the French rap group La Caution and the American artist Ciara. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.\nGenres.\nHindi films are primarily musicals, and are expected to have catchy song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers. A film's music, song, and dance portions are usually produced first, and these are often released before the film itself, increasing its audience.\nIndian audiences expect value for money, and a good film is generally referred to as \"paisa vasool\" (literally, \"money's worth\"). Songs, dances, love triangles, comedy and dare-devil thrills are combined in a three-hour show (with an intermission). These are called \"masala films\", after the Hindi word for a spice mixture. Like \"masalas\", they are a mixture of action, comedy, and romance; most have heroes who can fight off villains single-handedly. Bollywood plots have tended to be melodramatic, frequently using formulaic ingredients such as star-crossed lovers, angry parents, love triangles, family ties, sacrifice, political corruption, kidnapping, villains, kind-hearted courtesans, long-lost relatives and siblings, reversals of fortune and serendipity.\nParallel cinema films tended to be less popular at the box office. A large Indian diaspora in English-speaking countries and increased Western influence in India have nudged Bollywood films closer to Hollywood.\nAccording to film critic Lata Khubchandani, \"Our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence [that] the censor board came into being and so did all the strictures.\" Although Bollywood plots feature Westernised urbanites dating and dancing in clubs rather than pre-arranged marriages, traditional Indian culture continues to exist outside the industry and is an element of resistance by some to Western influences. Bollywood plays a major role, however, in Indian fashion. Studies have indicated that some people, unaware that changing fashion in Bollywood films is often influenced by globalisation, consider the clothes worn by Bollywood actors as authentically Indian.\nScripts, dialogues, and lyrics.\nFilm scripts (known as dialogues in Indian English) and their song lyrics are often written by different people. Earlier, scripts were usually written in an unadorned Hindustani, which would be understood by the largest possible audience. Post-Independence, Hindi films tended to use a colloquial register of Hindustani, mutually intelligible by Hindi and Urdu speakers, but the use of the latter has declined over years. Some films have used regional dialects to evoke a village setting, or archaic Urdu in medieval historical films. A number of the dominant early scriptwriters of Hindi cinema primarily wrote in Urdu; Salim-Javed wrote in Urdu script, which was then transcribed by an assistant into Devanagari script so Hindi readers could read them. During the 1970s, Urdu writers Krishan Chander and Ismat Chughtai said that \"more than seventy-five per cent of films are made in Urdu\" but were categorised as Hindi films by the government. \"Encyclopedia of Hindi Cinema\" noted a number of top Urdu writers for preserving the language through film. Urdu poetry has strongly influenced Hindi film songs, whose lyrics also draw from the ghazal tradition (filmi-ghazal). According to Javed Akhtar in 1996, despite the loss of Urdu in Indian society, Urdu diction dominated Hindi film dialogue and lyrics.\nIn her book, \"The Cinematic ImagiNation\", Jyotika Virdi wrote about the presence and decline of Urdu in Hindi films. Virdi notes that although Urdu was widely used in classic Hindi cinema decades after partition because it was widely taught in pre-partition India, its use has declined in modern Hindi cinema: \"The extent of Urdu used in commercial Hindi cinema has not been stable ... the ultimate victory of Hindi in the official sphere has been more or less complete. This decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes. The fact is, for the most part, popular Hindi cinema has forsaken the florid Urdu that was part of its extravagance and retained a 'residual' Urdu, affected by an aggressive state policy that promoted a Sanskritized version of Hindi as the national language.\"\nContemporary mainstream films also use English, with the authors of an October 2005 \"South Asian Popular Culture\" article writing, \"English has begun to challenge the ideological work done by Urdu.\" Some film scripts are first written in Latin script. Characters may shift between languages to evoke a particular atmosphere (for example, English in a business setting and Hindi in an informal one). The blend of Hindi and English sometimes heard in modern Hindi films, known as Hinglish, has become increasingly common.\nBefore and following the turn of the millennium, cinematic language (in dialogues or lyrics) would often be melodramatic, invoking God, family, mother, duty, and self-sacrifice. Song lyrics are often about love and, especially in older films, frequently used the poetic vocabulary of court Urdu, with a number of Persian loanwords. Another source for love lyrics in films such as \"Jhanak Jhanak Payal Baje\" and \"Lagaan\" is the long Hindu tradition of poetry about the loves of Krishna, Radha, and the \"gopi\"s.\nMusic directors often prefer working with certain lyricists, and the lyricist and composer may be seen as a team. This phenomenon has been compared to the pairs of American composers and songwriters who created classic Broadway musicals.\nBefore 2008, Bollywood scripts were often handwritten because, in the industry, there is a perception that manual writing is the quickest way to create scripts.\nSound.\nSound in early Bollywood films was usually not recorded on location (sync sound). It was usually created (or re-created) in the studio, with the actors speaking their lines in the studio and sound effects added later; this created synchronisation problems. Commercial Indian films are known for their lack of ambient sound, and the Arriflex 3 camera necessitated dubbing. \"Lagaan\" (2001) was filmed with sync sound, and several Bollywood films have recorded on-location sound since then.\nFemale makeup artists.\nIn 1955, the Bollywood Cine Costume Make-Up Artist &amp; Hair Dressers' Association (CCMAA) ruled that female makeup artists were barred from membership. The Supreme Court of India ruled in 2014 that the ban violated Indian constitutional guarantees under Article 14 (right to equality), 19(1)(g) (freedom to work) and Article 21 (right to liberty). According to the court, the ban had no \"rationale nexus\" to the cause sought to be achieved and was \"unacceptable, impermissible and inconsistent\" with the constitutional rights guaranteed to India's citizens. The court also found illegal the rule which mandated that for any artist to work in the industry, they must have lived for five years in the state where they intend to work. In 2015, it was announced that Charu Khurana was the first woman registered by the Cine Costume Make-Up Artist &amp; Hair Dressers' Association.\nSong and dance.\nBollywood film music is called \"filmi\" (from the Hindi \"of films\"). Bollywood songs were introduced with Ardeshir Irani's \"Alam Ara\" (1931) song, \"De De Khuda Ke Naam pay pyaare\". Bollywood songs are generally pre-recorded by professional playback singers, with the actors then lip syncing the words to the song on-screen (often while dancing). Although most actors are good dancers, few are also singers; a notable exception was Kishore Kumar, who starred in several major films during the 1950s while having a rewarding career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were known as singers and actors, and some actors in the last thirty years have sung one or more songs themselves.\nSongs can make and break a film, determining whether it will be a flop or a hit: \"Few films without successful musical tracks, and even fewer without any songs and dances, succeed\". Globalization has changed Bollywood music, with lyrics an increasing mix of Hindi and English. Global trends such as salsa, pop and hip hop have influenced the music heard in Bollywood films.\nPlayback singers are featured in the opening credits, and have fans who will see an otherwise-lackluster film to hear their favourites. Notable singers are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam, Alka Yagnik and Shreya Goshal (female), and K. L. Saigal, Kishore Kumar, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam (male). Composers of film music, known as music directors, are also well-known. Remixing of film songs with modern rhythms is common, and producers may release remixed versions of some of their films' songs with the films' soundtrack albums.\nDancing in Bollywood films, especially older films, is modeled on Indian dance: classical dance, dances of north-Indian courtesans (tawaif) or folk dances. In modern films, Indian dance blends with Western dance styles as seen on MTV or in Broadway musicals; Western pop and classical-dance numbers are commonly seen side-by-side in the same film. The hero (or heroine) often performs with a troupe of supporting dancers. Many song-and-dance routines in Indian films contain unrealistically-quick shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in natural surroundings or architecturally-grand settings.\nSongs typically comment on the action taking place in the film. A song may be worked into the plot, so a character has a reason to sing. It may externalise a character's thoughts, or presage an event in the film (such as two characters falling in love). The songs are often referred to as a \"dream sequence\", with things happening which would not normally happen in the real world. Song and dance scenes were often filmed in Kashmir but, due to political unrest in Kashmir since the end of the 1980s, they have been shot in western Europe (particularly Switzerland and Austria).\nContemporary movie stars attracted popularity as dancers, including Madhuri Dixit, Hrithik Roshan, Aishwarya Rai Bachchan, Sridevi, Meenakshi Seshadri, Malaika Arora Khan, Shahid Kapoor, Katrina Kaif and Tiger Shroff. Older dancers include Helen (known for her cabaret numbers), Madhubala, Vyjanthimala, Padmini, Hema Malini, Mumtaz, Cuckoo Moray, Parveen Babi , Waheeda Rahman, Meena Kumari, and Shammi Kapoor.\nFilm producers have been releasing soundtracks (as tapes or CDs) before a film's release, hoping that the music will attract audiences; a soundtrack is often more popular than its film. Some producers also release music videos, usually (but not always) with a song from the film.\nFinances.\nBollywood films are multi-million dollar productions, with the most expensive productions costing up to \u20b9 1 billion (about US$20\u00a0million). The science-fiction film \"Ra.One\" was made on a budget of \u20b9 1.35 billion (about $27 million), making it the most expensive Bollywood film of all time. Sets, costumes, special effects and cinematography were less than world-class, with some notable exceptions, until the mid-to-late 1990s. As Western films and television are more widely distributed in India, there is increased pressure for Bollywood films to reach the same production levels (particularly in action and special effects). Recent Bollywood films, like \"Krrish\" (2006), have employed international technicians such as Hong Kong-based action choreographer Tony Ching. The increasing accessibility of professional action and special effects, coupled with rising film budgets, have seen an increase in action and science-fiction films.\nSince overseas scenes are attractive at the box office, Mumbai film crews are filming in Australia, Canada, New Zealand, the United Kingdom, the United States, Europe and elsewhere. Indian producers have also obtained funding for big-budget films shot in India, such as \"Lagaan\" and \"Devdas\".\nFunding for Bollywood films often comes from private distributors and a few large studios. Although Indian banks and financial institutions had been forbidden from lending to film studios, the ban has been lifted. Finances are not regulated; some funding comes from illegitimate sources such as the Mumbai underworld, which is known to influence several prominent film personalities. Mumbai organised-crime hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan, in January 2000. In 2001, the Central Bureau of Investigation seized all prints of \"Chori Chori Chupke Chupke\" after the film was found to be funded by members of the Mumbai underworld.\nAnother problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before they are released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is an established small-scale industry in parts of south and southeast Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually from unlicensed home videos and DVDs. In addition to the homegrown market, demand for these copies is large amongst portions of the Indian diaspora. Bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Pakistani government has banned their sale, distribution and telecast. Films are frequently broadcast without compensation by small cable-TV companies in India and other parts of South Asia. Small convenience stores, run by members of the Indian diaspora in the US and the UK, regularly stock tapes and DVDs of dubious provenance; consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to industry losses.\nSatellite TV, television and imported foreign films are making inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now, fewer do. Most Bollywood producers make money, however, recouping their investments from many sources of revenue (including the sale of ancillary rights). There are increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly being noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films. In 2002, Bollywood sold 3.6 billion tickets and had a total revenue (including theatre tickets, DVDs and television) of $1.3 billion; Hollywood films sold 2.6 billion tickets, and had a total revenue of $51 billion.\nAdvertising.\nA number of Indian artists hand-painted movie billboards and posters. M. F. Husain painted film posters early in his career; human labour was found to be cheaper than printing and distributing publicity material. Most of the large, ubiquitous billboards in India's major cities are now created with computer-printed vinyl. Old hand-painted posters, once considered ephemera, are collectible folk art.\nReleasing film music, or music videos, before a film's release may be considered a form of advertising. A popular tune is believed to help attract audiences. Bollywood publicists use the Internet as a venue for advertising. Most bigger-budget films have a websites on which audiences can view trailers, stills and information on the story, cast, and crew. Bollywood is also used to advertise other products. Product placement, used in Hollywood, is also common in Bollywood.\nInternational filming.\nBollywood's increasing use of international settings such as Switzerland, London, Paris, New York, Mexico, Brazil and Singapore does not necessarily represent the people and cultures of those locales. Contrary to these spaces and geographies being filmed as they are, they are actually Indianised by adding Bollywood actors and Hindi speaking extras to them. While immersing in Bollywood films, viewers get to see their local experiences duplicated in different locations around the world.\nAccording to Shakuntala Rao, \"Media representation can depict India's shifting relation with the world economy, but must retain its 'Indianness' in moments of dynamic hybridity\"; \"Indianness\" (cultural identity) poses a problem with Bollywood's popularity among varied diaspora audiences, but gives its domestic audience a sense of uniqueness from other immigrant groups.\nAwards.\nThe Filmfare Awards are some of the most prominent awards given to Hindi films in India. The Indian screen magazine \"Filmfare\" began the awards in 1954 (recognising the best films of 1953), and they were originally known as the Clare Awards after the magazine's editor. Modeled on the Academy of Motion Picture Arts and Sciences' poll-based merit format, individuals may vote in separate categories. A dual voting system was developed in 1956.\nThe National Film Awards were also introduced in 1954. The Indian government has sponsored the awards, given by its Directorate of Film Festivals (DFF), since 1973. The DFF screens Bollywood films, films from the other regional movie industries, and independent/art films. The awards are made at an annual ceremony presided over by the president of India. Unlike the Filmfare Awards, which are chosen by the public and a committee of experts, the National Film Awards are decided by a government panel.\nOther awards ceremonies for Hindi films in India are the Screen Awards (begun in 1995) and the Stardust Awards, which began in 2003. The International Indian Film Academy Awards (begun in 2000) and the Zee Cine Awards, begun in 1998, are held abroad in a different country each year.\nGlobal markets.\nIn addition to their popularity among the Indian diaspora from Nigeria and Senegal to Egypt and Russia, generations of non-Indians have grown up with Bollywood. Indian cinema's early contacts with other regions made inroads into the Soviet Union, the Middle East, Southeast Asia, and China. Bollywood entered the consciousness of Western audiences and producers during the late 20th century, and Western actors now seek roles in Bollywood films.\nAsia-Pacific.\nSouth Asia.\nBollywood films are also popular in Bangladesh, Pakistan and Nepal, where Hindustani language is widely understood. Many Pakistanis understand Hindi, due to its linguistic similarity to Urdu. Although Pakistan banned the import of Bollywood films in 1965, trade in unlicensed DVDs and illegal cable broadcasts ensured their continued popularity. Exceptions to the ban were made for a few films, such as the colourised re-release of \"Mughal-e-Azam\" and \"Taj Mahal\" in 2006. Early in 2008, the Pakistani government permitted the import of 16 films. More easing followed in 2009 and 2010. Although it is opposed by nationalists and representatives of Pakistan's small film industry, it is embraced by cinema owners who are making a profit after years of low receipts. Bollywood films in Nepal earn more than Nepali films, and Salman Khan, Akshay Kumar and Shah Rukh Khan are popular in the country.\nThe films are also popular in Afghanistan due to its proximity to the Indian subcontinent and their cultural similarities, particularly in music. Popular actors include Shah Rukh Khan, Ajay Devgan, Sunny Deol, Aishwarya Rai, Preity Zinta, and Madhuri Dixit. A number of Bollywood films were filmed in Afghanistan and some dealt with the country, including \"Dharmatma\", \"Kabul Express\", \"Khuda Gawah\" and \"Escape From Taliban\".\nSoutheast Asia.\nBollywood films are popular in Southeast Asia, particularly in maritime Southeast Asia. The three Khans are very popular in the Malay world, including Indonesia, Malaysia, and Singapore. The films are also fairly popular in Thailand.\nIndia has cultural ties with Indonesia, and Bollywood films were introduced to the country at the end of World War II in 1945. The \"angry young man\" films of Amitabh Bachchan and Salim\u2013Javed were popular during the 1970s and 1980s before Bollywood's popularity began gradually declining in the 1980s and 1990s. It experienced an Indonesian revival with the release of Shah Rukh Khan's \"Kuch Kuch Hota Hai\" (1998) in 2001, which was a bigger box-office success in the country than \"Titanic\" (1997). Bollywood has had a strong presence in Indonesia since then, particularly Shah Rukh Khan films such as \"Mohabbatein\" (2000), \"Kabhi Khushi Kabhie Gham\" (2001), \"Kal Ho Naa Ho\", \"Chalte Chalte\" and \"Koi...\u00a0Mil Gaya\" (all 2003), and \"Veer-Zaara\" (2004).\nEast Asia.\nSome Bollywood films have been widely appreciated in China, Japan, and South Korea. Several Hindi films have been commercially successful in Japan, including Mehboob Khan's \"Aan\" (1952, starring Dilip Kumar) and Aziz Mirza's \"Raju Ban Gaya Gentleman\" (1992, starring Shah Rukh Khan). The latter sparked a two-year boom in Indian films after its 1997 release, with \"Dil Se..\" (1998) a beneficiary of the boom. The highest-grossing Hindi film in Japan is \"3 Idiots\" (2009), starring Aamir Khan, which received a Japanese Academy Award nomination. The film was also a critical and commercial success in South Korea.\n\"Dr. Kotnis Ki Amar Kahani\", \"Awaara\", and \"Do Bigha Zamin\" were successful in China during the 1940s and 1950s, and remain popular with their original audience. Few Indian films were commercially successful in the country during the 1970s and 1980s, among them Tahir Hussain's \"Caravan\", \"Noorie\" and \"Disco Dancer\". Indian film stars popular in China included Raj Kapoor, Nargis, and Mithun Chakraborty. Hindi films declined significantly in popularity in China during the 1980s. Films by Aamir Khan have recently been successful, and \"Lagaan\" was the first Indian film with a nationwide Chinese release in 2011. Chinese filmmaker He Ping was impressed by \"Lagaan\" (particularly its soundtrack), and hired its composer A. R. Rahman to score his \"Warriors of Heaven and Earth\" (2003).\nWhen \"3 Idiots\" was released in China, China was the world's 15th-largest film market (partly due to its widespread pirate DVD distribution at the time). The pirate market introduced the film to Chinese audiences, however, and it became a cult hit. According to the Douban film-review site, \"3 Idiots\" is China's 12th-most-popular film of all time; only one domestic Chinese film (\"Farewell My Concubine\") ranks higher, and Aamir Khan acquired a large Chinese fan base as a result. After \"3 Idiots\", several of Khan's other films (including 2007's and 2008's \"Ghajini\") also developed cult followings. China became the world's second-largest film market (after the United States) by 2013, paving the way for Khan's box-office success with \"Dhoom 3\" (2013), \"PK\" (2014), and \"Dangal\" (2016). The latter is the 16th-highest-grossing film in China, the fifth-highest-grossing non-English language film worldwide, and the highest-grossing non-English foreign film in any market. Several Khan films, including , \"3 Idiots\", and \"Dangal\", are highly rated on Douban. His next film, \"Secret Superstar\" (2017, starring Zaira Wasim), broke \"Dangal\"'s record for the highest-grossing opening weekend by an Indian film and cemented Khan's status as \"a king of the Chinese box office\"; \"Secret Superstar\" was China's highest-grossing foreign film of 2018 to date. Khan has become a household name in China, with his success described as a form of Indian soft power improving China\u2013India relations despite political tensions. With Bollywood competing with Hollywood in the Chinese market, the success of Khan's films has driven up the price for Chinese distributors of Indian film imports. Salman Khan's \"Bajrangi Bhaijaan\" and Irrfan Khan's \"Hindi Medium\" were also Chinese hits in early 2018.\nOceania.\nAlthough Bollywood is less successful on some Pacific islands such as New Guinea, it ranks second to Hollywood in Fiji (with its large Indian minority), Australia and New Zealand. Australia also has a large South Asian diaspora, and Bollywood is popular amongst non-Asians in the country as well. Since 1997, the country has been a backdrop for an increasing number of Bollywood films. Indian filmmakers, attracted to Australia's diverse locations and landscapes, initially used the country as a setting for song-and-dance scenes; however, Australian locations now figure in Bollywood film plots. Hindi films shot in Australia usually incorporate Australian culture. Yash Raj Films' \"Salaam Namaste\" (2005), the first Indian film shot entirely in Australia, was the most successful Bollywood film of 2005 in that country. It was followed by the box-office successes \"Heyy Babyy\", (2007) \"Chak De! India\" (2007), and \"Singh Is Kinng\" (2008). Prime Minister John Howard said during a visit to India after the release of \"Salaam Namaste\" that he wanted to encourage Indian filmmaking in Australia to increase tourism, and he appointed Steve Waugh as tourism ambassador to India. Australian actress Tania Zaetta, who appeared in \"Salaam Namaste\" and several other Bollywood films, was eager to expand her career in Bollywood.\nEastern Europe and Central Asia.\nBollywood films are popular in the former Soviet Union (Russia, Eastern Europe, and Central Asia), and have been dubbed into Russian. Indian films were more popular in the Soviet Union than Hollywood films and, sometimes, domestic Soviet films. The first Indian film released in the Soviet Union was \"Dharti Ke Lal\" (1946), directed by Khwaja Ahmad Abbas and based on the Bengal famine of 1943, in 1949. Three hundred Indian films were released in the Soviet Union after that; most were Bollywood films with higher average audience figures than domestic Soviet productions. Fifty Indian films had over 20 million viewers, compared to 41 Hollywood films. Some, such as \"Awaara\" (1951) and \"Disco Dancer\" (1982), had more than 60 million viewers and established actors Raj Kapoor, Nargis, Rishi Kapoor and Mithun Chakraborty in the country.\nAccording to diplomat Ashok Sharma, who served in the Commonwealth of Independent States,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The popularity of Bollywood in the CIS dates back to the Soviet days when the films from Hollywood and other Western cinema centers were banned in the Soviet Union. As there was no means of other cheap entertainment, the films from Bollywood provided the Soviets a cheap source of entertainment as they were supposed to be non-controversial and non-political. In addition, the Soviet Union was recovering from the onslaught of the Second World War. The films from India, which were also recovering from the disaster of partition and the struggle for freedom from colonial rule, were found to be a good source of providing hope with entertainment to the struggling masses. The aspirations and needs of the people of both countries matched to a great extent. These films were dubbed in Russian and shown in theatres throughout the Soviet Union. The films from Bollywood also strengthened family values, which was a big factor for their popularity with the government authorities in the Soviet Union.\nAfter the collapse of the Soviet film-distribution system, Hollywood filled the void in the Russian film market and Bollywood's market share shrank.\nIn Poland, Shah Rukh Khan has a large following. He was introduced to Polish audiences with the 2005 release of \"Kabhi Khushi Kabhie Gham\" (2001) and his other films, including \"Dil Se..\" (1998), \"Main Hoon Na\" (2004) and \"Kabhi Alvida Naa Kehna\" (2006), became hits in the country. Bollywood films are often covered in \"Gazeta Wyborcza\", formerly Poland's largest newspaper.\n\"Squad\" (2021) is the first Indian film to be shot in Belarus. A majority of the film was shot at Belarusfilm studios, in Minsk.\nMiddle East and North Africa.\nHindi films have become popular in Arab countries,\nand imported Indian films are usually subtitled in Arabic when they are released. Bollywood has progressed in Israel since the early 2000s, with channels dedicated to Indian films on cable television; MBC Bollywood and Zee Aflam show Hindi movies and serials.\nIn Egypt, Bollywood films were popular during the 1970s and 1980s. In 1987, however, they were restricted to a handful of films by the Egyptian government. Amitabh Bachchan has remained popular in the country and Indian tourists visiting Egypt are asked, \"Do you know Amitabh Bachchan?\"\nBollywood movies are regularly screened in Dubai cinemas, and Bollywood is becoming popular in Turkey; \"Barfi!\" was the first Hindi film to have a wide theatrical release in that country. Bollywood also has viewers in Central Asia (particularly Uzbekistan and Tajikistan).\nSouth America.\nBollywood films are not influential in most of South America, although its culture and dance is recognised. Due to significant South Asian diaspora communities in Suriname and Guyana, however, Hindi-language movies are popular. In 2006, \"Dhoom 2\" became the first Bollywood film to be shot in Rio de Janeiro. In January 2012, it was announced that UTV Motion Pictures would begin releasing films in Peru with \"Guzaarish\".\nAfrica.\nHindi films were originally distributed to some parts of Africa by Lebanese businessmen. In the 1950s, Hindi and Egyptian films were generally more popular than Hollywood films in East Africa. By the 1960s, East Africa was one of the largest overseas export markets for Indian films, accounting for about 20-50% of global earnings for many Indian films.\n\"Mother India\" (1957) continued to be screened in Nigeria decades after its release. Indian movies have influenced Hausa clothing, songs have been covered by Hausa singers, and stories have influenced Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Nigeria's Northern Region, and posters of Indian films hang on the walls of tailoring shops and mechanics' garages. Unlike Europe and North America, where Indian films cater to the expatriate market, Bollywood films became popular in West Africa despite the lack of a significant Indian audience. One possible explanation is cultural similarity: the wearing of turbans, animals in markets; porters carrying large bundles, and traditional wedding celebrations. Within Muslim culture, Indian movies were said to show \"respect\" toward women; Hollywood movies were seen as having \"no shame\". In Indian movies, women are modestly dressed; men and women rarely kiss and there is no nudity, so the films are said to \"have culture\" which Hollywood lacks. The latter \"don't base themselves on the problems of the people\"; Indian films are based on socialist values and the reality of developing countries emerging from years of colonialism. Indian movies permitted a new youth culture without \"becoming Western.\" The first Indian film shot in Mauritius was \"Souten\", starring Rajesh Khanna, in 1983.\nIn South Africa, film imports from India were watched by black and Indian audiences. Several Bollywood figures have travelled to Africa for films and off-camera projects. \"Padmashree Laloo Prasad Yadav\" (2005) was filmed in South Africa. \"Dil Jo Bhi Kahey...\" (2005) was also filmed almost entirely in Mauritius, which has a large ethnic-Indian population.\nBollywood, however, seems to be diminishing in popularity in Africa. New Bollywood films are more sexually explicit and violent. Nigerian viewers observed that older films (from the 1950s and 1960s) had more culture and were less Westernised. The old days of India avidly \"advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories\" were replaced. The emergence of Nollywood (West Africa's film industry) has also contributed to the declining popularity of Bollywood films, as sexualised Indian films became more like American films.\nKishore Kumar and Amitabh Bachchan have been popular in Egypt and Somalia. In Ethiopia, Bollywood movies are shown with Hollywood productions in town square theatres such as the Cinema Ethiopia in Addis Ababa. Less-commercial Bollywood films are also screened elsewhere in North Africa.\nWestern Europe and North America.\nThe first Indian film to be released in the Western world and receive mainstream attention was \"Aan\" (1952), directed by Mehboob Khan and starring Dilip Kumar and Nimmi. It was subtitled in 17 languages and released in 28 countries, including the United Kingdom, the United States, and France. \"Aan\" received significant praise from British critics, and \"The Times\" compared it favourably to Hollywood productions. Mehboob Khan's later Academy Award-nominated \"Mother India\" (1957) was a success in overseas markets, including Europe, Russia, the Eastern Bloc, French territories, and Latin America.\nMany Bollywood films have been commercially successful in the United Kingdom. The most successful Indian actor at the British box office has been Shah Rukh Khan, whose popularity in British Asian communities played a key role in introducing Bollywood to the UK with films such as \"Darr\" (1993), \"Dilwale Dulhaniya Le Jayenge\" (1995), and \"Kuch Kuch Hota Hai\" (1998). \"Dil Se\" (1998) was the first Indian film to enter the UK top ten. A number of Indian films, such as \"Dilwale Dulhaniya Le Jayenge\" and \"Kabhi Khushi Kabhie Gham\" (2001), have been set in London.\nBollywood is also appreciated in France, Germany, the Netherlands, and Scandinavia. Bollywood films are dubbed in German and shown regularly on the German television channel RTL II. Germany is the second-largest European market for Indian films, after the United Kingdom. The most recognised Indian actor in Germany is Shah Rukh Khan, who has had box-office success in the country with films such as \"Don 2\" (2011) and \"Om Shanti Om\" (2007). He has a large German fan base, particularly in Berlin (where the tabloid \"Die Tageszeitung\" compared his popularity to that of the pope).\nBollywood has experienced revenue growth in Canada and the United States, particularly in the South Asian communities of large cities such as Toronto, Chicago, and New York City. Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earned about $100 million per year in theatre screenings, video sales and the sale of movie soundtracks; Indian films earn more money in the United States than films from any other non-English speaking country. Since the mid-1990s, a number of Indian films have been largely (or entirely) shot in New York, Los Angeles, Vancouver or Toronto. Films such as \"The Guru\" (2002) and \"\" (2007) attempted to popularise Bollywood for Hollywood.\nPlagiarism.\nPressured by rushed production schedules and tight deadlines, some Hindi cinema writers and musicians have been notorious for plagiarising. Ideas, plot lines, tunes or riffs have been copied from other Indian film industries (including Telugu cinema, Tamil cinema, Malayalam cinema and others) or foreign films (including Hollywood and other Asian films) without acknowledging the source.\nBefore the 1990s, plagiarism occurred with impunity. Copyright enforcement was lax in India, and few actors or directors saw an official contract. The Hindi film industry was not widely known in the Global North (except in the Soviet states), who would be unaware that their material had been copied. Audiences may not have been aware of plagiarism, since many in India were unfamiliar with foreign films and music. Although copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are more aware of each other and Indian audiences are more familiar with foreign films and music. Organisations such as the India EU Film Initiative seek to foster a community between filmmakers and industry professionals in India and the European Union.\nA commonly reported justification for plagiarism in Bollywood is that cautious producers want to remake popular Hollywood films in an Indian context. Although screenwriters generally produce original scripts, many are rejected due to uncertainty about whether a film will be successful. Poorly-paid screenwriters have also been criticised for a lack of creativity. Some filmmakers see plagiarism in Bollywood as an integral part of globalisation, with which Western (particularly American) culture is embedding itself into Indian culture. Vikram Bhatt, director of \"Raaz\" (a remake of \"What Lies Beneath\") and \"Kasoor\" (a remake of \"Jagged Edge\"), has spoken about the influence of American culture and Bollywood's desire to produce box-office hits based along the same lines: \"Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture.\" According to Mahesh Bhatt, \"If you hide the source, you're a genius. There's no such thing as originality in the creative sphere\".\nAlthough very few cases of film-copyright violations have been taken to court because of a slow legal process, the makers of \"Partner\" (2007) and \"Zinda\" (2005) were targeted by the owners and distributors of the original films: \"Hitch\" and \"Oldboy\". The American studio 20th Century Fox brought Mumbai-based B. R. Films to court over the latter's forthcoming \"Banda Yeh Bindaas Hai\", which Fox alleged was an illegal remake of \"My Cousin Vinny\". B. R. Films eventually settled out of court for about $200,000, paving the way for its film's release. Some studios comply with copyright law; in 2008, Orion Pictures secured the rights to remake Hollywood's \"Wedding Crashers\".\nMusic.\nThe Pakistani Qawwali musician Nusrat Fateh Ali Khan had a major impact on Hindi film music, inspiring numerous Indian musicians working in Bollywood, especially during the 1990s. However, there were many instances of Indian music directors plagiarising Khan's music to produce hit filmi songs. Several popular examples include Viju Shah's hit song \"Tu Cheez Badi Hai Mast Mast\" in \"Mohra\" (1994) being plagiarised from Khan's popular Qawwali song \"Dam Mast Qalandar\", \"Mera Piya Ghar Aya\" used in \"Yaarana\" (1995), and \"Sanoo Ek Pal Chain Na Aaye\" in \"Judaai\" (1997). Despite the significant number of hit Bollywood songs plagiarised from his music, Nusrat Fateh Ali Khan reportedly tolerated plagiarism. One of the Bollywood music directors who frequently plagiarised him, Anu Malik, claimed that he loved Khan's music and was actually showing admiration by using his tunes. However, Khan was reportedly aggrieved when Malik turned his spiritual \"Allah Hoo, Allah Hoo\" into \"I Love You, I Love You\" in \"Auzaar\" (1997). Khan said \"he has taken my devotional song \"Allahu\" and converted it into \"I love you\". He should at least respect my religious songs.\"\nBollywood soundtracks also plagiarised Guinean singer Mory Kant\u00e9, particularly his 1987 album \"Akwaba Beach\". His song, \"Tama\", inspired two Bollywood songs: Bappi Lahiri's \"Tamma Tamma\" in \"Thanedaar\" (1990) and \"Jumma Chumma\" in Laxmikant\u2013Pyarelal's soundtrack for \"Hum\" (1991). The latter also featured \"Ek Doosre Se\", which copied Kant\u00e9's \"Inch Allah\". His song \"Y\u00e9 k\u00e9 y\u00e9 k\u00e9\" was used as background music in the 1990 Bollywood film \"Agneepath\", inspired the Bollywood song \"Tamma Tamma\" in \"Thanedaar\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4248", "revid": "20931057", "url": "https://en.wikipedia.org/wiki?curid=4248", "title": "Bowls", "text": "Sport involving rolling biased balls\nBowls, also known as lawn bowls or lawn bowling, is a sport in which players try to roll their ball (called a bowl) closest to a smaller ball (known as a \"jack\" or sometimes a \"kitty\"). The bowls are shaped (biased), so that they follow a curved path when being rolled. The game is played either in teams or one against one.\nThe game was first played in the 13th century. The game is played on grass, although other surfaces are sometimes used. Matches are held either until one player gets to a score, or when a number of \"ends\" are played.\nThe game is mostly played on a bowling green, which can vary by the type of bowls being played. Whilst the game is often played outdoors, there are indoor bowling venues, and can also be played on rollable carpets. For outdoor games, this is usually on grass; however, it can also be played on cotula in New Zealand.\nHistory.\nBowls is a variant of the \"boules\" games (Italian: \"bocce\"), which, in their general form, are of ancient or prehistoric origin. Ancient Greek variants are recorded that involved throwing light objects (such as flat stones, coins, or later also stone balls) as far as possible. The aspect of tossing the balls to approach a target as closely as possible is recorded in ancient Rome. This game was spread to Roman Gaul by soldiers or sailors. A Roman sepulchre in Florence shows people playing this game, stooping down to measure the points.\nBowls in England has been traced certainly to the 13th century, and conjecturally to the 12th century. William Fitzstephen (d. about 1190), in his biography of Thomas Becket, gives a graphic sketch of the London of his day and, writing of the summer amusements of young men, says that on holidays they were \"exercised in Leaping, Shooting, Wrestling, Casting of Stones [\"in jactu lapidum\"], and Throwing of Javelins fitted with Loops for the Purpose, which they strive to fling before the Mark; they also use Bucklers, like fighting Men.\" It is commonly supposed that by \"jactus lapidum\", Fitzstephen refers to an early variety of bowls, possibly played using round stone; there is a record of iron bowls being used, though at a much later date, on festive occasions at Nairn.. On the other hand, the \"jactus lapidum\" of which he speaks may have been more akin to shot put.\nIt is clear, at any rate, that a rudimentary form of the game was played in England in the 13th century. A manuscript of that period in the royal library, Windsor (No. 20, E iv.), contains a drawing representing two players aiming at a small cone instead of an earthenware ball or jack. The world's oldest surviving bowling green is the Southampton Old Bowling Green, which was first used in 1299.\nAnother manuscript of the same century has a crude but spirited picture which brings us into close touch with the existing game. Three figures are introduced and a jack. The first player's bowl has come to rest just in front of the jack; the second has delivered his bowl and is following after it with one of those eccentric contortions still not unusual on modern greens, the first player meanwhile making a repressive gesture with his hand, as if to urge the bowl to stop short of his own; the third player is depicted as in the act of delivering his bowl. A 14th-century manuscript, \"Book of Prayers\", in the Francis Douce collection in the Bodleian Library at Oxford, contains a drawing in which two persons are shown, but they bowl to no mark. Strutt (Sports and Pastimes) suggests that the first player's bowl may have been regarded by the second player as a species of jack; but in that case it is not clear what was the first player's target. In these three earliest illustrations of the pastime each player has one bowl only, and that the attitude in delivering it was as various five or six hundred years ago as it is today. In the third, he stands almost upright; in the first, he kneels; in the second, he stoops, halfway between the upright and the kneeling position.\nThe game eventually came under the ban of king and Parliament, both fearing it might jeopardise the practice of archery, then so important in battle. Statutes forbidding it and other sports were enacted in the reigns of Edward III, Richard II and other monarchs. Even when, on the invention of gunpowder and firearms, the bow had fallen into disuse as a weapon of war, the prohibition was continued. The discredit attaching to bowling alleys, first established in London in 1455, probably encouraged subsequent repressive legislation, for many of the alleys were connected with taverns frequented by the dissolute and gamesters.\nErasmus referred to the game as . The name of \"bowls\" is implied in the gerund \"bowlyn\", recorded in the mid-15th century. The term \"bowl\" for \"wooden ball\" is recorded in the early 1400s. The name is explicitly mentioned, as \"bowles\", in a list of unlawful games in a 1495 act by Henry VII (\"Tenys, Closshe, Dise, Cardes, Bowles\"). It occurs again in a similar statute by Henry VIII (1511). By a further act, the Unlawful Games Act 1541\u2014which was not repealed until 1845\u2014artificers, labourers, apprentices, servants and the like were forbidden to play bowls at any time except Christmas, and then only in their master's house and presence. It was further enjoined that any one playing bowls outside his own garden or orchard was liable to a penalty of 6s. 8d. (6 shillings and 8 pence), while those possessed of lands of the yearly value of \u00a3100 might obtain licences to play on their own private greens.\nIn 1864, William Wallace Mitchell (1803\u20131884), a Glasgow Cotton Merchant, published his \"Manual of Bowls Playing\" following his work as the secretary formed in 1849 by Scottish bowling clubs which became the basis of the rules of the modern game. Young Mitchell was only 11 years old when he played on Kilmarnock bowling green, the oldest club in Scotland, instituted in 1740.\nThe patenting of the first lawn mower in 1830, in Britain, is strongly believed to have been the catalyst for the worldwide preparation of modern-style greens, sporting ovals, playing fields, pitches, grass courts, etc. This, in turn, led to the codification of modern rules for many sports, including lawn bowls, most football codes, lawn tennis and others.\nNational Bowling Associations were established in the late 1800s. The Victorian Bowling Association was formed in Victoria, Australia in 1880. The Scottish Bowling Association was established in 1892, although there had been a failed attempt in 1848 by 200 Scottish clubs.\nToday, bowls is played in over 40 countries with more than 50 member national authorities.\nGame.\nLawn bowls is usually played on a large, rectangular, precisely levelled and manicured grass or synthetic surface known as a bowling green which is divided into parallel playing strips called rinks. In the simplest competition, singles, one of the two opponents flips a coin to see who wins the \"mat\" and begins a segment of the competition (in bowling parlance, an \"end\"), by placing the mat and rolling the jack to the other end of the green to serve as a target. Once it has come to rest, the jack is aligned to the centre of the rink and the players take turns to roll their bowls from the mat towards the jack and thereby build up the \"head\".\nA bowl may curve outside the rink boundary on its path, but must come to rest within the rink boundary to remain in play. Bowls falling into the ditch are dead and removed from play, except in the event when one has \"touched\" the jack on its way. \"Touchers\" are marked with chalk and remain alive in play even if they get into the ditch. Similarly if the jack is knocked into the ditch it is still alive unless it is out of bounds to the side resulting in a \"dead\" end which is replayed, though according to international rules the jack is \"respotted\" to the centre of the rink and the end is continued. After each competitor has delivered all of their bowls (four each in singles and pairs, three each in triples, and two bowls each in fours), the distance of the closest bowls to the jack is determined (the jack may have been displaced) and points, called \"shots\", are awarded for each bowl which a competitor has closer than the opponent's nearest to the jack. For instance, if a competitor has bowled two bowls closer to the jack than their opponent's nearest, they are awarded two shots. The exercise is then repeated for the next end, a game of bowls typically being of twenty-one ends.\nLawn bowls is played on grass and variations from green to green are common. Greens come in all shapes and sizes: the most common are fast, slow, big crown, small crown.\nBowls is generally played in a very good spirit, even at the highest professional level, acknowledgment of opponents' successes and near misses being quite normal.\nScoring.\nScoring systems vary from competition to competition. Games can be decided when:\nGames to a specified number of ends may also be drawn. The draw may stand, or the opponents may be required to play an extra end to decide the winner. These provisions are always published beforehand in the event's Conditions of Play.\nIn the Laws of the Sport of Bowls the winner in a singles game is the first player to score 21 shots. In all other disciplines (pairs, triples, fours), the winner is the team who has scored the most shots after 21 or 25 ends of play. Often local tournaments will play shorter games (usually 10 or 12 ends). Some competitions use a \"set\" scoring system, with the first to seven points awarded a set in a best-of-three or best-of-five set match. As well as singles competition, there can be two (pairs), three (triples) and four-player (fours) teams. In these, teams bowl alternately, with each player within a team bowling all their bowls, then handing over to the next player. The team captain or \"&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;skip\" always plays last and is instrumental in directing his team's shots and tactics. The current method of scoring in the professional tour (World Bowls Tour) is sets. Each set consists of nine ends and the player with the most shots at the end of a set wins the set. If the score is tied the set is halved. If a player wins two sets, or gets a win and a tie, that player wins the game. If each player wins a set, or both sets end tied, there is a 3-end tiebreaker to determine a winner.\nBias of bowls.\nBowls are designed to travel a curved path because of a weight bias which was originally produced by inserting weights in one side of the bowl. The word \"bias\" itself is recorded as a technical term of the game in the 1560s.\nThe insertion of weights is no longer permitted by the rules and bias is now produced entirely by the shape of the bowl. A bowler determines the bias direction of the bowl in his hand by a dimple or symbol on one side. Regulations determine the minimum bias allowed, and the range of diameters (), but within these rules bowlers can and do choose bowls to suit their own preference. They were originally made from lignum vitae, a dense wood giving rise to the term \"woods\" for bowls, but are now more typically made of a hard plastic composite material.\nBowls were once only available coloured black or brown, but they are now available in a variety of colours. They have unique symbol markings engraved on them for identification. Since many bowls look the same, coloured, adhesive stickers or labels are also used to mark the bowls of each team in bowls matches. Some local associations agree on specific colours for stickers for each of the clubs in their area. Provincial or national colours are often assigned in national and international competitions. These stickers are used by officials to distinguish teams.\nBowls have symbols unique to the set of four for identification. The side of the bowl with a larger symbol within a circle indicates the side away from the bias. That side with a smaller symbol within a smaller circle is the bias side toward which the bowl will turn. It is not uncommon for players to deliver a \"wrong bias\" shot from time to time and see their carefully aimed bowl crossing neighbouring rinks rather than heading towards their jack.\nWhen bowling there are several types of delivery. \"Draw\" shots are those where the bowl is rolled to a specific location without causing too much disturbance of bowls already in the head. For a right-handed bowler, \"forehand draw\" or \"finger peg\" is initially aimed to the right of the jack, and curves in to the left. The same bowler can deliver a \"backhand draw\" or \"thumb peg\" by turning the bowl over in his hand and curving it the opposite way, from left to right. In both cases, the bowl is rolled as close to the jack as possible, unless tactics demand otherwise. A \"drive\" or \"fire\" or \"strike\" involves bowling with force with the aim of knocking either the jack or a specific bowl out of play - and with the drive's speed, there is virtually no noticeable (or, at least, much less) curve on the shot. An \"upshot\" or \"yard on\" shot involves delivering the bowl with an extra degree of weight (often referred to as \"controlled\" weight or \"rambler\"), enough to displace the jack or disturb other bowls in the head without killing the end. A \"block\" shot is one that is intentionally placed short to defend from a drive or to stop an oppositions draw shot. The challenge in all these shots is to be able to adjust line and length accordingly, the faster the delivery, the narrower the line or \"green\".\nVariations of play.\nParticularly in team competition there can be a large number of bowls on the green towards the conclusion of the end, and this gives rise to complex tactics. Teams \"holding shot\" with the closest bowl will often make their subsequent shots not with the goal of placing the bowl near the jack, but in positions to make it difficult for opponents to get their bowls into the head, or to places where the jack might be deflected to if the opponent attempts to disturb the head.\nThere are many different ways to set up the game. Crown Green Bowling utilises the entire green. A player can send the jack anywhere on the green in this game and the green itself is more akin to a golf green, with much undulation. It is played with only two woods each. The jack also has a bias and is only slightly smaller than the woods. At the amateur level it is usual for several ends to be played simultaneously on one green. If two moving woods meet, both are taken back and the shots replayed. If a moving wood strikes a stationary wood or jack from another end, it is again taken back and replayed, but the bowl struck is replaced where contact took place. The game is played usually to 21-up in Singles and Doubles format with some competitions playing to 31-up. The Panel (Professional Crown Green Bowls) is played at the Red Lion Bowling Green, Westhoughton daily and is played to 41-up with greenside betting throughout play. The Green was formerly owned by the pub (now demolished) but was purchased in 2007 by The Panel who paid the brewery \u00a312,000 for the green and its surrounds.\nSingles, triples and fours and Australian pairs are some ways the game can be played. In singles, two people play against each other and the first to reach 21, 25, or 31 shots (as decided by the controlling body) is the winner. In one variation of singles play, each player uses two bowls only and the game is played over 21 ends. A player concedes the game before the 21st end if the score difference is such that it is impossible to draw equal or win within the 21 ends. If the score is equal after 21 ends, an extra end is played to decide the winner. An additional scoring method is set play. This comprises two sets over nine ends. Should a player win a set each, they then play a further 3 ends that will decide the winner.\nPairs allows both people on a team to play Skip and Lead. The lead throws two bowls, the skip delivers two, then the lead delivers his remaining two, the skip then delivers his remaining two bowls. Each end, the leads and skips switch positions. This is played over 21 ends or sets play. Triples is with three players while Fours is with four players in each team and is played over 21 ends.\nAnother pairs variation is 242 pairs (also known as Australian Pairs). In the first end of the game the A players lead off with 2 bowls each, then the B players play 4 bowls each, before the A players complete the end with their final 2 bowls. The A players act as lead and skip in the same end. In the second end the roles are reversed with the A players being in the middle. This alternating pattern continues through the game which is typically over 15 ends.\nShort Mat Bowls is an all-year sport unaffected by weather conditions and it does not require a permanent location as the rink mats can be rolled up and stowed away. This makes it particularly appropriate for small communities as it can be played in village halls, schools, sports and social clubs.\nBowls are played by the blind and paraplegic. Blind bowlers are extremely skillful. A string is run out down the centre of the lane and wherever the jack lands, it is moved across to the string and the length is called out by a sighted marker. When the woods are sent the distance from the jack is called out, in yards, feet and inches. The position in relation to the jack is given using the clock; 12.00 is behind the jack.\nTra bowls.\nIn the province of West Flanders (and surrounding regions), tra bowls is the most popular variation of bowls. As opposed to playing it on a flat or uneven terrain, the terrain is made smooth but hollow (\"tra\" just means \"hollow road\" in Flemish). The hollow road causes the path to be curving even more.\nThe balls are biased in the same way as the lawn bowls balls but with a diameter of about , a thickness of and a weight of about , they are a bit bigger than usual bowls. The target is an unmovable feather or metal plate on the ground, instead of a small ball. The length of the tra is about .\nThe scoring is also different, as a point is awarded for every shot that brings the ball closer to the target than any opponent's ball. This causes pure blocking strategies to be less effective.\nIn 1972, the West-Flemish tra bowls federation was founded to uniform the local differing rules and to organise a match calendar. Meanwhile, they also organise championships and tournaments.\nNew Zealand Indoor Bowls.\nA unique variant of indoor bowls developed in New Zealand in the early 20th century. It is played on a carpet mat 6.7\u00a0m long and 1.8\u00a0m wide. The sport reached its heyday in New Zealand in the 1960s and 1970s with upward of 70,000 registered players, but has seen a gradual decline in membership since. The New Zealand Indoor Bowls Federation is the sport's governing body and it is only played competitively in New Zealand, though there is a biennial Trans-Tasman fixture using custom rules compromising between those used in Australia and New Zealand.\nCompetitions.\nThere are various bowls competitions held around the world (see - World Bowls Events).\nBowls is one of the \"core sports\" that must be included at each edition of the Commonwealth Games. With the exception of the 1966 Games, the sport has been included in all Games since their inception in 1930. England has so far dominated the sport with 51 medals.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4249", "revid": "49712780", "url": "https://en.wikipedia.org/wiki?curid=4249", "title": "Barcelonnette", "text": "Barcelonnette (; , also ; obsolete ) is a commune of France and a subprefecture in the department of Alpes-de-Haute-Provence, in the Provence-Alpes-C\u00f4te d'Azur region. It is located in the southern French Alps, at the crossroads between Provence, Piedmont and the Dauphin\u00e9, and is the largest town in the Ubaye Valley. The town's inhabitants are known as \"Barcelonnettes\".\nToponymy.\nBarcelonnette was founded and named in 1231, by Ramon Berenguer IV, Count of Provence. While the town's name is generally seen as a diminutive form of Barcelona in Catalonia, Albert Dauzat and Charles Rostaing point out an earlier attestation of the name \"Barcilona\" in Barcelonnette in around 1200, and suggest that it is derived instead from two earlier stems signifying a mountain, *\"bar\" and *\"cin\" (the latter of which is also seen in the name of Mont Cenis).\nIn the Vivaro-Alpine dialect of Occitan, the town is known as \"Barcilona de Proven\u00e7a\" or more rarely \"Barciloneta\" according to the classical norm; under the Mistralian norm it is called \"Barcilouna de Prouven\u00e7a\" or \"Barcilouneto\". In \"Val\u00e9ian\" (the dialect of Occitan spoken in the Ubaye Valley), it is called \"Barcilouna de Prouven\u00e7a\" or \"Barciloun\u00e9ta\". \"Barcino Nova\" is the town's Latin name meaning \"new Barcelona\"; \"Barcino\" was the Roman name for Barcelona in Catalonia from its foundation by Emperor Augustus in 10\u00a0BC, and the name was changed to \"Barcelona\" only during the Middle Ages.\nThe inhabitants of the town are called \"Barcelonnettes\", or \"Vilandroises\" in Val\u00e9ian.\nHistory.\nOrigins.\nThe Barcelonnette region was populated by Ligures from the 1st millennium BC onwards, and the arrival of the Celts several centuries later led to the formation of a mixed Celto-Ligurian people, the Vesubians. Polybius described the Vesubians as belligerent but nonetheless civilised and mercantile, and Julius Caesar praised their bravery. The work \"History of the Gauls\" also places the Vesubians in the Ubaye Valley.\nFollowing the Roman conquest of Provence, Barcelonnette was included in a small province with modern Embrun as its capital and governed by Albanus Bassalus. This was integrated soon afterwards into Gallia Narbonensis. In 36 AD, Emperor Tiberius transferred Barcelonnette to the province of the Cottian Alps. The town was known as \"Rigomagensium\" under the Roman Empire and was the capital of a civitas (a provincial subdivision), though no Roman money has yet been found in the canton of Barcelonnette.\nMedieval town.\nThe town of Barcelonnette was founded in 1231 by Ramon Berenguer IV, Count of Provence. According to Charles Rostaing, this act of formal \"foundation\", according certain privileges to the town, was a means of regenerating the destroyed town of \"Barcilona\". The town was afforded a \"consulat\" (giving it the power to administer and defend itself) in 1240.\nControl of the area in the Middle Ages swung between the Counts of Savoy and of Provence. In 1388, after Count Louis II of Provence had left to conquer Naples, the Count of Savoy Amadeus VIII took control of Barcelonnette; however, it returned to Proven\u00e7al control in 1390, with the d'Audiffret family as its lords. On the death of Louis II in 1417 it reverted to Savoy, and, although Count Ren\u00e9 again retook the area for Provence in 1471, it had returned to Savoyard dominance by the start of the 16th century, by which point the County of Provence had become united with the Kingdom of France due to the death of Count Charles V in 1481.\nAncien R\u00e9gime.\nDuring Charles V's invasion of Provence in 1536, Francis I of France sent the Count of F\u00fcrstenberg's 6000 \"Landsknechte\" to ravage the area in a scorched earth policy. Barcelonnette and the Ubaye Valley remained under French sovereignty until the second Treaty of Cateau-Cambr\u00e9sis on 3 April 1559.\nIn 1588 the troops of Fran\u00e7ois, Duke of Lesdigui\u00e8res entered the town and set fire to the church and convent during their campaign against the Duke of Savoy. In 1600, after the Treaty of Vervins, conflict returned between Henry IV of France and Savoy, and Lesdigui\u00e8res retook Barcelonnette until the conclusion of the Treaty of Lyon on 17 January the following year. In 1628, during the War of the Mantuan Succession, Barcelonnette and the other towns of the Ubaye Valley were pillaged and burned by Jacques du Bl\u00e9 d'Uxelles and his troops, as they passed through towards Italy to the Duke of Mantua's aid. The town was retaken by the Duke of Savoy in 1630; and in 1691 it was captured by the troops of the Marquis de Vins during the War of the League of Augsburg.\nBetween 1614 and 1713, Barcelonnette was the seat of one of the four prefectures under the jurisdiction of the Senate of Nice. At this time, the community of Barcelonnette successfully purchased the \"seigneurie\" of the town as it was put to auction by the Duke of Savoy; it thereby gained its own justicial powers. In 1646, a college was founded in Barcelonnette.\nA \"significant\" part of the town's inhabitants had, by the 16th century, converted to Protestantism, and were repressed during the French Wars of Religion.\nThe \"viguerie\" of Barcelonnette (also comprising Saint-Martin and Entraunes) was reattached to France in 1713 as part of a territorial exchange with the Duchy of Savoy during the Treaties of Utrecht. The town remained the site of a \"viguerie\" until the French Revolution. A decree of the council of state on 25 December 1714 reunited Barcelonnete with the general government of Provence.\nRevolution.\nBarcelonnette was one of few settlements in Haute-Provence to acquire a Masonic Lodge before the Revolution, in fact having two:\nIn March 1789, riots took place as a result of a crisis in wheat production. In July, the Great Fear of aristocratic reprisal against the ongoing French Revolution struck France, arriving in the Barcelonnette area on 31 July 1789 (when the news of the storming of the Bastille first reached the town) before spreading towards Digne.\nThis agitation continued in the Ubaye Valley; a new revolt broke out on 14 June, and famine was declared in April 1792. The patriotic society of the commune was one of the first 21 created in Alpes-de-Haute-Provence, in spring 1792, by the envoys of the departmental administration. Around a third of the male population attended at the club. Another episode of political violence occurred in August 1792.\nBarcelonnette was the seat of the District of Barcelonnette from 1790 to 1800.\nModern history.\nIn December 1851, the town was home to a movement of republican resistance towards Napoleon III's coup. Though only a minority of the population, the movement rebelled on Sunday 7 December, the day after the news of the coup arrived. Town officials and gendarmes were disarmed and placed in the maison d'arr\u00eat. A committee of public health was created on 8 December; on 9 December the inhabitants of Jausiers and its surroundings formed a colony under the direction of general councillor Br\u00e8s, and Mayor Signoret of Saint-Paul-sur-Ubaye. This was stopped, however, on 10 December before it could reach Barcelonnette, as the priest of the subprefecture had intervened. On 11 December, several officials escaped and found refuge in L'Argenti\u00e8re in Piedmont. The arrival of troops on 16 December put a final end to the republican resistance without bloodshed, and 57 insurgents were tried; 38 were condemned to deportation (though several were pardoned in April).\nBetween 1850 and 1950, Barcelonnette was the source of a wave of emigration to Mexico. Among these emigrants was Jean Baptiste Ebrard, founder of the Liverpool department store chain in Mexico. On the edges of Barcelonnette and Jausiers there are several houses and villas of colonial style (known as \"maisons mexicaines\"), constructed by emigrants to Mexico who returned to France between 1870 and 1930. A plaque in the town commemorates the deaths of ten Mexican citizens who returned to Barcelonnette to fight in the First World War.\nDuring the Second World War, 26 Jews were arrested in Barcelonnette before being deported. The 89th \"compagnie de travailleurs \u00e9trangers\" (Company of Foreign Workers), consisting of foreigners judged as undesirable by the Third Republic and the Vichy regime and committed to forced labour, was established in Barcelonnette.\nThe 11th Battalion of \"Chasseurs alpins\" was garrisoned at Barcelonnette between 1948 and 1990.\nGeography.\nBarcelonnette is situated in the wide and fertile Ubaye Valley, of which it is the largest town. It lies at an elevation of 1132\u00a0m (3717\u00a0ft) on the right bank of the Ubaye River, and is surrounded by mountains which reach peaks of over 3000\u00a0m; the tallest of these is the Needle of Chambeyron at 3412\u00a0m. Barcelonnette is situated 210\u00a0km from Turin, 91\u00a0km from Nice and 68\u00a0km from Gap.\nBiodiversity.\nAs a result of its relief and geographic situation, the Ubaye Valley has an \"abundance of plant and animal species\". The fauna is largely constituted of golden eagles, marmots, ibex and vultures, and the flora includes a large proportion of larches, g\u00e9n\u00e9pis and white asphodels.\nClimate.\nThe Ubaye Valley has an alpine climate and winters are harsh as a result of the altitude, but there are only light winds as a result of the relief. There are on average almost 300 days of sun and 700\u00a0mm of rain per year.\nHazards.\nNone of the 200 communes of the department is entirely free of seismic risk; the canton of Barcelonnette is placed in zone 1b (low risk) by the determinist classification of 1991 based on seismic history, and zone 4 (average risk) according to the probabilistic EC8 classification of 2011. The commune is also vulnerable to avalanches, forest fires, floods, and landslides. Barcelonnette is also exposed to the possibility of a technological hazard in that road transport of dangerous materials is allowed to pass through on the RD900.\nThe town has been subject to several orders of natural disaster: floods and mudslides in 1994 and 2008, and landslides in 1996 and 1999. The strongest recorded earthquakes in the region occurred on 5\u00a0April\u00a01959, with its epicentre at Saint-Paul-sur-Ubaye and a recorded intensity of 6.5 at Barcelonnette, and on 17\u00a0February\u00a01947, with its epicentre at Prazzo over the Italian border.\nArchitecture.\nThe subprefecture has been situated since 1978 in a \"maison mexicaine\", the Villa l'Ubayette, constructed between 1901 and 1903.\nPopulation.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nIn 1471, the community of Barcelonnette (including several surrounding parishes) comprised 421 fires (households). In 1765, it had 6,674 inhabitants, but emigration, particularly to Mexico, slowed the town's growth in the period before the Second World War. According to the census of 2017, Barcelonnette has a population of 2,598 (municipal population) across a total area of 16.42\u00a0km2. The town is characterised by low population density. Between 1990 and 1999 the town's annual mean population growth was -0.6%, though between 1999 and 2007 this increased to an average of -0.2%.\nEconomy.\nThe city is mainly a tourist and resort centre, serving many ski lodges. The Pra-Loup resort is 7\u00a0km from Barcelonnette; Le Sauze is 5\u00a0km away. It and the Ubaye Valley are served by the Barcelonnette \u2013 Saint-Pons Airfield. Notably, Barcelonnette is the only subprefecture of France not served by rail transport; the Ubaye line which would have linked Chorges to Barcelonnette was never completed as a result of the First World War and the construction of the Serre-Pon\u00e7on Dam between 1955 and 1961.\nEducation.\nAn \"\u00e9cole normale\" (an institute for training primary school teachers) was founded in Barcelonnette in 1833, and remained there until 1888 when it was transferred to Digne. The \"lyc\u00e9e Andr\u00e9-Honnorat de Barcelonnette\", originally the \"coll\u00e8ge Saint-Maurice\" and renamed after the politician Andr\u00e9 Honnorat in 1919, is located in the town; Pierre-Gilles de Gennes and Carole Merle both studied there. Currently, three schools exist in Barcelonnette: a public nursery school, a public elementary school, and a private school (under a contract by which the teachers are paid by the national education system).\nIn 2010 the \"lyc\u00e9e Andr\u00e9-Honnorat\" opened a boarding school aimed at gifted students of poorer social backgrounds, in order to give them better conditions in which to study. It is located in the \"Quartier Craplet\", formerly the garrison of the 11th Battalion of \"Chasseurs Alpins\" and then the French Army's \"Centre d'instruction et d'entra\u00eenement au combat en montagne\" (CIECM).\nTransportation.\nBarcelonnette \u2013 Saint-Pons Airfield (IATA: BAE, ICAO LFMR) is located at Saint Pons, 3\u00a0km (2 miles) west of Barcelonnette.\nInternational links.\nBarcelonnette is twinned with:\nIt is also the site of a Mexican honorary consulate.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "4251", "revid": "50952835", "url": "https://en.wikipedia.org/wiki?curid=4251", "title": "Bah\u00e1\u02bc\u00ed Faith", "text": "Religion established by Bah\u00e1\u02bcu'll\u00e1h\nThe Bah\u00e1\u02bc\u00ed Faith is a religion established by Bah\u00e1\u02bcu'll\u00e1h in the 19th century that teaches the essential worth of all religions and the unity of all people. It initially developed in Iran and parts of the Middle East, where it has faced ongoing persecution since its inception. The religion has 7\u20138 million adherents known as Bah\u00e1\u02bc\u00eds who are spread throughout most of the world's countries and territories.\nThe Bah\u00e1\u02bc\u00ed Faith has three central figures: the B\u00e1b (1819\u20131850), executed for heresy, who taught that a prophet similar to Jesus and Muhammad would soon appear; Bah\u00e1\u02bcu'll\u00e1h (1817\u20131892), who claimed to be said prophet in 1863 and who had to endure both exile and imprisonment; and his son, \u02bbAbdu'l-Bah\u00e1 (1844\u20131921), who made teaching trips to Europe and the United States after his release from confinement in 1908. After \u02bbAbdu'l-Bah\u00e1's death in 1921, the leadership of the religion fell to his grandson Shoghi Effendi (1897\u20131957). Bah\u00e1\u02bc\u00eds annually elect local, regional, and national Spiritual Assemblies that govern the religion's affairs, and every five years an election is held for the Universal House of Justice, the nine-member governing institution of the worldwide Bah\u00e1\u02bc\u00ed community that is located in Haifa, Israel, near the Shrine of the B\u00e1b.\nAccording to Bah\u00e1\u02bc\u00ed teachings, religion is revealed in an orderly and progressive way by a single God through Manifestations of God, who are the founders of major world religions throughout human history; the Buddha, Jesus, and Muhammad are cited as the most recent of these Manifestations of God before the B\u00e1b and Bah\u00e1\u02bcu'll\u00e1h. Bah\u00e1\u02bc\u00eds regard the world's major religions as fundamentally unified in their purpose, but divergent in their social practices and interpretations. The Bah\u00e1\u02bc\u00ed Faith stresses the unity of all people as its core teaching; as a result, it explicitly rejects notions of racism, sexism, and nationalism. At the heart of Bah\u00e1\u02bc\u00ed teachings is the desire to establish a unified world order that ensures the prosperity of all nations, races, creeds, and classes.\nLetters and epistles by Bah\u00e1\u02bcu'll\u00e1h, along with writings and talks by his son \u02bbAbdu'l-Bah\u00e1, have been collected and assembled into a canon of Bah\u00e1\u02bc\u00ed scriptures. This collection also includes works by the B\u00e1b, who is regarded as Bah\u00e1\u02bcu'll\u00e1h's forerunner. Prominent among the works of Bah\u00e1\u02bc\u00ed literature are the \"Kit\u00e1b-i-Aqdas\", the \"Kit\u00e1b-i-\u00cdq\u00e1n\", \"Some Answered Questions\", \"God Passes By\", and \"The Dawn-Breakers\".\nEtymology.\nThe word \"Bah\u00e1\u02bc\u00ed\" (, ; ) is used either as an adjective to refer to the Bah\u00e1\u02bc\u00ed Faith or as a term for a follower of Bah\u00e1\u02bcu'll\u00e1h. The proper name of the religion is the \"Bah\u00e1\u02bc\u00ed Faith\", not \"Bah\u00e1\u02bc\u00ed\" or \"Bah\u00e1\u02bcism\" (the latter, once common among academics, is regarded as derogatory by the Bah\u00e1\u02bc\u00eds). It is derived from the Arabic \"Bah\u00e1\u02bc\" (), a name Bah\u00e1\u02bcu'll\u00e1h chose for himself, referring to the 'glory' or 'splendor' of God. In English, the word is commonly pronounced (), but the more accurate rendering of the Arabic is ().\nThe accent marks above the letters, representing long vowels, derive from a system of transliterating Arabic and Persian script that was adopted by Bah\u00e1\u02bc\u00eds in 1923, and which has been used in almost all Bah\u00e1\u02bc\u00ed publications since. Bah\u00e1\u02bc\u00eds prefer the orthographies \"Bah\u00e1\u02bc\u00ed\", \"the B\u00e1b\", \"Bah\u00e1\u02bcu'll\u00e1h\", and \"\u02bbAbdu'l-Bah\u00e1\". When accent marks are unavailable, \"Baha\u02bci\" or \"Baha\u02bcu\u02bcllah\" are often used.\nHistory.\nThe Bah\u00e1\u02bc\u00ed Faith traces its beginnings to the religion of the B\u00e1b and the Shaykhi movement that immediately preceded it. The B\u00e1b was a merchant who began preaching in 1844 that he was the bearer of a new revelation from God, but was rejected by the generality of Islamic clergy in Iran, ending in his public execution in 1850 for the crime of heresy. The B\u00e1b taught that God would soon send a new messenger, and Bah\u00e1\u02bc\u00eds consider Bah\u00e1\u02bcu'll\u00e1h to be that person. Although they are distinct movements, the B\u00e1b is so interwoven into Bah\u00e1\u02bc\u00ed theology and history that Bah\u00e1\u02bc\u00eds celebrate his birth, death, and declaration as holy days, and consider him one of their three central figures (along with Bah\u00e1\u02bcu'll\u00e1h and \u02bbAbdu'l-Bah\u00e1). A historical account of the B\u00e1b\u00ed movement (\"The Dawn-Breakers\") is considered one of three books that every Bah\u00e1\u02bc\u00ed should \"master\" and read \"over and over again\".\nThe Bah\u00e1\u02bc\u00ed community was mostly confined to the Iranian and Ottoman empires until after the death of Bah\u00e1\u02bcu'll\u00e1h in 1892; at that time, he had followers in 13 countries of Asia and Africa. Under the leadership of his son, \u02bbAbdu'l-Bah\u00e1, the religion gained a footing in Europe and America, and was consolidated in Iran, where it still suffers intense persecution. \u02bbAbdu'l-Bah\u00e1's death in 1921 marks the end of what Bah\u00e1\u02bc\u00eds call the \"heroic age\" of the religion.\nB\u00e1b.\nOn the evening of 22 May 1844, Siyyid \u02bbAl\u00ed-Muhammad of Shiraz gained his first convert and took on the title of \"the B\u00e1b\" ( \"Gate\"), referring to his later claim to the status of Mahdi of Shi\u02bba Islam. His followers were known as B\u00e1b\u00eds. As the B\u00e1b's teachings spread, which the Islamic clergy saw as blasphemous, his followers came under increased persecution and torture. The conflicts escalated in several places to military sieges by the Shah's army. The B\u00e1b himself was imprisoned and eventually executed in 1850.\nBah\u00e1\u02bc\u00eds see the B\u00e1b as the forerunner of the Bah\u00e1\u02bc\u00ed Faith, because the B\u00e1b's writings introduced the concept of \"He whom God shall make manifest\", a messianic figure whose coming, according to Bah\u00e1\u02bc\u00eds, was announced in the scriptures of all of the world's great religions, and whom Bah\u00e1\u02bcu'll\u00e1h, the founder of the Bah\u00e1\u02bc\u00ed Faith, claimed to be. The B\u00e1b's tomb, located on Mount Carmel in Haifa, Israel, is an important place of pilgrimage for Bah\u00e1\u02bc\u00eds. The remains of the B\u00e1b were brought secretly from Iran to the Holy Land and eventually interred in the tomb built for them in a spot specifically designated by Bah\u00e1\u02bcu'll\u00e1h. The writings of the B\u00e1b are considered inspired scripture by Bah\u00e1\u02bc\u00eds, though having been superseded by the laws and teachings of Bah\u00e1\u02bcu'll\u00e1h. The main written works translated into English of the B\u00e1b are compiled in \"Selections from the Writings of the B\u00e1b\" (1976) out of the estimated 135 works.\nBah\u00e1\u02bcu'll\u00e1h.\nM\u00edrz\u00e1 Husayn \u02bbAl\u00ed N\u00far\u00ed was one of the early followers of the B\u00e1b, and later took the title of Bah\u00e1\u02bcu'll\u00e1h. In August 1852, a few B\u00e1b\u00eds made a failed attempt to assassinate the Shah, Naser al-Din Shah Qajar. The Shah responded by ordering the killing and in some cases torturing of about 50 B\u00e1b\u00eds in Tehran. Further bloodshed spread throughout the country and hundreds were reported in period newspapers by October, and tens of thousands by the end of December. Bah\u00e1\u02bcu'll\u00e1h was not involved in the assassination attempt but was imprisoned in Tehran until his release was arranged four months later by the Russian ambassador, after which he joined other B\u00e1b\u00eds in exile in Baghdad.\nShortly thereafter he was expelled from Iran and traveled to Baghdad, in the Ottoman Empire. In Baghdad, his leadership revived the persecuted followers of the B\u00e1b in Iran, so Iranian authorities requested his removal, which instigated a summons to Constantinople (now Istanbul) from the Ottoman Sultan. In 1863, at the time of his removal from Baghdad, Bah\u00e1\u02bcu'll\u00e1h first announced his claim of prophethood to his family and followers, which he said came to him years earlier while in a dungeon of Tehran. From the time of the initial exile from Iran, tensions grew between him and Subh-i-Azal, the appointed leader of the B\u00e1b\u00eds, who did not recognize Bah\u00e1\u02bcu'll\u00e1h's claim. Throughout the rest of his life Bah\u00e1\u02bcu'll\u00e1h gained the allegiance of almost all of the B\u00e1b\u00eds, who came to be known as Bah\u00e1\u02bc\u00eds, while a remnant of B\u00e1b\u00eds became known as Azalis, and are regarded by Bah\u00e1'\u00eds as equivalent to apostates.\nHe spent less than four months in Constantinople. After receiving chastising letters from Bah\u00e1\u02bcu'll\u00e1h, Ottoman authorities turned against him and put him under house arrest in Adrianople (now Edirne), where he remained for four years, until a royal decree of 1868 banished all B\u00e1b\u00eds to either Cyprus or \u02bbAkk\u00e1.\nIt was in or near the Ottoman penal colony of \u02bbAkk\u00e1, in Palestine, that Bah\u00e1\u02bcu'll\u00e1h spent the remainder of his life. After initially strict and harsh confinement, he was allowed to live in a home near \u02bbAkk\u00e1, while still officially a prisoner of that city. He died there in 1892. Bah\u00e1\u02bc\u00eds regard his resting place at Bahj\u00ed as the Qiblih to which they turn in prayer each day.\nHe produced over 18,000 works in his lifetime, in both Arabic and Persian, of which only 8% have been translated into English. During the period in Adrianople, he began declaring his mission as a Messenger of God in letters to the world's religious and secular rulers, including Pope Pius IX, Napoleon III, and Queen Victoria.\n\u02bbAbdu'l-Bah\u00e1.\n\u02bbAbb\u00e1s Effendi was Bah\u00e1\u02bcu'll\u00e1h's eldest son, who chose for himself the title of \u02bbAbdu'l-Bah\u00e1 (\"Servant of Bah\u00e1\"). His father left a will that appointed \u02bbAbdu'l-Bah\u00e1 as the leader of the Bah\u00e1\u02bc\u00ed community. \u02bbAbdu'l-Bah\u00e1 had shared his father's long exile and imprisonment, which continued until \u02bbAbdu'l-Bah\u00e1's own release as a result of the Young Turk Revolution in 1908. Following his release, he led a life of travelling, speaking, teaching, and maintaining correspondence with communities of believers and individuals, expounding the principles of the Bah\u00e1\u02bc\u00ed Faith. Bah\u00e1'\u00eds consider \u02bbAbdu'l-Bah\u00e1 to be a perfect exemplar of the Bah\u00e1\u02bc\u00ed teachings.\nAs of 2020, there are over 38,000 extant documents containing the words of \u02bbAbdu'l-Bah\u00e1, which are of widely varying lengths. Only a fraction of these documents have been translated into English. Among the more well known are \"The Secret of Divine Civilization\", \"Some Answered Questions\", the \"Tablet to Auguste-Henri Forel\", the \"Tablets of the Divine Plan\", and the \"Tablet to The Hague\". Additionally notes taken of a number of his talks were published in various volumes like \"Paris Talks\" during his journeys to the West.\nShoghi Effendi.\nBah\u00e1\u02bcu'll\u00e1h's \"Kit\u00e1b-i-Aqdas\" and \"The Will and Testament of \u02bbAbdu'l-Bah\u00e1\" are foundational documents of the Bah\u00e1\u02bc\u00ed administrative order. Bah\u00e1\u02bcu'll\u00e1h established the elected Universal House of Justice, and \u02bbAbdu'l-Bah\u00e1 established the appointed hereditary Guardianship and clarified the relationship between the two institutions. In his Will, \u02bbAbdu'l-Bah\u00e1 appointed Shoghi Effendi, his eldest grandson, as the first Guardian of the Bah\u00e1\u02bc\u00ed Faith. Shoghi Effendi served for 36 years as the head of the religion until his death in 1957.\nThroughout his lifetime, Shoghi Effendi translated Bah\u00e1\u02bc\u00ed texts; developed global plans for the expansion of the Bah\u00e1\u02bc\u00ed community; developed the Bah\u00e1\u02bc\u00ed World Centre; carried on a voluminous correspondence with communities and individuals around the world; and built the administrative structure of the religion, preparing the community for the election of the Universal House of Justice. \nIn 1937, Shoghi Effendi launched a seven-year plan for the Bah\u00e1\u02bc\u00eds of North America, followed by another in 1946. In 1953, he launched the first international plan, the Ten Year World Crusade. This plan included extremely ambitious goals for the expansion of Bah\u00e1\u02bc\u00ed communities and institutions, the translation of Bah\u00e1\u02bc\u00ed texts into several new languages, and the sending of Bah\u00e1\u02bc\u00ed pioneers into previously unreached nations. He announced in letters during the Ten Year Crusade that it would be followed by other plans under the direction of the Universal House of Justice, which was elected in 1963 at the culmination of the Ten Year Crusade. \nShoghi Effendi died unexpectedly after a brief illness on 4 November 1957 in London, England, under conditions that did not allow for a successor to be appointed. He is buried in New Southgate Cemetery in Barnet, London. \nUniversal House of Justice.\nSince 1963, the Universal House of Justice has been the elected head of the Bah\u00e1\u02bc\u00ed Faith. The general functions of this body are defined through the writings of Bah\u00e1\u02bcu'll\u00e1h and clarified in the writings of Abdu'l-Bah\u00e1 and Shoghi Effendi. These functions include teaching and education, implementing Bah\u00e1\u02bc\u00ed laws, addressing social issues, and caring for the weak and the poor.\nStarting with the Nine Year Plan that began in 1964, the Universal House of Justice has directed the work of the Bah\u00e1\u02bc\u00ed community through a series of multi-year international plans. The Bah\u00e1\u02bc\u00ed leadership sought to continue the expansion of the religion but also to \"consolidate\" new members, meaning increase their knowledge of the Bah\u00e1\u02bc\u00ed teachings. In this vein, in the 1970s, the Ruhi Institute was founded by Bah\u00e1\u02bc\u00eds in Colombia to offer short courses on Bah\u00e1\u02bc\u00ed beliefs, ranging in length from a weekend to nine days. The associated Ruhi Foundation, whose purpose was to systematically \"consolidate\" new Bah\u00e1\u02bc\u00eds, was registered in 1992, and since the late 1990s the courses of the Ruhi Institute have been the dominant way of teaching the Bah\u00e1\u02bc\u00ed Faith around the world. By 2013 there were over 300 Bah\u00e1\u02bc\u00ed training institutes around the world and 100,000 people participating in courses. The courses of the Ruhi Institute train communities to self-organize classes for the spiritual education of children and youth, among other activities. Additional lines of action the Universal House of Justice has encouraged for the contemporary Bah\u00e1\u02bc\u00ed community include social action and participation in the prevalent discourses of society.\nAnnually, on 21 April, the Universal House of Justice sends a 'Ridv\u00e1n' message to the worldwide Bah\u00e1\u02bc\u00ed community, that updates Bah\u00e1\u02bc\u00eds on current developments and provides further guidance for the year to come.\nAt local, regional, and national levels, Bah\u00e1\u02bc\u00eds elect members to nine-person Spiritual Assemblies, which run the affairs of the religion. There are also appointed individuals working at various levels, including locally and internationally, which perform the function of propagating the teachings and protecting the community. The latter do not serve as clergy, which the Bah\u00e1\u02bc\u00ed Faith does not have. The Universal House of Justice remains the supreme governing body of the Bah\u00e1\u02bc\u00ed Faith, and its 9 members are elected every five years by the members of all National Spiritual Assemblies. Any male Bah\u00e1\u02bc\u00ed, 18 years or older, is eligible to be elected to the Universal House of Justice; all other positions are open to male and female Bah\u00e1\u02bc\u00eds.\nBeliefs.\nThe teachings of Bah\u00e1\u02bcu'll\u00e1h form the foundation of Bah\u00e1\u02bc\u00ed beliefs. Three principles are central to these teachings: the unity of God, the unity of religion, and the unity of humanity. Bah\u00e1'\u00eds believe that God periodically reveals his will through divine messengers, whose purpose is to transform the character of humankind and to develop, within those who respond, moral and spiritual qualities. Religion is thus seen as orderly, unified, and progressive from age to age.\nGod.\nBah\u00e1\u02bc\u00ed writings describe a single, personal, inaccessible, omniscient, omnipresent, imperishable, and almighty God who is the creator of all things in the universe. The existence of God and the universe are thought to be eternal, with no beginning or end. Even though God is not directly accessible, he is seen as being conscious of creation, with a will and a purpose which is expressed through messengers who are called Manifestations of God. The Bah\u00e1\u02bc\u00ed conception of God is of an \"unknowable essence\" who is the source of all existence and known through the perception of human virtues. In another sense, Bah\u00e1\u02bc\u00ed teachings on God are also panentheistic, seeing signs of God in all things, but the reality of God being exalted and above the physical world.\nBah\u00e1\u02bc\u00ed teachings state that God is too great for humans to fully comprehend, and based on them, humans cannot create a complete and accurate image of God by themselves. Therefore, human understanding of God is achieved through the recognition of the person of the Manifestation and through the understanding of his revelations via his Manifestations. In the Bah\u00e1\u02bc\u00ed Faith, God is often referred to by titles and attributes (for example, the All-Powerful, or the All-Loving), and there is a substantial emphasis on monotheism. Bah\u00e1\u02bc\u00ed teachings state that these attributes do not apply to God directly but are used to translate Godliness into human terms and to help people concentrate on their own attributes in worshipping God to develop their potential on their spiritual path. According to the Bah\u00e1\u02bc\u00ed teachings the human purpose is to learn to know and love God through such methods as prayer, reflection, and being of service to others.\nReligion.\nBah\u00e1\u02bc\u00ed notions of progressive religious revelation result in their accepting the validity of the well known religions of the world, whose founders and central figures are seen as Manifestations of God. Religious history is interpreted as a series of dispensations, where each \"manifestation\" brings a somewhat broader and more advanced revelation that is rendered as a text of scripture and passed on through history with greater or lesser reliability but at least true in substance, suited for the time and place in which it was expressed. Specific religious social teachings (for example, the direction of prayer, or dietary restrictions) may be revoked by a subsequent manifestation so that a more appropriate requirement for the time and place may be established. Conversely, certain general principles (for example, neighbourliness, or charity) are seen to be universal and consistent. In Bah\u00e1\u02bc\u00ed belief, this process of progressive revelation will not end; it is, however, believed to be cyclical. Bah\u00e1\u02bc\u00eds do not expect a new manifestation of God to appear within 1000 years of Bah\u00e1\u02bcu'll\u00e1h's revelation.\nBah\u00e1\u02bc\u00eds assert that their religion is a distinct tradition with its own scriptures and laws, and not a sect of another religion. Most religious specialists now see it as an independent religion, with its religious background in Shi\u02bba Islam being seen as analogous to the Jewish context in which Christianity was established. Bah\u00e1\u02bc\u00eds describe their faith as an independent world religion, differing from the other traditions in its relative age and modern context.\nThe Bah\u00e1\u02bc\u00ed Faith has been described as belonging to the family of Abrahamic religions, as it is monotheistic and recognizes its own descent from the prophet Abraham.\nHuman beings.\nThe Bah\u00e1\u02bc\u00ed writings state that human beings have a \"rational soul\", and that this provides the species with a unique capacity to recognize God's status and humanity's relationship with its creator. Every human is seen to have a duty to recognize God through his Messengers, and to conform to their teachings. Through recognition and obedience, service to humanity and regular prayer and spiritual practice, the Bah\u00e1\u02bc\u00ed writings state that the soul becomes closer to God, the spiritual ideal in Bah\u00e1\u02bc\u00ed belief. According to Bah\u00e1\u02bc\u00ed belief, when a human dies, the soul is permanently separated from the body and carries on in the next world where it is judged based on the person's actions in the physical world. Heaven and Hell are taught to be spiritual states of nearness or distance from God that describe relationships in this world and the next, and not physical places of reward and punishment achieved after death.\nThe Bah\u00e1\u02bc\u00ed writings emphasize the essential equality of human beings, and the abolition of prejudice. Humanity is seen as essentially one, though highly varied; its diversity of race and culture are seen as worthy of appreciation and acceptance. Doctrines of racism, nationalism, caste, social class, and gender-based hierarchy are seen as artificial impediments to unity. The Bah\u00e1\u02bc\u00ed teachings state that the unification of humanity is the paramount issue in the religious and political conditions of the present world.\nSocial principles.\nWhen \u02bbAbdu'l-Bah\u00e1 first traveled to Europe and America in 1911\u20131912, he gave public talks that articulated the basic principles of the Bah\u00e1\u02bc\u00ed Faith. These included preaching on the equality of men and women, race unity, the need for world peace, and other ideas considered progressive in the early 20th century. Published summaries of the Bah\u00e1\u02bc\u00ed teachings often include a list of these principles, and lists vary in wording and what is included.\nThe concept of the unity of humankind, seen by Bah\u00e1\u02bc\u00eds as an ancient truth, is the starting point for many of the ideas. The equality of races and the elimination of extremes of wealth and poverty, for example, are implications of that unity. Another outgrowth of the concept is the need for a united world federation, and some practical recommendations to encourage its realization involve the establishment of a universal language, a standard economy and system of measurement, universal compulsory education, and an international court of arbitration to settle disputes between nations. Nationalism, according to this viewpoint, should be abandoned in favor of allegiance to the whole of humankind. With regard to the pursuit of world peace, Bah\u00e1\u02bcu'll\u00e1h prescribed a world-embracing collective security arrangement.\nOther Bah\u00e1\u02bc\u00ed social principles revolve around spiritual unity. Religion is viewed as progressive from age to age, but to recognize a newer revelation one has to abandon tradition and independently investigate. Bah\u00e1\u02bc\u00eds are taught to view religion as a source of unity, and religious prejudice as destructive. Science is also viewed in harmony with true religion. Though Bah\u00e1\u02bcu'll\u00e1h and \u02bbAbdu'l-Bah\u00e1 called for a united world that is free of war, they also anticipate that over the long term, the establishment of a lasting peace (The Most Great Peace) and the purging of the \"overwhelming Corruptions\" requires that the people of the world unite under a universal faith with spiritual virtues and ethics to complement material civilization.\nShoghi Effendi, the head of the religion from 1921 to 1957, wrote the following summary of what he considered to be the distinguishing principles of Bah\u00e1\u02bcu'll\u00e1h's teachings, which, he said, together with the laws and ordinances of the \"Kit\u00e1b-i-Aqdas\" constitute the bedrock of the Bah\u00e1\u02bc\u00ed Faith:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The independent search after truth, unfettered by superstition or tradition; the oneness of the entire human race, the pivotal principle and fundamental doctrine of the Faith; the basic unity of all religions; the condemnation of all forms of prejudice, whether religious, racial, class or national; the harmony which must exist between religion and science; the equality of men and women, the two wings on which the bird of human kind is able to soar; the introduction of compulsory education; the adoption of a universal auxiliary language; the abolition of the extremes of wealth and poverty; the institution of a world tribunal for the adjudication of disputes between nations; the exaltation of work, performed in the spirit of service, to the rank of worship; the glorification of justice as the ruling principle in human society, and of religion as a bulwark for the protection of all peoples and nations; and the establishment of a permanent and universal peace as the supreme goal of all mankind\u2014these stand out as the essential elements [which Bah\u00e1\u02bcu'll\u00e1h proclaimed].\nCovenant.\nBah\u00e1\u02bc\u00eds highly value unity, and Bah\u00e1\u02bcu'll\u00e1h clearly established rules for holding the community together and resolving disagreements. Within this framework, no individual follower may propose 'inspired' or 'authoritative' interpretations of scripture, and individuals agree to support the line of authority established in Bah\u00e1\u02bc\u00ed scriptures. This practice has left the Bah\u00e1\u02bc\u00ed community unified and free of any serious fracturing. The Universal House of Justice is the final authority to resolve any disagreements among Bah\u00e1\u02bc\u00eds, and the few attempts at schism have all either become extinct or remained extremely small, numbering a few hundred adherents collectively. The followers of such divisions are regarded as Covenant-breakers and shunned.\nSacred texts.\nThe \"canonical texts\" of the Bah\u00e1\u02bc\u00ed Faith are the writings of the B\u00e1b, Bah\u00e1\u02bcu'll\u00e1h, \u02bbAbdu'l-Bah\u00e1, Shoghi Effendi and the Universal House of Justice, and the authenticated talks of \u02bbAbdu'l-Bah\u00e1. The writings of the B\u00e1b and Bah\u00e1\u02bcu'll\u00e1h are considered as divine revelation, the writings and talks of \u02bbAbdu'l-Bah\u00e1 and the writings of Shoghi Effendi as authoritative interpretation, and those of the Universal House of Justice as authoritative legislation and elucidation. Some measure of divine guidance is assumed for all of these texts.\nSome of Bah\u00e1\u02bcu'll\u00e1h's most important writings include the \"Kit\u00e1b-i-Aqdas\" (\"Most Holy Book\"), which defines many laws and practices for individuals and society, the \"Kit\u00e1b-i-\u00cdq\u00e1n\" (\"Book of Certitude\"), which became the foundation of much of Bah\u00e1\u02bc\u00ed belief, and \"Gems of Divine Mysteries\", which includes further doctrinal foundations. Although the Bah\u00e1\u02bc\u00ed teachings have a strong emphasis on social and ethical issues, a number of foundational texts have been described as mystical. These include the \"Seven Valleys\" and the \"Four Valleys\". \"The Seven Valleys\" was written to a follower of Sufism, in the style of \u02bbAttar, the Persian Muslim poet, and sets forth the stages of the soul's journey towards God. It was first translated into English in 1906, becoming one of the earliest available books of Bah\u00e1\u02bcu'll\u00e1h to the West. \"The Hidden Words\" is another book written by Bah\u00e1\u02bcu'll\u00e1h during the same period, containing 153 short passages in which Bah\u00e1\u02bcu'll\u00e1h claims to have taken the basic essence of certain spiritual truths and written them in brief form.\nDemographics.\nAs of 2020, there were about 8 million Bah\u00e1'\u00eds in the world. In 2013, two scholars of demography wrote that, \"The Baha'i Faith is the only religion to have grown faster in every United Nations region over the past 100 years than the general population; Baha\u02bci [sic] was thus the fastest-growing religion between 1910 and 2010, growing at least twice as fast as the population of almost every UN region.\" \nThe largest proportions of the total worldwide Bah\u00e1'\u00ed population were found in sub-Saharan Africa (29.9%) and South Asia (26.8%), followed by Southeast Asia (12.7%) and Latin America (12.2%). Lesser numbers are found in North America (7.6%) and the Middle East/North Africa (6.2%), with the smallest being in Europe (2.0%), Australasia (1.6%), and Northeast Asia (0.9%). In 2015, the Bah\u00e1'\u00ed Faith was the second largest religion in Iran, Panama, Belize, Bolivia, Zambia, and Papua New Guinea, and the third largest in Chad and Kenya.\nFrom the Bah\u00e1'\u00ed Faith's origins in the 19th century until the 1950s, the vast majority of Bah\u00e1\u02bc\u00eds were found in Iran; converts from outside Iran were mostly found in India and the Western world. From having roughly 200,000 Bah\u00e1\u02bc\u00eds in 1950, the religion grew to have over 4 million by the late 1980s, with a wide international distribution. As of 2008, there were about 110,000 followers in Iran. Most of the growth in the late 20th century was seeded out of North America by means of the planned migration of individuals. Yet, rather than being a cultural spread from either Iran or North America, in 2001, sociologist David B. Barrett wrote that the Bah\u00e1\u02bc\u00ed Faith is, \"A world religion with no racial or national focus\". However, the growth has not been even. From the late 1920s to the late 1980s, the religion was banned and its adherents were harassed in the Soviet-led Eastern Bloc, and then again from the 1970s into the 1990s across some countries in sub-Saharan Africa. The most intense opposition has been in Iran and neighboring Shia-majority countries, considered an attempted genocide by some scholars, watchdog agencies and human rights organizations. Meanwhile, in other times and places, the religion has experienced surges in growth. Before it was banned in certain countries, the religion \"hugely increased\" in sub-Saharan Africa. In 1989 the Universal House of Justice named Bolivia, Bangladesh, Haiti, India, Liberia, Peru, the Philippines, and Taiwan as countries where the growth of the religion had been notable in the previous decades. Bah\u00e1'\u00ed sources claimed \"more than five million\" Bah\u00e1'\u00eds in 1991\u201392. However, since around 2001 the Universal House of Justice has prioritized statistics of the community by their levels of activity rather than simply their population of avowed adherents or numbers of local assemblies.\nBecause Bah\u00e1'\u00eds do not represent the majority of the population in any country, and most often represent only a tiny fraction of countries' total populations, there are problems of under-reporting. In addition, there are examples where the adherents have their highest density among minorities in societies who face their own challenges.\nMalietoa Tanumafili II of Samoa, who became Bah\u00e1\u02bc\u00ed in 1968 and died in 2007, was the first serving head of state to embrace the Bah\u00e1\u02bc\u00ed Faith. Bah\u00e1\u02bc\u00eds consider Queen Marie of Romania to be the first crowned head to accept and promote the teachings of Bah\u00e1\u02bcu'll\u00e1h.\nSocial practices.\nExhortations.\nThe following are a few examples from Bah\u00e1\u02bcu'll\u00e1h's teachings on personal conduct that are required or encouraged of his followers:\nProhibitions.\nThe following are a few acts of personal conduct that are prohibited or discouraged by Bah\u00e1\u02bcu'll\u00e1h's teachings:\nThe observance of personal laws, such as prayer or fasting, is the sole responsibility of the individual. There are, however, occasions when a Bah\u00e1\u02bc\u00ed might be administratively expelled from the community for a public disregard of the laws, or gross immorality. Such expulsions are administered by the National Spiritual Assembly and do not involve shunning.\nWhile some of the laws in the \"Kit\u00e1b-i-Aqdas\" are applicable at the present time, other laws are dependent upon the existence of a predominantly Bah\u00e1\u02bc\u00ed society, such as the punishments for arson and murder. The laws, when not in direct conflict with the civil laws of the country of residence, are binding on every Bah\u00e1\u02bc\u00ed.\nMarriage.\nThe purpose of marriage in the Bah\u00e1\u02bc\u00ed Faith is mainly to foster spiritual harmony, fellowship and unity between a man and a woman and to provide a stable and loving environment for the rearing of children. The Bah\u00e1\u02bc\u00ed teachings on marriage call it a \"fortress for well-being and salvation\" and place marriage and the family as the foundation of the structure of human society. Bah\u00e1\u02bcu'll\u00e1h highly praised marriage, discouraged divorce, and required chastity outside of marriage; Bah\u00e1\u02bcu'll\u00e1h taught that a husband and wife should strive to improve the spiritual life of each other. Interracial marriage is also highly praised throughout Bah\u00e1\u02bc\u00ed scripture.\nBah\u00e1\u02bc\u00eds intending to marry are asked to obtain a thorough understanding of the other's character before deciding to marry. Although parents should not choose partners for their children, once two individuals decide to marry, they must receive the consent of all living biological parents, whether they are Bah\u00e1\u02bc\u00ed or not. The Bah\u00e1\u02bc\u00ed marriage ceremony is simple; the only compulsory part of the wedding is the reading of the wedding vows prescribed by Bah\u00e1\u02bcu'll\u00e1h which both the groom and the bride read, in the presence of two witnesses. The vows are \"We will all, verily, abide by the Will of God.\"\nTransgender people can gain recognition of their gender in the Bah\u00e1\u02bc\u00ed Faith if they have medically transitioned and undergone sex reassignment surgery (SRS). After SRS, they are considered transitioned and may have a Bah\u00e1\u02bc\u00ed marriage.\nWork.\nBah\u00e1\u02bcu'll\u00e1h prohibited a mendicant and ascetic lifestyle. Monasticism is forbidden, and Bah\u00e1\u02bc\u00eds are taught to practice spirituality while engaging in useful work. The importance of self-exertion and service to humanity in one's spiritual life is emphasised further in Bah\u00e1\u02bcu'll\u00e1h's writings, where he states that work done in the spirit of service to humanity enjoys a rank equal to that of prayer and worship in the sight of God.\nPlaces of worship.\nBah\u00e1'\u00ed devotional meetings in most communities currently take place in people's homes or Bah\u00e1'\u00ed centres, but in some communities Bah\u00e1'\u00ed Houses of Worship (also known as Bah\u00e1'\u00ed temples) have been built. Bah\u00e1'\u00ed Houses of Worship are places where both Bah\u00e1\u02bc\u00eds and non-Bah\u00e1\u02bc\u00eds can express devotion to God. They are also known by the name \"Mashriqu'l-Adhk\u00e1r\" (Arabic for \"Dawning-place of the remembrance of God\"). Only the holy scriptures of the Bah\u00e1'\u00ed Faith and other religions can be read or chanted inside, and while readings and prayers that have been set to music may be sung by choirs, no musical instruments may be played inside. Furthermore, no sermons may be delivered, and no ritualistic ceremonies practiced. All Bah\u00e1'\u00ed Houses of Worship have a nine-sided shape (nonagon) as well as nine pathways leading outward and nine gardens surrounding them. There are currently eight \"continental\" Bah\u00e1'\u00ed Houses of Worship and some local Bah\u00e1'\u00ed Houses of Worship completed or under construction. The Bah\u00e1'\u00ed writings also envision Bah\u00e1'\u00ed Houses of Worship being surrounded by institutions for humanitarian, scientific, and educational pursuits, though none has yet been built up to such an extent.\nCalendar.\nThe Bah\u00e1\u02bc\u00ed calendar is based upon the calendar established by the B\u00e1b. The year consists of 19 months, each having 19 days, with four or five intercalary days, to make a full solar year. The Bah\u00e1\u02bc\u00ed New Year corresponds to the traditional Iranian New Year, called Naw R\u00faz, and occurs on the vernal equinox, near 21 March, at the end of the month of fasting. Once every Bah\u00e1\u02bc\u00ed month there is a gathering of the Bah\u00e1\u02bc\u00ed community called a Nineteen Day Feast with three parts: first, a devotional part for prayer and reading from Bah\u00e1\u02bc\u00ed scripture; second, an administrative part for consultation and community matters; and third, a social part for the community to interact freely.\nEach of the 19 months is given a name which is an attribute of God; some examples include Bah\u00e1\u02bc (Splendour), \u02bbIlm (Knowledge), and Jam\u00e1l (Beauty). The Bah\u00e1\u02bc\u00ed week is familiar in that it consists of seven days, with each day of the week also named after an attribute of God. Bah\u00e1\u02bc\u00eds observe 11 Holy Days throughout the year, with work suspended on 9 of these. These days commemorate important anniversaries in the history of the religion.\nSymbols.\nThe symbols of the religion are derived from the Arabic word Bah\u00e1\u02bc ( \"splendor\" or \"glory\"), with a numerical value of nine. This numerical connection to the name of Bah\u00e1\u02bcu'll\u00e1h, as well as nine being the highest single-digit, symbolizing completeness, are why the most common symbol of the religion is a nine-pointed star, and Bah\u00e1\u02bc\u00ed temples are nine-sided. The nine-pointed star is commonly set on Bah\u00e1\u02bc\u00ed gravestones.\nThe ringstone symbol and calligraphy of the Greatest Name are also often encountered. The ringstone symbol consists of two five-pointed stars interspersed with a stylized Bah\u00e1\u02bc whose shape is meant to recall God, the Manifestation of God, and the world of man; the Greatest Name is a calligraphic rendering of the phrase Y\u00e1 Bah\u00e1\u02bcu'l-Abh\u00e1 ( \"O Glory of the Most Glorious!\") and is commonly found in Bah\u00e1\u02bc\u00ed temples and homes.\nSocio-economic development.\nSince its inception the Bah\u00e1\u02bc\u00ed Faith has had involvement in socio-economic development beginning by giving greater freedom to women, promulgating the promotion of female education as a priority concern, and that involvement was given practical expression by creating schools, agricultural co-ops, and clinics.\nThe religion entered a new phase of activity when a message from the Universal House of Justice dated 20 October 1983 was released. Bah\u00e1\u02bc\u00eds were urged to seek out ways, compatible with the Bah\u00e1\u02bc\u00ed teachings, in which they could become involved in the social and economic development of the communities in which they lived. Worldwide in 1979 there were 129 officially recognized Bah\u00e1\u02bc\u00ed socio-economic development projects. By 1987, the number of officially recognized development projects had increased to 1,482.\nCurrent initiatives of social action include activities in areas like health, sanitation, education, gender equality, arts and media, agriculture, and the environment. Educational projects include schools, which range from village tutorial schools to large secondary schools, and some universities. By 2017, the Bah\u00e1\u02bc\u00ed Office of Social and Economic Development estimated that there were 40,000 small-scale projects, 1,400 sustained projects, and 135 Bah\u00e1\u02bc\u00ed-inspired organizations.\nUnited Nations.\nBah\u00e1\u02bcu'll\u00e1h wrote of the need for world government in this age of humanity's collective life. Because of this emphasis the international Bah\u00e1\u02bc\u00ed community has chosen to support efforts of improving international relations through organizations such as the League of Nations and the United Nations, with some reservations about the present structure and constitution of the UN. The Bah\u00e1\u02bc\u00ed International Community is an agency under the direction of the Universal House of Justice in Haifa, and has consultative status with the following organizations:\nThe Bah\u00e1\u02bc\u00ed International Community has offices at the United Nations in New York and Geneva and representations to United Nations regional commissions and other offices in Addis Ababa, Bangkok, Nairobi, Rome, Santiago, and Vienna. In recent years, an Office of the Environment and an Office for the Advancement of Women were established as part of its United Nations Office. The Bah\u00e1\u02bc\u00ed Faith has also undertaken joint development programs with various other United Nations agencies. In the 2000 Millennium Forum of the United Nations a Bah\u00e1\u02bc\u00ed was invited as one of the only non-governmental speakers during the summit.\nPersecution.\nBah\u00e1\u02bc\u00eds continue to be persecuted in some majority-Islamic countries, whose leaders do not recognize the Bah\u00e1\u02bc\u00ed Faith as an independent religion, but rather as apostasy from Islam. The most severe persecutions have occurred in Iran, where more than 200 Bah\u00e1\u02bc\u00eds were executed between 1978 and 1998. The rights of Bah\u00e1\u02bc\u00eds have been restricted to greater or lesser extents in numerous other countries, including Egypt, Afghanistan, Indonesia, Iraq, Morocco, Yemen, and several countries in sub-Saharan Africa.\nIran.\nThe most enduring persecution of Bah\u00e1\u02bc\u00eds has been in Iran, the birthplace of the religion. When the B\u00e1b started attracting a large following, the clergy hoped to stop the movement from spreading by stating that its followers were enemies of God. These clerical directives led to mob attacks and public executions. Starting in the twentieth century, in addition to repression aimed at individual Bah\u00e1\u02bc\u00eds, centrally directed campaigns that targeted the entire Bah\u00e1\u02bc\u00ed community and its institutions were initiated. In one case in Yazd in 1903 more than 100 Bah\u00e1\u02bc\u00eds were killed. Bah\u00e1\u02bc\u00ed schools, such as the Tarbiyat boys' and girls' schools in Tehran, were closed in the 1930s and 1940s, Bah\u00e1\u02bc\u00ed marriages were not recognized and Bah\u00e1\u02bc\u00ed texts were censored.\nDuring the reign of Mohammad Reza Pahlavi, to divert attention from economic difficulties in Iran and from a growing nationalist movement, a campaign of persecution against the Bah\u00e1\u02bc\u00eds was instituted. An approved and coordinated anti-Bah\u00e1\u02bc\u00ed campaign (to incite public passion against the Bah\u00e1\u02bc\u00eds) started in 1955 and it included the spreading of anti-Bah\u00e1\u02bc\u00ed propaganda on national radio stations and in official newspapers. During that campaign, initiated by Mulla Muhammad Taghi Falsafi, the Bah\u00e1'\u00ed center in Tehran was demolished at the orders of Tehran military governor, General Teymur Bakhtiar. In the late 1970s the Shah's regime consistently lost legitimacy due to criticism that it was pro-Western. As the anti-Shah movement gained ground and support, revolutionary propaganda was spread which alleged that some of the Shah's advisors were Bah\u00e1\u02bc\u00eds. Bah\u00e1\u02bc\u00eds were portrayed as economic threats, and as supporters of Israel and the West, and societal hostility against the Bah\u00e1\u02bc\u00eds increased.\nSince the Islamic Revolution of 1979, Iranian Bah\u00e1\u02bc\u00eds have regularly had their homes ransacked or have been banned from attending university or from holding government jobs, and several hundred have received prison sentences for their religious beliefs, most recently for participating in study circles. Bah\u00e1\u02bc\u00ed cemeteries have been desecrated and property has been seized and occasionally demolished, including the House of M\u00edrz\u00e1 Buzurg, Bah\u00e1\u02bcu'll\u00e1h's father. The House of the B\u00e1b in Shiraz, one of three sites to which Bah\u00e1\u02bc\u00eds perform pilgrimage, has been destroyed twice. In May 2018, the Iranian authorities expelled a young woman student from university of Isfahan because she was Bah\u00e1\u02bc\u00ed. In March 2018, two more Bah\u00e1\u02bc\u00ed students were expelled from universities in the cities of Zanjan and Gilan because of their religion.\nAccording to a US panel, attacks on Bah\u00e1\u02bc\u00eds in Iran increased under Mahmoud Ahmadinejad's presidency. The United Nations Commission on Human Rights revealed an October 2005 confidential letter from Command Headquarters of the Armed Forces of Iran ordering its members to identify Bah\u00e1\u02bc\u00eds and to monitor their activities. Due to these actions, the Special Rapporteur of the United Nations Commission on Human Rights stated on 20 March 2006, that she \"also expresses concern that the information gained as a result of such monitoring will be used as a basis for the increased persecution of, and discrimination against, members of the Bah\u00e1\u02bc\u00ed faith, in violation of international standards. The Special Rapporteur is concerned that this latest development indicates that the situation with regard to religious minorities in Iran is, in fact, deteriorating.\"\nOn 14 May 2008, members of an informal body known as the \"Friends\" that oversaw the needs of the Bah\u00e1\u02bc\u00ed community in Iran were arrested and taken to Evin prison. The Friends court case has been postponed several times, but was finally underway on 12 January 2010. Other observers were not allowed in the court. Even the defense lawyers, who for two years have had minimal access to the defendants, had difficulty entering the courtroom. The chairman of the U.S. Commission on International Religious Freedom said that it seems that the government has already predetermined the outcome of the case and is violating international human rights law. Further sessions were held on 7 February 2010, 12 April 2010 and 12 June 2010. On 11 August 2010 it became known that the court sentence was 20 years imprisonment for each of the seven prisoners which was later reduced to ten years. After the sentence, they were transferred to Gohardasht prison. In March 2011 the sentences were reinstated to the original 20 years. On 3 January 2010, Iranian authorities detained ten more members of the Baha'i minority, reportedly including Leva Khanjani, granddaughter of Jamaloddin Khanjani, one of seven Baha'i leaders jailed since 2008 and in February, they arrested his son, Niki Khanjani.\nThe Iranian government claims that the Bah\u00e1\u02bc\u00ed Faith is not a religion, but is instead a political organization, and hence refuses to recognize it as a minority religion. However, the government has never produced convincing evidence supporting its characterization of the Bah\u00e1\u02bc\u00ed community. The Iranian government also accuses the Bah\u00e1\u02bc\u00ed Faith of being associated with Zionism. These accusations against the Bah\u00e1\u02bc\u00eds appear to lack basis in historical fact, with some arguing they were invented by the Iranian government in order to use the Bah\u00e1\u02bc\u00eds as scapegoats.\nIn 2019, the Iranian government made it impossible for the Bah\u00e1\u02bc\u00eds to legally register with the Iranian state. National identity card applications in Iran no longer include the \"other religions\" option effectively making the Bah\u00e1\u02bc\u00ed Faith unrecognized by the state.\nEgypt.\nDuring the 1920s, Egypt's religious Tribunal recognized the Baha'i Faith as a new religion, independent from Islam, due to the nature of the 'laws, principles and beliefs' of the Baha'is.\nBah\u00e1\u02bc\u00ed institutions and community activities have been illegal under Egyptian law since 1960. All Bah\u00e1\u02bc\u00ed community properties, including Bah\u00e1\u02bc\u00ed centers, libraries, and cemeteries, have been confiscated by the government and fatwas have been issued charging Bah\u00e1\u02bc\u00eds with apostasy.\nThe Egyptian identification card controversy began in the 1990s when the government modernized the electronic processing of identity documents, which introduced a de facto requirement that documents must list the person's religion as Muslim, Christian, or Jewish (the only three religions officially recognized by the government). Consequently, Bah\u00e1\u02bc\u00eds were unable to obtain government identification documents (such as national identification cards, birth certificates, death certificates, marriage or divorce certificates, or passports) necessary to exercise their rights in their country unless they lied about their religion, which conflicts with Bah\u00e1\u02bc\u00ed religious principle. Without documents, they could not be employed, educated, treated in hospitals, travel outside of the country, or vote, among other hardships. Following a protracted legal process culminating in a court ruling favorable to the Bah\u00e1\u02bc\u00eds, the interior minister of Egypt released a decree on 14 April 2009, amending the law to allow Egyptians who are not Muslim, Christian, or Jewish to obtain identification documents that list a dash in place of one of the three recognized religions. The first identification cards were issued to two Bah\u00e1\u02bc\u00eds under the new decree on 8 August 2009.\nOttoman Empire.\nThe Bah\u00e1'\u00eds arrived in Acre, Palestine, in the 19th century, fleeing persecution. Ottoman authorities viewed Bah\u00e1'u'll\u00e1h as politically dangerous, which led to his exile to various locations within the Ottoman Empire, including Constantinople (Istanbul) and Adrianople (Edirne). After several exiles, Bah\u00e1'u'll\u00e1h finally arrived in Acre, where he lived under house arrest until his passing in 1892. His followers later established the Bah\u00e1'\u00ed Gardens and shrines in Haifa and Acre, both of which are now UNESCO World Heritage sites.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nBooks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nEncyclopedias.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nJournals.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nNews media.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nOther.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "4252", "revid": "14383484", "url": "https://en.wikipedia.org/wiki?curid=4252", "title": "Bahais", "text": ""}
{"id": "4253", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4253", "title": "Baader-Meinhof-Gang", "text": ""}
{"id": "4256", "revid": "367949", "url": "https://en.wikipedia.org/wiki?curid=4256", "title": "Baiuvarii", "text": "Predecessors of the Bavarians and Austrians\nThe Baiuvarii, Baiovari or early Bavarians were a Germanic people who first appeared in the 6th century. They originally lived in what had been the Roman province Raetia, south of the Danube, in what is now southern Bavaria. From there, their territory expanded. Their culture, language and political institutions are the predecessors of those of the medieval Duchy of Bavaria. Their polity developed under the influence of the Frankish empire, and eventually became a stem duchy.\nThe Baiuvarii are first mentioned in contemporary records starting in the 6th century, soon after the end of the Western Roman Empire, which had included Raetia and many of the surrounding countries.\nAmong the the Bavarian language developed, which is a West Germanic language closely related to Standard German. Modern versions are still spoken not only by modern-day Bavarians, but also by Austrians and South Tyroleans.\nLanguage.\nEarly evidence of the language of the Baiuvarii is limited to personal names and a few Runic inscriptions. However, by the 8th century AD, the Austro-Bavarian language was already well-established.\nThe language of the Baiuvarii was West Germanic, like its descendants medieval Old High German, and the modern Bavarian language. It was so similar to the contemporary languages of the neighbouring Alamanni, Thuringi, and Langobards, that it is difficult to tell them apart. This southern group of languages or dialects which are precursors to Old High German are sometimes distinguished from closely related northern dialects, such as those spoken by the Franks, as \"Elbe Germanic\". However, the model used to define this term is now considered obsolete, in favour of the idea that all or most of continental West Germanic languages were in one dialect continuum after the Migration period.\nA peculiarity of Bavarian compared to its neighbours is that it appears to have loaned words from East Germanic languages, such as Gothic.\nName.\nThe name of the Baiuvarii had many written variants, but many of these differences can be explained in terms of the spelling conventions of the time. For example, the use of the letters \"b\", \"v\", \"u\", \"uu\" and \"o\" was common when representing the same w-like sound in words from Germanic languages. Similarly, versions with a letter \"g\" such as , are using that letter to represent a palatal glide, or y-like sound. Versions with an initial p such as reflect the normal Upper German version of the High German consonant shift, which still distinguishes southern dialects of German today, and so this represents a real variation in pronunciations.\nModern scholars reconstruct the original Germanic pronunciation before the first written forms as \"*Baiwari\", and singular \"*Bai(a)warj\u014dz\". According to R\u00fcbekiel, the standard modern terms such as German \"Baj-u-waren\" and English \"Bai-u-varii\" are based on a misunderstanding of early medieval spellings such as \"Baiuuari\", where \"uu\" really represented a single w-like consonant, and not an \"uw\" syllable.\nDifferent etymologies can be proposed, but modern scholars normally understand the name as a compound of two elements: \"*Baia-/Baijo-\" which is believed to be a Germanic evolution from the pre-imperial Celtic tribal name Boii; and \"-warj\u014dz\" which is a common Germanic suffix used to create the names of peoples, by associating them with nouns such as regional names.\nThe earliest attestations are the following, from the 6th century, when the term seems to have been new. The early 6th century biography of Severinus of Noricum by Eugippius does not mention them at all although it describes the region around Passau and K\u00fcnzing in present day eastern Bavaria, and Roman Raetia, as coming under attack from Alemanni and Thuringians in the late 5th century, while Roman Noricum, now in Austria, under the control of the Rugii in the north, and Ostrogoths in the south. \nPossible sources of the name.\nThe Baiuvarii probably didn't exist under that name before the 5th century. However, the Boii, who seem to be the basis of the first element of the Baiuvarii name, almost disappeared from the written record around the time when the Roman empire began, centuries earlier. The form of the name is Germanic, both because of the conversion of the o-sound to an a-sound, and also the -varii suffix. This has led scholars to propose different ways in which the Baiuvarii name can be indirectly derived from the much older name of the Boii tribe. By explaining this, scholars also hope to get indications about how the Baiuvarii came into existence. \nA common and old proposal is that the Baiuvarii name somehow evolved from the classical version of the geographical term \"Bohemia\" which was used by Latin and Greek writers in the first century AD. Strabo called it \"Buiaimon\", Velleius Paterculus called it \"Boiohaemum\", and Tacitus called it \"Boiemum\". Modern scholars see this as a Germanic word, coined by the Suebi who settled there under Maroboduus, long after the Boii departed. The second component is Germanic \"*haim-\", the source of modern English \"home\" and modern German \"Heim\". One of the last classical reports of this name is the 2nd century mention of a people called the Bainochaemae. Claudius Ptolemy described them in his \"Geography\" as living near the Elbe, east of the Melibokus mountains, and north of the Asciburgius mountains. However, the \"haim\" part of the placename, which would have made the evidence for this etymology clear, does not appear in the name of Bavarians, leaving only the name of the Boii. According to this proposal, the Baivari migrated south from the original Bohemia, which is generally believed to be roughly where the modern Bohemia is, now in the Czech Republic. Alternatively, if Bohemia was in Moravia then the migration was from the east, along the Danube. \nThere are proposals that the name does not come from Bohemia at all, but directly from the Boii name itself, which was preserved in various areas to the east of Raetia (which would become the core of Bavaria). Examples which have been proposed include the following:\nThere are also several proposals about the ethnic background of the population who brought the Baiuvarii name with them to Raetia, and made it the name of the mixed community. However, most of the proposals involve the Suebian Germanic peoples to the east of Raetia such as the Marcomanni and Quadi, who disappeared from contemporary records during the time of Hun rule in the area.\nA third proposal is that the name of the Boii still survived in Raetia itself, making no migration necessary. In other words, people who still identified themselves as dwellers upon old lands of the pre-imperial Celtic Boii were still living in the Norican\u2013Raetian region, and were the name-giving element in the mixed population that remained there after Rome abandoned Raetia. According to Karl Bosl in 1971, Bavarian migration to present-day Bavaria is a legend, and Walter Goffart more recently agreed that there is no reason to assume any single large immigration in order to explain the 5th century origins of the Baiuvarii.\nModern commentary.\nThe Baiuvarii emerged about 500 after Odoacer (died 493) destroyed the Rugian kingdom just to the east of Bavaria in 487. He and his successor as king of Italy, Theoderic the Great (died 526), both had their roots in the Middle Danubian area and led large numbers of people from there to Italy. The Rugian kings ruled the countryside on the Danube west of Vienna, in what had been the Roman province of Noricum, and the Boii civitas of the Roman province of Pannonia .\nThe Rugian territory was soon taken over in this same period by Langobards, who had moved from the Lower Elbe, and filled the power vacuum in the Middle Elbe. However, after they migrated with many warriors to Italy in 568, the region east of the Bavarians was dominated by the Pannonian Avars, and Slavic languages became an important language of that region.\nThe Baiuvarii are believed to have incorporated elements from several Germanic peoples who lived longer in the regions surrounding Raetia, including the Rugii, Sciri, Marcomanni, Heruli, Quadi, Alemanni, Naristi, Thuringi and Langobards. They might also have included non-Germanic Romance people (romanized Celtic people).\nIt has been proposed that the Baiuvarii came into being as a distinct territory, distinct from the similar Alemanni and Langobards, under the influence of greater powers interested in the area. First Theodoric the Great in Italy, and later the Frankish kings Theuderic I and his son Theudebert I (died 548), seem to have controlled from a distance from a distance. \nTheudebert claimed in a letter to the Byzantine emperor Justinian that he controlled the area from the North Sea to Pannonia, which would include Bavaria. After his death, his uncle Chlothar I appointed Garibald I as \"dux\" of Bavaria. Garibald established the Agilolfing dynasty with his power base at Augsburg or Regensburg.\nArchaeology.\nIn the 20th century the early Baiuvarii were associated with Friedenhain-P\u0159e\u0161\u0165ovice ceramics, but this is no longer accepted by scholars. \nThe funerary traditions of the Baiuvarii are similar to those of the Alemanni, but quite different from those of the Thuringi. \nThe Baiuvarii are distinguished by the presence of individuals with artificially deformed craniums in their cemeteries. These individuals were predominantly female; there is no undisputed evidence of males with artificially deformed skulls in Bavaria. Genetic and archeological evidence shows that these women were migrants from eastern cultures, who married Bavarii males, suggesting the importance of exogamy within the Bavarii culture. The migrant women were fully integrated in to Bavarii culture.\nGenetics.\nThe genetic study published in the \"Proceedings of the National Academy of Sciences of the United States of America\" in 2018 examined the remains of 41 individuals buried at a Bavarian cemetery ca. 500 AD. Of these, 11 whole genomes were generated. The males were found to be genetically homogeneous and of north-central European origin. The females were less homogeneous and were largely of Southern European origin, some of whom carried significant East Asian ancestry. The presence of these women among the Bavarii people indicates that men from the Bavarii culture practiced exogamy, preferentially marrying women from eastern populations.\nThere were significant gender differences in skin, hair and eye pigmentation in the studied sample. Of the local males, with normal or intermediate skulls, the majority (~80%) were likely to have blond hair and blue eyes. In contrast, the women with artificially deformed skulls (likely immigrants), generally had brown eyes (80%) and either brown (60%) or blonde (40%) hair. The study also notes that these immigrant women would have stood out from the local population due to their enlarged crania and their distinct eye, hair, and possibly skin pigmentation.\nNo significant admixture with Roman populations from territories further south of the area was detected. Among modern populations, the surveyed male individuals did not have modified skulls and were found to be most closely related to modern-day Germans.\nOrigin myth.\nA medieval origin story exists for the Baiuvarii, the \"Annolied\" written in the 11th century, says that the Bavarian tribe came long ago from Armenia, \"where Noah came out of the ark\". The leaders of the Bavarian army are said to have been Duke Boimunt and his brother Ingram. The story was also reflected in the \"Song of Roland\", which mentions a Bavarian duke Naimes. Also the epic \"Karl\" written by \"Der Stricker\" says that Naymes, the Bavarian duke, was born in \"Ormen\u00eee\".\nThese origin-legends stem from learned medieval conceptions.\nLaw code.\nA collection of Bavarian tribal laws was compiled in the 8th century. This document is known as \"Lex Baiuvariorum\". Elements of it possibly date back to the 6th century. It is very similar to \"Lex Thuringorum\", which was the legal code of the Thuringi, with whom the Baiuvarii had close relations.\nChristianity.\nBy the 8th century, many Baiuvarii had converted to Christianity.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
