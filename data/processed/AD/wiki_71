{"id": "39477", "revid": "5957417", "url": "https://en.wikipedia.org/wiki?curid=39477", "title": "History of Austria", "text": "The history of Austria covers the history of Austria and its predecessor states. In the late Iron Age Austria was occupied by people of the Hallstatt Celtic culture (c. 800 BC), they first organized as a Celtic kingdom referred to by the Romans as Noricum, dating from c. 800 to 400 BC. At the end of the 1st century BC, the lands south of the Danube became part of the Roman Empire. In the Migration Period, the 6th century, the Bavarii, a Germanic people, occupied these lands until it fell to the Frankish Empire established by the Germanic Franks in the 9th century. In the year 976 AD, the first state of Austria formed. The name \"Ostarr\u00eechi\" (Austria) has been in use since 996 AD when it was a margravate of the Duchy of Bavaria and from 1156 an independent duchy (later archduchy) of the Holy Roman Empire (962\u20131806).\nAustria was dominated by the House of Habsburg and House of Habsburg-Lorraine from 1273 to 1918. In 1806, when Emperor Francis II of Austria dissolved the Holy Roman Empire, Austria became the Austrian Empire, and was also part of the German Confederation until the Austro-Prussian War of 1866. In 1867, Austria formed a dual monarchy with Hungary: the Austro-Hungarian Empire. When this empire collapsed after the end of World War I in 1918, Austria was reduced to the main, mostly German-speaking areas of the empire (its current frontiers), and adopted the name, the Republic of German-Austria. However, union with Germany and the chosen country name were forbidden by the Allies at the Treaty of Versailles. This led to the creation of the First Austrian Republic (1919\u20131933).\nFollowing the First Republic, Kurt Schuschnigg and the Fatherland Front tried to keep Austria independent from the German Reich. Engelbert Dollfuss accepted that most Austrians were German and Austrian, but wanted Austria to remain independent from Germany. In 1938, Austrian-born Adolf Hitler annexed Austria to Germany, which was supported by a large majority of Austrians. After the German defeat in World War II, the German identity in Austria was weakened. Ten years after the Second World War Austria again became an independent republic as the Second Austrian Republic in 1955. Austria joined the European Union in 1995.\nHistoriography.\nSince the territory understood by the term 'Austria' underwent drastic changes over time, dealing with a \"History of Austria\" raises a number of questions, e.g., whether it is confined to the current or former Republic of Austria, or extends also to all lands formerly ruled by the rulers of Austria. Furthermore, should Austrian history include the period 1938\u20131945, when it nominally did not exist? Of the lands now part of the second Republic of Austria, many were added over time \u2013 only two of the nine provinces (Lower and Upper Austria) are strictly 'Austria', while other parts of its former sovereign territory are now part of other countries e.g., Italy, Croatia, Slovenia and Czechia. Within Austria there are regionally and temporally varying affinities to adjacent countries.\nPrehistory.\nPaleolithic.\nThe Alps were inaccessible during the Ice Age, so human habitation dates no earlier than the Middle Paleolithic era, during the time of the Neanderthals. The oldest traces of human habitation in Austria, more than 250,000 years ago, were found in the Repolust Cave at Badl, near Peggau in the Graz-Umgebung district of Styria. These include stone tools, bone tools, and pottery fragments together with mammalian remains. Some 70,000-year-old evidence was found in the Gudenus Cave in northwestern Lower Austria.\nUpper Paleolithic remains are more numerous in Lower Austria. The best known are in the Wachau region, including the sites of the two oldest pieces of art in Austria. These are figurative representations of women, the Venus of Galgenberg found near Stratzing and thought to be 32,000 years old, and the nearby Venus of Willendorf (26,000 years old) found at Willendorf, near Krems an der Donau. In 2005 in the same area, a double infant burial site was discovered at Krems-Wachtberg, dating from Gravettian culture (27,000 years old), the oldest burial ground found in Austria to date.\nMesolithic.\nMesolithic remains include rock shelters (abris) from Lake Constance and the Alpine Rhine Valley, a funeral site at Elsbethen and a few other sites with microlithic artifacts which demonstrate the transition from living as hunter-gatherers and sedentary farmers and ranchers.\nNeolithic.\nDuring the Neolithic era, most of those areas of Austria that were amenable to agriculture and were sources of raw materials were settled. Remains include those of the Linear pottery culture, one of the first agrarian cultures in Europe. The first recorded rural settlement from this time was at Brunn am Gebirge in M\u00f6dling. Austria's first industrial monument, the chert mine at \"Mauer-Antonsh\u00f6he\" in the Mauer neighborhood of the southern Vienna district of Liesing dates from this period. In the Lengyel culture, which followed Linear Pottery in Lower Austria, circular ditches were constructed.\nCopper Age.\nTraces of the Copper Age in Austria were identified in the Carpathian Basin hoard at Stollhof, Hohe Wand, Lower Austria. Hilltop settlements from this era are common in eastern Austria. During this time the inhabitants sought out and developed raw materials in the central Alpine areas. The most important find is considered to be the Iceman \u00d6tzi, a well-preserved mummy of a man frozen in the Alps dating from approximately 3,300 BC, although these finds are now in Italy on the Austrian border. Another culture is the Mondsee group, represented by stilt houses in the Alpine lakes.\nBronze Age.\nBy the beginning of the Bronze Age fortifications were appearing, protecting the commercial centers of the mining, processing, and trading of copper and tin. This flourishing culture is reflected in the grave artifacts, such as at Pitten, in Nu\u00dfdorf ob der Traisen, Lower Austria. In the late Bronze Age appeared the Urnfield culture, in which salt mining commenced in the northern salt mines at Hallstatt.\nIron Age.\nThe Iron Age in Austria is represented by the Hallstatt culture, which succeeded the Urnfield culture, under influences from the Mediterranean civilizations and Steppe peoples. This gradually transitioned into the Celtic La T\u00e8ne culture.\nHallstatt culture.\nThis early Iron Age culture is named after Hallstatt the type site in Upper Austria. The culture is often described in two zones, Western and Eastern, through which flowed the rivers Enns, Ybbs and Inn. The West Hallstatt area was in contact with the Greek colonies on the Ligurian coast. In the Alps, contacts with the Etruscans and under Greek influence regions in Italy were maintained. The East had close links with the Steppe Peoples who had passed over the Carpathian Basin from the southern Russian steppes.\nThe population of Hallstatt drew its wealth from the salt industry. Imports of luxury goods stretching from the North and Baltic seas to Africa have been discovered in the cemetery at Hallstatt. The oldest evidence of an Austrian wine industry was discovered in Zagersdorf, Burgenland in a grave mound. The Cult Wagon of Strettweg, Styria is evidence of contemporary religious life.\nLa T\u00e8ne (Celtic) culture.\nIn the later Iron Age, the Celtic La T\u00e8ne culture spread to Austria. This culture gave rise to the first-recorded local tribal (Taurisci, Ambidravi, Ambisontes) and place names. Out of this arose Noricum (2nd century to \"c\". 15 b.c.) \u2013 a confederation of Alpine Celtic tribes (traditionally twelve) under the leadership of the Norici. It was confined to present-day southern and eastern Austria and part of Slovenia. The West was settled by the Raeti.\nD\u00fcrrnberg and Hallein (Salzburg) were Celtic salt settlements. In eastern Styria and the Burgenland (e.g., Oberpullendorf) high-quality iron ore was mined and processed, then exported to the Romans as \"ferrum noricum\" (Noric iron). This led to the creation of a Roman trading outpost on the Magdalensberg in the early 1st century b.c., later replaced by the Roman town Virunum. Fortified hilltop settlements, e.g. Kulm (east Styria), Idunum (mod. Villach), Burg (Schwarzenbach), and Braunsberg (Hainburg), were centers of public life. Some cities, such as Linz, date back to this period also.\nRoman era.\nDuring the Roman Empire, the territory of present-day Austria corresponded roughly with the Roman province of Noricum which was annexed by the empire around 15 BC, beginning 500 years of \"Austria Romana\" (as it became known in the 19th century). The western and eastern extremities of present-day Austria were within the Roman provinces of Raetia, and Pannonia.\nDuring Emperor Claudius's reign (41\u201354 AD), Noricum was bounded on the east approximately by the Vienna Woods, the current eastern border of Styria, and parts of the Danube, Eisack, Drava rivers. Under Diocletian (284\u2013305), Noricum was divided along the main Alpine ridge into a north (\"Noricum ripense\") and a south (\"Noricum Mediterraneum\"). Across the Ziller in the west, corresponding approximately to the present provinces of Vorarlberg and Tyrol, lay the province of Raetia. Present day Burgenland in the east was in Pannonia. To the south was Region 10, \"Venetia et Histria\". The Danubian \"limes\", formed a defensive line separating Upper and Lower Austria from Germanic tribes, most importantly the Marcomanni.\nThe Romans built many Austrian cities that survive today. They include Vindobona (Vienna), Juvavum (Salzburg), Valdidena (Innsbruck), and Brigantium (Bregenz). Other important towns were Virunum (north of the modern Klagenfurt), Teurnia (near Spittal), and Lauriacum (Enns). Archaeological sites from the Roman period include Kleinklein (Styria) and Zollfeld (Magdalensberg).\nChristianity appeared in Austria in the 2nd century, prompting Church organization that can be traced back to the 4th century. After the arrival of the Bavarii, Austria became the object of new missionary efforts from the Frankish west, such as Rupert and Virgil of the Hiberno-Scottish mission.\nFourth and fifth centuries.\nAfter centuries of tension and war on the Danube border which ran through present day Austria, the power of the Marcomanni seems to have been broken by 300 AD. Many, perhaps most of them, were moved into the empire. It seems that the Rugii and Heruli may have already moved into the Marcomanni's traditional region north of the Danube soon afterwards. The \"Laterculus Veronensis\" shows that Heruli and Rugii were already present somewhere in western Europe in about 314. Similar listings from later in the 4th century, the \"Cosmographia\" of Julius Honorius, and probably also the \"Liber Generationis\", both listed the Heruli together with the Marcomanni and Quadi, in whose traditional region the Herule kingdom would later be found.\nIn 378 AD, Roman forces suffered a major defeat to the Goths at the Battle of Adrianople, which was caused by a sudden movement of peoples coming from present-day Ukraine, most notably the Goths and Huns. The Romans recovered control, but were forced to try new approaches to settling newcomers in large numbers. One of the armed groups responsible for the defeat, led by Alatheus and Saphrax, were settled into the Pannonian part of the Roman empire, including the east of Austria, and expected to do military service for Rome. \nAs the Roman Empire's control over these border regions crumbled, the ability of Raetia, Noricum, and Pannonia to defend themselves became increasingly problematic. A Gothic leader Radagaisus overran part of the country in 405. After several raids on Italy, the Visigoths arrived in Noricum in 408, under another Gothic leader, Alaric I. As described by Zosimus, Alaric set out from Emona (modern Ljubljana) which lay between Pannonia Superior and Noricum over the Carnic Alps arriving at Virunum in Noricum, as had been agreed to by the Roman general Stilicho, following several skirmishes between the two. Alaric was voted a large amount of money to maintain peace, by the Roman Senate, at Stilicho's instigation. From there he directed his operations against Italy, demanding Noricum among another territory, finally sacking Rome in 410 but dying on the route home that year. During this period, in 409, Saint Jerome wrote a letter mentioning that many of the peoples from around the region east of Austria, even from within the empire, were occupying Gaul at that time: \"Quadi, Vandals, Sarmatians, Alans, Gepids, Herules, Saxons, Burgundians, Allemanni and\u2014alas! for the commonweal!\u2014even Pannonians\".\nThere was a short period of stability around 431. In 427 the chronicle of Marcellinus Comes says that the provinces of Pannonia, \"which had been held by the Huns for fifty years, were reclaimed by the Romans\". However, in 433 Flavius A\u00ebtius effectively ceded Pannonia to Attila. In 451 the Huns and their allies, now under the command of Attila must have poured through the area on their way to Gaul where they were defeated the Battle of the Catalaunian Plains that year. Attila died a few years later in 453, and this was followed by the Battle of Nedao in 454, when the sons of Attila and their Ostrogothic allies were defeated, The victors were able to consolidate independent kingdoms north of the Middle Danube. North of the Danube in present day Austria where the Marcomanni had been were the Rugii, and Heruli. \nIn 468 the Ostrogoths won the Battle of Bolia, giving them hegemony over the Pannonian kingdoms. During the cold winter of 469/470, the Ostrogoths unexpectedly attacked Hunimund, ruler of a Suevian kingdom, by crossing the frozen Danube from the east. These Suevi were at this time in a confederation with the Alemanni, in an Alpine region with streams that flowed loudly into the Danube, Baiuvarii (early Bavarians) on the east, Franks on the west, Burgundians on the south, and Thuringians on the north. This is one of the first mentions of the early Bavarians. They subsequently came to dominate the western alpine parts of present day western Austria.\nIn 476 Odoacer became ruler of Italy with barbarian forces including Heruli and Rugii, and other peoples from the Danubian region. Remnants of the Roman organization survived south of the Danube in the form of fortified strongholds, but the barbarians raided frequently, as described in the biography Severinus of Noricum by Eugippius. Noricum was eventually abandoned in 488, while Raetia was abandoned by the Romans to the Alamanni. In 493 Theoderic the Great, an Ostrogothic king, killed Odoacer and took control of Italy. By 500 the Herulian kingdom on the Danube, apparently by now under a king named Rodulph, had conquered their neighbours the Rugii, and become allies with Theoderic in Italy. In 508 Rodulph was killed by the Langobards, a Germanic people who had been moving southwards in several steps, and had occupied the Rugian territory.\nSixth and seventh centuries.\nDuring the 540s, the Lombards crossed the Danube into Roman Pannonia, in the west of present day Austria, bringing them into conflict with the Gepids. After defeating them with help from the Avars in 567, the Lombards recruited many locals and moved into northern Italy, starting in 568. The Avars and their vassal Slavs subsequently began moving into the Pannonian area, having previously established themselves from the Baltic Sea to the Balkans. After the Avars suffered setbacks in the east in 626, the Slavs rebelled, establishing their own territories. The Carantanians (Alpine Slavs) elected a Bavarian, Odilo, as their Count, and successfully resisted further Avar subjugation.\nThe Carantanians migrated westward along the Drava into the Eastern Alps in the wake of the expansion of their Avar overlords during the 7th century, mixed with the Celto-Romanic population, and established the realm of Carantania (later Carinthia), which covered much of eastern and central Austrian territory and was the first independent Slavic state in Europe, centered at Zollfeld. Together with the indigenous population they were able to resist further encroachment of the neighboring Franks and Avars in the southeastern Alps.\nMeanwhile, the Germanic tribe of the Bavarii (Frankish vassals), had developed in the 5th and 6th century in the west of the country and in later-known Bavaria, while Alemans had settled in later-known Vorarlberg. In the northern alps the Bavarians were established as a stem dukedom by around 550, under Agilolfing rule until 788 as an eastern Frankish Empire outpost. Those lands that were occupied by the Bavarians extended south to later-known South Tyrol, and east to the Enns. The administrative center was at Regensburg. Those groups mixed with the Rhaeto-Romanic population and pushed it up into the mountains along the Puster Valley.\nIn the south of modern Austria, Slavs had settled in the valleys of the Drava, Mura and Save by 600. The westward Slavic migration stopped further Bavarian migration eastwards by 610. Their most westward expansion was reached in 650 at the Puster Valley, but gradually fell back to the Enns by 780. The settlement boundary between Slavs and Bavarians roughly corresponds to a line from Freistadt through Linz, Salzburg (Lungau), to East Tyrol (Lesachtal), with Avars and Slavs occupying eastern Austria and modern Bohemia.\nCarantania, under pressure of the Avars, became a vassal to Bavaria in 745 and was later incorporated into the Carolingian empire, first as a tribal margravate under Slavic dukes and, after the failed rebellion of Ljudevit Posavski in the early 9th century, under Frankish-appointed noblemen. During the following centuries, Bavarian settlers went down the Danube and up the Alps, a process through which Austria was to become a mostly German-speaking country. Only in southern Carinthia, the Slavs maintained their language and identity until the early 20th century, when assimilation reduced them to a small minority.\nMiddle Ages.\nEarly Middle Ages: Duchy of Bavaria (8th\u201310th centuries).\nBavarian relationship with the Franks varied, achieving temporary independence by 717, only to be subjugated by Charles Martel. Finally Charlemagne (Emperor 800\u2013814) deposed the last Agilolfing duke, Tassilo III, assuming direct Carolingian control in 788, with non-hereditary Bavarian kings. Charlemagne subsequently led the Franks and Bavarians against the eastern Avars in 791, so that by 803 they had fallen back to the east of the Fischa and Leitha rivers. These conquests enabled the establishment of a system of defensive marches (military borderlands) from the Danube to the Adriatic. By around 800, \u00d6sterreich, the \"Kingdom of the East,\" had been joined to the Holy Roman Empire.\nAmong these was an eastern march, the Avar March, corresponding roughly to present day Lower Austria, bordered by the rivers Enns, Raab and Drava, while to the south lay the March of Carinthia. Both marches were collectively referred to as the Marcha orientalis (Eastern March), a prefecture of the Duchy of Bavaria. In 805, the Avars, with Charlemagne's permission, led by the Avar Khagan, settled south-eastward from Vienna.\nA new threat appeared in 862, the Hungarians, following the pattern of displacement from more eastern territories by superior forces. By 896 the Hungarians were present in large numbers on the Hungarian Plain from which they raided the Frankish domains. They defeated the Moravians and in 907 defeated the Bavarians at the Battle of Pressburg and by 909 had overrun the marches forcing the Franks and Bavarians back to the Enns River.\nBavaria became a Margraviate under Engeldeo (890\u2013895) and was re-established as a Duchy under Arnulf the Bad (907\u2013937) who united it with the Duchy of Carinthia, occupying most of the eastern alps. This proved short lived. His son Eberhard (937\u2013938) found himself in conflict with the German King, Otto I (Otto the Great) who deposed him. The next Duke was Henry I (947\u2013955), who was Otto's brother. In 955 Otto successfully forced back the Hungarians at the Battle of Lechfeld, beginning a slow reconquest of the eastern lands, including Istria and Carniola.\nDuring the reign of Henry's son, Henry II (the Quarrelsome) (955\u2013976) Otto became the first Holy Roman Emperor (962) and Bavaria became a duchy of the Holy Roman Empire. Otto I re-established the eastern march, and was succeeded by Otto II in 967, and found himself in conflict with Henry who he deposed, allowing him to re-organise the duchies of his empire.\nOtto considerably reduced Bavaria, re-establishing Carinthia to the south. To the east, he established a new Bavarian Eastern March, subsequently known as Austria, under Leopold, count of Babenberg in 976. Leopold I, also known as Leopold the Illustrious ruled Austria from 976 to 994.\nBabenberg Austria (976\u20131246).\nMargraviate (976\u20131156).\nThe marches were overseen by a \"comes\" or \"dux\" as appointed by the emperor. These terms are usually translated as count or duke, but these terms conveyed very different meanings in the Early Middle Ages, so to avoid misunderstanding historians usually employ the Latin versions when discussing the titles and their holders. In Lombardic speaking countries, the title was eventually regularized to \"margrave\" (German: \"markgraf\") i.e. \"count of the mark\".\nThe first recorded instance of the name 'Austria' appeared in 996, in a document of King Otto III written as \"Ostarr\u00eechi\", referring to the territory of the Babenberg March. In addition, for a long time the form \"Osterlant\" was in use, the inhabitants being referred to as \"Ostermann\" or \"Osterfrau\". The Latinized name \"Austria\" applied to this area appears in the 12th Century writings in the time of Leopold III (1095\u20131136). (compare \"Austrasia\" as the name for the north-eastern part of the Frankish Empire). The term \"Ostmark\" is not historically certain and appears to be a translation of \"marchia orientalis\" that came up only much later.\nThe Babenbergs pursued a policy of settling the country, clearing forests and founding towns and monasteries. They ruled the March from P\u00f6chlarn initially, and later from Melk, continually expanding the territory eastward along the Danube valley, so that by 1002 it reached Vienna. The eastward expansion was finally halted by the newly Christianized Hungarians in 1030, when King Stephen (1001\u20131038) of Hungary defeated the Emperor, Conrad II (1024\u20131039) at Vienna.\nA 'core' territory had finally been established. The land contained the remnant of many prior civilisations, but the Bavarians predominated, except in the Lake Constance area to the west occupied by the Alemanni (Vorarlberg). Pockets of the Celto-Romanic population persisted, such as around Salzburg, and Roman place names persisted, such as Juvavum (Salzburg). In addition this population was distinguished by Christianity and by their language, a Latin dialect (Romansch). Salzburg was already a bishopric (739), and by 798 an archbishopric.\nAlthough the Germanic Bavarians steadily replaced Romansch as the main language, they adopted many Roman customs and became increasingly Christianized. Similarly in the east, German replaced the Slavic language. The March of Austria's neighbours were the Duchy of Bavaria to the west, the Kingdoms of Bohemia and Poland to the North, the Kingdom of Hungary to the east and the Duchy of Carinthia to the south. In this setting, Austria, still subject to Bavaria was a relatively small player.\nThe Babenberg Margraves controlled very little of modern Austria. Salzburg, historically part of Bavaria became an ecclesiastical territory, while Styria was part of the Carinthian Duchy. The Babenbergs had relatively small holdings, with not only Salzburg but the lands of the Diocese of Passau lying in the hands of the church, and the nobility controlling much of the rest. However they embarked on a programme of consolidating their power base. One such method was to employ indentures servants such as the \"Kuenringern\" family as \"Ministeriales\" and given considerable military and administrative duties. They survived as a dynasty through good fortune and skill at power politics, in that era dominated by the continual struggle between emperor and papacy.\nThe path was not always smooth. The fifth Margrave, Leopold II 'The Fair' (1075\u20131095) was temporarily deposed by the Emperor Henry IV (1084\u20131105) after finding himself on the wrong side of the Investiture Dispute. However Leopold's son, Leopold III 'The Good' (1095\u20131136) backed Henry's rebellious son, Henry V (1111\u20131125), contributed to his victory and was rewarded with the hand of Henry's sister Agnes von Waiblingen in 1106, thus allying himself with the Imperial family. Leopold then concentrated on pacifying the nobility. His monastic foundations, particularly Klosterneuburg and Heiligenkreuz, led to his posthumous canonisation in 1458, and he became Austria's patron saint.\nUnion with Bavaria 1139.\nLeopold III was succeeded by his son, Leopold IV 'The Generous' (1137\u20131141). Leopold further enhanced the status of Austria by also becoming Duke of Bavaria in 1139, as Leopold I. Bavaria itself had been in the hands of the Welf (Guelph) dynasty, who were pitted against the Hohenstaufen. The latter came to the imperial throne in 1138 in the person of Conrad III (1138\u20131152); the Duke of Bavaria, Henry the Proud, was himself a candidate for the imperial crown and disputed the election of Conrad, and was subsequently deprived of the Duchy, which was given to Leopold IV. When Leopold died, his lands were inherited by his brother Henry II (Heinrich Jasomirgott) (1141\u20131177).\nIn the meantime, Conrad had been succeeded as emperor by his nephew Frederick I Barbarossa (1155\u20131190), who was descended from both the Welfs and Hohenstauffens and sought to end the conflicts within Germany. To this end he returned Bavaria to the Welfs in 1156, but as compensation elevated Austria to a duchy through an instrument known as the Privilegium Minus. Henry II thus became Duke of Austria in exchange for losing the title of Duke of Bavaria.\nDuchy of Austria (1156\u20131246).\nAustria was now an independent dominion within the Holy Roman Empire, and Henry moved his official residence to Vienna that year.\nLeopold the Virtuous and union with Styria (1177\u20131194).\nIn 1186 the Georgenberg Pact bequeathed Austria's southern neighbour, the Duchy of Styria to Austria upon the death of the childless Duke of Styria, Ottokar IV, which occurred in 1192. Styria had been carved out of the northern marches of Carinthia, and only raised to the status of Duchy in 1180. However the territory of the Duchy of Styria extended far beyond the current state of Styria, including parts of present-day Slovenia (Lower Styria), and also parts of Upper Austria (the Traungau, the area around Wels and Steyr) and Lower Austria (the county of Pitten, today's districts of Wiener Neustadt and Neunkirchen).\nThe second Duke of Austria, Henry II's son Leopold V the Virtuous (1177\u20131194) became Duke of these combined territories. Leopold is perhaps best known for his imprisonment of the British king, Richard I following the Third Crusade (1189\u20131192), in 1192 at D\u00fcrnstein. The ransom money he received helped finance many of his projects.\nAt that time, the Babenberg Dukes came to be one of the most influential ruling families in the region, peaking in the reign of Henry's grandson Leopold VI the Glorious (1198\u20131230), the fourth Duke. under whom the culture of the High Middle Ages flourished, including the introduction of Gothic art.\nFrederick the Quarrelsome: Division of the land and end of a dynasty (1230\u20131246).\nOn Leopold's death, he was succeeded by his son Frederick II the Quarrelsome (1230\u20131246). In 1238 he divided the land into two areas divided by the River Enns. That part above the Enns became \"Ob(erhalb) der Enns\" (Above the Enns) or 'Upper Austria', although other names such as \"supra anasum\" (from an old Latin name for the river), and \"Austria superior\" were also in use. Those lands below the Enns or \"unter der Enns\" became known as Lower Austria. The Traungau and Steyr were made part of Upper Austria rather than Styria. Another of Frederick's achievements was a Patent of Protection for Jews in 1244.\nHowever Frederick was killed in the Battle of the Leitha River against the Hungarians, and had no surviving children. Thus the Babenburg dynasty became extinct in 1246.\nInterregnum (1246\u20131278).\nWhat followed was an \"interregnum\", a period of several decades during which the status of the country was disputed, and during which Frederick II's duchy fell victim to a prolonged power play between rival forces. During this time there were multiple claimants to the title, including Vladislaus, Margrave of Moravia son of King Wenceslaus I of Bohemia. King Wenceslaus aimed at acquiring the Duchy of Austria by arranging the marriage of Vladislaus to the last Duke's niece Gertrud, herself a potential heir and claimant.\nAccording to the Privilegium Minus issued by Emperor Frederick Barbarossa in 1156, the Austrian lands could be bequeathed through the female line. Vladislaus received the homage of the Austrian nobility, but died shortly afterwards, on 3 January 1247, before he could take possession of the duchy. Next came Herman of Baden in 1248. He also made claim by seeking Gertrud's hand but did not have the support of the nobility. Herman died in 1250, and his claim was taken up by his son Frederick, but his claim was thwarted by the Bohemian invasion of Austria.\nIn an attempt to end the turmoil a group of Austrian nobles invited the king of Bohemia, Ottokar II P\u0159emysl, Vladislaus' brother, to become Austria's ruler in 1251. His father had attempted to invade Austria in 1250. Ottokar then proceeded to ally himself to the Babenbergs by marrying Margaret, Duke Frederick II's sister and daughter to Leopold VI, therefore making him a potential claimant to the throne, in 1252. He subdued the quarrelsome nobles and made himself ruler of most of the area, including Austria, Styria (which was previously under the rule of Hungary) as well as Carniola and Carinthia, both of which he had claimed by a dubious right of inheritance.\nOttokar was a lawmaker and builder. Among his achievements was the founding of the Hofburg Palace in Vienna. Ottokar was in a position to establish a new empire, given the weakness of the Holy Roman Empire on the death of Frederick II (1220\u20131250) particularly after the Hohenstauffen dynasty was ended in 1254 with the death of Conrad IV and the ensuing \"Imperial interregnum\" (1254\u20131273). Thus Ottokar put himself forward as a candidate for the imperial throne, but was unsuccessful.\nReligious persecution.\nDuring the interregnum, Austria was the scene of intense persecution of heretics by the Inquisition. The first instances appear in 1260 in over forty parishes in the southern Danube region between the Salzkammergut and the Vienna Woods, and were mainly directed against the Waldensians.\nHabsburg ascent and death of Ottokar (1273\u20131278).\nOttokar again contested the Imperial Throne in 1273, being almost alone in this position in the electoral college. This time he refused to accept the authority of the successful candidate, Rudolf of Habsburg (Emperor 1273\u20131291). In November 1274 the Imperial Diet at Nuremberg ruled that all crown estates seized since the death of the Emperor Frederick II (1250) must be restored, and that King Ottokar II must answer to the Diet for not recognising the new emperor, Rudolf. Ottokar refused either to appear or to restore the duchies of Austria, Styria and Carinthia with the March of Carniola, which he had claimed through his first wife, a Babenberg heiress, and which he had seized while disputing them with another Babenberg heir, Margrave Hermann VI of Baden.\nRudolph refuted Ottokar's succession to the Babenberg patrimony, declaring that the provinces must revert to the Imperial crown due to the lack of male-line heirs (a position that however conflicted with the provisions of the Austrian \"Privilegium Minus\"). King Ottokar was placed under the imperial ban; and in June 1276 war was declared against him, Rudolf laying siege to Vienna. Having persuaded Ottokar's former ally Duke Henry XIII of Lower Bavaria to switch sides, Rudolph compelled the Bohemian king to cede the four provinces to the control of the imperial administration in November 1276.\nOttokar having relinquished his territories outside of the Czech lands, Rudolph re-invested him with the Kingdom of Bohemia, betrothed his youngest daughter, Judith of Habsburg, (to Ottokar's son Wenceslaus II), and made a triumphal entry into Vienna. Ottokar, however, raised questions about the execution of the treaty, made an alliance with some Piast chiefs of Poland, and procured the support of several German princes, again including Henry XIII of Lower Bavaria. To meet this coalition, Rudolph formed an alliance with King Ladislaus IV of Hungary and gave additional privileges to the Vienna citizens.\nOn 26 August 1278, the rival armies met at the Battle on the Marchfeld, northeast of Vienna, where Ottokar was defeated and killed. The Margraviate of Moravia was subdued and its government entrusted to Rudolph's representatives, leaving Ottokar's widow Kunigunda of Slavonia, in control of only the province surrounding Prague, while the young Wenceslaus II was again betrothed to Judith.\nRudolf was thus able to assume sole control over Austria, as Duke of Austria and Styria (1278\u20131282) which remained under Habsburg rule for over six centuries, until 1918.\nThe establishment of the Habsburg dynasty: Duchy of Austria (1278\u20131453).\nThus Austria and the Empire came under a single Habsburg crown, and after a few centuries (1438) would remain so almost continuously (see below) till 1806, when the empire was dissolved, obviating the frequent conflicts that had occurred previously.\nRudolph I and primogeniture (1278\u20131358).\nRudolf I spent several years establishing his authority in Austria, finding some difficulty in establishing his family as successors to the rule of the province. At length the hostility of the princes was overcome and he was able to bequeath Austria to his two sons. In December 1282, at the Diet of Augsburg, Rudolph invested the duchies of Austria and Styria on his sons, Albert I (1282\u20131308) and Rudolph II the Debonair (1282\u20131283) as co-rulers \"jointly and severally\", and so laid the foundation of the House of Habsburg. Rudolf continued his campaigns subduing and subjugating and adding to his domains, dying in 1291, but leaving dynastic instability in Austria, where frequently the Duchy of Austria was shared between family members. However Rudolf was unsuccessful in ensuring the succession to the imperial throne for the Dukes of Austria and Styria.\nThe conjoint dukedom lasted only a year until the Treaty of Rheinfelden in 1283 established the Habsburg order of succession. Establishing primogeniture, then eleven-year-old Duke Rudolph II had to waive all his rights to the thrones of Austria and Styria to the benefit of his elder brother Albert I. While Rudolph was supposed to be compensated, this did not happen, dying in 1290, and his son John subsequently murdered his uncle Albert I in 1308. For a brief period, Albert I also shared the duchies with Rudolph III the Good (1298\u20131307), and finally achieved the imperial throne in 1298.\nOn Albert I's death, the duchy but not the empire passed to his son, Frederick the Fair (1308\u20131330), at least not until 1314 when he became co-ruler of the empire with Louis IV. Frederick also had to share the duchy with his brother Leopold I the Glorious (1308\u20131326). Yet another brother, Albert II the Lame (1330\u20131358) succeeded Frederick.\nThe pattern of corule persisted, since Albert had to share the role with another younger brother Otto I the Merry (1330\u20131339), although he did attempt to unsuccessfully lay down the rules of succession in the \"Albertinian House Rule\". When Otto died in 1339, his two sons, Frederick II and Leopold II replaced him, making three simultaneous Dukes of Austria from 1339 to 1344 when both of them died in their teens without issue. Single rule in the Duchy of Austria finally returned when his son, Rudolph IV succeeded him in 1358.\nIn the 14th century the Habsburgs began to accumulate other provinces in the vicinity of the Duchy of Austria, which had remained a small territory along the Danube, and Styria, which they had acquired with Austria from Ottokar. In 1335 Albert II inherited the Duchy of Carinthia and the March of Carniola from the then rulers, the House of Gorizia.\nRudolph IV and the \"Privilegium Maius\" (1358\u20131365).\nRudolf IV the Founder (1358\u20131365) was the first to claim the title of Archduke of Austria, through the Privilegium Maius of 1359, which was actually a forgery and not recognized outside of Austria till 1453. However it would have placed him on a level footing with the other Prince-electors of the Holy Roman Empire. Rudolph was one of the most active rulers of his time, initiating many measures and elevating the importance of the City of Vienna.\nAt that time Vienna was ecclesiastically subordinate to the Diocese of Passau, which Rudolph subverted by founding St Stephen's Cathedral and appointing the provost as the Archchancellor of Austria. He also founded the University of Vienna. He improved the economy and established a stable currency, the Vienna Penny. When he died in 1365 he was without issue and the succession passed to his brothers jointly under the Rudolfinian House Rules.\nIn 1363, the County of Tyrol was acquired by Rudolph IV from Margaret of Tyrol. Thus Austria was now a complex country in the Eastern Alps, and these lands are often referred to as the Habsburg Hereditary Lands, as well as simply Austria, since the Habsburgs also began to accumulate lands far from their Hereditary Lands.\nAlbert III and Leopold III: A house divided (1365\u20131457).\nAlmost the entire 15th Century was a confusion of estate and family disputes, which considerably weakened the political and economic importance of the Habsburg lands. It was not until 1453 in the reign of Frederick V the Peaceful (1457\u20131493) that the country (at least the core territories) would be finally united again. Rudolph IV's brothers Albert III the Pigtail and Leopold III the Just quarreled ceaselessly and eventually agreed to split the realm in the Treaty of Neuberg in 1379, which was to result in further schisms later. Altogether this resulted in three separate jurisdictions.\nAlbertinian line (1379\u20131457).\nIn 1379 Albert III retained Austria proper, ruling till 1395. He was succeeded by his son Albert IV (1395\u20131404) and grandson Albert V (1404\u20131439) who regained the imperial throne for the Habsburgs and through his territorial acquisitions was set to become one of the most powerful rulers in Europe had he not died when he did, leaving only a posthumous heir, born four months later (Ladislaus the Posthumous 1440\u20131457). Instead it was Ladislaus' guardian and successor, the Leopoldian Frederick V the Peaceful (1457\u20131493) who benefited. The Albertinian line having become extinct, the title now passed back to the Leopoldians. Frederick was so aware of the potential of being the young Ladislaus' guardian that he refused to let him rule independently upon reaching majority (12 in Austria at the time) and had to be forced to release him by the Austrian Estates (League of Mailberg 1452).\nLeopoldian line (1379\u20131490).\nLeopold III took the remaining territories, ruling till 1386. He was succeeded by two of his sons jointly, William the Courteous (1386\u20131406) and Leopold IV the Fat (1386\u20131411). In 1402 yet another split in the Duchy occurred, since Leopold III had had four sons and neither Leopold IV or William had heirs. The remaining brothers then divided the territory.\nErnest the Iron (1402\u20131424) took Inner Austria, while Frederick IV of the Empty Pockets (1402\u20131439) took Further Austria. Once William died in 1406, this took formal effect with two separate ducal lines, the \"Elder Ernestine Line\" and \"Junior Tyrolean Line\" respectively.\nErnestine line (Inner Austria 1406\u20131457)\nThe Ernestine line consisted of Ernest and a joint rule by two of his sons upon his death in 1424, Albert VI the Prodigal (1457\u20131463) and Frederick V the Peaceful (1457\u20131493). They too quarreled and in turn divided what had now become both Lower and Inner Austria upon the death of Ladislaus in 1457 and extinction of the Albertinians. Albert seized Upper Austria in 1458, ruling from Linz, but in 1462 proceeded to besiege his elder brother in the Hofburg Palace in Vienna, seizing lower Austria too. However, since he died childless the following year (1463) his possessions automatically reverted to his brother, and Frederick now controlled all of the Albertinian and Ernestine possessions.\nFrederick's political career had advanced in a major way, since he inherited the Duchy of Inner Austria in 1424. From being a Duke, he became German King as Frederick IV in 1440 and Holy Roman Emperor as Frederick III (1452\u20131493).\nTyrolean line (Further Austria) 1406\u20131490\nThe Tyrolean line consisted of Frederick IV and his son, Sigismund the Rich (1439\u20131490). Frederick moved his court to Innsbruck but lost some of his possessions to Switzerland. Sigismund who succeeded him sold some of his lands to Charles the Bold in 1469 and was elevated to Archduke by Emperor Frederick III in 1477. He died childless, but in 1490, he abdicated in the face of unpopularity and Further Austria reverted to the then Archduke, Maximilian I the Last Knight (1490\u20131493), Frederick V's son who now effectively controlled all the Habsburg territory for the first time since 1365.\nReligious persecution.\nThe inquisition was also active under the Habsburgs, particularly between 1311 and 1315 when inquisitions were held in Steyr, Krems, St. P\u00f6lten and Vienna. The Inquisitor, Petrus Zwicker, conducted severe persecutions in Steyr, Enns, Hartberg, Sopron and Vienna between 1391 and 1402. In 1397 there were some 80\u2013100 Waldensians burnt in Steyr alone, now remembered in a 1997 monument.\nDuchy and Kingdom.\nDuring the Habsburg Duchy, there were 13 consecutive Dukes, of whom four were also crowned King of Germany, Rudolf I, Albert I, Frederick the Fair, and Albert V (Albert II as King of Germany), although none were recognised as Holy Roman Emperors by the Pope.\nWhen Duke Albert V (1404\u20131439) was elected as emperor in 1438 (as Albert II), as the successor to his father-in-law, Sigismund von Luxemburg (1433\u20131437) the imperial crown returned once more to the Habsburgs. Although Albert himself only reigned for a year (1438\u20131439), from then on, every emperor was a Habsburg (with only one exception: Charles VII 1742\u20131745), and Austria's rulers were also the Holy Roman Emperors until its dissolution in 1806.\nArchduchy of Austria: Becoming a Great Power (1453\u20131564).\nFrederick V (1453\u20131493): Elevation of the duchy.\nFrederick V (Duke 1424 Archduke 1453, died 1493) the Peaceful (Emperor Frederick III 1452-\u20131493) confirmed the \"Privilegium Maius\" of Rudolph IV in 1453, and so Austria became an official archduchy of the Holy Roman Empire, the next step in its ascendancy within Europe, and Ladislaus the Posthumous (1440\u20131457) the first official archduke for a brief period, dying shortly after. The document was a forgery, purportedly written by the Emperor Frederick I and \"rediscovered\". Frederick had a clear motive for this. He was a Habsburg, he was Duke of Inner Austria in addition to being Emperor, and, up till the previous year, had been guardian of the young Duke of Lower Austria, Ladislaus. He also stood to inherit Ladislaus's title, and did so when Ladislaus died four years later, becoming the second Archduke.\nThe Austrian Archdukes were now of equal status to the other Prince Electors that selected the emperors. Austrian governance was now to be based on primogeniture and indivisibility. Later Austria was to become officially known as \"Erzherzogtum \u00d6sterreich ob und unter der Enns\" (The Archduchy of Austria above and below the Enns). In 1861 it was again divided into Upper and Lower Austria.\nThe relative power of the emperor in the monarchy was not great, as many other aristocratic dynasties pursued their own political power inside and outside the monarchy. However Frederick, although lackluster, pursued a tough and effective rule. He pursued power through dynastic alliances. In 1477 Maximilian (Archduke and Emperor 1493\u20131519), Frederick's only son, married Mary, Duchess of Burgundy, thus acquiring most of the Low Countries for the family. The strategic importance of this alliance was that Burgundy, which lay on the western border of the empire, was demonstrating expansionist tendencies, and was at that time one of the richest and most powerful of the Western European nation states, with territories stretching from the south of France to the North Sea.\nThe alliance was achieved at no small cost, since France, which also claimed Burgundy, contested this acquisition, and Maximilian had to defend his new wife's territories from Louis XI, finally doing so upon Mary's death in 1482 at the Peace of Arras. Relationships with France remained difficult, Louis XI being defeated at the Battle of Guinegate in 1479. Matters with France were only concluded in 1493 at the Treaty of Senlis after Maximilian had become emperor.\nThis and Maximilian's later dynastic alliances gave rise to the saying:\n\"Bella gerant alii, tu felix Austria nube\",\n\"Nam quae Mars aliis, dat tibi regna Venus\"\nwhich became a motto of the dynasty. Frederick's reign was pivotal in Austrian history. He united the core lands by simply outliving the rest of his family. From 1439, when Albert V died and the responsibilities for both of the core territories lay with Frederick, he systematically consolidated his power base. The next year (1440) he marched on Rome as King of the Romans with his ward, Ladislaus the last Albertinian duke, and when he was crowned in Rome in 1452 he was not only the first Habsburg but also the last German king to be crowned in Rome by the Pope.\nThe dynasty was now en route to become a world power. The concept of \"pietas austriacae\" (the divine duty to rule) had originated with Rudolph I, but was reformulated by Frederick as AEIOU, \"Alles Erdreich ist \u00d6sterreich untertan\" or \"Austriae est imperare orbi universo\" (Austria's destiny is to rule the world), which came to symbolise Austrian power. However, not all events proceeded smoothly for Frederick. The Austrian-Hungarian War (1477\u20131488) resulted in the Hungarian king, Matthias Corvinus setting himself up in Vienna in 1485 till his death in 1490. Hungary occupied the entire Eastern Austria. Frederick therefore found himself with an itinerant court, predominantly in the Upper Austrian capital of Linz.\nMaximilian I (1493\u20131519): Reunification.\nMaximilian I shared rule with his father during the latter year of Frederick's reign, being elected King of the Romans in 1486. By acquiring the lands of the Tyrolean line of the Habsburgs in 1490 he finally reunited all the Austrian lands, divided since 1379. He also needed to deal with the Hungarian problem when Mathias I died in 1490. Maximilian reconquered the lost parts of Austria and established peace with Mathias's successor Vladislaus II at the Peace of Pressburg in 1491. However the dynastic pattern of division and unification would be one that kept repeating itself over time. With unsettled borders Maximilian found Innsbruck in the Tyrol a safer place for a capital, between his Burgundian and Austrian lands, although he was rarely in any place for very long, being acutely aware of how his father had been repeatedly besieged in Vienna.\nMaximilian raised the art of dynastic alliance to a new height and set about systematically creating a dynastic tradition, albeit through considerable revisionism. His wife Mary, was to die in 1482, only five years after they were married. He then married Anne, Duchess of Brittany (by proxy) in 1490, a move that would have brought Brittany, at that time independent, into the Habsburg fold, which was considered provocative to the French monarchy. Charles VIII of France had other ideas and annexed Brittany and married Anne, a situation complicated further by the fact that he was already betrothed to Maximilian's daughter Margaret, Duchess of Savoy. Maximilian's son, Philip the Fair (1478\u20131506) married Joanna, heiress of Castile and Aragon in 1496, and thus acquired Spain and its Italian (Naples, Kingdom of Sicily and Sardinia), African, and New World appendages for the Habsburgs.\nHowever \"Tu felix Austria nube\" was perhaps more romantic than strictly realistic, since Maximilian was not slow to wage war when it suited his purpose. Having settled matters with France in 1493, he was soon involved in the long Italian Wars against France (1494\u20131559). In addition to the wars against the French, there were the wars for Swiss independence. The Swabian War of 1499 marked the last phase of this struggle against the Habsburgs. Following defeat at the Battle of Dornach in 1499, Austria was forced to recognise Swiss independence at the Treaty of Basel in 1499, a process that was finally formalised by the Peace of Westphalia in 1648. This was significant as the Habsburgs had originated in Switzerland, their ancestral home being Habsburg Castle.\nIn domestic policy, Maximilian launched a series of reforms at the 1495 Diet of Worms, at which the Imperial Chamber Court was launched as the highest court. Another new institution of 1495 was the Reichsregiment or Imperial government, meeting at Nuremberg. This preliminary exercise in democracy failed and was dissolved in 1502. Attempts at creating a unified state were not very successful, but rather re-emerged the idea of the three divisions of Austria that existed prior to the unification of Frederick and Maximilian.\nShort of funds for his various schemes he relied heavily on banking families such as the Fugger's, and it was these bankers that bribed the prince electors to choose Maximilian's grandson Charles as his successor. One tradition he did away with was the centuries-old custom that the Holy Roman Emperor had to be crowned by the Pope in Rome. Unable to reach Rome, due to Venetian hostility, in 1508, Maximilian, with the assent of Pope Julius II, took the title \"Erw\u00e4hlter R\u00f6mischer Kaiser\" (\"Elected Roman Emperor\"). Thus his father Frederick was the last emperor to be crowned by the Pope in Rome, though his grandson Charles V would be crowned by the Pope outside of Rome; no other Holy Roman Emperor would be crowned by the Pope.\nCharles I and Ferdinand I (1519\u20131564).\nSince Philip the Fair (1478\u20131506) died before his father, Maximilian, the succession passed to Philip's son, Charles I (1519\u20131521) who became the Emperor Charles V, on Maximilian's death in 1519. He reigned as emperor from 1519 to 1556, when in poor health he abdicated, dying in 1558. Although crowned by Pope Clement VII in Bologna in 1530 (Charles had sacked Rome in 1527) he was the last emperor ever to be crowned by a Pope. Although he eventually fell short of his vision of universal monarchy, Charles I is still considered the most powerful of all the Habsburgs. His Chancellor, Mercurino Gattinara remarked in 1519 that he was \"on the path to universal monarchy\u00a0... unite all Christendom under one sceptre\" bringing him closer to Frederick V's vision of AEIOU, and Charles' motto \"Plus ultra\" (still further) suggested this was his ambition.\nHaving inherited his father's possessions in 1506, he was already a powerful ruler with extensive domains. On Maximilian's death these domains became vast. He was now ruler of three of Europe's leading dynasties\u2014the House of Habsburg of the Habsburg monarchy; the House of Valois-Burgundy of the Burgundian Netherlands; and the House of Trast\u00e1mara of the Crowns of Castile and Aragon. This made him ruler over extensive lands in Central, Western, and Southern Europe; and the Spanish colonies in the Americas and Asia. As the first king to rule Castile, Le\u00f3n, and Aragon simultaneously in his own right, he became the first King of Spain. His empire spanned nearly four million square kilometers across Europe, the Far East, and the Americas.\nA number of challenges stood in Charles's way, and were to shape Austria's history for a long time to come. These were France, the appearance of the Ottoman Empire to its East, and Martin Luther (see below).\nFollowing the dynastic tradition the Habsburgs' hereditary territories were separated from this enormous empire at the Diet of Worms in 1521, when Charles I left them to the regency of his younger brother, Ferdinand I (1521\u20131564), although he then continued to add to the Habsburg territories. Since Charles left his Spanish Empire to his son Philip II of Spain, the Spanish territories became permanently alienated from the northern Habsburg domains, although remained allies for several centuries.\nBy the time Ferdinand also inherited the title of Holy Roman Emperor from his brother in 1558 the Habsburgs had effectively turned an elective title into a \"de facto\" hereditary one. Ferdinand continued the tradition of dynastic marriages by marrying Anne of Bohemia and Hungary in 1521, effectively adding those two kingdoms to the Habsburg domains, together with the adjacent territories of Moravia, Silesia and Lusatia. This took effect when Anne's brother Louis II, King of Hungary and Bohemia (and hence the Jagiellon dynasty) died without heir at the Battle of Moh\u00e1cs in 1526 against Suleiman the Magnificent and the Ottomans. However, by 1538 the Kingdom of Hungary was divided into three parts:\nFerdinand's election to emperor in 1558 once again reunited the Austrian lands. He had had to cope with revolts in his own lands, religious turmoil, Ottoman incursions and even contest for the Hungarian throne from John Sigismund Z\u00e1polya. His lands were by no means the most wealthy of the Habsburg lands, but he succeeded in restoring internal order and keeping the Turks at bay, while enlarging his frontiers and creating a central administration.\nWhen Ferdinand died in 1564, the lands were once more divided up between his three sons, a provision he had made in 1554.\nAustria in the Reformation and Counter-Reformation (1517\u20131564).\nMartin Luther and the Protestant Reformation (1517\u20131545).\nWhen Martin Luther posted his ninety-five theses to the door of the Castle Church in Wittenberg in 1517, he challenged the very basis of the Holy Roman Empire, Catholic Christianity, and hence Habsburg hegemony. After the Emperor Charles V interrogated and condemned Luther at the 1521 Diet of Worms, Lutheranism and the Protestant Reformation spread rapidly in the Habsburg territories. Temporarily freed from war with France by the 1529 Treaty of Cambrai and the denouncement of the ban on Luther by the Protestant princes at Speyer that year, the Emperor revisited the issue next at the Diet of Augsburg in 1530, by which time it was well-established.\nWith the Ottoman threat growing (see below), he needed to ensure that he was not facing a major schism within Christianity. He refuted the Lutheran position (Augsburg Confession) with the Confutatio Augustana, and had Ferdinand elected King of the Romans on 5 January 1531, ensuring his succession as a Catholic monarch. In response, the Protestant princes and estates formed the Schmalkaldic League in February 1531 with French backing. Further Turkish advances in 1532 (which required him to seek Protestant aid) and other wars kept the emperor from taking further action on this front until 1547 when imperial troops defeated the League at the Battle of M\u00fchlberg, allowing him to once more impose Catholicism.\nIn 1541 Ferdinand's appeal to the estates general for aid against the Turks was met by demand for religious tolerance. The triumph of 1547 turned out to be short lived with French and Protestant forces again challenging the emperor in 1552 culminating in the Peace of Augsburg in 1555. Exhausted, Charles started to withdraw from politics and hand over the reins. Protestantism had proved too firmly entrenched to enable it to be uprooted.\nAustria and the other Habsburg hereditary provinces (and Hungary and Bohemia, as well) were much affected by the Reformation, but with the exception of Tyrol the Austrian lands shut out Protestantism. Although the Habsburg rulers themselves remained Catholic, the non-Austrian provinces largely converted to Lutheranism, which Ferdinand I largely tolerated.\nCounter-Reformation (1545\u20131563).\nThe Catholic response to the Protestant Reformation was a conservative one, but one that did address the issues raised by Luther. In 1545 the long running Council of Trent began its work of reform and a Counter-Reformation on the borders of the Habsburg domains. The Council continued intermittently until 1563. Ferdinand and the Austrian Habsburgs were far more tolerant than their Spanish brethren, and the process initiated at Trent. However his attempts at reconciliation at the Council in 1562 was rejected, and although a Catholic counteroffensive existed in the Habsburg lands from the 1550s it was based on persuasion, a process in which the Jesuits and Peter Canisius took the lead. Ferdinand deeply regretted the failure to reconcile religious differences before his death (1564).\nThe arrival of the Ottomans (1526\u20131562).\nWhen Ferdinand I married into the Hungarian dynasty in 1521 Austria first encountered the westward Ottoman expansion which had first come into conflict with Hungary in the 1370s. Matters came to a close when his wife Anne's brother the young king Louis was killed fighting the Turks under Suleiman the Magnificent at the Battle of Moh\u00e1cs in 1526, the title passing to Ferdinand. Louis' widow Mary fled to seek protection from Ferdinand.\nThe Turks initially withdrew following this victory, reappearing in 1528 advancing on Vienna and laying siege to it the following year. They withdrew that winter till 1532 when their advance was stopped by Charles V, although they controlled much of Hungary. Ferdinand was then forced to recognize John Z\u00e1polya. Ferdinand and the Turks continued to wage war between 1537 and a temporary truce in 1547 when Hungary was partitioned. However hostilities continued almost immediately till the Treaty of Constantinople of 1562 which confirmed the 1547 borders. The Ottoman threat was to continue for 200 years.\nRedivision of the Habsburg lands (1564\u20131620).\nFerdinand I had three sons who survived to adulthood, and he followed the potentially disastrous Habsburg tradition of dividing up his lands between them on his death in 1564. This considerably weakened Austria, particularly in the face of the Ottoman expansion. It was not until the reign of Ferdinand III (Archduke 1590\u20131637) that they were reunited again in 1620\u2014albeit briefly until 1623. It was not to be until 1665, under Leopold I that the Austrian lands were definitively united.\nDuring the next 60 years the Habsburg monarchy was divided into three jurisdictions:\nAs the eldest son, Maximilian II and his sons were granted the \"core\" territories of Lower and Upper Austria. Ferdinand II dying without living issue, his territories reverted to the core territories on his death in 1595, then under Rudolf V (1576\u20131608), Maximilian II's son.\nMaximilian II was succeeded by three of his sons none of whom left living heirs, so the line became extinct in 1619 upon the abdication of Albert VII (1619\u20131619). Thus Charles II's son Ferdinand III inherited all of the Habsburg lands. However he promptly lost Bohemia which rebelled in 1619 and was briefly (1619\u20131620) under the rule of Frederick I. Thus all the lands again came under one ruler again in 1620 when Ferdinand III invaded Bohemia, defeating Frederick I.\nAlthough technically an elected position, the title of Holy Roman Emperor was passed down through Maximilian II and the two sons (Rudolf V and Mathias) that succeeded him. Albert VII was Archduke for only a few months before abdicating in favour of Ferdinand III, who also became emperor.\n\"Lower Austria\".\nRudolf V (Archduke, Emperor Rudolf II 1576\u20131612), Maximilian's eldest son, moved his capital from Vienna to the safer venue of Prague, in view of the Ottoman threat. He was noted as a great patron of the arts and sciences but a poor governor. Among his legacies is the Imperial Crown of the Habsburgs. He preferred to parcel out his responsibilities among his many brothers (of whom six lived to adulthood), leading to a great heterogeneity of policies across the lands. Among these delegations was making his younger brother Mathias, Governor of Austria in 1593.\nIn acquiring \"Upper Austria\" in 1595, his powers were considerably increased, since the remaining Inner Austria territories were in the hands of Ferdinand III who was only 17 at the time. However he handed over the administration to Maximilian III, another younger brother. In 1593 he instigated a new conflict with the Ottomans, who had resumed raids in 1568, in the so-called Long or Fifteen-Year War of 1593 to 1606. Unwilling to compromise, and envisioning a new crusade the results were disastrous, the exhausted Hungarians revolting in 1604. The Hungarian problem was further exacerbated by attempts to impose a counterreformation there. As a result, he handed over Hungary to Mathias who concluded the Peace of Vienna with the Hungarians, and Peace of Zsitvatorok with the Turks in 1606. As a result, Transylvania became both independent and Protestant.\nThese events led to conflict between the brothers. Melchior Klesl engineered a conspiracy of the archdukes to ensure Mathias' ascendancy. By 1608 Rudolf had ceded much of his territory to the latter. Further conflict led to Mathias imprisoning his brother in 1611, who now gave up all power except the empty title of emperor, dying the following year and being succeeded by Mathias.\nThus Mathias succeeded to the Archduchy in 1608, and became emperor in 1612, until his death in 1619. His reign was marked by conflict with his younger brother Maximilian III who was a more intransigent Catholic and backed the equally fervent Ferdinand II of \"Inner Austria\" as successor, having served as his regent between 1593 and 1595, before taking over \"Upper Austria\". The conflicts weakened the Habsburgs relative to both the estates and the Protestant interests. Mathias moved the capital back to Vienna from Prague and bought further peace from Turkey, by a treaty in 1615. Meanwhile, religious fervor in the empire was mounting, and even Klesl by now Bishop of Vienna (1614) and Cardinal (1615) was considered too moderate by extremist Catholics, including Ferdinand II. War was in the air, and the assault on two roral officials in Prague on 23 May 1618 (The Defenestration of Prague) was to spark all out war. Mathias, like his brother Rudolf, became increasingly isolated by Ferdinand who had imprisoned Klesl.\nThe next brother in line for succession in 1619 was Albert VII, but he was persuaded to step down in favour of Ferdinand II within a few months.\nReformation and Counter-Reformation.\nReligion played a large part in the politics of this period, and even tolerance had its limits faced with the incompatible demands of both camps. As the Archduke closest to the Turkish threat, Maximilian II was to continue his father's policy of tolerance and reconciliation, granting \"Assekuration\" (legalisation of Protestantism for the nobles) in 1571, as did Charles II with \"Religionspazifikation\" in 1572, while in distant Tyrol, Ferdinand II could afford to be more aggressive. Maximilian II's policies were continued by his sons, Rudolf V and Mathias. The strength of the Reformation in Upper Austria was blunted by internal schisms, while in Lower Austria Melchior Khlesl led a vigorous Catholic response, expelling Protestant preachers and promoting reconversion. A further concession by Charles II in 1578, the \"Brucker Pazifikation\", met with more resistance.\nThe Catholic Revival started in earnest in 1595 when Ferdinand II, who was Jesuit-educated came of age. He had succeeded his father, Charles II in Inner Austria in 1590 and was energetic in suppressing heresy in the provinces which he ruled. Reformation Commissions initiated a process of forced recatholicisation and by 1600 was being imposed on Graz and Klagenfurt. Unlike previous Austrian rulers, Ferdinand II was unconcerned about the effect of religious conflict on the ability to withstand the Ottomans. The Counter-Reformation was to continue to the end of the Thirty Year War in 1648.\nAustria and the Thirty Years' War (1618\u20131648).\nFerdinand II (1619\u20131637) and Habsburg over-reach.\nWhen the ultra-pious and intransigent Ferdinand II (1619\u20131637) was elected Emperor (as Ferdinand II) in 1619 to succeed his cousin Mathias, he embarked on an energetic attempt to re-Catholicize not only the Hereditary Provinces, but Bohemia and Habsburg Hungary as well as most of Protestant Europe within the Holy Roman Empire.\nOutside his lands, Ferdinand II's reputation for strong headed uncompromising intolerance had triggered the religious Thirty Years' War in May 1618 in the polarizing first phase, known as the Revolt in Bohemia. Once the Bohemian Revolt had been put down in 1620, he embarked on a concerted effort to eliminate Protestantism in Bohemia and Austria, which was largely successful as was his efforts to reduce the power of the Diet. The religious suppression of the Counter-Reformation reached its height in 1627 with the Provincial Ordinance.\nAfter several initial reverses, Ferdinand II had become more accommodating but as the Catholics turned things around and began to enjoy a long string of successes at arms he set forth the Edict of Restitution in 1629 in an attempt to restore the \"status quo\" of 1555 (Peace of Augsburg), vastly complicating the politics of settlement negotiations and prolonging the rest of the war. Encouraged by the mid-war successes, Ferdinand II became even more forceful, leading to infamies by his armies such as the Frankenburg Lottery (\"Frankenburger W\u00fcrfelspiel\") (1625), suppression of the consequent Peasants' Revolt of 1626, and the Sack of Magdeburg (1631) Despite concluding the Peace of Prague (1635) with Saxony, and hence the internal, or civil, war with the Protestants, the war would drag on due to the intervention of many foreign states.\nFerdinand III and the peace process (1637\u20131648).\nBy the time of Ferdinand II's death in 1637, the war was progressing disastrously for the Habsburgs, and his son Ferdinand III (1637\u20131657) who had been one of his military commanders was faced with the task of salvaging the consequences of his father's extremism. Ferdinand III was far more pragmatic and had been considered the leader of the peace party at court and had helped negotiate the Peace of Prague in 1635. However, with continuing losses in the war he was forced to make peace in 1648 with the Peace of Westphalia, concluding the war. One of his acts during the latter part of the war was to give further independence to the German states (\"ius belli ac pacis\"\u2014rights in time of war and peace) which would gradually change the balance of power between emperor and states in favour of the latter.\nAssessment.\nWhile its ultimate causes prove to be elusive, the war was to prove a roller-coaster as Habsburg over-reach led to it spreading from a domestic dispute to involve most of Europe, and which while at times appearing to aid the Habsburg goal of political hegemony and religious conformity, ultimately eluded them except in their own central territories.\nThe forced conversions or evictions carried out in the midst of the Thirty Years' War, together with the later general success of the Protestants, had greatly negative consequences for Habsburg control of the Holy Roman Empire itself. Although territorial losses were relatively small for the Habsburgs, the Empire was greatly diminished, the power of the ruler reduced and the balance of power in Europe changed with new centres emerging on the empire's borders. The estates were now to function more like nation states.\nWhile deprived of the goal of universal monarchy, the campaigns within the Habsburg hereditary lands were relatively successful in religiously purification, although Hungary was never successfully re-Catholicized. Only in Lower Austria, and only among the nobility, was Protestantism tolerated. Large numbers of people either emigrated or converted, while others compromised as crypto-Protestants, ensuing relative conformity. The crushing of the Bohemian Revolt also extinguished Czech culture and established German as the tool of Habsburg absolutism. The Austrian monarchs thereafter had much greater control within the hereditary power base, the dynastic absolutism grip was tightened and the power of the estates diminished. On the other hand, Austria suffered demographically and financially, therefore becoming less vigorous as a nation-state.\nThe Baroque Austrian Monarchy was established. Despite the dichotomy between outward reality and inner conviction, the rest of the world saw Austria as the epitome of forcible conformity, and conflation of church and state.\nImpact of war.\nIn terms of human costs, the Thirty Years' War's many economic, social, and population dislocations caused by the hardline methods adopted by Ferdinand II's strict counter-reformation measures and almost continual employment of mercenary field armies contributed significantly to the loss of life and tragic depopulation of all the German states, during a war which some estimates put the civilian loss of life as high as 50% overall. Studies mostly cite the causes of death due to starvation or as caused (ultimately by the lack-of-food induced) weakening of resistance to endemic diseases which repeatedly reached epidemic proportions among the general Central European population\u2014the German states were the battle ground and staging areas for the largest mercenary armies theretofore, and the armies foraged among the many provinces stealing the food of those people forced onto the roads as refugees, or still on the lands, regardless of their faith and allegiances. Both townsmen and farmers were repeatedly ravaged and victimized by the armies on both sides leaving little for the populations already stressed by the refugees from the war or fleeing the Catholic counter-reformation repressions under Ferdinand's governance.\nDynastic succession and redivision of the lands.\nThe Austrian lands finally came under one archduchy in 1620, but Ferdinand II quickly redivided them in 1623 in the Habsburg tradition by parcelling out \"Upper Austria\" (Further Austria and Tirol) to his younger brother Leopold V (1623\u20131632) who was already governor there. Upper Austria would remain under Leopold's successors till 1665 when it reverted to the senior line under Leopold I.\nLeopold V's son Ferdinand Charles succeeded him in Upper Austria in 1632. However he was only four at the time, leaving his mother Claudia de' Medici as regent till 1646.\nEstablishing the monarchy: Austria's rise to power (1648\u20131740).\nDespite the setbacks of the Thirty Years' War, Austria was able to recover economically and demographically and consolidate the new hegemony often referred to as Austrian Baroque. By 1714 Austria had become a great power again. Yet the roots of Habsburg legitimacy, with its reliance on religious and political conformity, was to make it increasingly anachronistic in the Age of Enlightenment. Nevertheless, in the arts and architecture the baroque flourished in Austria. In peacetime Ferdinand III (1637\u20131657) proved to be a great patron of the arts and a musician.\nUpon Ferdinand's death in 1657 he was succeeded by his son Leopold I (1657\u20131705), whose reign was relatively long. Meanwhile, in \"Upper Austria\" Ferdinand Charles (1632\u20131662) although also an arts patron ruled in an absolutist and extravagant style. His brother Sigismund Francis (1662\u20131665) succeeded him briefly in 1662, but dying without heir in 1665 his lands reverted to Leopold I. Thus from 1665 Austria was finally reunited under one archduchy.\nLeopold I (1657\u20131705): Final unification and liberation from Ottoman Empire.\nLeopold I's reign was marked by a return to a succession of wars. Even before he succeeded his father in 1657, he was involved in the Second Northern War (1655\u20131660) a carry over from Sweden's involvement in the Thirty Years' War, in which Austria sided with Poland, defeating Transylvania, a Swedish ally and Ottoman protectorate.\nAt the end of that war the Ottomans overran Nagyv\u00e1rad in Transylvania in 1660, which would mark the beginning of the decline of that principality and increasing Habsburg influence. In vain the Transylvanians appealed to Vienna for help, unaware of secret Ottoman-Habsburg agreements.\nFortunately for Austria, Turkey was preoccupied elsewhere during the Thirty Years' War when she would have been vulnerable to attack on her eastern flanks. It was not until 1663 that the Turks developed serious intentions with regard to Austria, which let to a disastrous event for the Ottoman army, being defeated at the Battle of Saint Gotthard the following year.\nThe terms, dictated by the need to deal with the French in the west, were so disadvantageous that they infuriated the Hungarians who revolted. To make matters worse, after executing the leaders, Leopold attempted to impose a counter-reformation, starting a religious civil war, although he made some concessions in 1681. Thus by the early 1680s Leopold was facing Hungarian revolt, backed by the Ottomans and encouraged by the French on the opposite flank.\nMeanwhile, Austria became involved elsewhere with the Franco-Dutch War (1672\u20131678) which was concluded with the Treaties of Nijmegen giving the French considerable opportunities, indeed the activities of the French, now also a major power, distracted Leopold from following up his advantage with the Turks, and Austro-Ottoman relationships were governed by the Peace of Vasv\u00e1r which would grant some twenty years relief. However the reunions bought a badly needed French neutrality while Austria kept watch to the east. The Ottomans next moved against Austria in 1682 in retaliation against Habsburg raids, reaching Vienna in 1683, which proved well fortified, and set about besieging it. The allied forces eventually proved superior and the lifting of the siege was followed by a series of victories in 1686, 1687 and 1697, resulting in the Treaty of Karlowitz (1699), Belgrade having fallen in 1688 (but recaptured in 1690). This provided Austrian hegemony over southern Central Europe and introduced a large number of Serbs into the Empire, who were to have a major impact on policies over the ensuing centuries.\nWith the eastern frontier now finally secured, Vienna could flourish and expand beyond its traditional limits. In the east Leopold was learning that there was little to be gained by harsh measures, which policy bought his acceptance and he granted the Hungarian diet rights through the \"Diploma Leopoldianum\" of 1691. However, on the military front, this merely freed up Austria to engage in further western European wars. Austria was becoming more involved in competition with France in Western Europe, fighting the French in the War of the League of Augsburg (1688\u20131697).\nOn the domestic front, Leopold's reign was marked by the expulsion of the Jews from Vienna in 1670, the area being renamed Leopoldstadt. While in 1680 Leopold adopted the so-called \"Pragmatica\", which re-regulated the relationship between landlord and peasant.\nWar of Spanish Succession (1701\u20131714): Joseph I and Charles III.\nMost complex of all was the War of the Spanish Succession (1701\u20131714), in which the French and Austrians (along with their British, Dutch and Catalan allies) fought over the inheritance of the vast territories of the Spanish Habsburgs. The ostensible cause was the future Charles III of Austria (1711\u20131740) claiming the vacant Spanish throne in 1701. Leopold engaged in the war but did not live to see its outcome, being succeeded by his Joseph I in 1705. Joseph's reign was short and the war finally came to an end in 1714 by which time his brother Charles III had succeeded him.\nAlthough the French secured control of Spain and its colonies for a grandson of Louis XIV, the Austrians also ended up making significant gains in Western Europe, including the former Spanish Netherlands (now called the Austrian Netherlands, including most of modern Belgium), the Duchy of Milan in Northern Italy, and Naples and Sardinia in Southern Italy. (The latter was traded for Sicily in 1720). By the conclusion of the war in 1714 Austria had achieved a pivotal position in European power politics.\nThe end of the war saw Austria's allies desert her in terms of concluding treaties with the French, Charles finally signing off in the Treaty of Rastatt in 1714. While the Habsburgs may not have gained all they wanted, they still made significant gains through both Rastatt and Karlowitz, and established their power. The remainder of his reign saw Austria relinquish many of these fairly impressive gains, largely due to Charles's apprehensions at the imminent extinction of the House of Habsburg.\nCharles III: Succession and the Pragmatic Sanction (1713\u20131740).\nFor Charles now had succession problems of his own, having only two surviving daughters. His solution was to abolish sole male inheritance by means of the Pragmatic Sanction of 1713. In 1703 his father Leopold VI had made a pact with his sons that allowed for female inheritance but was vague on details, and left room for uncertainty. The Pragmatic Sanction strengthened this and in addition made provision for the inseparability of the Habsburg lands.\nThis was to form the legal basis for the union with Hungary and to legitimise the Habsburg monarchy. It would be confirmed by the Austro-Hungarian Compromise of 1867 and would last to 1918. He then needed to strengthen the arrangement by negotiating with surrounding states. Internal negotiation proved to be relatively simple and it became law by 1723.\nCharles was now willing to offer concrete advantages in territory and authority in exchange for other powers' worthless recognitions of the Pragmatic Sanction that made his daughter Maria Theresa his heir. Equally challenging was the question of the heir apparent's marital prospects and how they might influence the European balance of power. The eventual choice of Francis Stephen of Lorraine in 1736 proved unpopular with the other powers, particularly France.\nWar continued to be part of European life in the early 18th century. Austria was involved in the War of the Quadruple Alliance and the resulting 1720 Treaty of The Hague was to see the Habsburg lands reach their greatest territorial expansion. War with France had broken out again in 1733 with the War of the Polish Succession whose settlement at the Treaty of Vienna in 1738 saw Austria cede Naples and Sicily to the Spanish Infante Don Carlos in exchange for the tiny Duchy of Parma and Spain and France's adherence to the Pragmatic Sanction. The later years of Charles's reign also saw further wars against the Turks, beginning with a successful skirmish in 1716\u20131718, culminating in the Treaty of Passarowitz. Less successful was the war of 1737\u20131739 which resulted in the Austrian loss of Belgrade and other border territories at the Treaty of Belgrade.\nOn the domestic front military and political gains were accompanied by economic expansion and repopulation, as Austria entered the period of High Baroque with a profusion of new buildings, including the Belvedere (1712\u20131783) and Karlskirche (1716\u20131737), exemplified by the great architects of the period, such as Fischer, Hildebrandt and Prandtauer. However the Habsburgs' finances were fragile. They had relied on Jewish bankers such as Samuel Oppenheimer to finance their wars, and subsequently bankrupted him. However the financial system in Austria remained antiquated and inadequate. By the time of Charles' death in 1740 the treasury was almost depleted.\nReligious intolerance in Austria, once unquestioned in the Erblande and the neighbouring catholic territories became the subject of more intense scrutiny by 1731 when 22,000 suspected crypto-Protestants were expelled from Salzburg and the Salzkammergut by the Prince-Archbishop of Salzburg. Similar intolerance was displayed to the Jewish population in Bohemia and surrounding areas under the Familianten in 1726 and 1727. Worse would have followed had there not also been a realisation that there were economic consequences and that some accommodation was required to the more rationalist ideas of western Europe. Among these was cameralism which encouraged economic self-sufficiency in the nation state. Thus domestic industries such as the \"Linzer Wollzeugfabrik\" were founded and encouraged, but often such ideas were subjugated by vested interests such as aristocracy and church. Rationalist emphasis on the natural and popular were the antithesis of Habsburg elitism and divine authority. Eventually external powers forced rationalism on Austria.\nBy the time of his death in 1740, Charles III had secured the acceptance of the Pragmatic Sanction by most of the European powers. The remaining question was whether it was realistic in the complicated power games of European dynasties.\nMaria Theresa and reform (1740\u20131780).\nCharles III died on 20 October 1740, and was succeeded by his daughter Maria Theresa. However she did not become Empress immediately, that title passing to Charles VII (1742\u20131745) the only moment in which the imperial crown passed outside of the Habsburg line from 1440 to 1806, Charles VII being one of many who repudiated the 1713 Pragmatic Sanction. As many had anticipated all those assurances from the other powers proved of little worth to Maria Theresa.\nWar of Austrian Succession (1740\u20131748).\nOn 16 December 1740 Prussian troops invaded Silesia under King Frederick the Great. This was the first of three Silesian Wars fought between Austria and Prussia in this period (1740\u20131742, 1744\u20131745 and 1756\u20131763). Soon other powers began to exploit Austria's weakness. Charles VII claimed the inheritance to the hereditary lands and Bohemia, and was supported by the King of France, who desired the Austrian Netherlands. The Spanish and Sardinians hoped to gain territory in Italy, and the Saxons hoped to gain territory to connect Saxony with the Elector's Polish Kingdom. France even went so far as to prepare for a partition of Austria.\nAustria's allies, Britain, the Dutch Republic, and Russia, were all wary of getting involved in the conflict; ultimately, only Britain provided significant support. Thus began the War of the Austrian Succession (1740\u20131748), one of the more confusing and less eventful wars of European history, which ultimately saw Austria holding its own, despite the permanent loss of most of Silesia to the Prussians. That represented the loss of one of its richest and most industrialised provinces. For Austria the War of Succession was more a series of wars, the first concluding in 1742 with the Treaty of Breslau, the second (1744\u20131745) with the Treaty of Dresden. The overall war however continued until the Treaty of Aix-la-Chapelle (1748).\nIn 1745, following the reign of the Bavarian Elector as Emperor Charles VII, Maria Theresa's husband Francis of Lorraine, Grand Duke of Tuscany, was elected Emperor, restoring control of that position to the Habsburgs (or, rather, to the new composite house of Habsburg-Lorraine), Francis holding the titular crown until his death in 1765, but his empress consort Maria Theresa carrying out the executive functions. The Pragmatic Sanction of 1713 applied to the hereditary possessions of the Habsburgs and Archduchy of Austria but not the position of Holy Roman Emperor, which could not be held by women, thus Maria Theresa was Empress Consort not Empress Regnant.\nSeven Years' War and Third Silesian War (1756\u20131763).\nFor the eight years following the Treaty of Aix-la-Chapelle that ended the War of the Austrian Succession, Maria Theresa plotted revenge on the Prussians. The British and Dutch allies who had proved so reluctant to help her in her time of need were dropped in favour of the French in the so-called Reversal of Alliances of 1756, under the advice of Kaunitz, Austrian Chancellor (1753\u20131793). This resulted in the Treaty of Versailles of 1756. That same year, war once again erupted on the continent as Frederick, fearing encirclement, launched a pre-emptive invasion of Saxony and the defensive treaty became offensive. The ensuing Third Silesian War (1756\u20131763, part of the larger Seven Years' War) was indecisive, and its end saw Prussia holding onto Silesia, despite Russia, France, and Austria all combining against him, and with only Hanover as a significant ally on land.\nThe end of the war saw Austria, poorly prepared at its start, exhausted. Austria continued the alliance with France (cemented in 1770 with the marriage of Maria Theresa's daughter Archduchess Maria Antonia to the Dauphin), but also facing a dangerous situation in Central Europe, faced with the alliance of Frederick the Great of Prussia and Catherine the Great of Russia. The Russo-Turkish War of 1768\u20131774 caused a serious crisis in east-central Europe, with Prussia and Austria demanding compensation for Russia's gains in the Balkans, ultimately leading to the First Partition of Poland in 1772, in which Maria Theresa took Galicia from Austria's traditional ally.\nWar of Bavarian Succession (1778\u20131779).\nOver the next several years, Austro-Russian relations began to improve. When the War of Bavarian Succession (1778\u20131779) erupted between Austria and Prussia following the extinction of the Bavarian line of the Wittelsbach dynasty, Russia refused to support Austria, its ally from the Seven Years' War, but offered to mediate and the war was ended, after almost no bloodshed, on 13 May 1779, when Russian and French mediators at the Congress of Teschen negotiated an end to the war. In the agreement Austria received the Innviertel from Bavaria, but for Austria it was a case of \"status quo ante bellum\". This war was unusual for this era in that casualties from disease and starvation exceeded wounds, and is considered the last of the Cabinet wars in which diplomats played as large a part as troops, and as the roots of German Dualism (Austria\u2013Prussia rivalry).\nReform.\nAlthough Maria Theresa and her consort were Baroque absolutist conservatives, this was tempered by a pragmatic sense and they implemented a number of overdue reforms. Thus these reforms were pragmatic responses to the challenges faced by archduchy and empire, not ideologically framed in the Age of Enlightenment as seen by her successor, Joseph II.\nThe collision with other theories of nation states and modernity obliged Austria to perform a delicate balancing act between accepting changing economic and social circumstances while rejecting their accompanying political change. The relative failure to deal with modernity produced major changes in Habsburg power and Austrian culture and society. One of the first challenges that Maria Theresa and her advisers faced was to restore the legitimacy and authority of the dynasty, although was slowly replaced by a need to establish the needs of State.\nGovernance and finance.\nMaria Theresa promulgated financial and educational reforms, with the assistance of her advisers, notably Count Friedrich Wilhelm von Haugwitz and Gerard van Swieten. Many reforms were in the interests of efficiency. Her financial reforms considerably improved the state finances, and notably introduced taxation of the nobility for the first time, and achieved a balanced budget by 1775. At an administrative level, under Haugwitz she centralised administration, previously left to the nobility and church, along Prussian models with a permanent civil service. Haugwitz was appointed head of the new \"Directorium in publicis und cameralibus\" in 1749. By 1760 it was clear this was not solving Austria's problems and further reform was required. Kaunitz' proposal for a consultative body was accepted by Maria Theresa. This Council of State was to be based on the French Conseil d'\u00c9tat which believed that an absolutist monarch could still be guided by Enlightenment advisors. The council was inaugurated in January 1761, composed of Kaunitz the state chancellor, three members of the high nobility, including von Haugwitz as chair, and three knights, which served as a committee of experienced people who advised her. The council of state lacked executive or legislative authority. This marked Kaunitz' ascendency over von Haugwitz. The Directory was abolished and its functions absorbed into the new united Austrian and Bohemian chancelleries (\"B\u00f6hmisch-\u00d6sterreichische Hofkanzlei\") in 1761.\nEducation.\nWhile Von Haugwitz modernised the army and government, van Swieten reformed health care and education. Educational reform included that of Vienna University by Swieten from 1749, the founding of the Theresianum (1746) as a civil service academy as well as military and foreign service academies. An Education Commission was established in 1760 with a specific interest in replacing Jesuitical control, but it was the papal dissolution of the order in 1773 that accomplished this. The confiscation of their property enabled the next step. Aware of the inadequacy of bureaucracy in Austria and, in order to improve it, Maria Theresa and what was now referred to as the Party of Enlightenment radically overhauled the schools system. In the new system, based on the Prussian one, all children of both genders from the ages of 6 to 12 had to attend school, while teacher training schools were established. Education reform was met with hostility from many villages and the nobility to whom children represented labour. Maria Theresa crushed the dissent by ordering the arrest of all those opposed. Although the idea had merit, the reforms were not as successful as they were expected to be; in some parts of Austria, half of the population was illiterate well into the 19th century. However widespread access to education, education in the vernacular language, replacement of rote learning and blind obedience with reasoning was to have a profound effect on the relationship between people and state.\nCivil rights, industry and labour relations.\nOther reforms were in civil rights which were defied under the \"Codex Theresianus\", begun in 1752 and finished in 1766. Specific measures included abolition of torture, and witch burning. Also in industrial and agrarian policy along cameralist lines, the theory was to maximise the resources of the land to protect the integrity of the state. Widespread problems arising from war, famine unrest and abuse made implementation of landlord-peasant reforms both reasonable and reasonable. Maria Theresa and her regime had sought a new more direct link with the populace, now that administration was no longer to be farmed out, and this maternalism combined with cameralist thinking required taking a closer interest in the welfare of the peasantry and their protection, which transpired in the 1750s. However these had been more noted than observed. In the 1770s more meaningful control of rents became practical, further eroding privilege.\nWhile reforms assisted Austria in dealing with the almost constant wars, the wars themselves hindered the implementation of those reforms.\nReligion.\nA pious Catholic, her reforms which affected the relation between state and church in favour of the former, did not extend to any relaxation of religious intolerance, but she preempted Pope Clement XIV's suppression of the Jesuits in 1773 by issuing a decree which removed them from all the institutions of the monarchy. There was both a suspicion of their excesses and of their tendency to political interference which brought them into conflict with the progressive secularisation of culture. Thus they were removed from control of censorship in 1751, and the educational reforms threatened their control over education. She was hostile to Jews and Protestants but eventually abandoned efforts for conversion, but continued her father's campaign to exile crypto-Protestants (mainly to Transylvania as in 1750). In 1744 she even ordered the expulsion of Jews, but relented under pressure by 1748. In her later years though she took some measures to protect the Jewish population.\nSuccession and co-regency.\nMaria Theresa had a large family, sixteen in all, of whom six were daughters that lived to adulthood. They were only too aware that their fate was to be used as political pawns. The best known of these was the tragic figure of Maria Antonia (1755\u20131793).\nWhen Maria Theresa's consort Francis died in 1765, he was succeeded by his son Joseph II as emperor (1765\u20131790) because of male primogeniture. Joseph was also made co-ruler or co-regent with his mother. Joseph, 24 at the time, was more ideologically attuned to modernity and frequently disagreed with his mother on policy, and was often excluded from policy making. Maria Theresa always acted with a cautious respect for the conservatism of the political and social elites and the strength of local traditions. Her cautious approach repelled Joseph, who always sought the decisive, dramatic intervention to impose the one best solution, regardless of traditions or political opposition. Joseph and his mother's quarrels were usually mediated by Chancellor Wenzel Anton von Kaunitz who served for nearly 40 years as the principal minister to both Maria Theresa and Joseph.\nJoseph frequently used his position as leverage, by threatening resignation. The one area he was allowed more say on was in foreign policy. In this, he showed similar traits to Austria's arch-enemy Frederick the Great of Prussia (1740\u20131786), also his intellectual model. He was successful in siding with Kaunitz in Realpolitik, undertaking the first partition of Poland in 1772 over his mother's principled objections. However his enthusiasm for interfering in Bavarian politics by invoking his ties to his former brother in law, Maximilian III, ended Austria in the War of Bavarian Succession in 1778. Although largely shut out of domestic policy, he used his time to acquire knowledge of his lands and people, encouraged policies he was in accord with and made magnanimous gestures such as opening the Royal Parks of Prater and Augarten to the public in 1766 and 1775 (\"Alles f\u00fcr das Volk, nichts durch das Volk\"\u2014Everything for the people, nothing by the people).\nOn her husband's death Maria Theresa was therefore no longer empress, the title of which fell to her daughter-in-law Maria Josepha of Bavaria until her death in 1767 when the title fell vacant. When Maria Theresa died in 1780 she was succeeded in all her titles by Joseph II.\nThe Habsburg-Lorraine Dynasty: Joseph II and Leopold VII (1780\u20131792).\nJoseph II (1780\u20131790): Josephinism and enlightened despotism.\nAs the first of the Habsburg-Lorraine (\"Habsburg-Lothringen\") Dynasty Joseph II was the archetypical embodiment of The Enlightenment spirit of the 18th century reforming monarchs known as the \"enlightened despots\". When his mother Maria Theresa died in 1780, Joseph became the absolute ruler over the most extensive realm of Central Europe. There was no parliament to deal with. Joseph was always positive that the rule of reason, as propounded in the Enlightenment, would produce the best possible results in the shortest time. He issued edicts\u20146,000 in all, plus 11,000 new laws designed to regulate and reorder every aspect of the empire. The spirit was benevolent and paternal. He intended to make his people happy, but strictly in accordance with his own criteria.\n\"Josephinism\" (or Josephism) as his policies were called, is notable for the very wide range of reforms designed to modernize the creaky empire in an era when France and Prussia were rapidly upgrading. Josephinism elicited grudging compliance at best, and more often vehement opposition from all sectors in every part of his empire. Failure characterized most of his projects. Joseph set about building a rational, centralized, and uniform government for his diverse lands, a pyramid with himself as supreme autocrat. He expected government servants to all be dedicated agents of Josephinism and selected them without favor for class or ethnic origins; promotion was solely by merit. To impose uniformity, he made German the compulsory language of official business throughout the Empire. The Hungarian assembly was stripped of its prerogatives, and not even called together.\nAs President of the Court Audit Office, Count Karl von Zinzendorf (1781\u20131792) introduced Appalt, a uniform system of accounting for state revenues, expenditures, and debts of the territories of the Austrian crown. Austria was more successful than France in meeting regular expenditures and in gaining credit. However, the events of Joseph II's last years also suggest that the government was financially vulnerable to the European wars that ensued after 1792. Joseph reformed the traditional legal system, abolished brutal punishments and the death penalty in most instances, and imposed the principle of complete equality of treatment for all offenders. He ended censorship of the press and theatre.\nTo equalize the incidence of taxation, Joseph ordered a fresh appraisal of the value of all properties in the empire; his goal was to impose a single and egalitarian tax on land. The goal was to modernize the relationship of dependence between the landowners and peasantry, relieve some of the tax burden on the peasantry, and increase state revenues. Joseph looked on the tax and land reforms as being interconnected and strove to implement them at the same time. The various commissions he established to formulate and carry out the reforms met resistance among the nobility, the peasantry, and some officials. Most of the reforms were abrogated shortly before or after Joseph's death in 1790; they were doomed to failure from the start because they tried to change too much in too short a time, and tried to radically alter the traditional customs and relationships that the villagers had long depended upon.\nIn the cities the new economic principles of the Enlightenment called for the destruction of the autonomous guilds, already weakened during the age of mercantilism. Joseph II's tax reforms and the institution of \"Katastralgemeinde\" (tax districts for the large estates) served this purpose, and new factory privileges ended guild rights while customs laws aimed at economic unity. The intellectual influence of the Physiocrats led to the inclusion of agriculture in these reforms.\nCivil and criminal law.\nIn 1781\u201382 he extended full legal freedom to serfs. Rentals paid by peasants were to be regulated by imperial (not local) officials and taxes were levied upon all income derived from land. The landlords saw a grave threat to their status and incomes, and eventually reversed the policy. In Hungary and Transylvania, the resistance of the landed nobility was so great that Joseph compromised with halfway measures\u2014one of the few times he backed down. After the great peasant revolt of Horea, 1784\u201385, however, the emperor imposed his will by fiat. His Imperial Patent of 1785 abolished serfdom but did not give the peasants ownership of the land or freedom from dues owed to the landowning nobles. It did give them personal freedom. Emancipation of the Hungarian peasantry promoted the growth of a new class of taxable landholders, but it did not abolish the deep-seated ills of feudalism and the exploitation of the landless squatters.\nCapital punishment was abolished in 1787, although restored in 1795. Legal reforms gained comprehensive \"Austrian\" form in the civil code (\"ABGB: Allgemeine B\u00fcrgerliche Gesetzbuch\") of 1811 and have been seen as providing a foundation for subsequent reforms extending into the 20th century. The first part of the ABGB appeared in 1786, and the criminal code in 1787. These reforms incorporated the criminological writings of Cesare Beccaria, but also first time made all people equal in the eyes of the law.\nEducation and medicine.\nTo produce a literate citizenry, elementary education was made compulsory for all boys and girls, and higher education on practical lines was offered for a select few. He created scholarships for talented poor students, and allowed the establishment of schools for Jews and other religious minorities. In 1784 he ordered that the country change its language of instruction from Latin to German, a highly controversial step in a multilingual empire.\nBy the 18th century, centralization was the trend in medicine because more and better educated doctors requesting improved facilities; cities lacked the budgets to fund local hospitals; and the monarchies wanted to end costly epidemics and quarantines. Joseph attempted to centralize medical care in Vienna through the construction of a single, large hospital, the famous Allgemeines Krankenhaus, which opened in 1784. Centralization worsened sanitation problems causing epidemics a 20% death rate in the new hospital, which undercut Joseph's plan, but the city became preeminent in the medical field in the next century.\nReligion.\nJoseph's Catholicism was that of Catholic Reform and his goals were to weaken the power of the Catholic Church and introduce a policy of religious toleration that was the most advanced of any state in Europe. In 1789 he issued a charter of religious toleration for the Jews of Galicia, a region with a large, Yiddish-speaking, traditional Jewish population. The charter abolished communal autonomy whereby the Jews controlled their internal affairs; it promoted \"Germanization\" and the wearing of non-Jewish clothing.\nProbably the most unpopular of all his reforms was his attempted modernization of the highly traditional Roman Catholic Church. Calling himself the guardian of Catholicism, Joseph II struck vigorously at papal power. He tried to make the Catholic Church in his empire the tool of the state, independent of Rome. Clergymen were deprived of the tithe and ordered to study in seminaries under government supervision, while bishops had to take a formal oath of loyalty to the crown. He financed the large increase in bishoprics, parishes, and secular clergy by extensive sales of monastic lands. As a man of the Enlightenment he ridiculed the contemplative monastic orders, which he considered unproductive, as opposed to the service orders. Accordingly, he suppressed a `` of the monasteries (over 700 were closed) and reduced the number of monks and nuns from 65,000 to 27,000. Church courts were abolished and marriage was defined as a civil contract outside the jurisdiction of the Church. Joseph sharply cut the number of holy days and reduced ornamentation in churches. He greatly simplified the manner of celebration. Critics alleged that these reforms caused a crisis of faith, reduced piety and a decline in morality, had Protestant tendencies, promoted Enlightenment rationalism and a class of liberal bourgeois officials, and led to the emergence and persistence of anti-clericalism. Many traditional Catholics were energized in opposition to the emperor.\nForeign policy.\nThe Habsburg Empire developed a policy of war and trade as well as intellectual influence across the borders. While opposing Prussia and Turkey, Austria was friendly to Russia, though tried to remove Romania from Russian influence.\nIn foreign policy, there was no Enlightenment, only hunger for more territory and a willingness to undertake unpopular wars to get the land. Joseph was a belligerent, expansionist leader, who dreamed of making his Empire the greatest of the European powers. Joseph's plan was to acquire Bavaria, if necessary in exchange for Belgium (the Austrian Netherlands). Thwarted by King Frederick II of Prussia in 1778 in the War of Bavarian Succession, he renewed his efforts again in 1785 but Prussian diplomacy proved more powerful. This failure caused Joseph to seek territorial expansion in the Balkans, where he became involved in an expensive and futile war with the Turks (1787\u20131791), which was the price to be paid for friendship with Russia.\nThe Balkan policy of both Maria Theresa and Joseph II reflected the Cameralism promoted by Prince Kaunitz, stressing consolidation of the border lands by reorganization and expansion of the military frontier. Transylvania had been incorporated into the frontier in 1761 and the frontier regiments became the backbone of the military order, with the regimental commander exercising military and civilian power. \"Populationistik\" was the prevailing theory of colonization, which measured prosperity in terms of labor. Joseph II also stressed economic development. Habsburg influence was an essential factor in Balkan development in the last half of the 18th century, especially for the Serbs and Croats.\nReaction.\nThe nobility throughout his empire disliked Joseph's taxes, egalitarianism, despotism and puritanism. In Belgium and Hungary, his attempts to subordinate everything to his own personal rule in Vienna were not well received. Even commoners were affected by Joseph's reforms, such as a ban on baking gingerbread because Joseph thought it bad for the stomach, or a ban on corsets. Only a few weeks before Joseph's death, the director of the Imperial Police reported to him: \"All classes, and even those who have the greatest respect for the sovereign, are discontented and indignant.\"\nIn Lombardy (in northern Italy) the cautious reforms of Maria Theresa in Lombardy had enjoyed support from local reformers. Joseph II, however, by creating a powerful imperial officialdom directed from Vienna, undercut the dominant position of the Milanese principate and the traditions of jurisdiction and administration. In the place of provincial autonomy he established an unlimited centralism, which reduced Lombardy politically and economically to a fringe area of the Empire. As a reaction to these radical changes the middle class reformers shifted away from cooperation to strong resistance. From this basis appeared the beginnings of the later Lombard liberalism.\nBy 1788 Joseph's health but not his determination was failing. By 1789 rebellion had broken out in protest against his reforms in Belgium (Brabant Revolution) and Hungary, and his other dominions were restive under the burdens of his war with Turkey. His empire was threatened with dissolution, and he was forced to sacrifice some of his reform projects. The emperor died on 20 February 1790 at 48, mostly unsuccessful in his attempts to curtail feudal liberties.\nBehind his numerous reforms lay a comprehensive program influenced by the doctrines of enlightened absolutism, natural law, mercantilism, and physiocracy. With a goal of establishing a uniform legal framework to replace heterogeneous traditional structures, the reforms were guided at least implicitly by the principles of freedom and equality and were based on a conception of the state's central legislative authority. Joseph's accession marks a major break since the preceding reforms under Maria Theresa had not challenged these structures, but there was no similar break at the end of the Josephinian era. The reforms initiated by Joseph II had merit despite the way they were introduced. They were continued to varying degrees under his successors. They have been seen as providing a foundation for subsequent reforms extending into the 20th century.\nUpon his death in 1790, Joseph was briefly succeeded by his younger brother Leopold VII.\nLeopold II (1790\u20131792).\nJoseph's death proved a boon for Austria, as he was succeeded by his younger brother, Leopold II, previously the more cautiously reforming Grand Duke of Tuscany. Leopold knew when to cut his losses, and soon cut deals with the revolting Netherlanders and Hungarians. He also managed to secure a peace with Turkey in 1791, and negotiated an alliance with Prussia, which had been allying with Poland to press for war on behalf of the Ottomans against Austria and Russia. While restoring relative calm to what had been a crisis situation on his accession in 1790, Austria was surrounded by potential threats. While many reforms were by necessity rescinded, other reforms were initiated including more freedom of the press and restriction on the powers of the police. He replaced his brother's police minister, Johann Anton von Pergen, with Joseph Sonnenfels an advocate of social welfare rather than control.\nLeopold's reign also saw the acceleration of the French Revolution. Although Leopold was sympathetic to the revolutionaries, he was also the brother of the French queen. Furthermore, disputes involving the status of the rights of various imperial princes in Alsace, where the revolutionary French government was attempting to remove rights guaranteed by various peace treaties, involved Leopold as Emperor in conflicts with the French. The Declaration of Pillnitz, made in late 1791 jointly with the Prussian King Frederick William II and the Elector of Saxony, in which it was declared that the other princes of Europe took an interest in what was going on in France, was intended to be a statement in support of Louis XVI that would prevent the need from taking any kind of action. However, it instead inflamed the sentiments of the revolutionaries against the Emperor. Although Leopold did his best to avoid war with the French, he died in March 1792. The French declared war on his inexperienced eldest son Francis II a month later.\nThe Arts.\nVienna and Austria dominated European music during the late 18th and early 19th centuries, typified by the First Viennese School. This was the era of Haydn, and Mozart's Vienna period extended from 1781 to 1791 during which he was court composer. Opera, particularly German opera was flourishing. Mozart wrote many German operas including the Magic Flute. Initially the pillars of the establishment\u2014the monarchy, such as Joseph II and to a lesser extent his mother, the aristocracy and the religious establishment were the major patrons of the arts, until rising middle class aspirations incorporated music into the lives of the bourgeoisie. Meanwhile, the Baroque was evolving into the less grandiose form, the Rococo.\nThe virtual abolition of censorship under van Swieten also encouraged artistic expression and the themes of artistic work often reflected enlightenment thinking.\nFrancis II: French Revolution and wars (1792\u20131815).\nFrancis II (1792\u20131835) was only 24 when he succeeded his father Leopold VII in 1792, but was to reign for nearly half a century and a radical reorganisation of European politics. He inherited a vast bureaucracy created by his uncle whose legacy of reform and welfare was to last throughout the next two centuries. The image of the monarch had profoundly changed, as had the relationship between monarch and subject. His era was overshadowed by events in France, both in terms of the evolving Revolution and the onset of a new form of European warfare with mass citizen armies. Austria recoiled in horror at the execution of Francis' aunt Maria Antonia in 1793 (despite futile attempts at rescue and even negotiation for release), leading to a wave of repression to fend off such dangerous sentiments influencing Austrian politics. At the same time Europe was consumed by the French Revolutionary (1792\u20131802) and Napoleonic Wars (1803\u20131815). The French Revolution effectively ended Austria's experiment with modernity and reform from above, and marked a retreat to legitimacy.\nDomestic policy.\nFrancis started out cautiously. The bureaucracy was still Josephist and the legal reforms under the guidance of Sonnenfels resulting in the Criminal Code of 1803 and the Civil Code of 1811. On the other hand, he restored Pergen to his position of Chief of Police. The discovery of a Jacobin plot in 1794 was a catalyst to the onset of repression. The leaders were executed or imprisoned, but there was little evidence of a tangible threat to the Habsburgs. Suppression of dissent with the Recensorship Commission of 1803 created a void in cultural and intellectual life, yet some of the world's greatest music comes from this time (see below). There were still elements of Josephism abroad, and Stadion, the foreign minister with his propagandist Friedrich von Gentz was able to appeal to popular nationalism to defeat Napoleon.\nWhat exactly such nationalism actually represented is difficult to precisely identify\u2014certainly it was directed to German culture within the Habsburg lands, but it is not clear to what degree it differentiated between 'Austrian' and 'German'. Certainly many of those such nationalism appealed to were German romantics such as Karl Wilhelm Friedrich Schlegel, such that patriotism rather than true nationalism appeared to be the goal. Cultural museums were established and citizens militia established\u2014 but in the German-speaking lands.\nJosephism remained alive and well in the other members of Francis' generation. Archduke Johann (1782\u20131859) was a nationalism advocate who was behind the \"Landwehr\" movement, and with Joseph Hormayr incited revolt in Bavarian occupied Tyrol, while Archduke Charles carried out reform of the military. A statue to Joseph was even set up in Josephsplatz in 1807 to rally the populace. In this way the Archdukes' centralism contrasted with Stadion's decentralisation and attempt to give more say to the estates. Nevertheless, such nationalism was successful in rebuilding Austria throughout its various military and political setbacks of the French wars.\nFollowing Austria's resounding defeat in 1809, Francis blamed reform, and removed the Archdukes from their position. Stadion was replaced by Metternich, who, although a reformer, placed loyalty to the monarch above all. The \"Landwehr\" was abolished, and following the discovery of yet another planned Tyrolean uprising Hormayr and Archduke Johann were interned, and Johann exiled to Styria.\nRevolutionary wars (1792\u20131802).\nWar of the First Coalition (1792\u20131797).\nFrance declared war on Austria on 20 April 1792. The increasing radicalization of the French Revolution (including the execution of the king on 21 January 1793), as well as the French occupation of the Low Countries, brought Britain, the Dutch Republic, and Spain into the war, which became known as the War of the First Coalition. This first war with France, which lasted until 1797, proved unsuccessful for Austria. After some brief successes against the utterly disorganized French armies in early 1792, the tide turned, and the French overran the Austrian Netherlands in the last months of 1792. By the Battle of Valmy in September it was evident to Austria and their Prussian allies that victory against France would elude them, and Austria suffered a further defeat in November at Jemappes, losing the Austrian Netherlands (Belgium). While the Austrians were so occupied, their erstwhile Prussian allies stabbed them in the back with the 1793 Second Partition of Poland, from which Austria was entirely excluded. This led to the dismissal of Francis's chief minister, Philipp von Cobenzl, and his replacement with Franz Maria Thugut in March 1793.\nOnce again, there were initial successes against the disorganized armies of the French Republic in 1793, and the Netherlands were recovered. But in 1794 the tide turned once more, and Austrian forces were driven out of the Netherlands again\u2014this time for good. Meanwhile, the Polish Crisis again became critical, resulting in a Third Partition (1795), in which Austria managed to secure important gains. The war in the west continued to go badly, as most of the coalition made peace, leaving Austria with only Britain and Piedmont-Sardinia as allies. In 1796, the French Directory planned a two-pronged campaign in Germany to force the Austrians to make peace, with a secondary thrust planned into Italy. French forces entered Bavaria and the edge of the Tyrol, before encountering Austrian forces under Archduke Charles, the Emperor's brother, at Amberg (24 August 1796) who was successful in driving the French back in Germany. Meanwhile, the French Army of Italy, under the command of the young Corsican General Napoleon Bonaparte, was brilliantly successful, forcing Piedmont out of the war, driving the Austrians out of Lombardy and besieging Mantua. Following the capture of Mantua in early 1797, Bonaparte advanced north through the Alps against Vienna, while new French armies moved again into Germany. Austria sued for peace. By the terms of the Treaty of Campo Formio of 1797, Austria renounced its claims to the Netherlands and Lombardy, in exchange for which it was granted the territories of the Republic of Venice with the French. The Austrians also provisionally recognized the French annexation of the Left Bank of the Rhine, and agreed in principle that the German princes of the region should be compensated with ecclesiastical lands on the other side of the Rhine.\nWar of the Second Coalition (1798\u20131801).\nThe peace did not last for long. Soon, differences emerged between the Austrians and French over the reorganization of Germany, and Austria joined Russia, Britain, and Naples in the War of the Second Coalition in 1799. Although Austro-Russian forces were initially successful in driving the French from Italy, the tide soon turned\u2014the Russians withdrew from the war after a defeat at Z\u00fcrich (1799) which they blamed on Austrian recklessness, and the Austrians were defeated by Bonaparte who was now the First Consul, at Marengo, which forced them to withdraw from Italy, and then in Germany at Hohenlinden. These defeats forced Thugut's resignation, and Austria, now led by Ludwig Cobenzl, to make peace at Lun\u00e9ville in early 1801. The terms were mild\u2014the terms of Campo Formio were largely reinstated, but now the way was clear for a reorganization of the Empire on French lines. By the Imperial Deputation Report of 1803, the Holy Roman Empire was entirely reorganized, with nearly all of the ecclesiastical territories and free cities, traditionally the parts of the Empire most friendly to the House of Austria, eliminated.\nNapoleonic wars and end of Empire (1803\u20131815).\nWith Bonaparte's assumption of the title of Emperor of the First French Empire on 18 May 1804, Francis II, seeing the writing on the wall for the old Empire, and arbitrarily took the new title of \"Emperor of Austria\" as Francis I, in addition to his title of Holy Roman Emperor. This earned him the title of Double Emperor (Francis II of the Holy Roman Empire, Francis I of Austria). The arrival of a new, French, emperor on the scene and the restructuring of the old presented a larger threat to the Habsburgs than their territorial losses to date, for there was no longer any certainty that they would continue to be elected. Francis had himself made emperor of the new Austrian Empire on 11 August not long after Napoleon. The new empire referred to not a new state but to the lands ruled by Austria, that is the Habsburgs, which was effectively many states.\nWar of the Third Coalition (1805).\nSoon, Napoleon's continuing machinations in Italy, including the annexation of Genoa and Parma, led once again to war in 1805\u2014the War of the Third Coalition, in which Austria, Britain, Russia, and Sweden took on Napoleon. The Austrian forces began the war by invading Bavaria, a key French ally in Germany, but were soon outmaneuvered and forced to surrender by Napoleon at Ulm, before the main Austro-Russian force was defeated at Austerlitz on 2 December. Napoleon entered Vienna itself, as much a celebrity as conqueror. By the Treaty of Pressburg, Austria was forced to give up large amounts of territory\u2014Dalmatia to France, Venetia to Napoleon's Kingdom of Italy, the Tyrol to Bavaria, and Austria's various Swabian territories to Baden and W\u00fcrttemberg, although Salzburg, formerly held by Francis's younger brother, the previous Grand Duke of Tuscany, was annexed by Austria as compensation.\nThe defeat meant the end of the old Holy Roman Empire. Napoleon's satellite states in southern and Western Germany seceded from the Empire in the summer of 1806, forming the Confederation of the Rhine, and a few days later Francis proclaimed the Empire dissolved, and renounced the old imperial crown on 6 August 1806.\nWar of the Fifth Coalition (1809).\nOver the next three years Austria, whose foreign policy was now directed by Philipp Stadion, attempted to maintain peace with France, avoiding the War of the Fourth Coalition (1806\u20131807) but obliged to do France's bidding. The overthrow of the Spanish Bourbons in 1808 was deeply disturbing to the Habsburgs, who rather desperately went to war once again in 1809, the War of the Fifth Coalition this time with no continental allies, but the United Kingdom. Stadion's attempts to generate popular uprisings in Germany were unsuccessful, and the Russians honoured their alliance with France, so Austria was once again defeated at the Battle of Wagram, although at greater cost than Napoleon, who had suffered his first battlefield defeat in this war, at Aspern-Essling, had expected. However Napoleon had already re-occupied Vienna. The terms of the subsequent Treaty of Sch\u00f6nbrunn were quite harsh. Austria lost Salzburg to Bavaria, some of its Polish lands to Russia, and its remaining territory on the Adriatic (including much of Carinthia and Styria) to Napoleon's Illyrian Provinces. Austria became a virtual subject state of France.\nWar of the Sixth Coalition (1812\u20131814).\nKlemens von Metternich, the new Austrian foreign minister, aimed to pursue a pro-French policy. Francis II's daughter Marie Louise, was married to Napoleon in 1810. Austria was effectively bankrupt by 1811 and the paper money lost considerable value, but contributed an army to Napoleon's invasion of Russia in March 1812. With Napoleon's disastrous defeat in Russia at the end of the year, and Prussia's defection to the Russian side in March 1813, Metternich began slowly to shift his policy. Initially he aimed to mediate a peace between France and its continental enemies, but when it became apparent that Napoleon was not interested in compromise, Austria joined the allies and declared war on France in August 1813 in the War of the Sixth Coalition (1812\u20131814). The Austrian intervention was decisive. Napoleon was defeated at Leipzig in October, and forced to withdraw into France itself. As 1814 began, the Allied forces invaded France. Initially, Metternich remained unsure as to whether he wanted Napoleon to remain on the throne, a Marie Louise regency for Napoleon's young son, or a Bourbon restoration, but he was eventually brought around by British Foreign Secretary Lord Castlereagh to the last position. Napoleon abdicated on 3 April 1814, and Louis XVIII was restored, soon negotiating a peace treaty with the victorious allies at Paris in June, while Napoleon was exiled to Elba.\nWar of the Seventh Coalition (1815).\nNapoleon escaped in February 1815, Louis fled and thus the final phase of the war, the War of the Seventh Coalition, ensued\u2014the so-called Hundred Days of Napoleon's attempt at restoration. This culminated with the decisive Battle of Waterloo in June. The Napoleonic wars ended with the second Treaty of Paris that year, and Napoleon's final exile to St Helena.\nCongress of Vienna (1815).\nWith the completion of the long running French wars a new order was required in Europe and the heads of the European states gathered in Vienna for a prolonged discussion of Europe's future, although the Congress was actually convened in September 1814 prior to Napoleon's attempted return, and completed on 9 June 1815, nine days before the Battle of Waterloo. At the completion of the Napoleonic wars, Austria found itself on the winning side as a new European leader, largely due to Metternich's diplomatic skills. It was as much a grand social event of the representatives of the great powers as a true Congress and was chaired by Metternich. Its purpose was to restore a new European order to emerge from the chaos of the Napoleonic wars. While Austria was the diplomatic leader, the military victory was largely that of Russia and Prussia, aided by Britain and Spain, and Austria had little to add to the final defeat of Napoleon on 18 June 1815 at Waterloo. Metternich's intent was to create a balance of power in Europe, under a new entity, the German Confederation with Austrian leadership, out of the ashes of the Holy Roman Empire.\nThe resulting order was referred to as the Concert of Europe, which would now meet regularly to resolve outstanding differences. In addition to redrawing the political map, it established spheres of influence. Achieving the presidency of this new entity was Austria's greatest gain from the Congress. What the Congress could not do was to recover the old order or \"ancien r\u00e9gime\" on which Austrian and Habsburg authority had rested. In the new order, a Holy Alliance was created between Austria, Russia and Prussia. This was subsequently enlarged to include most European nations. Notable exceptions were Great Britain, wary of Metternich's strategy of repressive interventionism, and the Ottoman Empire. In the redrawing of the European map, Austrian gains were modest compared to Russia and Prussia, reflecting its relatively weak negotiating position. Austria regained most of the territory it had lost to Napoleon in the western part of the nation (Tyrol, Salzburg and Vorarlberg). In Italy it gained the Kingdom of Lombardy-Venetia, Tuscany, Modena, Parma and Piacenza. These latter political entities were ruled by various branches of the Habsburgs. It did not, however, regain Belgium and the Austrian Netherlands, but the trading of territories restored a contiguous territory, as envisaged by Joseph II, and gave Austria control over Italy. The Emperor Francis was unwilling to adapt to this new order, requiring diplomacy on Metternich's part, depicting it as conservative Romanticism, religion and order versus the revolutionary spirit of 1789. Thus, the Holy Alliance became a mechanism for countering any moves against legitimate order. Metternich has therefore been portrayed Europe's fireman, extinguishing any signs of revolutionary spirit. The resultant onset of peace provided the opportunity for both reforms and prosperity in Austria, but its backward looking policies within a Europe characterised by rapid change set the scene for eventual failure.\nThe arts.\nNapoleonic Vienna was the Vienna of Beethoven, whose single opera Fidelio was premiered there in 1805, attended by the French military. It was also the era of the third (Eroica) (1805) with its ambivalent relation to Napoleon, and the fifth (Schicksals-) and the sixth (Pastorale) symphonies (1808).\nThe 19th century (1815\u20131914).\nBiedermaier period (1815\u20131848).\nThe period following the Congress of Vienna was one of relative political stability and is better known for its culture (Biedermeier). Other names for the years 1815\u20131848 include \"Vorm\u00e4rz\" (\"before March\") referring to period before the revolution of March 1848 and \"The Age of Metternich\" referring to his dominant position in European politics. Under the control of Metternich (who became Chancellor in 1821), the Austrian Empire entered a period of censorship and a police state, with surveillance spying and imprisonment of the opposition, while others emigrated. For Europe it was a period characterised by increasing industrialisation, the social consequences of economic cycles, population mobility and nationalism. All of these were regarded warily by governments intent on preserving the existing order.\nIn 1823, the Emperor of Austria made the five Rothschild brothers barons. Nathan Mayer Rothschild in London chose not to take up the title. The family became famous as bankers in the major countries of Europe.\nForeign policy.\nThe Congress of Vienna was followed by a series of other congresses, referred to as the Congress System, including the Congresses of Aix-la-Chapelle (1818), Troppau (1820), Laibach (1821) and Verona (1822). These largely served to further suppress change, though a notable exception was the reintegration of France at the first of these, resulting in the Quintuple Alliance of the five great powers (Britain, Austria, Prussia, Russia and then France). Within the Quintuple Alliance, the more conservative Holy Alliance of Russia, Austria and Prussia tended to dominate the discourse. The Congress System of dispute resolution did not survive the 1820s, and in particular the intervention in the War of Greek Independence (1821-1832) over Metternich's objections, revealed the limits of his power. Further cracks in the old order appeared as South America broke away from Spain and Portugal, a liberal regime appeared in Portugal, and French Revolution and the independence of Belgium in 1830. Throughout there had been a division within the Quintuple Alliance between the more liberal western nations of Britain and France and the more conservative Holy Alliance in the east. The London conference of 1830 to decide on Belgian independence from the Netherlands effectively led to Britain's withdrawal from the Alliance. 1840 saw a number of related events that reflected on the unity of the Great Powers. Austria relied on Russia over the Eastern Question, and the Oriental crisis of that year split those powers, at least temporarily. While in the west, the Rhine crisis between France and the German Confederation, further fuelled the nascent German nationalism movement, typified by \"Die Wacht am Rhein\"\nCloser to home, Metternich exerted his influence in suppressing German nationalism. The Wartburg Festival of 1817 with its calls for German unity and condemnation of conservatism created alarm. Metternich prevented Austrian universities participating and further activism in 1819 with the assassination of August von Kotzebue in 1819 resulted in the Carlsbad Decrees of that year, suppressing free speech, and the Vienna Final Act of 1820 empowering the German Confederation to act against member states. Neither fully succeeded in delaying the nationalist movement for long.\nDomestic policy and the rise of nationalism.\nEarly efforts among the Italians to create a unified nation, including the Carbonari in Lombardy-Venetia were put down by Metternich with military intervention, similarly in Poland. Meanwhile, the minority populations within the empire, such as the Slavs and Poles were seeking national identities, distinct from Austria. Metternich sought to deflect these movements into cultural identity. Similarly in Germany, a Prussian led German Customs Union of the German states was formed in 1833, but Austria did not join, identifying within it German nationalism. One of the few administrative reforms was the granting of a diet to Galicia.\nMetternich kept a firm hand on government resisting the constitutional freedoms demanded by the liberals. Government was by custom and by imperial decree. He was both an oppressive reactionary opportunist but also a true conservative politician. His role in directing European affairs gave the Habsburgs a disproportionate influence relative to their actual powers. The extensive security mechanism was headed by Count Joseph Sedlnitzky, under Metternich's personal supervision, while religion was seen merely as a tool for supporting authority.\nEconomy.\nState intervention in fiscal matters was relatively restrained, although the National Bank of Austria was established in 1816 to restore the nation's credit status. Taxation was largely left to the provinces and uneven within the empire, Hungary paying disproportionally less. The aristocrats were also undertaxed. One of the results was that the military budget was relatively small, and thus unable to give much force to Metternich's foreign policies. Economic growth was relatively small and did not keep pace with population growth. Industrialisation in Austria began around 1830, primarily in Vienna and Vorarlberg. 1838 saw the first railway, connecting Vienna and Deutsch-Wagram, a distance of about 15\u00a0km, and construction on the Austrian Southern Railway (\"\u00d6sterreichische S\u00fcdbahn\") started the following year. In shipping, the Danube Steam Navigation Company was established in 1829, while Austrian Lloyd became the largest shipping company in the Mediterranean. These economic developments came at a cost as large numbers of farm workers migrated to the growing urban industries to form an expanding proletariat.\nMonarchy.\nFrancis firmly resisted Metternich's proposals for overhauling the Monarchy. When Francis died in 1835, his son Ferdinand I (1835-1848) \"Ferdinand the Benign\" succeeded him, but proved unfit to govern due to illness, with much of the decision making falling to his uncle Archduke Louis of Austria and Metternich. Consequently, Austria entered a period of political stagnation with Francis being unwilling to make reforms and Ferdinand being incapable of so doing, and Metternich committed to preserving the status quo.\n1848 Revolution.\nDuring this period both liberalism and nationalism were on the rise, which resulted in the Revolutions of 1848. Metternich and the mentally handicapped Emperor Ferdinand I were forced to resign to be replaced by the emperor's young nephew Franz Joseph.\nFranz Joseph I and the Belle \u00c9poque (1848\u20131914).\nPost-revolutionary Austria (1848\u20131866).\nSeparatist tendencies (especially in Lombardy and Hungary) were suppressed by military force. A constitution was enacted in March 1848, but it had little practical impact, although elections were held in June. The 1850s saw a return to neoabsolutism and abrogation of constitutionalism. That said, one of the concessions to revolutionaries with a lasting impact was the freeing of peasants in Austria. This facilitated industrialization, as many flocked to the newly industrializing cities of the Austrian domain (in the industrial centers of Bohemia, Lower Austria, Vienna, and Upper Styria). Social upheaval led to increased strife in ethnically mixed cities, leading to mass nationalist movements.\nOn the foreign policy front, Austria with its non-German constituencies, was faced with a dilemma in 1848 when Germany's Constituent National Assembly, of which Austria was a member, stated that members could not have a state connection with non-German states, leaving Austria to decide between Germany or its Empire and Hungarian union. These plans came to nothing for the time being, but the concept of a smaller Germany that excluded Austria was to re-emerge as the solution in 1866. Austria's neutrality during the Crimean War (1853\u20131856), while the emperor was preoccupied with his wedding, antagonized both sides and left Austria dangerously isolated, as subsequent events proved.\nThe Italian question (1859\u20131860).\nWhile Austria and the Habsburgs held hegemony over northern Italy, the south was the Kingdom of the Two Sicilies, with the Papal States intervening. Italy had been in a turmoil since the Congress of Vienna in 1815, with insurrections starting in 1820. King Ferdinand II of the Two Sicilies, an absolutist monarch, sought to strengthen his position by a further dynastic alliance with Austria. He already had a connection through his second wife, Maria Theresa, granddaughter of the emperor Leopold II. This he achieved by marrying his son, Francis II, to Duchess Maria Sophie of Bavaria in February 1859. Marie was a younger sister of the Empress Elisabeth of Austria, making Francis brother in law to the Emperor. Ferdinand died a few months later in May, and Francis and Maria Sophie ascended the throne.\nIn the meantime Austria had fallen into a trap set by the Italian \"risorgimento\". Piedmont, jointly ruled with Sardinia had been the site of earlier insurrections. This time they formed a secret alliance with France (\"Patto di Plombi\u00e8res\") whose emperor Napoleon III was a previous \"Carbonari\". Piedmont then proceeded to provoke Vienna with a series of military manoeuvres, successfully triggering an ultimatum to Turin on 23 April. Its rejection was followed by an Austrian invasion, and precipitated war with France (Second Italian War of Independence 1859). Austria mistakenly expected support and received none, and the country was ill-prepared for war, which went badly. The Habsburg rulers in Tuscany and Modena were forced to flee to Vienna.\nIn May 1859 Austria suffered a military defeat at the Battle of Varese and in June at Magenta against the combined forces of France and Sardinia. The emperor refused to acknowledge the seriousness of the situation which was causing great hardship at home, and took over direct command of the army, though not a professional soldier. Later that month a further defeat at Solf\u00e9rino sealed Austria's fate, and the emperor found himself having to accept Napoleon's terms at Villafranca. Austria agreed to cede Lombardy, and the rulers of the central Italian states were to be restored. The latter never happened, and the following year in plebiscites, all joined the Kingdom of Sardinia-Piedmont. By April 1860 Garibaldi had invaded and quickly subdued Sicily, and by February 1861 the Kingdom of the Two Sicilies ceased to exist; Francis and Maria fled to Austria.\nAftermath\u2014constitutional concessions.\nThese events severely weakened the emperor's position. The government's absolutist policies were unpopular and these setbacks led to domestic unrest, Hungarian secessionism, criticism of Austria's governance and allegations of corruption. The first casualties were the emperor's ministers. The Finance Minister, Karl Ludwig von Bruck killed himself. Other casualties were Count Karl Ferdinand von Buol (Foreign Minister), Interior Minister Baron Alexander von Bach, Police Minister Johann Freiherr von Kempen von Fichtenstamm, Adjutant General Karl Ludwig von Gr\u00fcnne, together with army generals.\nThe result was a reluctant undertaking by the emperor and his chief advisor Goluchowski to return to constitutional government, culminating in the October Diploma (October 1860) establishing constitutional monarchy through a legislative assembly and provincial autonomy. This was never completely implemented due to Hungarian resistance, demanding the full autonomy lost in 1849. Consequently, the October Diploma was replaced by the February Patent, in 1861 establishing a bicameral legislative body, the \"Reichsrat\". The upper house consisted of appointed and hereditary positions, while the lower house, the House of Deputies was appointed by the provincial diets. The \"Reichsrat\" would meet with or without the Hungarians, depending on the issues being considered. This was a first step towards the establishment of a separate Cisleithanian legislature, on the other hand the more limited role of the diets in the February Patent, compared to the October Diploma, angered the champions of regionalism. The \"Reichsrat\" was dominated by liberals, who were to be the dominant political force for the next two decades.\nThe Danish question (1864\u20131866).\nPrussia and Denmark had already fought one war in 1848\u20131851 over the territories that lined their common border, Schleswig-Holstein which resulted in Denmark retaining them. By 1864 Austria was at war again, this time allying itself with Prussia against Denmark in the Second Schleswig War, which although successful this time, turned out to be Austria's last military victory. The war concluded with the Treaty of Vienna by which Denmark ceded the territories. The following year the Gastein Convention resolved the control of the new territories, Holstein being allocated to Austria, after initial conflicts between the allies. This did little to ease the Austria\u2013Prussia rivalry over the German question. The ongoing efforts by Otto von Bismarck, the Prussian Minister President, to revoke the agreement and wrest control of the territories would soon lead to all out conflict between the two powers and achieve the desired weakening of Austria's position in central Europe.\nThe Hungarian question.\nFrom the 1848 revolution, in which much of the Hungarian aristocracy had participated, Hungary remained restless, restoration of the constitution and de-throne the House of Habsburg, opposing the centralist trials of Vienna and refusing to pay taxes. Hungary had little support in the court at Vienna which was strongly Bohemian and considered the Hungarians as revolutionaries. From the loss of the Italian territories in 1859, the Hungarian question became more prominent. Hungary was negotiating with foreign powers to support it, and most significantly with Prussia. Therefore, Hungary represented a threat to Austria in any opposition to Prussia within the German Confederation over the German Question. Therefore, cautious discussions over concessions, referred to as \"Conciliation\" by the Hungarians, started to take place. Emperor Franz Joseph traveled to Budapest in June 1865 and made a few concessions, such as abolishing the military jurisdiction, and granting an amnesty to the press. This fell far short of the requests of the Hungarian liberals whose minimal demands were restoration of the constitution and the emperor's separate coronation as King of Hungary. Chief among these were Gyula Andr\u00e1ssy and Ferenc De\u00e1k, who endeavoured to improve their influence at the court in Vienna. In January 1866 a delegation of the Hungarian parliament traveled to Vienna to invite the imperial family to make an official visit to Hungary, which they did, at some length from January to March.\nAustro-Prussian War (1866).\nWhile Andr\u00e1ssy was making frequent visits to Vienna from Budapest during early 1866, relations with Prussia were deteriorating. There was talk of war. Prussia had signed a secret treaty with the relatively new Kingdom of Italy on 8 April, while Austria concluded one with France on 12 June, in exchange for Venetia.\nWhile the motives for the war, Prussian masterplan or opportunism, are disputed, the outcome was a radical re-alignment of power in Central Europe. Austria brought the continuing dispute over Holstein before the German diet and also decided to convene the Holstein diet. Prussia, declaring that the Gastein Convention had thereby been nullified, invaded Holstein. When the German diet responded by voting for a partial mobilization against Prussia, Bismarck declared that the German Confederation was ended. Thus this may be considered a Third Schleswig War.\nHostilities broke out on 14 June as the Austro-Prussian War (June\u2013August 1866), in which Prussia and the north German states faced not only Austria but much of the rest of Germany, especially the southern states. Three days later Italy declared war on Austria in the Third Italian War of Independence, Italy now being Prussia's ally. Thus Austria had to fight on two fronts. Their first engagement resulted in a minor victory against the Italians at Custoza near Verona on 24 June. Yet, on the northern front Austria suffered a major military defeat at the Battle of K\u00f6niggr\u00e4tz in Bohemia on 3 July. Although Austria had a further victory against the Italians in a naval battle at Lissa on 20 July, it was clear by then that the war was over for Austria, Prussian armies threatening Vienna itself, forcing the evacuation of the court to Budapest. Napoleon III intervened resulting in an armistice at Nikolsburg on 21 July, and a peace treaty in Prague on 23 August. In the meantime the Italians who collected a series of successes throughout July, signed an armistice at Cormons on 12 August rather than face the remaining Austrian army freed from its northern front.\nAs a result of these wars Austria had now lost all its Italian territory and was now excluded from further German affairs, that were now reorganised under Prussian dominance in the new North German Confederation. The \"Kleindeutschland\" concept had prevailed. For the Austrians in Italy, the war had been tragically pointless, since Venetia had already been ceded.\nDual Monarchy (1867\u20131918).\nConciliation.\nWhile Austria was reeling from the effects of war, the Hungarians increased the pressure for their demands. Andr\u00e1ssy was regularly in Vienna, as was Ferenc De\u00e1k and the Hungarian position was backed by constitutionalists and liberals. While anti-Hungarian sentiments ran high at the court, the Emperor's position was becoming increasingly untenable, with the Prussian army now at Pressburg (now Bratislava), and Vienna crammed with exiles, while hope for French intervention proved to be fruitless. The Hungarians recruited Empress Elisabeth who became a strong advocate for their cause. Gy\u00f6rgy Klapka had organised a legion fighting for the Prussians, which Bismarck had supported, that entered Hungary and agitated for Hungarian independence.\nHowever the needs of the other provinces had to be considered before entering into any form of Hungarian dualism which would give Hungary special privileges, and started to fan the flames of Czech nationalism, since Slavic interests were likely to be submerged. People started to talk about the events of 1848 again. By February 1867 Count Belcredi resigned as Minister President over his concerns about Slavic interests, and was succeeded by foreign minister Ferdinand Beust, who promptly pursued the Hungarian option which had become a reality by the end of the month.\n\"Ausgleich\" (Compromise) 1867.\nAustria-Hungary was created through the mechanism of the Austro-Hungarian Compromise of 1867. Thus the Hungarians finally achieved much of their aims. The western half of the realm known as (Cisleithania) and the eastern Hungarian (Transleithania), that is the realms lying on each side of the Leitha tributary of the Danube river, now became two realms with different interior policy - there was no common citizenship and dual-citizenship was banned either -, but with a common ruler and a common foreign and military policy. The empire now had two capitals, two cabinets and two parliaments. Only three cabinet positions served both halves of the monarchy, war, foreign affairs and finance (when both sectors were involved). Costs were assigned 70:30 to Cisleithania, however the Hungarians represented a single nationality while Cisleithania included all the other kingdoms and provinces. Andr\u00e1ssy was appointed as the first Minister President of the new Hungary on 17 February. Feelings ran high in the provinces, and the Diets in Moravia and Bohemia were shut down in March.\nEmperor Franz Joseph made a speech from the throne in May to the \"Reichsrat\" (Imperial Council) asking for retroactive ratification and promising further constitutional reforms and increased autonomy to the provinces. This was a major retreat from absolutism. On 8 June, the Emperor and Empress were crowned King and Queen of Hungary in a ceremony whose pomp and splendour seemed out of keeping with Austria's recent military and political humiliation and the extent of financial reparations. As part of the celebrations the emperor announced further concessions that aggravated relationships between Hungary and the rest of the monarchy. An amnesty was declared for all political offences since 1848 (including Klapka and Kossuth) and reversal of the confiscation of estates. In addition the coronation Gift was directed to the families and veterans of the revolutionary \"Honv\u00e9d\"s, which was revived as the Royal Hungarian Honv\u00e9d.\nIn return for the Liberals support of the \"Ausgleich\", concessions were made to parliamentary prerogatives in the new constitutional law. The law of 21 December 1867, although frequently amended, was the foundation of Austrian governance for the remaining 50 years of the empire, and was largely based on the February Patent, the Imperial Council and included a bill of rights. Ultimately the political balance of the dual monarchy represented a compromise between authoritarianism and parliamentarianism \"(Rechtsstaat)\" (Hacohen 2002). Like most compromises it was rejected by extremists on both sides, including Kossuth.\nAustria-Hungary, 1867\u20131914.\n1873 marked the Silver Jubilee of Franz Joseph, and provided not only an occasion for celebration but also one of reflection on the progress of the monarchy since 1848. Vienna had grown from a population of 500,000 to over a million, the walls and fortifications had been demolished and the \"Ringstrasse\" constructed with many magnificent new buildings along it. The Danube was being regulated to reduce the risk of flooding, a new aqueduct constructed to bring fresh water into the city, and many new bridges, schools, hospitals, churches and a new university built.\nForeign policy\nWhat was supposed to be a temporary emergency measure was to last for half a century. Austria succeeded in staying neutral during the Franco Prussian War of 1870\u201371 despite those who saw an opportunity for revenge on Prussia for the events of 1866. However Austria's allies among the South German States were now allied with Prussia, and it was unlikely that Austria's military capacity had significantly improved in the meantime. Any residual doubts were rapidly dispelled by the speed of the Prussian advance and the subsequent overthrow of the Second Empire.\nIn November 1871 Austria made a radical change in foreign policy. Ferdinand Beust, the First Prime Minister (to 1867), Chancellor and Foreign Minister (1866\u20131871) of the Dual Monarchy, was dismissed. Beust was an advocate of \"revanche\" against Prussia, but was succeeded by the Hungarian Prime Minister, the liberal Gyula Andr\u00e1ssy as Foreign Minister (1871\u20131879), although both opposed the federalist policies of Prime Minister Karl Hohenwart (1871) while Prince Adolf of Auersperg became the new Prime Minister (1871\u20131879). Andr\u00e1ssy 's appointment caused concern among the conservative Court Party, but he worked hard to restore relationships between Berlin and Vienna, culminating in the Dual Alliance of 1879.\nIn 1878, Austria-Hungary occupied Bosnia and Herzegovina, which had been cut off from the rest of the Ottoman Empire by the creation of new states in the Balkans following the Russo-Turkish War of 1877\u201378 and the resulting Congress of Berlin (June\u2013July 1878). The territory was ceded to Austria-Hungary, and Andr\u00e1ssy prepared to occupy it. This led to a further deterioration of relations with Russia and was to lead to tragic consequences in the next century. Austrian troops encountered stiff resistance and suffered significant casualties. The occupation created controversy both within and without the empire and led to Andr\u00e1ssy's resignation in 1879. This territory was finally annexed in 1908 and put under joint rule by the governments of both Austria and Hungary.\nThe departure of the Liberal Government and of Andr\u00e1ssy from the Foreign Office (\"k. u. k. Ministerium des \u00c4u\u00dfern\") marked a sharp shift in Austria-Hungary's foreign policy, particularly in relation to Russia, Count Gustav K\u00e1lnoky (1881\u20131895) Andr\u00e1ssy's Conservative replacement pursuing a new rapprochement.\nEconomy\nThe second half of the 19th century saw a lot of construction, expansion of cities and railway lines, and development of industry. During the earlier part of this period, known as \"Gr\u00fcnderzeit\", Austria became an industrialized country, even though the Alpine regions remained characterized by agriculture. Austria was able to celebrate its newfound grandeur in the Vienna World Exhibition of 1873, attended by all the crowned heads of Europe, and beyond. This period of relative prosperity was followed by the 1873 Stock market crash.\nPolitics and governance.\nLiberalism in Cisleithania 1867\u20131879.\nPolitical parties became legitimate entities in Austria from 1848, apart from a brief lapse in the 1850s. However the structure of the legislative body created by the 1861 February Patent provided little scope for party organisation. Initial political organisation resembled the cleavages in Austrian culture. Since the time of the Counter-Reformation the Catholic Church had assumed a major role in the political life of the empire, in conjunction with the aristocracy and conservative rural elements. Allied against these forces were a more secular urban middle class, reflecting the Enlightenment and the French Revolution with its anti-clericism \"(Kulturkampf)\". Other elements on the left were German nationalism, defending Greater German interests against the Slavs, and found support among urban intelligentsia. However party structure was far from cohesive and both groupings contained factions which either supported or opposed the government of the day. These parties reflected the traditional right/left split of political vision. The left, or Liberal factions were known as the Constitutional Party, but both left and right were fragmented into factions. Without direct elections there was no place for constituency organisation, and affinities were intellectual not organisational. Nor, without ministerial responsibility, was there a need for such organisation. The affinities were driven by respective visions of the representative institutions. The left derived its name from its support in principle of the 1861\u20131867 constitution and were the driving elements of the 1848 revolution, the right supported historic rights. The left drew its support from the propertied bourgeoisie (\"Besitzb\u00fcrgertum\"), affluent professionals and the civil service. These were longstanding ideological differences. The 1867 elections saw the Liberals take control of the lower house under Karl Auersperg (1867\u20131868) and were instrumental in the adoption of the 1867 constitution and in abrogating the 1855 Concordat (1870).\nSuffrage progressively improved during the period 1860\u20131882. The selection of deputies to the \"Reichsrat\" by provincial legislatures proved unworkable particularly once the Bohemian diet effectively boycotted the Reichsrat in an attempt to acquire equal status with the Hungarians in a tripartite monarchy. As a result, suffrage was changed to direct election to the \"Reichsrat\" in 1873.\nEven then by 1873 only six percent of the adult male population were franchised (Hacohen 2002). The initial divisions into Catholic, liberal, national, radical and agrarian parties differed across ethnic grounds further fragmenting the political culture. However, there was now emerging the presence of extra-parliamentary parties whereas previously parties were purely intra-parliamentary. This provided an opportunity for the disenfranchised to find a voice. These changes were taking place against a rapidly changing backdrop of an Austrian economy that was modernising and industrialising and economic crises such as that of 1873 and its resultant depression (1873\u20131879), and the traditional parties were slow to respond to the demands of the populace. By the election of 1901, the last election under the defined classes of franchisement extraparliamentary parties won 76 of the 118 seats.\nThis era saw anti-liberal sentiments and declining fortunes of the Liberal party which had held power since 1867 apart from a brief spell of conservative government in 1870\u201371. In 1870 Liberal support for Prussia in the 1870 Franco-Prussian War displeased the Emperor and he turned to the Conservatives to form a government under Count Karl Sigmund von Hohenwart (1871). Hohenwart was the conservative leader in parliament, and the Emperor believed his more sympathetic views to Slavic aspirations and federalism would weaken the Austro-German Liberals. Hohenwart appointed Albert Sch\u00e4ffle as his commerce minister and drew up a policy known as the Fundamental Articles of 1871. The policy failed, the Emperor withdrew his support and the Liberals regained power.\nThe Liberal party became progressively unliberal and more nationalistic, and against whose social conservatism the progressive intellectuals would rebel (Hacohen 2002). During their 1870\u201371 opposition they blocked attempts to extend the dual monarchy to a tripartite monarchy including the Czechs, and promoted the concept of \"Deutschtum\" (the granting of all rights of citizenship to those who displayed the characteristics of the solid German \"B\u00fcrger\"). They also opposed the extension of suffrage because restricted suffrage favoured their electoral base (Hacohen 2002). In 1873 the party fragmented, with a radical faction of the Constitutional Party forming the Progressive Club, while a right-wing faction formed the conservative Constitutionalist Landlordism leaving a rump of 'Old Liberals'. The result was a proliferation of German Liberal and German National groups.\nPolitical realignment 1879.\nWhile Liberal achievements had included economic modernisation, expanding secular education and rebuilding the fabric and culture of Vienna, while collaborating with the Administration \"(Verwaltung)\", after 1873 a progressive series of schisms and mergers continued to weaken the party which effectively disappeared by 1911.\nThe Liberal cabinet of Adolf Auersperg (1871\u20131879) was dismissed in 1879 over its opposition to Foreign Minister Gyula Andr\u00e1ssy's (1871\u20131879) Balkan policy and the occupation of Bosnia-Herzegovina, which added more Slavs and further diluted German nationalism and identity. In the ensuing elections the Liberals lost control of parliament and went into opposition, the incoming government under Count Edward Taaffe (1879\u20131893) basically consisting of a group of factions (farmers, clergy and Czechs), the \"Iron Ring\", united in a determination to keep the Liberals out of power.\nAndr\u00e1ssy, who had nothing in common with Taaffe, tended his resignation on the grounds of poor health and to his surprise it was accepted. His name was raised again when the new Foreign minister, Haymerle died in office in 1881, but Taaffe and his coalition had no time for a Liberal foreign minister (let alone a Hungarian and Freemason), and he was passed over in favour of Count Gustav K\u00e1lnoky (1881\u20131895).\nHowever the Liberal opposition filibustered leading the government to seek electoral reform as a strategy to weaken their position, which was enacted in 1882. Despite this, the coalition, nominally conservative and committed to anti-socialism passed a series of social reforms over the decade 1880\u20131890, following the examples of Germany and Switzerland. These were reforms which the Liberals had been unable to get past a government strongly tied to the concept of individual's rights to self-determination free from government interference Such measures had the support of both the Liberals, now the United Left (\"Vereinigte Linke\" 1881) and the German National Party (\"Deutsche Nationalpartei\" 1891), an offshoot of the German National Movement. The electoral reforms of 1882 were the most influential in that it enfranchised proportionally more Germans.\nSocial reform now moved to become a platform of conservative Catholics like Prince Aloys de Paula Maria of Liechtenstein, Baron Karl von Vogelsang, and Count Egbert Belcredi The era of electoral reform saw the emergence of Georg von Schonerer's Pan-German League (1882), appealing to an anti-clerical middle class, and Catholic social reformers such as L. Psenner and A. Latschka created the Christian Social Association (\"Christlich-Sozialer Verein\") (1887). Around the same time F. Piffl, F. Stauracz, Ae. Schoepfer, A. Opitz, Karl Lueger and Prince Aloys Liechtenstein formed the United Christians to advocate Christian social reform. These two organisations merged in 1891 under Karl Lueger to form the Christian Social Party (\"Christlichsoziale Partei\", CS).\nHowever the Taaffe government's policy of ethnic inclusiveness fuelled nationalism among the German-speaking population. The Liberals had maintained the strong centralism of the absolutist era (with the exception of Galicia in 1867) while the Conservatives attempted a more federalist state that ultimately led to the fall of the Taaffe government in 1893, including a second attempt at Bohemian \"Ausgleich\" (Tripartite monarchy) in 1890\nOn the left the spread of anarchical ideas and oppressive government saw the emergence of a Marxist Social Democratic Party (\"Sozialdemokratische Arbeiterpartei \u00d6sterreichs\", SDAP\u00d6) in 1889 which succeeded in winning seats in the 1897 elections which followed further extension of suffrage in 1896 to include peasants and the working classes, establishing universal male suffrage, though not equal.\nDirect and equal suffrage for the Reichsrat (1907).\nThe universal male suffrage introduced in 1907 by Minister-President Freiherr von Beck changed the balance of power, formally tilted towards German Austrians, and revealed that they were now a minority in a predominantly Slavic empire. In the 1900 census, Germans were 36% of the Cisleithanian population but the largest single group, but never acted as a cohesive group (nor did any other national group), although they were the dominant group in the political life of the monarchy. Germans were followed by Czechs and Slovaks (23%), Poles (17), Ruthenians (13), Slovenes (5), Serbo-Croats (3), Italians (3) and Romanians 1%. However these national groups, especially the Germans were often scattered geographically. The Germans also dominated economically, and in level of education. The post reform 1907 parliament (\"Reichsrat\") was elected along national lines, with only the Christian-Social and Social Democrat parties predominantly German. However Austria was governed by the Emperor who appointed the Imperial Council of Ministers, who in turn answered to him, parliament being left free to criticise government policy. Technically it had the power to legislate from 1907, but in practice the Imperial government generated its own legislation, and the Emperor could veto his own minister's bills. The major parties were divided geographically and socially, with the social democrats base being the towns, predominantly Vienna, and having a very different perspective to the devout but illiterate peasantry in the countryside. The latter were joined by the aristocracy and bourgeoisie in supporting the \"status quo\" of the monarchy.\nThe 1911 elections elected a parliament that would carry Austria through the war and the end of the empire in 1918.\nHowever, the effectiveness of parliamentarism was hampered by conflicts between parties representing different ethnic groups, and meetings of the parliament ceased altogether during World War I.\nThe arts.\nThe initial years of the 19th century following the Congress of Vienna, up until the revolution of 1848 was characterised by the Biedermeier period of design and architecture, partly fueled by the repressive domestic scene that diverted attention to domesticity and the arts.\nWith the reign of Franz Joseph (1848\u20131916) came a new era of grandeur, typified by the Belle \u00c9poque style, with extensive building and the construction of the Ringstrasse in Vienna with its monumental buildings (officially opened 1 May 1865, after seven years). Architects of the period included Heinrich Ferstel (Votivkirche, Museum f\u00fcr angewandte Kunst Wien), Friedrich von Schmidt (Rathaus), Theophil Hansen (Parliament), Gottfried Semper (Kunsthistorisches Museum, Kunsthistorisches Museum, Burgtheater), Eduard van der N\u00fcll (Opera) and August Sicardsburg (Opera).\n1897 saw the resignation of a group of artists from the Association of Austrian Artists (\"Gesellschaft bildender K\u00fcnstler \u00d6sterreichs\"), headed by Gustav Klimt who became the first president of this group which became known as the Vienna Secession or Wiener Secession (\"Vereinigung Bildender K\u00fcnstler \u00d6sterreichs\"). The movement was a protest against the historicism and conservatism of the former organisation, following similar movements in Berlin and Munich. Partly this was a revolt against the perceived excesses of the earlier \"Ringstrasse\" era, and a yearning to return to the relative simplicity of Biedermaier. From this group Josef Hoffman and Koloman Moser formed the Vienna Arts and Crafts Workshop (\"Wiener Werkst\u00e4tte\") in 1903 to promote the development of applied arts. The Secession became associated with a specific building, the Secession Building (\"Wiener Secessionsgeb\u00e4ude\") built in 1897 and which housed their exhibitions, starting in 1898. The Secession as originally conceived splintered in 1905 when Klimt and others left over irreconcilable differences. The group however lasted until 1939 and the outbreak of the Second World War.\nArchitecturally this was the era of Jugendstil (Art Nouveau) and the contrasting work of men like Otto Wagner (Kirche am Steinhof) known for embellishment and Adolf Loos, who represented restraint. Art Nouveau and the modern style came relatively late to Austria, around 1900, and was distinguishable from the earlier movement in other European capitals.\nOne of the prominent literary figures was Karl Kraus, the essayist and satirist, known for his newspaper \"The Torch\", founded in 1899.\nOn the musical scene, Johan Strauss and his family dominated the Viennese scene over the entire period, which also produced Franz Schubert, Ludwig van Beethoven, Anton Bruckner, Johannes Brahms, Arnold Schoenberg, Franz Leh\u00e1r and Gustav Mahler among others.\nBy the opening years of the 20th century (Fin de si\u00e8cle) the avant garde were beginning to challenge traditional values, often shocking Viennese society, such as Arthur Schnitzler's play \"Reigen\", the paintings of Klimt, and the music of Schoenberg, Anton Webern and Alban Berg and the Second Viennese School.\nAustria in the First World War (1914\u20131918).\nNationalist strife increased during the decades until 1914. The assassination in Sarajevo by a Serb nationalist group of Archduke Franz Ferdinand, the heir to Franz Joseph as Emperor, helped to trigger World War I. In November 1916 the Emperor died, leaving the relatively inexperienced Charles (Karl) in command. The defeat of the Central Powers in 1918 resulted in the disintegration of Austria-Hungary, and the Emperor went into exile.\nGerman Austria and the First Republic (1918\u20131933).\nRepublic of German-Austria (1918\u20131919).\n1918.\nThe First World War ended for Austria on 3 November 1918, when its defeated army signed the Armistice of Villa Giusti at Padua following the Battle of Vittorio Veneto. This applied just to Austria-Hungary, but Hungary had withdrawn from the conflict on 31 October 1918. Austria was forced to cede all territory occupied since 1914, plus accept the formation of new nations across most of its pre-war territory, and the allies were given access to Austria. The empire was thus dissolved.\nThe Provisional National Assembly met in Vienna from 21 October 1918 to 19 February 1919, as the first parliament of the new Austria, in the Lower Austria parliamentary buildings. It consisted of those members of the \"Imperial Council\" elected in 1911 from German speaking territories with three presidents, Franz Dinghofer, Jodok Fink, and Karl Seitz. The National Assembly continued its work till 16 February 1919 when elections were held. On 30 October it adopted a provisional constitution and on 12 November it adopted German Austria as the name of the new state. Since the Emperor, Charles I (Karl I) had stated on 11 November that he no longer had \"any share in the affairs of state\", although he always said that he never abdicated, Austria was now a republic.\nHowever the provisional constitution stated that it was to be part of the new German (Weimar) Republic proclaimed three days earlier. Article 2 stated: \"German Austria is part of the German Republic\".\nKarl Renner was proclaimed Chancellor of Austria, succeeding Heinrich Lammasch and led the first three cabinets (12 November 1918\u00a0\u2013 7 July 1920) as a grand coalition of the largest parties, although some were composed of a large number of splinter groups of the German National and German Liberal movements, and were numerically the largest group in the assembly.\nOn 22 November Austria claimed the German-speaking territories of the former Habsburg Empire in Czechoslovakia (German Bohemia and parts of Moravia), Poland (Austrian Silesia) and the South Tyrol, annexed by Italy. However Austria was in no position to enforce these claims against either the victorious allies or the new nation states that emerged from the dissolution of the Empire and all the lands in question remained separated from the new Austria.\nFrench premier Georges Clemenceau said: \"Austria is what's left\". An empire of over 50 million had been reduced to a state of 6.5 million.\n1919.\nOn 19 February elections were held for what was now called the Constituent National Assembly. Although the Social Democrats won the most seats (41%) they did not have an absolute majority and formed a grand coalition with the second-largest party, the Christian Socialists. On 12 March the National Assembly declared \"German Austria\" to be part of the \"German Republic\".\nLarge sections of the population and most representatives of political parties were of the opinion that this \"residual\" or \"rump state\"\u00a0\u2013 without Hungary's agriculture sector and Bohemia's industry \u2013 would not be economically viable. The journalist Hellmut Andics (1922\u20131998) expressed this sentiment in his book entitled \"Der Staat, den keiner wollte\" (The state that nobody wanted) in 1962.\nAustria's exact future remained uncertain until formal treaties were signed and ratified. This process began with the opening of the Peace Conference in Paris on 18 January 1919 and culminated in the signing of the Treaty of Saint Germain on 10 September that year, although the National Assembly initially rejected the draft treaty on 7 June.\nThe First Republic, 1919\u20131933.\nTreaty of Saint Germain 1919.\nThe fledgling Republic of German-Austria was to prove short lived. The proposed merger with the German Empire (Weimar Republic) was vetoed by the Allied victors in the Treaty of Saint-Germain-en-Laye (10 September 1919) under Article 88 which prohibited economic or political union. The allies were fearful of the long-held \"Mitteleuropa\" dream\u2014a union of all German-speaking populations. The treaty was ratified by parliament on 21 October 1919. Austria was to remain independent, and was obliged to be so for at least 20 years.\nThe treaty also obliged the country to change its name from the \"Republic of German Austria\" to the \"Republic of Austria\", i.e., the First Republic, a name that persists to this day. The German-speaking bordering areas of Bohemia and Moravia (later called the \"Sudetenland\") were allocated to the newly founded Czechoslovakia. Many Austrians and Germans regarded this as hypocrisy since U.S. president Woodrow Wilson had proclaimed in his famous \"Fourteen Points\" the \"right of self-determination\" for all nations. In Germany, the constitution of the Weimar Republic explicitly stated this in article 61: \"Deutsch\u00f6sterreich erh\u00e4lt nach seinem Anschlu\u00df an das Deutsche Reich das Recht der Teilnahme am Reichsrat mit der seiner Bev\u00f6lkerung entsprechenden Stimmenzahl. Bis dahin haben die Vertreter Deutsch\u00f6sterreichs beratende Stimme.\"\u2014\"German Austria has the right to participate in the German Reichsrat (the constitutional representation of the federal German states) with a consulting role according to its number of inhabitants until unification with Germany.\" In Austria itself, almost all political parties together with the majority of public opinion continued to cling to the concept of unification laid out in Article 2 of the 1918 constitution.\nAlthough Austria-Hungary had been one of the Central Powers, the allied victors were much more lenient with a defeated Austria than with either Germany or Hungary. Representatives of the new Republic of Austria convinced the allies that it was unfair to penalize Austria for the actions of a now dissolved Empire, especially as other areas of the Empire were now perceived to be on the \"victor\" side, simply because they had renounced the Empire at the end of the war. Austria never did have to pay reparations because allied commissions determined that the country could not afford to pay.\nHowever, the Treaty of Saint Germain also confirmed Austria's loss of significant German-speaking territories, in particular the southern part of the County of Tyrol (now South Tyrol) to Italy and the German-speaking areas within Bohemia and Moravia to Czechoslovakia. In compensation (as it were) it was to be awarded most of the German-speaking part of Hungary in the Treaty of Trianon concluded between the Allies and that country; this was constituted the new federal state of Burgenland.\nEnd of grand coalition and new constitution (1920\u20131933).\nThe grand coalition was dissolved on 10 June 1920, being replaced by a CS- SDAP\u00d6 coalition under Michael Mayr as Chancellor (7 July 1920\u00a0\u2013 21 June 1921), necessitating new elections which were held on 17 October, for what now became the National Council, under the new constitution of 1 October. This resulted in the Christian Social party now emerging as the strongest party, with 42% of the votes and subsequently forming Mayr's second government on 22 October as a CS minority government (with the support of the GDVP) without the Social Democrats. The CS were to continue in power until the end of the first republic, in various combinations of coalitions with the GDVP and Landbund (founded 1919).\nThe borders continued to be somewhat uncertain because of plebiscites in the tradition of Woodrow Wilson. Plebiscites in the regions of Tyrol and Salzburg between 1919 and 1921 (Tyrol 24 April 1921, Salzburg 29 May 1921) yielded majorities of 98% and 99% in favour of unification with Germany, fearing that \"rump\" Austria was not economically viable. However such mergers were not possible under the treaty.\nOn 20 October 1920, a plebiscite in part of the Austrian state of Carinthia was held in which the population chose to remain a part of Austria, rejecting the territorial claims of the Kingdom of Serbs, Croats and Slovenes to the state. Mostly German-speaking parts of western Hungary was awarded to Austria as the new province Burgenland in 1921, with the exception of the city of Sopron and adjacent territories, whose population decided in a referendum (which is sometimes considered by Austrians to have been rigged) to remain with Hungary. The area had been discussed as the site of a Slavic corridor uniting Czechoslovakia to Yugoslavia. This made Austria the only defeated country to acquire additional territory as part of border adjustments.\nDespite the absence of reparations, Austria under the coalition suffered hyperinflation similar to that of Germany, destroying some of the financial assets of the middle and upper classes, and disrupting the economy. Adam Ferguson attributes hyperinflation to the existence of far too many people on the government payroll, failure to tax the working class, and numerous money losing government enterprises. The conservatives blamed the left for the hyperinflation; Ferguson blames policies associated with the left. Massive riots ensued in Vienna in which the rioters demanded higher taxes on the rich and reduced subsidies to the poor. In response to the riots, the government increased taxes but failed to reduce subsidies.\nThe terms of the Treaty of Saint Germain were further underlined by the Geneva Protocols of the League of Nations (which Austria joined on 16 December 1920) on 4 October 1922 between Austria and the Allies. Austria was given a guarantee of sovereignty provided it did not unite with Germany over the following 20 years. Austria also received a loan of 650 million Goldkronen which was successful in halting hyperinflation, but required major restructuring of the Austrian economy. The Goldkrone was replaced by the more stable Schilling, but resulted in unemployment and new taxes, loss of social benefits and major attrition of the public service.\nThe First World Congress of Jewish Women was held in Vienna in May 1923.\nPolitics and government.\nEmerging from the war, Austria had two main political parties on the right and one on the left. The right was split between clericalism and nationalism. The Christian Social Party, (\"Christlichsoziale Partei\", CS), had been founded in 1891 and achieved plurality from 1907\u20131911 before losing it to the socialists. Their influence had been waning in the capital, even before 1914, but became the dominant party of the First Republic, and the party of government from 1920 onwards. The CS had close ties to the Roman Catholic Church and was headed by a Catholic priest named Ignaz Seipel (1876\u20131932), who served twice as Chancellor (1922\u20131924 and 1926\u20131929). While in power, Seipel was working for an alliance between wealthy industrialists and the Roman Catholic Church. The CS drew its political support from conservative rural Catholics. In 1920 the Greater German People's Party (\"Gro\u00dfdeutsche Volkspartei\", GDVP) was founded from the bulk of liberal and national groups and became the junior partner of the CS.\nOn the left the Social Democratic Workers' Party of Austria (\"Sozialdemokratische Arbeiterpartei \u00d6sterreichs\", SDAP\u00d6) founded in 1898, which pursued a fairly left-wing course known as Austromarxism at that time, could count on a secure majority in \"Red Vienna\" (as the capital was known from 1918 to 1934), while right-wing parties controlled all other states. The SDAP\u00d6 were the strongest voting bloc from 1911 to 1918.\nBetween 1918 and 1920, there was a grand coalition government including both left and right-wing parties, the CS and the Social Democratic Workers' Party. This gave the Social Democrats their first opportunity to influence Austrian politics. The coalition enacted progressive socio-economic and labour legislation such as the vote for women on 27 November 1918, but collapsed on 22 October 1920. In 1920, the modern Constitution of Austria was enacted, but from 1920 onwards Austrian politics were characterized by intense and sometimes violent conflict between left and right. The bourgeois parties maintained their dominance but formed unstable governments while socialists remained the largest elected party numerically.\nBoth right-wing and left-wing paramilitary forces were created during the 20s. The \"Heimwehr\" (Home Resistance) first appeared on 12 May 1920 and became progressively organised over the next three years and the \"Republikanischer Schutzbund\" was formed in response to this on 19 February 1923. From 2 April 1923 to 30 September there were violent clashes between Socialists and Nazis in Vienna. That on 2 April, referred to as \"Schlacht auf dem Exelberg\" (Battle of Exelberg), involved 300 Nazis against 90 Socialists. Further episodes occurred on 4 May and 30 September 1923. A clash between those groups in Schattendorf, Burgenland, on 30 January 1927 led to the death of a man and a child. Right-wing veterans were indicted at a court in Vienna, but acquitted in a jury trial. This led to massive protests and a fire at the \"Justizpalast\" in Vienna. In the July Revolt of 1927, 89 protesters were killed by the Austrian police forces.\nPolitical conflict escalated until the early 1930s. The elections of 1930 which returned the Social Democrats as the largest bloc turned out to be the last till after World War II. On 20 May 1932, Engelbert Dollfu\u00df, Christian Social Party Agriculture Minister became Chancellor, with a majority of one.\nFederal State of Austria (1933\u20131938).\nEngelbert Dollfuss (1933\u20131934).\n1933: Dissolution of parliament and the formation of the Patriotic Front.\nDollfuss and the Christian Social Party, moved Austria rapidly towards centralized power similar to the Fascist model. He was concerned that German National Socialist leader Adolf Hitler had become Chancellor of Germany on 30 January 1933, after his party had become the largest group in the parliament and was quickly assuming absolute power. Similarly the Austrian National Socialists (DNSAP) could easily become a significant minority in future Austrian elections. Fascism scholar Stanley G. Payne, estimated that if elections had been held in 1933, the DNSAP could have secured about 25% of the votes. \"Time\" magazine suggested an even higher level of support of 50%, with a 75% approval rate in the Tyrol region bordering Nazi Germany. The events in Austria during March 1933 echoed those of Germany, where Hitler also effectively installed himself as dictator in the same month.\nMarch coup d'\u00e9tat.\nOn 4 March 1933, there occurred an irregularity in the parliamentary voting procedures. Karl Renner (Social Democratic Party of Austria, \"Sozialdemokratische Partei \u00d6sterreichs\" SP\u00d6), president of the National Council (\"Nationalrat\": lower house of parliament) resigned in order to be able to cast a vote on a controversial proposal to deal with the railroad strike that was likely to pass by a very small margin, which he was not able to do while holding that office. Consequently, the two vice-presidents representing the other parties, Rudolf Ramek (Christian Social Party) and Sepp Straffner (Greater German People's Party) also resigned for the same reason. In the absence of the President the session could not be concluded.\nAlthough there were procedural rules which could have been followed in this unprecedented and unforeseen event, the Dollfuss cabinet seized the opportunity to declare the parliament unable to function. While Dollfuss described this event as \"self-elimination of Parliament\" it was actually the beginning of a coup d'etat that would establish the \"St\u00e4ndestaat\" lasting to 1938.\nUsing an emergency provision enacted during the First World War, the Economic War Powers Act (\"Kriegswirtschaftliches Erm\u00e4chtigungsgesetz\", KWEG 24. Juli 1917 RGBl. Nr. 307) the executive assumed legislative power on 7 March and advised President Wilhelm Miklas to issue a decree adjourning it indefinitely. The First Republic and democratic government therefore effectively ended in Austria, leaving Dollfuss to govern as a dictator with absolute powers. Immediate measures included removing the right of public assembly and freedom of the press. The opposition accused him of violating the constitution.\nAn attempt by the Greater German People's Party and the Social Democrats to reconvene the council on 15 March was prevented by barring the entrance with police and advising President Wilhelm Miklas to adjourn it indefinitely. Dollfuss would have been aware that Nazi troops had seized power in neighbouring Bavaria on 9 March. Finally, on 31 March, the Republikanischer Schutzbund (paramilitary arm of the Social Democratic Party) was dissolved (but continued illegally).\nSubsequent events.\nDollfuss then met with Benito Mussolini for the first time in Rome on 13 April. On 23 April, the National Socialists (DNSAP) gained 40 per cent of the vote in the Innsbruck communal elections, becoming the largest voting bloc, so in May all state and communal elections were banned.\nOn 20 May 1933, Dollfuss replaced the \"Democratic Republic\" with a new entity, merging his Christian Social Party with elements of other nationalist and conservative groups, including the Heimwehr, which encompassed many workers who were unhappy with the radical leadership of the socialist party, to form the \"Patriotic Front\" (\"Vaterl\u00e4ndische Front\"), though the Heimwehr continued to exist as an independent organization until 1936, when Dollfuss' successor Kurt von Schuschnigg forcibly merged it into the Front, instead creating the unabidingly loyal \"Frontmiliz\" as a paramilitary task force. The new entity was allegedly bipartisan and represented those who were \"loyal to the government\".\nThe DNSAP was banned in June 1933. Dollfuss was also aware of the Soviet Union's increasing influence in Europe throughout the 1920s and early 1930s, and also banned the communists, establishing a one-party conservative dictatorship largely modeled after Italian fascism, tied to Catholic corporatism and anti-secularism. He dropped all pretence of Austrian reunification with Germany so long as the Nazi Party remained in power there.\nAlthough all Austrian parties, including the Social Democratic Labour Party (SDAP\u00d6) were banned, Social Democrats continued to exist as an independent organization, including its paramilitary \"Republikaner Schutzbund\", which could muster tens of thousands against Dollfuss' government.\nIn August 1933, Mussolini's government issued a guarantee of Austrian independence (\"if necessary, Italy would defend Austria's independence by force of arms\"). Dollfuss also exchanged 'Secret Letters' with Benito Mussolini about ways to guarantee Austrian independence. Mussolini was interested in Austria forming a buffer zone against Nazi Germany. Dollfuss always stressed the similarity of the regimes of Hitler in Germany and Joseph Stalin in the Soviet Union, and was convinced that Austria and Italy could counter totalitarian national socialism and communism in Europe.\nDollfuss escaped an assassination attempt in October 1933 by Rudolf Dertil, a 22-year-old who had been ejected from the military for his national socialist views.\n1934: Civil war and assassination.\nDespite the March coup d'\u00e9tat, the SP\u00d6 continued to seek a peaceful resolution, but when the government tried to search the party's office in Linz for weapons on 12 February 1934, members of the \"Schutzbund\" fought back. The resistance spread and became the Austrian Civil War, in which the weakened party and its supporters were quickly defeated and the party and its various ancillary organisations were banned.\nOn 1 May 1934, the Dollfuss cabinet approved a new constitution that abolished freedom of the press, established one party system and created a total state monopoly on employer-employee relations. This system remained in force until Austria became part of Nazi Germany in 1938. The Patriotic Front government frustrated the ambitions of pro-Hitlerite sympathizers in Austria who wished both political influence and unification with Germany, leading to Dollfuss' assassination by a group of Nazis on 25 July 1934.\nKurt Schuschnigg (1934\u20131938).\nHis successor Kurt Schuschnigg maintained the ban on pro-Hitlerite activities in Austria, but was forced to resign on 11 March 1938 following a demand by Adolf Hitler for power-sharing with pro-German circles. Following Schuschnigg's resignation, German troops occupied Austria with no resistance.\nAnschluss and unification with Germany (1938\u20131945).\nAlthough the Treaty of Versailles and the Treaty of St. Germain had explicitly forbidden the unification of Austria and Germany, the native Austrian Hitler was vastly striving to annex Austria during the late 1930s, which was fiercely resisted by the Austrian Schuschnigg dictatorship. When the conflict was escalating in early 1938, Chancellor Schuschnigg announced a plebiscite on the issue on 9 March, which was to take place on 13 March. On 12 March, German troops entered Austria, who met celebrating crowds, in order to install Nazi puppet Arthur Seyss-Inquart as Chancellor. With a Nazi administration already in place the country was now integrated into Nazi Germany renamed as \"Ostmark\" until 1942, when it was renamed again as \"Alpen-und Donau-Reichsgaue\" (\"Alpine and Danubian Gaue\"). A rigged referendum on 10 April was used to demonstrate the alleged approval of the annexation with a majority of 99.73% for the annexation.\nAs a result, Austria ceased to exist as an independent country. This annexation was enforced by military invasion but large parts of the Austrian population were in favour of the Nazi regime, and many Austrians participated in its crimes. The Jews, Communists, Socialist and hostile politicians were sent to concentration camps, murdered or forced into exile.\nJust before the end of the war, on 28 March 1945, American troops entered Austria and the Soviet Union's Red Army crossed the eastern border two days later, taking Vienna on 13 April. American and British forces occupied the western and southern regions, preventing Soviet forces from completely overrunning and controlling the country.\nThe Second Republic (since 1945).\nAllied occupation.\nAccording to the plans by Winston Churchill, a south German state would be formed including Austria and Bavaria.However, in April 1945 Karl Renner, an Austrian elder statesman, declared Austria separate from the other German-speaking lands and set up a government which included socialists, conservatives and communists. A significant number of these were returning from exile or Nazi detention, having thus played no role in the Nazi government. This contributed to the Allies' treating Austria more as a liberated, rather than defeated, country, and the government was recognized by the Allies later that year. The country was occupied by the Allies from 9 May 1945, and under the Allied Commission for Austria established by an agreement on 4 July 1945, it was divided into Zones occupied respectively by American, British, French and Soviet Army personnel, with Vienna being also divided similarly into four sectors, with an International Zone at its heart.\nThough under occupation, this Austrian government was officially permitted to conduct foreign relations with the approval of the Four Occupying Powers under the agreement of 28 June 1946. As part of this trend, Austria was one of the founding members of the Danube Commission, which was formed on 18 August 1948. Austria would benefit from the Marshall Plan, but economic recovery was slow.\nUnlike the First Republic, which had been characterized by sometimes violent conflict between the different political groups, the Second Republic became a stable democracy. The two largest leading parties, the Christian-democratic Austrian People's Party (\u00d6VP) and the Social Democratic Party (SP\u00d6), remained in a coalition led by the \u00d6VP until 1966. The Communist Party of Austria (KP\u00d6), who had hardly any support in the Austrian electorate, remained in the coalition until 1950 and in parliament until the 1959 election. For much of the Second Republic, the only opposition party was the Freedom Party of Austria (FP\u00d6), which included German nationalist and liberal political currents. It was founded in 1955 as a successor organisation to the short-lived Federation of Independents (VdU).\nThe United States countered starvation in 1945\u201346 with emergency supplies of food delivered by the US Army, by the United Nations Relief and Rehabilitation Administration (UNRRA), and by the privately organized Cooperative for American Remittances to Europe (CARE). Starting in 1947, it funded the Austrian trade deficit. Large-scale Marshall Plan aid began in 1948 and operated in close cooperation with the Austrian government. However, tensions arose when Austria\u2014which never joined NATO\u2014was ineligible for the American shift toward rearmament in military spending. The US was also successful in helping Austrian popular culture adopt American models. In journalism, for example, it sent in hundreds of experts (and controlled the newsprint), closed down the old party-line newspapers, introduced advertising and wire services, and trained reporters and editors, as well as production workers. It founded the \"Wiener Kurier,\" which became popular, as well as many magazines such as \"Medical News from the United States,\" which informed doctors on new treatments and drugs. The Americans also thoroughly revamped the radio stations, in part with the goal of countering the Soviet-controlled stations. On an even larger scale the education system was modernized and democratized by American experts.\nIndependence and political development during the Second Republic.\nThe two major parties strove towards ending allied occupation and restoring a fully independent Austria. The Austrian State Treaty was signed on 15 May 1955. Upon the termination of allied occupation, Austria was proclaimed a neutral country, and everlasting neutrality was incorporated into the Constitution on 26 October 1955.\nThe political system of the Second Republic came to be characterized by the system of \"Proporz\", meaning that posts of some political importance were split evenly between members of the SP\u00d6 and \u00d6VP. Interest group representations with mandatory membership (e.g., for workers, businesspeople, farmers etc.) grew to considerable importance and were usually consulted in the legislative process, so that hardly any legislation was passed that did not reflect widespread consensus. The Proporz and consensus systems largely held even during the years between 1966 and 1983, when there were non-coalition governments.\nThe \u00d6VP-SP\u00d6 coalition ended in 1966, when the \u00d6VP gained a majority in parliament. However, it lost it in 1970, when SP\u00d6 leader Bruno Kreisky formed a minority government tolerated by the FP\u00d6. In the elections of 1971, 1975 and 1979 he obtained an absolute majority. The 70s were then seen as a time of liberal reforms in social policy. Today, the economic policies of the Kreisky era are often criticized, as the accumulation of a large national debt began, and non-profitable nationalized industries were strongly subsidized.\nFollowing severe losses in the 1983 elections, the SP\u00d6 entered into a coalition with the FP\u00d6 under the leadership of Fred Sinowatz. In Spring 1986, Kurt Waldheim was elected president amid considerable national and international protest because of his possible involvement with the Nazis and war crimes during World War II. Fred Sinowatz resigned, and Franz Vranitzky became chancellor.\nIn September 1986, in a confrontation between the German-national and liberal wings, J\u00f6rg Haider became leader of the FP\u00d6. Chancellor Vranitzky rescinded the coalition pact between FP\u00d6 and SP\u00d6, and after new elections, entered into a coalition with the \u00d6VP, which was then led by Alois Mock. J\u00f6rg Haider's populism and criticism of the Proporz system allowed him to gradually expand his party's support in elections, rising from 4% in 1983 to 27% in 1999. The Green Party managed to establish itself in parliament from 1986 onwards.\nRecent years.\nThe SP\u00d6\u2013\u00d6VP coalition persisted until 1999. Austria joined the European Union in 1995 (http://), and Austria was set on the track towards joining the Eurozone, when it was established in 1999.\nIn 1993, the Liberal Forum was founded by dissidents from the FP\u00d6. It managed to remain in parliament until 1999.\nViktor Klima succeeded Vranitzky as chancellor in 1997.\nIn 1999, the \u00d6VP fell back to third place behind the FP\u00d6 in the elections. Even though \u00d6VP chairman and Vice Chancellor Wolfgang Sch\u00fcssel had announced that his party would go into opposition in that case, he entered into a coalition with the FP\u00d6\u2014with himself as chancellor\u2014in early 2000 under considerable national and international outcry. J\u00f6rg Haider resigned as FP\u00d6 chairman, but retained his post as governor of Carinthia and kept substantial influence within the FP\u00d6.\nIn 2002, disputes within the FP\u00d6 resulting from losses in state elections caused the resignation of several FP\u00d6 government members and a collapse of the government. Wolfgang Sch\u00fcssel's \u00d6VP emerged as the winner of the subsequent election, ending up in first place for the first time since 1966. The FP\u00d6 lost more than half of its voters, but reentered the coalition with the \u00d6VP. Despite the new coalition, the voter support for the FP\u00d6 continued to dwindle in all most all local and state elections. Disputes between \"nationalist\" and \"liberals\" wings of the party resulted in a split, with the founding of a new liberal party called the Alliance for the Future of Austria (BZ\u00d6) and led by J\u00f6rg Haider. Since all FP\u00d6 government members and most FP\u00d6 members of parliament decided to join the new party, the Sch\u00fcssel coalition remained in office (now in the constellation \u00d6VP\u2013BZ\u00d6, with the remaining FP\u00d6 in opposition) until the next elections. On 1 October 2006 the SP\u00d6 won a head on head elections and negotiated a grand coalition with the \u00d6VP. This coalition started its term on 11 January 2007 with Alfred Gusenbauer as Chancellor of Austria. For the first time, the Green Party of Austria became the third-largest party in a nationwide election, overtaking the FP\u00d6 by a narrow margin of only a few hundred votes.\nThe grand coalition headed by Alfred Gusenbauer collapsed in the early summer of 2008 over disagreements about the country's EU policy. The early elections held on 28 September resulted in extensive losses for the two ruling parties and corresponding gains for Heinz-Christian Strache's FP\u00d6 and J\u00f6rg Haider's BZ\u00d6 (the Green Party was relegated to the 5th position). Nevertheless, SP\u00d6 and \u00d6VP renewed their coalition under the leadership of the new SP\u00d6 party chairman Werner Faymann. In 2008 J\u00f6rg Haider died in a controversial car accident and was succeeded as BZ\u00d6 party chairman by Herbert Scheibner and as governor of Carinthia by Gerhard D\u00f6rfler.\nIn the legislative elections of 2013, SP\u00d6 received 27% of the vote and 52 seats; \u00d6VP 24% and 47 seats, thus controlling together the majority of the seats. FP\u00d6 received 40 seats and 21% of the votes, while the Greens received 12% and 24 seats. Two new parties, Stronach and the NEOS, received less than 10% of the vote, and 11 and nine seats respectively.\nOn 17 May 2016, Christian Kern from SP\u00d6 was sworn in as new chancellor. He continued governing in a \"grand coalition\" with \u00d6VP. He took the office after former chancellor, also from SP\u00d6, Werner Faymann's resignation.\nOn 26 January 2017, Alexander Van der Bellen was sworn into as the mostly ceremonial - but symbolically significant - role of Austrian president.\nAfter the Grand Coalition broke in Spring 2017 a snap election was proclaimed for October 2017. \u00d6VP with its new young leader Sebastian Kurz emerged as the largest party in the National Council, winning 31.5% of votes and 62 of the 183 seats. SP\u00d6 finished second with 52 seats and 26.9% votes, slightly ahead of FP\u00d6, which received 51 seats and 26%. NEOS finished fourth with 10 seats (5.3 percent of votes), and PILZ (which split from the Green Party at the start of the campaign) entered parliament for the first time and came in fifth place with 8 seats and 4.4% The Green Party failed with 3.8% to cross the 4% threshold and was ejected from parliament, losing all of its 24 seats. The \u00d6VP decided to form a coalition with the FP\u00d6. The new government between the centre-right wing and the right-wing populist party under the new chancellor Sebastian Kurz was sworn in on 18 December 2017, but the coalition government later collapsed in the wake of the \u201cIbiza\u201d corruption scandal and new elections were called for 29 September 2019. The elections lead to another landslide victory (37.5%) of the Austrian People's Party (\u00d6VP) who formed a coalition-government with the reinvigorated (13.9%) Greens, which was sworn in with Kurz as chancellor on 7 January 2020.\nOn 11 October 2021, Chancellor Sebastian Kurz resigned, after pressure triggered by a corruption scandal. Foreign Minister Alexander Schallenberg of \u00d6VP succeeded him as chancellor. Following a corruption scandal involving the ruling \u00d6VP, Austria got its third conservative chancellor in two months after Karl Nehammer was sworn into office on 6 December 2021. His predecessor Alexander Schallenberg had left the office after less than two months. \u00d6VP and the Greens continued to govern together.\nSee also.\nArticles.\nAustria.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOther.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nLists.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39478", "revid": "50920475", "url": "https://en.wikipedia.org/wiki?curid=39478", "title": "History of Liechtenstein", "text": " \nPolitical identity came to the territory now occupied by the Principality of Liechtenstein in 814, with the formation of the subcountry of Lower Rh\u00e6tia. Liechtenstein's borders have remained unchanged since 1434, when the Rhine established the border between the Holy Roman Empire and the Swiss cantons.\nAntiquity.\nThe area that is now Liechtenstein was part of the Roman province of Rhaetia. A Roman road crossed the region from south to north, traversing the Alps by the Spl\u00fcgen Pass and, following the right bank of the Rhine at the edge of the floodplain, was uninhabited for long lengths of time because of periodic flooding. Roman villas have been excavated in Schaanwald and Nendeln. The late Roman influx of the Alemanni from the north is memorialized by the remains of a Roman fort at Schaan.\nMiddle Ages.\nThe area, part of Raetia, was incorporated into the Carolingian empire, and divided into countships, which became subdivided over the generations. Because the Duchy of Swabia lost its duke in 1268 and was never restored, all vassals of the duchy became immediate vassals of the Imperial Throne (as has happened in much of Westphalia when the duchy of Saxons was divided and partially dissolved in aftermath of the defeat of Henry the Lion). Until about 1100, the predominant language of the area was Romansch, but thereafter German gained ground, and in 1300 an Alemannic population called the Walsers (originating in Valais) entered the region. In the 21st century, the mountain village of Triesenberg still preserves features of Walser dialect.\nThe medieval County of Vaduz was formed in 1342 as a small subdivision of the Werdenberg county of the dynasty of Montfort of Vorarlberg. The 15th century brought three wars and some devastation. Centuries later, Carl Alexander Heideloff was the one who built the Lichtenstein Castle (with the help of peasants). However, the 17th century was a low point. The area that was to become Liechtenstein was invaded by both Austrian and Swedish troops during the Thirty Years' War of 1618\u20131648. During the 17th century the country was afflicted by a plague and also by the witch trials, in which more than 100 people were persecuted and executed.\nThe House of Liechtenstein, which would later form the modern principality, derives its name from the Liechtenstein Castle in Lower Austria, built in the 12th century. The heads of the Liechtenstein family were often subservient and acted as advisors to the Holy Roman Emperor, which allowed them to obtain lands mostly in Moravia, Lower Austria and Styria over the centuries. Karl of Liechtenstein was the Obersthofmeister in the court of Rudolf II, a position which he held until 1607. However, during the dispute between Rudolf and his brother Matthias, Karl sided with Matthias. In return, Matthias, now Archduke of Austria, elevated the House of Liechtenstein to hereditary prince status on 20 December 1608, thus making Karl the first sovereign Prince of Liechtenstein as Karl I.\nDuring the Bohemian Revolt (1618\u20131620), the house of Liechtenstein lost most of its possessions in Moravia. Karl I and his brother Maximilian played a leading role in the Battle of White Mountain in November 1620, when the following year Karl I conducted the trials and executions of the revolt leaders on behalf of Ferdinand II. In exchange for their services, Ferdinand II again elevated Karl I, along with his brothers Maximillian and Gundaker, to Imperial prince along with granting the family numerous estates in Moravia and Silesia. In 1622, the House of Liechtenstein purchased the lands of M\u00e4hrisch Kromau and Ungarisch Ostra. In 1633, the two areas were bestowed the name of the Principality of Liechtenstein by Ferdinand II, though this name largely fell out of use by 1647.\nEarly modern era.\nPrince Johann Adam Andreas of Liechtenstein bought the domain of Schellenberg in 1699 and the County of Vaduz in 1712. This Prince of Liechtenstein had wide landholdings in Austria, Bohemia and Moravia, but none of his lands were held directly from the Emperor. Thus, the prince was barred from entry to the Imperial Diet and the prestige and influence that would entail.\nBy acquiring the Lordships of Schellenberg and Vaduz, modest areas of mountain villages each of which was directly subordinate to the Emperor because there no longer being a Duke of Swabia, the Prince of Liechtenstein achieved his goal. The territory took the name of the family which now ruled it. On 23 January 1719, Charles VI, Holy Roman Emperor, decreed that the counties of Vaduz and Schellenberg be promoted to a principality with the name Liechtenstein for his servant Anton Florian of Liechtenstein whereby he and his successors became Princes of the Holy Roman Empire. \nThe prince never visited his principality, and the first visit of a sovereign prince to Liechtenstein did not occur until 1842. Instead, they were represented by the local district office based in Vaduz, consisting of a number of officials, which was regulated by civil service law. It served as the only form of administration in the country until its disbandment in 1848, and was primarily responsible for controlling the country's tax office and reporting significant developments to the prince, who resided in Vienna. Until the end of the 17th century, officials were primarily recruited from neighbouring Vorarlberg, and were paid directly by the prince.\nNineteenth century.\nFrench revolutionary and Napoleonic wars.\nIn the War of the First Coalition, Liechtenstein, as part of the Holy Roman Empire contributed approximately 20 troops to the coalition forces from 1793 to 1796. During the War of the Second Coalition, France invaded the country on 6 March 1799 and plundered several towns, including Nendeln that was burned by French troops, which resulted in the deaths of four people. The Austrian and Volgraberg state militias under command by Lieutenant field marshal Franjo Jela\u010di\u0107 defeated 18,000 French troops stationed in Liechtenstein under command of General Andr\u00e9 Mass\u00e9na and liberated the country by 14 May.\nIn 1806, Liechtenstein was one of the principalities and counties Maximilian I of Bavaria wanted to annex as his price for joining the Confederation of the Rhine but Napoleon refused because he had appreciated the personal qualities of Johann I as a negotiator, Austria's envoy during the negotiations leading to the Treaty of Pressburg. Thus Liechtenstein became a sovereign state later that year when it joined Napoleon's Confederation of the Rhine upon the dissolution of the Holy Roman Empire.\nThe French under Napoleon occupied the country for a few years, but Liechtenstein retained its independence in 1815. Soon afterward, Liechtenstein joined the German Confederation (20 June 1815 \u2013 24 August 1866, which was presided over by the Emperor of Austria). In 1818, Johann I granted a constitution, although it was limited in its nature. 1818 also saw the first visit of a member of the house of Liechtenstein, Prince Alois. However, the first visit by a sovereign prince did not occur until 1842.\nIn 1833, Michael Menzinger applied for the role of . The role originated from the 16th century and functioned as the head of the district office (), subordinate to the court of House of Liechtenstein. It was previously an undesired role within the court, but Menzinger applying for the role changed this. For this reason, he is considered the first governor of Liechtenstein.\nConstitution and latter century.\nLike most of Europe at the time, Liechtenstein was subject to the German revolutions of 1848\u20131849 which caused increased opposition against the absolute monarchy of Aloys II. The aim of the revolution was to improve the economic and political situation of ordinary citizens in Liechtenstein, primarily fuelled by the worsening economy in the country in the years prior. On 22 March 1848, the people's committee appointed a three-person committee to lead the Liechtenstein revolutionary movement, which included Peter Kaiser, Karl Sch\u00e4dler and Ludwig Grass. Together, they managed to maintain order in Liechtenstein and formed a constitutional council. Liechtenstein was a member of the National Assembly in Frankfurt until April 1849.\nFollowing the revolution, a constitutional council was elected on 27 July 1848 in response to popular demand from the revolutionaries, of which Sch\u00e4dler was elected as its president. The primary task of the council was the creation the draft for a new Liechtenstein constitution, of which the work was done primarily by him and Michael Menzinger. As a concession towards the revolution, the district office was disbanded and replaced by a District Council that was formed on 7 March 1849 with 24 elected representatives and acted as the first democratic representation in Liechtenstein, with Sch\u00e4dler was elected as District Administrator. In addition, the title of Landvogt was changed to governor () with Menzinger continuing in the role.\nAfter the failure of the German revolutions, Aloys II once again instated absolute power over Liechtenstein on 20 July 1852 and disbanded the district council. However, calls for a new constitution once again appeared early in the reign of Johann II and the constitutional council was reformed again led by Karl Sch\u00e4dler, once again tasked with drafting a new constitution, of which, similarly to 1848, he did most of the work. The draft was reviewed by an unknown German legal expert and formed the basis of the 1862 Constitution of Liechtenstein, which was ratified on 26 September. It was heavily inspired by the constitution of Vorarlberg and largely addressed the demands of the revolutionaries in Liechtenstein. This constitution established civil liberties in the country and formed the Landtag of Liechtenstein for the first time.\nDuring the Austro-Prussian War of 1866, Prince Johann II placed his soldiers at the disposal of the Confederation but only to \u201cdefend the German territory of Tyrol\u201d. However, the Landtag had not been consulted regarding the deployment and the war was unpopular among the population, as such it faced resistance from the Landtag. As a result, Johann II promised a loan to the country and refused to have his men fight against other Germans. The Liechtenstein contingent took up position on the Stilfser Joch under the command of Peter Rheinberger in the south of Liechtenstein to defend the Liechtenstein/Austrian border against attacks by the Italians under Garibaldi. A reserve of 20 men remained in Liechtenstein at Vaduz Castle. When the war ended on 22 July, the army of Liechtenstein marched home to a ceremonial welcome in Vaduz. Popular legend claims that 80 men went to war, but 81 came back. It is disputed who the additional person was: An Austrian liaison officer may have joined up with the contingent on the way back, but it has also been claimed that the man was an Italian farmer.\nThe German Confederation dissolved in 1866. In combination with its unpopularity among the population and the rising cost to maintain it, Liechtenstein disbanded its army of 80 men on 12 February 1868 and declared its permanent neutrality, neither joining the new German Empire in 1871, nor the Austrian Empire. In 1893, former soldiers of the Liechtenstein army founded a veterans association, which had 141 members in 1896. Its last surviving member, Andreas Kieber, died in 1939, aged 94 years old. This neutrality was respected during both World Wars, and ultimately would allow the country to avoid the fate of the other German monarchies.\nLiechtenstein during the world wars.\nWorld War I.\nLiechtenstein did not participate in World War I, claiming neutrality. However, until the end of the war, it was closely tied to Austria-Hungary due to the customs union between the two countries and was sympathetic to the Central Powers. The majority of the Liechtenstein government did not expect the war to last long, thus no food or economic preparations were made for it. At the outbreak of the war France, Russia and the United Kingdom interned Liechtensteiners and partially confiscated their assets. As a result, the Liechtenstein government made various declarations that the country was neutral and a separate entity from Austria-Hungary.\nFrom September, food deliveries from Austria-Hungary, which Liechtenstein relied on, began to decrease. This quickly reduced the initial level of support for the war. In addition, Switzerland was pressured by Britain and France to end its food exports to Liechtenstein due to the latter's close ties to Austria-Hungary. In response, the Liechtenstein government, led by Leopold Freiherr von Imhof, issued emergency commissions throughout the country on 14 December 1914. These commissions aimed to manage the procurement of food and raw materials, now in short supply, and to distribute them to the population.\nForeign citizens living in Liechtenstein were conscripted into the armies of their respective home countries, primarily Austria-Hungary and Germany, of which 27 did not return. In addition, many Liechtensteiners also voluntarily enlisted in both armies, including several members of the house of Liechtenstein. In total, 4 Liechtenstein citizens are known to have been killed in the war despite the country being neutral, including Prince Heinrich of Liechtenstein, who is the highest member of the house of Liechtenstein to have been killed in action. Three Liechtensteiners were imprisoned for espionage during the war.The Entente powers imposed an economic embargo on Liechtenstein in 1916. The country faced economic devastation and food shortages as a result due to the lack of natural resources, which increased smuggling within the country significantly and forced the country to reduce its reliance on Austria-Hungary and seek closer economic ties with Switzerland. By 1916 all food deliveries from Austria-Hungary had ceased, which forced Liechtenstein to seek closer ties with Switzerland in order to ensure food deliveries continued.\nAs the war dragged on, the country faced increasing civil unrest and dissatisfaction, particularly of that towards to the government of Leopold Freiherr von Imhof. Figures such as Wilhelm Beck formed an opposition group against him, and in November 1918 he was subject of a de facto coup d'\u00e9tat against him. The coup forced Imhof's government to resign and the establishment of a Provisional Executive Committee in his place until 7 December headed by Martin Ritter, who was the first Liechtensteiner head of government. Despite diplomatic efforts by Liechtenstein, they received no representation in the negotiations or signing of the Treaty of Versailles, though the country received indirect recognition of its sovereignty in the Treaty of Saint-Germain-en-Laye.\nInterwar period (1919\u20131939).\nIn 1919, following the dissolution of Austria-Hungary the Liechtenstein government could no longer rely on Austria to fulfil their monetary and diplomatic needs. Liechtenstein and Switzerland signed a treaty under which Switzerland assumed the representation of Liechtenstein's interests at the diplomatic and consular level in countries where it maintains a representation and Liechtenstein does not. Liechtenstein adopted the Swiss franc in 1920 and the two countries entered a customs union in 1924. Liechtenstein applied to join the League of Nations in 1920, though unsuccessfully. Switzerland was the only country to vote in favour of their accession at the League of Nations Assembly on 17 December 1920, as opposed to 28 against.\nThe November 1918 putsch would begin a period of the next three years where both the Progressive Citizens' Party and Christian-Social People's Party worked together in creating a new constitution based on a constitutional monarchy, much of which was loosely based on the Swiss Federal Constitution. The drafting process included prominent politicians such as Wilhelm Beck, Josef Ospelt and Josef Peer, of which Beck and Peer created the first draft for the constitution. It was signed into law by Prince Karl Aloys on behalf of Johann II and Josef Ospelt as a government representative on 5 October 1921. It established the rule of partial parliamentary democracy mixed with that of constitutional monarchy, as well as providing for referendums on decisions of the Landtag. It also abolished the three seats in the Landtag appointed by the Prince and lowered the voting age from 24 to 21 with universal male suffrage.\n1928 embezzlement scandal.\nThe country was subject to an embezzlement scandal in 1928, where it was revealed that leading members of the Christian-Social People's Party had embezzled funds from the National Bank of Liechtenstein into various speculative transactions. The scandal forced the government of Gustav Sch\u00e4dler to resign and early elections to be called.\nRotter kidnapping.\nSince the rise of Nazi Germany in 1933 and the introduction of anti-Jewish laws in Germany, Liechtenstein experienced a large rise of Jewish emigrants to the country in which the government led by Josef Hoop had supported the naturalization of the refugees under a new citizenship law. In doing this, Liechtenstein faced attacks from German press and internal sources such as the Liechtenstein Homeland Service. Hoop personally attempted to temper relations with Germany through the use of private contacts and actively downplayed the threat of National-socialism within Liechtenstein. Most notably, German film directors and theatre managers Fritz and Alfred Rotter with a Jewish background were naturalized in Liechtenstein in 1931. Following German press and demands for their extradition local Liechtenstein Nazis used the event to attempt to kidnap the two men and forcefully return them to Nazi Germany in the Rotter kidnapping. However, this failed and as a result of a highly publicized trial it held back the formation of an organized Nazi party in Liechtenstein until 1938.\n1937 spy affair.\nIn January 1937, Carl Freiherr von Vogelsang, it was revealed that the editor of \"Liechtensteiner Vaterland\" and a founding member of the Liechtenstein Homeland Service, had sent a letter asking the police Friedrichshafen or the border guards in Lindau to arrest Ludwig Hasler, the head of the Liechtenstein tax office, claiming that his upcoming trip to Germany for a foreign exchange was a part of a conspiracy by German-Jewish emigrants three years prior in 1934. As a result, Hoop ordered a search of the offices of the \"Vaterland\" for any incriminating letters and Vogelsang promptly left the country. A majority of the Landtag approved of Hoop's actions, but members of the Patriotic Union called for his resignation over the issue, believing the search to be unconstitutional. It was decided that two special judges would determine the legal implications of the case. Eventually, in July 1937, it was concluded by both judges that Hoop had not acted unconstitutionally by ordering the search against Vogelsang, and Hoop was subsequently legally acquitted of any wrong-doing.\n1938\u20131939 crisis and failed putsch.\nOn 31 March 1938, in the wake of the Anschluss of Austria, Franz I made his grandnephew Franz Joseph II regent and moved to Feldberg, Czechoslovakia. On 25 July, he died while at one of his family's castles, Castle Feldberg, and Franz Joseph formally succeeded him as prince of Liechtenstein as Franz Joseph II. Around the same time, the German National Movement in Liechtenstein (VDBL), a local Liechtenstein Nazi party, was formed and advocated for the annexation of Liechtenstein into Nazi Germany. There were plans for the party, with connections to the Volksdeutsche Mittelstelle, to be democratically elected into power via funding from Germany, then it would end the customs union with Switzerland and align towards Germany, leading to an eventual annexation of Liechtenstein into Germany. The plans were reportedly supported by Joseph Goebbels. However, it was personally blocked by Adolf Hitler himself as he did not want to complicate relations with Switzerland.\nUnder the initiative of Franz Joseph, the Progressive Citizens' Party and Patriotic Union started negotiations for the formation of a coalition government, led by Josef Hoop and Otto Schaedler respectively. This coalition was designed to avoid political deadlock while there was ongoing threat from Nazi Germany, and more importantly, prevent the VDBL from gaining any seats within the Landtag. A compromise for the coalition was to introduce a proportional representation to the country, despite it being rejected via referendum three years prior. It was introduced unanimously on 18 January 1939. Shortly after, Franz Joseph, in agreement with both parties, disbanded the Landtag and called for new elections. However, the subsequent 1939 general election was only used to distribute a roughly equal number of seats in the Landtag between the two parties, as such it became known as the \"silent election\" as no actual voting took place. This was primarily due to both parties desire to not hold an election campaign period that would jeopardize the recently formed coalition government and allow for the VDBL to be able to gain support.\nIn March 1939, Franz Josef, Hoop and Alois Vogt paid an official visit to Berlin where they met Adolf Hitler and Joachim von Ribbentrop in which they discussed safeguarding Liechtenstein's independence and neutrality while maintaining good relations. Franz Joseph later reminisced on the visit and stated that Hitler showed little interest in them and that it only took place in order to \"flatter Hitler's ego\".\nOn 24 March 1939, the 1939 putsch took place. The plan was for members of the VDBL to march on Vaduz and seize control of the government, which was hoped would cause clashes between them and the government. German troops from Feldkirch would then move into Liechtenstein in response to a call for help and incorporate the country into Germany. The plan failed however, as they were stopped by opponents, and most VDBL members were arrested or fled. No German invasion took place as it was blocked by Hitler's orders following intervention by Alois Vogt. It is not exactly known why Hitler decided to not intervene in the coup, though it has been speculated that he had little interest in Liechtenstein, and that he did not want to provoke a war with Switzerland. This led to the Liechtenstein Loyalty Association, a nonpartisan organisation designed to oppose the actions of the VDBL that was formed earlier in the year, to heighten its operations and launch a signature campaign reaffirming Liechtenstein's independence, which gained 2492 signatures.\nWorld War II.\nDuring World War II, Liechtenstein remained neutral, while family treasures within the war zone were brought to Liechtenstein (and London) for safekeeping. At the same time, Liechtenstein tied itself as closely as possible to Switzerland during the war in hopes of retaining the country's neutrality. It achieved the de facto inclusion of Liechtenstein in the Swiss national supply. Franz Joseph himself periodically sent congratulatory letters to Hitler, such as the thwarting of the 20 July plot, of which he briefly replied. Though Nazi Germany did have plans for the annexation of Liechtenstein, primarily in Operation Tannenbaum, these were never implemented and Liechtenstein's neutrality was not violated during the war. In 1943, at the request of both the Progressive Citizens' Party and Patriotic Union, Franz Joseph extended the government's term indefinitely while there was ongoing threat from Nazi Germany, primarily to prevent the (VDBL) from gaining seats in the Landtag. General elections were not held again until April 1945, shortly before the end of the war.\nNotable figures in the Liechtenstein government, such as Alois Vogt, retained contacts with Nazi Germany during the war, such as Volksdeutsche Mittelstelle, who regarded him as a trusted contact. Three Liechtensteiners were sentenced to death by Switzerland for spying for Nazi Germany during the war. Most notably, Alfred Quaderer, a Liechtenstein citizen who became an agent for the Volksdeutsche Mittelstelle was sentenced to death for treason against Switzerland in March 1944. Despite efforts by Quaderer's sister and mother to have him pardoned, such as pleading to Franz Joseph for a private audience, they were denied, and he was executed by firing squad on 7 June 1944, aged 24 years old.\nJust before the end of the war, Franz Joseph granted political asylum to First Russian National Army pro-Axis pro-emperor Grand Duke Vladimir Kirillovich of Russia White emigres led by General Boris Smyslovsky, who were being cared for by the Liechtenstein Red Cross. On 16 August 1945, the Soviet Union sent a delegation to Liechtenstein in an attempt to repatriate the Russians, which was refused despite increasing Soviet pressure to participate in the repatriation program. Eventually the government of Argentina offered the Russians asylum, and about a hundred people left. This is commemorated by a monument at the border town of Hinterschellenberg which is marked on the country's tourist map. According to prime minister Alexander Frick, with the support of Franz Joseph, the Russians were at no point in danger of being extradited and the general population of Liechtenstein supported the government in providing asylum to them.\nAt the close of the conflict, Czechoslovakia and Poland, acting to seize what they considered to be German possessions, expropriated the entirety of the Liechtenstein dynasty's hereditary lands and possessions in Bohemia, Moravia, and Silesia \u2014 the princes of Liechtenstein lived in Vienna until the Anschluss of 1938. During the war, Liechtenstein's princely family owned land in Austria whose managers hired Nazi forced labor, but a much later inquiry found the family not to have known about this. The expropriations (subject to modern legal dispute at the International Court of Justice) included over of agricultural and forest land (most notably the UNESCO listed Lednice\u2013Valtice Cultural Landscape), and several family castles and palaces. Citizens of Liechtenstein were also forbidden from entering Czechoslovakia during the Cold War. In August 1945, Pierre Laval, the Prime Minister of Vichy France, had attempted to seek refuge in Liechtenstein after being flown to the American-occupied zone of Austria, but was turned away.\nPost-War era.\nCold war.\nAfter World War II, the country's low taxes spurred strong economic growth. Liechtenstein became increasingly important as a financial center. In dire financial straits following the war, the Liechtenstein dynasty often resorted to selling family artistic treasures, including for instance the portrait \"Ginevra de' Benci\" by Leonardo da Vinci, which was purchased by the National Gallery of Art of the United States in 1967. Liechtenstein prospered, however, during the decades following, as its economy modernized with the advantage of low corporate tax rates which drew many companies to the country.\nIn 1949, Liechtenstein ceded the Ellhorn mountain to Switzerland as a result of Swiss demands and threats to, among other things, end the customs union between the two countries. Despite the local community in Balzers previously refusing to do so in November 1948, the transfer was approved by the Landtag of Liechtenstein the following month. In exchange to the transfer, Switzerland agreed to forgive much of Liechtenstein's debt that it had acquired to the country throughout World War II.\nLiechtenstein was neutral during the Cold War, but sided with the West ideologically, politically and economically. The nuclear threat has led to the expansion of civil defence since the 1960s in Liechtenstein. In 1964\u20131965, the Liechtenstein government built a command bunker with protection against nuclear bombs in Vaduz. Liechtenstein condemned the suppression of the Hungarian Revolution of 1956 and the 1968 invasion of Czechoslovakia. Liechtenstein boycotted the Olympic Games twice- in 1956 in Melbourne in protest against the suppression of the Hungarian uprising and in 1980 in Moscow due to the Soviet war in Afghanistan. Women in Liechtenstein received voting rights for the first time, following a referendum on the topic (among men only) in 1984.Franz Joseph II handed over most of his powers to his son, Hans-Adam on 26 August 1984. On 13 November 1989, he succeeded him as prince as Hans-Adam II. In 1996, Russia returned the Liechtenstein family's archives, ending a long-running dispute between the two countries. In 1978, Liechtenstein became a member of the Council of Europe, and then joined the United Nations in 1990 following Security Council Resolution 663 and a member of the European Free Trade Association (EFTA) in its own right in 1991.\n1990s.\nOn 6 December 1992 a referendum was to be held in Switzerland on a federal resolution on the accession to the European Economic Area (EEA). In correspondence with the customs union between the two countries, a similar referendum was to be held in Liechtenstein at a similar time.\nHans-Adam II called for the referendum to be held before the corresponding referendum in Switzerland, against the countries custom union and the wishes of the government with the Landtag of Liechtenstein. On 28 October 1992, he threatened to dismiss the Landtag and prime minister Hans Brunhart over the dispute and appoint an acting Prime Minister in his place. In response, approximately 2000 people demonstrated in front of the government house in Vaduz. In the same day, the government and Hans-Adam II negotiated and came to an agreement that scheduled the referendum after the corresponding one in Switzerland, though notably it affirmed that Liechtenstein would commit to agreements with the EEA despite the result in Switzerland.\nAs a result, the 1923 customs union treaty between Liechtenstein and Switzerland was now compromised, and was no longer viable due to conflicting interests regarding accession to the EEA. In 1994, the treaty was revised to allow for greater freedom for Liechtenstein in defining its foreign policy. A referendum on the topic took place on 9 April 1995, which was accepted by 55.9% of voters. Liechtenstein subsequently joined the EEA in May the same year.\nIn 1997, at the start of Mario Frick's second term, the coalition government that had existed between the Progressive Citizens' Party and Patriotic Union since 1938 was dissolved, marking the first time either party had been in opposition to each other since.\nLiechtenstein during the 21st century.\nIn a referendum on 16 March 2003, Prince Hans-Adam, who had threatened to leave the country if he lost, won a large majority (64.3%) in favour of overhauling the constitution to effectively give him more powers than any other European monarch. The new constitution gave the prince the right to dismiss governments and approve judicial nominees and allowed him to veto laws simply by refusing to sign them within a six-month period.\nOn 15 August 2003, Hans-Adam announced he would step down in one year and hand over the reins to his son Alois. On 15 August 2004, Prince Hans-Adam handed over the practical running of the principality to his son, Crown Prince Alois, although still remaining official head of state.\nOn 1 July 2007, the first two consuls in the history of the Principality were appointed to represent Liechtenstein in the United States of America. On 1 March 2007, the Swiss Armed Forces \"invaded\" Liechtenstein territory by mistake, with 170 Swiss Army troops crossing the border.\nOn 27 November 2005, Liechtenstein voters rejected an initiative that would prohibit abortion and birth control in the country. The initiative was supported by Roman Catholic Archbishop Wolfgang Haas. Prince Alois was initially sympathetic to the proposal, but he became neutral during the run-up to the vote. Instead, a government-sponsored counter proposal was ratified. In 2011, Alois announced he would veto any relaxing of the ban on abortion in Liechtenstein, which was an subject for referendum later that year. Such a veto was not necessary, however, as the voters rejected the proposal.\nFollowing the prince's threat, an initiative called \"\" (\"So that your voice counts\") was launched to change the constitution of Liechtenstein to prevent the prince from vetoing legislation approved in referendums. The referendum was held on 1 July 2012, and 76% of voters upheld the prince's power to veto referendum results.\nIn November 2022, the Landtag of Liechtenstein passed a motion calling on the government to introduce a bill legalizing same-sex marriage, with broad support from across the political spectrum. A bill legalizing same-sex marriage was introduced in February 2024 and passed its final reading in the Landtag on 16 May 2024 by a 24\u20131 vote. \nOn 21 October 2024, Liechtenstein joined the International Monetary Fund."}
{"id": "39481", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=39481", "title": "Lillian Moller Gilbreth", "text": "American psychologist and industrial engineer\nLillian Evelyn Gilbreth (n\u00e9e\u00a0Moller; May 24, 1878 \u2013 January 2, 1972) was an American psychologist, industrial engineer, consultant, and educator who was an early pioneer in applying psychology to time-and-motion studies. She was described in the 1940s as \"a genius in the art of living.\"\nGilbreth, one of the first female engineers to earn a Ph.D., is considered to be the first industrial/organizational psychologist. She and her husband, Frank Bunker Gilbreth, were efficiency experts who contributed to the study of industrial engineering, especially in the areas of motion study and human factors.\n\"Cheaper by the Dozen\" (1948) and \"Belles on Their Toes\" (1950), written by two of their children (Ernestine and Frank Jr.) tell the story of their family life and describe how time-and-motion studies were applied to the organization and daily activities of their large family. Both books were later made into feature films.\nEarly life and education.\nLillie Evelyn Moller was born in Oakland, California, on May 24, 1878, to Annie (n\u00e9e\u00a0Delger) and William Moller, a builder's supply merchant. She was their second child and the eldest of the family's nine surviving children. Their first child, Anna Adelaide, had died at age four months. Her maternal grandfather Frederick Delger was a German immigrant who became the richest man in Oakland.\nEducated at home until the age of nine, Moller began formal schooling in the first grade at a public elementary school and was rapidly promoted through the grade levels. She was elected vice president of her senior class at Oakland High School and graduated with exemplary grades in May 1896.\nAlthough Moller wanted to go to college, her father was opposed to such education for his daughters, but persuaded her father to let her try college for a year and was admitted to the University of California.\nAfter her first-year grades were near the top of her class, her father agreed to allow her to continue her studies. She commuted from home on the streetcar, and in the evenings helped her mother with the household and her siblings with their homework. She majored in English, also studying philosophy and psychology, and had enough education courses to earn a teaching certificate. She also won a prize for poetry and acted in student plays. In the spring of her senior year the new university president, Benjamin Ide Wheeler, asked her to be one of the student speakers at the commencement ceremonies. On May 16, 1900, she graduated from the university and became the first woman to speak at a University of California commencement. The title of her speech was \"Life: A Means or an End\".\nMoller had begun to think of a professional career rather than staying at home after graduation. She now wished to be called Lillian because she felt it was a more dignified name for a university graduate, and she left home to enroll in graduate school at Columbia University in New York City. Her literature professor Charles Gayley had suggested she study there with Brander Matthews. Graduate enrollment at Columbia was almost half women at the time, but Matthews would not allow them in his classes. Instead, she studied literature with George Edward Woodberry. A lasting influence was her study with the psychologist Edward Thorndike, newly appointed at Columbia. Though she became ill with pleurisy and was brought home by her father, she continued to refer to him in her later work. Back in California, she returned to the University of California in August 1901 to work toward a master's degree in literature. Under the supervision of Gayley, she wrote a thesis on Ben Jonson's play \"Bartholomew Fair\", and received her master's degree in the spring of 1902.\nMoller began studies for a PhD at the University of California, but took time off to travel through Europe in the spring of 1903. Following her marriage to Frank Bunker Gilbreth in 1904 and relocation to New York, she completed a dissertation for a doctorate from the University of California, Berkeley, in 1911, but was not awarded the degree due to her noncompliance with residency requirements for doctoral candidates. The dissertation was published as \"The Psychology of Management: The Function of the Mind in Determining, Teaching and Installing Methods of Least Waste\" in 1914.\nAfter the Gilbreths relocated their family to Providence, Rhode Island, Lillian enrolled at Brown University. She earned a Ph.D. in applied psychology in 1915, which made her the first of the pioneers of industrial management to have a doctorate. The topic of her dissertation was efficient teaching methods and titled \"Some Aspects of Eliminating Waste in Teaching\". Her doctoral dissertation was published in 2019 as a book titled \"Eliminating Waste in Teaching\" ().\nMarriage and family.\nLillian Moller met Frank Bunker Gilbreth in June 1903 in Boston, Massachusetts, en route to Europe with her chaperone, who was Frank's cousin. He had apprenticed in several building trades in the East and established a contracting business with offices in Boston, New York, and London.\nThe couple married on October 19, 1904, in Oakland, California, and settled in New York. They later moved to Providence, Rhode Island, and eventually relocated their family to Montclair, New Jersey.\nAs planned, the Gilbreths became the parents of a large family that included twelve children. One died young in 1912; one was still-born in 1915; and eleven of them lived to adulthood, including Ernestine Gilbreth, Frank Bunker Gilbreth Jr., and Robert Moller Gilbreth.\nAfter Frank died of a heart attack on June 14, 1924, Lillian never remarried.\nCareer.\nFor just under 50 years, Gilbreth's career combined psychology with the study of scientific management and engineering. She also included her perspectives as a wife and mother in her research, writing, and consulting work. Gilbreth became a pioneer in what is now known as industrial and organizational psychology. She helped industrial engineers recognize the importance of the psychological dimensions of work. In addition, she became the first American engineer ever to create a synthesis of psychology and scientific management. (Gilbreth introduced the concept of using psychology to study management at the Dartmouth College Conference on Scientific Management in 1911).\nIn addition to jointly running Gilbreth, Incorporated, their business and engineering consulting firm, Lillian and Frank wrote numerous publications as sole authors, as well as co-authoring multiple books and more than fifty papers on a variety of scientific topics. However, in their joint publications, Lillian was not always named as a co-author, possibly due to publishers' concerns about naming a female writer. Although her credentials included a doctorate in psychology, she was less frequently credited in their joint publications than her husband, who did not attend college.\nThe Gilbreths were certain that the revolutionary ideas of Frederick Winslow Taylor would be neither easy to implement nor sufficient; their implementation would require hard work by engineers and psychologists to make them successful. The Gilbreths also believed that scientific management as formulated by Taylor fell short when it came to managing the human element on the shop floor. The Gilbreths helped formulate a constructive critique of Taylorism; this critique had the support of other successful managers.\nIn 1934, Gilbreth organized the energy-saving kitchen, along with the nursery and the clothery for America's Little House. Designed by architect, Roger Bullard, it was a project from Better Homes in America for a two-story suburban type home to be situated in New York City on Park Avenue and 39th Street, amongst all the skyscrapers.\nAfter Frank's passing and the mourning period, Lillian found that the homages to her husband were not a sign of her own taking, when three of her biggest clients did not renew or cancelled contracts. Close associates offered her employment in their firms, but she wanted to keep Frank's business afloat.\nTime, motion, and fatigue study.\nGilbreth and her husband were equal partners in the engineering and management consulting firm of Gilbreth, Incorporated. She continued to lead the company for decades after his death in 1924. The Gilbreths, both pioneers in scientific management, were especially adept at performing time-and-motion studies. They named their methodology the Gilbreth System and used the slogan, \"The One Best Way to Do Work,\" to promote it. The Gilbreths also developed a new technique for their studies that used a motion-picture camera to record work processes. These filmed observations enabled the Gilbreths to redesign machinery to better suit workers' movements to improve efficiency and reduce fatigue. Their research on fatigue study was a forerunner to ergonomics. In addition, the Gilbreths applied a human approach to scientific management to develop innovations in workplace efficiency, such as improved lighting and regular breaks, as well as ideas for workplace psychological well-being, such as suggestion boxes and free books.\nDomestic management and home economics.\nGilbreth collaborated with her husband until his death in 1924. Afterwards, she continued to research, write, and teach, in addition to consulting with businesses and manufacturers. She also participated in professional organizations such as the American Society of Mechanical Engineers until her own death nearly fifty years later in 1972. In addition, Gilbreth turned her attention to the home, despite her aversion to housework and the fact that she had long employed full-time household help. Her children once described her kitchen as a \"model of inefficiency.\"\nDue to discrimination within the engineering community, Gilbreth shifted her efforts toward research projects in the female-friendly arena of domestic management and home economics. She applied the principles of scientific management to household tasks and \"sought to provide women with shorter, simpler, and easier ways of doing housework to enable them to seek paid employment outside the home.\" The Gilbreth children often took part in the experiments.\nIn addition, Gilbreth was instrumental in the development of the modern kitchen, creating the \"work triangle\" and linear-kitchen layouts that are often used today. In the late 1920s, she collaborated with Mary E. Dillon, president of Brooklyn Borough Gas Company on the creation of an efficient kitchen, equipped with gas-powered appliances and named the \"Kitchen Practical\". Inspired by Dillon's criticisms of her own kitchen, it was designed on three principles: the correct and uniform height of working surfaces; a circular work place; and a general \"circular routing of working\", all carefully analyzed to reduce the time and effort required in the preparation of meals. It was unveiled in 1929 at a Women's Exposition.\nShe is also credited with the invention of the foot-pedal trash can, adding shelves to the inside of refrigerator doors (including the butter tray and egg keeper), and wall-light switches, all now standard. Gilbreth filed numerous patents for her designs, including one to improve the electric can opener and another for a wastewater hose for washing machines. When Gilbreth was an industrial engineer working at General Electric, she \"interviewed over 4,000 women to design the proper height for stoves, sinks, and other kitchen fixtures as she worked on improving kitchen designs\".\nAfter World War I, the Gilbreths did pioneering work with the rehabilitation of war-veteran amputees. Lillian continued consulting with businesses and manufacturers after Frank's death. Her clients included Johnson &amp; Johnson and Macy's, among others. Lillian spent three years at Macy's to find solutions to their sales and human resource issues. Solutions included changing light fixtures to reduce eye fatigue and eliminating duplicate recordings of sales checks.\nIn 1926, when Johnson &amp; Johnson hired her as a consultant to do marketing research on sanitary napkins, Gilbreth and the firm benefited in three ways. First, Johnson &amp; Johnson could use her training as a psychologist in the measurement and analysis of attitudes and opinions. Second, it could give her experience as an engineer specializing in the interaction between bodies and material objects. Third, her public image as a mother and a modern career woman could help the firm build consumer trust in its products. In addition to her work with Johnson &amp; Johnson, Gilbreth was instrumental in the design of a desk in cooperation with IBM for display at the Chicago World's Fair in 1933\nVolunteer work and government service.\nGilbreth continued her private consulting practice while serving as a volunteer and an adviser to several government agencies and nonprofit groups. In 1927 she became a charter member of the Altrusa Club of New York City, an organization for Professional and Business Women started in 1917 for the purpose of providing community service. Gilbreth's government work began as a result of her longtime friendship with Herbert Hoover and his wife Lou Henry Hoover, both of whom she had known in California (Gilbreth had presided over the Women's Branch of the Engineers' Hoover for President campaign).\nLou Hoover urged Gilbreth to join the Girl Scouts as a consultant in 1929. She remained active in the organization for more than twenty years, becoming a member of its board of directors. During the Great Depression, President Hoover appointed Gilbreth to the Organization on Unemployment Relief as head of the \"Share the Work\" program. In 1930, under the Hoover administration, she headed the women's section of the President's Emergency Committee for Employment and helped to gain the cooperation of women's groups for reducing unemployment. During World War II Gilbreth continued advising governmental groups and also provided expertise on education and labor issues (especially women in the workforce) for organizations such as the War Manpower Commission, the Office of War Information, and the U.S. Navy. In her later years, Gilbreth served on the Chemical Warfare Board and on Harry Truman's Civil Defense Advisory Council. During the Korean War she served on the Defense Advisory Committee on Women in the Services.\nAuthor and educator.\nGilbreth had a lifelong interest in teaching and education. As an undergraduate at the University of California, Berkeley, she took enough education courses to earn a teacher's certificate, and her doctoral dissertation at Brown University was on applying the principles of scientific management to secondary school teaching.\nWhile residing in Providence, Rhode Island, Gilbreth and her husband taught free, two-week-long summer schools in scientific management from 1913 to 1916. The Gilbreths also discussed teaching the Gilbreth System of time-and-motion study to members of industry, but it was not until after her husband's death in 1924 that she created a formal motion-study course. Gilbreth presented this idea at the First Prague International Management Congress in Prague in July 1924. Her first course began in January 1925. Gilbreth's classes offered to \"prepare a member of an organization, who has adequate training both in scientific method and in plant problems, to take charge of Motion Study work in that organization.\" Coursework included laboratory projects and field trips to private firms to witness the application of scientific management. She ran a total of seven motion study courses out of her home in Montclair, New Jersey until 1930.\nTo earn additional income to support her large family, Gilbreth delivered numerous addresses to business and industry gatherings, as well as on college and university campuses such as Harvard, Yale, Colgate, the University of Michigan, MIT, Stanford, and Purdue University. In 1925 she succeeded her husband as a visiting lecturer at Purdue, where he had been delivering annual lectures. In 1935 she became a professor of management at Purdue's School of Mechanical Engineering, and the country's first female engineering professor. She was promoted to a full professor at Purdue in 1940. Gilbreth divided her time between Purdue's departments of industrial engineering, industrial psychology, home economics, and the dean's office, where she consulted on careers for women. In cooperation with Marvin Mundel, Gilbreth established and supervised a time-and-motion-study laboratory at Purdue's School of Industrial Engineering. She also demonstrated how time-and-motion studies could be used in agricultural studies and later transferred motion-study techniques to the home economics department under the banner of \"work simplification\". Gilbreth retired from Purdue's faculty in 1948.\nAfter Gilbreth's retirement from Purdue, she continued to travel and deliver lectures. She also taught at several other colleges and universities, and became head of the Newark College of Engineering in 1941. Gilbreth was appointed the Knapp Visiting Professor at the University of Wisconsin's School of Engineering in 1955. She also taught at Bryn Mawr College and Rutgers University. Whilst teaching at Bryn Mawr, she met then student of social economy, Anne Gillespie Shaw, who later worked for Gilbreth Management Consultants, doing commercial research studies and became a lifelong friend and colleague. In 1964, at the age of eighty-six, Gilbreth became resident lecturer at Massachusetts Institute of Technology. In 1968, when her health finally began to fail, Gilbreth retired from her active public life and eventually entered a nursing home.\nDeath and legacy.\nGilbreth died of a stroke on January 2, 1972, in Phoenix, Arizona at the age of ninety-three. Her ashes were scattered at sea.\nGilbreth was best known for her work as an industrial engineer and a pioneer in the field of management theory. Dubbed \"America's first lady of engineering,\" she brought her training in psychology to time-and-motion studies and demonstrated how companies and industries could improve their management techniques, efficiency, and productivity. Gilbreth's extensive research and writings on her own and in collaboration with her husband emphasized \"the human element in scientific management.\" Her expertise and major contribution to the field of scientific management was integrating the psychological and mental processes with the time-and-motion studies. She also helped make these types of studies widely accepted. In addition, Gilbreth was among the first to establish industrial engineering curricula in college and university engineering schools. Gilbreth's book, \"The Psychology of Management\" (1914), was an early major work in the history of engineering thought and the first to combine psychology with elements of management theory. Major repositories of Gilbreth materials are at the Archives Center of the Smithsonian National Museum of American History in Washington, D.C., and at Purdue University Library, Archives and Special Collections, at West Lafayette, Indiana.\nGilbreth also made contributions on behalf of women. Her pioneering work in industrial engineering influenced women in the field. In addition to her lectures on various engineering topics, she encouraged women to study industrial engineering and management. Purdue awarded its first PhD in engineering to a woman in 1950, two years after Gilbreth retired from the university.\nSeveral engineering awards have been named in Gilbreth's honor. The National Academy of Engineering established the Lillian M. Gilbreth Lectureships in 2001 to recognize outstanding young American engineers. The highest honor bestowed by the Institute of Industrial Engineers is the Frank and Lillian Gilbreth Industrial Engineering Award for \"those who have distinguished themselves through contributions to the welfare of mankind in the field of industrial engineering\". The Lillian M. Gilbreth Distinguished Professor award at Purdue University is bestowed on a member of the industrial engineering department. The Society of Women Engineers awards the Lillian Moller Gilbreth Memorial Scholarship to female engineering undergraduates.\nTwo of the Gilbreth children also paid tribute to their mother in books about their family life. \"Cheaper by the Dozen\" (1948), a bestseller by Gilbreth's son, Frank Jr., and daughter, Ernestine, was made into a motion picture in 1950 starring Myrna Loy as Lillian and Clifton Webb as Frank. The book's sequel, \"Belles on Their Toes\" (1950), also written by Frank Jr. and Ernestine, was made into a motion picture sequel in 1952. Frank Jr. also paid tribute to his mother in \"Time Out for Happiness\" (1972).\nIn 1962, the Industrial Engineering building at the University of Rhode Island that was built the previous year was dedicated to https:// for their pioneering work in industrial engineering.\nIn 2018, the College of Engineering at Purdue University established the prestigious Lillian Gilbreth Postdoctoral Fellowship Program to attract and prepare outstanding individuals with recently awarded Ph.D.'s for a career in engineering academia through interdisciplinary research, training, and professional development.\nAwards and honors.\nGilbreth received numerous awards and honors for her contributions.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39482", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=39482", "title": "Mai Zetterling", "text": "Swedish actress (1925\u20131994)\nMai Elisabeth Zetterling (; 24 May 1925 \u2013 17 March 1994) was a Swedish film director, novelist and actress.\nEarly life.\nZetterling was born in V\u00e4ster\u00e5s, Sweden to a working class family. She started her career as an actor at the age of 17 at the Royal Dramatic Theatre, the Swedish national theatre, appearing in war-era films.\nCareer.\nZetterling appeared in film and television productions spanning six decades from the 1940s to the 1990s. Her breakthrough as an actress came in the 1944 film \"Torment\" written for her by Ingmar Bergman, in which she played a controversial role as a tormented shopgirl. Shortly afterwards, she moved to England and gained instant success there with her title role in Basil Dearden's \"Frieda\" (1947), playing opposite David Farrar.\nAfter a brief return to Sweden, in which she worked with Bergman again in his film \"Music in Darkness\" (1948), she returned to Britain and starred in a number of UK films. Some of her notable films as an actress include \"Quartet\" (1948), a film based on some of W. Somerset Maugham's short stories, \"The Romantic Age\" (1949) directed by Edmond T. Gr\u00e9ville, \"Only Two Can Play\" (1962) co-starring Peter Sellers and directed by Sidney Gilliat, and \"The Witches\" (1990), an adaptation of Roald Dahl's book directed by Nicolas Roeg. Having gained a reputation as a sex symbol in dramas and thrillers, she was equally effective in comedies, and was active in British television in the 1950s and 1960s.\nIn 1960, she appeared in \"Danger Man\" as Nadia in the episode \"The Sisters\".\nShe began directing and publishing novels and non-fiction in the early 1960s, her films starting with political documentaries and a short film titled \"The War Game\" (1963), which was nominated for a BAFTA award, and won a Silver Lion at Venice, both for the Best Short Film. Her directorial feature film debut \"\u00c4lskande par\" (1964, \"Loving Couples\"), based on the novels of Agnes von Krusenstjerna, caused a scandal at the 1965 Cannes Film Festival for its sexual explicitness and nudity. Kenneth Tynan of \"The Observer\" later called it \"one of the most ambitious debuts since \"Citizen Kane\"\". It was not the only film she made that caused controversy for its frank sexuality.\nWhen critics reviewing her debut feature stated that \"Mai Zetterling directs like a man\", she began to explore feminist themes more explicitly in her work. \"The Girls\", which had an all-star Swedish cast that included Bibi Andersson and Harriet Andersson, discussed women's liberation (or lack thereof) in a society controlled by men, as the protagonists compare their lives to characters in the play \"Lysistrata\", and find that things have not progressed very much for women since ancient times. In 1966, she appeared as a storyteller on the BBC children's programme \"Jackanory\", and in five episodes narrated Tove Jansson's \"Finn Family Moomintroll\".\nPersonal life.\nZetterling was married to Norwegian actor Tutte Lemkow from 1944 to 1953. They had a daughter, Etienne and a son, Louis, who is professor of environmental sociology at the Autonomous University of Barcelona. She published an autobiography, \"All Those Tomorrows\". From 1958 to 1979, she was married to British author David Hughes, who collaborated with her on her first films as director. \nDocuments at the National Archives in London show that, as a member of the Hollywood Left, she was watched by MI5 as a suspected Communist. It did not hamper her career, however. \nDeath.\nOn 17 March 1994, a year after her final role on television, Zetterling died from cancer at her home in London. She was 68 years old.\nFilmography.\nAs Director\nAs Actor\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39483", "revid": "48983", "url": "https://en.wikipedia.org/wiki?curid=39483", "title": "Nicolas Copernicus", "text": ""}
{"id": "39484", "revid": "23316696", "url": "https://en.wikipedia.org/wiki?curid=39484", "title": "1499", "text": "Calendar year\nYear 1499 (MCDXCIX)9827961590 was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39485", "revid": "1302566815", "url": "https://en.wikipedia.org/wiki?curid=39485", "title": "1497", "text": "Calendar year\nThe year 1497 (MCDXCVII) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39486", "revid": "23316696", "url": "https://en.wikipedia.org/wiki?curid=39486", "title": "1496", "text": "Calendar year\nYear 1496 (MCDXCVI) was a leap year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39487", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39487", "title": "1494", "text": "Calendar year\nYear 1494 (MCDXCIV) was a common year starting on Wednesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39488", "revid": "4328878", "url": "https://en.wikipedia.org/wiki?curid=39488", "title": "1491", "text": "Calendar year\nYear 1491 (MCDXCI) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39489", "revid": "4328878", "url": "https://en.wikipedia.org/wiki?curid=39489", "title": "1490", "text": "Calendar year\nYear 1490 (MCDXC) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39490", "revid": "45925219", "url": "https://en.wikipedia.org/wiki?curid=39490", "title": "1488", "text": "Year 1488 (MCDLXXXVIII) was a leap year starting on Tuesday of the Julian calendar.\nCalendar year\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39491", "revid": "50984809", "url": "https://en.wikipedia.org/wiki?curid=39491", "title": "1489", "text": "Calendar year\nYear 1489 (MCDLXXXIX) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39492", "revid": "40283880", "url": "https://en.wikipedia.org/wiki?curid=39492", "title": "1486", "text": "Calendar year\nYear 1486 (MCDLXXXVI) was a common year starting on Sunday.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "39493", "revid": "24465790", "url": "https://en.wikipedia.org/wiki?curid=39493", "title": "1485", "text": "Calendar year\nYear 1485 (MCDLXXXV) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39494", "revid": "45925219", "url": "https://en.wikipedia.org/wiki?curid=39494", "title": "1484", "text": "Calendar year\nYear 1484 (MCDLXXXIV) was a leap year starting on Thursday of the Julian calendar, the 1484th year of the Common Era (CE) and Anno Domini (AD) designations, the 484th year of the 2nd millennium, the 84th year of the 15th century, and the 5th year of the 1480s decade.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39495", "revid": "35138333", "url": "https://en.wikipedia.org/wiki?curid=39495", "title": "1482", "text": "Calendar year\nYear 1482 (MCDLXXXII) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39496", "revid": "5718152", "url": "https://en.wikipedia.org/wiki?curid=39496", "title": "1479", "text": "Calendar year\nYear 1479 (MCDLXXIX) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nOngoing.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39497", "revid": "23316696", "url": "https://en.wikipedia.org/wiki?curid=39497", "title": "1475", "text": "Calendar year\nYear 1475 (MCDLXXV) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39498", "revid": "40625145", "url": "https://en.wikipedia.org/wiki?curid=39498", "title": "1474", "text": "Calendar year\nYear 1474 (MCDLXXIV) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39499", "revid": "46866511", "url": "https://en.wikipedia.org/wiki?curid=39499", "title": "1472", "text": "Calendar year\nYear 1472 (MCDLXXII) was a leap year starting on Wednesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39500", "revid": "50405494", "url": "https://en.wikipedia.org/wiki?curid=39500", "title": "1470", "text": "Calendar year\nYear 1470 (MCDLXX) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39501", "revid": "23316696", "url": "https://en.wikipedia.org/wiki?curid=39501", "title": "1468", "text": "Calendar year\nYear 1468 (MCDLXVIII) was a leap year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39502", "revid": "1319589457", "url": "https://en.wikipedia.org/wiki?curid=39502", "title": "1467", "text": "Calendar year\nYear 1467 (MCDLXVII) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39503", "revid": "23316696", "url": "https://en.wikipedia.org/wiki?curid=39503", "title": "1466", "text": "Calendar year\nYear 1466 (MCDLXVI) was a common year starting on Wednesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39504", "revid": "33333849", "url": "https://en.wikipedia.org/wiki?curid=39504", "title": "1464", "text": "Calendar year\nYear 1464 (MCDLXIV) was a leap year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39505", "revid": "50186069", "url": "https://en.wikipedia.org/wiki?curid=39505", "title": "1463", "text": "Calendar year\nYear 1463 (MCDLXIII) was a common year starting on Saturday of the Julian calendar, the 1463rd year of the Common Era (CE) and Anno Domini (AD) designations, the 463rd year of the 2nd millennium, the 63rd year of the 15th century, and the 4th year of the 1460s decade.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39506", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=39506", "title": "1460", "text": "Calendar year\nYear 1460 (MCDLX) was a leap year starting on Tuesday of the Julian calendar, the 1460th year of the Common Era (CE) and \"Anno Domini\" (AD) designations, the 460th year of the 2nd millennium, the 60th year of the 15th century, and the 1st year of the 1460s decade.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39507", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=39507", "title": "1456", "text": "Calendar year\nYear 1456 (MCDLVI) was a leap year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39508", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=39508", "title": "1455", "text": "Calendar year\nYear 1455 (MCDLV) was a common year starting on Wednesday (full) of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nOctober\u2013December.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39509", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=39509", "title": "1452", "text": "Calendar year\nYear 1452 (MCDLII) was a leap year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39510", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=39510", "title": "1450", "text": "Calendar year\nYear 1450 (MCDL) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39511", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39511", "title": "1349", "text": "Calendar year\nYear 1349 (MCCCXLIX) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39512", "revid": "36336112", "url": "https://en.wikipedia.org/wiki?curid=39512", "title": "1347", "text": "Calendar year\nYear 1347 (MCCCXLVII) was a common year starting on Monday of the Julian calendar, and a common year starting on Sunday of the Proleptic Gregorian calendar.\nEvents.\n&lt;onlyinclude&gt;\nAsia.\nWestern Asia.\nThe Mamluk Empire is hit by the plague in the autumn. Baghdad is hit in the same year.\nSouth Asia.\nAfter years of resistance against the Delhi Sultan Muhammad bin Tughluq, the Bahmani Kingdom, a Muslim Sultanate in the Deccan, was established on August 3, when King Ala-ud-din Hasan Bahman Shah was crowned in a mosque in Daulatabad. Later in the year, the Kingdom's capital was moved from Daulatabad to the more central Gulbarga. Southeast Asia suffered a drought which dried up an important river which ran through the capital city of the Kingdom of Ayodhya, forcing the King to move the capital to a new location on the Lop Buri River.\nEurope.\nEastern and Scandinavian.\nOn February 2 the Byzantine Empire's civil war between John VI Kantakouzenos and the regency ended with John VI entering Constantinople. On February 8, an agreement was concluded with the empress Anna of Savoy, whereby he and John V Palaiologos would rule jointly. The agreement was finalized in May when John V married Kantakouzenos' 15-year-old daughter. The war had come at a high cost economically and territorially, and much of the Empire was in need of rebuilding. To make matters worse, in May Genoese ships fleeing the Black Death in Kaffa stopped in Constantinople. The plague soon spread from their ships to the city. By autumn, the epidemic had spread throughout the Balkans, possibly through contact with Venetian ports along the Adriatic Sea. Specific cases were recorded in the northern Balkans on December 25, in the city of Split.\nAfter being proclaimed Tsar of Serbia in the previous year by the newly promoted Serbian Patriarch Joanikije II, Stefan Du\u0161an continued his southern expansion by conquering Epirus, Aetolia and Acarnania, appointing his half-brother, despot Simeon Uro\u0161 as governor of those provinces.\nCentral.\nOn May 20 Cola di Rienzo, a Roman commoner, declared himself Emperor of Rome in front of a huge crowd in response to what had been several years of power struggles among the upper-class barony. Pope Clement VI, along with several of Rome's upper-class nobility, united to drive him out of the city in November. In October, Genoese ships arrived in southern Italy with the Black Plague, beginning the spread of the disease in the region.\nJews were first accused of ritual murders in Poland in 1347. Casimir III of Poland issues Poland's first codified collection of laws after the diet of Wi\u015blica. Separate laws are codified for greater and lesser Poland.\nWestern Europe.\nIn the continuing Hundred Years' War, the English won the city of Calais in a treaty signed in September. In a meeting with the Estates General in November, the French King Phillip was told that in the recent war efforts they had \"lost all and gained nothing.\" Phillip, however, was granted a portion of the money he requested and was able to continue his war effort. The English King Edward offered Calais a package of economic boosts which would make Calais the key city connecting England with France economically. Edward returned to England at that height of his popularity and power and for six months celebrated his successes with others in the English nobility. Although the Kingdom's funds were largely pushed towards the war, building projects among the more wealthy continued, with, for example, the completion of Pembroke College in this year.\nThe French city of Marseille recognized the plague on September 1 and by November 1 it had spread to Aix-en-Provence. The earliest recorded invasion of the plague into Spanish territory was in Mallorca in December 1347, probably through commercial ships. Three years of plague began in England.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39513", "revid": "2045389", "url": "https://en.wikipedia.org/wiki?curid=39513", "title": "1345 (summary)", "text": ""}
{"id": "39514", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39514", "title": "1344", "text": "Calendar year\nYear 1344 (MCCCXLIV) was a leap year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39515", "revid": "1303137540", "url": "https://en.wikipedia.org/wiki?curid=39515", "title": "1343", "text": "Calendar year\nYear 1343 (MCCCXLIII) was a common year starting on Wednesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39516", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39516", "title": "1342", "text": "Calendar year\nYear 1342 (MCCCXLII) was a common year starting on Tuesday and current year of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39517", "revid": "18113352", "url": "https://en.wikipedia.org/wiki?curid=39517", "title": "1340", "text": "Calendar year\nYear 1340 (MCCCXL) was a leap year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39518", "revid": "18113352", "url": "https://en.wikipedia.org/wiki?curid=39518", "title": "1350", "text": "Calendar year\nYear 1350 (MCCCL) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39519", "revid": "1568174", "url": "https://en.wikipedia.org/wiki?curid=39519", "title": "1352", "text": "Calendar year\nYear 1352 (MCCCLII) was a leap year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39520", "revid": "1568174", "url": "https://en.wikipedia.org/wiki?curid=39520", "title": "1353", "text": "Calendar year\nYear 1353 (MCCCLIII) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39521", "revid": "613068", "url": "https://en.wikipedia.org/wiki?curid=39521", "title": "1354", "text": "Calendar year\nYear 1354 (MCCCLIV) was a common year starting on Wednesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39522", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39522", "title": "1355", "text": "Calendar year\nYear 1355 (MCCCLV) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39523", "revid": "11096", "url": "https://en.wikipedia.org/wiki?curid=39523", "title": "1356", "text": "Calendar year\nYear 1356 (MCCCLVI) was a leap year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nTillg\u00e4nglig p\u00e5 Internet: https://"}
{"id": "39524", "revid": "6104108", "url": "https://en.wikipedia.org/wiki?curid=39524", "title": "1357", "text": "Calendar year\nYear 1357 (MCCCLVII) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39525", "revid": "45523345", "url": "https://en.wikipedia.org/wiki?curid=39525", "title": "1358", "text": "Calendar year\nYear 1358 (MCCCLVIII) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39526", "revid": "45902543", "url": "https://en.wikipedia.org/wiki?curid=39526", "title": "1361", "text": "Calendar year\nYear 1361 (MCCCLXI) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39527", "revid": "1568174", "url": "https://en.wikipedia.org/wiki?curid=39527", "title": "1362", "text": "Calendar year\nYear 1362 (MCCCLXII) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39528", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39528", "title": "1363", "text": "Calendar year\nYear 1363 (MCCCLXIII) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39529", "revid": "50928572", "url": "https://en.wikipedia.org/wiki?curid=39529", "title": "1992 United States presidential election", "text": "The 1992 United States presidential election was the presidential election, held in the United States, on November 3, 1992. The Democratic ticket of Arkansas governor Bill Clinton and Senator from Tennessee Al Gore defeated the Republican ticket of incumbent president George H. W. Bush and vice president Dan Quayle and the independent ticket of businessman Ross Perot and vice admiral James Stockdale. The election marked the end of 12 consecutive years of Republican rule of the White House, as well as the end of a longer period of Republican dominance in American presidential politics that began in 1968, with the exception of Jimmy Carter's narrow victory in 1976.\nBush had alienated many conservatives in his party by breaking his 1988 campaign , but he fended off a primary challenge from paleoconservative commentator Pat Buchanan without losing a single contest. Bush's popularity following his success in the Gulf War dissuaded high-profile Democratic candidates such as Mario Cuomo from entering the 1992 Democratic primaries. Clinton, a leader of the centrist Democratic Leadership Council, established himself as the front-runner for the Democratic nomination by sweeping the Super Tuesday primaries. He defeated former governor of California Jerry Brown, former Massachusetts senator Paul Tsongas, and other candidates to win the nomination, and chose Tennessee senator Al Gore as his running mate. The billionaire Perot launched an independent campaign, emphasizing his opposition to the North American Free Trade Agreement (which at time was being actively negotiated) and his plan to reduce the national debt.\nThe economy had recovered from a recession in the spring of 1991, followed by 19 consecutive months of growth, but perceptions of the economy's slow growth harmed Bush, for he had inherited a substantial economic boom from his predecessor Ronald Reagan. Bush's greatest strength, foreign policy, was regarded as much less important following the dissolution of the Soviet Union and the end of the Cold War, as well as the relatively peaceful climate in the Middle East after the Gulf War. Perot led in several polls taken in June 1992, but severely damaged his candidacy by temporarily dropping out of the race in July. The Bush campaign criticized Clinton's character and emphasized Bush's foreign policy successes, while Clinton focused on the economy.\nClinton won a plurality in the popular vote and a majority of the electoral vote, breaking a streak of three consecutive Republican victories. Clinton flipped a total of 22 states that had voted Republican in the election of 1988. Perot won 18.9% of the popular vote, the highest share of the vote won by a candidate outside of the two major parties since 1912.\nDemocratic Party nomination.\nOverview.\nFollowing the successful performance by U.S. and coalition forces in the Persian Gulf War, President George H. W. Bush's approval ratings were 89%. His re-election was considered very likely; several high-profile candidates, such as Mario Cuomo and Jesse Jackson, refused to seek the Democratic nomination. U.S. Senator Al Gore from Tennessee refused to seek the nomination due to the fact his son had been struck by a car and was undergoing surgery and physical therapy; however, Tom Harkin, Paul Tsongas, Jerry Brown, Larry Agran, Bob Kerrey, Douglas Wilder, and Bill Clinton chose to run as candidates.\nHarkin, a U.S. Senator from Iowa, ran as a populist liberal with labor union support. Tsongas, a former U.S. Senator from Massachusetts, highlighted his political independence and fiscal conservatism. Jerry Brown, the former California governor who had run for the Democratic nomination in 1976 and 1980, declared a significant reform agenda, including Congressional term limits, campaign finance reform, and the adoption of a flat income tax. Nebraska senator Bob Kerrey was an attractive candidate based on his business and military background, but made several gaffes on the campaign trail. Arkansas governor Bill Clinton positioned himself as a centrist, or New Democrat. He was relatively unknown nationally before the primary season. That quickly changed when Gennifer Flowers alleged an extramarital affair. Clinton denied the story, appearing on \"60 Minutes\" with his wife, Hillary Clinton; later in 1998, he admitted the affair.\nThe primary began with Harkin winning his native Iowa as expected. Tsongas won the New Hampshire primary on February 18, but Clinton's second-place finish, helped by his speech labeling himself \"The Comeback Kid\", energized his campaign. Brown won the Maine caucus and Kerrey won South Dakota. Clinton won his first primary in Georgia. Tsongas won the Utah and Maryland primaries and a caucus in Washington. Harkin won caucuses in Idaho and Minnesota while Brown won Colorado. Kerrey dropped out two days later. Clinton won the South Carolina and Wyoming primaries and Tsongas won Arizona. Harkin dropped out. Brown won the Nevada caucus. Clinton swept nearly all of the Super Tuesday primaries on March 10 making him the solid front runner. Clinton won the Michigan and Illinois primaries. Tsongas dropped out after finishing third in Michigan; however, Brown began to pick up steam, aided by using a phone number to receive funding from small donors. Brown scored surprising wins in Connecticut, Vermont and Alaska. As the race moved to the primaries in New York and Wisconsin, Brown had taken the lead in polls in both states. Then he made a serious gaffe by announcing to an audience of New York City's Jewish community that he would consider Reverend Jackson as a vice presidential candidate; Jackson had offended many Jewish people with remarks he had made during his own presidential campaigns. Clinton won dramatically in New York (41%\u201326%) and closely in Wisconsin (37%\u201334%). Clinton then proceeded to win a long streak of primaries leading up to Brown's home state of California. Clinton won this state 48% to 41% and secured the delegates needed to lock the nomination.\nThe convention met in New York City, and the official tally was:\nClinton chose Gore to be his running mate on July 9, 1992. Choosing fellow Southerner Gore went against the popular strategy of balancing a Southern candidate with a Northern partner. Gore served to balance the ticket in other ways, as he was perceived as strong on family values and environmental issues, while Clinton was not. Gore's similarities to Clinton also allowed him to push some of his key campaign themes, such as centrism and generational change.\nRepublican Party nomination.\nPaleoconservative journalist Pat Buchanan was the primary opponent of President Bush; Ron Paul, the Libertarian Party's presidential nominee in 1988, had planned to run against the President, but dropped out shortly after Buchanan's entry in December. Buchanan's best showing was in the New Hampshire primary on February 18, 1992\u2014where Bush won by a 53\u201338% margin. President Bush won 73% of all primary votes, with 9,199,463 votes. Buchanan won 2,899,488 votes; unpledged delegates won 287,383 votes, and David Duke, Grand Wizard of the Ku Klux Klan, won 119,115 votes. Just over 100,000 votes were cast for all other candidates, half of which were write-in votes for H. Ross Perot. Former Minnesota governor Harold Stassen, who had run for president nine times since 1944, also mounted his final campaign.\nPresident George H. W. Bush and Vice President Dan Quayle easily won renomination by the Republican Party. However, the success of the opposition forced the moderate Bush to move further to the right than in the previous election, and to incorporate many socially conservative planks in the party platform. Bush allowed Buchanan to give a prime time address at the Republican National Convention in Houston, Texas, and his \"Culture War\" speech alienated liberal Republicans.\nWith intense pressure on the Buchanan delegates to relent, the tally for president went as follows:\nVice President Dan Quayle was renominated by voice vote.\nRoss Perot candidacy.\nThe public's concern about the federal budget deficit and fears of professional politicians led to the independent candidacy of billionaire Texan Ross Perot increasing in popularity in the polls\u2014at one point Perot was leading the major party candidates. Perot crusaded against the North American Free Trade Agreement (NAFTA), and internal and external national debt, tapping into voters' potential fear of the deficit. His volunteers succeeded in collecting enough signatures to get his name on the ballot in all 50 states. In June, Perot led the national public opinion polls with support from 39% of the voters (versus 31% for Bush and 25% for Clinton). Perot severely damaged his credibility by dropping out of the presidential contest in July and remaining out of the race for several weeks before re-entering. He compounded this damage by eventually claiming, without evidence, that his withdrawal was due to Republican operatives attempting to disrupt his daughter's wedding.\nPerot and retired Vice Admiral James Stockdale drew 19,743,821 votes (19% of the popular vote).\nMinor parties and independents.\nLibertarian Party nomination.\nLibertarian candidates:\nThe 6th Libertarian Party National Convention was held in Chicago, Illinois. There, the Libertarian Party nominated Andre Marrou, former Alaska State Representative and the Party's 1988 vice presidential candidate, for president. Nancy Lord was his running mate.\nMarrou and Lord drew 291,627 votes (0.28% of the popular vote).\nNew Alliance Party nomination.\nNew Alliance candidate:\nLenora Fulani, who was the 1988 presidential nominee of the New Alliance Party, received a second consecutive nomination from the Party in 1992. Unlike in 1988, Fulani failed to gain ballot access in every state, deciding to concentrate some of that campaign funding towards exposure of her candidacy and the Party to the national public.\nFulani also sought the endorsement of the Peace and Freedom Party of California, but despite winning a majority in that party's primary, she would lose the nomination to Ronald Daniels, the former Director the National Rainbow Coalition. Rather than pursuing a ballot space of her own, Fulani would endorse Daniels's candidacy in California.\nFulani and her running mate Maria Elizabeth Mu\u00f1oz received 73,622 votes (0.1% of the popular vote).\nNatural Law Party nomination.\nThe newly formed Natural Law Party nominated scientist and researcher John Hagelin for president and Mike Tompkins for vice president. The Natural Law Party had been founded in 1992 by Hagelin and 12 others who felt that governmental problems could be solved more effectively by following \"Natural Laws\". The party platform included preventive health care, sustainable agriculture and renewable energy technologies. During this and future campaigns, Hagelin favored abortion rights without public financing, campaign finance law reform, improved gun control, a flat tax, the eradication of PACs, a ban on soft money contributions, and school vouchers.\nThe party's first presidential ticket appeared on the ballot in 28 states and drew 37,137 votes (&lt;0.1% of the popular vote).\nU.S. Taxpayers' Party nomination.\nU.S. Taxpayers' candidates:\nThe U.S. Taxpayers Party ran its first presidential ticket in 1992, having only been formed the prior year. Initially Howard Phillips had hoped to successfully entice a prominent conservative politician, such as the former senator Gordon J. Humphrey from New Hampshire, or even Patrick Buchanan who at the time had only been mulling over running against President Bush (he would officially declare in December 1991).\nNo one, however, announced any intention to seek the Taxpayers Party nomination; Buchanan himself in the end endorsed President Bush at the Republican National Convention in Houston. Phillips had been unofficially nominated earlier in the year so as to allow the Party to be able to seek ballot access properly. While initially a temporary post, it was made permanent at the party's national convention, which was held in New Orleans on September 4 and 5. At the convention, which was attended by delegates from thirty-two states and Washington, D.C., Phillips received 264 votes on the first ballot, while Albion Knight was approved as his running mate by acclamation.412\nEarlier that year, in the June 2 California primary, Phillips had received 15,456 votes in the American Independent Party primary. On August 30, the American Independent Party voted to affiliate with the U.S. Taxpayers Party, an affiliation which continued until 2008.378\nPhillips and Knight drew 43,369 votes (&lt;0.1% of the popular vote).\nPopulist Party nomination.\nPopulist candidate:\nFormer United States Army Special Forces officer and Vietnam veteran Bo Gritz was the nominee of the Populist Party, facing virtually no opposition. Under the campaign slogan \"God, Guns and Gritz\" and publishing his political manifesto \"The Bill of Gritz\" (playing on his last name rhyming with \"rights\"), he called for staunch opposition to what he called \"global government\" and \"The New World Order\", ending all foreign aid, abolishing federal income tax, and abolishing the Federal Reserve System. During the campaign, Gritz openly proclaimed the United States to be a \"Christian Nation\", stating that the country's legal statutes \"should reflect unashamed acceptance of Almighty God and His Laws\". His run on the America First/Populist Party ticket was prompted by his association with another far-right political Christian talk radio host, Tom Valentine. During his campaign, part of Gritz's standard stump speech was an idea to pay off the national debt by minting a coin at the Treasury and sending it to the Federal Reserve. This predates the 2012 trillion-dollar coin concept.\nDuring August 1992, Gritz attracted national attention as mediator during the government standoff with Randy Weaver at Ruby Ridge, Idaho.\nHe received 106,152 votes nationwide (0.1% of the popular vote). In two states he had a respectable showing for a minor third-party candidate: Utah, where he received 3.8% of the vote and Idaho, where he received 2.1% of the vote. In some counties, his support topped 10%, and in Franklin County, Idaho, was only a few votes away from pushing Bill Clinton into fourth place in the county.\nLyndon LaRouche's candidacy.\nWhile officially running for the Democratic presidential nomination, Lyndon LaRouche also decided to run as an Independent in the general election, standing as the National Economic Recovery candidate. LaRouche was in jail at the time, having been convicted of conspiracy to commit mail fraud in December 1988; it was only the second time in history that the presidency was sought from a prison cell (after Socialist Party candidate Eugene V. Debs, while imprisoned for his opposition to U.S. involvement in World War I, ran in 1920). His running-mate was James Bevel, a civil rights activist who had represented the LaRouche movement in its pursuit of the Franklin child prostitution ring allegations.\nIn addition to the displayed states, LaRouche had nearly made the ballot in the states of New York and Mississippi. In the case of New York, while his petition was valid and had enough signatures, none of his electors filed declarations of candidacy; in the cases of Mississippi a sore-loser law was in place, and because he ran in that state's Democratic presidential primary he was ineligible to run as an Independent in the general. Ohio also had a sore-loser law, but it was ruled in Brown vs. Taft that it did not apply to presidential candidates.\nLaRouche and Beval drew 22,863 votes. (&lt;0.1% of the popular vote).\nSocialist Workers' Party nomination.\nSocialist Workers candidate:\nJames Warren, who was the 1988 presidential nominee of the Socialist Workers Party, received a second consecutive nomination from the Party on the first of November 1991. Warren had two running mates that varied from state to state; Estelle DeBates and Willie Mae Reid, the latter also a resident of Illinois.\nWarren received 22,882 votes (&lt;0.1% of the popular vote).\nRon Daniels candidacy.\nRonald Daniels was the former executive director for the Center for Constitutional Rights, the former director of the National Rainbow Coalition, and the worked on both of Jesse Jackson's campaigns for the Democratic presidential nomination. Asiba Tupahache, a Native American activist from New York was his running-mate.\nThough running an Independent campaign under the label \"Campaign for a Better Tomorrow\", Daniels was endorsed by a number of third parties across the states, most notably the Peace and Freedom Party of California; though he had lost that party's presidential primary to Lenora Fulani, the nominee of the New Alliance Party, the delegates at its convention voted in favor of his candidacy 110\u201391, the only time it has ever nominated someone other than the winner of the primary.\nDaniels and Tupachache drew 27,396 votes (&lt;0.1% of the popular vote).\nOther nominations.\nThe 1992 campaign also marked the entry of Ralph Nader into presidential politics as a candidate. Despite the advice of several liberal and environmental groups, Nader did not formally run. Rather, he tried to make an impact in the New Hampshire primaries, urging members of both parties to write-in his name. As a result, several thousand Democrats and Republicans wrote-in Nader's name. Despite supporting mostly liberal legislation during his career as a consumer advocate, Nader received more votes from Republicans than Democrats.\nThe Worker's League nominated Helen Halyard for president; she was the party's nominee for vice president in 1984 and 1988. Fred Mazelis was nominated for vice president. Halyard and Mazelis drew 3,050 votes.\nJohn Viamouyiannis candidacy.\nBallot access: Michigan, New Jersey \"(33 Electoral)\"\nJohn Yiamouyiannis, a major opponent of water fluoridation, ran as an Independent under the label \"Take Back America\". Allen C. McCone was his running-mate. Yiamouyiannis and McCone drew 2,199 votes.\nSocialist Party nomination.\nBallot access: Arkansas, Iowa, Louisiana, Tennessee \"(33 Electoral)\"\nThe Socialist Party nominated J. Quinn Brisben for president and Barbara Garson for vice president. Brisben and Garson drew 2,909 votes.\nGrassroots Party nomination.\nBallot access: DC, Tennessee, Utah, Wisconsin \"(30 Electoral)\"\nThe Grassroots Party nominated Jack Herer, a noted cannabis activist for president and Derrick Grimmer for vice president. Herer and Grimmer drew 3,875 votes.\nProhibition Party nomination.\nBallot access: Iowa, Minnesota, Wisconsin \"(28 Electoral)\"\nThe Prohibition Party nominated Earl Dodge, the party's chairman for president and George Ormsby for vice president. Dodge and Ormsby drew 935 votes.\nDrew Bradford candidacy.\nBallot access: Arkansas, New Mexico, Tennessee \"(22 Electoral)\"\nDrew Bradford was an Independent candidate for the Presidency; he did not have a running-mate. Bradford drew 4,749 votes.\nDelbert Ehlers candidacy.\nBallot access: Wisconsin \"(11 Electoral)\"\nDelbert Ehlers was an Independent candidate for the Presidency. His running-mate was Rick Wendt. Ehlers and Wendt drew 1,149 votes.\nGeneral election.\nCampaign.\nAfter Bill Clinton secured the Democratic Party's nomination in the spring of 1992, polls showed Ross Perot leading the race, followed by President Bush and Clinton in third place after a grueling nomination process. Two-way trial heats between Bush and Clinton in early 1992 showed Bush in the lead. As the economy continued to sour and the President's approval rating continued to slide, the Democrats began to rally around their nominee. On July 9, 1992, Clinton chose Tennessee senator and former 1988 presidential candidate Al Gore to be his running mate. As Governor Clinton's nomination acceptance speech approached, Ross Perot dropped out of the race, convinced that staying in the race with a \"revitalized Democratic Party\" would cause the race to be decided by the United States House of Representatives. Clinton gave his acceptance speech on July 16, 1992, promising to bring a \"new covenant\" to America, and to work to heal the gap that had developed between the rich and the poor during the Reagan/Bush years. The Clinton campaign received the biggest convention \"bounce\" in history which brought him from 25% in the spring, behind Bush and Perot, to 55% versus Bush's 31%.\nAfter the convention, Clinton and Gore began a bus tour around the United States, while the Bush/Quayle campaign began to criticize Clinton's character, highlighting accusations of infidelity and draft dodging. The Bush campaign emphasized its foreign policy successes such as Desert Storm, and the end of the Cold War. Bush also contrasted his military service to Clinton's lack thereof, and criticized Clinton's lack of foreign policy expertise. However, as the economy was the main issue, Bush's campaign floundered across the nation, even in strongly Republican areas, and Clinton maintained leads with over 50% of the vote nationwide consistently, while Bush typically saw numbers in the upper 30s. As Bush's economic edge had evaporated, his campaign looked to energize its socially conservative base at the 1992 Republican National Convention in Houston, Texas. At the convention, Bush's primary campaign opponent Pat Buchanan gave his famous , criticizing Clinton's and Gore's social progressiveness, and voicing skepticism on his \"New Democrat\" brand. After President Bush accepted his renomination, his campaign saw a small bounce in the polls, but this was short-lived, as Clinton maintained his lead. The campaign continued with a lopsided lead for Clinton through September, until Ross Perot decided to re-enter the race. Ross Perot's re-entry in the race was welcomed by the Bush campaign, as Fred Steeper, a poll taker for Bush, said, \"He'll be important if we accomplish our goal, which is to draw even with Clinton.\" Initially, Perot's return saw the Texas billionaire's numbers stay low, until he was given the opportunity to participate in a trio of unprecedented three-man debates. The race narrowed, as Perot's numbers significantly improved as Clinton's numbers declined, while Bush's numbers remained more or less the same from earlier in the race as Perot and Bush began to hammer at Clinton on character issues once again.\nPresidential debates.\nThe Commission on Presidential Debates organised four presidential debates\nCharacter issues.\nMany character issues were raised during the campaign, including allegations that Clinton had dodged the draft during the Vietnam War, and had used marijuana, which Clinton claimed he had \u2018tried\u2019 to smoke, but \"didn't inhale.\" Bush also accused Clinton of meeting with communists on a trip to Russia he took as a student. Clinton was often accused of being a philanderer by political opponents.\nAllegations were also made that Bill Clinton had engaged in a long-term extramarital affair with Gennifer Flowers. Clinton denied ever having an affair with Flowers, but later admitted, under threat of perjury, that he had a brief sexual encounter with her in 1977.\nResults.\nClinton was declared the winner by CBS, NBC, and CNN at 10:48 P.M. EST and by ABC at 10:50 P.M. EST. He received 370 electoral votes from 32 states and D.C. and 43% of the popular vote while Bush received 168 electoral votes from 18 states and 37.4% of the popular vote and Perot received 18.9% of the popular vote and no electoral votes. The remaining candidates took 0.6% of the popular vote. It was the first time since 1968 that a candidate won the White House with under 50% of the popular vote. Only Washington, D.C., and Clinton's home state of Arkansas gave the majority of their votes to a single candidate in the entire country; the rest were won by pluralities of the vote. Clinton was the first Democrat since 1964 to win a majority of states.\nEven though Clinton received roughly 3,100,815 more votes than Democratic nominee Michael Dukakis had four years earlier, the Democrats recorded a 2.7 percentage point \"decrease\" in their share of the popular vote compared to 1988 due to the higher turnout. His 43% share of the popular vote was the second-lowest for any winning candidate in the 20th century after Woodrow Wilson in 1912 (41.8%). President Bush's 37.4% was the lowest percentage total for a sitting president seeking re-election since William Howard Taft, also in 1912 (23.2%). 1992 was, as the 1912 election was, a three-way race (that time between Taft, Wilson, and Theodore Roosevelt). It was also the lowest percentage for a major-party candidate since Alf Landon received 36.5% of the vote in 1936. Bush had a lower percentage of the popular vote than even Herbert Hoover, who was defeated in 1932 (39.7%). However, none of these races included a major third-party candidate.\nIndependent candidate Ross Perot received 19,743,821 with 18.9% of the popular vote. The billionaire used his own money to advertise extensively, and is the only non-major party candidate and the only non-party affiliated candidate ever allowed into the nationally televised presidential debates with both major party candidates (independent John Anderson debated Republican Ronald Reagan in 1980, but without Democrat Jimmy Carter, who had refused to appear in a three-man debate). Speaking about the North American Free Trade Agreement, Perot described its effect on American jobs as causing a \"giant sucking sound\". For a period of time, Perot was leading in the polls, but he lost much of his support when he temporarily withdrew from the election, only to declare himself a candidate again soon after. This was also the most recent time that a non-major party candidate and a non-party affiliated candidate won at least one county.\nPerot's 18.9% of the popular vote made him the most successful non-major party presidential candidate in terms of popular vote since Theodore Roosevelt in the 1912 election. His share of the popular vote was also the highest ever for a candidate who did not win any electoral votes. Although he did not win any states, Perot managed to finish ahead of one of the major party candidates in two states: In Maine, he received 30.44% of the vote to Bush's 30.39% (Clinton won Maine with 38.77%); in Utah, which Bush won with 43.36% of the popular vote, Perot collected 27.34% of the vote to Clinton's 24.65%. Perot also came in 2nd in Maine's 2nd Congressional District, where he had his best overall showing, winning 33.2% of the vote there and missing the district's 1 elector by only 4.6% of the vote.\nThe election was the most recent in which Montana voted for the Democratic candidate, both the last time Florida backed the losing candidate and last time Georgia voted for the Democratic candidate until 2020, and the last time that Colorado voted Democratic until 2008. This was also the first time since Texas' admission to the Union in 1845 that a Democrat won the White House without winning the state, and the second time a Democrat won the White House without North Carolina (the first was 1844), and the second time since Florida's admission (also in 1845) that a Democrat won without winning the state (John F. Kennedy in 1960 was the first).\nClinton was also the only Democrat at that point to win every electoral vote in the Northeast except for Lyndon Johnson in 1964. John Kerry and Barack Obama have been the only Democrats to repeat this since. Also, this was the first time since 1964 that the following nine states had voted Democratic: California, Colorado, Illinois, Montana, Nevada, New Hampshire, New Jersey, New Mexico, and Vermont.\nThe 168 electoral votes received by Bush, added to the 426 electoral votes he received in 1988, gave him the most total electoral votes received by any candidate who was elected to the office of president only once (594), and the tenth largest number of electoral votes received by any candidate who was elected to the office of president behind Grover Cleveland's 664, Barack Obama's 697, Woodrow Wilson's 712, Bill Clinton's 749, Donald Trump's 848, Dwight Eisenhower's 899, Ronald Reagan's 1,015, Richard Nixon's 1,040 and Franklin D. Roosevelt's 1,876 total electoral votes.\nBush and Clinton achieved an exact tie in Ware County, Georgia, the most recent time in American history a county was tied between the two major-party presidential candidates. Perot also achieved an exact tie with Bush in Morris County, Kansas.\nAnalysis.\nSeveral factors made the results possible. First, the campaign came on the heels of an economic slowdown. Exit polling showed that 75% thought the economy was in fairly or very bad shape while 63% thought their personal finances were better or the same as four years ago. The decision by Bush to accept a tax increase adversely affected his re-election bid. Pressured by rising budget deficits, Bush agreed to a budget compromise with Congress which raised taxes and reduced the federal budget deficit. Clinton was able to condemn the tax increase effectively on both its own merits and as a reflection of Bush's dishonesty. Effective Democratic TV ads were aired showing a clip of Bush's 1988 acceptance speech in which he promised \"\" Most importantly, Bush's coalition was in disarray, for both the aforementioned reasons and for unrelated reasons. The end of the Cold War allowed old rivalries among conservatives to re-emerge and meant that other voters focused more on domestic policy, to the detriment of Bush, a social and fiscal moderate. The consequence of such a perception depressed conservative turnout.\nThe election was compared to the 1945 United Kingdom general election, in which Winston Churchill, while a respected conservative wartime leader (like Bush) was not regarded as a good peacetime leader, and thus was voted out once the conflict was over.\nUnlike Bush, Clinton was able to unite his fractious and ideologically diverse party behind his candidacy, even when its different wings conflicted. To garner the support of moderates and conservative Democrats, he attacked Sister Souljah, an obscure rap musician whose lyrics Clinton condemned. Furthermore, Clinton made clear his support of the death penalty and would later champion making school uniforms in public schools a requirement. Clinton could also point to his centrist record as governor of Arkansas. More liberal Democrats were impressed by Clinton's record on abortion and affirmative action. His strong connections to African Americans also played a key role. In addition, he organized significant numbers of young voters and became a symbol of the rise of the baby boomer generation to political power. Supporters remained energized and confident, even in times of scandal or missteps.\nThe effect of Ross Perot's candidacy has been a contentious point of debate for many years. In the ensuing months after the election, various Republicans asserted that Perot had acted as a spoiler, enough to the detriment of Bush to lose him the election. While many disaffected conservatives may have voted for Ross Perot to protest Bush's tax increase, further examination of the Perot vote in the Election Night exit polls not only showed that Perot siphoned votes nearly equally among Bush and Clinton, but roughly two-thirds of those voters who cited Bush's broken \"No New Taxes\" pledge as \"very important\" (25%) voted for Bill Clinton. The voting numbers reveal that to win the electoral vote Bush would have had to win 10 of the 11 states Clinton won by less than five percentage points. For Bush to earn a majority of the popular vote, he would have needed 12.2% of Perot's 18.9% of the vote, 65% of Perot's support base. State exit polls suggested that Perot did not alter the electoral college count, except potentially in one state (Ohio), which nonetheless showed a result in the margin of error. Furthermore, Perot was most popular in states that strongly favored either Clinton or Bush, limiting his real electoral impact for either candidate.\nPerot gained relatively little support in the Southern states and happened to have the best showing in states with few electoral votes. Perot appealed to disaffected voters all across the political spectrum who had grown weary of the two-party system. NAFTA played a role in Perot's support, and Perot voters were relatively moderate on hot-button social issues. A 1999 study in the \"American Journal of Political Science\" estimated that Perot's candidacy hurt the Clinton campaign, reducing \"Clinton's margin of victory over Bush by seven percentage point.\" In 2016, \"FiveThirtyEight\" noted that it was \"unlikely\" that Perot was a spoiler.\nClinton, Bush, and Perot did not focus on abortion during the campaign. Exit polls, however, showed that attitudes toward abortion \"significantly influenced\" the vote, as pro-choice Republicans defected from Bush.\nThe South was the only region where Bush received a majority of the electoral votes and two-thirds of his total electoral votes came from the South.\nImplications.\nAccording to Seymour Martin Lipset, this election had several unique characteristics. Voters felt that economic conditions were worse than they actually were, which harmed Bush. A rare event was a strong third-party candidate. Liberals launched a backlash against 12 years of a conservative White House. The chief factor was Clinton's uniting his party, and winning over a number of heterogeneous groups.\nClinton's election ended an era in which the Republican Party had controlled the White House for 12 consecutive years, and for 20 of the previous 24 years. The election also brought the Democrats full control of the legislative and executive branches of the federal government, including both houses of U.S. Congress and the presidency, for the first time since the administration of the last Democratic president, Jimmy Carter. This would not last for very long, however, as the Republicans won control of both the House and Senate in 1994. Reelected in 1996, Clinton would become the first Democratic president since Franklin D. Roosevelt to serve two full terms in the White House and the first to leave office at the end of his second full term since Woodrow Wilson.\n1992 was arguably a political realignment election. It made the Democratic Party dominant in presidential elections in the Northeast, the Great Lakes region, and the West Coast, where many states had previously either been swing states or Republican-leaning. Clinton picked up several states that went Republican in 1988, and which have remained in the Democratic column ever since: California, Connecticut, Delaware, Illinois, most of Maine (besides the state's second congressional district, which broke the state's total straight Democratic voting record since, when it voted for Republican presidential candidate Donald Trump in 2016, along with the two following presidential elections in 2020, and 2024), Maryland, New Jersey, and Vermont. Vermont, carried by Clinton, had been heavily Republican for generations prior to the election, voting for a Democrat only once (in 1964). The state has been won by the Democratic nominee in every presidential election since. Bill Clinton narrowly defeated Bush in New Jersey (by two points), which had voted for the Republican nominee all but twice since 1948. Clinton would later win the state in 1996 by eighteen points; like Vermont, Republicans have not won the state since. California, which had been a Republican stronghold since 1952, was now trending Democratic. Clinton, a native Southerner, was able to carry several states in the South that the GOP had won for much of the past two decades, but ultimately won only four of eleven former Confederate states. This reflected the final shift of the South to the Republican Party. In subsequent presidential elections from 1996 to 2020, 28 out of the 50 states were carried by the same party as in 1992 (15 for the Democrats and 13 for the Republicans).\nDetailed results.\nSource (Popular Vote): \nSource (Electoral Vote): \nResults by state.\nSource:\n\u2020Maine and Nebraska each allowed their electoral votes to be split between candidates using the Congressional District Method for electoral vote assignment. In both states, two electoral votes were awarded to the winner of the statewide race and one electoral vote was awarded to the winner of each congressional district. District results for Maine and Nebraska do not include results for Marrou or other candidates and so totals differ from those for the states' at-large. Because Perot finished in 2nd place in some districts, the margins of the districts do not match the margin at-large. Nebraska split its electoral votes this way for the first time.\nClose states.\nStates with margin of victory less than 1% (27 electoral votes):\nStates/Districts with margin of victory less than 5% (175 electoral votes):\nStates with margin of victory between 5% and 10% (131 electoral votes):\nSource: New York Times President Map\nStatistics.\nCounties with Highest Percent of Vote (Democratic)\nCounties with Highest Percent of Vote (Republican)\nCounties with Highest Percent of Vote (Other)\nVoter demographics.\nSource: Voter News Service exit poll, reported in \"The New York Times\", November 10, 1996, 28.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39530", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39530", "title": "1366", "text": "Calendar year\nYear 1366 (MCCCLXVI) was a common year starting on Thursday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39531", "revid": "51064137", "url": "https://en.wikipedia.org/wiki?curid=39531", "title": "1988 United States presidential election", "text": "Presidential elections were held in the United States on November 8, 1988. The Republican ticket of incumbent vice president George H. W. Bush and Indiana senator Dan Quayle defeated the Democratic ticket of Massachusetts governor Michael Dukakis and Texas senator Lloyd Bentsen. The election was the third consecutive landslide victory for the Republican Party.\nPresident Ronald Reagan was ineligible to seek a third term because of the 22nd Amendment. As a result, it was the first election since 1968 to lack an incumbent president on the ballot, and also the first incumbent president since Dwight D. Eisenhower in 1960 to be barred from seeking reelection. Bush entered the Republican primaries as the front-runner, defeating Kansas senator Bob Dole and televangelist Pat Robertson. He selected Indiana senator Dan Quayle as his running mate. Dukakis, campaigning on his state's record of strong economic growth, won the Democratic primaries after Gary Hart (a prominent \"Atari Democrat\" representing the party's moderate wing) withdrew and Ted Kennedy (representing the party's traditional liberal wing) declined to run. Dukakis selected Texas Senator Lloyd Bentsen as his running mate.\nBush ran an aggressive campaign that concentrated mainly on the strong economy, reduction in crime, and continuance with Reagan's policies. He attacked Dukakis as an elitist \"Massachusetts liberal\" and soft-on-crime, to which Dukakis ineffectively responded. Despite Dukakis initially leading in the polls, Bush pulled ahead after the Republican National Convention and extended his lead after two strong debate performances. Bush won a decisive victory over Dukakis, winning the Electoral College and the popular vote by sizable margins. Bush became the fourth sitting vice president to be elected president after John Adams in 1796, Thomas Jefferson in 1800, and Martin Van Buren in 1836, and remains the most recent to do so. Despite his loss, Dukakis flipped nine states that had voted Republican in 1984: Hawaii, Iowa, Massachusetts, New York, Oregon, Rhode Island, Washington, West Virginia, and Wisconsin.\nVoters 44 years of age or younger (born in 1944 or later) were estimated by the exit poll to comprise 55% of the electorate. As such, Baby boomers and Generation X constituted the majority of the voting public.\nAs of 2024, it remains the most recent election in which a candidate won over 400 electoral votes, as well as 40 or more states. It is also the most recent open seat election in which the outgoing president's party retained the White House and for the first time since 1928, this is also the most recent time a party won the presidency for a third consecutive term. As of 2025, this is the earliest election in which at least one of the major party nominees for president (Dukakis) or vice president (Quayle) is still alive.\nRepublican Party nomination.\nWithdrawn candidates.\nWhile Bush had long been seen as Reagan's natural successor, there was still a degree of opposition within the party to his candidacy. Historical precedent was not seen to favor Bush's chances, as no incumbent vice president had been elected as president since Martin Van Buren in 1836. Dole attracted support among those who were concerned that Bush, whose electoral experience outside of his campaigns with Reagan was limited to running unsuccessfully for the Senate and twice successfully for the House of Representatives in the 1960s, had not done enough to establish himself as a candidate in his own right. Others who wished to further continue the shift towards social conservatism that had begun during Reagan's presidency supported Robertson.\nBush unexpectedly came in third in the Iowa caucus, which he had won in 1980, behind Dole and Robertson. Dole was also leading in the polls of the New Hampshire primary, and the Bush camp responded by running television commercials portraying Dole as a tax raiser, while Governor John H. Sununu campaigned for Bush. Dole did nothing to counter these ads and Bush won, thereby gaining crucial momentum, which he called \"Big Mo\". Once the multiple-state primaries such as Super Tuesday began, Bush's organizational strength and fundraising lead were impossible for the other candidates to match, and the nomination was his. The Republican Party convention was held in New Orleans, Louisiana. Bush was nominated unanimously and selected U.S. senator Dan Quayle from Indiana as his running mate. In his acceptance speech, Bush made the pledge \"\", which contributed to his loss in 1992.\nDemocratic Party nomination.\nIn 1984, the Democrats had nominated Walter Mondale, a traditional New Deal-type liberal, who advocated for those constituencies that Franklin D. Roosevelt forged into a majority coalition, as their candidate. When Mondale was defeated in a landslide, party leaders became eager to find a new approach to get away from the 1980 and 1984 debacles. After Bush's image was affected by his involvement on the Iran-Contra scandal much more than Reagan's, and after the Democrats won back control of the U.S. Senate in the 1986 congressional elections following an economic downturn, the party's leaders felt optimistic about having a closer race with the GOP in 1988, although probabilities of winning the presidency were still marginal given the climate of prosperity.\nOne goal of the party was to find a new, fresh candidate who could move beyond the traditional New Deal-Great Society ideas of the past and offer a new image of the Democrats to the public. To this end party leaders tried to recruit New York governor Mario Cuomo to be a candidate. Cuomo had impressed many Democrats with his keynote speech at the 1984 Democratic Convention, and they believed he would be a strong candidate. After Cuomo chose not to run, the Democratic frontrunner for most of 1987 was former Colorado senator Gary Hart. He had made a strong showing in the 1984 presidential primaries; after Mondale's defeat, he had positioned himself as the moderate and centrist many Democrats felt their party would need to win.\nQuestions and rumors about extramarital affairs and past debts dogged Hart's campaign. Hart had told \"New York Times\" reporters who questioned him about these rumors that, if they followed him around, they would \"be bored\". In a separate investigation, the \"Miami Herald\" had received an anonymous tip from a friend of Donna Rice that Rice was involved with Hart. After his affair emerged, the Herald reporters found Hart's quote in a pre-print of \"The New York Times Magazine\". After the \"Herald\"'s findings were publicized, many other media outlets picked up the story and Hart's ratings in the polls plummeted. On May 8, 1987, a week after the Rice story broke, Hart dropped out of the race. His campaign chair, Representative Patricia Schroeder, tested the waters for about four months after Hart's withdrawal, but decided in September 1987 that she would not run. In December 1987, Hart surprised many pundits by resuming his campaign, but the allegations of adultery had delivered a fatal blow to his candidacy, and he did poorly in the primaries before dropping out again.\nSenator Ted Kennedy of Massachusetts had been considered a potential candidate, but he ruled himself out of the race in the fall of 1985. Two other politicians mentioned as possible candidates, both from Arkansas, did not join the race: Senator Dale Bumpers and Governor and future president Bill Clinton. Joe Biden's campaign also ended in controversy after he was accused of plagiarizing a speech by Neil Kinnock, then-leader of the British Labour Party. The Dukakis campaign secretly released a video in which Biden was filmed repeating a Kinnock stump speech with only minor modifications. Biden later called his failure to attribute the quotes an oversight, and in related proceedings the Delaware Supreme Court's Board on Professional Responsibility cleared him of a separate plagiarism charge, leveled for plagiarizing an article during his law school. This ultimately led him to drop out of the race. Dukakis later revealed that his campaign had leaked the tape, and two members of his staff resigned. Biden later ran twice more for the Democratic nomination, unsuccessfully in 2008 and successfully in 2020. He was inaugurated as the 47th vice president in 2009, serving two terms under President Barack Obama. In 2021, he became the 46th president, over 33 years after his first campaign for the office ended.\nAl Gore, a senator from Tennessee, chose to run for the nomination. Turning 40 in 1988, he would have been the youngest man to contest the presidency on a major party ticket since William Jennings Bryan in 1896, and the youngest president ever if elected, younger than John F. Kennedy at election age and Theodore Roosevelt at age of assumption of office. He eventually became the 45th vice president of the United States under Bill Clinton, then the Democratic presidential nominee in 2000, losing to George W. Bush, George H. W. Bush's son.\nPrimaries.\nAfter Hart withdrew from the race, no clear frontrunner emerged before the primaries and caucuses began. The Iowa caucus was won by Dick Gephardt, who had been sagging heavily in the polls until, three weeks before the vote, he began campaigning as a populist and his numbers surged. Illinois senator Paul M. Simon finished a surprising second, and Massachusetts governor Michael Dukakis finished third. In the New Hampshire primary, Dukakis came in first, Gephardt fell to second, and Simon came in third. In an effort to weaken Gephardt's candidacy, both Dukakis and Gore ran negative television ads against Gephardt. The ads convinced the United Auto Workers, which had endorsed Gephardt, to withdraw their endorsement; this crippled Gephardt, as he relied heavily on the support of labor unions.\nIn the Super Tuesday races, Dukakis won six primaries, to Gore's five, Jesse Jackson five and Gephardt one, with Gore and Jackson splitting the Southern states. The next week, Simon won Illinois with Jackson finishing second. Jackson captured 6.9 million votes and won 11 contests: seven primaries (Alabama, the District of Columbia, Georgia, Louisiana, Mississippi, Puerto Rico, and Virginia) and four caucuses (Delaware, Michigan, South Carolina and Vermont). He also scored March victories in Alaska's caucuses and Texas's local conventions, despite losing the Texas primary. Briefly, after he won 55% of the vote in the Michigan Democratic caucus, he had more pledged delegates than all the other candidates. Jackson's campaign suffered a significant setback less than two weeks later when he was defeated in the Wisconsin primary by Dukakis. Dukakis's win in New York and then in Pennsylvania effectively ended Jackson's hopes for the nomination.\nDemocratic Convention.\nThe Democratic Party Convention was held in Atlanta, Georgia from July 18\u201321. Arkansas governor Bill Clinton placed Dukakis's name in nomination, and delivered his speech, scheduled to be 15 minutes long but lasting so long that some delegates began booing to get him to finish; he received great cheering when he said, \"In closing...\" Texas state treasurer Ann Richards, who was elected the state governor two years later, gave a speech attacking Bush, including the line \"Poor George, he can't help it, he was born with a silver foot in his mouth\". With only Jackson remaining as an active candidate to oppose Dukakis, the tally for president was as follows:\nJackson's supporters said that since their candidate had finished in second place, he was entitled to the vice presidential nomination. Dukakis disagreed, and instead selected Senator Lloyd Bentsen from Texas. Bentsen's selection led many in the media to dub the ticket the \"Boston-Austin\" axis, and to compare it to the pairing of John F. Kennedy and Lyndon B. Johnson in the 1960 presidential campaign. Like Dukakis and Bentsen, Kennedy and Johnson were from Massachusetts and Texas, respectively.\nOther nominations.\nLibertarian Party.\nRon Paul and Andre Marrou formed the ticket for the Libertarian Party. Their campaign called for the adoption of a global policy on military nonintervention, advocated an end to the federal government's involvement with education, and criticized Reagan's \"bailout\" of the Soviet Union. Paul was a former member of the U.S. House of Representatives, first elected as a Republican from Texas in an April 1976 special election. He was known as an opponent of the war on drugs.\nNew Alliance Party.\nLenora Fulani ran for the New Alliance Party, and focused on issues concerning unemployment, healthcare, and homelessness. The party had full ballot access, meaning Fulani and her running mate, Joyce Dattner, were the first pair of women to receive ballot access in all 50 states. Fulani was the first African American to do so.\nSocialist Party.\nWilla Kenoyer and Ron Ehrenreich ran for the Socialist Party, advocating a decentralist government approach with policies determined by the needs of the workers.\nPopulist Party.\nDavid Duke stood for the Populist Party. A former leader of the Louisiana Ku Klux Klan, he advocated a mixture of White nationalist and separatist policies with more traditionally conservative positions, such as opposition to most immigration from Latin America and to affirmative action.\nGeneral election.\nCampaign.\nDuring the election, the Bush campaign sought to portray Dukakis as an unreasonable \"Massachusetts liberal\". Dukakis was attacked for such positions as opposing mandatory recitation of the Pledge of Allegiance in schools, and being a \"card-carrying member of the ACLU\" (a statement Dukakis made early in the primary campaign to appeal to liberal voters). Dukakis responded by saying that he was a \"proud liberal\" and that the phrase should not be a bad word in America. Bush pledged to continue Reagan's policies but also vowed a \"kinder and gentler nation\" in an attempt to win over more moderate voters. The duties delegated to him during Reagan's second term (mostly because of the President's advanced age, Reagan turning 78 just after he left office) gave him an unusually high level of experience for a vice president.\nA graduate of Yale University, Bush derided Dukakis for having \"foreign-policy views born in Harvard Yard's boutique\". \"New York Times\" columnist Maureen Dowd asked, \"Wasn't this a case of the pot calling the kettle elite?\" Bush said that, unlike Harvard, Yale's reputation was \"so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it ... Harvard boutique to me has the connotation of liberalism and elitism\", and said he intended Harvard to represent \"a philosophical enclave\", not a statement about class. Columnist Russell Baker wrote, \"Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets.\"\nDukakis was badly damaged by the Republicans' campaign commercials, including \"Boston Harbor\", which attacked his failure to clean up environmental pollution in the harbor, and especially by two commercials that were accused of being racially charged, \"Revolving Door\" and \"Weekend Passes\", that portrayed him as soft on crime. Dukakis was a strong supporter of Massachusetts's prison furlough program, which had begun before he was governor. As governor, Dukakis vetoed a 1976 plan to bar inmates convicted of first-degree murder from the furlough program. In 1986, the program had resulted in the release of convicted murderer Willie Horton, an African American man who committed a rape and assault in Maryland while out on furlough. A number of false rumors about Dukakis were reported in the media, including Idaho Republican Senator Steve Symms's claim that Dukakis's wife Kitty had burned an American flag to protest the Vietnam War, as well as the claim that Dukakis himself had been treated for mental illness.\n\"Dukakis in the tank\".\nDukakis attempted to quell criticism that he was ignorant on military matters by staging a photo op in which he rode in an M1 Abrams tank outside a General Dynamics plant in Sterling Heights, Michigan. The move ended up being regarded as a major public relations blunder, with many mocking Dukakis's appearance as he waved to the crowd from the tank. The Bush campaign used the footage in an attack ad, accompanied by a rolling text listing Dukakis's vetoes of military-related bills. The incident remains a commonly cited example of backfired public relations.\nDan Quayle.\nOne reason for Bush's choice of Senator Dan Quayle as his running mate was to appeal to younger Americans identified with the \"Reagan Revolution\". Quayle's looks were praised by Senator John McCain: \"I can't believe a guy that handsome wouldn't have some impact.\" But Quayle was not a seasoned politician, and made a number of embarrassing statements. The Dukakis team attacked Quayle's credentials, saying he was \"dangerously inexperienced to be first-in-line to the presidency.\"\nDuring the vice presidential debate, Quayle attempted to dispel such allegations by comparing his experience with that of pre-1960 John F. Kennedy, who had also been a young politician when running for the presidency (Kennedy had served 13 years in Congress to Quayle's 12). Quayle said, \"I have as much experience in the Congress as Jack Kennedy did when he sought the presidency.\" Dukakis's running mate Lloyd Bentsen responded, \"Senator, I served with Jack Kennedy. I knew Jack Kennedy. Jack Kennedy was a friend of mine. Senator, you're no Jack Kennedy.\" Quayle responded, \"That was really uncalled for, Senator\", to which Bentsen said, \"You are the one that was making the comparison, Senator, and I'm one who knew him well. And frankly I think you are so far apart in the objectives you choose for your country that I did not think the comparison was well-taken.\"\nDemocrats replayed Quayle's reaction to Bentsen's comment in subsequent ads as an announcer intoned, \"Quayle: just a heartbeat away.\" Despite much press about the Kennedy comments, this did not reduce Bush's lead in the polls. Quayle had sought to use the debate to criticize Dukakis as too liberal rather than go point for point with the more seasoned Bentsen. Bentsen's attempts to defend Dukakis received little recognition, with greater attention on the Kennedy comparison.\nGreek issue.\nDukakis constantly used his Greek roots in his speeches, and Greek Americans fundraised more than 3 million dollars for his campaign. The Washington Post named Dukakis' ethnicity \"the great unspoken issue in this election\" with his ethnicity not playing well with WASP communities.\nJennifer Fitzgerald and Donna Brazile firing.\nDuring the course of the campaign, Dukakis fired his deputy field director Donna Brazile after she spread unsubstantiated rumors that Bush had had an affair with his assistant Jennifer Fitzgerald. Bush and Fitzgerald's relationship was briefly rehashed in the 1992 campaign.\nPresidential debates.\nThere were two presidential debates and one vice-presidential debate. Voters were split as to who won the first presidential debate. Bush improved in the second debate. Before the second debate, Dukakis had been suffering from the flu and spent much of the day in bed. His performance was generally seen as poor and played to his reputation of being intellectually cold. Reporter Bernard Shaw opened the debate by asking Dukakis whether he would support the death penalty if Kitty Dukakis were raped and murdered; Dukakis said \"no\" and discussed the statistical ineffectiveness of capital punishment. Some commentators thought the question itself was unfair, in that it injected an overly emotional element into the discussion of a policy issue, but many observers felt Dukakis's answer lacked the normal emotions one would expect of a person talking about a loved one's rape and murder. Tom Brokaw of NBC reported on his October 14 newscast, \"The consensus tonight is that Vice President George Bush won last night's debate and made it all the harder for Governor Michael Dukakis to catch and pass him in the 25 days remaining. In all of the Friday morning quarterbacking, there was common agreement that Dukakis failed to seize the debate and make it his night.\"\nDukakis' new strategy.\nBy October, the Dukakis campaign was in dire straits. A poor performance in the second debate caused his numbers to flatline, and an NBC News poll showed Bush leading Dukakis by seventeen percentage points. In response, Dukakis shifted his campaign rhetoric. He embraced the label \"liberal\", referring to himself as \"a liberal in the tradition of Franklin Roosevelt and Harry Truman and John Kennedy\". He also promoted unabashedly populist economic themes reminiscent of primary challenger Dick Gephardt and adopted the slogan \"We're on your side\", suggesting that Bush's economic policies were regressive and elitist. In one Texas ad, Bentsen pointed out that wages were higher in Japan than Texas, claiming he and Dukakis would \"put America first\" when elected. Speaking in Lexington, Kentucky, Dukakis declared, \"We\u2019re going to take back our government from the influence peddlers and the sleaze merchants\", as well as \"dishonest contractors and polluters\". Bush responded by calling Dukakis a liberal and accusing him of dividing the country with his populist, anti-establishment rhetoric. As a new national poll showed Dukakis cutting Bush's lead down to eight percentage points amidst gains with union voters, Newsweek's \"Conventional Wisdom\" showed a down arrow next to Bush and an up arrow next to Dukakis, adding \"Old CW: It\u2019s over. New CW: Did someone say it\u2019s over? Not \"us\"\".\nSensing the changing winds, the Dukakis campaign revised their strategy, abandoning their national campaign and instead waging a targeted effort in eighteen Dukakis-leaning and competitive states in the Northeast, Midwest, and West Coast that, if carried together, would amount to a 272-electoral-vote victory even if Dukakis lost the national popular vote. They also identified states in the Plains and West that were still competitive despite leaning Bush overall, such as the Dakotas, Montana, New Mexico, Colorado, Missouri, and Kentucky. Bush did not change his campaign strategy and continued to air negative attacks against Dukakis.\nResults.\nIn the November 8 election, Bush won a majority of the popular vote and the Electoral College. Neither his popular vote percentage (53.4%), his total electoral votes (426), nor his number of states won (40) have been surpassed in any subsequent presidential election. Conversely, it began an ongoing streak of presidential elections that were decided by a single-digit popular vote margin.\nThis is the most recent election whereby both major party candidates shared the same birth state, which in this case, was Massachusetts. Like Reagan in 1980 and 1984, Bush performed very strongly among suburban voters, in areas such as the collar counties of Chicago (winning over 60% in DuPage and Lake counties), Philadelphia (sweeping the Main Line counties), Baltimore, Los Angeles (winning over 60% in the Republican bastions of Orange and San Diego counties) and New York. As of 2024, Bush is the last Republican to win the heavily suburban states of California, Connecticut, Delaware, Illinois, Maryland, and New Jersey. He is also the last Republican candidate to win rural Vermont, which was historically Republican but by this time shifting away from the party, as well as the last Republican candidate to win Maine in its entirety, although Donald Trump won one electoral vote from the state in 2016, 2020, and 2024. Bush lost New York state by just over 4%. Bush is the first Republican to win the presidency without Iowa. In contrast to the suburbs, a solidly Republican constituency, Bush received a significantly lower level of support than Reagan in rural regions. Farm states had fared poorly during the Reagan administration, and Dukakis was the beneficiary.\nThis is the last election where Michigan and Pennsylvania voted Republican until 2016, New Mexico until 2004, and Arkansas, Kentucky, Louisiana, Missouri, Nevada, New Hampshire, Ohio, and Tennessee until 2000. This is the last election where no state was decided by a margin under 1%. In Illinois, Bush lost a number of downstate counties that previously went for Reagan, and he lost Iowa by a wide margin, even losing in traditionally Republican areas. Bush also performed weaker in Missouri's northern counties, narrowly winning that state. In three typically solid Republican states, Kansas, South Dakota, and Montana, the vote was much closer than usual. The rural state of West Virginia, though not an agricultural economy, narrowly flipped back into the Democratic column. Bush performed strongest in the South and West. Despite Bentsen's presence on the Democratic ticket, Bush won Texas by 12 points. He lost the states of the Pacific Northwest but narrowly held California in the Republican column for the sixth straight time. As of 2024[ [update]], this was the last election in which the Republican candidate won the support of a majority or plurality of women voters.\nThis is the last presidential election in which the Rust Belt states of Michigan, Pennsylvania, and Wisconsin did not vote for the same candidate. In addition, it is the most recent presidential election in which the Democratic candidate gave a concession speech on election night. It is the most recent election where the Democratic candidate did not receive 200 electoral votes. \nElectoral results.\nSource (popular vote): , \nSource (electoral vote): \n(a) \"West Virginia faithless elector Margarette Leach voted for Bentsen as president and Dukakis as vice president in order to make a statement against the U.S. Electoral College.\" \n(b) \"Fulani's running mate varied from state to state. Among the six vice presidential candidates were Joyce Dattner, Harold Moore, and Wynonia Burke.\"\nResults by state.\nMaine allowed its electoral votes to be split between candidates. Two electoral votes were awarded to the winner of the statewide race and one electoral vote to the winner of each congressional district. Bush won all four votes. This was the last election in which Nebraska awarded its electors in a winner-take-all format before switching to the congressional district method.\nClose states.\nStates with margin of victory less than 5% (195 electoral votes)\nStates with margin of victory between 5% and 10% (70 electoral votes):\nStatistics.\nCounties with highest percent of vote (Republican)\nCounties with highest percent of vote (Democratic)\nVoter demographics.\nSource: CBS News and \"The New York Times\" exit poll from the Roper Center for Public Opinion Research ()\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39532", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39532", "title": "1367", "text": "Calendar year\nYear 1367 (MCCCLXVII) was a common year starting on Friday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39533", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=39533", "title": "1368", "text": "Calendar year\n1368 (MCCCLXVIII) was a leap year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39534", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39534", "title": "1369", "text": "Calendar year\nYear 1369 (MCCCLXIX) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39535", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39535", "title": "1370", "text": "Calendar year\nYear 1370 (MCCCLXX) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39536", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=39536", "title": "1372", "text": "Calendar year\nYear 1372 (MCCCLXXII) was a leap year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39537", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39537", "title": "1373", "text": "Calendar year\nYear 1373 (MCCCLXXIII) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39538", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=39538", "title": "1375", "text": "Calendar year\nYear 1375 (MCCCLXXV) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39539", "revid": "6104108", "url": "https://en.wikipedia.org/wiki?curid=39539", "title": "1376", "text": "Calendar year\nYear 1376 (MCCCLXXVI) was a leap year starting on Tuesday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39540", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39540", "title": "1377", "text": "Calendar year\nYear 1377 (MCCCLXXVII) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39541", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39541", "title": "1378", "text": "Calendar year\nYear 1378 (MCCCLXXVIII) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39542", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39542", "title": "1379", "text": "Calendar year\nYear 1379 (MCCCLXXIX) was a common year starting on Saturday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39543", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39543", "title": "1381", "text": "Calendar year\nYear 1381 (MCCCLXXXI) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39544", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39544", "title": "1383", "text": "Calendar year\nYear 1383 (MCCCLXXXIII) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39545", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=39545", "title": "1384", "text": "Calendar year\nYear 1384 (MCCCLXXXIV) was a leap year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39546", "revid": "15130", "url": "https://en.wikipedia.org/wiki?curid=39546", "title": "1385", "text": "Calendar year\nYear 1385 (MCCCLXXXV) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39547", "revid": "11096", "url": "https://en.wikipedia.org/wiki?curid=39547", "title": "1386", "text": "Calendar year\nYear 1386 (MCCCLXXXVI) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39548", "revid": "49620088", "url": "https://en.wikipedia.org/wiki?curid=39548", "title": "1387", "text": "Calendar year\nYear 1387 (MCCCLXXXVII) was a common year starting on Tuesday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39549", "revid": "82432", "url": "https://en.wikipedia.org/wiki?curid=39549", "title": "1392", "text": "Calendar year\nYear 1392 (MCCCXCII) was a leap year starting on Monday (link will display full calendar) of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39550", "revid": "27089876", "url": "https://en.wikipedia.org/wiki?curid=39550", "title": "1391", "text": "Calendar year\nYear 1391 (MCCCXCI) was a common year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39551", "revid": "40625145", "url": "https://en.wikipedia.org/wiki?curid=39551", "title": "1390", "text": "Calendar year\nYear 1390 (MCCCXC) was a common year starting on Saturday (link will display full calendar) of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39552", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=39552", "title": "1395", "text": "Calendar year\nYear 1395 (MCCCXCV) was a common year starting on Friday (link will display full calendar) of the Julian calendar, the 1395th year of the Common Era (CE) and \"Anno Domini\" (AD) designations, the 395th year of the 2nd millennium, the 95th year of the 14th century, and the 6th year of the 1390s decade.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39553", "revid": "8383321", "url": "https://en.wikipedia.org/wiki?curid=39553", "title": "1396", "text": "Calendar year\nYear 1396 (MCCCXCVI) was a leap year starting on Saturday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39554", "revid": "29463730", "url": "https://en.wikipedia.org/wiki?curid=39554", "title": "1397", "text": "Calendar year\nYear 1397 (MCCCXCVII) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39555", "revid": "50620675", "url": "https://en.wikipedia.org/wiki?curid=39555", "title": "1399", "text": "Calendar year\nYear 1399 (MCCCXCIX) was a common year starting on Wednesday of the Julian calendar.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39556", "revid": "599562", "url": "https://en.wikipedia.org/wiki?curid=39556", "title": "1401", "text": "Calendar year\n \nYear 1401 (MCDI) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39557", "revid": "4328878", "url": "https://en.wikipedia.org/wiki?curid=39557", "title": "1404", "text": "Calendar year\nYear 1404 (MCDIV) was a leap year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39558", "revid": "11096", "url": "https://en.wikipedia.org/wiki?curid=39558", "title": "1406", "text": "Calendar year\nYear 1406 (MCDVI) was a common year starting on Friday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39559", "revid": "4328878", "url": "https://en.wikipedia.org/wiki?curid=39559", "title": "1407", "text": "Calendar year\nYear 1407 (MCDVII) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39560", "revid": "4328878", "url": "https://en.wikipedia.org/wiki?curid=39560", "title": "1408", "text": "Calendar year\nYear 1408 (MCDVIII) was a leap year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nDate unknown.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39561", "revid": "6104108", "url": "https://en.wikipedia.org/wiki?curid=39561", "title": "1409", "text": "Calendar year\nYear 1409 (MCDIX) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39562", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=39562", "title": "Geochemistry", "text": "Science that applies chemistry to analyze geological systems\nGeochemistry is the science that uses the tools and principles of chemistry to explain the mechanisms behind major geological systems such as the Earth's crust and its oceans. The realm of geochemistry extends beyond the Earth, encompassing the entire Solar System, and has made important contributions to the understanding of a number of processes including mantle convection, the formation of planets and the origins of granite and basalt. It is an integrated field of chemistry and geology. \n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nThe term \"geochemistry\" was first used by the Swiss-German chemist Christian Friedrich Sch\u00f6nbein in 1838: \"a comparative geochemistry ought to be launched, before geognosy can become geology, and before the mystery of the genesis of our planets and their inorganic matter may be revealed.\" However, for the rest of the century the more common term was \"chemical geology\", and there was little contact between geologists and chemists.\nGeochemistry emerged as a separate discipline after major laboratories were established, starting with the United States Geological Survey (USGS) in 1884, which began systematic surveys of the chemistry of rocks and minerals. The chief USGS chemist, Frank Wigglesworth Clarke, noted that the elements generally decrease in abundance as their atomic weights increase, and summarized the work on elemental abundance in \"The Data of Geochemistry\".\nThe composition of meteorites was investigated and compared to terrestrial rocks as early as 1850. In 1901, Oliver C. Farrington hypothesised that, although there were differences, the relative abundances should still be the same. This was the beginnings of the field of cosmochemistry and has contributed much of what we know about the formation of the Earth and the Solar System.\nIn the early 20th century, Max von Laue and William L. Bragg showed that X-ray scattering could be used to determine the structures of crystals. In the 1920s and 1930s, Victor Goldschmidt and associates at the University of Oslo applied these methods to many common minerals and formulated a set of rules for how elements are grouped. Goldschmidt published this work in the series \"Geochemische Verteilungsgesetze der Elemente\" [Geochemical Laws of the Distribution of Elements].\nThe research of Manfred Schidlowski from the 1960s to around the year 2002 was concerned with the biochemistry of the Early Earth with a focus on isotope-biogeochemistry and the evidence of the earliest life processes in Precambrian.\nSubfields.\nSome subfields of geochemistry are:\nChemical elements.\nThe building blocks of materials are the chemical elements. These can be identified by their atomic number Z, which is the number of protons in the nucleus. An element can have more than one value for N, the number of neutrons in the nucleus. The sum of these is the mass number, which is roughly equal to the atomic mass. Atoms with the same atomic number but different neutron numbers are called isotopes. A given isotope is identified by a letter for the element preceded by a superscript for the mass number. For example, two common isotopes of chlorine are 35Cl and 37Cl. There are about 1700 known combinations of Z and N, of which only about 260 are stable. However, most of the unstable isotopes do not occur in nature. In geochemistry, stable isotopes are used to trace chemical pathways and reactions, while radioactive isotopes are primarily used to date samples.\nThe chemical behavior of an atom \u2013 its affinity for other elements and the type of bonds it forms \u2013 is determined by the arrangement of electrons in orbitals, particularly the outermost (valence) electrons. These arrangements are reflected in the position of elements in the periodic table. Based on position, the elements fall into the broad groups of alkali metals, alkaline earth metals, transition metals, semi-metals (also known as metalloids), halogens, noble gases, lanthanides and actinides.\nAnother useful classification scheme for geochemistry is the Goldschmidt classification, which places the elements into four main groups. \"Lithophiles\" combine easily with oxygen. These elements, which include Na, K, Si, Al, Ti, Mg and Ca, dominate in the Earth's crust, forming silicates and other oxides. \"Siderophile\" elements (Fe, Co, Ni, Pt, Re, Os) have an affinity for iron and tend to concentrate in the core. \"Chalcophile\" elements (Cu, Ag, Zn, Pb, S) form sulfides; and \"atmophile\" elements (O, N, H and noble gases) dominate the atmosphere. Within each group, some elements are refractory, remaining stable at high temperatures, while others are volatile, evaporating more easily, so heating can separate them.\nDifferentiation and mixing.\nThe chemical composition of the Earth and other bodies is determined by two opposing processes: differentiation and mixing. In the Earth's mantle, differentiation occurs at mid-ocean ridges through partial melting, with more refractory materials remaining at the base of the lithosphere while the remainder rises to form basalt. After an oceanic plate descends into the mantle, convection eventually mixes the two parts together. Erosion differentiates granite, separating it into clay on the ocean floor, sandstone on the edge of the continent, and dissolved minerals in ocean waters. Metamorphism and anatexis (partial melting of crustal rocks) can mix these elements together again. In the ocean, biological organisms can cause chemical differentiation, while dissolution of the organisms and their wastes can mix the materials again.\nFractionation.\nA major source of differentiation is fractionation, an unequal distribution of elements and isotopes. This can be the result of chemical reactions, phase changes, kinetic effects, or radioactivity.\nOn the largest scale, \"planetary differentiation\" is a physical and chemical separation of a planet into chemically distinct regions. For example, the terrestrial planets formed iron-rich cores and silicate-rich mantles and crusts. In the Earth's mantle, the primary source of chemical differentiation is partial melting, particularly near mid-ocean ridges. This can occur when the solid is heterogeneous or a solid solution, and part of the melt is separated from the solid. The process is known as \"equilibrium\" or \"batch\" melting if the solid and melt remain in equilibrium until the moment that the melt is removed, and \"fractional\" or \"Rayleigh\" melting if it is removed continuously.\nIsotopic fractionation can have mass-dependent and mass-independent forms. Molecules with heavier isotopes have lower ground state energies and are therefore more stable. As a result, chemical reactions show a small isotope dependence, with heavier isotopes preferring species or compounds with a higher oxidation state; and in phase changes, heavier isotopes tend to concentrate in the heavier phases. Mass-dependent fractionation is largest in light elements because the difference in masses is a larger fraction of the total mass.\nRatios between isotopes are generally compared to a standard. For example, sulfur has four stable isotopes, of which the two most common are 32S and 34S. The ratio of their concentrations, \"R\"\n34S/32S, is reported as\nformula_1\nwhere \"R\"s is the same ratio for a standard. Because the differences are small, the ratio is multiplied by 1000 to make it parts per thousand (referred to as parts per mil). This is represented by the symbol \u2030.\nEquilibrium.\n\"Equilibrium fractionation\" occurs between chemicals or phases that are in equilibrium with each other. In equilibrium fractionation between phases, heavier phases prefer the heavier isotopes. For two phases A and B, the effect can be represented by the factor\nformula_2\nIn the liquid-vapor phase transition for water, \"a\"l-v at 20 degrees Celsius is 1.0098 for 18O and 1.084 for 2H. In general, fractionation is greater at lower temperatures. At 0\u00a0\u00b0C, the factors are 1.0117 and 1.111.\nKinetic.\nWhen there is no equilibrium between phases or chemical compounds, \"kinetic fractionation\" can occur. For example, at interfaces between liquid water and air, the forward reaction is enhanced if the humidity of the air is less than 100% or the water vapor is moved by a wind. Kinetic fractionation generally is enhanced compared to equilibrium fractionation and depends on factors such as reaction rate, reaction pathway and bond energy. Since lighter isotopes generally have weaker bonds, they tend to react faster and enrich the reaction products.\nBiological fractionation is a form of kinetic fractionation since reactions tend to be in one direction. Biological organisms prefer lighter isotopes because there is a lower energy cost in breaking energy bonds. In addition to the previously mentioned factors, the environment and species of the organism can have a large effect on the fractionation.\nCycles.\nThrough a variety of physical and chemical processes, chemical elements change in concentration and move around in what are called \"geochemical cycles\". An understanding of these changes requires both detailed observation and theoretical models. Each chemical compound, element or isotope has a concentration that is a function \"C\"(r,\"t\") of position and time, but it is impractical to model the full variability. Instead, in an approach borrowed from chemical engineering, geochemists average the concentration over regions of the Earth called \"geochemical reservoirs\". The choice of reservoir depends on the problem; for example, the ocean may be a single reservoir or be split into multiple reservoirs. In a type of model called a \"box model\", a reservoir is represented by a box with inputs and outputs.\nGeochemical models generally involve feedback. In the simplest case of a linear cycle, either the input or the output from a reservoir is proportional to the concentration. For example, salt is removed from the ocean by formation of evaporites, and given a constant rate of evaporation in evaporite basins, the rate of removal of salt should be proportional to its concentration. For a given component \"C\", if the input to a reservoir is a constant \"a\" and the output is \"kC\" for some constant \"k\", then the \"mass balance\" equation is\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nThis expresses the fact that any change in mass must be balanced by changes in the input or output. On a time scale of \"t\" \n 1/k, the system approaches a steady state in which \"C\"steady \n \"a\"/\"k\". The \"residence time\" is defined as\nformula_3\nwhere \"I\" and \"O\" are the input and output rates. In the above example, the steady-state input and output rates are both equal to \"a\", so \u03c4res \n 1/\"k\".\nIf the input and output rates are nonlinear functions of \" C\", they may still be closely balanced over time scales much greater than the residence time; otherwise, there will be large fluctuations in \" C\". In that case, the system is always close to a steady-state and the lowest order expansion of the mass balance equation will lead to a linear equation like Equation (1). In most systems, one or both of the input and output depend on \"C\", resulting in feedback that tends to maintain the steady-state. If an external forcing perturbs the system, it will return to the steady-state on a time scale of 1/\"k\".\nAbundance of elements.\nSolar System.\nThe composition of the Solar System is similar to that of many other stars, and aside from small anomalies it can be assumed to have formed from a solar nebula that had a uniform composition, and the composition of the Sun's photosphere is similar to that of the rest of the Solar System. The composition of the photosphere is determined by fitting the absorption lines in its spectrum to models of the Sun's atmosphere. By far the largest two elements by fraction of total mass are hydrogen (74.9%) and helium (23.8%), with all the remaining elements contributing just 1.3%. There is a general trend of exponential decrease in abundance with increasing atomic number, although elements with even atomic number are more common than their odd-numbered neighbors (the Oddo\u2013Harkins rule). Compared to the overall trend, lithium, boron and beryllium are depleted and iron is anomalously enriched.\nThe pattern of elemental abundance is mainly due to two factors. The hydrogen, helium, and some of the lithium were formed in about 20 minutes after the Big Bang, while the rest were created in the interiors of stars.\nMeteorites.\nMeteorites come in a variety of compositions, but chemical analysis can determine whether they were once in planetesimals that melted or differentiated. Chondrites are undifferentiated and have round mineral inclusions called chondrules. With the ages of 4.56 billion years, they date to the early solar system. A particular kind, the CI chondrite, has a composition that closely matches that of the Sun's photosphere, except for depletion of some volatiles (H, He, C, N, O) and a group of elements (Li, B, Be) that are destroyed by nucleosynthesis in the Sun. Because of the latter group, CI chondrites are considered a better match for the composition of the early Solar System. Moreover, the chemical analysis of CI chondrites is more accurate than for the photosphere, so it is generally used as the source for chemical abundance, despite their rareness (only five have been recovered on Earth).\nGiant planets.\nThe planets of the Solar System are divided into two groups: the four inner planets are the terrestrial planets (Mercury, Venus, Earth and Mars), with relatively small sizes and rocky surfaces. The four outer planets are the giant planets, which are dominated by hydrogen and helium and have lower mean densities. These can be further subdivided into the gas giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune) that have large icy cores.\nMost of our direct information on the composition of the giant planets is from spectroscopy. Since the 1930s, Jupiter was known to contain hydrogen, methane and ammonium. In the 1960s, interferometry greatly increased the resolution and sensitivity of spectral analysis, allowing the identification of a much greater collection of molecules including ethane, acetylene, water and carbon monoxide. However, Earth-based spectroscopy becomes increasingly difficult with more remote planets, since the reflected light of the Sun is much dimmer; and spectroscopic analysis of light from the planets can only be used to detect vibrations of molecules, which are in the infrared frequency range. This constrains the abundances of the elements H, C and N. Two other elements are detected: phosphorus in the gas phosphine (PH3) and germanium in germane (GeH4).\nThe helium atom has vibrations in the ultraviolet range, which is strongly absorbed by the atmospheres of the outer planets and Earth. Thus, despite its abundance, helium was only detected once spacecraft were sent to the outer planets, and then only indirectly through collision-induced absorption in hydrogen molecules. Further information on Jupiter was obtained from the \"Galileo\" probe when it was sent into the atmosphere in 1995; and the final mission of the Cassini probe in 2017 was to enter the atmosphere of Saturn. In the atmosphere of Jupiter, He was found to be depleted by a factor of 2 compared to solar composition and Ne by a factor of 10, a surprising result since the other noble gases and the elements C, N and S were enhanced by factors of 2 to 4 (oxygen was also depleted but this was attributed to the unusually dry region that Galileo sampled).\nSpectroscopic methods only penetrate the atmospheres of Jupiter and Saturn to depths where the pressure is about equal to 1 bar, approximately Earth's atmospheric pressure at sea level. The Galileo probe penetrated to 22 bars. This is a small fraction of the planet, which is expected to reach pressures of over 40 Mbar. To constrain the composition in the interior, thermodynamic models are constructed using the information on temperature from infrared emission spectra and equations of state for the likely compositions. High-pressure experiments predict that hydrogen will be a metallic liquid in the interior of Jupiter and Saturn, while in Uranus and Neptune it remains in the molecular state. Estimates also depend on models for the formation of the planets. Condensation of the presolar nebula would result in a gaseous planet with the same composition as the Sun, but the planets could also have formed when a solid core captured nebular gas.\nIn current models, the four giant planets have cores of rock and ice that are roughly the same size, but the proportion of hydrogen and helium decreases from about 300 Earth masses in Jupiter to 75 in Saturn and just a few in Uranus and Neptune. Thus, while the gas giants are primarily composed of hydrogen and helium, the ice giants are primarily composed of heavier elements (O, C, N, S), primarily in the form of water, methane, and ammonia. The surfaces are cold enough for molecular hydrogen to be liquid, so much of each planet is likely a hydrogen ocean overlaying one of heavier compounds. Outside the core, Jupiter has a mantle of liquid metallic hydrogen and an atmosphere of molecular hydrogen and helium. Metallic hydrogen does not mix well with helium, and in Saturn, it may form a separate layer below the metallic hydrogen.\nTerrestrial planets.\nTerrestrial planets are believed to have come from the same nebular material as the giant planets, but they have lost most of the lighter elements and have different histories. Planets closer to the Sun might be expected to have a higher fraction of refractory elements, but if their later stages of formation involved collisions of large objects with orbits that sampled different parts of the Solar System, there could be little systematic dependence on position.\nDirect information on Mars, Venus and Mercury largely comes from spacecraft missions. Using gamma-ray spectrometers, the composition of the crust of Mars has been measured by the Mars Odyssey orbiter, the crust of Venus by some of the Venera missions to Venus, and the crust of Mercury by the \"MESSENGER\" spacecraft. Additional information on Mars comes from meteorites that have landed on Earth (the Shergottites, Nakhlites, and Chassignites, collectively known as SNC meteorites). Abundances are also constrained by the masses of the planets, while the internal distribution of elements is constrained by their moments of inertia.\nThe planets condensed from the solar nebula, and much of the details of their composition are determined by fractionation as they cooled. The phases that condense fall into five groups. First to condense are materials rich in refractory elements such as Ca and Al. These are followed by nickel and iron, then magnesium silicates. Below about 700 kelvins (700 K), FeS and volatile-rich metals and silicates form a fourth group, and in the fifth group FeO enter the magnesium silicates. The compositions of the planets and the Moon are \"chondritic\", meaning that within each group the ratios between elements are the same as in carbonaceous chondrites.\nThe estimates of planetary compositions depend on the model used. In the \"equilibrium condensation\" model, each planet was formed from a \"feeding zone\" in which the compositions of solids were determined by the temperature in that zone. Thus, Mercury formed at 1400 K, where iron remained in a pure metallic form and there was little magnesium or silicon in solid form; Venus at 900 K, so all the magnesium and silicon condensed; Earth at 600 K, so it contains FeS and silicates; and Mars at 450 K, so FeO was incorporated into magnesium silicates. The greatest problem with this theory is that volatiles would not condense, so the planets would have no atmospheres and Earth no atmosphere.\nIn \"chondritic mixing\" models, the compositions of chondrites are used to estimate planetary compositions. For example, one model mixes two components, one with the composition of C1 chondrites and one with just the refractory components of C1 chondrites. In another model, the abundances of the five fractionation groups are estimated using an index element for each group. For the most refractory group, uranium is used; iron for the second; the ratios of potassium and thallium to uranium for the next two; and the molar ratio FeO/(FeO+MgO) for the last. Using thermal and seismic models along with heat flow and density, Fe can be constrained to within 10 percent on Earth, Venus, and Mercury. U can be constrained within about 30% on Earth, but its abundance on other planets is based on \"educated guesses\". One difficulty with this model is that there may be significant errors in its prediction of volatile abundances because some volatiles are only partially condensed.\nEarth's crust.\nThe more common rock constituents are nearly all oxides; chlorides, sulfides and fluorides are the only important exceptions to this and their total amount in any rock is usually much less than 1%. By 1911, F. W. Clarke had calculated that a little more than 47% of the Earth's crust consists of oxygen. It occurs principally in combination as oxides, of which the chief are silica, alumina, iron oxides, and various carbonates (calcium carbonate, magnesium carbonate, sodium carbonate, and potassium carbonate). The silica functions principally as an acid, forming silicates, and all the commonest minerals of igneous rocks are of this nature. From a computation based on 1672 analyses of numerous kinds of rocks Clarke arrived at the following as the average percentage composition of the Earth's crust: SiO2=59.71, Al2O3=15.41, Fe2O3=2.63, FeO=3.52, MgO=4.36, CaO=4.90, Na2O=3.55, K2O=2.80, H2O=1.52, TiO2=0.60, P2O5=0.22, (total 99.22%). All the other constituents occur only in very small quantities, usually much less than 1%.\nThese oxides combine in a haphazard way. For example, potash (potassium carbonate) and soda (sodium carbonate) combine to produce feldspars. In some cases, they may take other forms, such as nepheline, leucite, and muscovite, but in the great majority of instances they are found as feldspar. Phosphoric acid with lime (calcium carbonate) forms apatite. Titanium dioxide with ferrous oxide gives rise to ilmenite. Part of the lime forms lime feldspar. Magnesium carbonate and iron oxides with silica crystallize as olivine or enstatite, or with alumina and lime form the complex ferromagnesian silicates of which the pyroxenes, amphiboles, and biotites are the chief. Any excess of silica above what is required to neutralize the bases will separate out as quartz; excess of alumina crystallizes as corundum. These must be regarded only as general tendencies. It is possible, by rock analysis, to say approximately what minerals the rock contains, but there are numerous exceptions to any rule.\nMineral constitution.\nExcept in acid or siliceous igneous rocks containing greater than 66% of silica, known as felsic rocks, quartz is not abundant in igneous rocks. In basic rocks (containing 20% of silica or less) it is rare for them to contain as much silicon, these are referred to as mafic rocks. If magnesium and iron are above average while silica is low, olivine may be expected; where silica is present in greater quantity over ferromagnesian minerals, such as augite, hornblende, enstatite or biotite, occur rather than olivine. Unless potash is high and silica relatively low, leucite will not be present, for leucite does not occur with free quartz. Nepheline, likewise, is usually found in rocks with much soda and comparatively little silica. With high alkalis, soda-bearing pyroxenes and amphiboles may be present. The lower the percentage of silica and alkali's, the greater is the prevalence of plagioclase feldspar as contracted with soda or potash feldspar.\nEarth's crust is composed of 90% silicate minerals and their abundance in the Earth is as follows: plagioclase feldspar (39%), alkali feldspar (12%), quartz (12%), pyroxene (11%), amphiboles (5%), micas (5%), clay minerals (5%); the remaining silicate minerals make up another 3% of Earth's crust. Only 8% of the Earth is composed of non-silicate minerals such as carbonates, oxides, and sulfides.\nThe other determining factor, namely the physical conditions attending consolidation, plays, on the whole, a smaller part, yet is by no means negligible. Certain minerals are practically confined to deep-seated intrusive rocks, e.g., microcline, muscovite, diallage. Leucite is very rare in plutonic masses; many minerals have special peculiarities in microscopic character according to whether they crystallized in-depth or near the surface, e.g., hypersthene, orthoclase, quartz. There are some curious instances of rocks having the same chemical composition, but consisting of entirely different minerals, e.g., the hornblendite of Gran, in Norway, which contains only hornblende, has the same composition as some of the camptonites of the same locality that contain feldspar and hornblende of a different variety. In this connection, we may repeat what has been said above about the corrosion of porphyritic minerals in igneous rocks. In rhyolites and trachytes, early crystals of hornblende and biotite may be found in great numbers partially converted into augite and magnetite. Hornblende and biotite were stable under the pressures and other conditions below the surface, but unstable at higher levels. In the ground-mass of these rocks, augite is almost universally present. But the plutonic representatives of the same magma, granite, and syenite contain biotite and hornblende far more commonly than augite.\nFelsic, intermediate and mafic igneous rocks.\nThose rocks that contain the most silica, and on crystallizing yield free quartz, form a group generally designated the \"felsic\" rocks. Those again that contain the least silica and most magnesia and iron, so that quartz is absent while olivine is usually abundant, form the \"mafic\" group. The \"intermediate\" rocks include those characterized by the general absence of both quartz and olivine. An important subdivision of these contains a very high percentage of alkalis, especially soda, and consequently has minerals such as nepheline and leucite not common in other rocks. It is often separated from the others as the \"alkali\" or \"soda\" rocks, and there is a corresponding series of mafic rocks. Lastly, a small sub-group rich in olivine and without feldspar has been called the \"ultramafic\" rocks. They have very low percentages of silica but much iron and magnesia.\nExcept these last, practically all rocks contain felspars or feldspathoid minerals. In the acid rocks, the common feldspars are orthoclase, perthite, microcline, and oligoclase\u2014all having much silica and alkalis. In the mafic rocks labradorite, anorthite, and bytownite prevail, being rich in lime and poor in silica, potash, and soda. Augite is the most common ferromagnesian in mafic rocks, but biotite and hornblende are on the whole more frequent in felsic rocks.\nRocks that contain leucite or nepheline, either partly or wholly replacing felspar, are not included in this table. They are essentially of intermediate or of mafic character. We might in consequence regard them as varieties of syenite, diorite, gabbro, etc., in which feldspathoid minerals occur, and indeed there are many transitions between syenites of ordinary type and nepheline \u2014 or leucite \u2014 syenite, and between gabbro or dolerite and theralite or essexite. But, as many minerals develop in these \"alkali\" rocks that are uncommon elsewhere, it is convenient in a purely formal classification like that outlined here to treat the whole assemblage as a distinct series.\nThis classification is based essentially on the mineralogical constitution of the igneous rocks. Any chemical distinctions between the different groups, though implied, are relegated to a subordinate position. It is admittedly artificial, but it has grown up with the growth of the science and is still adopted as the basis on which more minute subdivisions are erected. The subdivisions are by no means of equal value. The syenites, for example, and the peridotites, are far less important than the granites, diorites, and gabbros. Moreover, the effusive andesites do not always correspond to the plutonic diorites but partly also to the gabbros. As the different kinds of rock, regarded as aggregates of minerals, pass gradually into one another, transitional types are very common and are often so important as to receive special names. The quartz-syenites and nordmarkites may be interposed between granite and syenite, the tonalites and adamellites between granite and diorite, the monzonites between syenite and diorite, norites and hyperites between diorite and gabbro, and so on.\nTrace metals in the ocean.\nTrace metals readily form complexes with major ions in the ocean, including hydroxide, carbonate, and chloride and their chemical speciation changes depending on whether the environment is oxidized or reduced. Benjamin (2002) defines complexes of metals with more than one type of ligand, other than water, as mixed-ligand-complexes. In some cases, a ligand contains more than one \"donor\" atom, forming very strong complexes, also called chelates (the ligand is the chelator). One of the most common chelators is EDTA (ethylenediaminetetraacetic acid), which can replace six molecules of water and form strong bonds with metals that have a plus two charge. With stronger complexation, lower activity of the free metal ion is observed. One consequence of the lower reactivity of complexed metals compared to the same concentration of free metal is that the chelation tends to stabilize metals in the aqueous solution instead of in solids.\nConcentrations of the trace metals cadmium, copper, molybdenum, manganese, rhenium, uranium and vanadium in sediments record the redox history of the oceans. Within aquatic environments, cadmium(II) can either be in the form CdCl+(aq) in oxic waters or CdS(s) in a reduced environment. Thus, higher concentrations of Cd in marine sediments may indicate low redox potential conditions in the past. For copper(II), a prevalent form is CuCl+(aq) within oxic environments and CuS(s) and Cu2S within reduced environments. The reduced seawater environment leads to two possible oxidation states of copper, Cu(I) and Cu(II). Molybdenum is present as the Mo(VI) oxidation state as MoO42\u2212(aq) in oxic environments. Mo(V) and Mo(IV) are present in reduced environments in the forms MoO2+(aq) and MoS2(s). Rhenium is present as the Re(VII) oxidation state as ReO4\u2212 within oxic conditions, but is reduced to Re(IV) which may form ReO2 or ReS2. Uranium is in oxidation state VI in UO2(CO3)34\u2212(aq) and is found in the reduced form UO2(s). Vanadium is in several forms in oxidation state V(V); HVO42\u2212 and H2VO4\u2212. Its reduced forms can include VO2+, VO(OH)3\u2212, and V(OH)3. These relative dominance of these species depends on pH.\nIn the water column of the ocean or deep lakes, vertical profiles of dissolved trace metals are characterized as following \"conservative\u2013type\", \"nutrient\u2013type\", or \"scavenged\u2013type\" distributions. Across these three distributions, trace metals have different residence times and are used to varying extents by planktonic microorganisms. Trace metals with conservative-type distributions have high concentrations relative to their biological use. One example of a trace metal with a conservative-type distribution is molybdenum. It has a residence time within the oceans of around 8 x 105 years and is generally present as the molybdate anion (MoO42\u2212). Molybdenum interacts weakly with particles and displays an almost uniform vertical profile in the ocean. Relative to the abundance of molybdenum in the ocean, the amount required as a metal cofactor for enzymes in marine phytoplankton is negligible.\nTrace metals with nutrient-type distributions are strongly associated with the internal cycles of particulate organic matter, especially the assimilation by plankton. The lowest dissolved concentrations of these metals are at the surface of the ocean, where they are assimilated by plankton. As dissolution and decomposition occur at greater depths, concentrations of these trace metals increase. Residence times of these metals, such as zinc, are several thousand to one hundred thousand years. Finally, an example of a scavenged-type trace metal is aluminium, which has strong interactions with particles as well as a short residence time in the ocean. The residence times of scavenged-type trace metals are around 100 to 1000 years. The concentrations of these metals are highest around bottom sediments, hydrothermal vents, and rivers. For aluminium, atmospheric dust provides the greatest source of external inputs into the ocean.\nIron and copper show hybrid distributions in the ocean. They are influenced by recycling and intense scavenging. Iron is a limiting nutrient in vast areas of the oceans and is found in high abundance along with manganese near hydrothermal vents. Here, many iron precipitates are found, mostly in the forms of iron sulfides and oxidized iron oxyhydroxide compounds. Concentrations of iron near hydrothermal vents can be up to one million times the concentrations found in the open ocean.\nUsing electrochemical techniques, it is possible to show that bioactive trace metals (zinc, cobalt, cadmium, iron, and copper) are bound by organic ligands in surface seawater. These ligand complexes serve to lower the bioavailability of trace metals within the ocean. For example, copper, which may be toxic to open ocean phytoplankton and bacteria, can form organic complexes. The formation of these complexes reduces the concentrations of bioavailable inorganic complexes of copper that could be toxic to sea life at high concentrations. Unlike copper, zinc toxicity in marine phytoplankton is low and there is no advantage to increasing the organic binding of Zn2+. In high-nutrient, low-chlorophyll regions, iron is the limiting nutrient, with the dominant species being strong organic complexes of Fe(III).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39564", "revid": "50034649", "url": "https://en.wikipedia.org/wiki?curid=39564", "title": "BASIC09", "text": "BASIC09 is a structured BASIC programming language dialect developed by Microware on behalf of Motorola for the then-new Motorola 6809 CPU and released in February 1980. It is primarily used with the OS-9 operating system, released in 1979. Microware also released a version for OS-9/68k on the 68000 as Microware BASIC.\nIn contrast to typical BASICs of the era, BASIC09 includes a multi-pass compiler that produces compact bytecode known as I-code. I-code replaces a number of data structures found in other BASICs with direct pointers to code and values, speeding performance. Users can further compile code using the codice_1 command, at which point it can be called directly by OS-9 and operated as native code. In the case of PACKed code, a cut-down version of the BASIC09 runtime system is used, Runb, further improving memory footprint and load time.\nThe language includes a number of structured programming additions, including local variables, the ability to ignore line numbers in favor of named routines, user-defined structures, and several distinct base data types including 16-bit and 8-bit (byte) integers, in addition to floating point and strings.\nSyntax.\nProgram organization.\nA key difference between BASIC09 and conventional BASICs of the era, like the canonical Microsoft BASIC, is the addition of the codice_2 structure which created separately executable blocks of code. Code in a codice_2 had more in common with complete programs in other BASICs, including the variables being local to the code, and their ability to be executed in a stand-alone fashion. codice_2s were called by name using the codice_5 command, and could include variables for function-call semantics; for instance, codice_6 calls a procedure named codice_7 that takes two parameters. Parameters were imported into the procedure using the codice_8 keyword, in this example codice_9:\n PROCEDURE add\n PARAM a,b\n PRINT a+b\nA side-effect of the use of named procedures is that the resulting memory workspace is, in effect, its own namespace. In this respect, the BASIC09 system appears to the user to be a directory of callable programs. This contrasts with typical BASICs, where only one program is available at a given time and the construction of larger programs calling library-like code generally requires the source code to be copied and pasted between separate programs. In BASIC09, the user can codice_10 procedures by name into the workspace and then call them from their own code to construct larger programs from the separately stored procedures.\nIn addition to code in the workspace, if the program invokes codice_5 with a procedure name that could not be found, it would then look for a disk file with the same name and load and run that file. This worked not only with BASIC09 code, but also any other executable program, including machine language files. This meant that BASIC09 could easily call system routines.\nIn addition to codice_5, other common BASIC commands likewise used names. For instance, codice_13 would print out the source code (\"list\") the procedure named \"bob\", while codice_14 prints out all of the procedures currently in memory. The prettyprinted output from codice_15 could be redirected to a file or a printer with a shell-like notation, e.g. codice_16. One could also codice_17 and codice_10 procedures from storage.\nStructured programming.\nIn addition to the organizational properties of the codice_2, BASIC09 also included a number of extensions to the flow control statements found in BASIC to provide more structure. For instance, the codice_20 statement could be used in the traditional codice_20...codice_22 format on a single line, or it could be used in a structured multi-line format:\n IF x&gt;10 THEN\n PRINT \"x is larger than 10\"\n ELSE\n PRINT \"x is smaller than 10\"\n ENDIF\ncodice_23 loops naturally have a structured format as the codice_24 can be placed on any line, but BASIC09 also added codice_25 and codice_26 for additional clarity when working with non-indexed loops. It also included the center-exit codice_27 which used the codice_28 statement for testing anywhere in the loop's body.\nData types.\nBASIC09 included several built-in data types. In addition to the traditional string (STRING) and 40-bit floating point (REAL) types found in most BASICs of the era, it also included the 16-bit signed INTEGER, the 8-bit unsigned BYTE, and the logical BOOLEAN type. The BOOLEAN types were not packed into bytes, a single BOOLEAN used an entire 8-bit byte to store a single value. The language provided separate bytewise boolean operators for bitwise operations on BYTEs and INTEGERs. In contrast to other BASICs that also operated different base types, BASIC09 did not \"decorate\" the variable name to indicate the type, and instead used the codice_29 for definitions; for instance, codice_30 to declare two BOOLEAN variables, or codice_31 for an array of five INTEGERs.\nAdditionally, BASIC09 included the codice_32 keyword, which allowed compound types to be defined, with each \"element\" listed on a single line separated by semicolons. For instance:\n TYPE employee_record=name:STRING;number(2):INTEGER;former:BOOLEAN\ndefines an employee record type named codice_33 with three elements, codice_34, codice_35 and codice_36. The employee_record type can now be used in a definition like any other type, for instance, codice_37, which defines an array of 100 employee_record's. The elements are accessed in code using dot notation, for instance, codice_38.\nRuntime.\nEditing.\nLine numbers were used in most BASIC dialects primarily as a way to support the editor. Users would edit particular lines of code by typing a number, with the text following either adding to or replacing the lines already in memory. As every line of code had a number, this also made them suitable for indicating the target of a codice_39 or codice_40, compared to other languages like FORTRAN where a separate \"line label\" was used for this purpose.\nBASIC09 did not normally use line numbers, so its editor had to be modified to allow the user to edit lines without referring to them by number. However, BASIC09 did not assume any sort of full-screen capability, so using cursor keys was not an option. Instead, the system had a separate editor prompt and allowed the user to move about using the + and - keys, moving forward or backward one line at a time. To insert a new line of code without a line number, the user left a blank at the start of the statement.\nNote that the language is case sensitive for user-provided values like procedure and variable names, but not for keywords. Keywords typed into the editor in lower case will be shown in upper case when the program is codice_15ed. BASIC09 allowed multiple statements on a single line of code, but used the \\ as a separator instead of the : used in most dialects. This is because it used the colon in the codice_42 assignment operator, which was in addition to the normal codice_43. codice_42 was identical in effect to codice_43, but made the difference between assignments and comparisons more obvious.\nCompiler.\nThe internal multipass compiler converts BASIC09 source code into a tokenized, optimized, bytecode, called I-code. I-code differs from the more traditional tokenizing approach found in most BASICs in that a number of items were placed directly in memory instead of using references that then had to be looked up.\nFor instance, in MS-based interpreters, a variable reference in code is left in string format; the variable codice_46 would be represented in memory by the three ASCII characters \"VAR\". During execution, when this variable is encountered in the code the interpreter has to look up that string in a table of variables, find the associated storage address in memory, and then finally read the value stored in that location. The table is usually constructed so that the value follows the name, to save time during the final lookup.\nIn contrast, in I-code the address of the variable is determined in advance and the reference in code is replaced by that address. This avoids a runtime search through the variable table. Other optimizations include a separate codice_23 routine used when the index variable is an INTEGER, and separate INTEGER and REAL math libraries.\nFor added performance, BASIC09 also included the codice_1 command which took a procedure name and returned an optimized version. Some of these optimizations included removing non-coding instructions like code comments and the replacement of constant expressions to a single value. For instance, codice_1 would recognize that codice_50 contains only constants on the right, and replaces it with the code codice_51, which requires only a single operation at runtime, the addition, removing the division and square root. codice_1 reduced the memory footprint of the procedure and improved performance by about 10 to 30%.\nLightweight runtime.\nAlthough it was common to run programs within the BASIC09 environment, as it was in other BASICs, BASIC09 also shipped with a separate run-only version of the code known as Runb. Runb removed the editing and debugging features of the system, and was about half the size of the full BASIC09 as a result.\nThe purpose of Runb was primarily to run PACKed modules when called from other programs. This meant that if the user typed in the name of a BASIC09 module in the OS/9 command line, and that module has been marked as PACKed, it is opened and run by Runb instead of the BASIC09. This reduces memory footprint and improves load time.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39565", "revid": "41364938", "url": "https://en.wikipedia.org/wiki?curid=39565", "title": "Noble savage", "text": "Stock character\nIn Western anthropology, philosophy, and literature, the Myth of the Noble savage refers to a stock character who is uncorrupted by civilization. As such, the \"noble\" savage symbolizes the innate goodness and moral superiority of a primitive people living in harmony with nature. In the heroic drama of the stageplay \"The Conquest of Granada by the Spaniards\" (1672), John Dryden represents the \"noble savage\" as an archetype of Man-as-Creature-of-Nature.\nThe intellectual politics of the Stuart Restoration (1660\u20131688) expanded Dryden's playwright usage of \"savage\" to denote a human \"wild beast\" and a \"wild man\". Concerning civility and incivility, in the \"Inquiry Concerning Virtue, or Merit\" (1699), the philosopher Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, said that men and women possess an innate morality, a sense of right and wrong conduct, which is based upon the intellect and the emotions, and not based upon religious doctrine.\nIn 18th-century anthropology, the term \"noble savage\" then denoted \"nature's gentleman\", an ideal man born from the sentimentalism of moral sense theory. In the 19th century, in the essay \"The Noble Savage\" (1853) Charles Dickens rendered the noble savage into a rhetorical oxymoron by satirizing the British romanticisation of Primitivism in philosophy and in the arts made possible by moral sentimentalism.\nIn many ways, the myth of the noble savage entails fantasies about the non-West that cut to the core of the conversation in the social sciences about Orientalism, colonialism and exoticism. One question that emerges is whether an admiration of \"the Other\" as noble undermines or reproduces the dominant hierarchy, whereby the Other is subjugated by Western powers.\nOrigins.\nThe first century Roman work \"De origine et situ Germanorum\" (\"On the Origin and Situation of the Germans\") by Publius Cornelius Tacitus introduced the idea of the \"noble savage\" to the Western World in 98 AD, describing the ancient Germanic people as aligned with ancient Roman virtues, such as bravery and honesty.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Thus with their virtue protected they live uncorrupted by the allurements of public shows or the stimulant of feastings. Clandestine correspondence is equally unknown to men and women. Very rare for so numerous a population is adultery [...] No one in Germany laughs at vice, nor do they call it the fashion to corrupt and to be corrupted.\u2014\u200a\nThe 12th-century Andalusian allegorical novel \"Hayy ibn Yaqdhan\" developed the idea through its \"noble savage\" titular protagonist understanding natural theology in a \"tabula rasa\" existence without any education or contact with the outside world, inspiring later Western philosophy and literature during Age of Enlightenment.\nThe stock character of the \"noble savage\" appears in the essay \"Of Cannibals\" (1580), about the Tupinamb\u00e1 people of Brazil, wherein the philosopher Michel de Montaigne presents \"Nature's Gentleman\", the \"bon sauvage\" counterpart to civilized Europeans in the 16th century.\nThe first usage of the term \"noble savage\" in English literature occurs in John Dryden's stageplay \"The Conquest of Granada by the Spaniards\" (1672), about the troubled love of the hero Almanzor and the Moorish beauty Almahide, in which the protagonist defends his life as a free man by denying a prince's right to put him to death, because he is not a subject of the prince:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nBy the 18th century, Montaigne's predecessor to the noble savage, \"nature's gentleman\" was a stock character usual to the sentimental literature of the time, for which a type of non-European Other became a background character for European stories about adventurous Europeans in the strange lands beyond continental Europe. For the novels, the opera, and the stageplays, the stock of characters included the \"Virtuous Milkmaid\" and the \"Servant-More-Clever-Than-the-Master\" (e.g. Sancho Panza and Figaro), literary characters who personify the moral superiority of working-class people in the fictional world of the story.\nIn English literature, British North America was the geographic \"locus classicus\" for adventure and exploration stories about European encounters with the noble savage natives, such as the historical novel \"The Last of the Mohicans: A Narrative of 1757\" (1826), by James Fenimore Cooper, and the epic poem \"The Song of Hiawatha\" (1855), by Henry Wadsworth Longfellow, both literary works presented the primitivism (geographic, cultural, political) of North America as an ideal place for the European man to commune with Nature, far from the artifice of civilisation; yet in the poem \u201cAn Essay on Man\u201d (1734), the English poet Alexander Pope portrays the American Indian thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nTo Pope, the American Indian was an abstract being unlike his insular European self; thus, from the Western perspective of \"An Essay on Man\", Pope's metaphoric usage of \"poor\" means \"uneducated and a heathen\", but also denotes a savage who is happy with his rustic life in harmony with Nature, and who believes in deism, a form of natural religion \u2014 the idealization and devaluation of the non-European Other derived from the mirror logic of the Enlightenment belief that \"men, everywhere and in all times, are the same\".\nLike Dryden's \"noble savage\" term, Pope's phrase \"Lo, the Poor Indian!\" was used to dehumanize the natives of North America for European purposes, and so justified white settlers' conflicts with the local Indians for possession of the land. In the mid-19th century, the journalist-editor Horace Greeley published the essay \"Lo! The Poor Indian!\" (1859), about the social condition of the American Indian in the modern United States:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I have learned to appreciate better than hitherto, and to make more allowance for the dislike, aversion, contempt wherewith Indians are usually regarded by their white neighbors, and have been since the days of the Puritans. It needs but little familiarity with the actual, palpable aborigines to convince anyone that the poetic Indian \u2014 the Indian of Cooper and Longfellow \u2014 is only visible to the poet's eye. To the prosaic observer, the average Indian of the woods and prairies is a being who does little credit to human nature \u2014 a slave of appetite and sloth, never emancipated from the tyranny of one animal passion, save by the more ravenous demands of another.&lt;br&gt;&lt;br&gt;\nAs I passed over those magnificent bottoms of the Kansas, which form the reservations of the Delawares, Potawatamies, etc., constituting the very best corn-lands on Earth, and saw their owners sitting around the doors of their lodges at the height of the planting season, and in as good, bright planting weather as sun and soil ever made, I could not help saying: \"These people must die out \u2014 there is no help for them. God has given this earth to those who will subdue and cultivate it, and it is vain to struggle against His righteous decree.\"\nMoreover, during the American Indian Wars (1609\u20131924) for possession of the land, European white settlers considered the Indians \"an inferior breed of men\" and mocked them by using the terms \"Lo\" and \"Mr. Lo\" as disrespectful forms of address. In the Western U.S., those terms of address also referred to East Coast humanitarians whose conception of the mythical noble-savage American Indian was unlike the warrior who confronted and fought the frontiersman. Concerning the story of the settler Thomas Alderdice, whose wife was captured and killed by Cheyenne Indians, \"The Leavenworth, Kansas, Times and Conservative\" newspaper said: \"We wish some philanthropists, who talk about civilizing the Indians, could have heard this unfortunate and almost broken-hearted man tell his story. We think [that the philanthropists] would at least have wavered a little in their [high] opinion of the Lo family.\"\nCultural stereotype.\nThe Roman Empire.\nIn Western literature, the Roman book \"De origine et situ Germanorum\" (\"On the Origin and Situation of the Germans\", 98 CE), by the historian Publius Cornelius Tacitus, introduced the anthropologic concept of the \"noble savage\" to the Western World; later a cultural stereotype who featured in the exotic-place tourism reported in the European travel literature of the 17th and the 18th centuries.\nAl-Andalus.\nThe 12th-century Andalusian novel \"The Living Son of the Vigilant\" (\"\u1e24ayy ibn Yaq\u1e93\u0101n\", 1160), by the polymath Ibn Tufail, explores the subject of natural theology as a means to understand the material world. The protagonist is a \"wild man\" isolated from his society, whose trials and tribulations lead him to knowledge of Allah by living a rustic life in harmony with Mother Nature.\nKingdom of Spain.\nIn the 15th century, soon after arriving to the Americas in 1492, the Europeans employed the term \"savage\" to dehumanise the \"indig\u00e8nes\" (noble-savage natives) of the newly discovered \"New World\" as ideological justification for the European colonization of the Americas, called the Age of Discovery (1492\u20131800); thus with the dehumanizing stereotypes of the \"noble savage\" and the \"indig\u00e8ne\", the \"savage\" and the \"wild man\" the Europeans granted themselves the right to colonize the natives inhabiting the islands and the continental lands of the northern, the central, and the southern Americas.\nThe conquistador mistreatment of the indigenous peoples of the Viceroyalty of New Spain (1521\u20131821) eventually produced bad-conscience recriminations amongst the European intelligentsias for and against colonialism. As the Roman Catholic Bishop of Chiapas, the priest Bartolom\u00e9 de las Casas witnessed the enslavement of the \"indig\u00e8nes\" of New Spain, yet idealized them into morally innocent noble savages living a simple life in harmony with Mother Nature. At the Valladolid debate (1550\u20131551) of the moral philosophy of enslaving the native peoples of the Spanish colonies, Bishop de las Casas reported the noble-savage culture of the natives, especially noting their plain-manner social etiquette and that they did not have the social custom of telling lies.\nKingdom of France.\nIn the intellectual debates of the late 16th and 17th centuries, philosophers used the racist stereotypes of the \"savage\" and the \"good savage\" as moral reproaches of the European monarchies fighting the Thirty Years' War (1618\u20131648) and the French Wars of Religion (1562\u20131598). In the essay \"Of Cannibals\" (1580), Michel de Montaigne reported that the Tupinamb\u00e1 people of Brazil ceremoniously eat the bodies of their dead enemies, as a matter of honour, whilst reminding the European reader that such \"wild man\" behavior was analogous to the religious barbarism of burning at the stake: \"One calls \u2018barbarism\u2019 whatever he is not accustomed to.\" The academic Terence Cave further explains Montaigne's point of moral philosophy:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The cannibal practices are admitted [by Montaigne] but presented as part of a complex and balanced set of customs and beliefs which \"make sense\" in their own right. They are attached to a powerfully positive morality of valor and pride, one that would have been likely to appeal to early modern codes of honor, and they are contrasted with modes of behavior in the France of the wars of religion, which appear as distinctly less attractive, such as torture and barbarous methods of execution.\nAs philosophic reportage, \"Of Cannibals\" applies cultural relativism to compare the civilized European to the uncivilized noble savage. Montaigne's anthropological report about cannibalism in Brazil indicated that the Tupinamb\u00e1 people were neither a noble nor an exceptionally good folk, yet neither were the Tupinamb\u00e1 culturally or morally inferior to his contemporary, 16th-century European civilization. From the perspective of Classical liberalism of Montaigne's humanist portrayal of the customs of honor of the Tupinamb\u00e1 people indicates Western philosophic recognition that people are people, despite their different customs, traditions, and codes of honor. The academic David El Kenz explicates Montaigne's background concerning the violence of customary morality:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In his \"Essais\" ... Montaigne discussed the first three wars of religion (1562\u201363; 1567\u201368; 1568\u201370) quite specifically; he had personally participated in [the wars], on the side of the [French] royal army, in southwestern France. The [anti-Protestant] St. Bartholomew's Day massacre [1572] led him to retire to his lands in the P\u00e9rigord region, and remain silent on all public affairs until the 1580s. Thus, it seems that he was traumatized by the massacre. To him, cruelty was a criterion that differentiated the Wars of Religion [1562\u20131598] from previous conflicts, which he idealized. Montaigne considered that three factors accounted for the shift from regular war to the carnage of civil war: popular intervention, religious demagogy, and the never-ending aspect of the conflict. ...\nHe chose to depict cruelty through the image of hunting, which fitted with the tradition of condemning hunting for its association with blood and death, but it was still quite surprising, to the extent that this practice was part of the aristocratic way of life. Montaigne reviled hunting by describing it as an urban massacre scene. In addition, the man\u2013animal relationship allowed him to define virtue, which he presented as the opposite of cruelty. ... [As] a sort of natural benevolence based on ... personal feelings.\nMontaigne associated the [human] propensity to cruelty toward animals, with that exercised toward men. After all, following the St. Bartholomew's Day massacre, the invented image of Charles IX shooting Huguenots from the Louvre Palace window did combine the established reputation of the King as a hunter, with a stigmatization of hunting, a cruel and perverted custom, did it not?\nLiterature.\nThe themes about the person and \"persona\" of the mythical noble savage are the subjects of the novel \"Oroonoko: Or the Royal Slave\" (1688), by Aphra Behn, which is the tragic love story between Oroonoko and the beautiful Imoinda, an African king and queen respectively. At Coramantien, Ghana, the protagonist is deceived and delivered into the Atlantic slave trade (16th\u201319th centuries), and Oroonoko becomes a slave of plantation colonists in Surinam (Dutch Guiana, 1667\u20131954). In the course of his enslavement, Oroonoko meets the woman who narrates to the reader the life and love of Prince Oroonoko, his enslavement, his leading a slave rebellion against the Dutch planters of Surinam, and his consequent execution by the Dutch colonialists.\nDespite Behn having written the popular novel for money, \"Oroonoko\" proved to be political-protest literature against slavery, because the story, plot, and characters followed the narrative conventions of the European romance novel. In the event, the Irish playwright Thomas Southerne adapted the novel \"Oroonoko\" into the stage play \"Oroonoko: A Tragedy\" (1696) that stressed the pathos of the love story, the circumstances, and the characters, which consequently gave political importance to the play and the novel for the candid cultural representation of slave-powered European colonialism.\nUses of the stereotype.\nRomantic primitivism.\nIn the 1st century CE, in the book \"Germania\", Tacitus ascribed to the Germans the cultural superiority of the \"noble savage\" way of life, because Rome was too civilized, unlike the savage Germans. The art historian Erwin Panofsky explains that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There had been, from the beginning of Classical speculation, two contrasting opinions about the natural state of man, each of them, of course, a \"Gegen-Konstruktion\" to the conditions under which it was formed. One view, termed \"soft\" primitivism in an illuminating book by Lovejoy and Boas, conceives of primitive life as a golden age of plenty, innocence, and happiness \u2014 in other words, as civilized life purged of its vices. The other, \"hard\" form of primitivism conceives of primitive life as an almost subhuman existence full of terrible hardships and devoid of all comforts \u2014 in other words, as civilized life stripped of its virtues.\u2014\u200a\nIn the novel \"The Adventures of Telemachus, Son of Ulysses\" (1699), in the \u201cEncounter with the Mandurians\u201d (Chapter IX), the theologian Fran\u00e7ois F\u00e9nelon presented the \"noble savage\" stock character in conversation with civilized men from Europe about possession and ownership of Nature:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;On our arrival upon this coast we found there a savage race who ... lived by hunting and by the fruits which the trees spontaneously produced. These people ... were greatly surprised and alarmed by the sight of our ships and arms and retired to the mountains. But since our soldiers were curious to see the country and hunt deer, they were met by some of these savage fugitives.&lt;br&gt;\nThe leaders of the savages accosted them thus: \u201cWe abandoned for you, the pleasant sea-coast, so that we have nothing left, but these almost inaccessible mountains: at least, it is just that you leave us in peace and liberty. Go, and never forget that you owe your lives to our feeling of humanity. Never forget that it was from a people whom you call rude and savage that you receive this lesson in gentleness and generosity. ... We abhor that brutality which, under the gaudy names of ambition and glory, ... sheds the blood of men who are all brothers. ... We value health, frugality, liberty, and vigor of body and mind: the love of virtue, the fear of the gods, a natural goodness toward our neighbors, attachment to our friends, fidelity to all the world, moderation in prosperity, fortitude in adversity, courage always bold to speak the truth, and abhorrence of flattery. ...&lt;br&gt;\nIf the offended gods so far blind you as to make you reject peace, you will find, when it is too late, that the people who are moderate and lovers of peace are the most formidable in war.\u201d\u2014\u200a\nIn the Kingdom of France, critics of the Crown and Church risked censorship and summary imprisonment without trial, and primitivism was political protest against the repressive imperial r\u00e8gimes of Louis XIV and Louis XV. In his travelogue of North America, the writer Louis-Armand de Lom d'Arce de Lahontan, Baron de Lahontan, who had lived with the Huron Indians (Wyandot people), ascribed deist and egalitarian politics to Adario, a Canadian Indian who played the role of noble savage for French explorers:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Adario sings the praises of Natural Religion. ... As against society, he puts forward a sort of primitive Communism, of which the certain fruits are Justice and a happy life. ... [The Savage] looks with compassion on poor civilized man \u2014 no courage, no strength, incapable of providing himself with food and shelter: a degenerate, a moral \"cretin\", a figure of fun in his blue coat, his red hose, his black hat, his white plume and his green ribands. He never really lives, because he is always torturing the life out of himself to clutch at wealth and honors, which, even if he wins them, will prove to be but glittering illusions. ... For science and the arts are but the parents of corruption. The Savage obeys the will of Nature, his kindly mother, therefore he is happy. It is civilized folk who are the real barbarians.\u2014\u200a\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Interest in the remote peoples of the Earth, in the unfamiliar civilizations of the East, in the untutored races of America and Africa, was vivid in France in the 18th century. Everyone knows how Voltaire and Montesquieu used Hurons or Persians to hold up the [looking] glass to Western manners and morals, as Tacitus used the Germans to criticize the society of Rome. But very few ever look into the seven volumes of the Abb\u00e9 Raynal's \"History of the Two Indies\", which appeared in 1772. It is however one of the most remarkable books of the century. Its immediate practical importance lay in the array of facts which it furnished to the friends of humanity in the movement against negro slavery. But it was also an effective attack on the Church and the sacerdotal system. ... Raynal brought home to the conscience of Europeans the miseries which had befallen the natives of the New World through the Christian conquerors and their priests. He was not indeed an enthusiastic preacher of Progress. He was unable to decide between the comparative advantages of the savage state of nature and the most highly cultivated society. But he observes that \"the human race is what we wish to make it\", that the felicity of Man depends entirely on the improvement of legislation, and ... his view is generally optimistic.\u2014\u200a\nBenjamin Franklin.\nBenjamin Franklin was critical of government indifference to the Paxton Boys massacre of the Susquehannock in Lancaster County, Pennsylvania in December 1763. Within weeks of the murders, he published \"A Narrative of the Late Massacres in Lancaster County\", in which he referred to the Paxton Boys as \"Christian white savages\" and called for judicial punishment of those who carried the Bible in one hand and a hatchet in the other.\nWhen the Paxton Boys led an armed march on Philadelphia in February 1764, with the intent of killing the Moravian Lenape and Mohican who had been given shelter there, Franklin recruited associators including Quakers to defend the city and led a delegation that met with the Paxton leaders at Germantown outside Philadelphia. The marchers dispersed after Franklin convinced them to submit their grievances in writing to the government.\nIn his 1784 pamphlet \"Remarks Concerning the Savages of North America\", Franklin especially noted the racism inherent to the colonists using the word \"savage\" as a synonym for indigenous people:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Savages\" we call them, because their manners differ from ours, which we think the perfection of civility; they think the same of theirs.\nFranklin praised the way of life of indigenous people, their customs of hospitality, their councils of government, and acknowledged that while some Europeans had foregone civilization to live like a \"savage\", the opposite rarely occurred, because few indigenous people chose \"civilization\" over \"savagery\".\nJean-Jacques Rousseau.\nLike the Earl of Shaftesbury in the \"Inquiry Concerning Virtue, or Merit\" (1699), Jean-Jacques Rousseau likewise believed that Man is innately good, and that urban civilization, characterized by jealousy, envy, and self-consciousness, has made men bad in character. In \"Discourse on the Origins of Inequality Among Men\" (1754), Rousseau said that in the primordial state of nature, man was a solitary creature who was not \"m\u00e9chant\" (bad), but was possessed of an \"innate repugnance to see others of his kind suffer.\"\nMoreover, as the \"philosophe\" of the Jacobin radicals of the French Revolution (1789\u20131799), ideologues accused Rousseau of claiming that the mythical \"noble savage\" was a real type of man, despite the term not appearing in work written by Rousseau; in addressing \"The Supposed Primitivism of Rousseau\u2019s Discourse on Inequality\" (1923), the academic Arthur O. Lovejoy said that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The notion that Rousseau\u2019s \"Discourse on Inequality\" was essentially a glorification of the State of Nature, and that its influence tended to wholly or chiefly to promote \u201cPrimitivism\u201d is one of the most persistent historical errors.\nIn the \"Discourse on the Origins of Inequality\", Rousseau said that the rise of humanity began a \"formidable struggle for existence\" between the species man and the other animal species of Nature. That under the pressure of survival emerged \"le caract\u00e8re sp\u00e9cifique de l'esp\u00e8ce humaine\", the specific quality of character, which distinguishes man from beast, such as intelligence capable of \"almost unlimited development\", and the \"facult\u00e9 de se perfectionner\", the capability of perfecting himself.\nHaving invented tools, discovered fire, and transcended the state of nature, Rousseau said that \"it is easy to see. . . . that all our labors are directed upon two objects only, namely, for oneself, the commodities of life, and consideration on the part of others\"; thus \"amour propre\" (self-regard) is a \"factitious feeling arising, only in society, which leads a man to think more highly of himself than of any other.\" Therefore, \"it is this desire for reputation, honors, and preferment which devours us all . . . this rage to be distinguished, that we own what is best and worst in men \u2014 our virtues and our vices, our sciences and our errors, our conquerors and our philosophers \u2014 in short, a vast number of evil things and a small number of good [things]\"; that is the aspect of character \"which inspires men to all the evils which they inflict upon one another.\"\nMen become men only in a civil society based upon law, and only a reformed system of education can make men good; the academic Lovejoy explains that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;For Rousseau, man's good lay in departing from his \"natural\" state \u2014 but not too much; \"perfectability\", up to a certain point, was desirable, though beyond that point an evil. Not its infancy but its \"jeunesse\" [youth] was the best age of the human race. The distinction may seem to us slight enough; but in the mid-eighteenth century it amounted to an abandonment of the stronghold of the primitivistic position. Nor was this the whole of the difference. As compared with the then-conventional pictures of the savage state, Rousseau's account, even of this third stage, is far less idyllic; and it is so because of his fundamentally unfavorable view of human nature \"qu\u00e2\" human. ... [Rousseau's] savages are quite unlike Dryden's Indians: \"Guiltless men, that danced away their time, / Fresh as the groves and happy as their clime\" or Mrs. Aphra Behn's natives of Surinam, who represented an absolute idea of the first state of innocence \"before men knew how to sin.\" The men in Rousseau's \"nascent society\" already had 'bien des querelles et des combats\" [many quarrels and fights]; \"l'amour propre\" was already manifest in them ... and slights or affronts were consequently visited with \"vengeances terribles.\"\nRousseau proposes reorganizing society with a social contract that will \"draw from the very evil from which we suffer the remedy which shall cure it\"; Lovejoy notes that in the \"Discourse on the Origins of Inequality\", Rousseau:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...declares that there is a dual process going on through history; on the one hand, an indefinite progress in all those powers and achievements which express merely the potency of man's intellect; on the other hand, an increasing estrangement of men from one another, an intensification of ill-will and mutual fear, culminating in a monstrous epoch of universal conflict and mutual destruction. And the chief cause of the latter process Rousseau, following Hobbes and [Bernard] Mandeville, found, as we have seen, in that unique passion of the self-conscious animal \u2014 pride, self esteem, \"le besoin de se mettre au dessus des autres\" [the need to put oneself above others]. A large survey of history does not belie these generalizations, and the history of the period since Rousseau wrote lends them a melancholy verisimilitude. Precisely the two processes, which he described have ... been going on upon a scale beyond all precedent: immense progress in man's knowledge and in his powers over nature, and, at the same time, a steady increase of rivalries, distrust, hatred and, at last, \"the most horrible state of war\" ... [Moreover, Rousseau] failed to realize fully how strongly \"amour propre\" tended to assume a collective form ... in pride of race, of nationality, of class.\nCharles Dickens.\nIn 1853, in the weekly magazine \"Household Words\", Charles Dickens published a negative review of the Indian Gallery cultural program, by the portraitist George Catlin, which then was touring England. About Catlin's oil paintings of the North American natives, the poet and critic Charles Baudelaire said that \"He [Catlin] has brought back alive the proud and free characters of these chiefs; both their nobility and manliness.\"\nDespite European idealization of the mythical noble savage as a type of morally superior man, in the essay \u201cThe Noble Savage\u201d (1853), Dickens expressed repugnance for the American Indians and their way of life, because they were dirty and cruel and continually quarrelled among themselves. In the satire of romanticised primitivism Dickens showed that the painter Catlin, the Indian Gallery of portraits and landscapes, and the white people who admire the idealized American Indians or the bushmen of Africa are examples of the term \"noble savage\" used as a means of Othering a person into a racialist stereotype. Dickens begins by dismissing the mythical noble savage as not being a distinct human being:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;To come to the point at once, I beg to say that I have not the least belief in the Noble Savage. I consider him a prodigious nuisance and an enormous superstition. . . .&lt;br&gt;\nI don't care what he calls me. I call him a savage, and I call a savage a something highly desirable to be civilized off the face of the Earth. . . .&lt;br&gt;\nThe noble savage sets a king to reign over him, to whom he submits his life and limbs without a murmur or question, and whose whole life is passed chin deep in a lake of blood; but who, after killing incessantly, is in his turn killed by his relations and friends the moment a grey hair appears on his head. All the noble savage's wars with his fellow-savages (and he takes no pleasure in anything else) are wars of extermination \u2014 which is the best thing I know of him, and the most comfortable to my mind when I look at him. He has no moral feelings of any kind, sort, or description; and his \"mission\" may be summed up as simply diabolical.\nDickens ends his cultural criticism by reiterating his argument against the romanticized \"persona\" of the mythical noble savage:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;To conclude as I began. My position is that if we have anything to learn from the Noble Savage it is what to avoid. His virtues are a fable; his happiness is a delusion; his nobility, nonsense. We have no greater justification for being cruel to the miserable object, than for being cruel to a WILLIAM SHAKESPEARE or an ISAAC NEWTON; but he passes away before an immeasurably better and higher power than ever ran wild in any earthly woods, and the world will be all the better when this place [Earth] knows him no more.\nTheories of racialism.\nIn 1860, the physician John Crawfurd and the anthropologist James Hunt identified the racial stereotype of \"the noble savage\" as an example of scientific racism, yet, as advocates of polygenism \u2014 that each race is a distinct species of Man \u2014 Crawfurd and Hunt dismissed the arguments of their opponents by accusing them of being proponents of \"Rousseau's Noble Savage\". Later in his career, Crawfurd re-introduced the \"noble savage\" term to modern anthropology and deliberately ascribed coinage of the term to Jean-Jacques Rousseau.\nModern perspectives.\nSupporters of primitivism.\nIn \"The Prehistory of Warfare: Misled by Ethnography\" (2006), the researchers Jonathan Haas and Matthew Piscitelli challenged the idea that the human species is innately disposed towards being aggressive or is inclined to engage in violent conflict and propose rather that warfare is an occasional activity by a society and is not an inherent part of human culture. Moreover, the UNESCO's Seville Statement on Violence (1986) specifically rejects claims that the human propensity towards violence has a genetic basis.\nAnarcho-primitivists, such as the philosopher John Zerzan, rely upon a strong ethical dualism between Anarcho-primitivism and civilization; hence, \"life before domestication [and] agriculture was, in fact, largely one of leisure, intimacy with nature, sensual wisdom, sexual equality, and health.\" Zerzan's claims about the moral superiority of primitive societies are based on a certain reading of the works of anthropologists, such as Marshall Sahlins and Richard Borshay Lee, wherein the anthropologic category of \"primitive society\" is restricted to hunter-gatherer societies who have no domesticated animals or agriculture, e.g. the stable social hierarchy of the American Indians of the north-west North America, who live from fishing and foraging, is attributed to having domesticated dogs and the cultivation of tobacco, that animal husbandry and agriculture equal civilization.\nIn anthropology, the argument has been made that key tenets of the myth of the noble savage idea inform cultural investments in places seemingly removed from the Tropics, such as the Mediterranean and specifically Greece, during the debt crisis by European institutions (such as documenta) and by various commentators who found Greece to be a positive inspiration for resistance to austerity policies and the neoliberalism of the EU These commentators' positive embrace of the periphery (their mythical noble savage ideal) is the other side of the mainstream views, also dominant during that period, that stereotyped Greece and the South as lazy and corrupt.\nOpponents of primitivism.\nIn \"War Before Civilization: the Myth of the Peaceful Savage\" (1996), the archaeologist Lawrence H. Keeley said that the \"widespread myth\" that \"civilized humans have fallen from grace from a simple, primeval happiness, a peaceful golden age\" is contradicted and refuted by archeologic evidence that indicates that violence was common practice in early human societies. That the \"noble savage\" paradigm has warped anthropological literature to political ends. Moreover, the anthropologist Roger Sandall likewise accused anthropologists of exalting the mythical noble savage above civilized man, by way of \"designer tribalism\", a form of romanticised primitivism that dehumanises Indigenous peoples into the cultural stereotype of the \"indig\u00e8ne\" peoples who live a primitive way of life demarcated and limited by tradition, which discouraged Indigenous peoples from cultural assimilation into the dominant Western culture.\nIn the 2003 book, \"\" written by Steven LeBlanc, a professor of archaeology at Harvard University who specializes in the American Southwest, LeBlanc further documents the mythical notion of primitive non-violence against foreign tribal peoples, internal strife and internecine violence, as well as violence against animals and wildlife. In many of these instances the homicide rate even rose to substantially higher levels than any seen in modernity on a proportionate basis.\nSee also.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\nInformational notes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39566", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=39566", "title": "English Renaissance theatre", "text": "Theatre of England between 1558 and 1642\nThe English Renaissance theatre or Elizabethan theatre was the theatre of England from 1558 to 1642. Its most prominent playwrights were William Shakespeare, Christopher Marlowe and Ben Jonson.\nBackground.\nThe term \"English Renaissance theatre\" encompasses the period between 1562\u2014following a performance of \"Gorboduc\", the first English play using blank verse, at the Inner Temple during the Christmas season of 1561\u2014and the ban on theatrical plays enacted by the English Parliament in 1642.\nIn a strict sense \"Elizabethan\" only refers to the period of Queen Elizabeth's reign (1558\u20131603). \"English Renaissance theatre\" may be said to encompass \"Elizabethan theatre\" from 1562 to 1603, \"Jacobean theatre\" from 1603 to 1625, and \"Caroline theatre\" from 1625 to 1642.\nAlong with the economics of the profession, the character of the drama changed towards the end of the period. Under Elizabeth, the drama was a unified expression as far as social class was concerned: the Court watched the same plays the commoners saw in the public playhouses. With the development of the private theatres, drama became more oriented towards the tastes and values of an upper-class audience. By the later part of the reign of Charles I, few new plays were being written for the public theatres, which sustained themselves on the accumulated works of the previous decades.\nSites of dramatic performance.\nGrammar schools.\nThe English grammar schools, like those on the continent, placed special emphasis on the \"trivium\": grammar, logic, and rhetoric. Though rhetorical instruction was intended as preparation for careers in civil service such as law, the rhetorical canons of memory (\"memoria\") and delivery (\"pronuntiatio\"), gesture and voice, as well as exercises from the \"progymnasmata\", such as the \"prosopopoeia\", taught theatrical skills. Students would typically analyse Latin and Greek texts, write their own compositions, memorise them, and then perform them in front of their instructor and their peers. Records show that in addition to this weekly performance, students would perform plays on holidays, and in both Latin and English.\nChoir schools.\nChoir schools connected with the Elizabethan court included St. George\u2019s Chapel, the Chapel Royal, and St. Paul\u2019s. These schools performed plays and other court entertainments for the Queen. Between the 1560s and 1570s these schools had begun to perform for general audiences as well. Playing companies of boy actors were derived from choir schools. John Lyly is an earlier example of a playwright contracted to write for the children's companies; Lyly wrote \"Gallathea\", \"Endymion\", and \"Midas\" for Paul\u2019s Boys. Another example is Ben Jonson, who wrote \"Cynthia\u2019s Revels\".\nUniversities.\nAcademic drama stems from late medieval and early modern practices of miracles and morality plays as well as the Feast of Fools and the election of a Lord of Misrule. The Feast of Fools includes mummer plays. The universities, particularly Oxford and Cambridge, were attended by students studying for bachelor's degrees and master's degrees, followed by doctorates in Law, Medicine, and Theology. In the 1400s, dramas were often restricted to mummer plays with someone who read out all the parts in Latin. With the rediscovery and redistribution of classical materials during the English Renaissance, Latin and Greek plays began to be restaged. These plays were often accompanied by feasts. Queen Elizabeth I viewed dramas during her visits to Oxford and Cambridge. A well-known play cycle which was written and performed in the universities was the \"Parnassus Plays\".\nInns of Court.\nUpon graduation, many university students, especially those going into law, would reside and participate in the Inns of Court. The Inns of Court were communities of working lawyers and university alumni. Notable literary figures and playwrights who resided in the Inns of Court include John Donne, Francis Beaumont, John Marston, Thomas Lodge, Thomas Campion, Abraham Fraunce, Sir Philip Sidney, Sir Thomas More, Sir Francis Bacon, and George Gascoigne. Like the university, the Inns of Court elected their own Lord of Misrule. Other activities included participation in moot court, disputation, and masques. Plays written and performed in the Inns of Court include \"Gorboduc\", \"Gismund of Salerne\", and \"The Misfortunes of Arthur\". An example of a famous masque put on by the Inns was James Shirley's \"The Triumph of Peace\". Shakespeare's \"The Comedy of Errors\" and \"Twelfth Night\" were also performed here, although written for commercial theater.\nEstablishment of playhouses.\nThe first permanent English theatre, the Red Lion, opened in 1567 but it was a short-lived failure. The first successful theatres, such as The Theatre, opened in 1576.\nThe establishment of large and profitable public theatres was an essential enabling factor in the success of English Renaissance drama. Once they were in operation, drama could become a fixed and permanent, rather than transitory, phenomenon. Their construction was prompted when the Mayor and Corporation of London first banned plays in 1572 as a measure against the plague, and then formally expelled all players from the city in 1575. This prompted the construction of permanent playhouses outside the jurisdiction of London, in the liberties of Halliwell/Holywell in Shoreditch and later the Clink, and at Newington Butts near the established entertainment district of St. George's Fields in rural Surrey. The Theatre was constructed in Shoreditch in 1576 by James Burbage with his brother-in-law John Brayne (the owner of the unsuccessful Red Lion playhouse of 1567) and the Newington Butts playhouse was set up, probably by Jerome Savage, some time between 1575 and 1577. The Theatre was rapidly followed by the nearby Curtain Theatre (1577), the Rose (1587), the Swan (1595), the Globe (1599), the Fortune (1600), and the Red Bull (1604). \nElsewhere, the Vagabonds Act 1572 left itinerant actors liable to prosecution as vagrants and caused them to seek wealthy sponsors who could provide a permanent play house for them. At the same time, members of the aristocracy found it useful to have a playing company available to entertain royal or noble guests and thus advance their social status.\nPlayhouse architecture.\nArchaeological excavations on the foundations of the Rose and the Globe in the late 20th century showed that all the London theatres had individual differences, but their common function necessitated a similar general plan. The public theatres were three stories high and built around an open space at the centre. Usually polygonal in plan to give an overall rounded effect, although the Red Bull and the first Fortune were square. The three levels of inward-facing galleries overlooked the open centre, into which jutted the stage: essentially a platform surrounded on three sides by the audience. The rear side was restricted for the entrances and exits of the actors and seating for the musicians. The upper level behind the stage could be used as a balcony, as in \"Romeo and Juliet\" and \"Antony and Cleopatra\", or as a position from which an actor could harangue a crowd, as in \"Julius Caesar\". The pit was the place where the poorest audience members could view the show. Around the 1600s a new area was introduced into the theaters, a 'gullet'. A gullet was an invisible corridor that the actors used to go to the wings of the stage where people usually changed clothes quickly.\nThe playhouses were generally built with timber and plaster. Individual theatre descriptions give additional information about their construction, such as flint stones being used to build the Swan. Theatres were also constructed to be able to hold a large number of people.\nA different model was developed with the Blackfriars Theatre, which came into regular use on a long-term basis in 1599. The Blackfriars was small in comparison to the earlier theatres and roofed rather than open to the sky. It resembled a modern theatre in ways that its predecessors did not. Other small enclosed theatres followed, notably the Whitefriars (1608) and the Cockpit (1617). With the building of the Salisbury Court Theatre in 1629 near the site of the defunct Whitefriars, the London audience had six theatres to choose from: three surviving large open-air public theatres\u2014the Globe, the Fortune, and the Red Bull\u2014and three smaller enclosed private theatres: the Blackfriars, the Cockpit, and the Salisbury Court. Audiences of the 1630s benefited from a half-century of vigorous dramaturgical development; the plays of Marlowe and Shakespeare and their contemporaries were still being performed on a regular basis, mostly at the public theatres, while the newest works of the newest playwrights were abundant as well, mainly at the private theatres.\nAudiences.\nAround 1580, when both the Theater and the Curtain were full on summer days, the total theater capacity of London was about 5000 spectators. With the building of new theater facilities and the formation of new companies, London's total theater capacity exceeded 10,000 after 1610.\nTicket prices in general varied during this time period. The cost of admission was based on where in the theatre a person wished to be situated, or based on what a person could afford. If people wanted a better view of the stage or to be more separate from the crowd, they would pay more for their entrance. Due to inflation that occurred during this time period, admission increased in some theaters from a penny to a sixpence or even higher.\nCommercial theaters were largely located just outside the boundaries of the City of London, since City authorities tended to be wary of the adult playing companies, but plays were performed by touring companies all over England. English companies even toured and performed English plays abroad, especially in Germany and in Denmark.\nUpper class spectators would pay for seats in the galleries, using cushions for comfort. In the Globe Theatre, nobles could sit directly by the side on the stage.\nPerformances.\nThe acting companies functioned on a repertory system: unlike modern productions that can run for months or years on end, the troupes of this era rarely acted the same play two days in a row. Thomas Middleton's \"A Game at Chess\" ran for nine straight performances in August 1624 before it was closed by the authorities; but this was due to the political content of the play and was a unique, unprecedented and unrepeatable phenomenon. The 1592 season of Lord Strange's Men at the Rose Theatre was far more representative: between 19 February and 23 June the company played six days a week, minus Good Friday and two other days. They performed 23 different plays, some only once, and their most popular play of the season, \"The First Part of Hieronimo\", based on Kyd's \"The Spanish Tragedy\", 15 times. They never played the same play two days in a row, and rarely the same play twice in a week. The workload on the actors, especially the leading performers like Richard Burbage or Edward Alleyn, must have been tremendous.\nOne distinctive feature of the companies was that only men or boys performed. Female parts were played by adolescent boy players in women's costume. Some companies were composed entirely of boy players. Performances in the public theatres (like the Globe) took place in the afternoon with no artificial lighting, but when, in the course of a play, the light began to fade, candles were lit. In the enclosed private theatres (like the Blackfriars) artificial lighting was used throughout. Plays contained little to no scenery as the scenery was described by the actors or indicated by costume through the course of the play.\nIn the Elizabethan era, research has been conclusive about how many actors and troupes there were in the 16th century, but little research delves into the roles of the actors on the English renaissance stage. The first point is that during the Elizabethan era, women were not allowed to act on stage. The actors were all male; in fact, most were boys. For plays written that had male and female parts, the female parts were played by the youngest boy players. Stronger female roles in tragedies were acted by older boy players because they had more experience. As a boy player, many skills had to be implemented such as voice and athleticism (fencing was one).\nIn Elizabethan entertainment, troupes were created and they were considered the actor companies. They travelled around England as drama was the most entertaining art at the time.\nElizabethan actors never played the same show on successive days and added a new play to their repertoire every other week. These actors were getting paid within these troupes so for their job, they would constantly learn new plays as they toured different cities in England. In these plays, there were bookkeepers that acted as the narrators of these plays and they would introduce the actors and the different roles they played. At some points, the bookkeeper would not state the narrative of the scene, so the audience could find out for themselves. In Elizabethan and Jacobean plays, the plays often exceeded the number of characters/roles and did not have enough actors to fulfil them, thus the idea of doubling roles came to be. Doubling roles is used to reinforce a plays theme by having the actor act out the different roles simultaneously. The reason for this was for the acting companies to control salary costs, or to be able to perform under conditions where resources such as other actor companies lending actors were not present.\nThere were two acting styles implemented: formal and natural. Formal acting is objective and traditional, while natural acting attempts to create an illusion for the audience by remaining in character and imitating the fictional circumstances. The formal actor symbolises while the natural actor interprets. The natural actor impersonates while the formal actor represents the role. Natural and formal are opposites of each other, where natural acting is subjective. Overall, the use of these acting styles and the doubled roles dramatic device made Elizabethan plays very popular.\nCostumes.\nOne of the main uses of costume during the Elizabethan era was to make up for the lack of scenery, set, and props on stage. It created a visual effect for the audience, and it was an integral part of the overall performance. Since the main visual appeal on stage were the costumes, they were often bright in colour and visually entrancing. Colours symbolized social hierarchy, and costumes were made to reflect that. For example, if a character was royalty, their costume would include purple. The colours, as well as the different fabrics of the costumes, allowed the audience to know the status of each character when they first appeared on stage.\nCostumes were collected in inventory. More often than not, costumes wouldn't be made individually to fit the actor. Instead, they would be selected out of the stock that theatre companies would keep. A theatre company reused costumes when possible and would rarely get new costumes made. Costumes themselves were expensive, so usually players wore contemporary clothing regardless of the time period of the play. The most expensive pieces were given to higher class characters because costuming was used to identify social status on stage. The fabrics within a playhouse would indicate the wealth of the company itself. The fabrics used the most were: velvet, satin, silk, cloth-of-gold, lace and ermine. For less significant characters, actors would use their own clothes.\nActors also left clothes in their will for following actors to use. Masters would also leave clothes for servants in their will, but servants weren't allowed to wear fancy clothing, instead, they sold the clothes back to theatre companies. \nIn the Tudor and Elizabethan eras, there were laws stating that certain classes could only wear clothing fitting of their status in society. \nThere was a discrimination of status within the classes. Higher classes flaunted their wealth and power through the appearance of clothing, however, courtesans and actors were the only exceptions \u2013 as clothing represented their 'working capital', as it were, but they were only permitted to dress so while working. If actors belonged to a licensed acting company, they were allowed to dress above their standing in society for specific roles in a production.\nPlaywrights.\nThe growing population of London, the growing wealth of its people, and their fondness for spectacle produced a dramatic literature of remarkable variety, quality and extent. About 3,000 plays were written for the Elizabethan stage, and although most of them have been lost, at least 543 remain.\nThe people who wrote these plays were primarily self-made men from modest backgrounds. Some of them were educated at either Oxford or Cambridge, but many were not. Although William Shakespeare and Ben Jonson were actors, the majority do not seem to have been performers, and no major author who came on to the scene after 1600 is known to have supplemented his income by acting. Their lives were subject to the same levels of danger and earlier mortality as all who lived during the early modern period: Christopher Marlowe was killed in an apparent tavern brawl, while Ben Jonson killed an actor in a duel. Several were probably soldiers.\nPlaywrights were normally paid in increments during the writing process, and if their play was accepted, they would also receive the proceeds from one day's performance. However, they had no ownership of the plays they wrote. Once a play was sold to a company, the company owned it, and the playwright had no control over casting, performance, revision or publication.\nThe profession of dramatist was challenging and far from lucrative. Entries in Philip Henslowe's Diary show that in the years around 1600 Henslowe paid as little as \u00a36 or \u00a37 per play. This was probably at the low end of the range, though even the best writers could not demand too much more. A playwright, working alone, could generally produce two plays a year at most. In the 1630s Richard Brome signed a contract with the Salisbury Court Theatre to supply three plays a year, but found himself unable to meet the workload. Shakespeare produced fewer than 40 solo plays in a career that spanned more than two decades: he was financially successful because he was an actor and, most importantly, a shareholder in the company for which he acted and in the theatres they used. Ben Jonson achieved success as a purveyor of Court masques, and was talented at playing the patronage game that was an important part of the social and economic life of the era. Those who were purely playwrights fared far less well: the biographies of early figures like George Peele and Robert Greene, and later ones like Brome and Philip Massinger, are marked by financial uncertainty, struggle and poverty.\nPlaywrights dealt with the natural limitation on their productivity by combining into teams of two, three, four, and even five to generate play texts. The majority of plays written in this era were collaborations, and the solo artists who generally eschewed collaborative efforts, like Jonson and Shakespeare, were the exceptions to the rule. Dividing the work, of course, meant dividing the income; but the arrangement seems to have functioned well enough to have made it worthwhile. Of the 70-plus known works in the canon of Thomas Dekker, roughly 50 are collaborations. In a single year (1598) Dekker worked on 16 collaborations for impresario Philip Henslowe, and earned \u00a330, or a little under 12 shillings per week\u2014roughly twice as much as the average artisan's income of 1\"s\". per day. At the end of his career, Thomas Heywood would famously claim to have had \"an entire hand, or at least a main finger\" in the authorship of some 220 plays. A solo artist usually needed months to write a play (though Jonson is said to have done \"Volpone\" in five weeks); Henslowe's Diary indicates that a team of four or five writers could produce a play in as little as two weeks. Admittedly, though, the Diary also shows that teams of Henslowe's house dramatists\u2014Anthony Munday, Robert Wilson, Richard Hathwaye, Henry Chettle, and the others, even including a young John Webster\u2014could start a project, and accept advances on it, yet fail to produce anything stageworthy.\nGenres.\nGenres of the period included the history play, which depicted English or European history. Shakespeare's plays about the lives of kings, such as \"Richard III\" and \"Henry V\", belong to this category, as do Christopher Marlowe's \"Edward II\" and George Peele's \"Famous Chronicle of King Edward the First\". History plays also dealt with more recent events, like \"A Larum for London\" which dramatizes the sack of Antwerp in 1576. A better known play, Peele's \"The Battle of Alcazar\" (c. 1591), depicts the battle of Alc\u00e1cer Quibir in 1578.\nTragedy was a very popular genre. Marlowe's tragedies were exceptionally successful, such as \"Dr. Faustus\" and \"The Jew of Malta\". The audiences particularly liked revenge dramas, such as Thomas Kyd's \"The Spanish Tragedy\". The four tragedies considered to be Shakespeare's greatest (\"Hamlet\", \"Othello\", \"King Lear\" and \"Macbeth\") were composed during this period.\nComedies were common, too. A subgenre developed in this period was the city comedy, which deals satirically with life in London after the fashion of Roman New Comedy. Examples are Thomas Dekker's \"The Shoemaker's Holiday\" and Thomas Middleton's \"A Chaste Maid in Cheapside\".\nThough marginalised, the older genres like pastoral (\"The Faithful Shepherdess\", 1608), and even the morality play (\"Four Plays in One\", ca. 1608\u201313) could exert influences. After about 1610, the new hybrid subgenre of the tragicomedy enjoyed an efflorescence, as did the masque throughout the reigns of the first two Stuart kings, James I and Charles I.\nPlays on biblical themes were common, Peele's \"David and Bethsabe\" being one of the few surviving examples.\nPrinted texts.\nOnly a minority of the plays of English Renaissance theatre were ever printed. Of Heywood's 220 plays, only about 20 were published in book form. A little over 600 plays were published in the period as a whole, most commonly in individual quarto editions. (Larger collected editions, like those of Shakespeare's, Ben Jonson's, and Beaumont and Fletcher's plays, were a late and limited development.) Through much of the modern era, it was thought that play texts were popular items among Renaissance readers that provided healthy profits for the stationers who printed and sold them. By the turn of the 21st century, the climate of scholarly opinion shifted somewhat on this belief: some contemporary researchers argue that publishing plays was a risky and marginal business\u2014though this conclusion has been disputed by others. Some of the most successful publishers of the English Renaissance, like William Ponsonby or Edward Blount, rarely published plays.\nA small number of plays from the era survived not in printed texts but in manuscript form.\nThe end of English Renaissance theatre.\nThe rising Puritan movement was hostile toward theatre, as they felt that \"entertainment\" was sinful. Politically, playwrights and actors were clients of the monarchy and aristocracy, and most supported the Royalist cause. The Puritan faction, long powerful in London, gained control of the city early in the First English Civil War, and on 2 September 1642, the Long Parliament, pushed by the Parliamentarian party, under Puritan influence, banned the staging of plays in the London theatres though it did not, contrary to what is commonly stated, order the closure, let alone the destruction, of the theatres themselves:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Whereas the distressed Estate of Ireland, steeped in her own Blood, and the distracted Estate of England, threatened with a Cloud of Blood by a Civil War, call for all possible Means to appease and avert the Wrath of God, appearing in these Judgements; among which, Fasting and Prayer, having been often tried to be very effectual, having been lately and are still enjoined; and whereas Public Sports do not well agree with Public Calamities, nor Public Stage-plays with the Seasons of Humiliation, this being an Exercise of sad and pious Solemnity, and the other being Spectacles of Pleasure, too commonly expressing lascivious Mirth and Levity: It is therefore thought fit, and Ordained, by the Lords and Commons in this Parliament assembled, That, while these sad causes and set Times of Humiliation do continue, Public Stage Plays shall cease, and be forborn, instead of which are recommended to the People of this Land the profitable and seasonable considerations of Repentance, Reconciliation, and Peace with God, which probably may produce outward Peace and Prosperity, and bring again Times of Joy and Gladness to these Nations.\u2014\u200a\nThe Act purports the ban to be temporary (\"... while these sad causes and set Times of Humiliation do continue, Public Stage Plays shall cease and be forborn\") but does not assign a time limit to it.\nEven after 1642, during the English Civil War and the ensuing Interregnum (English Commonwealth), some English Renaissance theatre continued. For example, short comical plays called drolls were allowed by the authorities, while full-length plays were banned. The theatre buildings were not closed but rather were used for purposes other than staging plays.\nThe performance of plays remained banned for most of the next eighteen years, becoming allowed again after the Restoration of the monarchy in 1660. The theatres began performing many of the plays of the previous era, though often in adapted forms. New genres of Restoration comedy and spectacle soon evolved, giving English theatre of the later seventeenth century its distinctive character.\nList of playwrights.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nActors.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPlayhouses.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPlaying companies.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nTimeline of English Renaissance playing companies.\nEnglish Renaissance playing company timeline\nNotes and references.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "39567", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=39567", "title": "History of Angola", "text": "Angola was first settled by San hunter-gatherer societies before the northern domains came under the rule of Bantu states such as Kongo and Ndongo. In the 15th century, Portuguese colonists began trading, and a settlement was established at Luanda during the 16th century. Portugal annexed territories in the region which were ruled as a colony from 1655, and Angola was incorporated as an overseas province of Portugal in 1951. After the Angolan War of Independence, which ended in 1974 with an army mutiny and leftist coup in Lisbon, Angola achieved independence in 1975 through the Alvor Agreement. After independence, Angola entered a long period of civil war that lasted until 2002.\nPrehistory.\nThe area of present-day Angola was inhabited during the Paleolithic and Neolithic eras, as attested by remains found in Luanda, Congo, and the Namibe desert. At the beginning of recorded history other cultures and people also arrived.\nThe first ones to settle were the San people. This changed at the beginning of the sixth century AD, when the Bantu, already in possession of metal-working technology, ceramics and agriculture began migrating from the north. When they reached what is now Angola they encountered the San and other groups. The establishment of the Bantu took many centuries and gave rise to a variety of groupings that took on different ethnic characteristics.\nKingdom of Kongo.\nThe first large political entity in the area, known to history as the Kingdom of Kongo, appeared in the thirteenth century and stretched from Gabon in the north to the river Kwanza in the south, and from the Atlantic in the west to the river Cuango in the east.\nThe wealth of the Kongo came mainly from agriculture. Power was in the hands of the Mani, aristocrats who occupied key positions in the kingdom and who answered only to the all-powerful King of the Kongo. Mbanza was the name given to a territorial unit administered and ruled by a Mani; Mbanza Congo, the capital, had a population of over fifty thousand in the sixteenth century.\nThe Kingdom of Kongo was divided into six provinces and included some dependent kingdoms, such as Ndongo to the south. Trade was the main activity, based on highly productive agriculture and increasing exploitation of mineral wealth. In 1482, Portuguese caravels commanded by Diogo C\u00e3o arrived in the Congo and he explored the extreme north-western coast of what today is Angola in 1484. Other expeditions followed, and close relations were soon established between the two states. The Portuguese brought firearms and many other technological advances, as well as a new religion (Christianity); in return, the King of the Congo offered plenty of slaves, ivory, and minerals.\nThe King of the Kongo soon converted to Christianity and adopted a similar political structure to the Europeans. He became a well-known figure in Europe, to the point of receiving missives from the Pope.\nKingdom of Ndongo.\nTo the south of the Kingdom of the Kongo, around the river Kwanza, there were various important states. The most important of these was the Kingdom of Ndongo or Dongo, ruled by the \"ngolas\" (chief or king). At the time of the arrival of the Portuguese, Ngola Kiluange was in power. By maintaining a policy of alliances with neighbouring states, he managed to hold out against the foreigners for several decades but in the late 1620s was eventually beheaded in Luanda. Years later, the Ndongo rose to prominence again when Jinga Mbandi (Queen Jinga) took power in 1631. A wily politician, she kept the Portuguese in check with carefully prepared agreements. After undertaking various journeys she succeeded in 1635 in forming a grand coalition with the states of Matamba and Ndongo, Kongo, Kassanje, Dembos and Kissamas. At the head of this formidable alliance, she forced the Portuguese to retreat.\nMeanwhile, Portugal had lost its King and the Spanish took control of the Portuguese monarchy. By this time, focus on Portugal's overseas territories had taken secondary importance. The Dutch took advantage of this situation and occupied Luanda in 1641. Jinga entered into an alliance with the Dutch, thereby strengthening her coalition and confining the Portuguese to Massangano, which they fortified strongly, sallying forth on occasion to capture slaves in a later war. Slaves from Angola were essential to the development of the Portuguese colony of Brazil, but the traffic had been interrupted by these events.\nPortugal having regained its independence, a large force from Brazil under the command of Salvador Correia de S\u00e1 retook Luanda in 1648, leading to the return of the Portuguese in large numbers. Jinga's coalition then fell apart; the absence of their Dutch allies with their firearms and the strong position of Correia de S\u00e1 delivered a deadly blow to the morale of the native forces. After Jinga died in 1663, the King of Congo committed all his forces in an attempt to capture the island of Luanda, occupied by Correia de S\u00e1, but they were defeated and lost their independence. The Kingdom of Ndongo submitted to the Portuguese Crown in 1671.\nPortuguese Angola.\nThe Portuguese colony of Angola was founded in 1575 with the arrival of Paulo Dias de Novais with a hundred Portuguese families and 400 soldiers. Its center at Luanda was granted the status of city in 1605.\nTrade was mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the ports of Luanda and Benguela. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong Brazilian influence was also exercised by the Jesuits in religion and education. War gradually gave way to the philosophy of trade. Slave-trading routes and the conquests that made them possible were the driving force for activities between the different areas; independent states slave markets were now focused on the demands of American slavery. In the high plains (the Planalto), the most important states were those of Bi\u00e9 and Bailundo, the latter being noted for its production of foodstuffs and rubber. The interior remained largely free of Portuguese control as late as the 19th century.\nThe slave trade was not abolished until 1836, and in 1844 Angola's ports were opened to foreign shipping with the Portuguese unable to enforce the laws, especially dependent on English naval security. This facilitated the continuation of slave smuggling to the United States and Brazil. By 1850, Luanda was one of the largest Portuguese cities in the Portuguese Empire outside Mainland Portugal exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products \u2013 almost all the produce of a continued forced labour system.\nThe Berlin Conference compelled Portugal to move towards the immediate occupation of all the territories it laid claim to but had been unable to effectively conquer. The territory of Cabinda (province), to the north of the river Zaire, was also ceded to Portugal on the legal basis of the Treaty of Simulambuko Protectorate, concluded between the Portuguese Crown and the princes of Cabinda in 1885. In the 19th century they slowly and hesitantly began to establish themselves in the interior. Angola as a Portuguese colony encompassing the present territory was not established before the end of the 19th century, and \"effective occupation\", as required by the Berlin Conference (1884) was achieved only by the 1920s.\nColonial economic strategy was based on agriculture and the export of raw materials. Trade in rubber and ivory, together with the taxes imposed on the population of the Empire (including the mainland), brought vast income to Lisbon.\nPortuguese policy in Angola was modified by certain reforms introduced at the beginning of the twentieth century. The fall of the Portuguese monarchy and a favourable international climate led to reforms in administration, agriculture, and education. In 1951, with the advent of the New State regime (\"Estado Novo\") extended to the colony, Angola became a province of Portugal (Ultramarine Province), called the \"Prov\u00edncia Ultramarina de Angola\" (Overseas Province of Angola).\nHowever, Portuguese rule remained characterized by deep-seated racism, mass forced labour, and an almost complete failure to modernize the country. By 1960, after 400 years of colonial rule, there was not a single university in the entire territory. To counter this lack of education facilities, overtly political organizations first appeared in the 1950s, and began to make organized demands for human and civil rights, initiating diplomatic campaigns throughout the world in their fight for independence. The Portuguese regime, meanwhile, refused to accede to the nationalists' demands for independence, thereby provoking the armed conflict that started in 1961 when guerrillas attacked colonial assets in cross-border operations in northeastern Angola. The war came to be known as the Colonial War.\nIn this struggle, the principal protagonists were the MPLA (Popular Movement for the Liberation of Angola), founded in 1956, the FNLA (National Front for the Liberation of Angola), which appeared in 1961, and UNITA (National Union for the Total Independence of Angola), founded in 1966. After many years of conflict, the nation gained its independence on 11 November 1975, after the 1974 coup d'\u00e9tat in Lisbon, Portugal. Portugal's new leaders began a process of democratic change at home and acceptance of the independence of its former colonies.\nIndependence and civil war.\nA 1974 coup d'\u00e9tat in Portugal established a military government led by President Ant\u00f3nio de Sp\u00ednola. The Sp\u00ednola government agreed to give all of Portugal's colonies independence, and handed power in Angola over to a coalition of the three largest nationalist movements, the MPLA, UNITA, and the FNLA, through the Alvor Agreement. The coalition quickly broke down, however, and the country descended into civil war. The MPLA gained control of the capital Luanda and much of the rest of the country. With the support of the United States, Za\u00efre and South Africa intervened militarily in favour of the FNLA and UNITA with the intention of taking Luanda before the declaration of independence. In response, Cuba intervened in favor of the MPLA. In the meantime the South Africans and UNITA had come as close as 200\u00a0km to the south of the capital, the FNLA and Zairian forces as far as Kifangondo, 30\u00a0km to the east.\nWith Cuban support, the MPLA held Luanda and declared independence as the Angolan People's Republic on 11 November 1975, the day the Portuguese left the country. Agostinho Neto became the first president. FNLA and UNITA proclaimed their own short-lived republics (the Angolan Democratic Republic and the Angolan Social Democratic Republic) on 24 November 1975, for the zones they controlled with Holden Roberto and Jonas Savimbi as co-presidents of the Angolan People's Democratic Republic in Huambo. This joint FNLA-UNITA government was dissolved on 11 February 1976 after a MPLA offensive. By the end of January 1976 the MPLA army (FAPLA) and the Cubans had all but crushed FNLA, Zairians and UNITA, and the South African forces withdrew.\nOn 27 May 1977, a coup attempt, including some former members of the MPLA government such as Nito Alves, led to retaliation by the government and Cuban forces, resulting in the execution of thousands, if not tens of thousands. Alves was tortured and killed. The movement is known as Fraccionismo.\nThe proxy war continued. The MPLA government, recognized internationally (although not by the United States), requested that Cuban forces remain in the country. Led by Jonas Savimbi, UNITA received clandestine support from the U.S. and other nations and took up military resistance in the southeast of the country while the MPLA government was supported by the USSR and Eastern Bloc countries. South Africa continued to pursue South-West Africa People's Organisation (SWAPO) forces in Southern Angola, soon established bases and increased support of UNITA, which gained control of more and more territory. In an effort to deliver a final blow to UNITA and to drive South Africa out of the country, in 1987 the People's Armed Forces for the Liberation of Angola (FAPLA), with Soviet support, launched a campaign fraught with failures and defeats. Again, the Cubans intervened, stopping UNITA and South African advances, leading to the Battle of Cuito Cuanavale from 13 January to 23 March, the largest battle in African history since World War II.\nThe MPLA and the U.S. had been in negotiations for a peaceful solution since June 1987. The U.S. agreed to include Cuba in direct talks. Cuba joined the negotiations 28 January 1988; South Africa joined 9 March. Angola, Cuba and South Africa signed the Tripartite Accord on 22 December 1988, in which the withdrawal of Cuban troops from Angola was linked to the retreat of South African soldiers from Angola and Namibia.\nThe Bicesse Accord in 1991 spelled out an electoral process for a democratic Angola under the supervision of the United Nations. MPLA won the first round with 49% of the votes, against 40% for UNITA. UNITA leader Jonas Savimbi rejected the results and returned to war. From 30 October to 1 November 1992 the Halloween Massacre occurred in which thousands of UNITA and FNLA supporters in Luanda were killed by MPLA troops. Estimates for the death toll nationwide reach 25,000 to 30,000. A second peace accord, the Lusaka Protocol, was brokered in Lusaka, Zambia and signed on 20 November 1994.\nThe peace accord between the government and UNITA provided for the integration of former UNITA insurgents into the government and armed forces. However, in 1995, localized fighting resumed. A national unity government was installed in April 1997, but serious fighting resumed in late 1998 when Savimbi renewed the war for a second time, claiming that the MPLA was not fulfilling its obligations. The UN Security Council voted on 28 August 1997, to impose sanctions on UNITA. The Angolan military launched a massive offensive in 1999 that destroyed UNITA's conventional capacity and recaptured all major cities previously held by Savimbi's forces. Savimbi then declared that UNITA would return to guerrilla tactics, and much of the country remained in turmoil.\nThe extended civil war rendered hundreds of thousands of people homeless. Up to 1 million lives may have been lost in fighting over the past quarter century. It only ended when Savimbi was killed in 2002.\nPost-independence.\nA Russian freighter delivered 500 tons of Ukrainian 7.62\u00a0mm ammunition to Simportex, a division of the Angolan government, with the help of a shipping agent in London on 21 September 2000. The ship's captain declared his cargo \"fragile\" to minimize inspection. The next day, the MPLA began attacking UNITA, winning victories in several battles from 22 to 25 September. The government gained control over military bases and diamond mines in Lunda Norte and Lunda Sul, hurting Savimbi's ability to pay his troops.\nAngola agreed to trade oil to Slovakia in return for arms, buying six Sukhoi Su-17 attack aircraft on 3 April 2000. The Spanish government in the Canary Islands prevented a Ukrainian freighter from delivering 636 tons of military equipment to Angola on 24 February 2001. The captain of the ship had inaccurately reported his cargo, falsely claiming the ship carried automobile parts. The Angolan government admitted Simportex had purchased arms from Rosvooruzhenie, the Russian state-owned arms company, and acknowledged the captain might have violated Spanish law by misreporting his cargo, a common practice in arms smuggling to Angola.\nGovernment troops captured and destroyed UNITA's Epongoloko base in Benguela province and Mufumbo base in Cuanza Sul in October 2001. The Slovak government sold fighter jets to the Angolan government in 2001 in violation of the European Union Code of Conduct on Arms Exports.\nGovernment troops killed Savimbi on 22 February 2002, in Moxico province. UNITA Vice President Ant\u00f3nio Dembo took over, but died from diabetes twelve days later on 3 March, and Secretary-General Paulo Lukamba Gato became UNITA's leader. After Savimbi's death, the government came to a crossroads over how to proceed. After initially indicating the counter-insurgency might continue, the government announced it would halt all military operations on 13 March. Military commanders for UNITA and the MPLA met in Cassamba and agreed to a cease-fire. However, Carlos Morgado, UNITA's spokesman in Portugal, said that the UNITA's Portugal wing had been under the impression General Kamorteiro, the UNITA general who agreed to the ceasefire, had been captured more than a week earlier. Morgado did say that he had not heard from Angola since Savimbi's death. The military commanders signed a Memorandum of Understanding as an addendum to the Lusaka Protocol in Luena on 4 April, Dos Santos and Lukamba Gato observing.\nThe United Nations Security Council passed Resolution 1404 on 18 April, extending the monitoring mechanism of sanctions by six months. Resolutions 1412 and 1432, passed on 17 May and 15 August respectively, suspended the UN travel ban on UNITA officials for 90 days each, finally abolishing the ban through Resolution 1439 on 18 October. UNAVEM III, extended an additional two months by Resolution 1439, ended on 19 December.\nIn August 2002, UNITA declared itself a political party and officially demobilised its armed forces. That same month, the United Nations Security Council replaced the United Nations Office in Angola with the United Nations Mission in Angola, a larger, non-military, political presence.\nThe civil war produced four million internally displaced persons (IDPs), one third of Angola's population. The government spent $187 million settling IDPs between 4 April 2002 and 2004, after which the World Bank gave $33 million to continue the settling process. Militant forces laid approximately 15 million landmines by 2002. The HALO Trust charity began demining in 1994, destroying 30,000 by July 2007. There are 1,100 Angolans and seven foreign workers who are working for HALO Trust in Angola, with operations expected to finish sometime between 2011 and 2014.\nHuman Rights Watch estimates UNITA and the government employed more than 86,000 and 3,000 child soldiers respectively, some forcibly impressed, during the war. Human rights analysts found 5,000 to 8,000 underage girls married to UNITA militants. Some girls were ordered to go and forage for food to provide for the troops. If the girls did not bring back enough food as judged by their commander, then the girls would not eat. After victories, UNITA commanders would be rewarded with women who were often then sexually abused. The government and UN agencies identified 190 child soldiers in the Angolan army and relocated seventy of them by November 2002, but the government continued to knowingly employ other underage soldiers.\nFernando Vendrell produced and Z\u00e9z\u00e9 Gamboa directed \"The Hero\", a film about the life of average Angolans after the civil war, in 2004. The film follows the lives of three individuals; Vit\u00f3rio, a war veteran crippled by a landmine who returns to Luanda, Manu, a young boy searching for his soldier father, and Joana, a teacher who mentors the boy and begins a love affair with Vit\u00f3rio. \"The Hero\" won the 2005 Sundance World Dramatic Cinema Jury Grand Prize. A joint Angolan, Portuguese, and French production, Gamboa filmed \"The Hero\" entirely in Angola.\nJos\u00e9 Eduardo dos Santos stepped down as President of Angola after 38 years in 2017, being peacefully succeeded by Jo\u00e3o Louren\u00e7o, Santos' chosen successor. However, President Jo\u00e3o Louren\u00e7o started a campaign against corruption of the dos Santos era. In November 2017, Isabel dos Santos, the billionaire daughter of former President Jos\u00e9 Eduardo dos Santos, was fired from her position as head of the country's state oil company Sonangol. In August 2020, Jos\u00e9 Filomeno dos Santos, son of Angola's former president, was sentenced for five years in jail for fraud and corruption.\nIn July 2022, ex-president Jos\u00e9 Eduardo dos Santos died in Spain.\nIn August 2022, the ruling party, MPLA, won another outright majority and President Joao Lourenco won a second five-year term in the election. However, the election was the tightest in Angola's history.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nAttribution:"}
{"id": "39568", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=39568", "title": "History of Anguilla", "text": "The history of Anguilla runs from the beginning of human habitation, probably via settlement from South America, through its colonisation by the English in the early modern period, to the present day. Following a series of rebellions and a short-lived period as an independent republic during the 1960s, Anguilla has been a separate British overseas territory since 1980.\nPre-Columbian Anguilla.\nThe earliest inhabitants of Anguilla were Amerindian people from South America, commonly (if imprecisely) referred to as Arawaks. These people travelled to the island on rafts and in dugout canoes, settling in fishing, hunting and farming groups. Forty Arawak villages have been excavated, the largest being those at Island Harbour, Sandy Ground, Sandy Hill, Rendezvous Bay, and Shoal Bay East. The Amerindian name for the island was \"Malliouhana\". The earliest Amerindian artefacts found on Anguilla have been dated to around 1300\u00a0BC, and remains of settlements dating from AD\u00a0600 have been uncovered. Religious artefacts and remnants of ceremonies found at locations, such as Big Springs and Fountain Cavern, suggest that the pre-European inhabitants were extremely religious in nature. The Arawaks are popularly said to have been later displaced by fiercer Carib, but this version of events and characterisation is disputed by some.\nColonial Anguilla.\nThe European discovery and renaming of the island is uncertain. Some claim it had been sighted by Columbus; others credit it to the French explorer Ren\u00e9 Goulaine de Laudonni\u00e8re during his voyages in 1564 and 1565. The Dutch West India Company established a fort on the island in 1631. The Dutch withdrew after the destruction of the fort by Spanish forces in 1633.\nAnguilla was conquered and colonised by English settlers from St. Christopher beginning in 1650. A local council was formed, overseen by Antigua. Six years later, natives from another island attacked, killing most of the men and enslaving the women and children. In 1666, 300 Frenchmen attacked the island, driving the settlers into the forests. It was subsequently returned to the English by the terms of the 1667 Treaty of Breda. The French army assisted by a limited number of Anglo-Irish attacked in 1688, driving the English off the island to Antigua, and periods of drought during the 1680s left conditions so poor that many Anguillians left for St Croix and the British Virgin Islands in 1694. During this drought laden period there were several abortive attempt to settle on Crab Island off the coast of Puerto Rico, as the island was seen as more habitable in comparison to dry and arid Anguilla. The effort to settle Crab Island was led by Abraham Howell, and saw a handful of Anguillians partake, however, the settlers were eventually forcibly evicted by Spanish forces. In 1724, the population had rebuilt to 360 Europeans and 900 Africans.\nIn 1744, during the War of the Austrian Succession, 300 Anguillians and 2 privateers from St. Christopher invaded the French half of neighbouring Saint Martin, holding it until the 1748 Treaty of Aix-la-Chapelle. Two French frigates landed 700 or 1000 men at Crocus Bay on Anguilla in 1745 but were repulsed by 150 militiamen under Governor Hodge.\nIn 1758, during the Seven Year's War, two runaway slaves, named Pero and George, boarded an enemy French Schooner and piloted and assisted the crew in raiding Anguilla. They were both captured and sentenced to death. Pero by hanging, and George by burning alive. The Seven Year's War saw Anguilla being utilized as a base for British privateers, and a Court of Vice-Admiralty was established there for the adjudication of captured vessels and cargoes. As such, the colony was a target for French corsairs and raiders. \nOn 27 November 1796, amid the Napoleonic Wars, the French warships \"D\u00e9cius\" and \"Vaillante\" landed 400 Frenchmen at Rendezvous Bay under Victor Hugues. These were able to destroy the villages at South Hill and The Valley, but the local British regrouped on the Long Path before Sandy Hill Fort. The frigate HMS \"Lapwing\", sailing from St. Christopher under Captain Barton, was able to defeat the French ships and the assault again ended in failure. Hugues' attack was noticeably brutal, with several Anguillians being murdered and kidnapped. The landing also left many homes and buildings completely destroyed. This left the island in a bleak state, reeling for decades to come.\nAttempts were made from the early days of the colony to develop Anguilla into a plantation-based economy employing enslaved Africans, but the island's soil and climate were unfavourable and the plantations were largely unsuccessful. Despite this, slaves in Anguilla were not immune to the atrocities and injustices of chattel slavery common during era of enslavement in the West Indies, as slaves were still subject to brutal punishment, forced labour and ill-treatment at the hands of their masters. The lack of successful large-scale plantations and a formal colonization meant limited oversight, colonial communication and proper administration, giving Anguilla an unlawful reputation. This also paved the way for many cases of slave abuse on the island. Anguilla's population is estimated to have fallen from a peak of around 10,000 to just 2000. In 1819, there were 360 Europeans, 320 free Africans, and 2451 slaves. Several mixed-race mulatto slaves that were children of their masters were often willed freedom and in some cases willed land. In addition to the mulatto class of manumissions, there are cases of Anguillian slaves and aprentices being rented for their labour to other islands and using the compensation to purchase their freedom and plots of land for paltry sums, as the land was often seen as fruitless by their previous masters. In March of 1831, the free black and coloured class petitioned for equal treatment; however, this was dismissed by the white ruling class, with the foreman of the jury claiming that it would lead to a white exodus. There were droughts and famines in the 1830s and 1840s. These periods of drought and famine exacerbated the already existing destitution, leading to widespread poverty, crop failure, economic hardship, and suffering on the island, which in turn led to lawlessness and administrative failure. As a result, theft, assault, and smuggling became abundant as a means of survival, especially amongst the slaves, later turned apprenticed labourers. Amidst the turmoil, slaveowners couldn't afford to feed or clothe their slaves, leading many of the enslaved to become insubordinate and abscond from their estates and or refuse to work. The British abolished slavery in their colonies during the 1830s. During the early years of abolition and the period leading up to the decree, several slave owners and planters illicitly removed black Anguillians off-island to be sold in neighboring islands that still practiced slavery in order to recoup financial losses. Emancipation increased the number of runaway slaves to Anguilla from the neighbouring French and Dutch islands, who were yet to be freed. While the plantation owners returned to Europe, or migrated to other parts of the Americas, the freedmen continued to eke out livings on Anguilla as subsistence farmers and fishermen. \nThe British government attempted to send the entire population of the island to Demerara in British Guiana (modern Guyana) but most remained. In the 19th century, the large lake in the center of the island was exploited for salt exported to the United States; around 3,000,000 bushels were produced each year. This formed the island's principal trade, although sugar, cotton, and tobacco were also produced.\nIn 1871, Anguilla was forced into a federation with St Kitts; the next year, the islands petitioned the British colonial office to permit separate and direct rule. Around this time, the population had risen to 3000. In 1882, Nevis was added. The population had risen to 3890 by the time of the First World War. By that time, charcoal production had essentially deforested the entire island, but the expanded pastureland permitted export of cattle to Saint Thomas. Phosphate of lime was also produced.\nIt was not until 1951 that Anguilla had a greater say in its administration, the British colony of Saint Christopher-Nevis-Anguilla, itself part of the Federal Colony of the Leeward Islands. Between 1958 and 1962, the tri-state was part of the West Indies Federation.\nModern Anguilla.\nOn 27\u00a0 1967 ()\u00a0(1967--), Britain granted the territory of Saint Christopher-Nevis-Anguilla the status of \"associated state\", with its own constitution and a considerable degree of self-government. Many Anguillans strenuously objected to the continuing political subservience to Saint Kitts, and on 30 May 1967 (known as \"Anguilla Day\"), the Kittian police were evicted from the island. The provisional government requested United States administration, which was declined. On 11 July 1967 a referendum on Anguilla's secession from the fledgling state was held. The results were 1,813 votes for secession and 5 against. A separate legislative council was immediately declared. Peter Adams served as the first Chairman of the Anguilla Island Council. After eight days of negotiation on Barbados, on 31 July, Adams agreed to return Anguilla to the Anguilla\u2013StKitts\u2013Nevis federation, in exchange for granting Anguilla limited self-rule similar to that enjoyed by Nevis. Adams agreed to support this pact in principle, but the Council rejected it, replacing Adams as chairman with Ronald Webster. In December, two members of Britain's Parliament worked out an interim agreement by which for one year a British official would exercise basic administrative authority along with the Anguilla Council. Tony Lee took the position in January 1968, but by the end of the term no agreement have been reached on the long-term future of the island's government.\nOn 7 February 1969 Anguilla held a second referendum resulting in a vote of 1,739 to 4 against returning to association with Saint Kitts. At this point Anguilla declared itself an independent republic, with Webster again serving as chairman. A new British envoy, William Whitlock, arrived on 11 March 1969 with a proposal for a new interim British administration. He was quickly expelled. On 19 March 1969, a contingent of 2nd Battalion, the Parachute Regiment, and 40 Metropolitan Police officers peacefully landed on the island, ostensibly to \"restore order\". That autumn the troops left and Army engineers were brought in to improve the public works. Tony Lee returned as Commissioner and in 1971 worked out another \"interim agreement\" with the islanders. Effectively Anguilla was allowed to secede from Saint Kitts and Nevis, receiving its first constitution on 12 February 1976. It was not until 19 December 1980 that Anguilla was formally disassociated from Saint Kitts to become a separate British dependency by the Anguilla Act 1980. While Saint Kitts and Nevis went on to gain full independence from Britain in 1983, Anguilla still remains a British overseas territory.\nIn recent years Anguilla has become an up-market tourist destination, and tourism is one of the mainstays of the economy. Fishing is another important economic activity, and a financial services sector is also being developed. The modern population of Anguilla is largely of African descent, with a minority having European (mainly English) ancestry.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39569", "revid": "5831124", "url": "https://en.wikipedia.org/wiki?curid=39569", "title": "University of Glasgow", "text": "Public university in Scotland\nThe University of Glasgow (abbreviated as Glas. in post-nominals; ) is a public research university in Glasgow, Scotland. Founded by papal bull in 1451 [O.S. 1450], it is the fourth-oldest university in the English-speaking world and one of Scotland's four ancient universities. Along with the universities of St Andrews, Aberdeen, and Edinburgh, the university was part of the Scottish Enlightenment during the 18th century. Glasgow is the second largest university in Scotland by total enrolment and 9th-largest in the United Kingdom.\nIn common with universities of the pre-modern era, Glasgow originally educated students primarily from wealthy backgrounds; however, it became a pioneer in British higher education in the 19th century by also providing for the needs of students from the growing urban and commercial middle class. Glasgow University served all of these students by preparing them for professions: law, medicine, civil service, teaching, and the church. It also trained smaller but growing numbers for careers in science and engineering. \nGlasgow has the fifth-largest endowment of any university in the UK and the annual income of the institution for 2023\u201324 was \u00a3950\u00a0million of which \u00a3221.1\u00a0million was from research grants and contracts, with an expenditure of \u00a3658.6\u00a0million.&lt;ref name=\"Glasgow Financial Statement 23/24\"&gt;&lt;/ref&gt; It is a member of Universitas 21, the Russell Group and the Guild of European Research-Intensive Universities.\nThe university was originally located in the city's High Street; since 1870, its main campus has been at Gilmorehill in the City's West End. Additionally, a number of university buildings are located elsewhere, such as the Veterinary School in Bearsden, and the Crichton Campus in Dumfries.\nThe alumni of the University of Glasgow include some of the major figures of modern history, including James Wilson, a signatory of the United States Declaration of Independence, 3 Prime Ministers of the United Kingdom (William Lamb, Henry Campbell-Bannerman and Bonar Law), 3 Scottish First Ministers (Humza Yousaf, Nicola Sturgeon and Donald Dewar), economist Adam Smith, philosopher Francis Hutcheson, engineer James Watt, physicist Lord Kelvin, surgeon Joseph Lister along with 4 Nobel Prize laureates (in total 8 Nobel Prize winners are affiliated with the University) and numerous Olympic gold medallists, including the current chancellor, Dame Katherine Grainger.\nHistory.\nThe University of Glasgow was founded in 1451 by a charter or papal bull from Pope Nicholas V, at the suggestion of King James II, giving Bishop William Turnbull, a graduate of the University of St Andrews, permission to add a university to the city's Cathedral. It is the second-oldest university in Scotland after St Andrews and the fourth-oldest in the English-speaking world. The universities of St Andrews, Glasgow, and Aberdeen were ecclesiastical foundations, while Edinburgh was a civic foundation. As one of the ancient universities of the United Kingdom, Glasgow is one of only eight institutions to award undergraduate master's degrees in certain disciplines.\nThe university has been without its original Bull since the mid-sixteenth century. In 1560, during the political unrest accompanying the Scottish Reformation, the then chancellor, Archbishop James Beaton, a supporter of the Marian cause, fled to France. He took with him, for safe-keeping, many of the archives and valuables of the cathedral and the university, including the Mace and the Bull. Although the Mace was sent back in 1590, the archives were not. Principal James Fall told the Parliamentary Commissioners of Visitation on 28 August 1690, that he had seen the Bull at the Scots College in Paris, together with the many charters granted to the university by the monarchs of Scotland from James II to Mary, Queen of Scots. The university enquired of these documents in 1738 but was informed by Thomas Innes and the superiors of the Scots College that the original records of the foundation of the university were not to be found. If they had not been lost by this time, they certainly went astray during the French Revolution when the Scots College was under threat. Its records and valuables were moved for safe-keeping out of the city of Paris. The Bull remains the authority by which the university awards degrees.\nTeaching at the university began in the Chapter House of Glasgow Cathedral, subsequently moving to nearby Rottenrow, in a building known as the \"Auld Pedagogy\". The university was given of land belonging to the Black Friars (Dominicans) on High Street by Mary, Queen of Scots, in 1563. By the late 17th century its building centred on two courtyards surrounded by walled gardens, with a clock tower, which was one of the notable features of Glasgow's skyline\u2014reaching in height\u2014and a chapel adapted from the church of the former Dominican (Blackfriars) friary. Remnants of this Scottish Renaissance building, mainly parts of the main fa\u00e7ade, were transferred to the Gilmorehill campus and renamed as the \"Pearce Lodge\", after Sir William Pearce, the shipbuilding magnate who funded its preservation. The Lion and Unicorn Staircase was also transferred from the old college site and is now attached to the Main Building.\nJohn Anderson, while professor of natural philosophy at the university, and with some opposition from his colleagues, pioneered vocational education for working men and women during the Industrial Revolution. To continue this work in his will, he founded Anderson's College, which was associated with the university before merging with other institutions to become the University of Strathclyde in 1964.\nIn 1973, Delphine Parrott became its first female professor, as Gardiner Professor of Immunology.\nIn October 2014, the university court voted for the university to become the first academic institution in Europe to divest from the fossil fuel industry. This followed a 12-month campaign led by the Glasgow University Climate Action Society and involved over 1,300 students.\nCampus.\nThe university is currently spread over a few campuses. The main one is the Gilmorehill campus, in Hillhead. As well as this there is the Garscube Estate in Bearsden, housing the Veterinary School, Observatory, ship model basin and much of the university's sports facilities, the Dental School in the city center, the section of Mental Health and Well Being at Gartnavel Royal Hospital on Great Western Road, the Teaching and Learning Centre at the Queen Elizabeth University Hospital and the Crichton Campus in Dumfries (operated jointly by the University of Glasgow, the University of the West of Scotland and the Open University).\nThe Imaging Centre of Excellence (ICE) was opened at the Queen Elizabeth University Hospital on 29 March 2017, including a Clinical Innovation Zone spanning of collaboration space for researchers and industry.\nHigh Street.\nThe university's initial accommodation including Glasgow University Library was part of the complex of religious buildings in the precincts of Glasgow Cathedral. In 1460, the university received a grant of land from James, Lord Hamilton, on the east side of the High Street, immediately north of the Blackfriars Church, on which it had its home for the next four hundred years. In the mid-seventeenth century, the Hamilton Building was replaced with a very grand two-court building with a decorated west front facing the High Street, called the 'Nova Erectio', or New Building. This foundation is widely considered to have been one of the finest 17th-century buildings in Scotland. Decorated fragments from it, including a complete exterior stairway, were rescued and built into its 19th-century replacement. In Sir Walter Scott's best-selling 1817 novel \"Rob Roy\", set at the time of the Jacobite rising of 1715, the lead character fights a duel in the New Building grounds before the contest is broken up by Rob Roy MacGregor.\nOver the following centuries, the university's size and scope continued to expand. In 1757 it built the Macfarlane Observatory and later Scotland's first public museum, the Hunterian. It was a center of the Scottish Enlightenment and subsequently of the Industrial Revolution, and its expansion in the High Street was constrained. The area around the university declined as well-off residents moved westwards with the expansion of the city and overcrowding of the immediate area by less well-off residents. It was this rapid slumming of the area that was a chief catalyst of the university's migration westward.\nGilmorehill.\nIn 1870, the university moved to a (then greenfield) site on Gilmorehill in the West End of the city, around west of its previous location, enclosed by a large meander of the River Kelvin. The original site on the High Street was sold to the City of Glasgow Union Railway and replaced by the college goods yard. The new-build campus was designed by Sir George Gilbert Scott in the Gothic revival style. The largest of these buildings echoed, on a far grander scale, the original High Street campus's twin-quadrangle layout, and may have been inspired by Ypres' late-medieval cloth hall; Gilmorehill, in turn, inspired the design of the Clocktower complex of buildings for the new University of Otago in New Zealand. In 1879, Gilbert Scott's son, Oldrid, completed this original vision by building an open undercroft forming two quadrangles, above which is his grand Bute Hall (used for examinations and graduation ceremonies), named after its donor, John Crichton-Stuart, 3rd Marquess of Bute. Oldrid also later added a spire to the building's signature gothic bell tower in 1887, bringing it to a total height of some . The local Bishopbriggs blond sandstone cladding and Gothic design of the building's exterior belie the modernity of its Victorian construction; Scott's building is structured upon what was then a cutting-edge riveted iron frame construction, supporting a lightweight wooden-beam roof. The building also forms the second-largest example of Gothic revival architecture in Britain, after the Palace of Westminster. An illustration of the Main Building previously featured on the reverse side of \u00a3100 notes issued by Clydesdale Bank.\nThe university's Hunterian Museum resides in the Main Building, and the related Hunterian Gallery is housed in buildings adjacent to the University Library. The latter includes \"The Mackintosh House\", a rebuilt terraced house designed by, and furnished after, architect Charles Rennie Mackintosh.\nEven these enlarged premises could not contain the expanding university, which quickly spread across much of Gilmorehill. The 1930s saw the construction of the award-winning round Reading Room (it is now a category-A listed building) and an aggressive program of house purchases, in which the university (fearing the surrounding district of Hillhead was running out of suitable building land) acquired several terraces of Victorian houses and joined them together internally. The departments of Psychology, Computing Science, and most of the Arts Faculty continue to be housed in these terraces.\nMore buildings were built to the west of the Main Building, developing the land between University Avenue and the River Kelvin with natural science buildings and the faculty of medicine. The medical school spread into neighboring Partick and joined with the Western Infirmary. At the eastern flank of the Main Building, the James Watt Engineering Building was completed in 1959. The growth and prosperity of the city, which had originally forced the university's relocation to Hillhead, again proved problematic when more real estate was required. The school of veterinary medicine, which was founded in 1862, moved to a new campus in the leafy surrounds of Garscube Estate, around west of the main campus, in 1954. The university later moved its sports ground and associated facilities to Garscube and also built student halls of residence in both Garscube and Maryhill.\nThe expected growth of tertiary education in the 1960s following publication of the Robbins Report led the university to build numerous modern buildings across Hillhead in a development zone, originally comprising mainly residential tenements, that had been designated on the north side of University Avenue in 1945. Several of these new buildings were in the brutalist style; the Mathematics Building at the west end of University Avenue (opened 1968, demolished 2017), the Rankine Building at the east end of University Avenue (opened 1970), the multipurpose Adam Smith Building (opened 1967) on the crest of the hill above University Gardens, and the new Queen Margaret Union building (opened 1968) on the University Gardens site previously occupied by the University Observatory. These were joined by others in various modernist styles; both the Library and Boyd Orr Building (opened 1968 and 1972 respectively) were configured as tower blocks, as was the Genetics Building at the very south end of the campus on Dumbarton Road (opened 1967, named for Guido Pontecorvo in 1994, demolished 2021), while the amber-brick Geology Building (opened 1980, named for John Walter Gregory in 1998, renamed for Silas Molema in 2021) was built to a low-rise design on the former site of eight terraced houses in Lilybank Gardens. \nTo further cater to the expanding student population, a new refectory\u2014known as the Hub\u2014was opened adjacent to the library in 1966, and the Glasgow University Union building at the eastern end of University Avenue was extended in 1965.\nIn October 2001 the century-old Bower Building (previously home to the university's botany department) was gutted by fire. The interior and roof of the building were largely destroyed, though the main fa\u00e7ade remained intact. After a \u00a310.8 million refit, the building re-opened in November 2004.\nThe Wolfson Medical School Building, with its award-winning glass-fronted atrium, opened in 2002, and in 2003, the St Andrews Building was opened, housing what is now the School of Education. It is sited a short walk from Gilmorehill, in the Woodlands area of the city on the site of the former Queens College, which had in turn been bought by Glasgow Caledonian University, from whom the university acquired the site. It replaced the St Andrews Campus in Bearsden. The university also procured the former Hillhead Congregational Church, converting it into a lecture theatre in 2005. The Sir Alwyn Williams building, designed by Reiach and Hall, was completed at Lilybank Terrace in 2007, housing the School of Computing Science.\nIn September 2016, in partnership with Glasgow City Council, Glasgow Life, and the National Library of Scotland, the transformed Kelvin Hall was brought into new public use including in Phase I the Hunterian Collections and Study Centre.\nThe Mathematics Building, on University Way adjacent to the Boyd Orr Building, was demolished in 2017 to make way for a new 'Learning Hub' intended to provide individual and group study spaces for more than 2,500 students, as well as a 500-seat lecture theatre. Built at a cost of \u00a390.6million, it opened in April 2021 and is named for James McCune Smith, the first African American to earn a degree in medicine and a University of Glasgow alumnus. A further investment of over \u00a3900million is being made across the Gilmorehill campus, focused mainly on redeveloping the site between University Avenue and Dumbarton Road that was occupied by the Western Infirmary between 1874 and 2015.\nChapel.\nThe University Chapel was constructed as a memorial to the 755 sons of the university who had died in the First World War. Designed by Sir John Burnet, it was completed in 1929 and dedicated on 4 October. Tablets on the wall behind the Communion Table list the names of those who died, while other tablets besides the stalls record the 405 members of the university community who gave their lives in the Second World War. Most of the windows are the work of Douglas Strachan, although some have been added over the years, including those on the South Wall, created by Alan Younger.\nDaily services are held in the chapel during term-time, as well as seasonal events. Before Christmas, there is a Service of Nine Lessons and Carols on the last Sunday of term, and a Watchnight service on Christmas Eve. Graduates, students, members of staff, and the children of members of staff are entitled to be married in the chapel, which is also used for baptisms and funerals. Civil marriages and civil partnerships may be blessed in the chapel, although under UK law may not be performed there.\nThe current chaplain of the university is the Reverend Stuart MacQuarrie, and the university appoints honorary chaplains of other denominations.\nLibrary and archives.\nGlasgow University Library is situated on Hillhead Street opposite the Main Building. The current 12-storey building was opened in 1968 and hosts approximately 2.5 million books and journals, and provides electronic resources, including over 50,000 electronic journals. It houses sections for periodicals, microfilms, special collections and rare materials.\nIn addition to the main library, subject libraries exist for Medicine, Chemistry, Dental Medicine, Veterinary Medicine, Education, Law, History of Art, and the faculty of Social Sciences, which are held in branch libraries around the campus. In 2007, a section to house the library's collection of historic photographs was opened, funded by the Wolfson Foundation.\nThe Archives of the University of Glasgow maintains the historical records of the university, created and accumulated since its foundation in 1451.\nCrichton campus, Dumfries.\nThe university opened the Crichton campus in Dumfries, Dumfries and Galloway during the 1980s. It was designed to meet the needs for tertiary education in an area far from major cities and is operated jointly by the University of Glasgow, the University of the West of Scotland and the Open University. It offers a modular undergraduate curriculum, leading to one of a small number of liberal arts degrees, as well as providing the region's only access to postgraduate study.\nNon-teaching facilities.\nAs well as the teaching campuses, the university has halls of residence in and around the north-west of the city, accommodating a total of approximately 3,500 students. These include the Murano Street Student Village in Maryhill; Wolfson halls on the Garscube Estate; Queen Margaret halls in Kelvinside; and Cairncross House and Kelvinhaugh Gate, in Yorkhill. In recent years, Dalrymple House and Horslethill halls in Dowanhill, Reith halls in North Kelvinside and the Maclay halls in Park Circus (near Kelvingrove Park), have closed and been sold, as the development value of such property increased.\nThe Stevenson Building on Gilmorehill opened in 1961 and provides students with the use of a fitness suite, squash courts, sauna, and six-lane, 25-metre swimming pool. The university also has a large sports complex on the Garscube Estate, besides their Wolfson Halls and Vet School. This is a new facility, replacing the previous \"Westerlands\" sports ground in the Anniesland area of the city. The university also has use of half of the East Boathouse situated at Glasgow Green on the River Clyde where Glasgow University Boat Club train.\nGovernance and administration.\nIn common with the other ancient universities of Scotland the university's constitution is laid out in the Universities (Scotland) Acts. These Acts create a tripartite structure of bodies: the University Court (governing body), the Academic Senate (academic affairs), and the General Council (advisory). There is also a clear separation between governance and executive administration.\nThe university's constitution, academic regulations, and appointments are described in the university calendar, while other aspects of its story and constitution are detailed in a separate \"history\" document.\nUniversity officials.\nThe university's three most significant officials are its chancellor, principal, and rector, whose rights and responsibilities are largely derived from the Universities (Scotland) Act 1858.\nThe Chancellor is the titular head of the university and President of the General Council. They award all degrees, although this duty is generally carried out by the Vice-Chancellor, appointed by them. The current Chancellor is Dame Katherine Grainger, a former rower who is Britain's most decorated female Olympian, the current chair of UK Sport, and former Chancellor of Oxford Brookes University. She is an alumna of the university, with a Master of Philosophy (MPhil) in Medical Law and Medical Ethics. She is the first woman to hold the office in the university.\nDay-to-day management of the university is undertaken by the University Principal (who is also Vice-Chancellor). The current principal is Andy Schofield who replaced Sir Anton Muscatelli in October 2025. There are also several Vice-Principals, each with a specific remit. They, along with the Clerk of Senate, play a major role in the day-to-day management of the university.\nAll students at the university are eligible to vote in the election of the Rector (officially styled \"Lord Rector\"), who holds office for a three-year term and chairs the University Court. In the past, this position has been a largely honorary and ceremonial one, and has been held by political figures including William Gladstone, Benjamin Disraeli, Bonar Law, Robert Peel, Raymond Poincar\u00e9, Arthur Balfour, Charles Kennedy and 1970s union activist Jimmy Reid, and latterly by celebrities such as TV presenters Arthur Montford and Johnny Ball, musician Pat Kane, and actors Richard Wilson, Ross Kemp and Greg Hemphill. In 2004, for the first time in its history, the university was left without a Rector as no nominations were received. When the elections were run in December, Mordechai Vanunu was chosen for the post, even though he was unable to attend due to restrictions placed upon him by the Israeli government. In 2014, Edward Snowden, an American computer specialist, a former Central Intelligence Agency (CIA) employee, and former National Security Agency (NSA) contractor\u2014who came to international attention when he disclosed a large number of classified NSA documents to several media outlets\u2014was elected. In 2017, Aamer Anwar a Scottish lawyer and former student of the university was elected rector until 2020 when rector elections had to be postponed due to the COVID-19 pandemic. On April 21, 2021, Rita Rae, Lady Rae a Scottish lawyer, judge and former Senator of the College of Justice was appointed Rector after a decisive victory. The current office holder is Dr. Ghassan Abu-Sittah, who was installed in the position on 11 April 2024, after winning 80% of the vote and while under investigation by the University.\nUniversity Court.\nThe governing body of the university is the University Court, which is responsible for contractual matters, employing staff, and all other matters relating to finance and administration. The Court takes decisions about the deployment of resources as well as formulating strategic plans for the university. The Court is chaired by the Rector, who is elected by all the matriculated students at the university. The University Secretary is the Head of University Services and assists the Principal in day-to-day management. The current University Secretary is David Duncan.\nAcademic Senate.\nThe Academic Senate (or University Senate) is the body which is responsible for the management of academic affairs, and which recommends the conferment of degrees by the Chancellor. Membership of the Senate comprises all Professors of the university, as well as elected academic members, representatives of the Student's Representative Council, the Secretary of Court and directors of university services (e.g. Library). The President of the Senate is the principal.\nThe Clerk of Senate, who has a status equivalent to that of a Vice-Principal and is a member of the Senior Management Group, has responsibility for regulation of the university's academic policy, such as dealing with plagiarism and the conduct of examinations. Notable Clerks of Senate have included the chemist, Joseph Black; John Anderson, father of the University of Strathclyde; and the economist, John Millar.\nCommittees.\nThere are also a number of committees of both the Court and Senate that make important decisions and investigate matters referred to them. As well as these bodies there is a General Council made up of the university graduates that is involved in the running of the university. The graduates also elect the Chancellor of the university.\nResearch System and Repository.\nThe University maintains an in-house constructed research information system containing data on all institutional research, including financial and personnel information. This Research System is closely linked to the \"Enlighten\" institutional repository, which is effectively a collection of research output in the form of publications and theses.\nFinances.\nIn the financial year ending 31 July 2024, the University of Glasgow had a total income of \u00a3950\u00a0million (2022/23 \u2013 \u00a3944.2\u00a0million) and total expenditure of \u00a3658.6\u00a0million (2022/23 \u2013 \u00a3827.4\u00a0million). Key sources of income included \u00a3387.8\u00a0million from tuition fees and education contracts (2022/23 \u2013 \u00a3403.8\u00a0million), \u00a3182.7\u00a0million from funding body grants (2022/23 \u2013 \u00a3185.9\u00a0million), \u00a3221.1\u00a0million from research grants and contracts (2022/23 \u2013 \u00a3220.7\u00a0million), \u00a340.2\u00a0million from investment income (2022/23 \u2013 \u00a323\u00a0million) and \u00a37.3\u00a0million from donations and endowments (2022/23 \u2013 \u00a38\u00a0million).\nAt year end, Glasgow had endowments of \u00a3262.4\u00a0million (2023 \u2013 \u00a3234.3\u00a0million) and total net assets of \u00a31.409\u00a0billion (2023 \u2013 \u00a31.079\u00a0billion). It holds the fifth-largest endowment of any university in the UK.\nOrganisation.\nThere are currently four Colleges, each containing a number of Schools. They are:\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nAt the university's foundation in 1451, there were four original faculties: Arts, Divinity, Law, and Medicine. The Faculty of Divinity became a constituent school of the Faculty of Arts in 2002, while the Faculty of Law was changed in 1984 into the Faculty of Law and Financial Studies, and in 2005 became the Faculty of Law, Business and Social Sciences. Although one of the original faculties established, teaching in the Faculty of Medicine did not begin formally until 1714, with the revival of the Chair in the Practice of Medicine. The Faculty of Science was formed in 1893 from Chairs removed from the Faculties of Arts and Medicine, and subsequently divided in 2000 to form the three Faculties of Biomedical and Life Sciences, Computing Science, Mathematics and Statistics (now Information and Mathematical Sciences) and Physical Sciences. The Faculty of Social Sciences was formed from Chairs in the Faculty of Arts in 1977, and merged to form the Faculty of Law, Business and Social Sciences in 2005, the two having operated as a single 'resource unit' since 2002. The Faculty of Engineering was formally established in 1923, although engineering had been taught at the university since 1840 when Queen Victoria founded the UK's first Chair of Engineering. Through a concordat ratified in 1913, Royal Technical College (later Royal College of Science and Technology and now University of Strathclyde) students received Glasgow degrees in applied sciences, particularly engineering. It was in 1769 when James Watt's engineering at Glasgow led to a stable steam engine and, subsequently, the Industrial Revolution. The Faculty of Veterinary Medicine was established in 1862 as the independent Glasgow Veterinary College, being subsumed into the university in 1949 and gaining independent Faculty status in 1969. The Faculty of Education was formed in 1999 when the university merged with St Andrew's College of Education, which had been formed in 1981 through the merger of two Catholic colleges: Notre Dame College of Education, Glasgow, founded in 1895 and Craiglockhart College of Education, Edinburgh, founded in 1920.\nOn 1 August 2010, the former faculties of the university were removed and replaced by a system of four larger Colleges, intended to encourage interdisciplinary research and make the university more competitive. This structure was similar to that at other universities, including the University of Edinburgh.\nAcademic profile.\nRankings and reputation.\nThe university is a member of the Russell Group of research-led British universities and was a founding member of the organisation Universitas 21, an international grouping of universities dedicated to setting worldwide standards for higher education. The university currently has fifteen Regius Professorships, more than in any other UK university.\nIn the QS World University Rankings, Glasgow climbed to 51st in 2013.\nIn national rankings, Glasgow places within the top 30 in the UK and 3rd in Scotland for the employability of its graduates as ranked by recruiters from the UK's major companies.\nIn the 2008 Research Assessment Exercise (RAE), almost 70% of research carried out at the university was in the top two categories (88% in the top three categories). Eighteen subject areas were rated top ten in the UK, whilst fourteen subject areas were rated the best in Scotland. The 2008 Times RAE table ranks according to an 'average' score across all departments, of which Glasgow posted an average of 2.6/4. The overall average placed Glasgow as the 33rd-highest of all UK universities, perhaps reflecting the broadness of the university's activities. In terms of research 'power', however, Glasgow placed 14th in the UK and 2nd in Scotland.\nAdmissions.\nAs of 2023/24, the university had undergraduate and postgraduate students. Glasgow has a large (for the UK) proportion of \"home\" students, with almost 40 per cent of the student body coming from the West of Scotland. In the 2016\u201317 academic year, the university had a domicile breakdown of 71:11:18 of UK:EU:non-EU students, respectively, with a female-to-male ratio of 59:41.\nFor undergraduate entry, in 2018 course requirements ranged from A*A*A* (for second year entry) to BBB (for minimum requirements for Primary Teaching) in A-levels. Glasgow had the third highest average entry qualification for undergraduates of any UK university in 2015, with new students averaging 420 UCAS points, equivalent to ABBbb in A-level grades.\nAs the number of places available for Scottish applicants are capped by the Scottish Government as they do not pay tuition fees, students applying from the rest of the UK and outside of the UK have a higher likelihood of an offer. For most courses, with the exceptions of Medicine, Dentistry, Veterinary Medicine and Law, the university guarantees unconditional offers to applicants who have achieved AAAA or AAABB in one sitting at Scottish Highers. The other components of the applicant's UCAS form (such as predicted grades and the personal statement) are only taken into account if the applicant has not achieved these grades.\nStudent life.\nUnlike other universities in Scotland, Glasgow does not have a single students' association; instead, there exist a number of bodies concerned with the representation, welfare, and entertainment of its students. Due to the university's retention of its separate male and female students' unions, which since 1980 have admitted both sexes as full members, there are two independent students' unions, as well as a sports association and the students' representative council. None of these are affiliated to the National Union of Students: membership has been rejected on a number of occasions, most recently in November 2006, on both economic and political grounds. A student-run \"No to NUS\" campaign won a campuswide referendum with more than 90% of the vote.\nIn common with the other ancient universities of Scotland, students at Glasgow also elect a Rector.\nThe university has an eclectic body of clubs and societies, including sports teams, political and religious groups, and gaming societies.\nStudents' Representative Council.\nGlasgow University Students' Representative Council is the legal representative body for students, as recognized by the Universities (Scotland) Act 1889. The SRC is responsible for representing students' interests to the management of the university, to local and national government, and for health and welfare issues. Under the Universities (Scotland) Acts, all students of the university automatically become members of the SRC; however, they are entitled to opt-out of this. Members of the SRC sit on various committees throughout the university, from the departmental level to the Senate and Court.\nThe SRC organizes Media Week, RAG (Raising And Giving) Week, and Welfare Week, as well as funding some 130 clubs and societies.\nUnions.\nIn addition to the Students' Representative Council, students are commonly members of one of the university's two students' unions, the Glasgow University Union (GUU) and the Queen Margaret Union (QMU). Unlike many other student unions in the UK, membership to either GUU or QMU is not automatic and students must apply, for free, to become a member of either. Students are also permitted to be a member of both. These are largely social and cultural institutions, providing their members with facilities for debating, dining, recreation, socializing, and drinking, and both have a number of meeting rooms available for rental to members. Postgraduate students, mature students and staff were previously able to join the Hetherington Research Club; however, large debts led to the club being closed in February 2010. However, in February 2011, students gained access to the old HRC building, situated at 13 University Gardens (Hetherington House) and \"reopened\" it as the Free Hetherington, a social centre for learning and lectures, as well as protesting the shutting down of the club. Attempts to evict this occupation resulted in complaints of heavy-handed policing and much controversy on campus.\nThe separate unions exist due to the university's previous male-only status; the GUU was founded before the admission of women, while the QMU was originally the union of Queen Margaret College, a women-only college which merged with the university in 1892. Their continued separate existence is due largely to their individual atmospheres. The GUU's focus is mainly towards people involved in sports and debates (as among its founders were the Athletic Association and Dialectic Society), the QMU is one of Glasgow's music venues, and has played host to Nirvana, Red Hot Chili Peppers, Biffy Clyro and Franz Ferdinand.\nIn 1955, the GUU won the Observer Mace, now the John Smith Memorial Mace, named after the deceased GUU debater and former leader of the British Labour Party. The GUU has since won the mace debating championship fourteen more times, more than any other university. The GUU has also won the World Universities Debating Championships five times, more than any other university or club in the series' history.\nSports association.\nSporting affairs are regulated by the Glasgow University Sports Association (GUSA) (previously the Glasgow University Athletics Club) which works closely with the Sport and Recreation Service. There are a large number of varied clubs, including Squash, Gaelic Football, Basketball, Cycling, Football, Hockey, Netball, Martial Arts and Rowing, who regularly compete in BUCS competitions. Students who join one of the sports clubs affiliated with the university must also join GUSA. However, there are also regular classes and drop-in sessions for various sports which are non-competitive and available to all university gym members.\nMature Students' Association.\nThe community of mature students\u2014that is those students aged 21 or over\u2014are served by the Mature Students' Association located at 62 Oakfield Avenue. The MSA aims are to provide all mature students with facilities for recreation and study. Throughout the year, the MSA also organizes social events and peer support for the wide range of subjects studied by the university's mature students.\nMedia.\nThere is an active student media scene at the university, part of, but editorially independent from, the SRC. There is a newspaper, the \"Glasgow University Guardian\"; \"Glasgow University Magazine\"; Glasgow University Student Television; and Subcity Radio. In recent years, independent of the SRC, the Queen Margaret Union has published a fortnightly magazine, \"qmunicate\", and Glasgow University Union has produced the \"G-you\" magazine, formerly known as GUUi.\nMountaineering Club.\nGlasgow University Mountaineering Club is an outdoor association whose membership is composed of students and staff. Its origins are known from the late 1930s when students were already meeting on the Arrochar Alps; however, the club was officially constituted at the university in March 1941.\nNotable alumni and staff.\nMany distinguished figures have taught, worked and studied at the University of Glasgow, including seven Nobel laureates and three Prime Ministers, William Lamb, 2nd Viscount Melbourne, Sir Henry Campbell-Bannerman and Bonar Law. Famous names include the physicist Lord Kelvin, his pupil, and later partner of the Carnegie Steel Corporation, George Lauder, 'father of economics' Adam Smith, engineer James Watt, inventors Henry Faulds and John Logie Baird, chemists William Ramsay, Frederick Soddy and Joseph Black, biologist Sir John Boyd Orr, philosophers Francis Hutcheson, Thomas Reid and Dugald Stewart, mathematician Colin Maclaurin, ethnologist James George Frazer, missionary David Livingstone, writers James Boswell, John Buchan, A. J. Cronin, Amy Hoff, Tobias Smollett and Edwin Morgan, and surgeon Joseph Lister. Famous orientalist and president of the Asiatic Society of Bengal Henry Beveridge, University of Aberdeen founder Bishop William Elphinstone also graduated from Glasgow. In June 1933 Albert Einstein gave the first Gibson Lecture, on his general theory of relativity; he subsequently received an honorary degree from the university. Also John Macintyre, pioneer of radiology and Jocelyn Bell Burnell who discovered radio pulsars. In 1974, professors Graham Teasdale and Bryan Jennett developed the Glasgow Coma Scale.\nIn more recent times, the university was the focus of the \"Glasgow Group\" of poets and literary critics, including Philip Hobsbaum, Tom Leonard and Alasdair Gray. The university boasts one of Europe's largest collections of life scientists, as well as having been the training ground of numerous politicians including former Prime Ministers Bonar Law and Sir Henry Campbell-Bannerman, former First Minister Donald Dewar, former leader of the Liberal Democrats and former Rector Charles Kennedy, Defence Secretaries Liam Fox and Des Browne, the founder of the UK Independence Party Alan Sked, former Labour Party leader John Smith, Business Secretary Vince Cable, former leader of the Liberal Democrats Sir Menzies Campbell, and former First Ministers Nicola Sturgeon and Humza Yousaf. Other notable alumni include banker Fred Goodwin, actor Gerard Butler, Rangers and Scottish footballer Neil Murray, actor, writer, television and radio broadcaster Colin Lamont (aka Scottie McClue), novelist Robin Jenkins, founder of the world's largest non-governmental development organisation BRAC Fazle Hasan Abed, television writers Armando Iannucci and Steven Moffat, comedian Greg Hemphill, television presenter Neil Oliver, journalists Andrew Neil and Raman Bhardwaj, and musicians Emeli Sand\u00e9 and Simon Neil.\nWorld Changing Alumni Award.\nWith the World-Changing Alumni Award, formerly the Young Alumnus of the Year Award, the university is recognizing and celebrating the achievements of alumni who have graduated within the last 15 years and made a major contribution to the community, arts, sciences, or business.\nThe award was established in 2001 as part of the university's 550th-anniversary celebrations and is given out once per year. The trophy was donated by the Old Boys of Allan Glen's School, is presented to the winning candidate at one of the year's graduation ceremonies or flagship events.\nWinners:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39570", "revid": "17188112", "url": "https://en.wikipedia.org/wiki?curid=39570", "title": "John Logie Baird", "text": "Scottish inventor, known for first demonstrating television (1888\u20131946)\nJohn Logie Baird (; 13 August 1888\u00a0\u2013 14 June 1946) was a Scottish inventor, electrical engineer, and innovator who demonstrated the world's first mechanical television system on 26 January 1926. He went on to invent the first publicly demonstrated colour television system and the first viable purely electronic colour television picture tube.\nIn 1928, the Baird Television Development Company achieved the first transatlantic television transmission. Baird's early technological successes and his role in the practical introduction of broadcast television for home entertainment have earned him a prominent place in television's history.\nIn 2006, Baird was named as one of the 10 greatest Scottish scientists in history, having been listed in the National Library of Scotland's 'Scottish Science Hall of Fame'. In 2015, he was inducted into the Scottish Engineering Hall of Fame. In 2017, IEEE unveiled a bronze street plaque at 22 Frith Street (Bar Italia), London, dedicated to Baird and the invention of television. In 2021, the Royal Mint unveiled a John Logie Baird 50p coin commemorating the 75th anniversary of his death.\nEarly years.\nBaird was born on 13 August 1888 in Helensburgh, Dunbartonshire, and was the youngest of four children of the Reverend John Baird, the Church of Scotland's minister for the local St Bride's Church, and Jessie Morrison Inglis, the orphaned niece of the wealthy Inglis family of shipbuilders from Glasgow.\nHe was educated at Larchfield Academy (now part of Lomond School) in Helensburgh; the Glasgow and West of Scotland Technical College; and the University of Glasgow. While at college, Baird undertook a series of engineering apprentice jobs as part of his course. The conditions in industrial Glasgow at the time helped form his socialist convictions but also contributed to his ill health. He became an agnostic, though this did not strain his relationship with his father. His degree course was interrupted by the First World War and he never returned to graduate.\nAt the beginning of 1915 he volunteered for service in the British Army but was classified as unfit for active duty. Unable to go to the front, he took a job with the Clyde Valley Electrical Power Company, which was engaged in munitions work.\nTelevision experiments.\nIn early 1923, and in poor health, Baird moved to 21 Linton Crescent, Hastings, on the south coast of England. He later rented a workshop in the Queen's Arcade in the town. Baird built what was to become the world's first working television set using items that included an old hatbox and a pair of scissors, some darning needles, a few bicycle light lenses, a used tea chest, and sealing wax and glue that he purchased. In February 1924, he demonstrated to the \"Radio Times\" that a semi-mechanical analogue television system was possible by transmitting moving silhouette images. In July of the same year he received a 1000-volt electric shock, surviving with only a burnt hand, and was asked by his landlord to vacate the premises. Soon after arriving in London, looking for publicity, Baird visited the \"Daily Express\" newspaper to promote his invention. The news editor was terrified and he was quoted by one of his staff as saying: \"For God's sake, go down to reception and get rid of a lunatic who's down there. He says he's got a machine for seeing by wireless! Watch him\u2014he may have a razor on him.\"\nIn these attempts to develop a working television system, Baird experimented using the Nipkow disk. Paul Gottlieb Nipkow had invented this scanning system in 1884. Television historian Albert Abramson calls Nipkow's patent \"the master television patent\". Nipkow's work is important because Baird, followed by many others, chose to develop it into a broadcast medium.\nIn his laboratory on 2 October 1925, Baird successfully transmitted the first television picture with a greyscale image: the head of a ventriloquist's dummy nicknamed \"Stooky Bill\" in a 32-line vertically scanned image, at five pictures per second. Baird went downstairs and fetched an office worker, 20-year-old William Edward Taynton, to see what a human face would look like, and Taynton became the first person to be televised in a full tonal range.\nIn June 1924, Baird purchased thallium sulfide (developed by Theodore Case in the US) from Cyril Frank Elwell. The chemical became an important part in the development of \"talking pictures.\" Baird's implementation of the thallium sulfide resulted in the first live-animated image on lens from reflected light. He improved the signal conditioning from the thallium sulfide \"cell\" via temperature optimisation (cooling) and his own custom-designed video amplifier, pioneering the technology we now use today.\nFirst public demonstrations.\nBaird gave the first public demonstration of moving silhouette images by television at Selfridges department store in London in a three-week period beginning on 25 March 1925.\nOn 26 January 1926, Baird gave the first public demonstration of true television images for members of the Royal Institution and a reporter from \"The Times\" in his laboratory at 22 Frith Street in the Soho district of London, where Bar Italia is now located. Baird initially used a scan rate of 5 pictures per second, improving this to 12.5 pictures per second c.1927. It was the first demonstration of a television system that could scan and display live moving images with tonal graduation.\nHe demonstrated the world's first colour transmission on 3 July 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with a filter of a different primary colour; and three light sources at the receiving end, with a commutator to alternate their illumination. In the same year he also demonstrated stereoscopic television.\nBroadcasting.\nIn 1927, Baird transmitted the world's first long-distance television pictures over of telephone line between London and the Central Hotel at Glasgow Central station.\nThis transmission was Baird's response to a 225-mile, long-distance telecast between stations of AT&amp;T Bell Labs. The Bell stations were in New York and Washington, DC. The earlier telecast took place in April 1927, a month before Baird's demonstration.\nBaird set up the Baird Television Development Company Ltd, which in 1928 made the first transatlantic television transmission, from London to Hartsdale, New York, and in 1929 the first television programmes officially transmitted by the BBC. In November 1929, Baird and Bernard Natan established France's first television company, T\u00e9l\u00e9vision-Baird-Natan. Broadcast on the BBC on 14 July 1930, \"The Man with the Flower in His Mouth\" was the first drama shown on UK television. The BBC transmitted Baird's first live outside broadcast with the televising of The Derby in 1931. He demonstrated a theatre television system, with a screen two feet by five feet (60\u00a0cm by 150\u00a0cm), in 1930 at the London Coliseum, Berlin, Paris, and Stockholm. By 1939 he had improved his theatre projection to televise a boxing match on a screen by .\nFrom 1929 to 1935, the BBC transmitters were used to broadcast television programmes using the 30-line Baird system, and from 1932 to 1935 the BBC also produced the programmes in their own studio, first at Broadcasting House and then later at 16 Portland Place. In addition, from 1933 Baird and the Baird Company were producing and broadcasting a small number of television programmes independent of the BBC from Baird's studios and transmitter at the Crystal Palace in south London.\nOn 2 November 1936, from Alexandra Palace located on the high ground of the north London ridge, the BBC began alternating Baird 240-line transmissions with EMI's electronic scanning system, which had recently been improved to 405-lines after a merger with Marconi. The Baird system at the time involved an intermediate film process, where footage was shot on cinefilm, which was rapidly developed and scanned. The trial was due to last for 6 months but the BBC ceased broadcasts with the Baird system in February 1937, due in part to a disastrous fire in the Baird facilities at Crystal Palace. It was becoming apparent to the BBC that the Baird system would ultimately fail due in large part to the lack of mobility of the Baird system's cameras, with their developer tanks, hoses, and cables. Commercially Baird's contemporaries, such as George William Walton and William Stephenson, were ultimately more successful as their patents underpinned the early television system used by Scophony Limited who operated in Britain up to WWII and then in the US. \"Of all the electro-mechanical television techniques invented and developed by the mid 1930s, the technology known as Scophony had no rival in terms of technical performance.\" In 1948 Scophony acquired John Logie Baird Ltd.\nBaird's television systems were replaced by the first fully electronic television system developed by the newly formed company EMI-Marconi under Sir Isaac Shoenberg, who headed a research group that developed an advanced camera tube (the Emitron) and a relatively efficient hard-vacuum cathode-ray tube for the television receiver. Philo T. Farnsworth's electronic \"Image Dissector\" camera was available to Baird's company via a patent-sharing agreement. However, the Image Dissector camera was found to be lacking in light sensitivity, requiring excessive levels of illumination. The Baird company used the Farnsworth tubes instead to scan cinefilm, in which capacity they proved serviceable though prone to drop-outs and other problems. Farnsworth himself came to London to the Baird Crystal Palace laboratories in 1936 but was unable to fully solve the problem; the fire that burned Crystal Palace to the ground later that year further hampered the Baird company's ability to compete.\nFully electronic.\nBaird made many contributions to the field of electronic television after mechanical systems became obsolete. In 1939, he showed a system known today as hybrid colour using a cathode-ray tube in front of which revolved a disc fitted with colour filters, a method taken up by CBS and RCA in the United States.\nAs early as 1940, Baird had started work on a fully electronic system he called the \"Telechrome\". Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-colour image could be obtained.\n In 1941, he patented and demonstrated this system of three-dimensional television at a definition of 500 lines. On 16 August 1944, he gave the world's first demonstration of a practical fully electronic colour television display. His 600-line colour system used triple interlacing, using six scans to build each picture.\nIn 1943, the Hankey Committee was appointed to oversee the resumption of television broadcasts after the war. Baird persuaded them to make plans to adopt his proposed 1000-line Telechrome electronic colour system as the new post-war broadcast standard. The picture resolution on this system would have been comparable to today's HDTV (High Definition Television). The Hankey Committee's plan lost all momentum partly due to the challenges of postwar reconstruction. The monochrome 405-line standard remained in place until 1985 in some areas, and the 625-line system was introduced in 1964 and (PAL) colour in 1967. A demonstration of large screen three-dimensional television by the BBC was reported in March 2008, over 60 years after Baird's demonstration.\nOther inventions.\nSome of Baird's early inventions were not fully successful. In his twenties he tried to create diamonds by heating graphite. Later Baird invented a glass razor, which was rust-resistant, but shattered. Inspired by pneumatic tyres he attempted to make pneumatic shoes, but his prototype contained semi-inflated balloons, which burst (years later this same idea was successfully adopted for Dr. Martens boots). He also invented a thermal undersock (the Baird undersock), which was moderately successful. Baird suffered from cold feet, and after several trials, he found that an extra layer of cotton inside the sock provided warmth.\nBetween 1926 and 1928, he attempted to develop an early video recording device, which he dubbed Phonovision. The system consisted of a large Nipkow scanning disk attached by a mechanical linkage to a record-cutting lathe. The result was a disc that could record a 30-line video signal. Technical difficulties with the system prevented its further development, but some of the original Phonovision discs have been preserved.\nBaird's other developments were in fibre-optics, radio direction finding, infrared night viewing and radar. There is discussion about his exact contribution to the development of radar, for his wartime defence projects have never been officially acknowledged by the UK government. According to Malcolm Baird, his son, what is known is that in 1926 Baird filed a patent for a device that formed images from reflected radio waves, a device remarkably similar to radar, and that he was in correspondence with the British government at the time. The radar contribution is in dispute. According to some experts, Baird's \"Noctovision\" is not radar. Unlike radar (except continuous wave radar), Noctovision is incapable of determining the distance to the scanned subject. Noctovision also cannot determine the coordinates of the subject in three-dimensional space.\nDeath.\nFrom December 1944, Logie Baird lived at 1 Station Road, Bexhill-on-Sea, East Sussex, he later died there on 14 June 1946 after suffering a stroke in February. The house was demolished in 2007 and the site is now occupied by apartments named Baird Court. Logie Baird is buried beside his parents in Helensburgh Cemetery, Argyll, Scotland.\nHonours and portrayals.\nAustralian television's Logie Awards were named in honour of John Logie Baird's contribution to the invention of the television.\nBaird became the only posthumous subject of \"This Is Your Life\" when he was honoured by Eamonn Andrews at the BBC Television Theatre in 1957.\nIn 2014, the Society of Motion Picture and Television Engineers (SMPTE) inducted Logie Baird into The Honor Roll, which \"posthumously recognizes individuals who were not awarded Honorary Membership during their lifetimes but whose contributions would have been sufficient to warrant such an honor\".\nIn 2023, John MacKay portrayed John Logie Baird in both the ITV series \"Nolly\" and the \"Doctor Who\" episode \"The Giggle\".\nLegacy.\nIn 2013, Historic Environment Scotland awarded a plaque to commemorate Logie Baird. It can be found in Helensburgh.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nBooks\nPatents"}
{"id": "39572", "revid": "14423536", "url": "https://en.wikipedia.org/wiki?curid=39572", "title": "Nephiline", "text": ""}
{"id": "39573", "revid": "49747462", "url": "https://en.wikipedia.org/wiki?curid=39573", "title": "Tone (linguistics)", "text": "Use of pitch to distinguish lexical or grammatical meaning\nTone is the use of pitch in language to distinguish lexical or grammatical meaning\u2014that is, to distinguish or to inflect words. All oral languages use pitch to express emotional and other para-linguistic information and to convey emphasis, contrast and other such features in what is called intonation, but not all languages use tones to distinguish words or their inflections, analogously to consonants and vowels. Languages that have this feature are called tonal languages; the distinctive tone patterns of such a language are sometimes called tonemes, by analogy with \"phoneme\". Tonal languages are common in East and Southeast Asia, Africa, the Americas, and the Pacific.\nTonal languages are different from pitch-accent languages in that tonal languages can have each syllable with an independent tone whilst pitch-accent languages may have one syllable in a word or morpheme that is more prominent than the others.\nMechanics.\nMost languages use pitch as intonation to convey prosody and pragmatics, but this does not make them tonal languages. In tonal languages, each syllable has an inherent pitch contour, and thus minimal pairs (or larger minimal sets) exist between syllables with the same segmental features (consonants and vowels) but different tones. Vietnamese and Chinese have heavily studied tone systems, as well as amongst their various dialects.\nBelow is a table of the six Vietnamese tones and their corresponding tone accent or diacritics:\nMandarin Chinese, which has five tones, transcribed by letters with diacritics over vowels:\nThese tones combine with a syllable such as \"ma\" to produce different words. A minimal set based on \"ma\" are, in pinyin transcription:\nThese may be combined into a tongue-twister:\nSimplified: \nTraditional: \nPinyin: \"M\u0101ma m\u00e0 m\u01cede m\u00e1 ma?\"\nIPA \nTranslation: 'Is mom scolding the horse's hemp?'\nSee also one-syllable article.\nA well-known tongue-twister in Standard Thai is:\nIPA: \nTranslation: 'Does new silk burn?'\nA Vietnamese tongue twister:\nIPA: \nTranslation: 'Recently, you've been setting up the seven traps incorrectly.'\nA Cantonese tongue twister:\nJyutping: \"jat1 jan4 jan1 jat1 jat6 jan5 jat1 jan6 jat1 jan3 ji4 jan2\"\nIPA: \nTranslation: 'One person endures a day with one knife and one print.'\nTone is most frequently manifested on vowels, but in most tonal languages where voiced syllabic consonants occur they will bear tone as well. This is especially common with syllabic nasals, for example in many Bantu and Kru languages, but also occurs in Serbo-Croatian. It is also possible for lexically contrastive pitch (or tone) to span entire words or morphemes instead of manifesting on the syllable nucleus (vowels), which is the case in Punjabi.\nTones can interact in complex ways through a process known as tone sandhi.\nPhonation.\nIn a number of East Asian languages, tonal differences are closely intertwined with phonation differences. In Vietnamese, for example, the and tones are both high-rising but the former is distinguished by having glottalization in the middle. Similarly, the and tones are both low-falling, but the tone is shorter and pronounced with creaky voice at the end, while the tone is longer and often has breathy voice. In some languages, such as Burmese, pitch and phonation are so closely intertwined that the two are combined in a single phonological system, where neither can be considered without the other. The distinctions of such systems are termed \"registers\". The \"tone register\" here should not be confused with \"register tone\" described in the next section.\nPhonation type.\nGordon and Ladefoged established a continuum of phonation, where several types can be identified.\nRelationship with tone.\nKuang identified two types of phonation: pitch-dependent and pitch-independent. Contrast of tones has long been thought of as differences in pitch height. However, several studies pointed out that tone is actually multidimensional. Contour, duration, and phonation may all contribute to the differentiation of tones. Investigations from the 2010s using perceptual experiments seem to suggest phonation counts as a perceptual cue.\nTone and pitch accent.\nMany languages use tone in a more limited way. In Japanese, fewer than half of the words have a drop in pitch; words contrast according to which syllable this drop follows. Such minimal systems are sometimes called pitch accent since they are reminiscent of stress accent languages, which typically allow one principal stressed syllable per word. However, there is debate over the definition of pitch accent and whether a coherent definition is even possible.\nTone and intonation.\nBoth lexical or grammatical tone and prosodic intonation are cued by changes in pitch, as well as sometimes by changes in phonation. Lexical tone coexists with intonation, with the lexical changes of pitch like waves superimposed on larger swells. For example, Luksaneeyanawin (1993) describes three intonational patterns in Thai: falling (with semantics of \"finality, closedness, and definiteness\"), rising (\"non-finality, openness and non-definiteness\") and \"convoluted\" (contrariness, conflict and emphasis). The phonetic realization of these intonational patterns superimposed on the five lexical tones of Thai (in citation form) are as follows:\nWith convoluted intonation, it appears that high and falling tone conflate, while the low tone with convoluted intonation has the same contour as rising tone with rising intonation.\nTonal polarity.\nLanguages with simple tone systems or pitch accent may have one or two syllables specified for tone, with the rest of the word taking a default tone. Such languages differ in which tone is marked and which is the default. In Navajo, for example, syllables have a low tone by default, whereas marked syllables have high tone. In the related language Sekani, however, the default is high tone, and marked syllables have low tone. There are parallels with stress: English stressed syllables have a higher pitch than unstressed syllables.\nTypes.\nRegister tones and contour tones.\nIn many Bantu languages, tones are distinguished by their pitch level relative to each other. In multisyllable words, a single tone may be carried by the entire word rather than a different tone on each syllable. Often, grammatical information, such as past versus present, \"I\" versus \"you\", or positive versus negative, is conveyed solely by tone.\nIn the most widely spoken tonal language, Mandarin Chinese, tones are distinguished by their distinctive shape, known as contour, with each tone having a different internal pattern of rising and falling pitch. Many words, especially monosyllabic ones, are differentiated solely by tone. In a multisyllabic word, each syllable often carries its own tone. Unlike in Bantu systems, tone plays little role in the grammar of modern standard Chinese, though the tones descend from features in Old Chinese that had morphological significance (such as changing a verb to a noun or vice versa).\nMost tonal languages have a combination of register and contour tones. Tone is typical of languages including Kra\u2013Dai, Vietic, Sino-Tibetan, Afroasiatic, Khoisan, Niger-Congo and Nilo-Saharan languages. Most tonal languages combine both register and contour tones, such as Cantonese, which produces three varieties of contour tone at three different pitch levels, and the Omotic (Afroasiatic) language Bench, which employs five level tones and one or two rising tones across levels.\nMost varieties of Chinese use contour tones, where the distinguishing feature of the tones are their shifts in pitch (that is, the pitch is a contour), such as rising, falling, dipping, or level. Most Bantu languages (except northwestern Bantu) on the other hand, have simpler tone systems usually with high, low and one or two contour tone (usually in long vowels). In such systems there is a default tone, usually low in a two-tone system or mid in a three-tone system, that is more common and less salient than other tones. There are also languages that combine relative-pitch and contour tones, such as many Kru languages and other Niger-Congo languages of West Africa.\nFalling tones tend to fall further than rising tones rise; high\u2013low tones are common, whereas low\u2013high tones are quite rare. A language with contour tones will also generally have as many or more falling tones than rising tones. However, exceptions are not unheard of; Mpi, for example, has three level and three rising tones, but no falling tones.\nWord tones and syllable tones.\nAnother difference between tonal languages is whether the tones apply independently to each syllable or to the word as a whole. In Cantonese, Thai, and Kru languages, each syllable may have a tone, whereas in Shanghainese, Swedish, Norwegian and many Bantu languages, the contour of each tone operates at the word level. That is, a trisyllabic word in a three-tone syllable-tone language has many more tonal possibilities (3 \u00d7 3 \u00d7 3 = 27) than a monosyllabic word (3), but there is no such difference in a word-tone language. For example, Shanghainese has two contrastive (phonemic) tones no matter how many syllables are in a word. Many languages described as having pitch accent are word-tone languages.\nTone sandhi is an intermediate situation, as tones are carried by individual syllables, but affect each other so that they are not independent of each other. For example, a number of Mandarin Chinese suffixes and grammatical particles have what is called (when describing Mandarin Chinese) a \"neutral\" tone, which has no independent existence. If a syllable with a neutral tone is added to a syllable with a full tone, the pitch contour of the resulting word is entirely determined by that other syllable:\nAfter high level and high rising tones, the neutral syllable has an independent pitch that looks like a mid-register tone\u00a0\u2013 the default tone in most register-tone languages. However, after a falling tone it takes on a low pitch; the contour tone remains on the first syllable, but the pitch of the second syllable matches where the contour leaves off. And after a low-dipping tone, the contour spreads to the second syllable: the contour remains the same () whether the word has one syllable or two. In other words, the tone is now the property of the word, not the syllable. Shanghainese has taken this pattern to its extreme, as the pitches of all syllables are determined by the tone before them, so that only the tone of the initial syllable of a word is distinctive.\nLexical tones and grammatical tones.\nLexical tones are used to distinguish lexical meanings. Grammatical tones, on the other hand, change the grammatical categories. To some authors, the term includes both inflectional and derivational morphology. Tian described a grammatical tone, the \"induced creaky tone\", in Burmese.\nNumber of tones.\nLanguages may distinguish up to five levels of pitch, though the Chori language of Nigeria is described as distinguishing six surface tone registers. Since tone contours may involve up to two shifts in pitch, there are theoretically 5 \u00d7 5 \u00d7 5 = 125 distinct tones for a language with five registers. However, the most that are actually used in a language is a tenth of that number.\nSeveral Kam\u2013Sui languages of southern China have nine contrastive tones, including contour tones. For example, the Kam language has 9 tones: 3 more-or-less fixed tones (high, mid and low); 4 unidirectional tones (high and low rising, high and low falling); and 2 bidirectional tones (dipping and peaking). This assumes that checked syllables are not counted as having additional tones, as they traditionally are in China. For example, in the traditional reckoning, the Kam language has 15 tones, but 6 occur only in syllables closed with the voiceless stop consonants , or and the other 9 occur only in syllables not ending in one of these sounds.\nPreliminary work on the Wobe language (part of the Wee continuum) of Liberia and C\u00f4te d'Ivoire, the Ticuna language of the Amazon and the Chatino languages of southern Mexico suggests that some dialects may distinguish as many as fourteen tones or more. The Guere language, Dan language and Mano language of Liberia and Ivory Coast have around 10 tones, give or take. The Oto-Manguean languages of Mexico have a huge number of tones as well. The most complex tonal systems are actually found in Africa and the Americas, not East Asia.\nTonal change.\nTone terracing.\nTones are realized as pitch only in a relative sense. \"High tone\" and \"low tone\" are only meaningful relative to the speaker's vocal range and in comparing one syllable to the next, rather than as a contrast of absolute pitch such as one finds in music. As a result, when one combines tone with sentence prosody, the absolute pitch of a high tone at the end of a prosodic unit may be lower than that of a low tone at the beginning of the unit, because of the universal tendency (in both tonal and non-tonal languages) for pitch to decrease with time in a process called downdrift.\nTones may affect each other just as consonants and vowels do. In many register-tone languages, low tones may cause a downstep in following high or mid tones; the effect is such that even while the low tones remain at the lower end of the speaker's vocal range (which is itself descending due to downdrift), the high tones drop incrementally like steps in a stairway or terraced rice fields, until finally the tones merge and the system has to be reset. This effect is called tone terracing.\nSometimes a tone may remain as the sole realization of a grammatical particle after the original consonant and vowel disappear, so it can only be heard by its effect on other tones. It may cause downstep, or it may combine with other tones to form contours. These are called floating tones.\nTone sandhi.\nIn many contour-tone languages, one tone may affect the shape of an adjacent tone. The affected tone may become something new, a tone that only occurs in such situations, or it may be changed into a different existing tone. This is called tone sandhi. In Mandarin Chinese, for example, a dipping tone between two other tones is reduced to a simple low tone, which otherwise does not occur in Mandarin Chinese, whereas if two dipping tones occur in a row, the first becomes a rising tone, indistinguishable from other rising tones in the language. For example, the words \u5f88 ('very') and \u597d ('good') produce the phrase \u5f88\u597d ('very good'). The two transcriptions may be conflated with reversed tone letters as .\nRight- and left-dominant sandhi.\nTone sandhi in Sinitic languages can be classified with a left-dominant or right-dominant system. In a language of the right-dominant system, the right-most syllable of a word retains its citation tone (i.e., the tone in its isolation form). All the other syllables of the word must take their sandhi form. Taiwanese Southern Min is known for its complex sandhi system. Example: from \u9e79 \"kiam5\" 'salty', \u9178 \"sng1\" 'sour' and \u751c \"tinn1\" 'sweet' is the word \u9e79\u9178\u751c \"kiam5\u20137 sng1\u20137 tinn1\", also transcribed \"kiam7 sng7 tinn1\" 'candied fruit'. In this example, only the last syllable remains unchanged.\nTone change.\nTone change must be distinguished from tone sandhi. Tone sandhi is a compulsory change that occurs when certain tones are juxtaposed. Tone change, however, is a morphologically conditioned alternation and is used as an inflectional or a derivational strategy. Lien indicated that causative verbs in modern Southern Min are expressed with tonal alternation, and that tonal alternation may come from earlier affixes. Examples: \u9577 tng5 'long' vs. tng2 'grow'; \u65b7 tng7 'break' vs. tng2 'cause to break'. Also, \u6bd2 in Taiwanese Southern Min has two pronunciations: to\u030dk (entering tone) means 'poison' or 'poisonous', while th\u0101u (departing tone) means 'to kill with poison'. The same usage can be found in Min, Yue, and Hakka.\nUses of tone.\nIn East Asia, tone is typically lexical. That is, tone is used to distinguish words which would otherwise be homonyms. This is characteristic of heavily tonal languages such as Chinese, Vietnamese, Thai, and Hmong.\nHowever, in many African languages, especially in the Niger\u2013Congo family, tone can be both lexical and grammatical. In the Kru languages, a combination of these patterns is found: nouns tend to have complex tone systems but are not much affected by grammatical inflections, whereas verbs tend to have simple tone systems, which are inflected to indicate tense and mood, person, and polarity, so that tone may be the only distinguishing feature between \"you went\" and \"I won't go\".\nIn Yoruba, much of the lexical and grammatical information is carried by tone. In languages of West Africa such as Yoruba, people may even communicate with so-called \"talking drums\", which are modulated to imitate the tones of the language, or by whistling the tones of speech.\nNote that tonal languages are not distributed evenly across the same range as non-tonal languages. Instead, the majority of tone languages belong to the Niger-Congo, Sino-Tibetan and Vietic groups, which are then composed by a large majority of tone languages and dominate a single region. Only in limited locations (South Africa, New Guinea, Mexico, Brazil and a few others) do tone languages occur as individual members or small clusters within a non-tone dominated area. In some locations, like Central America, it may represent no more than an incidental effect of which languages were included when one examines the distribution; for groups like Khoi-San in Southern Africa and Papuan languages, whole families of languages possess tonality but simply have relatively few members, and for some North American tone languages, multiple independent origins are suspected.\nIf generally considering only complex-tone vs. no-tone, it might be concluded that tone is almost always an ancient feature within a language family that is highly conserved among members. However, when considered in addition to \"simple\" tone systems that include only two tones, tone, as a whole, appears to be more labile, appearing several times within Indo-European languages, several times in American languages, and several times in Papuan families. That may indicate that rather than a trait unique to some language families, tone is a latent feature of most language families that may more easily arise and disappear as languages change over time.\nA 2015 study by Caleb Everett argued that tonal languages are more common in hot and humid climates, which make them easier to pronounce, even when considering familial relationships. If the conclusions of Everett's work are sound, this is perhaps the first known case of influence of the environment on the structure of the languages spoken in it. The proposed relationship between climate and tone is controversial, and logical and statistical issues have been raised by various scholars.\nTone and inflection.\nTone has long been viewed as a phonological system. It was not until recent years that tone was found to play a role in inflectional morphology. Palancar and L\u00e9onard (2016) provided an example with Tlatepuzco Chinantec (an Oto-Manguean language spoken in Southern Mexico), where tones are able to distinguish mood, person, and number:\nIn Iau language (the most tonally complex Lakes Plain language, predominantly monosyllabic), nouns have an inherent tone (e.g. be\u02e7 'fire' but be\u02e6\u02e7 'flower'), but verbs don't have any inherent tone. For verbs, a tone is used to mark aspect. The first work that mentioned this was published in 1986. Example paradigms:\nTones are used to differentiate cases as well, as in Maasai language (a Nilo-Saharan language spoken in Kenya and Tanzania):\nCertain varieties of Chinese are known to express meaning by means of tone change although further investigations are required. Examples from two Yue dialects spoken in Guangdong Province are shown below. In Taishan, tone change indicates the grammatical number of personal pronouns. In Zhongshan, perfective verbs are marked with tone change.\nThe following table compares the personal pronouns of Sixian dialect (a dialect of Taiwanese Hakka) with Zaiwa and Jingpho (both Tibeto-Burman languages spoken in Yunnan and Burma). From this table, we find the distinction between nominative, genitive, and accusative is marked by tone change and sound alternation.\nPhonetic notation.\nThere are several approaches to notating tones in the description of a language. A fundamental difference is between \"phonemic\" and \"phonetic\" transcription.\nA phonemic notation will typically lack any consideration of the actual phonetic values of the tones. Such notations are especially common when comparing dialects with wildly different phonetic realizations of what are historically the same set of tones. In Chinese, for example, the \"four tones\" may be assigned numbers, such as \u2460 to \u2463 or \u2013 after the historical tone split that affected all Chinese languages to at least some extent \u2013 \u2460 to \u2467 (with odd numbers for the \"yin\" tones and even numbers for the \"yang\"). In traditional Chinese notation, the equivalent diacritics \u27e8\u27e9 are attached to the Chinese character, marking the same distinctions, plus underlined \u27e8\u27e9 for the \"yang\" tones where a split has occurred. If further splits occurred in some language or dialect, the results may be numbered '4a' and '4b' or something similar. Among the Kra-Dai languages, tones are typically assigned the letters A through D, or, after a historical tone split similar to what occurred in Chinese, A1 to D1 and A2 to D2; see Proto-Tai language. With such a system, it can be seen which words in two languages have the same historical tone (say tone \u2462) even though they no longer sound anything alike.\nAlso phonemic are upstep and downstep, which are indicated by the IPA diacritics \u27e8\u27e9 and \u27e8\u27e9, respectively, or by the typographic substitutes \u27e8\u27e9 and \u27e8\u27e9, respectively. Upstep and downstep affect the tones within a language as it is being spoken, typically due to grammatical inflection or when certain tones are brought together. (For example, a high tone may be stepped down when it occurs after a low tone, compared to the pitch it would have after a mid tone or another high tone.)\nPhonetic notation records the actual relative pitch of the tones. Since tones tend to vary over time periods as short as centuries, this means that the historical connections among the tones of two language varieties will generally be lost by such notation, even if they are dialects of the same language.\nThe Chao tone letters have two variants. The left-stem letters, \u27e8\u27e9, are used for tone sandhi. These are especially important for the Min Chinese languages. For example, a word may be pronounced in isolation, but in a compound the tone will shift to . This can be notated morphophonemically as \u27e8\u27e9, where the back-to-front tone letters simultaneously show the underlying tone and the value in this word. Using the local (and internationally ambiguous) non-IPA numbering system, the compound may be written \u27e8\u27e9. Left-stem letters may also be combined to form contour tones. \nThe second Chao letter variant are the dotted tone letters \u27e8\u27e9, which are used to indicate the pitch of neutral tones. These are phonemically null, and may be indicated with the digit '0' in a numbering system, but take specific pitches depending on the preceding phonemic tone. When combined with tone sandhi, the left-stem dotted tone letters \u27e8\u27e9 are seen.\nAn IPA/Chao tone letter will rarely be composed of more than three elements (which are sufficient for peaking and dipping tones). Occasionally, however, peaking\u2013dipping and dipping\u2013peaking tones, which require four elements \u2013 or even double-peaking and double-dipping tones, which require five \u2013 are encountered. This is usually only the case when prosody is superposed on lexical or grammatical tone, but a good computer font will allow an indefinite number of tone letters to be concatenated. The IPA diacritics placed over vowels and other letters have not been extended to this level of complexity.\nAfrica.\nIn African linguistics (as well as in many African orthographies), a set of diacritics is usual to mark tone. The most common are a subset of the International Phonetic Alphabet:\nMinor variations are common. In many three-tone languages, it is usual to mark high and low tone as indicated above but to omit marking of the mid tone: \"m\u00e1\" (high), \"ma\" (mid), \"m\u00e0\" (low). Similarly, in two-tone languages, only one tone may be marked explicitly, usually the less common or more 'marked' tone (see markedness).\nWhen digits are used, typically 1 is high and 5 is low, except in Omotic languages, where 1 is low and 5 or 6 is high. In languages with just two tones, 1 may be high and 2 low, etc.\nAsia.\nIn the Chinese tradition, digits are assigned to various tones (see tone number). For instance, Standard Mandarin Chinese, the official language of China, has four lexically contrastive tones, and the digits 1, 2, 3, and 4 are assigned to four tones. Syllables can sometimes be toneless and are described as having a neutral tone, typically indicated by omitting tone markings. Chinese varieties are traditionally described in terms of four tonal categories \"ping\" ('level'), \"shang\" ('rising'), \"qu\" ('exiting'), \"ru\" ('entering'), based on the traditional analysis of Middle Chinese (see Four tones); note that these are not at all the same as the four tones of modern standard Mandarin Chinese. Depending on the dialect, each of these categories may then be divided into two tones, typically called \"yin\" and \"yang.\" Typically, syllables carrying the \"ru\" tones are closed by voiceless stops in Chinese varieties that have such coda(s) so in such dialects, \"ru\" is not a tonal category in the sense used by Western linguistics but rather a category of syllable structures. Chinese phonologists perceived these checked syllables as having concomitant short tones, justifying them as a tonal category. In Middle Chinese, when the tonal categories were established, the \"shang\" and \"qu\" tones also had characteristic final obstruents with concomitant tonic differences whereas syllables bearing the \"ping\" tone ended in a simple sonorant. An alternative to using the Chinese category names is assigning to each category a digit ranging from 1 to 8, sometimes higher for some Southern Chinese dialects with additional tone splits. Syllables belonging to the same tone category differ drastically in actual phonetic tone across the varieties of Chinese even among dialects of the same group. For example, the \"yin ping\" tone is a high level tone in Beijing Mandarin Chinese but a low level tone in Tianjin Mandarin Chinese.\nMore iconic systems use tone numbers or an equivalent set of graphic pictograms known as \"Chao tone letters\". These divide the pitch into five levels, with the lowest being assigned the value 1 and the highest the value 5. (This is the opposite of equivalent systems in Africa and the Americas.) The variation in pitch of a tone contour is notated as a string of two or three numbers. For instance, the four Mandarin Chinese tones are transcribed as follows (the tone letters will not display properly without a compatible font installed):\nA mid-level tone would be indicated by /33/, a low level tone /11/, etc. The doubling of the number is commonly used with level tones to distinguish them from tone numbers; tone 3 in Mandarin Chinese, for example, is not mid /3/. However, it is not necessary with tone letters, so /33/ = or simply . If a distinction is made, it may be that is mid tone in a register system and is mid level tone in a contour system, or may be mid tone on a short syllable or a mid checked tone, while is mid tone on a long syllable or a mid unchecked tone.\nIPA diacritic notation is also sometimes seen for Chinese. One reason it is not more widespread is that only two contour tones, rising and falling , are widely supported by IPA fonts while several Chinese varieties have more than one rising or falling tone. One common workaround is to retain standard IPA and for high-rising (e.g. ) and high-falling (e.g. ) tones and to use the subscript diacritics and for low-rising (e.g. ) and low-falling (e.g. ) tones.\nNorth America.\nSeveral North American languages have tone, one of which is Cherokee, an Iroquoian language. Oklahoma Cherokee has six tones (1 low, 2 medium, 3 high, 4 very high, 5 rising and 6 falling). The Tanoan languages have tone as well. For instance, Kiowa has three tones (high, low, falling), while Jemez has four (high, mid, low, and falling).\nIn Mesoamericanist linguistics, /1/ stands for high tone and /5/ stands for low tone, except in Oto-Manguean languages for which /1/ may be low tone and /3/ high tone. It is also common to see acute accents for high tone and grave accents for low tone and combinations of these for contour tones. Several popular orthographies use \u27e8j\u27e9 or \u27e8h\u27e9 after a vowel to indicate low tone. The Southern Athabascan languages that include the Navajo and Apache languages are tonal, and are analyzed as having two tones: high and low. One variety of Hopi has developed tone, as has the Cheyenne language.\nTone orthographies.\nIn Roman script orthographies, a number of approaches are used. Diacritics are common, as in pinyin, but they tend to be omitted. Thai uses a combination of redundant consonants and diacritics. Tone letters may also be used, for example in Hmong RPA and several minority languages in China. Tone may simply be ignored, as is possible even for highly tonal languages: for example, the Chinese navy has successfully used toneless pinyin in government telegraph communications for decades. Likewise, Chinese reporters abroad may file their stories in toneless pinyin. Dungan, a variety of Mandarin Chinese spoken in Central Asia, has, since 1927, been written in orthographies that do not indicate tone. Ndjuka, in which tone is less important, ignores tone except for a negative marker. However, the reverse is also true: in the Congo, there have been complaints from readers that newspapers written in orthographies without tone marking are insufficiently legible.\nStandard Central Thai has five tones\u2013mid, low, falling, high and rising\u2013often indicated respectively by the numbers zero, one, two, three and four. The Thai alphabet is an alphasyllabary, which specifies the tone unambiguously. Tone is indicated by an interaction of the initial consonant of a syllable, the vowel length, the final consonant (if present), and sometimes a tone mark. A particular tone mark may denote different tones depending on the initial consonant. The Shan alphabet, derived from the Burmese script, has five tone letters: \u1087, \u1088, \u1038, \u1089, \u108a; a sixth tone is unmarked.\nVietnamese uses the Latin alphabet and its six tones are marked by letters with diacritics above or below a certain vowel. Basic notation for Vietnamese tones are as follows:\nThe Latin-based Hmong and Iu Mien alphabets use full letters for tones. In Hmong, one of the eight tones (the tone) is left unwritten while the other seven are indicated by the letters \"b, m, d, j, v, s, g\" at the end of the syllable. Since Hmong has no phonemic syllable-final consonants, there is no ambiguity. That system enables Hmong speakers to type their language with an ordinary Latin-letter keyboard without having to resort to diacritics. In the Iu Mien, the letters \"v, c, h, x, z\" indicate tones but unlike Hmong, it also has final consonants written before the tone.\nThe Standard Zhuang and Zhuang languages used to use a unique set of six \"tone letters\" based on the shapes of numbers, but slightly modified, to depict what tone a syllable was in. This was replaced in 1982 with the use of normal letters in the same manner, like Hmong.\nThe syllabary of the Nuosu language depicts tone in a unique manner, having separate glyphs for each tone other than for the mid-rising tone, which is denoted by the addition of a diacritic. Take the difference between \ua26c nge [\u014b\u026f\u00b3\u00b3], and \ua26b ngex [\u014b\u026f\u00b3\u2074]. In romanisation, the letters t, x, and p are used to demarcate tone. As codas are forbidden in Nuosu there is no ambiguity.\nOrigin and development.\nAndr\u00e9-Georges Haudricourt established that Vietnamese tone originated in earlier consonantal contrasts and suggested similar mechanisms for Chinese. It is now widely held that Old Chinese did not have phonemically contrastive tone. The historical origin of tone is called tonogenesis, a term coined by James Matisoff.\nTone as an areal feature.\nTone is sometimes an areal rather than a phylogenetic feature. That is to say, a language may acquire tones through bilingualism if influential neighbouring languages are tonal or if speakers of a tonal language shift to the language in question and bring their tones with them. The process is referred to as contact-induced tonogenesis by linguists. In other cases, tone may arise spontaneously and surprisingly fast: the dialect of Cherokee in Oklahoma has tone, but the dialect in North Carolina does not, even though they were only separated in 1838. Hong Kong English is tonal, a result of the contact between non-tonal British English with Hong Kong Cantonese, a tonal language; a similar process of tonogenesis has happened in Singapore English, although under slightly different conditions of linguistic contact, resulting in different tonal outcomes.\nExamples.\nTone arose in the Athabascan languages at least twice, in a patchwork of two systems. In some languages, such as Navajo, syllables with glottalized consonants (including glottal stops) in the syllable coda developed low tones, whereas in others, such as Slavey, they developed high tones, so that the two tonal systems are almost mirror images of each other. Syllables without glottalized codas developed the opposite tone. For example, high tone in Navajo and low tone in Slavey are due to contrast with the tone triggered by the glottalization.\nOther Athabascan languages, namely those in western Alaska (such as Koyukon) and the Pacific coast (such as Hupa), did not develop tone. Thus, the Proto-Athabascan word ' ('water') is toneless ' in Hupa, high-tone ' in Navajo, and low-tone \"t\u00f9\" in Slavey; while Proto-Athabascan ' ('knee') is toneless ' in Hupa, low-tone ' in Navajo, and high-tone \"\" in Slavey. provides a phonetic explanation for the opposite development of tone based on the two different ways of producing glottalized consonants with either tense voice on the preceding vowel, which tends to produce a high fundamental frequency, or creaky voice, which tends to produce a low fundamental frequency. Languages with \"stiff\" glottalized consonants and tense voice developed high tone on the preceding vowel and those with \"slack\" glottalized consonants with creaky voice developed low tone.\nThe Bantu languages also have \"mirror\" tone systems in which the languages in the northwest corner of the Bantu area have the opposite tones of other Bantu languages.\nThree Algonquian languages developed tone independently of one another and of neighboring languages: Cheyenne, Arapaho, and Kickapoo. In Cheyenne, tone arose via vowel contraction; the long vowels of Proto-Algonquian contracted into high-pitched vowels in Cheyenne while the short vowels became low-pitched. In Kickapoo, a vowel with a following [h] acquired a low tone, and this tone later extended to all vowels followed by a fricative. In Afrikaans the glottal fricative also lowers the tone of surrounding vowels.\nIn Mohawk, a glottal stop can disappear in a combination of morphemes, leaving behind a long falling tone. Note that it has the reverse effect of the postulated rising tone in Cantonese or Middle Chinese, derived from a lost final glottal stop.\nIn Korean, a 2013 study which compared voice recordings of Seoul speech from 1935 and 2005 found that in recent years, lenis consonants (\u3142\u3148\u3137\u3131), aspirated consonants (\u314d\u314a\u314c\u314b) and fortis consonants (\u3143\u3149\u3138\u3132) were shifting from a distinction via voice onset time to that of pitch change, and suggests that the modern Seoul dialect is currently undergoing tonogenesis. These sound shifts still show variations among different speakers, suggesting that the transition is still ongoing. Among 141 examined Seoul speakers, these pitch changes were originally initiated by females born in the 1950s, and have almost reached completion in the speech of those born in the 1990s.\nTonogenesis.\nTriggers of tonogenesis.\n\"There is tonogenetic potential in various series of phonemes: glottalized vs. plain consonants, unvoiced vs. voiced, aspirated vs. unaspirated, geminates vs. simple (...), and even among vowels\". Very often, tone arises as an effect of the loss or merger of consonants. In a nontonal language, voiced consonants commonly cause following vowels to be pronounced at a lower pitch than other consonants. That is usually a minor phonetic detail of voicing. However, if consonant voicing is subsequently lost, that incidental pitch difference may be left over to carry the distinction that the voicing previously carried (a process called transphonologization) and thus becomes meaningful (phonemic).\nThis process happened in the Punjabi language: the Punjabi murmured (voiced aspirate) consonants have disappeared and left tone in their wake. If the murmured consonant was at the beginning of a word, it left behind a low tone; at the end, it left behind a high tone. If there was no such consonant, the pitch was unaffected; however, the unaffected words are limited in pitch and did not interfere with the low and high tones. That produced a tone of its own, mid tone. The historical connection is so regular that Punjabi is still written as if it had murmured consonants, and tone is not marked. The written consonants tell the reader which tone to use.\nSimilarly, final fricatives or other consonants may phonetically affect the pitch of preceding vowels, and if they then weaken to and finally disappear completely, the difference in pitch, now a true difference in tone, carries on in their stead. This was the case with Chinese. Two of the three tones of Middle Chinese, the \"rising\" and the \"departing\" tones, arose as the Old Chinese final consonants and disappeared, while syllables that ended with neither of these consonants were interpreted as carrying the third tone, \"even\". Most varieties descending from Middle Chinese were further affected by a tone split in which each tone divided in two depending on whether the initial consonant was voiced. Vowels following a voiced consonant (depressor consonant) acquired a lower tone as the voicing lost its distinctiveness.\nThe same changes affected many other languages in the same area, and at around the same time (AD 1000\u20131500). The tone split, for example, also occurred in Thai and Vietnamese.\nIn general, voiced initial consonants lead to low tones while vowels after aspirated consonants acquire a high tone. When final consonants are lost, a glottal stop tends to leave a preceding vowel with a high or rising tone (although glottalized vowels tend to be low tone so if the glottal stop causes vowel glottalization, that will tend to leave behind a low vowel). A final fricative tends to leave a preceding vowel with a low or falling tone. Vowel phonation also frequently develops into tone, as can be seen in the case of Burmese.\nStages of tonogenesis.\nThe table below shows the process of tonogenesis in White Hmong, described by Martha Ratliff. The tone values described in the table are from Christina Esposito.\nThe table below shows the tonogenesis of the Vietnamese language. The tone values are taken from James Kirby.\nThe table below is the tonogenesis of Tai Dam (Black Tai). Displayed in the first row is Proto-Southern Kra-Dai, as reconstructed by Peter K. Norquest.\nThe table below shows the tonogenesis of the Chinese languages.\nThe tone values are listed below:\nThe tones across all varieties (or dialects) of Chinese correspond to each other, although they may not correspond to each other perfectly. Moreover, listed above are citation tones, but in actual conversations, obligatory sandhi rules will reshape them. The Sixian and Hailu Hakka in Taiwan are famous for their near-regular and opposite pattern (of pitch height). Both will be compared with Standard Chinese below.\nThe table below shows Punjabi tonogenesis in bisyllabic words. Unlike the above four examples, Punjab does not fall under the East Asian tone sprachbund, instead developing phonemic tone separately. In addition, unlike the above languages, which developed tone from syllable-final consonants, Punjabi developed tone from its voiced aspirated stops losing their aspiration. Tone occurs in monosyllabic words as well, but is not discussed in the chart below.\nList of tonal languages.\nAfrica.\nMost languages of Sub-Saharan Africa are members of the Niger-Congo family, which is predominantly tonal; notable exceptions are Swahili (in the southeast), most languages spoken in the Senegambia (among them Wolof, Serer and Cangin languages), and Fulani. The Afroasiatic languages include both tonal (Chadic, Omotic) and nontonal (Semitic, Berber, Egyptian, and most Cushitic) branches. All three Khoisan language families\u2014Khoe, Kx'a and Tuu\u2014are tonal. Most languages of the Nilo-Saharan family are tonal.\nAsia.\nNumerous tonal languages are widely spoken in China and Mainland Southeast Asia. Sino-Tibetan languages (including Meitei-Lon, Burmese, Mog and most varieties of Chinese; though some, such as Shanghainese, are only marginally tonal) and Kra\u2013Dai languages (including Thai and Lao) are mostly tonal. The Hmong\u2013Mien languages are some of the most tonal languages in the world, with as many as twelve phonemically distinct tones. Austronesian and Austroasiatic languages are mostly non-tonal, with a number of exceptions, e.g. Vietnamese (Austroasiatic), C\u00e8muh\u00ee and Yabem (Austronesian). Tones in Vietnamese and Tsat may result from Chinese influence on both languages. There were tones in Middle Korean and a few tones in Japanese. Other languages represented in the region, such as Mongolian and Uyghur, belong to language families that do not contain any tonality as defined here. In South Asia tonal languages are rare, but some Indo-Aryan languages have tonality, including Punjabi, Haryanvi, Hindko, Khariboli, and Dogri, Sylheti, Chittagonian, Rohingya, Chakma as well as the Eastern Bengali dialects.\nAmericas.\nA large number of North, South and Central American languages are tonal, including many of the Athabaskan languages of Alaska and the American Southwest (including Navajo), and the Oto-Manguean languages of Mexico. Among the Mayan languages, which are mostly non-tonal, Yucatec (with the largest number of speakers), Uspantek, and one dialect of Tzotzil have developed tone systems. The Ticuna language of the western Amazon is perhaps the most tonal language of the Americas. Other languages of the western Amazon have fairly simple tone systems as well. However, although tone systems have been recorded for many American languages, little theoretical work has been completed for the characterization of their tone systems. In different cases, Oto-Manguean tone languages in Mexico have been found to possess tone systems similar to both Asian and African tone languages.\nEurope.\nNorwegian and Swedish share tonal language features via the 'Single' and 'Double' tones, which can be marked in phonetic descriptions by either a preceding ' (single tone) or \u17f4 (double tone). The single tone starts low and rises to a high note (). The double tone starts higher than the single tone, falls, and then rises again to a higher pitch than the start (), similar to the Mandarin third tone (as in the word \"n\u01d0\", ).\nExamples in Norwegian: 'b\u00f8nder (farmers) and \u17f4b\u00f8nner (beans) are, apart from the intonation, phonetically identical (despite the spelling difference). Similarly, and with in this case identical spelling, 't\u00f8mmer (timber) and \u17f4t\u00f8mmer (present tense of verb t\u00f8mme \u2013 to empty) are distinguished only through intonation.\nThe Scandinavian tone system is more correctly described as a pitch accent system because it only appears in combination with stress. It became phonemic because the number of syllables in certain words changed since the Old Norse period. A former one-syllable word which developed an additional syllable because of an epenthetic vowel or an added suffix kept its one-syllable pronunciation in contrast with a former two-syllable word that it was otherwise homophonous with. It previously also existed in Danish but has in nearly all forms of Danish developed into st\u00f8d which is a rather a difference in vowel phonation but morphologically also behaves like a pitch accent.\nA pitch accent system also developed within the Balto-Slavic languages and still exists in Lithuanian, Latvian (with one tone resembling the Danish st\u00f8d), Slovenian and Serbo-Croatian.\nAccording to Watson, Scouse contrasts certain tones, and some forms of Rhineland German can also be described as having a pitch accent system.\nSummary.\nLanguages that are tonal include:\nIn some cases, it is difficult to determine whether a language is tonal. For example, the Ket language of Siberia has been described as having up to eight tones by some investigators, as having four tones by others, but by some as having no tone at all. In cases such as these, the classification of a language as tonal may depend on the researcher's interpretation of what tone is. For instance, the Burmese language has phonetic tone, but each of its three tones is accompanied by a distinctive phonation (creaky, murmured or plain vowels). It could be argued either that the tone is incidental to the phonation, in which case Burmese would not be phonemically tonal, or that the phonation is incidental to the tone, in which case it would be considered tonal. Something similar appears to be the case with Ket.\nThe 19th-century constructed language Solresol can consist of only tone, but unlike all natural tonal languages, Solresol's tone is absolute, rather than relative, and no tone sandhi occurs.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39574", "revid": "50756391", "url": "https://en.wikipedia.org/wiki?curid=39574", "title": "History of Antigua and Barbuda", "text": "The history of Antigua and Barbuda covers the period from the arrival of the Archaic peoples thousands of years ago to the present day. Prior to European colonization, the lands encompassing present-day Antigua and Barbuda were inhabited by three successive Amerindian societies. The island was claimed by England, who colonized the islands in 1632. Under English/British control, the islands witnessed an influx of both Britons and African slaves to the island. In 1981, the islands were granted independence as the modern state of Antigua and Barbuda.\nEarly history (2900 BC\u201317th century).\nAntigua was first settled by pre-agricultural Amerindians known as \"Archaic People\" (although they are commonly, but erroneously known in Antigua as Siboney, a pre-ceramic Cuban people). The earliest settlements on the island date to 2900\u00a0BC. They were succeeded by ceramic-using agriculturalist Saladoid people who migrated up the island chain from Venezuela. They were later replaced by Arawakan speakers around 1200\u00a0AD and around 1500 by Island Caribs.\nThe Arawaks were the first well-documented group of Antiguans. They paddled to the island by canoe (piragua) from Venezuela, and were ejected by the Caribs\u2014another people Indigenous to the area. Arawaks introduced agriculture to Antigua and Barbuda, raising, among other crops, the famous Antiguan \"black\" pineapple. They also cultivated various other foods including corn, sweet potatoes (white variety), guava, tobacco, and cotton. Some of the vegetables listed, such as corn and sweet potatoes, still play an important role in Antiguan cuisine. For example, a popular Antiguan dish, Ducuna (DOO-koo-NAH), is a sweet, steamed dumpling made from grated sweet potatoes, flour and spices. In addition, one of the Antiguan staple foods, fungee (FOON-ji), is a cooked paste made of cornmeal and water.\nThe bulk of the Arawaks left Antigua about 1100\u00a0AD. Those who remained were subsequently raided by the Caribs. According to the Catholic Encyclopedia, the Caribs' superior weapons and seafaring prowess allowed them to defeat most Arawak nations in the West Indies\u2014enslaving some and cannibalising others.\nThe Catholic Encyclopedia does make it clear that the Spanish explorers had some difficulty identifying and differentiating between the various native peoples they encountered. As a result, the number and types of ethnic-tribal-national groups in existence at the time may be much more varied and numerous than the two mentioned.\nAccording to \"A Brief History of the Caribbean\" (Jan Rogozinski, Penguin Putnam, Inc September 2000), European and African diseases, malnutrition and slavery eventually destroyed the vast majority of the Caribbean's native population. No researcher has conclusively proven any of these causes as the real reason for the destruction of West Indian natives. Some historians believe that the psychological stress of slavery may also have played a part in the massive number of native deaths while in servitude. Others believe that the reportedly abundant, but starchy, low-protein diet may have contributed to severe malnutrition of the \"Indians\" who were used to a diet fortified with protein from sea-life.\nThe Indigenous West Indians made sea vessels that they used to sail the Atlantic and Caribbean. As a result, Caribs and Arawaks populated much of South American and the Caribbean Islands. Relatives of the Antiguan Arawaks and Caribs still live in various countries in South America, notably Brazil, Venezuela and Colombia. The smaller remaining native populations in the West Indies maintain a pride in their heritage.\nBritish rule (1632\u20131981).\nChristopher Columbus sighted islands in 1493 during his second voyage naming the larger one \"Santa Maria de la Antigua\". However, early attempts by Europeans to settle the islands failed due to the Caribs' excellent defenses. England succeeded in colonising the islands in 1632, with Thomas Warner as the first governor. Settlers used slave labor to raise tobacco, indigo, ginger and sugarcane as cash crops. Sir Christopher Codrington established the first large sugar estate in Antigua in 1674, and leased Barbuda to raise provisions for his plantations. Barbuda's only town is named after him. In the fifty years after Codrington established his initial plantation, the sugar industry became so profitable that many farmers replaced other crops with sugar, making it the economic backbone of the islands.\nSlavery was common in Barbuda in the 1700s and until 1834. The island was a source of slaves for other locations, too. No new slaves had arrived on the island since the mid-1700s but their population grew naturally. An estimate in 1977 by Lowenthal and Clark indicated that during 1779 to 1834 the number of slaves exported totalled 172; most were taken to Antigua but 37 went to the Leeward and Windward islands and some to the southern US. Several slave rebellions took place on the island, with the most serious in 1834\u20135. Britain emancipated slaves in most of its colonies in 1834, but that did not include Barbuda, so the island then freed its own slaves. For some years thereafter, the freed slaves had little opportunity of survival on their own because of limited agricultural land and the lack of available credit to buy some. Hence, they continued to work on the plantations for nominal wages or lived in shantytowns and worked as occasional labourers. Sugar cane production remained the primary economy for over a century.\nDuring the 18th century, Antigua was used as the headquarters of the British Royal Navy Caribbean fleet. English Harbour Dockyard, as it came to be called, a sheltered and well-protected deepwater port, was the main base and facilities there were greatly expanded during the later 18th century. Admiral Horatio Nelson commanded the British fleet for much of this time, and made himself unpopular with local merchants by enforcing the Navigation Act, a British ruling that only British-registered ships could trade with British colonies. As the United States were no longer British colonies, the act posed a problem for merchants, who depended on trade with the fledgling country.\nAs the main cash crop changed over the years, the main cash crops/products grown between 1953 and 1956 were cotton, sugar, meat, cereals, and local fruits and vegetables. Over time, the importance of crops and produce went into decline as other nations were able to sell goods at a price no longer feasible to sustain in the Antiguan economy. In more recent times, however, Antigua has developed a primarily service-based economy relying on tourism as their leading source of income. Much like other islands and nations that rely on tourism, this can become problematic as their success depends on the willingness of others to travel and explore the area. Moreover, this has tendency to follow a seasonal pattern leaving the country vulnerable at certain times in the year.\nPolitical development.\nAlong with most colonies of the British Empire, all slaves in Antigua were emancipated in 1833, but remained economically dependent upon the island's white plantation owners. Economic opportunities for the freed population were limited by a lack of surplus farming land, no access to credit and an economy built on agriculture rather than manufacturing. Poor labour conditions persisted until 1939 when a member of a British Crown commission urged the formation of a trade union movement.\nThe Antigua Trades and Labour Union, formed shortly afterward, became the political vehicle for Sir Vere Cornwall Bird, who became the union's president in 1943. The Antigua Labour Party (ALP), formed by Bird and other trade unionists, first ran candidates in the 1946 elections and became the majority party in 1951, beginning a long history of electoral victories. Voted out of office in the 1971 general elections that swept the progressive labour movement into power, Bird and the ALP returned to office in 1976.\nUntil 1958, Antigua and Barbuda were part of the British Leeward Islands. From 1958 to 1962, Antigua and Barbuda were part of the West Indies Federation.\nSocial class and ethnic composition.\nThe development of social class of Antigua and Barbuda primarily occurred during the colonial era, where the immigration of British colonists (and subsequent importation of African slaves) created a strict hierarchy based both on race and class; Antigua and Barbuda has been described as \"a classic case of the superimposition of race on class and vice versa.\" Both before and after the abolition of slavery in 1833, the two islands were dominated by a small minority of white plantation owners who constituted the colonial upper class. Beneath them were the Afro-Caribbean population, who \"constituted the subordinate working class.\" In between these two groups were several middlemen minorities: free people of color, along with Portuguese and Syrian immigrants, who dominated the professions of law, medicine, and architecture \"and the white-collar positions in banks, businesses, and the civil service.\"\nBetween 1847 and 1852, 2,500 Portuguese immigrants from the island of Madeira emigrated to Antigua due to a severe famine. There, they established numerous small businesses and quickly joined the ranks of the colonial middle class, which up until then had been dominated by the island's mulatto population. As noted by historian Jo-Anne Ferreira, following \"the abolition of slavery, post-abolition migration became a matter of economic survival for many plantation owners, because of the impending labor problems. There was an increasing interest in and desire for European labor, so the Portuguese, among others, were imported throughout the West Indies to increase the European population \"vis-\u00e0-vis\" the African population.\" In contrast to the Portuguese, Syrian immigrants to Antigua and Barbuda did not start arriving until the 1950s, and \"are primarily involved in the import business and have managed to establish themselves in academic professions.\" As of 2008, there were approximately 475 to 500 permanent residents of Antigua and Barbuda who are of Syrian descent.\nThe Irish first came to Antigua either as indentured servants or merchants; Irish indentured servants were primarily transported to Antigua during the Cromwellian conquest of Ireland. As increasing numbers of African slaves were transported to Antigua, the island's Irish population began to leave in search of opportunities in the rest of the British West Indies or in Britain's North American colonies. Numerous Irish merchants in Antigua belonged to business families from County Galway, and several Irish-Antiguans formed relationships with Irish servants in Montserrat.\nThe Afro-Caribbean inhabitants of Antigua and Barbuda, who \"account for about 91% of the country\u2019s population\", are primarily descended from African slaves who were transported from West and Central Africa during the slave trade, in regions such as the Bight of Biafra, the Gold Coast, Sierra Leone, the Gulf of Guinea, the Bight of Benin, and Senegambia. 4.4% of the Black Antiguan and Barbudan population are mixed-raced.\nIndependent Antigua and Barbuda (1981\u2013present).\nThe islands gained independence from the United Kingdom in 1981, becoming the nation of Antigua and Barbuda. The country became part of the Commonwealth of Nations, and a constitutional monarchy, with the first Queen of Antigua and Barbuda being Elizabeth II. The monarch is represented in the country by the governor-general of Antigua and Barbuda.\nIn 1997, Prime Minister Lester Bird announced that a group of ecologically sensitive islands just off Antigua's northeastern coast, previously proposed for national park status, were being turned over to Malaysian developers. The Guiana Island Development Project deal, calling for a 1,000-room hotel, an 18-hole golf course and a world-class casino, sparked widespread criticism by environmentalists, minority members in parliament and the press. The issue came to a head when a local resident shot the PM's brother. Today, the proposed development is mired in lawsuits and politics.\nThe Antigua and Barbuda Labour Party (ABLP) won renewed mandates in the general elections in 1984 and 1989. In the 1989 elections, the ruling ABLP won all but two of the 17 seats. During elections in March 1994, power passed from Vere Bird to his son, Lester Bird, but remained within the ABLP which won 11 of the 17 parliamentary seats. The United Progressive Party won the 2004 elections and Baldwin Spencer became Prime Minister, removing from power the longest-serving elected government in the Caribbean. Baldwin was the Prime Minister of Antigua and Barbuda from 2004 to 2014.\nIn 2014 the Antigua and Barbuda Labour Party regained power from a massive win with the leader being the \"World Boss\", Gaston A. Browne. A snap election was called three years later, and the Antigua and Barbuda Labour Party led by the incumbent Prime Minister Hon. Gaston Browne dominated the elections with a landslide victory of 15-1-1. General elections were held in Antigua and Barbuda on 18 January 2023 to elect members of the House of Representatives. The Labour Party (ABLP) has held an absolute majority of 15 seats in the House of Representatives after the 2018 general election, with Gaston Browne remaining as prime minister. Browne initiated a constitutional referendum after the 2018 election, which was rejected by voters, and following the death of Elizabeth II in 2022, he announced his intention to hold a referendum for the country's transition to a republican system. Besides ABLP, the United Progressive Party (UPP), Democratic National Alliance, Barbuda People's Movement (BPM), and three independent politicians filed candidacies for the 2023 general election.\nDuring the election campaign, UPP proposed to raise the minimum wage and expressed support for small businesses, while ABLP pledged to construct more homes and open two polyclinics. ABLP retained its majority in the House of Representatives, although it won a reduced 9 seats, while UPP won 6 seats. Trevor Walker, the leader of the BPM, retained his seat in Barbuda, while Asot Michael, an independent politician and former member of ABLP, won his seat in the St. Peter constituency. Browne was sworn in for his third consecutive term as prime minister a day after the election.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39576", "revid": "1719098", "url": "https://en.wikipedia.org/wiki?curid=39576", "title": "Alpine skiing", "text": "Sport of skiing downhill\nAlpine skiing, or downhill skiing, is the pastime of sliding down snow-covered slopes on skis with fixed-heel bindings, unlike other types of skiing (cross-country, Telemark, or ski jumping), which use skis with free-heel bindings. Whether for recreation or for sport, it is typically practiced at ski resorts, which provide such services as ski lifts, artificial snow making, snow grooming, restaurants, and ski\u00a0patrol.\n\"Off-piste\" skiers\u2014those skiing outside ski area boundaries\u2014may employ snowmobiles, helicopters or snowcats to deliver them to the top of a slope. Back-country skiers may use specialized equipment with a free-heel mode, including 'sticky' skins on the bottoms of the skis to stop them sliding backwards during an ascent, then locking the heel and removing the skins for their descent.\nAlpine ski racing has been held at the Winter Olympics since 1936. A competition corresponding to modern slalom was introduced in Norway at Oslo in\u00a01886.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nParticipants and venues.\nAs of 2023, there were estimated to be 55 million people worldwide who engaged in alpine skiing. The estimated number of skiers, who practiced alpine, cross-country skiing, and related snow sports, amounted to 30 million in Europe, 20 million in North America, and 14 million in Japan. As of 1996, there were reportedly 4,500 ski areas, operating 26,000 ski lifts and enjoying skier visits. The predominant region for downhill skiing was Europe, followed by Japan and the United States.\nHistory.\nThe ancient origins of skiing can be traced back to prehistoric times in Russia, Finland, Sweden and Norway where varying sizes and shapes of wooden planks were found preserved in peat bogs. The word \"ski\" is related to the Old Norse word , which means \"split piece of wood or firewood.\" Skis were first invented to cross wetlands and marshes in the winter when they froze over. Skiing was an integral part of transportation in colder countries for thousands of years. In the 1760s, skiing was recorded as being used in military training. The Norwegian army held skill competitions involving skiing down slopes, around trees and obstacles while shooting. The birth of modern alpine skiing is often dated to the 1850s, and during the late 19th century, skiing was adapted from a method of transportation to a competitive and recreational sport. Norwegian legend Sondre Norheim first began the trend of skis with curved sides, and bindings with stiff heel bands made of willow. Norheim also introduced the slalom turn style. The wooden skis designed by Norheim closely resemble the shape of modern slalom skis. Norheim was the champion of the first downhill skiing competition, reportedly held in Oslo, Norway in 1868. Norheim impressed spectators when he used the stem christie in Christiania (Oslo) in 1868, the technique was originally called \"christiania turn\" (norwegian: \"christianiasving\" or \"kristianiasving\") after the city (first printed in 1901 in guidelines for ski jumping). The telemark turn was the alternative technique. The christiania turn later developed into parallel turn as the standard technique in alpine skiing.\nThe term \"slalom\" is from Norwegian dialects \"slal\u00e5m\" meaning a trail (\"l\u00e5m\") on a slope (\"sla\"). In Telemark in the 1800s, the steeper and more difficult trails were called \"ville l\u00e5mir\" (wild trails). Skiing competitions in Telemark often began on a steep mountain, continued along a logging-slides (\"t\u00f8mmerslepe\") and were completed with a sharp turn (\"Telemark turn\") on a field or frozen lake. This type of competition used the natural and typical terrain in Telemark. Some races were on \"bumpy courses\" (\"kneikel\u00e5m\") and sometimes included \"steep jumps\" (\"spr\u00f8ytehopp\") for difficulty. The first known slalom competitions were presumably held in Telemark around 1870 in conjunction with ski jumping competitions, involving the same athletes and on slopes next to the ski jump. Husebyrennet from 1886 included \"svingrenn\" (turning competition on hills), the term \"slal\u00e5m\" had not been introduced at that time. \"Slalom\" was first used at a skiing competition in Sonnenberg in 1906. Two to three decades later, the sport spread to the rest of Europe and the US. The first slalom ski competition occurred in M\u00fcrren, Switzerland in 1922.\nTechnique.\nA skier following the fall line will reach the maximum possible speed for that slope. A skier with skis pointed perpendicular to the fall line, across the hill instead of down it, will accelerate more slowly. The speed of descent down any given hill can be controlled by changing the angle of motion in relation to the fall line, skiing across the hill rather than down it.\nDownhill skiing technique focuses on the use of turns to smoothly turn the skis from one direction to another. Additionally, the skier can use the same techniques to turn the ski away from the direction of movement, generating skidding forces between the skis and snow which further slow the descent. Good technique results in a fluid flowing motion from one descent angle to another one, adjusting the angle as needed to match changes in the steepness of the run. This looks more like a single series of S's than turns followed by straight sections.\nStemming.\nThe oldest and still common type of turn on skis is the stem, angling the tail of the ski off to the side, while the tips remain close together. In doing so, the snow resists passage of the stemmed ski, creating a force that retards downhill speed and sustains a turn in the opposite direction. When both skis are stemmed, there is no net turning force, only retardation of downhill speed.\nCarving.\nCarving is based on the shape of the ski itself; when the ski is rolled onto its edge, the shape cut into its side (also known as a sidecut) causes it to bend into an arc. The contact between the arc of the ski edges and the snow naturally causes the ski to tend to move along that arc, changing the skiers direction of motion.\nChecking.\nThis is an advanced form of speed control by increasing the pressure on one inside edge (for example the right ski), then releasing the pressure and shifting immediately to increasing the other inside edge (the left ski). Then repeat if necessary. Each increased pressure slows the speed. Alternating right and left allows the skis to remain parallel and point ahead without turning. The increase and release sequence results in the up and down motions of the upper body. Some skiers go down the top of moguls and control the speed by checking at the tops. This is how they can practically go straight down the fall line without gaining speed.\nSnowplough turn.\nThe snowplough turn is the simplest form of turning and is usually learned by beginners. To perform the snowplough turn one must be in the snowplough position while going down the ski slope. While doing this they apply more pressure to the inside of the opposite foot of which the direction they would like to turn. This type of turn allows the skier to keep a controlled speed and introduces the idea of turning across the fall line.\u00a0\nEquipment.\nSkis.\nModern alpine skis are shaped to enable carve turning, and have evolved significantly since the 1980s. Variants include powder skis, freestyle skis, all-mountain skis, backcountry skis, race skis, and children's skis. Powder skis are usually used when there is a large amount of fresh snow; the shape of a powder ski is wide, allowing the ski to float on top of the snow, compared to a normal downhill ski which would most likely sink into the snow. Freestyle skis are used by skiers who ski terrain parks. These skis are meant to help a skier who skis jumps, rails, and other features placed throughout the terrain park. Freestyle skis are usually fully symmetric, meaning they are the same dimensions from the tip of the ski to the backside (tail) of the ski. All-mountain skis are the most common type of ski, and tend to be used as a typical alpine ski. All-mountain skis are built to do a little bit of everything; they can be used in fresh snow (powder) or used when skiing groomed runs. Slalom race skis, usually referred to as race skis, are short, narrow skis, which tend to be stiffer because they are meant for those who want to go fast as well as make quick sharp turns.\nBindings.\nThe binding is a device used to connect the skier's boot to the ski. The purpose of the binding is to allow the skier to stay connected to the ski, but if the skier falls the binding can safely release them from the ski to prevent injury. There are two types of bindings: the heel and toe system (step-in) and the plate system binding.\nBoots.\nSki boots are one of the most important accessories to skiing. They connect the skier to the skis, allowing them full control over the ski. When ski boots first came about they were made of leather and laces were used. The leather ski boots started off as low-cut, but gradually became taller, allowing for more ankle support, as injuries became more common . Eventually the tied laces were replaced with buckles and the leather boots were replaced with plastic. This allowed the bindings to be more closely matched to the fit of the boot, and offer improved performance. The new plastic model contained two parts of the boots: an inner boot and an outer shell. The inner part of the boot (also called the liner) is the cushioning part of the boot and contains a footbed along with a cushion to keep a skier's foot warm and comfortable. The outer shell is the part of the boot that is made of plastic and contains the buckles. Most ski boots contain a strap at shin level to allow for extra strength when tightening the boots.\nPoles.\nSki poles, one in each hand, are used for balance and propulsion.\nHelmet.\nSki helmets reduce the chances of head injury while skiing. Ski helmets also help to provide warmth to the head since they incorporate an inner liner that traps warmth. Helmets are available in many styles, and typically consist of a hard plastic/resin shell with inner padding. Modern ski helmets may include many additional features such as vents, earmuffs, headphones, goggle mounts, and camera mounts.\nProtective gear.\nThe protective gear used in alpine skiing includes: helmets, mouth guards, shin guards, chin guards, arm guards, back protectors, pole guards, and padding. Mouth guards can reduce the effects of a concussion and protect the teeth of the athlete. Shin guards, pole guards, arm guards and chin guards are mainly used in slalom skiing in order to protect the body parts having impact with the gates. Back protectors and padding, also known as stealth, is worn for giant slalom and other speed events in order to better protect the body if an athlete were to have an accident at high speeds.\nCompetition.\nElite competitive skiers participate in the FIS World Cup, the World Championships, and the Winter Olympics. Broadly speaking, competitive skiing is divided into two disciplines:\nOther disciplines administered by the FIS but not usually considered part of alpine are speed skiing and grass skiing.\nThe triple crown of alpine skiing consists of winning all three World Cup titles in one season or all three Gold medals at the Winter Olympic Games in Slalom, Giant slalom, and Downhill skiing events. Only two people have ever accomplished the feat:\nSki trail ratings.\nIn most ski resorts, the runs are graded according to comparative difficulty so that skiers can select appropriate routes. The grading schemes around the world are related, although with significant regional variations. A beginner-rated trail at a large mountain may be more of an intermediate-rated trail on a smaller mountain.\nIn the United States and Canada, there are four rating symbols: Easy (green circle), Intermediate (blue square), and Difficult (black diamond), and Experts Only (double black diamond) Ski trail difficulty is measured by percent slope, not degree angle. A 100% slope is a 45-degree angle. In general, beginner slopes (green circle) are between 6% and 25%. Intermediate slopes (blue square) are between 25% and 40%. Difficult slopes (black diamond) are 40% and up. Although slope gradient is the primary consideration in assigning a trail difficulty rating, other factors come into play. A trail will be rated by its most difficult part, even if the rest of the trail is easy. Ski resorts assign ratings to their own trails, rating a trail compared only with other trails at that resort. Also considered are width of the trail, sharpest turns, terrain roughness, and whether the resort regularly grooms the trail.\nSafety.\nIn 2014, there were more than 114,000 alpine skiing-related injuries treated in hospitals, doctor's offices, and emergency rooms.\nThe most common types of ski injuries are those of the knee, head, neck and shoulder area, hands and back. Ski helmets are highly recommended by professionals as well as doctors. Head injuries caused in skiing can lead to death or permanent brain damage.\u00a0\u00a0In alpine skiing, for every 1000 people skiing in a day, on average between two and four will require medical attention. Most accidents are the result of user error leading to an isolated fall. Learning how to fall correctly and safely can reduce the risk of injury.\nHealth.\nAccording to a 2004 Harvard Medical School study, alpine skiing burns between 360 and 532 calories per hour.\nClimate change.\nWinter season lengths are projected to decline at ski areas across North America and Europe due to the effects of global warming. In the United States, winter season lengths are projected to decline by more than 50 percent by 2050 and by 80 percent by 2090 if greenhouse gas emissions continue at current rates. About half of the 103 ski resorts in the Northeastern United States operating in 2012 may not be able to maintain an economically viable ski season by 2050. In Europe, half of the glacial ice in the Alps has melted and the European Geosciences Union projects snowpack in the mountains could decline 70 percent by 2100 (however, if humans manage to keep global warming below 2\u00a0\u00b0C, the snow-cover reduction would be limited to 30 percent by 2100).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39577", "revid": "3444830", "url": "https://en.wikipedia.org/wiki?curid=39577", "title": "Ice dance", "text": "Discipline of figure skating that draws from ballroom dancing\nIce dance (sometimes referred to as ice dancing) is a discipline of figure skating that historically draws from ballroom dancing. It joined the World Figure Skating Championships in 1952, and became a Winter Olympic Games medal sport in 1976. According to the International Skating Union (ISU), the governing body of figure skating, an ice dance team consists of one woman and one man.\nIce dance, like pair skating, has its roots in the \"combined skating\" developed in the 19th century by skating clubs and organizations and in recreational social skating. Couples and friends would skate waltzes, marches, and other social dances. The first steps in ice dance were similar to those used in ballroom dancing. In the late 1800s, American Jackson Haines, known as \"the Father of Figure Skating\", brought his style of skating, which included waltz steps and social dances, to Europe. By the end of the 19th century, waltzing competitions on the ice became popular throughout the world. By the early 1900s, ice dance was popular around the world and was primarily a recreational sport, although during the 1920s, local skating clubs in Britain and the U.S. conducted informal dance contests. Recreational skating became more popular during the 1930s in England.\nThe first national competitions occurred in England, Canada, the U.S., and Austria during the 1930s. The first international ice dance competition took place as a special event at the World Championships in 1950 in London. British ice dance teams dominated the sport throughout the 1950s and 1960s, then Soviet teams up until the 1990s. Ice dance was formally added to the 1952 World Figure Skating Championships; it became an Olympic sport in 1976. In the 1980s and 1990s, there was an attempt by ice dancers, their coaches, and choreographers to move ice dance away from its ballroom origins to more theatrical performances. The ISU pushed back by tightening rules and definitions of ice dance to emphasize its connection to ballroom dancing. In the late 1990s and early 2000s, ice dance lost much of its integrity as a sport after a series of judging scandals, which also affected the other figure skating disciplines. There were calls to suspend the sport for a year to deal with the dispute, which seemed to affect ice dance teams from North America the most. Teams from North America began to dominate the sport starting in the early 2000s.\nBefore the 2010\u201311 figure skating season, there were three segments in ice dance competitions: the compulsory dance (CD), the original dance (OD), and the free dance (FD). In 2010, the ISU voted to change the competition format by eliminating the CD and the OD and adding the new short dance (SD) segment to the competition schedule. In 2018, the ISU voted to rename the short dance to the rhythm dance (RD).\nIce dance has required elements that competitors must perform and that make up a well-balanced ice dance program. They include the dance lift, the dance spin, the step sequence, twizzles, and choreographic elements. These must be performed in specific ways, as described in published communications by the ISU, unless otherwise specified. Each year the ISU publishes a list specifying the points that can be deducted from performance scores for various reasons, including falls, interruptions, and violations of the rules concerning time, music, and clothing.\nHistory.\nBeginnings.\nIce dance, like pair skating, has its roots in the \"combined skating\" developed in the 19th century by skating clubs and organizations and in recreational social skating. Couples and friends would skate waltzes, marches, and other social dances together. According to writer Ellyn Kestnbaum, ice dance began with late 19th-century attempts by the Viennese and British to create ballroom-style performances on ice skates. However, figure skating historian James Hines argues that ice dance had its beginnings in hand-in-hand skating, a short-lived but popular discipline of figure skating in England in the 1890s; many of the positions used in modern ice dance can be traced back to hand-in-hand skating. The first steps in ice dance were similar to those used in ballroom dancing, so unlike modern ice dance, skaters tended to keep both feet on the ice most of the time, without the \"long and flowing edges associated with graceful figure skating\".\nIn the late 1800s, American Jackson Haines, known as \"the Father of Figure Skating\", brought his style of skating to Europe. He taught people in Vienna how to dance on the ice, both singly and with partners. Capitalizing on the popularity of the waltz in Vienna, Haines introduced the American waltz, a simple four-step sequence, each step lasting one beat of music, repeated as the partners moved in a circular pattern. By the 1880s, it and the Jackson Haines waltz, a variation of the American waltz, were among the most popular ice dances. Other popular ice dance steps included the mazurka, a version of the Jackson Haines waltz developed in Sweden, and the three-step waltz, which Hines considers \"the direct predecessor of ice dancing in the modern sense\". The three-step waltz, which was done around the perimeter of the ice rink, was first skated in 1894 in Paris and within a few years became a craze throughout Europe.\nBy the end of the 19th\u00a0century, the three-step waltz, called the English waltz in Europe, became the standard for waltzing competitions. It was first skated in Paris in 1894; Hines states that it was responsible for the popularity of ice dance in Europe. The three-step waltz was easy and could be done by less skilled skaters, although more experienced skaters added variations to make it more difficult. Two other steps, the killian and the ten-step, survived into the 20th\u00a0century. The ten-step, which became the fourteen-step, was first skated by Franz Sch\u00f6ller in 1889. Also in the 1890s, combined and hand-in-hand skating moved skating away from basic figures to the continuous movement of ice dancers around an ice rink. Hines insists that the popularity of skating waltzes, which depended upon the speed and flow across the ice of couples in dance positions and not just on holding hands with a partner, ended the popularity of hand-in-hand skating. Hines writes that Vienna was \"the dancing capital of Europe, both on and off skates\" during the 19th\u00a0century; by the end of the century, waltzing competitions became popular throughout the world. The killian, first skated in 1909 by Austrian Karl Schreiter, was the last ice dance invented before World War I still being done as of the 21st\u00a0century.\nEarly years.\nBy the early 1900s, ice dance was popular around the world and was primarily a recreational sport, although during the 1920s, local clubs in Britain and the U.S. conducted informal dance contests in the ten-step, the fourteen-step, and the killian, which were the only three dances used in competition until the 1930s. Recreational skating became more popular during the 1930s in England, and new and more difficult set-pattern dances, which later were used in compulsory dances during competitions, were developed. According to Hines, the development of new ice dances was necessary to expand upon the three dances already developed; three British teams in the 1930s\u2014Erik van der Wyden and Eva Keats, Reginald Wilkie and Daphne B. Wallis, and Robert Dench and Rosemarie Stewart\u2014created one-fourth of the dances used in International Skating Union (ISU) competitions by 2006. In 1933, the Westminster Skating Club conducted a competition encouraging the creation of new dances. Beginning in the mid-1930s, national organizations began to introduce skating proficiency tests in set-pattern dances, improve the judging of dance tests, and oversee competitions. The first national competitions occurred in England in 1934, Canada in 1935, the U.S. in 1936, and Austria in 1937. These competitions included one or more compulsory dances, the original dance, and the free dance. By the late 1930s, ice dancers swelled memberships in skating clubs throughout the world, and in Hines' words \"became the backbone of skating clubs\".\nThe ISU began to develop rules, standards, and international tests for ice dance in the 1950s. The first international ice dance competition occurred as a special event during the 1950 World Figure Skating Championships in London; Lois Waring and Michael McGean of the U.S. won the event, much to the embarrassment of the British, who considered themselves the best ice dancers in the world. A second event was planned the following year, at the 1951 World Championships in Milan; Jean Westwood and Lawrence Demmy of Great Britain came in first place. Ice dance, with the CD and FD segments, was formally added to the World Championships in 1952. Westwood and Demmy won that year, and went on to dominate ice dance, winning the next four World Championships as well. British teams won every world ice dance title through 1960. Eva Romanova and Pavel Roman of Czechoslovakia were the first non-British ice dancers to win a world title, in 1962.\n1970s to 1990s.\nIce dance became an Olympic sport in 1976; Lyudmila Pakhomova and Alexandr Gorshkov from the Soviet Union were the first gold medalists. The Soviets dominated ice dance during most of the 1970s, as they did in pair skating. They won every Worlds and Olympic title between 1970 and 1978, and won medals at every competition between 1976 and 1982. In 1984, British dancers Jayne Torvill and Christopher Dean, who Hines calls \"the greatest ice dancers in the history of the sport\", briefly interrupted Soviet domination of ice dance by winning a gold medal at the Olympic Games in Sarajevo. Their free dance to Ravel's \"Bol\u00e9ro\" has been called \"probably the most well known single program in the history of ice dance\". Hines asserts that Torvill and Dean, with their innovative choreography, dramatically altered \"established concepts of ice dancing\".\nDuring the 1970s, there was a movement in ice dance away from its ballroom roots to a more theatrical style. The top Soviet teams were the first to emphasize the dramatic aspects of ice dance, as well as the first to choreograph their programs around a central theme. They also incorporated elements of ballet techniques, especially \"the classic ballet \"pas de deux\" of the high-art instance of a man and woman dancing together\". They performed as predictable characters, included body positions that were no longer rooted in traditional ballroom holds, and used music with less predictable rhythms.\nThe ISU pushed back during the 1980s and 1990s by tightening rules and definitions of ice dance to emphasize its connection to ballroom dancing, especially in the free dance. The restrictions introduced during this period were designed to emphasize skating skills rather than the theatrical and dramatic aspects of ice dance. Kestnbaum argues that there was a conflict in the ice dance community between social dance, represented by the British, the Canadians, and the Americans, and theatrical dance represented by the Russians. Initially the historic and traditional cultural school of ice dance prevailed, but in 1998 the ISU reduced penalties for violations and relaxed rules on technical content, in what Hines describes as a \"major step forward\" in recognizing the move towards more theatrical skating in ice dance.\nAt the 1998 Olympics, while ice dance was struggling to retain its integrity and legitimacy as a sport, writer Jere Longman reported that ice dance was \"mired in controversies\", including bloc voting by the judges that favored European dance teams. There were even calls to suspend the sport for a year to deal with the dispute, which seemed to impact ice dance teams from North America the most. A series of judging scandals in the late 1990s and early 2000s, affecting most figure skating disciplines, culminated in a controversy at the 2002 Olympics.\n21st century.\nThe European dominance of ice dance was interrupted at the 2010 Winter Olympics in Vancouver by Canadians Tessa Virtue and Scott Moir and Americans Meryl Davis and Charlie White. The Canadian ice dance team won the first Olympic ice dance gold medal for North America, and the Americans won the silver. Russians Oksana Domnina and Maxim Shabalin won bronze, but it was the first time Europeans had not won a gold medal in the history of ice dance at the Olympics. \nThe U.S. began to dominate international competitions in ice dance; at the 2014 Olympics in Sochi, Davis and White won the Olympic gold medal. In 2018, at the Olympics in Pyeongchang, Virtue and Moir became the most decorated figure skaters in Olympic history after winning the gold medal there. In 2022, Gabriella Papadakis and Guillaume Cizeron of France won the Olympic gold medal; they went on to win the gold medal at the World championships a few months later, ending the North American domination on ice dance. Papadakis and Cizeron broke the world record at both events.\nAccording to Caroline Silby, a consultant with U.S. Figure Skating, ice dance teams and pair skaters have the added challenge of strengthening partnerships and ensuring that teams stay together for several years; unresolved conflict between partners can often cause the early break-up of a team. Silby further asserts that the early demise or break-up of a team is often caused by consistent and unresolved conflict between partners. Both ice dancers and pairs skaters face challenges that make conflict resolution and communication difficult: fewer available boys for girls to partner with; different priorities regarding commitment and scheduling; differences in partners' ages and developmental stages; differences in family situations; the common necessity of one or both partners moving to train at a new facility; and different skill levels when the partnership is formed. Silby estimates that the lack of effective communication within dance and pairs teams is associated with a six-fold increase in the risk of ending their partnerships. Teams with strong skills in communication and conflict resolution, however, tend to produce more successful medalists at national championship events.\nCompetition segments.\nHistory.\nBefore the 2010\u20132011 figure skating season, there were three segments in ice dance competitions: the compulsory dance (CD), the original dance (OD), and the free dance (FD). In 2010, after many years of pressure from the International Olympic Committee (IOC) to restructure competitive ice dance to follow the other figure skating disciplines, the ISU voted to change the competition format by eliminating the CD and the OD and adding the new short dance segment to the competition schedule. According to the then-president of the ISU, Ottavio Cinquanta, the changes were also made because \"the compulsory dances were not very attractive for spectators and television\". This new ice dance competition format was first included in the 2010\u20132011 season, incorporating just two segments: the short dance (renamed the rhythm dance, or RD in 2018) and the free dance.\nRhythm dance.\nThe RD is the first segment performed in all junior and senior ice dance competitions. As of 2022, senior skaters no longer had to include a pattern dance; instead, they were judged for performing a choreographic rhythm section, which was evaluated as a choreographic element. The RD must also include a short six-second lift, a set of twizzles, and a step sequence.\nThe rhythms and themes of the RD are determined by the ISU prior to the start of each new season. The RD should be \"developed through skating skill and quality\", instead of through \"non-skating actions such as sliding on one knee\"&lt;ref name=\"S&amp;P/ID 2024-148\"&gt;S&amp;P/ID 2024, p. 148&lt;/ref&gt; or through the use of toe steps (which should only be used to reflect the dance's character and the music's nuances and underlying rhythm). The RD must have a duration of two\u00a0minutes and fifty\u00a0seconds, unless otherwise decided by and announced by the ISU.&lt;ref name=\"S&amp;P/ID 2024-82\"&gt;S&amp;P/ID 2024, p. 82&lt;/ref&gt; The ISU states that the time a program begins \"must be reckoned\" from the moment the skaters begin to move or skate until they come to a complete stop at the end of their program.\nThe first RD in international competitions was performed by U.S. junior ice dancers Anastasia Cannuscio and Colin McManus, at the 2010 Junior Grand Prix Courchevel. American ice dancers Madison Chock and Evan Bates hold the highest RD score of 93.91, which they achieved at the 2023 World Team Trophy.\nFree dance.\nThe free dance (FD) takes place after the rhythm dance in all junior and senior ice dance competitions. The ISU defines the FD as \"the skating by the couple of a creative dance program blending dance steps and movements expressing the character/rhythm(s) of the dance music chosen by the couple\". The FD must incorporate a combination of new or known dance steps and movements, along with the required elements. The program must \"utilize the full ice surface,\"&lt;ref name=\"S&amp;P/ID 2024-149\"&gt;S&amp;P/ID 2024, p. 149&lt;/ref&gt; and be well-balanced. It must contain required combinations of elements (spins, lifts, steps, and movements), and choreography that express both the characters of the competitors and the music chosen by them. It must also display the skaters' \"excellent skating technique\" and creativity in expression, concept, and arrangement. The FD's choreography must reflect the music's accents, nuances, and dance character, and the ice dancers must \"skate primarily in time to the rhythmic beat of the music and not to the melody alone\". For senior ice dancers, the FD must have a duration of four minutes; for juniors, 3.5 minutes.\nMadison Chock and Evan Bates hold the highest FD score of 138.41 points, which they achieved at the 2023 World Team Trophy.\nDiscontinued segments.\nCompulsory dance.\nBefore 2010, the compulsory dance (CD) was the first segment performed in ice dance competitions. The teams performed the same pattern around two circuits of the rink, one team after another, using the same step sequences and the same standardized tempo chosen by the ISU before the beginning of each season. The CD has been compared with compulsory figures; competitors were \"judged for their mastery of fundamental elements\". Early in ice dance history, the CD contributed 60% of the total score.\nThe 2010 World Championships was the last event to include a CD (the Golden Waltz); Federica Faiella and Massimo Scali from Italy were the last ice dance team to perform a CD in international competition.\nOriginal dance.\nThe OD or OSP (Original Set Pattern) was first added to ice dance competitions in 1967 (1983 in WC and 1984 in Olympics). It was called the \"original set pattern dance\" until 1990, when it became known simply as the \"original dance\". The OD remained the second competition segment (sandwiched between the CD and the free dance) until the end of the 2009\u20132010 season. Ice dancers were able to create their own routines, but they had to use a set rhythm and type of music which, like the compulsory dances, changed every season and was selected by the ISU in advance. The timing and interpretation of the rhythm were considered to be the most important aspects of the routine, and were worth the highest proportion of the OD score. The routine had a two-minute time limit and the OD accounted for 30% of the overall competition score.\nCanadian ice dancers Tessa Virtue and Scott Moir hold the highest OD score of 70.27\u00a0points, achieved at the 2010 World Championships.\nCompetition elements.\nThe ISU announces the list of required elements in the rhythm dance and free dance, and each element's specific requirements, each year. The following elements may be included: the dance lift, the dance spin, the step sequence, turn sequences, choreographic elements, and, in the rhythm dance, pattern dance elements.\nRules and regulations.\nSkaters must execute the prescribed elements at least once; any extra or unprescribed elements will not be counted in their score. In 1974, the ISU published the first judges' handbook for ice dance.In 2022, the ISU voted to gradually raise the minimum age requirement for participation in international competitions at the senior level from 15 years old to 17 years old over the course of the next three seasons. As of 2024, the maximum age of female ice dancers is 21 years old. Also as of 2024, the maximum age gap between partners at the junior level of competition is seven years. Violations in ice dance include falls and interruptions, time, music, and clothing.\nFalls and interruptions.\nAccording to ice dancer and commentator Tanith White, unlike in other disciplines, wherein skaters can make up for their falls in other elements, falls in ice dance usually mean that the team will not win. White argues that falls are rare in ice dance, and since falls constitute interruptions, they tend to have large deductions because the mood of their program's theme is broken. The ISU defines a fall as the \"loss of control by a Skater with the result that the majority of his/her own body weight is on the ice supported by any other part of the body other than the blades; e.g. hand(s), knee(s), back, buttock(s) or any part of the arm\". The ISU defines an interruption as \"the period of time starting immediately when the Competitor stops performing the program or is ordered to do so by the Referee, whichever is earlier, and ending when the Competitor resumes his performance\".&lt;ref name=\"S&amp;P/ID 2024-9192\"&gt;S&amp;P/ID 2024, pp. 91\u201492&lt;/ref&gt; A study conducted during a U.S. national competition recorded 58 ice dancers, with an average of 0.97 injuries per athlete.\nIn ice dance, teams can lose one point for every fall by one partner, and two points if both partners fall. If there is an interruption while performing their program, ice dancers can lose one point if it lasts more than ten seconds but not over twenty seconds. They can lose two points if the interruption lasts twenty seconds but not over thirty seconds, and three points if it lasts thirty seconds but not more than forty seconds. They can lose five points if the interruption lasts three or more minutes. \nAn interruption can also be caused by an \"adverse condition\", which is unrelated to the skaters and/or their equipment, such as lighting, ice condition, items thrown onto the ice, etc. If an adverse condition occurs, the skaters may stop and report it to the referee as soon as they become aware of the problem. They must also stop skating when the referee signals them to do so. Adverse conditions related to the competitors and their equipment that occur during their programs include injuries. Other adverse conditions related to them or their equipment include, but are not limited to, their laces coming undone or damage to their clothing or skates. At that point, competitors must stop when they are warned by the referee or they become aware of the problem, whichever comes first.\nWhen the problem is solved, skaters can continue from the point at which the interruption occurred, or if it occurred at the entrance to or during an element, immediately before the element. If the interruption caused by an adverse condition lasts over ten minutes, a second warm-up takes place. After the warm-up, the skaters must continue their program from the point at which it was interrupted, or if the interruption occurred at the entrance to or during the element, immediately before the element. No deductions are applied for interruptions unrelated to the competitors or their equipment.\nTime.\nJudges penalize ice dancers one point up to every five seconds for ending their pattern dances too early or too late. Dancers can also be penalized one point for up to every five seconds \"in excess of [the] permitted time after the last prescribed step\" (their final movement and/or pose) in their pattern dances. If they start their programs between one and thirty seconds late, they can lose one point.&lt;ref name=\"S&amp;P/ID2024-18\"&gt;S&amp;P/ID 2024, p. 18&lt;/ref&gt; They can complete these programs within plus or minus ten seconds of the required times; if they cannot, judges can deduct points for finishing their program up to five seconds too early or too late. If they begin skating any element after their required time (plus the required ten seconds they have to begin), they earn no points for those elements. If the program's duration is \"thirty seconds or more under the required time range, no marks will be awarded\".\nIf a team performs a dance lift that exceeds the permitted duration, judges can deduct one point. The minimum required length for a dance lift is three seconds. White argues that deductions in ice dance, in the absence of a fall or interruption, are most often due to \"extended lifts\", or lifts that last too long.\nMusic.\nAll programs in each discipline of figure skating must be skated to music. The ISU has allowed vocals in the music used in ice dance since the 1997\u20131998 season, most likely because of the difficulty in finding suitable music without words for certain genres.\nViolations against the music requirements have a two-point deduction, and violations against the dance tempo requirements have a one-point deduction. If the quality or tempo of the music the team uses in their program is deficient, or if there is a stop or interruption in their music, for any reason, they must stop skating when they become aware of the problem \"or at the acoustic signal of the Referee\", whichever occurs first. If any problems with the music occur within 20 seconds after the program has begun, the team can choose to either restart the program or continue from the point where it was stopped. If they decide to continue from the point where they stopped, they will continue to be judged at that point onward, as well as their performance up to that point. If any of the mentioned problems occur over 20 seconds after the start of their program, the team can resume their program from the point of the interruption or at the point immediately before an element, if the interruption occurred at the entrance to or during the element. The element must be deleted from the team's score, and the team can repeat the deleted element when they resume their program. No deductions are made for interruptions caused by music deficiencies.\nThe ISU provides the following definitions of musical terms used in the scoring of ice dance:\nClothing.\nThe clothing worn by ice dancers at all international competitions must be \"modest, dignified and appropriate for athletic competition\u2014not garish or theatrical in design\".&lt;ref name=\"S&amp;P/ID2024-81\"&gt;S&amp;P/ID 2024, p. 81&lt;/ref&gt; Rules about clothing tend to be more strict in ice dance; Juliet Newcomer from U.S. Figure Skating has speculated limits in the kind of costumes ice dancers chose were pushed farther during the 1990s and early 2000s than in the other disciplines, resulting in stricter rules. Clothing can, however, reflect the character of ice dancers' chosen music. Their costumes must not \"give the effect of excessive nudity inappropriate for the discipline\".\nAll men must wear trousers. Female ice dancers must wear skirts or trousers. Accessories and props on the costumes of both dancers are not allowed. The decorations on costumes must be \"non-detachable\"; judges can deduct one point per program if part of the competitors' costumes or decorations fall on the ice. If there is a costume or prop violation, the judges can deduct one point per program. Judges penalize ice dance teams with a deduction to their scores if these guidelines are not followed, although exceptions to these clothing and costume restrictions may be announced by the ISU. Costume deductions, however, are rare. According to Newcomer, by the time skaters get to a national or world championship, they have received enough feedback about their costumes and are no longer willing to risk losing points.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39582", "revid": "525927", "url": "https://en.wikipedia.org/wiki?curid=39582", "title": "History of Australia", "text": "The history of Australia is the history of the land and peoples which comprise the Commonwealth of Australia. The modern nation came into existence on 1 January 1901 as a federation of former British colonies. The human history of Australia, however, commences with the arrival of the first ancestors of Aboriginal Australians from Maritime Southeast Asia between 50,000 and 65,000 years ago, and continues to the present day multicultural democracy.\nAboriginal Australians settled throughout continental Australia and many nearby islands. The artistic, musical and spiritual traditions they established are among the longest surviving in human history. The ancestors of today's ethnically and culturally distinct Torres Strait Islanders arrived from what is now Papua New Guinea around 2,500 years ago, and settled the islands on the northern tip of the Australian landmass.\nDutch navigators explored the western and southern coasts in the 17th century and named the continent New Holland. Macassan trepangers visited Australia's northern coasts from around 1720, and possibly earlier. In 1770, Lieutenant James Cook charted the east coast of Australia and claimed it for Great Britain. He returned to London with accounts favouring colonisation at Botany Bay (now in Sydney). The First Fleet of British ships arrived at Botany Bay in January 1788 to establish a penal colony. In the century that followed, the British established other colonies on the continent, and European explorers ventured into its interior. This period saw a decline in the Aboriginal population and the disruption of their cultures due to introduced diseases, violent conflict and dispossession of their traditional lands. From 1871, the Torres Strait Islanders welcomed Christian Missionaries, and the islands were later annexed by Queensland, choosing to remain a part of Australia when Papua New Guinea gained independence from Australia a century later.\nGold rushes and agricultural industries brought prosperity. Transportation of British convicts to Australia was phased out from 1840 to 1868. Autonomous parliamentary democracies began to be established throughout the six British colonies from the mid-19th century. The colonies voted by referendum to unite in a federation in 1901, and modern Australia came into being. Australia fought as part of British Empire and later Commonwealth in the two world wars and was to become a long-standing ally of the United States through the Cold War to the present. Trade with Asia increased and a post-war immigration program received more than 7 million migrants from every continent. Supported by immigration of people from almost every country in the world since the end of World War II, the population increased to more than 25.5 million by 2021, with 30 per cent of the population born overseas.\nIndigenous prehistory.\nThe ancestors of Aboriginal Australians moved into what is now the Australian continent about 50,000 to 65,000 years ago, during the last glacial period, arriving by land bridges and short sea crossings from what is now Southeast Asia.\nThe Madjedbebe rock shelter in Arnhem Land, in the north of the continent, is perhaps the oldest site of human occupation in Australia. From the north, the population spread into a range of very different environments. Devil's Lair in the extreme south-west of the continent was occupied around 47,000 years ago and Tasmania by 39,000 years ago. The oldest human remains found are at Lake Mungo in New South Wales, which have been dated to around 41,000 years ago. The site suggests one of the world's oldest known cremations, indicating early evidence for religious ritual among humans.\nThe spread of the population also altered the environment. From 46,000 years ago, fire-stick farming was used in many parts of Australia to clear vegetation, make travel easier, and create open grasslands rich in animal and vegetable food sources.\nThe Aboriginal population faced significant changes in the climate and environment. About 30,000 years ago, sea levels began to fall, temperatures in the south-east of the continent dropped by as much as , and the interior of Australia became more arid. About 20,000 years ago, New Guinea and Tasmania were connected to the Australian continent, which was more than a quarter larger than today.\nAbout 19,000 years ago temperatures and sea levels began to rise. Tasmania became separated from the mainland some 14,000 years ago, and between 8,000 and 6,000 years ago thousands of islands in the Torres Strait and around the coast of Australia were formed.\nThe warmer climate was associated with new technologies. Small back-bladed stone tools appeared 15\u201319 thousand years ago. Wooden javelins and boomerangs have been found dating from 10,000 years ago. Stone points for spears have been found dating from 5\u20137 thousand years ago. Spear throwers were probably developed more recently than 6,500 years ago.\nAboriginal Tasmanians were isolated from the mainland from about 14,000 years ago. As a result, they only possessed one quarter of the tools and equipment of the adjacent mainland. Coastal Tasmanians switched from fish to abalone and crayfish and more Tasmanians moved to the interior.\nAbout 4,000 years ago, the first phase of occupation of the Torres Strait Islands began. By 2,500 years ago more of the islands were occupied and a distinctive Torres Strait Islander maritime culture emerged. Agriculture also developed on some islands and by 700 years ago villages appeared.\nAboriginal society consisted of family groups organised into bands and clans averaging about 25 people, each with a defined territory for foraging. Clans were attached to tribes or nations, associated with particular languages and country. At the time of European contact there were about 600 such groups and 250 distinct languages with various dialects. Estimates of the Aboriginal population at this time range from 300,000 to one million.\nAboriginal society was egalitarian with no formal government or chiefs. Authority rested with elders and group decisions were generally made through the consensus of elders. The traditional economy was cooperative, with males generally hunting large game while females gathered local staples such as small animals, shellfish, vegetables, fruits, seeds and nuts. Food was shared within groups and exchanged across groups. Some Aboriginal groups engaged in fire-stick farming, fish farming, and built semi-permanent shelters. The extent to which some groups engaged in agriculture is controversial. Some Anthropologists describe traditional Aboriginal Australia as a \"complex hunter-gatherer\" society.\nAboriginal groups were semi-nomadic, generally ranging over a specific territory defined by natural features. Members of a group would enter the territory of another group through rights established by marriage and kinship or by invitation for specific purposes such as ceremonies and sharing abundant seasonal foods. As all natural features of the land were created by ancestral beings, a group's particular country provided physical and spiritual nourishment.\nAboriginal Australians developed a unique artistic and spiritual culture. The earliest Aboriginal rock art consists of hand-prints, hand-stencils, and engravings of circles, tracks, lines and cupules, and has been dated to 35,000 years ago. Around 20,000 year ago Aboriginal artists were depicting humans and animals. According to Australian Aboriginal mythology and the animist framework, the Dreaming is a sacred era in which ancestral totemic spirit beings formed The Creation. The Dreaming established the laws and structures of society and the ceremonies performed to ensure continuity of life and land.\nEarly European exploration.\nDutch discovery and exploration.\nThe Dutch East India Company ship, , captained by Willem Janszoon, made the first documented European landing in Australia in 1606. Later that year, Lu\u00eds Vaz de Torres sailed to the north of Australia through Torres Strait, along New Guinea's southern coast.\nIn 1616, Dirk Hartog, sailing off course, en route from the Cape of Good Hope to Batavia, landed on an island off Shark Bay, Western Australia. In 1622\u201323 the ship \"Leeuwin\" made the first recorded rounding of the southwest corner of the continent.\nIn 1627, the south coast of Australia was discovered by Fran\u00e7ois Thijssen and named after Pieter Nuyts. In 1628, a squadron of Dutch ships explored the northern coast particularly in the Gulf of Carpentaria.\nAbel Tasman's voyage of 1642 was the first known European expedition to reach Van Diemen's Land (later Tasmania) and New Zealand, and to sight Fiji. On his second voyage of 1644, he also contributed significantly to the mapping of the Australian mainland (which he called \"New Holland\"), making observations on the land and people of the north coast below New Guinea.\nFollowing Tasman's voyages, the Dutch were able to make almost complete maps of Australia's northern and western coasts and much of its southern and south-eastern Tasmanian coasts\".\"\nBritish and French exploration.\nWilliam Dampier, an English buccaneer and explorer, landed on the north-west coast of New Holland in 1688 and again in 1699, and published influential descriptions of the Aboriginal people.\nIn 1769, Lieutenant James Cook in command of , travelled to Tahiti to observe and record the transit of Venus. Cook also carried secret Admiralty instructions to locate the supposed Southern Continent. Unable to find this continent, Cook decided to survey the east coast of New Holland, the only major part of that continent that had not been charted by Dutch navigators.\nOn 19 April 1770, \"Endeavour\" reached the east coast of New Holland and ten days later anchored at Botany Bay. Cook charted the coast to its northern extent and formally took possession of the east coast of New Holland on 21/22 August 1770 when on Possession Island off the west coast of Cape York Peninsula.\nHe noted in his journal that he could;\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;land no more upon this Eastern coast of New Holland, and on the Western side I can make no new discovery the honour of which belongs to the Dutch Navigators and as such they may lay Claim to it as their property but the Eastern Coast from the Latitude of 38 South down to this place I am confident was never seen or viseted by any European before us and therefore by the same Rule belongs to great Brittan [...].\nIn March 1772 Marc-Joseph Marion du Fresne, in command of two French ships, reached Van Diemen's land on his way to Tahiti and the South Seas. His party became the first recorded European to encounter the Indigenous Tasmanians and to kill one of them.\nIn the same year, a French expedition led by Louis Aleno de St Alo\u00fcarn, became the first European to formally claim sovereignty over the west coast of Australia, but no attempt was made to follow this with colonisation.\nColonisation.\nPlans for colonisation before 1788.\nAlthough various proposals for the colonisation of Australia were made prior to 1788, none were attempted. In 1717, Jean-Pierre Purry sent a plan to the Dutch East India Company for the colonisation of an area in modern South Australia. The company rejected the plan with the comment that, \"There is no prospect of use or benefit to the Company in it, but rather very certain and heavy costs\".\nIn contrast, Emanuel Bowen, in 1747, promoted the benefits of exploring and colonising the country, writing:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It is impossible to conceive a Country that promises fairer from its Situation than this of , no longer incognita, as this Map demonstrates, but the Southern Continent Discovered. It lies precisely in the richest climates of the World... and therefore whoever perfectly discovers and settles it will become infalliably possessed of Territories as Rich, as fruitful, and as capable of Improvement, as any that have hitherto been found out, either in the East Indies or the West.John Harris, in his \"Navigantium atque Itinerantium Bibliotheca, or Voyages and Travels\" (1744\u20131748, 1764) recommended exploration of the east coast of New Holland, with a view to a British colonisation. John Callander put forward a proposal in 1766 for Britain to found a colony of banished convicts in the South Sea or in Terra Australis. Sweden's King Gustav III had ambitions to establish a colony for his country at the Swan River in 1786 but the plan was stillborn.\nThe American Revolutionary War (1775\u20131783) saw Britain lose most of its North American colonies and consider establishing replacement territories. Britain had transported about 50,000 convicts to the New World from 1718 to 1775 and was now searching for an alternative. The temporary solution of floating prison hulks had reached capacity and was a public health hazard, while the option of building more jails and workhouses was deemed too expensive.\nIn 1779, Sir Joseph Banks, the eminent scientist who had accompanied James Cook on his 1770 voyage, recommended Botany Bay as a suitable site for a penal settlement. Banks's plan was to send 200 to 300 convicts to Botany Bay where they could be left to their own devices and not be a burden on the British taxpayer.\nUnder Banks's guidance, the American Loyalist James Matra, who had also travelled with Cook, produced a new plan for colonising New South Wales in 1783. Matra argued that the country was suitable for plantations of sugar, cotton and tobacco; New Zealand timber and hemp or flax could prove valuable commodities; it could form a base for Pacific trade; and it could be a suitable compensation for displaced American Loyalists. Following an interview with Secretary of State Lord Sydney in 1784, Matra amended his proposal to include convicts as settlers, considering that this would benefit both \"Economy to the Publick, &amp; Humanity to the Individual\".\nThe major alternative to Botany Bay was sending convicts to Africa. From 1775 convicts had been sent to garrison British forts in west Africa, but the experiment had proved unsuccessful. In 1783, the Pitt government considered exiling convicts to a small river island in Gambia where they could form a self-governing community, a \"colony of thieves\", at no expense to the government.\nIn 1785, a parliamentary select committee chaired by Lord Beauchamp recommended against the Gambia plan, but failed to endorse the alternative of Botany Bay. In a second report, Beauchamp recommended a penal settlement at Das Voltas Bay in modern Namibia. The plan was dropped, however, when an investigation of the site in 1786 found it to be unsuitable. Two weeks later, in August 1786, the Pitt government announced its intention to send convicts to Botany Bay. The Government incorporated the settlement of Norfolk Island into their plan, with its attractions of timber and flax, proposed by Banks's Royal Society colleagues, Sir John Call and Sir George Young.\nThere has been a longstanding debate over whether the key consideration in the decision to establish a penal colony at Botany Bay was the pressing need to find a solution to the penal management problem, or whether broader imperial goals \u2013 such as trade, securing new supplies of timber and flax for the navy, and the desirability of strategic ports in the region \u2013 were paramount. Christopher and Maxwell-Stewart argue that whatever the government's original motives were in establishing the colony, by the 1790s it had at least achieved the imperial objective of providing a harbour where vessels could be careened and resupplied.\nColony of New South Wales.\nEstablishment of the colony (1788 to 1792).\nThe colony of New South Wales was established with the arrival of the First Fleet of 11 vessels under the command of Captain Arthur Phillip in January 1788. It consisted of more than a thousand settlers, including 778 convicts (192 women and 586 men). A few days after arrival at Botany Bay the fleet moved to the more suitable Port Jackson where a settlement was established at Sydney Cove on 26 January 1788. This date later became Australia's national day, Australia Day. The colony was formally proclaimed by Governor Phillip on 7 February 1788 at Sydney. Sydney Cove offered a fresh water supply and a safe harbour, which Phillip described as being,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;with out exception the finest Harbour in the World [...] Here a Thousand Sail of the Line may ride in the most perfect Security [...].\nThe territory of New South Wales claimed by Britain included all of Australia eastward of the meridian of 135\u00b0 East. This included more than half of mainland Australia. The claim also included \"all the Islands adjacent in the Pacific\" between the latitudes of Cape York and the southern tip of Van Diemen's Land (Tasmania). In 1817, the British government withdrew the extensive territorial claim over the South Pacific, passing an act specifying that Tahiti, New Zealand and other islands of the South Pacific were not within His Majesty's dominions. However, it is unclear whether the claim ever extended to the current islands of New Zealand.\nGovernor Phillip was vested with complete authority over the inhabitants of the colony. His intention was to establish harmonious relations with local Aboriginal people and try to reform as well as discipline the convicts of the colony. Early efforts at agriculture were fraught and supplies from overseas were scarce. Between 1788 and 1792 about 3546 male and 766 female convicts were landed at Sydney. Many new arrivals were sick or unfit for work and the condition of healthy convicts also deteriorated due to the hard labour and poor food. The food situation reached crisis point in 1790 and the Second Fleet which finally arrived in June 1790 had lost a quarter of its passengers through sickness, while the condition of the convicts of the Third Fleet appalled Phillip. From 1791, however, the more regular arrival of ships and the beginnings of trade lessened the feeling of isolation and improved supplies.\nIn 1788, Phillip established a subsidiary settlement on Norfolk Island in the South Pacific where he hoped to obtain timber and flax for the navy. The island, however, had no safe harbour, which led the settlement to be abandoned and the settlers evacuated to Tasmania in 1807. The island was subsequently re-established as a site for secondary transportation in 1825.\nPhillip sent exploratory missions in search of better soils, fixed on the Parramatta region as a promising area for expansion, and moved many of the convicts from late 1788 to establish a small township, which became the main centre of the colony's economic life. This left Sydney Cove only as an important port and focus of social life. Poor equipment and unfamiliar soils and climate continued to hamper the expansion of farming from Farm Cove to Parramatta and Toongabbie, but a building program, assisted by convict labour, advanced steadily. Between 1788 and 1792, convicts and their gaolers made up the majority of the population; however, a free population soon began to grow, consisting of emancipated convicts, locally born children, soldiers whose military service had expired and, finally, free settlers from Britain. Governor Phillip departed the colony for England on 11 December 1792, with the new settlement having survived near starvation and immense isolation for four years.\nConsolidation (1793 to 1821).\nAfter the departure of Phillip, the colony's military officers began acquiring land and importing consumer goods obtained from visiting ships. Former convicts also farmed land granted to them and engaged in trade. Farms spread to the more fertile lands surrounding Parramatta, Windsor, Richmond and Camden, and by 1803 the colony was self-sufficient in grain. Boat building developed in order to make travel easier and exploit the marine resources of the coastal settlements. Sealing and whaling became important industries.\nThe New South Wales Corps was formed in England in 1789 as a permanent regiment of the British Army to relieve the marines who had accompanied the First Fleet. Officers of the Corps soon became involved in the corrupt and lucrative rum trade in the colony. Governor William Bligh (1806\u20131808) tried to suppress the rum trade and the illegal use of Crown Land, resulting in the Rum Rebellion of 1808. The Corps, working closely with the newly established wool trader John Macarthur, staged the only successful armed takeover of government in Australian history, deposing Bligh and instigating a brief period of military rule prior to the arrival from Britain of Governor Lachlan Macquarie in 1810.\nMacquarie served as the last autocratic Governor of New South Wales, from 1810 to 1821, and had a leading role in the social and economic development of New South Wales which saw it transition from a penal colony to a budding civil society. He established a bank, a currency and a hospital. He employed a planner to design the street layout of Sydney and commissioned the construction of roads, wharves, churches, and public buildings. He sent explorers out from Sydney and, in 1815, a road across the Blue Mountains was completed, opening the way for large scale farming and grazing in the lightly wooded pastures west of the Great Dividing Range.\nCentral to Macquarie's policy was his treatment of the emancipists, whom he considered should be treated as social equals to free-settlers in the colony. He appointed emancipists to key government positions including Francis Greenway as colonial architect and William Redfern as a magistrate. His policy on emancipists was opposed by many influential free settlers, officers and officials, and London became concerned at the cost of his public works. In 1819, London appointed J. T. Bigge to conduct an inquiry into the colony, and Macquarie resigned shortly before the report of the inquiry was published.\nExpansion (1821 to 1850).\nIn 1820, British settlement was largely confined to a around Sydney and to the central plain of Van Diemen's land. The settler population was 26,000 on the mainland and 6,000 in Van Diemen's Land. Following the end of the Napoleonic Wars in 1815, the transportation of convicts increased rapidly and the number of free settlers grew steadily. From 1821 to 1840, 55,000 convicts arrived in New South Wales and 60,000 in Van Diemen's Land. However, by 1830, free settlers and the locally born exceeded the convict population of New South Wales.\nFrom the 1820s squatters increasingly established unauthorised cattle and sheep runs beyond the official limits of the settled colony. In 1836, a system of annual licences authorising grazing on Crown Land was introduced in an attempt to control the pastoral industry, but booming wool prices and the high cost of land in the settled areas encouraged further squatting. By 1844 wool accounted for half of the colony's exports and by 1850 most of the eastern third of New South Wales was controlled by fewer than 2,000 pastoralists.\nIn 1825, the western boundary of New South Wales was extended to longitude 129\u00b0 East, which is the current nominal eastern boundary of Western Australia. As a result, the territory of New South Wales reached its greatest extent, covering the area of the modern state as well as modern Queensland, Victoria, Tasmania, South Australia and the Northern Territory.\nBy 1850 the settler population of New South Wales had grown to 180,000, not including the 70,000\u201375,000 living in the area which became the separate colony of Victoria in 1851.\nEstablishment of further colonies.\nAfter hosting Nicholas Baudin's French naval expedition in Sydney in 1802, Governor Phillip Gidley King decided to establish a settlement in Van Diemen's Land (modern Tasmania) in 1803, partly to forestall a possible French settlement. The British settlement of the island soon centred on Launceston in the north and Hobart in the south. From the 1820s free settlers were encouraged by the offer of land grants in proportion to the capital the settlers would bring. Van Diemen's Land became a separate colony from New South Wales in December 1825 and continued to expand through the 1830s, supported by farming, sheep grazing and whaling. Following the suspension of convict transportation to New South Wales in 1840, Van Diemen's land became the main destination for convicts. Transportation to Van Diemen's Land ended in 1853 and in 1856 the colony officially changed its name to Tasmania.\nPastoralists from Van Diemen's land began squatting in the Port Phillip hinterland on the mainland in 1834, attracted by its rich grasslands. In 1835, John Batman and others negotiated the transfer of of land from the Kulin people. However, the treaty was annulled the same year when the British Colonial Office issued the \"Proclamation of Governor Bourke.\" The proclamation meant that from then, all people found occupying land without the authority of the government would be considered illegal trespassers.\nIn 1836, Port Phillip was officially recognised as a district of New South Wales and opened for settlement. The main settlement of Melbourne was established in 1837 as a planned town on the instructions of Governor Bourke. Squatters and settlers from Van Diemen's Land and New South Wales soon arrived in large numbers. In 1851, the Port Phillip District separated from New South Wales as the colony of Victoria.\nIn 1826, the governor of New South Wales, Ralph Darling, sent a military garrison to King George Sound to deter the French from establishing a settlement in New Holland. In 1827, the head of the expedition, Major Edmund Lockyer, formally annexed the western portion of the continent not already claimed by Britain as a British colony. In 1829, the Swan River colony was established at the sites of modern Fremantle and Perth, becoming the first convict-free and privatised colony in Australia. However, by 1850 there were a little more than 5,000 settlers. The colony accepted convicts from that year because of the acute shortage of labour.\nThe Province of South Australia was established in 1836 as a privately financed settlement based on the theory of \"systematic colonisation\" developed by Edward Gibbon Wakefield. Convict labour was banned in the hope of making the colony more attractive to \"respectable\" families and promote an even balance between male and female settlers. The city of Adelaide was to be planned with a generous provision of churches, parks and schools. Land was to be sold at a uniform price and the proceeds used to secure an adequate supply of labour through selective assisted migration. Various religious, personal and commercial freedoms were guaranteed, and the Letters Patent enabling the South Australia Act 1834 included a guarantee of Aboriginal land rights.\nThe colony, however, was badly hit by the depression of 1841\u201344. Conflict with Indigenous traditional landowners also reduced the protections they had been promised. In 1842, the settlement became a Crown colony administered by the governor and an appointed Legislative Council. The economy recovered and by 1850 the settler population had grown to 60,000. In 1851, the colony achieved limited self-government with a partially elected Legislative Council.\nIn 1824, the Moreton Bay penal settlement was established on the site of present-day Brisbane. In 1842, the penal colony was closed and the area was opened for free settlement. By 1850 the population of Brisbane had reached 8,000 and increasing numbers of pastoralists were grazing cattle and sheep in the Darling Downs west of the town. Frontier violence between settlers and the Indigenous population became severe as pastoralism expanded north of the Tweed River. A series of disputes between northern pastoralists and the government in Sydney led to increasing demands from the northern settlers for separation from New South Wales. In 1857, the British government agreed to the separation and in 1859 the colony of Queensland was proclaimed.\nConvicts and colonial society.\nConvicts and emancipists.\nBetween 1788 and 1868, approximately 161,700 convicts were transported to the Australian colonies of New South Wales, Van Diemen's Land and Western Australia. The literacy rate of convicts was above average and they brought a range of useful skills to the new colony including building, farming, sailing, fishing and hunting. The small number of free settlers meant that early governors also had to rely on convicts and emancipists for professions such as lawyers, architects, surveyors and teachers.\nConvicts initially worked on government farms and public works such as land clearing and building. After 1792, the majority were assigned to work for private employers including emancipists. Emancipists were granted small plots of land for farming and a year of government rations. Later they were assigned convict labour to help them work their farms. Some convicts were assigned to military officers to run their businesses. These convicts learnt commercial skills which could help them work for themselves when their sentence ended or they were granted a \"ticket of leave\" (a form of parole).\nConvicts soon established a system of piece work which allowed them to work for wages once their allocated tasks were completed. By 1821 convicts, emancipists and their children owned two-thirds of the land under cultivation, half the cattle and one-third of the sheep. They also worked in trades and small business. Emancipists employed about half of the convicts assigned to private masters.\nA series of reforms recommended by J. T. Bigge in 1822 and 1823 worsened conditions for convicts. The food ration was cut and their opportunities to work for wages restricted. More convicts were assigned to rural work gangs, bureaucratic control and surveillance of convicts was made more systematic, isolated penal settlements were established as places of secondary punishment, the rules for tickets of leave were tightened, and land grants were skewed to favour free settlers with large capital. As a result, convicts who arrived after 1820 were far less likely to become property owners, to marry, and to establish families.\nFree settlers.\nThe Bigge reforms also aimed to encourage free settlers by offering them land grants in proportion to their capital. From 1831, the colonies replaced land grants with land sales by auction at a fixed minimum price per acre, the proceeds being used to fund the assisted migration of workers. From 1821 to 1850, Australia attracted 200,000 immigrants from the United Kingdom. However, the system of land allocations led to the concentration of land in the hands of a small number of affluent settlers.\nTwo-thirds of the migrants to Australia during this period received assistance from the British or colonial governments. Families of convicts were also offered free passage and about 3,500 migrants were selected under the English Poor Laws. Various special-purpose and charitable schemes, such as those of Caroline Chisholm and John Dunmore Lang, also provided migration assistance.\nWomen.\nWomen comprised only about 15% of convicts transported. Due to the shortage of women in the colony they were more likely to marry than men and tended to choose as husbands older, skilled men with property. The early colonial courts enforced the property rights of women independently of their husbands, and the ration system also gave women and their children some protection from abandonment. Women were active in business and agriculture from the early years of the colony, among the most successful being the former convict turned entrepreneur Mary Reibey and the agriculturalist Elizabeth Macarthur. One-third of the shareholders of the first colonial bank (founded in 1817) were women.\nOne of the goals of the assisted migration programs from the 1830s was to promote migration of women and families to provide a more even gender balance in the colonies. Caroline Chisholm established a shelter and labour exchange for migrant women in New South Wales in the 1840s and promoted the settlement of single and married women in rural areas.\nBetween 1830 and 1850 the female proportion of the Australian settler population increased from 24 per cent to 41 per cent.\nReligion.\nThe Church of England was the only recognised church before 1820 and its clergy worked closely with the governors. Richard Johnson (chief chaplain 1788\u20131802) was charged by Governor Arthur Phillip, with improving \"public morality\" in the colony and was also heavily involved in health and education. Samuel Marsden (various ministries 1795\u20131838) became known for his missionary work, the severity of his punishments as a magistrate, and the vehemence of his public denunciations of Catholicism and Irish convicts.\nAbout a quarter of convicts were Catholics. The lack of official recognition of Catholicism was combined with suspicion of Irish convicts which only increased after the Irish-led Castle Hill Rebellion of 1804. Only two Catholic priests operated temporarily in the colony before Governor Macquarie appointed official Catholic chaplains in New South Wales and Van Diemen's Land in 1820.\nThe Bigge reports recommended that the status of the Anglican Church be enhanced. An Anglican archdeacon was appointed in 1824 and allocated a seat in the first advisory Legislative Council. The Anglican clergy and schools also received state support. This policy was changed under Governor Burke by the Church Acts of 1836 and 1837. The government now provided state support for the clergy and church buildings of the four largest denominations: Anglican, Catholic, Presbyterian and, later, Methodist.\nMany Anglicans saw state support of the Catholic Church as a threat. The prominent Presbyterian minister John Dunmore Lang also promoted sectarian divisions in the 1840s. State support, however, led to a growth in church activities. Charitable associations such as the Catholic Sisters of Charity, founded in 1838, provided hospitals, orphanages and asylums for the old and disabled. Religious organisations were also the main providers of school education in the first half of the nineteenth century, a notable example being Lang's Australian College which opened in 1831. Many religious associations, such as the Sisters of St Joseph, co-founded by Mary MacKillop in 1866, continued their educational activities after the provision of secular state schools grew from the 1850s.\nExploration of the continent.\nIn 1798\u201399 George Bass and Matthew Flinders set out from Sydney in a sloop and circumnavigated Tasmania, thus proving it to be an island. In 1801\u201302 Matthew Flinders in led the first circumnavigation of Australia. Aboard ship was the Aboriginal explorer Bungaree, who became the first person born on the Australian continent to circumnavigate it.\nIn 1798, the former convict John Wilson and two companions crossed the Blue Mountains, west of Sydney, in an expedition ordered by Governor Hunter. Hunter suppressed news of the feat for fear that it would encourage convicts to abscond from the settlement. In 1813, Gregory Blaxland, William Lawson and William Wentworth crossed the mountains by a different route and a road was soon built to the Central Tablelands.\nIn 1824, Hamilton Hume and William Hovell led an expedition to find new grazing land in the south of the colony, and also to find out where New South Wales' western rivers flowed. Over 16 weeks in 1824\u201325, they journeyed to Port Phillip and back. They discovered the Murray River (which they named the \"Hume\") and many of its tributaries, and good agricultural and grazing lands.\nCharles Sturt led an expedition along the Macquarie River in 1828 and discovered the Darling River. Leading a second expedition in 1829, Sturt followed the Murrumbidgee River into the Murray River. His party then followed this river to its junction with the Darling River. Sturt continued down river on to Lake Alexandrina, where the Murray meets the sea in South Australia.\nSurveyor General Sir Thomas Mitchell conducted a series of expeditions from the 1830s to follow up these previous expeditions. Mitchell employed three Aboriginal guides and recorded many Aboriginal place names. He also recorded a violent encounter with traditional owners on the Murray in 1836 in which his men pursued them, \"shooting as many as they could.\"\nThe Polish scientist and explorer Count Paul Edmund Strzelecki conducted surveying work in the Australian Alps in 1839 and, led by two Aboriginal guides, became the first European to ascend Australia's highest peak, which he named Mount Kosciuszko in honour of the Polish patriot Tadeusz Ko\u015bciuszko.\nThe German scientist Ludwig Leichhardt led three expeditions in northern Australia in the 1840s, sometimes with the help of Aboriginal guides. He and his party disappeared in 1848 while attempting to cross the continent from east to west. Edmund Kennedy led an expedition into what is now far-western Queensland in 1847 before being speared by Aboriginals in the Cape York Peninsula in 1848.\nIn 1860, Burke and Wills led the first south\u2013north crossing of the continent from Melbourne to the Gulf of Carpentaria. Lacking bushcraft and unwilling to learn from the local Aboriginal people, Burke and Wills died in 1861, having returned from the Gulf to their rendezvous point at Coopers Creek only to discover the rest of their party had departed the location only a matter of hours previously. They became tragic heroes to the European settlers, their funeral attracting a crowd of more than 50,000 and their story inspiring numerous books, artworks, films and representations in popular culture.\nIn 1862, John McDouall Stuart succeeded in traversing central Australia from south to north. His expedition mapped out the route which was later followed by the Australian Overland Telegraph Line.\nThe completion of this telegraph line in 1872 was associated with further exploration of the Gibson Desert and the Nullarbor Plain. While exploring central Australia in 1872, Ernest Giles sighted Kata Tjuta from a location near Kings Canyon and called it \"Mount Olga\". The following year Willian Gosse observed Uluru and named it \"Ayers Rock\", in honour of the Chief Secretary of South Australia, Sir Henry Ayers.\nIn 1879, Alexander Forrest trekked from the north coast of Western Australia to the overland telegraph, discovering land suitable for grazing in the Kimberley region.\nImpact of British settlement on Indigenous population.\nWhen the First Fleet arrived in Sydney Cove with some 1,300 colonists in January 1788 the Aboriginal population of the Sydney region is estimated to have been about 3,000 people. The first governor of New South Wales, Arthur Phillip, arrived with instructions to:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;endeavour by every possible means to open an intercourse with the natives, and to conciliate their affections, enjoining all our subjects to live in amity and kindness with them.\nDisease.\nThe relative isolation of the Indigenous population for some 60,000 years meant that they had little resistance to many introduced diseases. An outbreak of smallpox in April 1789 killed about half the Aboriginal population of the Sydney region. The source of the outbreak is controversial; some researchers contend that it originated from contact with Indonesian fisherman in the far north while others argue that it is more likely to have been inadvertently, or deliberately, spread by settlers.\nThere were further smallpox outbreaks devastating Aboriginal populations from the late 1820s (affecting south-eastern Australia), in the early 1860s (travelling inland from the Coburg Peninsula in the north to the Great Australian Bight in the south), and in the late 1860s (from the Kimberley to Geraldton). According to Josephine Flood, the estimated Aboriginal mortality rate from smallpox was 60 per cent on first exposure, 50 per cent in the tropics, and 25 per cent in the arid interior.\nOther introduced diseases such as measles, influenza, typhoid and tuberculosis also resulted in high death rates in Aboriginal communities. Butlin estimates that the Aboriginal population in the area of modern Victoria was around 50,000 in 1788 before two smallpox outbreaks reduced it to about 12,500 in 1830. Between 1835 and 1853, the Aboriginal population of Victoria fell from 10,000 to around 2,000. It is estimated that about 60 per cent of these deaths were from introduced diseases, 18 per cent from natural causes and 15 per cent from settler violence.\nVenereal diseases were also a factor in Indigenous depopulation, reducing Aboriginal fertility rates in south-eastern Australia by an estimated 40 per cent by 1855. By 1890 up to 50 per cent of the Aboriginal population in some regions of Queensland were affected.\nConflict and dispossession.\nThe British settlement was initially planned to be a self-sufficient penal colony based on agriculture. Karskens argues that conflict broke out between the settlers and the traditional owners of the land because of the settlers' assumptions about the superiority of British civilisation and their entitlement to land which they had \"improved\" through building and cultivation.\nConflict also arose from cross-cultural misunderstandings and from reprisals for previous actions such as the kidnapping of Aboriginal men, women and children. Reprisal attacks and collective punishments were perpetrated by colonists and Aboriginal groups alike. Sustained Aboriginal attacks on settlers, the burning of crops and the mass killing of livestock were more obviously acts of resistance to the loss of traditional land and food resources.\nThere were serious conflicts between settlers in the Sydney region and Aboriginals (Darug people) from 1794 to 1800 in which 26 settlers and up to 200 Darug were killed. Conflict also erupted south-west of Sydney (in Dharawal country) from 1814 to 1816, culminating in the Appin massacre (April 1816) in which at least 14 Aboriginal people were killed.\nIn the 1820s, the colony spread over the Great Dividing Range, opening the way for large scale farming and grazing in Wiradjuri country. From 1822 to 1824 Windradyne led a group of 50\u2013100 Aboriginal men in raids which resulted in the death of 15\u201320 colonists. Estimates of Aboriginal deaths in the conflict range from 15 to 100.\nIn Van Diemen's land, the Black War broke out in 1824, following a rapid expansion of settler numbers and sheep grazing in the island's interior. Martial law was declared in November 1828 and in October 1830 a \"Black Line\" of around 2,200 troops and settlers swept the island with the intention of driving the Aboriginal population from the settled districts. From 1830 to 1834, George Augustus Robinson and Aboriginal ambassadors including Truganini led a series of \"Friendly Missions\" to the Aboriginal tribes which effectively ended the war. Around 200 settlers and 600 to 900 Aboriginal Tasmanians were killed in the conflict and the Aboriginal survivors were eventually relocated to Flinders Island.\nThe spread of settlers and pastoralists into the region of modern Victoria in the 1830s also sparked conflict with traditional landowners. Broome estimates that 80 settlers and 1,000\u20131,500 Aboriginal people died in frontier conflict in Victoria from 1835 to 1853.\nThe growth of the Swan River Colony in the 1830s led to conflict with Aboriginal people, culminating in the Pinjarra massacre in which some 15 to 30 Aboriginal people were killed. According to Neville Green, 30 settlers and 121 Aboriginal people died in violent conflict in Western Australia between 1826 and 1852.\nThe spread of sheep and cattle grazing after 1850 brought further conflict with Aboriginal tribes more distant from the closely settled areas. Aboriginal casualty rates in conflicts increased as the colonists made greater use of mounted police, Native Police units, and newly developed revolvers and breech-loaded guns. Conflict was particularly intense in NSW in the 1840s and in Queensland from 1860 to 1880. In central Australia, it is estimated that 650 to 850 Aboriginal people, out of a population of 4,500, were killed by colonists from 1860 to 1895. In the Gulf Country of northern Australia five settlers and 300 Aboriginal people were killed before 1886. The last recorded massacre of Aboriginal people by settlers was at Coniston in the Northern Territory in 1928 where at least 31 Aboriginal people were killed.\nThe spread of British settlement also led to an increase in inter-tribal Aboriginal conflict as more people were forced off their traditional lands into the territory of other, often hostile, tribes. Butlin estimated that of the 8,000 Aboriginal deaths in Victoria from 1835 to 1855, 200 were from inter-tribal violence.\nBroome estimates the total death toll from settler-Aboriginal conflict between 1788 and 1928 as 1,700 settlers and 17\u201320,000 Aboriginal people. Reynolds has suggested a higher \"guesstimate\" of 3,000 settlers and up to 30,000 Aboriginals killed. A project team at the University of Newcastle, Australia, has reached a preliminary estimate of 8,270 Aboriginal deaths in frontier massacres from 1788 to 1930.\nAccommodation and protection.\nIn the first two years of settlement the Aboriginal people of Sydney mostly avoided the newcomers. In November 1790, Bennelong led the survivors of several clans into Sydney, 18 months after the smallpox epidemic that had devastated the Aboriginal population. Bungaree, a Kuringgai man, joined Matthew Flinders in his circumnavigation of Australia from 1801 to 1803, playing an important role as emissary to the various Indigenous peoples they encountered.\nGovernor Macquarie attempted to assimilate Aboriginal people, providing land grants, establishing Aboriginal farms, and founding a Native Institution to provide education to Aboriginal children. However, by the 1820s the Native Institution and Aboriginal farms had failed. Aboriginal people continued to live on vacant waterfront land and on the fringes of the Sydney settlement, adapting traditional practices to the new semi-urban environment.\nFollowing escalating frontier conflict, \"Protectors of Aborigines\" were appointed in South Australia and the Port Phillip District in 1839, and in Western Australia in 1840. The aim was to extend the protection of British law to Aboriginal people, to distribute rations, and to provide education, instruction in Christianity, and occupational training. However, by 1857 the protection offices had been closed due to their cost and failure to meets their goals.\nIn 1825, the New South Wales governor granted for an Aboriginal Christian mission at Lake Macquarie. In the 1830s and early 1840s there were also missions in the Wellington Valley, Port Phillip and Moreton Bay. The settlement for Aboriginal Tasmanians on Flinders Island operated effectively as a mission under George Robinson from 1835 to 1838.\nIn New South Wales, 116 Aboriginal reserves were established between 1860 and 1894. Most reserves allowed Aboriginal people a degree of autonomy and freedom to enter and leave. In contrast, the \"Victorian Board for the Protection of Aborigines\" (created in 1869) had extensive power to regulate the employment, education and place of residence of Aboriginal Victorians, and closely managed the five reserves and missions established since self government in 1858. In 1886, the protection board gained the power to exclude \"half caste\" Aboriginal people from missions and stations. The Victorian legislation was the forerunner of the racial segregation policies of other Australian governments from the 1890s.\nIn more densely settled areas, most Aboriginal people who had lost control of their land lived on reserves and missions, or on the fringes of cities and towns. In pastoral districts the British Waste Land Act 1848 gave traditional landowners limited rights to live, hunt and gather food on Crown land under pastoral leases. Many Aboriginal groups camped on pastoral stations where Aboriginal men were often employed as shepherds and stockmen. These groups were able to retain a connection with their lands and maintain aspects of their traditional culture.\nForeign pearlers moved into the Torres Strait Islands from 1868 bringing exotic diseases which halved the Indigenous population. In 1871, the London Missionary Society began operating in the islands and most Torres Strait Islanders converted to Christianity which they considered compatible with their beliefs. Queensland annexed the islands in 1879.\nFrom autonomy to federation.\nColonial self-government and the gold rushes.\nTowards representative government.\nImperial legislation in 1823 had provided for a Legislative Council nominated by the governor of New South Wales, and a new Supreme Court, providing additional limits to the power of governors. A number of prominent colonial figures, including William Wentworth, campaigned for a greater degree of self-government, although there were divisions about the extent to which a future legislative body should be popularly elected. Other issues included traditional British political rights, land policy, transportation and whether a large population of convicts and former convicts could be trusted with self-government. The Australian Patriotic Association was formed in 1835 by Wentworth and William Bland to promote representative government for New South Wales.\nTransportation to New South Wales was suspended in 1840. In 1842 Britain granted limited representative government to the colony by reforming the Legislative Council so that two-thirds of its members would be elected by male voters. However, a property qualification meant that only 20 per cent of males were eligible to vote in the first Legislative Council elections in 1843.\nThe increasing number of free settlers and people born in the colonies led to further agitation for liberal and democratic reforms. In the Port Phillip District there was agitation for representative government and independence from New South Wales. In 1850, Britain granted Van Diemen's Land, South Australia and the newly created colony of Victoria semi-elected Legislative Councils on the New South Wales model.\nGold rushes of the 1850s.\nIn February 1851, Edward Hargraves discovered gold near Bathurst, New South Wales. Further discoveries were made later that year in Victoria, where the richest gold fields were found. New South Wales and Victoria introduced a gold mining licence with a monthly fee, the revenue being used to offset the cost of providing infrastructure, administration and policing of the goldfields.\nThe gold rush initially caused inflation and labour shortages as male workers moved to the goldfields. Immigrants poured in from Britain, Europe, the United States and China, many of whom sought to go to the goldfields. The Australian population increased from 430,000 in 1851 to 1,170,000 in 1861. Victoria became the most populous colony and Melbourne the largest city.\nChinese migration was a particular concern for colonial officials due to the widespread belief that it represented a danger to white Australian living standards and morality. Colonial governments responded by imposing taxes and restrictions on Chinese migrants and residents. Anti-Chinese riots erupted on the Victorian goldfields in 1856 and in New South Wales in 1860.\nEureka stockade.\nFaced with increasing competition, Victorian miners increasingly complained about the licence fee, corrupt and heavy-handed officials, and the lack of voting rights for itinerant miners. Protests intensified in October 1854 when three miners were arrested following a riot at Ballarat. Protesters formed the Ballarat Reform League to support the arrested men and demanded manhood suffrage, reform of the mining licence and administration, and land reform to promote small farms. Further protests followed and protesters built a stockade on the Eureka Field at Ballarat. On 3 December troops overran the stockade, killing about 20 protesters. Five troops were killed and 12 seriously wounded.\nFollowing a Royal Commission, the monthly licence was replaced with a cheaper annual miner's right which gave holders the right to vote and build a dwelling on the goldfields. The administration of the Victorian goldfields was also reformed. The Eureka rebellion soon became a part of Australian nationalist mythology.\nSelf-government and democracy.\nElections for the semi-representative Legislative Councils, held in New South Wales, Victoria, South Australia and Van Diemen's Land in 1851, produced a greater number of liberal members who agitated for full self-government. In 1852, the British Government announced that convict transportation to Van Diemen's Land would cease and invited the eastern colonies to draft constitutions enabling self-government.\nThe constitutions for New South Wales, Victoria and Van Diemen's Land (renamed Tasmania in 1856) gained Royal Assent in 1855, that for South Australia in 1856. The constitutions varied, but each created a lower house elected on a broad male franchise and an upper house which was either appointed for life (New South Wales) or elected on a more restricted property franchise. When Queensland became a separate colony in 1859 it immediately became self-governing. Western Australia was granted self-government in 1890.\nThe secret ballot was adopted in Tasmania, Victoria and South Australia in 1856, followed by New South Wales (1858), Queensland (1859) and Western Australia (1877). South Australia introduced universal male suffrage for its lower house in 1856, followed by Victoria in 1857, New South Wales (1858), Queensland (1872), Western Australia (1893) and Tasmania (1900). Queensland excluded Aboriginal males from voting in 1885. In Western Australia a property qualification for voting existed for male Aboriginals, Asians, Africans and people of mixed descent.\nSocieties to promote women's suffrage were formed in Victoria in 1884, South Australia in 1888 and New South Wales in 1891. The Women's Christian Temperance Union also established branches in most Australian colonies in the 1880s, promoting votes for women and a range of social causes. Female suffrage, and the right to stand for office, was first won in South Australia in 1895. Women won the vote in Western Australia in 1899, with racial restrictions. Women in the rest of Australia only won full rights to vote and to stand for elected office in the decade after Federation, although there were some racial restrictions.\nThe long boom (1860 to 1890).\nFrom the 1850s to 1871 gold was Australia's largest export and allowed the colonies to import a range of consumer and capital goods. The increase in population in the decades following the gold rush stimulated demand for housing, consumer goods, services and urban infrastructure.\nIn the 1860s, New South Wales, Victoria, Queensland and South Australia introduced Selection Acts intended to promote family farms and mixed farming and grazing. Improvements in farming technology and the introduction of crops adapted to Australian conditions eventually led to the diversification of rural land use. The expansion of the railways from the 1860s allowed wheat to be cheaply transported in bulk, stimulating the development of a wheat belt from South Australia to Queensland.\nThe period 1850 to 1880 saw a revival in bushranging. The resurgence of bushranging from the 1850s drew on the grievances of the rural poor (several members of the Kelly gang, the most famous bushrangers, were the sons of impoverished small farmers). The exploits of Ned Kelly and his gang garnered considerable local community support and extensive national press coverage at the time. After Kelly's capture and execution for murder in 1880 his story inspired numerous works of art, literature and popular culture and continuing debate about the extent to which he was a rebel fighting social injustice and oppressive police, or a murderous criminal.\nBy the 1880s half the Australian population lived in towns, making Australia more urbanised than the United Kingdom, the United States and Canada. Between 1870 and 1890 average income per person in Australia was more than 50 per cent higher than that of the United States, giving Australia one of the highest living standards in the world.\nThe size of the government sector almost doubled from 10 per cent of national expenditure in 1850 to 19 per cent in 1890. Colonial governments spent heavily on infrastructure such as railways, ports, telegraph, schools and urban services. Much of the money for this infrastructure was borrowed on the London financial markets, but land-rich governments also sold land to finance expenditure and keep taxes low.\nIn 1856, building workers in Sydney and Melbourne were the first in the world to win the eight hour working day. The 1880s saw trade unions grow and spread to lower skilled workers and also across colonial boundaries. By 1890 about 20 per cent of male workers belonged to a union, one of the highest rates in the world.\nEconomic growth was accompanied by expansion into northern Australia. Gold was discovered in northern Queensland in the 1860s and 1870s, and in the Kimberley and Pilbara regions of Western Australia in the 1880s. Sheep and cattle runs spread to northern Queensland and on to the Gulf Country of the Northern Territory and the Kimberley region of Western Australia in the 1870s and 1880s. Sugar plantations also expanded in northern Queensland during the same period.\nFrom the late 1870s trade unions, Anti-Chinese Leagues and other community groups campaigned against Chinese immigration and low-wage Chinese labour. Following inter-colonial conferences on the issue in 1880\u201381 and 1888, colonial governments responded with a series of laws which progressively restricted Chinese immigration and citizenship rights.\n1890s depression.\nFalling wool prices and the collapse of a speculative property bubble in Melbourne heralded the end of the long boom. A number of major banks suspended business and the economy contracted by 20 per cent from 1891 to 1895. Unemployment rose to almost a third of the workforce. The depression was followed by the \"Federation Drought\" from 1895 to 1903.\nIn 1890, a strike in the shipping industry spread to wharves, railways, mines and shearing sheds. Employers responded by locking out workers and employing non-union labour, and colonial governments intervened with police and troops. The strike failed, as did subsequent strikes of shearers in 1891 and 1894, and miners in 1892 and 1896.\nThe defeat of the 1890 Maritime Strike led trade unions to form political parties. In New South Wales, the Labor Electoral League won a quarter of seats in the elections of 1891 and held the balance of power between the Free Trade Party and the Protectionist Party. Labor parties also won seats in the South Australian and Queensland elections of 1893. The world's first Labor government was formed in Queensland in 1899, but it lasted only a week.\nAt an Inter-colonial Conference in 1896, the colonies agreed to extend restrictions on Chinese immigration to \"all coloured races\". Labor supported the Reid government of New South Wales in passing the \"Coloured Races Restriction and Regulation Act\", a forerunner of the White Australia Policy. However, after Britain and Japan voiced objections to the legislation, New South Wales, Tasmania and Western Australia instead introduced European language tests to restrict \"undesirable\" immigrants.\nGrowth of nationalism.\nBy the late 1880s, a majority of people living in the Australian colonies were native born, although more than 90 per cent were of British and Irish heritage. The Australian Natives Association, campaigned for an Australian federation within the British Empire, promoted Australian literature and history, and successfully lobbied for the 26 January to be Australia's national day.\nMany nationalists spoke of Australians sharing common blood as members of the British \"race\". Henry Parkes stated in 1890, \"The crimson thread of kinship runs through us all...we must unite as one great Australian people.\"\nA minority of nationalists saw a distinctive Australian identity rather than shared \"Britishness\" as the basis for a unified Australia. Some, such as the radical magazine \"The Bulletin\" and the Tasmanian attorney-general Andrew Inglis Clark, were republicans, while others were prepared to accept a fully independent country of Australia with only a ceremonial role for the British monarch.\nA unified Australia was usually associated with a white Australia. In 1887, \"The Bulletin\" declared that all white men who left the religious and class divisions of the old world behind were Australians. A white Australia also meant the exclusion of cheap Asian labour, an idea strongly promoted by the labour movement.\nThe growing nationalist sentiment in the 1880s and 1890s was associated with the development of a distinctively Australian art and literature. Artists of the Heidelberg School such as Arthur Streeton, Frederick McCubbin and Tom Roberts followed the example of the European Impressionists by painting in the open air. They applied themselves to capturing the light and colour of the Australian landscape and exploring the distinctive and the universal in the \"mixed life of the city and the characteristic life of the station and the bush\".\nIn the 1890s Henry Lawson, Banjo Paterson and other writers associated with \"The Bulletin\" produced poetry and prose exploring the nature of bush life and themes of independence, stoicism, masculine labour, egalitarianism, anti-authoritarianism and mateship. Protagonists were often shearers, boundary riders and itinerant bush workers. In the following decade Lawson, Paterson and other writers such as Steele Rudd, Miles Franklin, and Joseph Furphy helped forge a distinctive national literature. Paterson's ballad \"The Man from Snowy River\" (1890) achieved popularity, and his lyrics to the song \"Waltzing Matilda\" (c.\u20091895) helped make it the unofficial national anthem for many Australians.\nFederation movement.\nGrowing nationalist sentiment coincided with business concerns about the economic inefficiency of customs barriers between the colonies, the duplication of services by colonial governments and the lack of a single national market for goods and services. Colonial concerns about German and French ambitions in the region also led to British pressure for a federated Australian defence force and a unified, single-gauge railway network for defence purposes.\nA Federal Council of Australasia was formed in 1885 but it had few powers and New South Wales and South Australia declined to join.\nAn obstacle to federation was the fear of the smaller colonies that they would be dominated by New South Wales and Victoria. Queensland, in particular, although generally favouring a white Australia policy, wished to maintain an exception for South Sea Islander workers in the sugar cane industry.\nAnother major barrier was the free trade policies of New South Wales which conflicted with the protectionist policies dominant in Victoria and most of the other colonies. Nevertheless, the NSW premier Henry Parkes was a strong advocate of federation and his Tenterfield Oration in 1889 was pivotal in gathering support for the cause.\nIn 1891, a National Australasian Convention was held in Sydney, with all the colonies and New Zealand represented. A draft constitutional Bill was adopted, but the worsening economic depression and opposition in colonial parliaments delayed progress.\nCitizen Federation Leagues were formed, and at a conference in Corowa in July 1893 they developed a new plan for federation involving a constitutional convention with directly elected delegates and a referendum in each colony to endorse the proposed constitution. The new NSW premier, George Reid, endorsed the \"Corowa plan\" and in 1895 convinced the majority of other premiers to adopt it.\nAll of the colonies except Queensland sent representatives to a constitutional convention which held sessions in 1897 and 1898. The convention drafted a proposed constitution for a Commonwealth of federated states under the British Crown.\nReferendums held in 1898 resulted in solid majorities for the constitution in Victoria, South Australia and Tasmania. However, the referendum failed to gain the required majority in New South Wales. The premiers of the other colonies agreed to a number of concessions to New South Wales (particularly that the future Commonwealth capital would be located in that state), and in 1899 further referendums were held in all the colonies except Western Australia. All resulted in yes votes.\nIn March 1900, delegates were dispatched to London, including leading federation advocates Edmund Barton and Alfred Deakin. Following negotiations with the British government, the federation Bill was passed by the imperial parliament on 5 July 1900 and gained Royal Assent on 9 July. Western Australia subsequently voted to join the new federation.\nFrom federation to war (1901\u20141914).\nThe Commonwealth of Australia was proclaimed by the Governor-General, Lord Hopetoun on 1 January 1901, and Barton was sworn in as Australia's first prime minister. The first Federal elections were held in March 1901 and resulted in a narrow plurality for the Protectionist Party over the Free Trade Party with the Australian Labor Party (ALP) polling third. Labor declared it would support the party which offered concessions to its program, and Barton's Protectionists formed a government, with Deakin as Attorney-General.\nThe Immigration Restriction Act 1901 was one of the first laws passed by the new Australian parliament. This centrepiece of the White Australia policy, the act used a dictation test in a European language to exclude Asian migrants, who were considered a threat to Australia's living standards and majority British culture.\nWith federation, the Commonwealth inherited the small defence forces of the six former Australian colonies. By 1901, units of soldiers from all six Australian colonies had been active as part of British forces in the Boer War. When the British government asked for more troops from Australia in early 1902, the Australian government obliged with a national contingent. Some 16,500 men had volunteered for service by the war's end in June 1902.\nIn 1902, the government introduced female suffrage in the Commonwealth jurisdiction, but at the same time excluded Aboriginal people from the franchise unless they already had the vote in a state jurisdiction.\nThe government also introduced a tariff on imports, designed to raise revenue and protect Australian industry. However, disagreements over industrial relations legislation led to the fall of Deakin's Protectionist government in April 1904 and the appointment of the first national Labor government under prime minister Chris Watson. The Watson government itself fell in April and a Free Trade government under prime minister Reid successfully introduced legislation for a Commonwealth Conciliation and Arbitration Court to settle interstate industrial disputes.\nIn July 1905, Deakin formed a Protectionist government with the support of Labor. The new government embarked on a series of social reforms and a program dubbed \"new protection\" under which tariff protection for Australian industries would be linked to their provision of \"fair and reasonable\" wages. In the Harvester case of 1907, H. B. Higgins of the Conciliation and Arbitration Court set a basic wage based on the needs of a male breadwinner supporting a wife and three children. By 1914 the Commonwealth and all the states had introduced systems to settle industrial disputes and fix wages and conditions.\nThe base of the Labor Party was the Australian Trade Union movement which grew from under 100,000 members in 1901 to more than half a million in 1914. The party also drew considerable support from clerical workers, Catholics and small farmers. In 1905, the Labor party adopted objectives at the federal level which included the \"cultivation of an Australian sentiment based upon the maintenance of racial purity\" and \"the collective ownership of monopolies\". In the same year, the Queensland branch of the party adopted an overtly socialist objective.\nAfter the December 1906 elections Deakin's Protectionist government remained in power, but following the passage of legislation for old age pensions and a new protective tariff in 1908, Labor withdrew its support for the government. In November, Andrew Fisher became the second Labor prime minister. In response, opposition parties formed an anti-Labor coalition and Deakin became prime minister in June 1909.\nIn the elections of May 1910, Labor won a majority in both houses of parliament and Fisher again became prime minister. The Labor government introduced a series of reforms including a progressive land tax (1910), invalid pensions (1910) and a maternity allowance (1912). The government established the Commonwealth Bank (1911) but referendums to nationalise monopolies and extend Commonwealth trade and commerce powers were defeated in 1911 and 1913. The Commonwealth took over responsibility for the Northern Territory from South Australia in 1911. The government increased defence spending, expanding the system of compulsory military training which had been introduced by the previous government and establishing the Royal Australian Navy.\nThe new Commonwealth Liberal Party won the May 1913 elections and former Labor leader Joseph Cook became prime minister. The Cook government's attempt to pass legislation abolishing preferential treatment for union members in the Commonwealth Public Service triggered a double dissolution of parliament. Labor comfortably won the September 1914 elections and Fisher resumed office.\nThe prewar period saw strong growth in the population and economy. The economy grew by 75 per cent, with rural industries, construction, manufacturing and government services leading the way. The population increased from four million in 1901 to five million in 1914. From 1910 to 1914 just under 300,000 migrants arrived, all white, and almost all from Britain.\nWorld War I.\nAustralia at war (1914\u201318).\nWhen the United Kingdom declared war on Germany on 4 August 1914, the declaration automatically involved all of Britain's colonies and dominions. Both major parties offered Britain 20,000 Australian troops. As the \"Defence Act 1903\" precluded sending conscripts overseas, a new volunteer force, the Australian Imperial Force (AIF), was raised to meet this commitment.\nPublic enthusiasm for the war was high, and the initial quota for the AIF was quickly filled. The troops left for Egypt on 1 November 1914, one of the escort ships, HMAS \"Sydney\", sinking the German cruiser \"Emden\" along the way. Meanwhile, in September, a separate Australian expeditionary force had captured German New Guinea.\nAfter arriving in Egypt, the AIF was incorporated into an Australian and New Zealand Army Corps (ANZAC). The Anzacs formed part of the Mediterranean Expeditionary Force with the task of opening the Dardanelles to allied battleships, threatening Constantinople, the capital of the Ottoman Empire which had entered the war on the side of the Central Powers. The Anzacs, along with French, British and Indian troops, landed on the Gallipoli peninsula on 25 April 1915. The Australian and New Zealand position at Anzac Cove was vulnerable to attack and the troops suffered heavy losses in establishing a narrow beachhead. After it had become clear that the expeditionary force would be unable to achieve its objectives, the Anzacs were evacuated in December, followed by the British and French in early January.\nThe Australians suffered about 8,000 deaths in the campaign. Australian war correspondents variously emphasised the bravery and fighting qualities of the Australians and the errors of their British commanders. The 25 April soon became an Australian national holiday known as Anzac Day, centring on themes of \"nationhood, brotherhood and sacrifice\".\nIn 1916, five infantry divisions of the AIF were sent to the Western Front. In July 1916, at Fromelles, the AIF suffered 5,533 casualties in 24 hours, the most costly single encounter in Australian military history. Elsewhere on the Somme, 23,000 Australians were killed or wounded in seven weeks of attacks on German positions. In Spring 1917, Australian troops suffered 10,000 casualties at the First Battle of Bullecourt and the Second Battle of Bullecourt. In the summer and autumn of 1917, Australian troops also sustained heavy losses during the British offensive around Ypres. Overall, almost 22,000 Australian troops were killed in 1917.\nIn November 1917 the five Australian divisions were united in the Australian Corps, and in May 1918 the Australian general John Monash took over command. The Australian Corps was heavily involved in halting the German Spring Offensive of 1918 and in the allied counter-offensive of August that year.\nIn the Middle East, the Australian Light Horse brigades were prominent at the Battle of Romani in August 1916. In 1917, they participated in the allied advance through the Sinai Peninsula and into Palestine. In 1918, they pressed on through Palestine and into Syria in an advance that led to the Ottoman surrender on 31 October.\nBy the time the war ended on 11 November 1918, 324,000 Australians had served overseas. Casualties included 60,000 dead and 150,000 wounded\u2014the highest casualty rate of any allied force. Australian troops also had higher rates of unauthorised absence, crime and imprisonment than other allied forces.\nHome front.\nIn October 1914, the Fisher Labor government introduced the \"War Precautions Act\" which gave it the power to make regulations \"for securing the public safety and defence of the Commonwealth\". After Billy Hughes replaced Fisher as prime minister in October 1915, regulations under the act were increasingly used to censor publications, penalise public speech and suppress organisations that the government considered detrimental to the war effort. Anti-German leagues were formed and 7,000 Germans and other \"enemy aliens\" were sent to internment camps during the war.\nThe economy contracted by 10 per cent during the course of hostilities. Inflation rose in the first two years of war and real wages fell. Lower wages and perceptions of profiteering by some businesses led, in 1916, to a wave of strikes by miners, waterside workers and shearers.\nEnlistments in the military also declined, falling from 35,000 a month at its peak in 1915 to 6,000 a month in 1916. In response, Hughes decided to hold a referendum on conscription for overseas service. Following the narrow defeat of the October 1916 conscription referendum, Hughes and 23 of his supporters left the parliamentary Labor party and formed a new Nationalist government with the former opposition. The Nationalists comfortably won the May 1917 elections and Hughes continued as prime minister.\nFrom August to October 1917 there was a major strike of New South Wales railway, transport, waterside and coal workers which was defeated after the Commonwealth and New South Wales governments arrested strike leaders and organised special constables and non-union labour. A second referendum on conscription was also defeated in December. Enlistments in 1918 were the lowest for the war, leading to the disbandment of 12 battalions and mutinies in the AIF.\nParis peace conference.\nHughes attended the Imperial War Conference and Imperial War Cabinet in London from June 1918 where Australia, New Zealand, Canada and South Africa won British support for their separate representation at the eventual peace conference. At the Paris Peace Conference in 1919, Hughes argued that Germany should pay the full cost of the war, but ultimately gained only \u00a35 million in war reparations for Australia. Australia and the other self-governing British dominions won the right to become full members of the new League of Nations, and Australia obtained a special League of Nations mandate over German New Guinea allowing Australia to control trade and immigration. Australia also gained a 42 per cent share of the formerly German-ruled island of Nauru, giving access to its rich superphosphate reserves. Australia argued successfully against a Japanese proposal for a racial equality clause in the League of Nations covenant, as Hughes feared that it would jeopardise the White Australia policy. As a signatory to the Treaty of Versailles and a full member of the League of Nations, Australia took an important step towards international recognition as a sovereign nation.\nInter-war years.\nMen, money and markets (1920s).\nAfter the war, Prime Minister Billy Hughes led a new conservative force, the Nationalist Party, formed from the old Liberal party and breakaway elements of Labor (of which he was the most prominent), after the deep and bitter split over Conscription. An estimated 12,000 Australians died as a result of the Spanish flu pandemic of 1919, almost certainly brought home by returning soldiers.\nThe success of the Bolshevik Revolution in Russia posed a threat in the eyes of many Australians, although to a small group of socialists it was an inspiration. The Communist Party of Australia was formed in 1920 and, though remaining electorally insignificant, it obtained some influence in the trade union movement and was banned during World War II for its support for the Molotov\u2013Ribbentrop Pact and the Menzies Government unsuccessfully tried to ban it again during the Korean War. Despite splits, the party remained active until its dissolution at the end of the Cold War.\nThe Country Party (today's National Party) formed in 1920 to promulgate its version of agrarianism, which it called \"Countrymindedness\". The goal was to enhance the status of the graziers (operators of big sheep ranches) and small farmers, and secure subsidies for them. Enduring longer than any other major party save the Labor party, it has generally operated in coalition with the Liberal Party (since the 1940s), becoming a major party of government in Australia\u2014particularly in Queensland.\nOther significant after-effects of the war included ongoing industrial unrest, which included the 1923 Victorian Police strike. Industrial disputes characterised the 1920s in Australia. Other major strikes occurred on the waterfront, in the coalmining and timber industries in the late 1920s. The union movement had established the Australian Council of Trade Unions (ACTU) in 1927 in response to the Nationalist government's efforts to change working conditions and reduce the power of the unions.\nThe consumerism, entertainment culture, and new technologies that characterised the 1920s in the United States were also found in Australia. Prohibition was not implemented in Australia, though anti-alcohol forces were successful in having hotels closed after 6\u00a0pm, and closed altogether in a few city suburbs.\nThe fledgling film industry declined through the decade, despite more than 2\u00a0million Australians attending cinemas weekly at 1250 venues. A Royal Commission in 1927 failed to assist and the industry that had begun so brightly with the release of the world's first feature film, The Story of the Kelly Gang (1906), atrophied until its revival in the 1970s.\nStanley Bruce became prime minister in 1923, when members of the Nationalist Party Government voted to remove W.M. Hughes. Speaking in early 1925, Bruce summed up the priorities and optimism of many Australians, saying that \"men, money and markets accurately defined the essential requirements of Australia\" and that he was seeking such from Britain. The migration campaign of the 1920s, operated by the Development and Migration Commission, brought almost 300,000 Britons to Australia, although schemes to settle migrants and returned soldiers \"on the land\" were generally not a success. \"The new irrigation areas in Western Australia and the Dawson Valley of Queensland proved disastrous\".\nIn Australia, the costs of major investment had traditionally been met by state and Federal governments and heavy borrowing from overseas was made by the governments in the 1920s. A Loan Council was set up in 1928 to co-ordinate loans, three-quarters of which came from overseas. Despite Imperial Preference, a balance of trade was not successfully achieved with Britain. \"In the five years from 1924. .. to ... 1928, Australia bought 43.4% of its imports from Britain and sold 38.7% of its exports. Wheat and wool made up more than two-thirds of all Australian exports\", a dangerous reliance on just two export commodities.\nAustralia embraced the new technologies of transport and communication. Coastal sailing ships were finally abandoned in favour of steam, and improvements in rail and motor transport heralded dramatic changes in work and leisure. In 1918, there were 50,000 cars and lorries in the whole of Australia. By 1929 there were 500,000. The stage coach company Cobb and Co, established in 1853, finally closed in 1924. In 1920, the Queensland and Northern Territory Aerial Service (to become the Australian airline Qantas) was established. The Reverend John Flynn, founded the Royal Flying Doctor Service, the world's first air ambulance in 1928. Daredevil pilot, Sir Charles Kingsford Smith pushed the new flying machines to the limit, completing a round Australia circuit in 1927 and in 1928 traversed the Pacific Ocean, via Hawaii and Fiji from the US to Australia in the aircraft \"Southern Cross\". He went on to global fame and a series of aviation records before vanishing on a night flight to Singapore in 1935.\nDominion status.\nAustralia achieved independent Sovereign Nation status after World War I, under the Statute of Westminster. This formalised the Balfour Declaration of 1926, a report resulting from the 1926 Imperial Conference of British Empire leaders in London, which defined Dominions of the British empire in the following way: \"They are autonomous Communities within the British Empire, equal in status, in no way subordinate one to another in any aspect of their domestic or external affairs, though united by a common allegiance to the Crown, and freely associated as members of the British Commonwealth of Nations.\"; however, Australia did not ratify the Statute of Westminster until 1942. According to historian Frank Crowley, this was because Australians had little interest in redefining their relationship with Britain until the crisis of World War II.\nThe Australia Act 1986 removed any remaining links between the British Parliament and the Australian states.\nFrom 1 February 1927 until 12 June 1931, the Northern Territory was divided up as North Australia and Central Australia at latitude 20\u00b0S. New South Wales has had one further territory surrendered, namely Jervis Bay Territory comprising 6,677 hectares, in 1915. The external territories were added: Norfolk Island (1914); Ashmore Island, Cartier Islands (1931); the Australian Antarctic Territory transferred from Britain (1933); Heard Island, McDonald Islands, and Macquarie Island transferred to Australia from Britain (1947).\nThe Federal Capital Territory (FCT) was formed from New South Wales in 1911 to provide a location for the proposed new federal capital of Canberra (Melbourne was the seat of government from 1901 to 1927). The FCT was renamed the Australian Capital Territory (ACT) in 1938. The Northern Territory was transferred from the control of the South Australian government to the Commonwealth in 1911.\nGreat Depression.\nWell before the Wall Street crash of 1929, the Australian economy was facing high overseas debt and a slowing economy. As the economy slowed in 1927, so did manufacturing and the country slipped into recession as profits slumped and unemployment rose. Australia was deeply affected by the Great Depression of the 1930s, particularly due to its heavy dependence on exports of wool and wheat, and on borrowing from London. The world prices of wool and wheat halved, and lending ended.\nAt elections held in October 1929, the Labor Party was swept into power in a landslide victory; Stanley Bruce, the former prime minister, lost his own seat. The new prime minister, James Scullin, and his largely inexperienced government were immediately faced with a series of crises. Hamstrung by their lack of control of the Senate, a lack of control of the banking system and divisions within their party about how best to deal with the situation, the government was forced to accept solutions that eventually split the party. Some gravitated to New South Wales Premier Lang, others to Prime Minister Scullin.\nVarious \"plans\" to resolve the crisis were suggested; Sir Otto Niemeyer, a representative of the English banks who visited in mid-1930, proposed a deflationary plan, involving cuts to government spending and wages. Treasurer Ted Theodore proposed a mildly inflationary plan, while the Labor Premier of New South Wales, Jack Lang, proposed a radical plan which repudiated overseas debt. The \"Premier's Plan\" finally accepted by federal and state governments in June 1931, followed the deflationary model advocated by Niemeyer and included a reduction of 20 per cent in government spending, a reduction in bank interest rates and an increase in taxation. In March 1931, Lang announced that interest due in London would not be paid and the Federal government stepped in to meet the debt. In May, the Government Savings Bank of New South Wales was forced to close. The Melbourne Premiers' Conference agreed to cut wages and pensions as part of a severe deflationary policy but Lang renounced the plan. The grand opening of the Sydney Harbour Bridge in 1932 provided little respite to the growing crisis straining the young federation. With multimillion-pound debts mounting, public demonstrations and move and counter-move by Lang and then Scullin, then Lyons federal governments, the governor of New South Wales, Philip Game, had been examining Lang's instruction not to pay money into the Federal Treasury. Game judged it was illegal. Lang refused to withdraw his order and, on 13 May, he was dismissed by Governor Game. At June elections, Lang Labor's seats collapsed.\nMay 1931 had seen the creation of a new conservative political force, the United Australia Party formed by breakaway members of the Labor Party combining with the Nationalist Party. At Federal elections in December 1931, the United Australia Party, led by former Labor member Joseph Lyons, easily won office. They remained in power until September 1940. The Lyons government has often been credited with steering recovery from the depression, although just how much of this was owed to their policies remains contentious. Stuart Macintyre also points out that although Australian GDP grew from \u00a3386.9\u00a0million to \u00a3485.9\u00a0million between 1931 and 1932 and 1938\u201339, real domestic product per head of population was still \"but a few shillings greater in 1938\u201339 (\u00a370.12), than it had been in 1920\u201321 (\u00a370.04).\"\nAustralia recovered relatively quickly from the financial downturn of 1929\u20131930, with recovery beginning around 1932. The prime minister, Joseph Lyons, favoured the tough economic measures of the Premiers' Plan, pursued an orthodox fiscal policy and refused to accept the proposals of the premier of New South Wales, Jack Lang, to default on overseas debt repayments. According to author Anne Henderson of the Sydney Institute, Lyons held a steadfast belief in \"the need to balance budgets, lower costs to business and restore confidence\" and the Lyons period gave Australia \"stability and eventual growth\" between the drama of the Depression and the outbreak of the Second World War. A lowering of wages was enforced and industry tariff protections maintained, which together with cheaper raw materials during the 1930s saw a shift from agriculture to manufacturing as the chief employer of the Australian economy\u2014a shift which was consolidated by increased investment by the commonwealth government into defence and armaments manufacture. Lyons saw restoration of Australia's exports as the key to economic recovery.\nThe extent of unemployment in Australia, often cited as peaking at 29 per cent in 1932 is debated. \"Trade union figures are the most often quoted, but the people who were there...regard the figures as wildly understating the extent of unemployment\" wrote historian Wendy Lowenstein in her collection of oral histories of the depression; however, David Potts argued that \"over the last thirty years ...historians of the period have either uncritically accepted that figure (29% in the peak year 1932) including rounding it up to 'a third', or they have passionately argued that a third is far too low.\" Potts himself though suggested a peak national figure of 25 per cent unemployed. Measurement is difficult in part because there was great variation, geographically, by age and by gender, in the level of unemployment. Statistics collected by historian Peter Spearritt show 17.8 per cent of men and 7.9 per cent of women unemployed in 1933 in the comfortable Sydney suburb of Woollahra. This is not to say that 81.9 per cent of women were working but that 7.9 per cent of the women interested/looking for work were unable to find it, a much lower figure than maybe first thought, as many women stayed home and were not in the job force in those years, especially if they were unable to find work.\nIn the working class suburb of Paddington, 41.3 per cent of men and 20.7 per cent of women were listed as unemployed. Geoffrey Spenceley stated that apart from variation between men and women, unemployment was also much higher in some industries, such as the building and construction industry, and comparatively low in the public administrative and professional sectors.\nIn country areas, worst hit were small farmers in the wheat belts as far afield as north-east Victoria and Western Australia, who saw more and more of their income absorbed by interest payments.\nExtraordinary sporting successes did something to alleviate the spirits of Australians during the economic downturn. In a Sheffield Shield cricket match at the Sydney Cricket Ground in 1930, Don Bradman, a young New South Welshman of just 21 years of age wrote his name into the record books by smashing the previous highest batting score in first-class cricket with 452 runs not out in just 415 minutes. The rising star's world beating cricketing exploits were to provide Australians with much needed joy through the emerging Great Depression in Australia and post-World War II recovery. Between 1929 and 1931 the racehorse Phar Lap dominated Australia's racing industry, at one stage winning fourteen races in a row. Famous victories included the 1930 Melbourne Cup, following an assassination attempt and carrying 9 stone 12 pounds weight. Phar Lap sailed for the United States in 1931, going on to win North America's richest race, the Agua Caliente Handicap in 1932. Soon after, on the cusp of US success, Phar Lap developed suspicious symptoms and died. Theories swirled that the champion race horse had been poisoned and a devoted Australian public went into shock. The 1938 British Empire Games were held in Sydney from 5\u201312 February, timed to coincide with Sydney's sesqui-centenary (150 years since the foundation of British settlement in Australia).\nIndigenous policy.\nFollowing federation Aboriginal affairs was a state responsibility, although the Commonwealth became responsible for the Aboriginal population of the Northern Territory from 1911. By that date the Commonwealth and all states except Tasmania had passed legislation establishing Protectors of Aborigines and Protection Boards with extensive powers to regulate the lives of Aboriginal Australians including their ownership of property, place of residence, employment, sexual relationships and custody of their children. Reserves were established, ostensibly for the protection of the Aboriginal population who had been dispossessed of their land. Church groups also ran missions throughout Australia providing shelter, food, religious instruction and elementary schooling for Indigenous people.\nSome officials were concerned by the growing number of Aboriginal children of mixed heritage, particularly in northern Australia where large Indigenous, South Sea Islander and Asian populations were seen as inconsistent with the white Australia policy. Laws concerning Aboriginal Australians were progressively tightened to make it easier for officials to remove Aboriginal children of mixed descent from their parents and place them in reserves, missions, institutions and employment with white employers.\nThe segregation of Aboriginal people on reserves and in institutions was never systematically accomplished due to funding constraints, differing policy priorities in the states and territories, and resistance from Aboriginal people. In the more densely settled areas of Australia, about 20 per cent of Aboriginal people lived on reserves in the 1920s. The majority lived in camps on the fringes of country towns and a small percentage lived in cities. During the Great Depression more Aboriginal people moved to reserves and missions for food and shelter. By 1941 almost half of the Aboriginal population of New South Wales lived on reserves.\nIn northern Australia, the majority of employed Aboriginal people worked in the pastoral industry where they lived in camps, often with their extended families. Many also camped on the margins of towns and reserves where they could avoid most of the controls imposed by the administrators of reserves, compounds and missions.\nThe 1937 Native Welfare conference of state and Commonwealth officials endorsed a policy of biological absorption of mixed-descent Aboriginal Australians into the white community.[T]he destiny of the natives of aboriginal origin, but not of the full blood, lies in their ultimate absorption by the people of the Commonwealth and it therefore recommends that all efforts be directed to that end.The officials saw the policy of Aboriginal assimilation by absorption into the white community as progressive, aimed at eventually achieving civil and economic equality for mixed-descent Aboriginal people... efforts of all State authorities should be directed towards the education of children of mixed aboriginal blood at white standards, and their subsequent employment under the same conditions as whites with a view to their taking their place in the white community on an equal footing with the whites.The following decades saw an increase in the number of Aboriginal Australians of mixed descent removed from their families, although the states and territories progressively adopted a policy of cultural, rather than biological, assimilation, and justified removals on the grounds of child welfare. In 1940, New South Wales became the first state to introduce a child welfare model whereby Aboriginal children of mixed descent were removed from their families under general welfare provisions by court order. Other jurisdictions introduced a welfare model after the war.\nWorld War II.\nDefence policy in the 1930s.\nUntil the late 1930s, defence was not a significant issue for Australians. At the 1937 elections, both political parties advocated increased defence spending, in the context of increased Japanese aggression in China and Germany's aggression in Europe; however, there was a difference in opinion about how the defence spending should be allocated. The United Australia Party government emphasised co-operation with Britain in \"a policy of imperial defence\". The lynchpin of this was the British naval base at Singapore and the Royal Navy battle fleet \"which, it was hoped, would use it in time of need\". Defence spending in the inter-war years reflected this priority. In the period 1921\u20131936 totalled \u00a340\u00a0million on the Royal Australian Navy, \u00a320\u00a0million on the Australian Army and \u00a36\u00a0million on the Royal Australian Air Force (established in 1921, the \"youngest\" of the three services). In 1939, the Navy, which included two heavy cruisers and four light cruisers, was the service best equipped for war.\nFearing Japanese intentions in the Pacific, Menzies established independent embassies in Tokyo and Washington to receive independent advice about developments. Gavin Long argues that the Labor opposition urged greater national self-reliance through a build-up of manufacturing and more emphasis on the Army and RAAF, as Chief of the General Staff, John Lavarack also advocated. In November 1936, Labor leader John Curtin said \"The dependence of Australia upon the competence, let alone the readiness, of British statesmen to send forces to our aid is too dangerous a hazard upon which to found Australia's defence policy.\" According to John Robertson, \"some British leaders had also realised that their country could not fight Japan and Germany at the same time.\" But \"this was never discussed candidly at...meeting(s) of Australian and British defence planners\", such as the 1937 Imperial Conference.\nBy September 1939 the Australian Army numbered 3,000 regulars. A recruiting campaign in late 1938, led by Major-General Thomas Blamey increased the reserve militia to almost 80,000. The first division raised for war was designated the 6th Division, of the 2nd AIF, there being 5 Militia Divisions on paper and a 1st AIF in the First World War.\nWar.\nOn 3 September 1939, the prime minister, Robert Menzies, made a national radio broadcast: \"My fellow Australians. It is my melancholy duty to inform you, officially, that, in consequence of the persistence by Germany in her invasion of Poland, Great Britain has declared war upon her, and that, as a result, Australia is also at war.\"\nThus began Australia's involvement in the six-year global conflict. Australians were to fight in an extraordinary variety of locations, including withstanding the advance of German Panzers in the Siege of Tobruk, turning back the advance of the Imperial Japanese Army in the New Guinea Campaign, undertaking bomber missions over Europe, engaging in naval battles in the Mediterranean. At home, Japanese attacks included mini-submarine raids on Sydney Harbour and very heavy air raids on and near the Northern Territory's capital, Darwin.\nThe recruitment of a volunteer military force for service at home and abroad was announced, the 2nd Australian Imperial Force and a citizen militia organised for local defence. Troubled by Britain's failure to increase defences at Singapore, Menzies was cautious in committing troops to Europe. By the end of June 1940, France, Norway, Denmark and the Low Countries had fallen to Nazi Germany. Britain stood alone with its dominions. Menzies called for \"all-out war\", increasing federal powers and introducing conscription. Menzies' minority government came to rely on just two independents after the 1940 election.\nIn January 1941, Menzies flew to Britain to discuss the weakness of Singapore's defences. Arriving in London during The Blitz, Menzies was invited into Winston Churchill's British War Cabinet for the duration of his visit. Returning to Australia, with the threat of Japan imminent and with the Australian army suffering badly in the Greek and Crete campaigns, Menzies re-approached the Labor Party to form a War Cabinet. Unable to secure their support, and with an unworkable parliamentary majority, Menzies resigned as prime minister. The coalition held office for another month, before the independents switched allegiance and John Curtin was sworn in as prime minister. Eight weeks later, Japan attacked Pearl Harbor.\nFrom 1940 to 1941, Australian forces played prominent roles in the fighting in the Mediterranean theatre, including Operation Compass, the Siege of Tobruk, the Greek campaign, the Battle of Crete, the Syria\u2013Lebanon Campaign and the Second Battle of El Alamein.\nA garrison of around 14,000 Australian soldiers, commanded by Lieutenant General Leslie Morshead was besieged in Tobruk, Libya, by the German-Italian army of General Erwin Rommel between April and August 1941. The Nazi propagandist Lord Haw Haw derided the defenders as 'rats', a term the soldiers adopted as an ironic compliment: \"The Rats of Tobruk\". Vital in the defence of Egypt and the Suez Canal, the siege saw the advance of the German army halted for the first time and provided a morale boost for the British Commonwealth, which was then standing alone against Hitler.\nThe war came closer to home when was lost with all hands in battle with the German raider \"Kormoran\" in November 1941.\nWith most of Australia's best forces committed to fight against Hitler in the Middle East, Japan attacked Pearl Harbor, the US naval base in Hawaii, on 8 December 1941 (eastern Australia time). The British battleship and battlecruiser sent to defend Singapore were sunk soon afterwards. Australia was ill-prepared for an attack, lacking armaments, modern fighter aircraft, heavy bombers, and aircraft carriers. While demanding reinforcements from Churchill, on 27 December 1941 Curtin published an historic announcement: \"The Australian Government... regards the Pacific struggle as primarily one in which the United States and Australia must have the fullest say in the direction of the democracies' fighting plan. Without inhibitions of any kind, I make it clear that Australia looks to America, free of any pangs as to our traditional links or kinship with the United Kingdom.\"\nBritish Malaya quickly collapsed, shocking the Australian nation. British, Indian and Australian troops made a disorganised last stand at Singapore, before surrendering on 15 February 1942. Around 15,000 Australian soldiers became prisoners of war. Curtin predicted that the \"battle for Australia\" would now follow. On 19 February, Darwin suffered a devastating air raid, the first time the Australian mainland had ever been attacked by enemy forces. For the following 19 months, Australia was attacked from the air almost 100 times.\nTwo battle-hardened Australian divisions were already steaming from the Middle East for Singapore. Churchill wanted them diverted to Burma, but Curtin refused, and anxiously awaited their return to Australia. US president Franklin D. Roosevelt ordered his commander in the Philippines, General Douglas MacArthur, to formulate a Pacific defence plan with Australia in March 1942. Curtin agreed to place Australian forces under the command of General MacArthur, who became \"Supreme Commander of the South West Pacific\". Curtin had thus presided over a fundamental shift in Australia's foreign policy. MacArthur moved his headquarters to Melbourne in March 1942 and American troops began massing in Australia. In late May 1942, Japanese midget submarines sank an accommodation vessel in a daring raid on Sydney Harbour. On 8 June 1942, two Japanese submarines briefly shelled Sydney's eastern suburbs and the city of Newcastle.\nIn an effort to isolate Australia, the Japanese planned a seaborne invasion of Port Moresby, in the Australian Territory of New Guinea. In May 1942, the US Navy engaged the Japanese in the Battle of the Coral Sea and halted the attack. The Battle of Midway in June effectively defeated the Japanese navy and the Japanese army launched a land assault on Moresby from the north. Between July and November 1942, Australian forces repulsed Japanese attempts on the city by way of the Kokoda Track, in the highlands of New Guinea. The Battle of Milne Bay in August 1942 was the first Allied defeat of Japanese land forces.\nMeanwhile, in North Africa, the Axis powers had driven Allies back into Egypt. A turning point came between July and November 1942, when Australia's 9th Division played a crucial role in some of the heaviest fighting of the First and Second Battle of El Alamein, which turned the North Africa Campaign in favour of the Allies.\nThe Battle of Buna\u2013Gona, between November 1942 and January 1943, set the tone for the bitter final stages of the New Guinea campaign, which persisted into 1945. The offensives in Papua and New Guinea of 1943\u201344 were the single largest series of connected operations ever mounted by the Australian armed forces. On 14 May 1943, the Australian Hospital Ship \"Centaur\", though clearly marked as a medical vessel, was sunk by Japanese raiders off the Queensland coast, killing 268, including all but one of the nursing staff, further enraging popular opinion against Japan.\nAustralian prisoners of war were at this time suffering severe ill-treatment in the Pacific Theatre. In 1943, 2,815 Australian Pows died constructing Japan's Burma-Thailand Railway. In 1944, the Japanese inflicted the Sandakan Death March on 2,000 Australian and British prisoners of war\u2014only 6 survived. This was the single worst war crime perpetrated against Australians in war.\nMacArthur largely excluded Australian forces from the main push north into the Philippines and Japan. It was left to Australia to lead amphibious assaults against Japanese bases in Borneo. Curtin suffered from ill health from the strains of office and died weeks before the war ended, replaced by Ben Chifley.\nOf Australia's wartime population of seven million, almost one million men and women served in a branch of the services during the six years of warfare. By war's end, gross enlistments totalled 727,200 men and women in the Australian Army (of whom 557,800 served overseas), 216,900 in the RAAF and 48,900 in the RAN. More than 39,700 were killed or died as prisoners of war, about 8,000 of whom died as prisoners of the Japanese.\nAustralian home front.\nWhile the Australian civilian population suffered less at the hands of the Axis powers than did other Allied nations in Asia and Europe, Australia nevertheless came under direct attack by Japanese naval forces and aerial bombardments, particularly through 1942 and 1943, resulting in hundreds of fatalities and fuelling fear of Japanese invasion. Axis naval activity in Australian waters also brought the war close to home for Australians. Austerity measures, rationing and labour controls measures were all implemented to assist the war effort. Australian civilians dug air raid shelters, trained in civil defence and first aid, and Australian ports and cities were equipped with anti-aircraft and sea defences.\nThe Australian economy was markedly affected by World War II. Expenditure on war reached 37 per cent of GDP by 1943\u201344, compared to 4 per cent expenditure in 1939\u20131940. Total war expenditure was \u00a32,949\u00a0million between 1939 and 1945.\nAlthough the peak of army enlistments occurred in June\u2013July 1940, when more than 70,000 enlisted, it was the Curtin Labor government, formed in October 1941, that was largely responsible for \"a complete revision of the whole Australian economic, domestic and industrial life\". Rationing of fuel, clothing and some food was introduced, (although less severely than in Britain) Christmas holidays curtailed, \"brown outs\" introduced and some public transport reduced. From December 1941, the Government evacuated all women and children from Darwin and northern Australia, and more than 10,000 refugees arrived from South East Asia as Japan advanced. In January 1942, the Manpower Directorate was set up \"to ensure the organisation of Australians in the best possible way to meet all defence requirements.\" Minister for War Organisation of Industry, John Dedman introduced a degree of austerity and government control previously unknown, to such an extent that he was nicknamed \"the man who killed Father Christmas\".\nIn May 1942 uniform tax laws were introduced in Australia, ending state governments' control of income taxation. \"The significance of this decision was greater than any other... made throughout the war, as it added extensive powers to the Federal Government and greatly reduced the financial autonomy of the states.\"\nManufacturing grew significantly because of the war. \"In 1939, there were only three Australian firms producing machine tools, but by 1943 there were more than one hundred doing so.\" From having few front line aircraft in 1939, the RAAF had become the fourth largest allied Air force by 1945. A number of aircraft were built under licence in Australia before the war's end, notably the Beaufort and Beaufighter, although the majority of aircraft were from Britain and later, the US. The Boomerang fighter, designed and built in four months of 1942, emphasised the desperate state Australia found itself in as the Japanese advanced.\nAustralia also created, virtually from nothing, a significant female workforce engaged in direct war production. Between 1939 and 1944 the number of women working in factories rose from 171,000 to 286,000. Dame Enid Lyons, widow of former prime minister Joseph Lyons, became the first woman elected to the House of Representatives in 1943, joining the Robert Menzies' new centre-right Liberal Party of Australia, formed in 1945. At the same election, Dorothy Tangney became the first woman elected to the Senate.\nPost-war boom.\nMenzies and Liberal dominance (1949\u201372).\nPolitically, Robert Menzies and the Liberal Party of Australia dominated much of the immediate post war era, defeating the Labor government of Ben Chifley in 1949, in part because of a Labor proposal to nationalise banks and following a crippling coal strike led by the Australian Communist Party. Menzies became the country's longest-serving prime minister and the Liberal party, in coalition with the rural based Country Party, won every federal election until 1972.\nAs in the United States in the early 1950s, allegations of communist influence in society saw tensions emerge in politics. Refugees from Soviet dominated Eastern Europe immigrated to Australia, while to Australia's north, Mao Zedong's Chinese Communist Party won the Chinese Civil War in 1949 and in June 1950, Communist North Korea invaded South Korea. The Menzies government responded to a United States led United Nations Security Council request for military aid for South Korea and diverted forces from occupied Japan to begin Australia's involvement in the Korean War. After fighting to a bitter standstill, the UN and North Korea signed a ceasefire agreement in July 1953. Australian forces had participated in such major battles as Kapyong and Maryang San. 17,000 Australians had served and casualties amounted to more than 1,500, of whom 339 were killed.\nDuring the course of the Korean War, the Liberal government attempted to ban the Communist Party of Australia, first by legislation in 1950 and later by referendum, in 1951. While both attempts were unsuccessful, further international events such as the defection of minor Soviet Embassy official Vladimir Petrov, added to a sense of impending threat that politically favoured Menzies' Liberal-CP government, as the Labor Party split over concerns about the influence of the Communist Party on the trade union movement. The tensions led to another bitter split and the emergence of the breakaway Democratic Labor Party (DLP). The DLP remained an influential political force, often holding the balance of power in the Senate, until 1974. Its preferences supported the Liberal and Country Party. The Labor party was led by H.V. Evatt after Chifley's death in 1951. Evatt had served as President of the United Nations General Assembly during 1948\u201349 and helped draft the United Nations Universal Declaration of Human Rights (1948). Evatt retired in 1960 amid signs of mental ill-health, and Arthur Calwell succeeded him as leader, with a young Gough Whitlam as his deputy.\nMenzies presided during a period of sustained economic boom and the beginnings of sweeping social change, which included youth culture and its rock and roll music and, in the late 1950s, the arrival of television broadcasting. In 1958, Australian country music singer Slim Dusty, who would become the musical embodiment of rural Australia, had Australia's first international music chart hit with his bush ballad \"Pub With No Beer\", while rock and roller Johnny O'Keefe's \"Wild One\" became the first local recording to reach the national charts, peaking at No. 20. Australian cinema produced little of its own content in the 1950s, but British and Hollywood studios produced a string of successful epics from Australian literature, featuring home grown stars Chips Rafferty and Peter Finch.\nMenzies remained a staunch supporter of links to the monarchy and Commonwealth of Nations and formalised an alliance with the United States, but also launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner.\nWhen Menzies retired in 1965, he was replaced as Liberal leader and prime minister by Harold Holt. Holt drowned while swimming at a surf beach in December 1967 and was replaced by John Gorton (1968\u20131971) and then by William McMahon (1971\u20131972).\nPost-war immigration.\nFollowing World War II, the Chifley Labor government instigated a massive programme of European immigration. In 1945, Minister for Immigration, Arthur Calwell wrote \"If the experience of the Pacific War has taught us one thing, it surely is that seven million Australians cannot hold three million square miles of this earth's surface indefinitely.\" All political parties shared the view that the country must \"populate or perish\". Calwell stated a preference for ten British immigrants for each one from other countries; however, the numbers of British migrants fell short of what was expected, despite government assistance.\nMigration brought large numbers of southern and central Europeans to Australia for the first time. A 1958 government leaflet assured readers that unskilled non-British migrants were needed for \"labour on rugged projects ... work which is not generally acceptable to Australians or British workers\". The Australian economy stood in sharp contrast to war-ravaged Europe, and newly arrived migrants found employment in a booming manufacturing industry and government assisted programmes such as the Snowy Mountains Scheme. This hydroelectricity and irrigation complex in south-east Australia consisted of sixteen major dams and seven power stations constructed between 1949 and 1974. It remains the largest engineering project undertaken in Australia. Necessitating the employment of 100,000 people from more than 30 countries, to many it denoted the birth of multicultural Australia.\nSome 4.2\u00a0million immigrants arrived between 1945 and 1985, about 40 per cent of whom came from Britain and Ireland. The 1957 novel \"They're a Weird Mob\" was a popular account of an Italian migrating to Australia, although written by Australian-born author John O'Grady. The Australian population reached 10 million in 1959\u2013with Sydney its most populous city.\nIn May 1958, the Menzies Government passed the Migration Act 1958 which replaced the Immigration Restriction Act's arbitrarily applied dictation test with an entry permit system, that reflected economic and skills criteria. Further changes in the 1960s effectively ended the White Australia policy. It legally ended in 1973.\nEconomic growth and suburban living.\nAustralia enjoyed significant growth in prosperity in the 1950s and 1960s, with increases in both living standards and in leisure time. The manufacturing industry, previously playing a minor part in an economy dominated by primary production, greatly expanded. The first Holden motor car came out of General Motors-Holden's Fisherman's Bend factory in November 1948. Car ownership rapidly increased\u2014from 130 owners in every 1,000 in 1949 to 271 owners in every 1,000 by 1961. By the early 1960s, four competitors to Holden had set up Australian factories, employing between 80,000 and 100,000 workers, \"at least four-fifths of them migrants\".\nIn the 1960s, about 60 per cent of Australian manufacturing was protected by tariffs. Pressure from business interests and the union movement ensured these remained high. Historian Geoffrey Bolton suggests that this high tariff protection of the 1960s caused some industries to \"lapse into lethargy\", neglecting research and development and the search for new markets. The CSIRO was expected to fulfil research and development.\nPrices for wool and wheat remained high, with wool the mainstay of Australia's exports. Sheep numbers grew from 113 million in 1950 to 171\u00a0million in 1965. Wool production increased from 518,000 to 819,000 tonnes in the same period. Wheat, wool and minerals ensured a healthy balance of trade between 1950 and 1966.\nThe great housing boom of the post war period saw rapid growth in the suburbs of the major Australian cities. By the 1966 census, only 14 per cent lived in rural Australia, down from 31 per cent in 1933, and only 8 per cent lived on farms. Virtual full employment meant high standards of living and dramatic increases in home ownership, and by the sixties, Australia had the most equitable spread of income in the world. By the beginning of the sixties, an Australia-wide McNair survey estimated that 94% of homes had a fridge, 50% a telephone, 55% a television, 60% a washing machine, and 73% a vacuum cleaner. In addition, most households had now acquired a car. According to one study, \"In 1946, there was one car for every 14 Australians; by 1960, it was one to 3.5. The vast majority of families had access to a car.\"\nCar ownership flourished during the postwar period, with 1970/1971 census data estimating that 96.4 per cent of Australian households in the early Seventies owned at least one car; however, not all felt the rapid suburban growth was desirable. Distinguished Architect and designer Robin Boyd, a critic of Australia's built surroundings, described Australia as \"'the constant sponge lying in the Pacific', following the fashions of overseas and lacking confidence in home-produced, original ideas\". In 1956, dadaist comedian Barry Humphries performed the character of Edna Everage as a parody of a house-proud housewife of staid 1950s Melbourne suburbia (the character only later morphed into a critique of self-obsessed celebrity culture). It was the first of many of his satirical stage and screen creations based around quirky Australian characters: Sandy Stone, a morose elderly suburbanite, Barry McKenzie a naive Australian expat in London and Sir Les Patterson, a vulgar parody of a Whitlam-era politician.\nSome writers defended suburban life. Journalist Craig Macgregor saw suburban life as a \"...solution to the needs of migrants...\" Hugh Stretton argued that \"plenty of dreary lives are indeed lived in the suburbs... but most of them might well be worse in other surroundings\". Historian Peter Cuffley has recalled life for a child in a new outer suburb of Melbourne as having a kind of joyous excitement. \"Our imaginations saved us from finding life too humdrum, as did the wild freedom of being able to roam far and wide in different kinds of (neighbouring) bushland...Children in the suburbs found space in backyards, streets and lanes, playgrounds and reserves...\"\nIn 1954, the Menzies Government formally announced the introduction of the new two-tiered TV system\u2014a government-funded service run by the ABC, and two commercial services in Sydney and Melbourne, with the 1956 Summer Olympics in Melbourne being a major driving force behind the introduction of television to Australia. Colour TV began broadcasting in 1975.\nIndigenous assimilation and child removal.\nThe 1951 Native Welfare Conference of state and Commonwealth officials had agreed on a policy of cultural assimilation for all Aboriginal Australians. Paul Hasluck, the Commonwealth Minister for Territories, stated: \"Assimilation means, in practical terms, that, in the course of time, it is expected that all persons of aboriginal blood or mixed blood in Australia will live like other white Australians do.\"\nControls over the daily lives of Aboriginal people and the removal of Aboriginal children of mixed descent continued under the policy of assimilation, although the control was now largely exercised by Welfare Boards and removals were justified on welfare grounds. The number of Aboriginal people deemed to be wards of the state under Northern Territory welfare laws doubled to 11,000 from 1950 to 1965.\nDuring this period, the policy of assimilation attracted increasing criticism from Aboriginal people and their supporters on the grounds of its negative effects on Aboriginal families and its denial of Aboriginal cultural autonomy. Removals of Aboriginal children of mixed descent from their families slowed by the late 1960s and by 1973 the Commonwealth had adopted a policy of self-determination for Indigenous Australians.\nIn 1997, the Human Rights and Equal Opportunity Commission estimated that between 10 per cent and one-third of Aboriginal children had been removed from their families from 1910 to 1970. Regional studies indicate that 15 per cent of Aboriginal children were removed in New South Wales from 1899 to 1968, while the figure for Victoria was about 10 per cent. Robert Manne estimates that the figure for Australia as a whole was closer to 10 per cent.\nSummarising the policy of assimilation and forced removals of Aboriginal children of mixed descent, Richard Broome concludes: \"Even though the children's material conditions and Western education may have been improved by removal, even though some removals were necessary, and even though some people were thankful for it in retrospect, overall it was a disaster...It was a rupturing of tens of thousands of Aboriginal families, aimed at eradicating Aboriginality from the nation in the cause of homogeneity and in fear of difference.\"\nAlliances (1950\u20131972).\nIn the early 1950s, the Menzies government saw Australia as part of a \"triple alliance\" in concert with both the US and traditional ally Britain. At first, \"the Australian leadership opted for a consistently pro-British line in diplomacy\", while at the same time looking for opportunities to involve the US in South East Asia. Thus, the government committed military forces to the Korean War and the Malayan Emergency and hosted British nuclear tests after 1952. Australia was also the only Commonwealth country to offer support to the British during the Suez Crisis.\nMenzies oversaw an effusive welcome to Queen Elizabeth II on the first visit to Australia by a reigning monarch, in 1954. He made the following remarks during a light-hearted speech to an American audience in New York, while on his way to attend her coronation in 1953: \"We in Australia, of course, are British, if I may say so, to the boot heels...but we stand together \u2013 our people stand together \u2013 till the crack of doom.\"\nAs British influence declined in South East Asia, the US alliance came to have greater significance for Australian leaders and the Australian economy. British investment in Australia remained significant until the late 1970s, but trade with Britain declined through the 1950s and 1960s. In the late 1950s the Australian Army began to re-equip using US military equipment. In 1962, the US established a naval communications station at North West Cape, the first of several built during the next decade. Most significantly, in 1962, Australian Army advisors were sent to help train South Vietnamese forces, in a developing conflict in which the British had no part.\nAccording to diplomat Alan Renouf, the dominant theme in Australia's foreign policy under Australia's Liberal\u2013Country Party governments of the 1950s and 1960s was anti-communism. Another former diplomat, Gregory Clark, suggested that it was specifically a fear of China that drove Australian foreign policy decisions for twenty years. The ANZUS security treaty, which had been signed in 1951, had its origins in Australia's and New Zealand's fears of a rearmed Japan. Its obligations on the US, Australia and New Zealand are vague, but its influence on Australian foreign policy thinking, at times has been significant. The SEATO treaty, signed only three years later, clearly demonstrated Australia's position as a US ally in the emerging Cold War.\nAs Britain struggled to enter the Common Market in the 1960s, Australia saw that its historic ties with the mother country were rapidly fraying. Canberra was alarmed but kept a low profile, not wanting to alienate London. Russel Ward states that the implications of British entry into Europe in 1973: \"seemed shattering to most Australians, particularly to older people and conservatives.\" Carl Bridge, however, points out that Australia had been \"hedging its British bets\" for some time. The ANZUS treaty and Australia's decision to enter the Vietnam War did not involve Britain and by 1967 Japan was Australia's leading export partner and the US her largest source of imports. According to Bridge, Australia's decision not to follow Britain's devaluation of her currency in 1967 \"marked the demise of British Australia.\"\nVietnam War.\nBy 1965, Australia had increased the size of the Australian Army Training Team Vietnam (AATTV), and in April the Government made a sudden announcement that \"after close consultation with the United States\", a battalion of troops was to be sent to South Vietnam. In parliament, Menzies emphasised the argument that \"our alliances made demands on us\". The alliance involved was presumably, the Southeast Asia Treaty Organization (SEATO), and Australia was providing military assistance because South Vietnam, a signatory to SEATO, had apparently requested it. Documents released in 1971 indicated that the decision to commit troops was made by Australia and the US, not at the request of South Vietnam. By 1968, there were three Australian Army battalions at any one time at the 1st Australian Task Force (1ATF) base at Nui Dat in addition to the advisers of the AATTV placed throughout Vietnam, and personnel reached a peak total of almost 8,000, comprising about one third of the Army's combat capacity. Between 1962 and 1972 almost 60,000 personnel served in Vietnam, including ground troops, naval forces and air assets.\nIn July 1966, new prime minister Harold Holt expressed his government's support for the US and its role in Vietnam in particular. \"I don't know where people would choose to look for the security of this country were it not for the friendship and strength of the United States.\" While on a visit in the same year to the US, Holt assured President Lyndon B. Johnson \"...I hope there is corner of your mind and heart which takes cheer from the fact that you have an admiring friend, a staunch friend, [Australia] that will be all the way with LBJ.\"\nThe Liberal-CP Government was returned with a massive majority in elections held in December 1966, fought over national security issues including Vietnam. The opposition Labor Party had advocated the withdrawal of all conscripts from Vietnam, but its deputy leader Gough Whitlam had stated that a Labor government might maintain regular army troops there. Arthur Calwell, who had been leader of the Labor Party since 1960, retired in favour of Whitlam a few months later.\nDespite Holt's sentiments and his government's electoral success in 1966, the war became unpopular in Australia, as it did in the United States. The movements to end Australia's involvement gathered strength after the Tet Offensive of early 1968 and compulsory national service (selected by ballot) became increasingly unpopular. In the 1969 elections, the government hung on despite a significant decline in popularity. Moratorium marches held across Australia in mid-1970 attracted large crowds\u2014the Melbourne march of 100,000 being led by Labor MP Jim Cairns. As the Nixon administration proceeded with Vietnamization of the war and began the withdrawal of troops, so did the Australian Government. In November 1970 1st Australian Task Force was reduced to two battalions and in November 1971, 1ATF was withdrawn from Vietnam. The last military advisers of the AATTV were withdrawn by the Whitlam Labor government in mid-December 1972.\nThe Australian military presence in Vietnam had lasted 10 years, and in purely human cost, more than 500 had been killed and more than 2,000 wounded. The war cost Australia $218\u00a0million between 1962 and 1972.\nReform and reaction (1972\u20131996).\nWhitlam government (1972\u201375).\nElected in December 1972 after 23 years in opposition, Labor won office under Gough Whitlam, introducing significant reforms and expanding the federal budget. Welfare benefits were extended and payment rates increased, a national health insurance scheme was introduced, and divorce laws liberalised. Commonwealth expenditure on schools trebled in the two years to mid-1975 and the Commonwealth assumed responsibility for funding higher education, abolishing tuition fees. In foreign affairs, the new government prioritised the Asia Pacific region, formally abolishing the White Australia policy, recognising Communist China and enhancing ties with Indonesia. Conscription was abolished and the remaining Australian troops in Vietnam withdrawn. The Australian national anthem was changed from God Save the Queen to Advance Australia Fair, the imperial honours system was replaced at the Commonwealth level by the Order of Australia, and Queen Elizabeth II was officially styled Queen of Australia. Relations with the US, however, became strained after government members criticised the resumption of the US bombing campaign in North Vietnam.\nIn Indigenous affairs, the government introduced a policy of self-determination for Aboriginal people in economic, social and political affairs. Federal expenditure on Aboriginal services increased from $23 million to $141 million during the three years of the government. One of the first acts of the Whitlam government was to establish a Royal Commission into land rights in the Northern Territory under Justice Woodward. Legislation based on its findings was passed into law by the Fraser government in 1976, as the Aboriginal Land Rights Act 1976.\nAs the Whitlam government did not control the Senate, much of its legislation was rejected or amended. After Labor was re-elected with a reduced majority at elections in May 1974, the Senate remained an obstacle to its political agenda. The government's popularity was also harmed by deteriorating economic conditions and a series of political scandals. Increased government spending, rapid wage growth, booming commodity prices and the first OPEC oil shock led to economic instability. The unemployment rate reached post-war high of 3.6 per cent in late 1974 and the annual inflation rate hit 17 per cent.\nIn 1974\u201375 the government began negotiations for US$4\u00a0billion in foreign loans to fund state development of Australia's mineral and energy resources. Minister Rex Connor conducted secret discussions with a loan broker from Pakistan, and Treasurer Jim Cairns misled parliament about the issue. Arguing the government was incompetent following the Loans Affair, the opposition Liberal-Country Party Coalition delayed passage of the government's money bills in the Senate, until the government would promise a new election. Whitlam refused and the deadlock ended when his government was controversially dismissed by the Governor-General, John Kerr on 11 November 1975. Opposition leader Malcolm Fraser was installed as caretaker prime minister, pending an election.\nFraser government (1975\u201383).\nThe Federal elections of December 1975 resulted in a landslide victory for the Liberal-Country Party coalition and Malcolm Fraser continued as prime minister. The coalition government won subsequent elections in 1977 and 1980, making Fraser the second longest serving Australian prime minister up to that time. The Fraser government espoused a policy of administrative competence and economic austerity leavened by progressive humanitarian, social and environmental interventions. The government enacted the Whitlam government's land rights bill with few changes, increased immigration, and resettled Indochinese refugees. It promoted multiculturalism and in 1978 established the Special Broadcasting Service (SBS) as a multicultural broadcaster. In foreign policy, the government continued Labor's friendly relations with China and Indonesia, repaired the frayed relationship with the US and opposed white minority rule in South Africa and Rhodesia. Environmental policies included banning resource development on Fraser Island and the Great Barrier Reef, creating Kakadu National Park and banning whaling. However, the government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982 and the resulting grassroots campaign against the dam contributed to the emergence of an influential environmental movement in Australia.\nOn the economic front, the Fraser government followed a \"fight inflation first\" strategy centred on budget cuts and wage restraint. Welfare benefits were restricted, the universal healthcare system was partially dismantled, and university funding per student cut. However, by the early 1980s economic conditions were deteriorating. The second oil shock in 1979 increased inflation which was exacerbated by a boom in commodity prices and a sharp increase in real wages. An international recession, the collapse of the resources boom and a severe drought in eastern Australia saw unemployment rise. The government responded with Keynesian deficit spending in its 1982 Budget, but by 1983 both unemployment and annual inflation exceeded 10 per cent. At the Federal elections in March 1983 the coalition government was comfortably defeated by Labor under its popular new leader Bob Hawke.\nLabor governments (1983\u20131996).\nThe Hawke government pursued a mixture of free market reforms and consensus politics featuring \"summits\" of government representatives, business leaders, trade unions and non-government organisations in order to reach consensus on key issues such as economic policy and tax reform. The centrepiece of this policy mix was an Accord with trade unions under which wage demands would be curtailed in return for increased social benefits. Welfare payments were increased and better targeted to those on low incomes, and a retirement benefits scheme (superannuation) was extended to most employees. A new universal health insurance scheme, Medicare, was introduced. The Treasurer Paul Keating oversaw a program of deregulation and micro-economic reforms which broke with the Keynesian economics that had traditionally been favoured by the Labor party. These reforms included floating the Australian dollar, deregulating capital markets and allowing competition from foreign banks. Business regulation and competition policy was streamlined, tariffs and quotas on imports were reduced, and a number of government enterprises were privatised. The higher education system was restructured and significantly expanded, partly funded by the reintroduction of fees in the form of student loans and \"contributions\" (HECS). Paul Kelly concludes that, \"In the 1980s both Labor and non-Labor underwent internal philosophical revolutions to support a new set of ideas\u2014faith in markets, deregulation, a reduced role for government, low protection and the creation of a new cooperative enterprise culture.\"\nThe government's environmental interventions included stopping the Franklin Dam in Tasmania, banning new uranium mines at Jabiluka, and proposing Kakadu National park for world heritage listing. In foreign policy, the government maintained strong relations with the US and was instrumental in the formation of the Asia Pacific Economic Cooperation (APEC) group. Australia contributed naval ships and troops to UN forces in the Gulf War after Iraq had invaded Kuwait in 1990.\nThe government took other initiatives aimed at fostering national unity. The Australia Act 1986 eliminated the last vestiges of British legal authority at the federal level. The Australian Bicentenary in 1988 was the focus of year-long celebrations with multicultural themes. The World Expo 88 was held in Brisbane and a new Parliament House in Canberra was opened.\nStrong economic growth, falling unemployment, an unstable opposition, and Bob Hawke's popularity with the public contributed to the re-election of the Hawke government in 1984, 1987 and 1990. However, the economy went into recession in 1990 and by late 1991 the unemployment rate had risen above 10 per cent. With the government's popularity falling, Paul Keating successfully challenged for the leadership and became prime minister in December 1991.\nThe Keating government's first priority was economic recovery. In February 1992 it released the \"One Nation\" job creation package and later legislated tax cuts to corporations and individuals to boost economic growth. Unemployment reached 11.4 per cent in 1992\u2014the highest since the Great Depression in Australia. The Liberal-National opposition had proposed an ambitious plan of economic reform to take to the 1993 Election, including the introduction of a Goods and Services Tax. Keating campaigned strongly against the tax and was returned to office in March 1993.\nIn May 1994 a more ambitious \"Working Nation\" jobs program was introduced. The Keating government also pursued a number of \"big picture\" issues throughout its two terms including increased political and economic engagement in the Asia Pacific region, Indigenous reconciliation, and an Australian republic. The government engaged closely with the Indonesian president, Suharto and other regional partners, and successfully campaigned to increase the role of APEC as a major forum for strategic and economic co-operation. A Council for Aboriginal Reconciliation was established and, following the High Court of Australia's historic Mabo decision in 1992, the first national Native Title legislation was introduced to regulate claims and provide compensation for loss of native title. In 1993, Keating established a Republic Advisory Committee to examine options for Australia becoming a republic. The government also introduced family payments and a superannuation guarantee with compulsory employer contributions.\nUnder the Hawke government the annual migration intake had more than doubled from 54,500 in 1984\u201385 to more than 120,000 in 1989\u201390. The Keating government responded to community concerns about the pace of immigration by cutting the immigration intake and introducing mandatory detention for illegal immigrants arriving without a valid visa. Immigration fell to 67,900 in 1992\u201393.\nWith foreign debt, inflation and unemployment still stubbornly high, Keating lost the March 1996 Election to the Liberals' John Howard.\nAustralia in a globalised world (1996\u20132022).\nHoward government (1996\u20132007).\nJohn Howard with a Liberal\u2013National Party coalition served as Prime Minister from 1996 until 2007, winning re-election in 1998, 2001 and 2004 to become the second-longest-serving prime minister after Menzies. The Howard government introduced a nationwide gun control scheme following a mass shooting at Port Arthur. The coalition introduced industrial relations reforms in 1996 which promoted individual contracts and enterprise bargaining. In 2006, it introduced the WorkChoices legislation, which made it easier for small businesses to terminate employment. After the 1996 election, Howard and treasurer Peter Costello proposed a Goods and Services Tax (GST) which they successfully took to the electorate in 1998 and implemented in July 2000.\nThe government responded to the populist anti-immigration policies of Pauline Hanson and her One Nation party by publicly criticising elites and political correctness and emphasising Australian values. The coalition initially cut immigration intakes, abolished the Office of Multicultural Affairs and other multicultural agencies, and introduced citizenship tests for migrants. Following a sharp increase in unauthorised arrivals by boat from 1999, the government opened new mandatory detention centres in remote areas of Australia and issued temporary visas for those found to be refugees. Following the Children Overboard affair and the Tampa Affair in 2001, the government introduced the Pacific Solution, which involved moving unauthorised immigrants to detention centres in Nauru and Papua New Guinea while their refugee status was determined, as well as a policy of turning back vessels intercepted at sea.\nIn Indigenous affairs the Prime Minister rejected calls for a treaty with Indigenous Australians and an apology for past actions which had harmed them. Instead, the government pursued a policy of \"practical reconciliation\" involving specific measures to improve Indigenous education, health, employment and housing. In response to the High Court's decision in \"Wik Peoples v Queensland\", in 1996, the government amended native title legislation to limit native title claims. In 2007, following the release of the \"Little Children are Sacred\" report detailing widespread abuse in Aboriginal communities, the Howard government launched the Northern Territory Intervention in order to create a safe environment for Indigenous children. The government's response was criticised by the co-chairs of the report, but was supported by the Labor opposition.\nHonouring an election commitment, the Howard government set up a people's convention on an Australian republic. The resulting 1999 referendum on a republic failed. Howard, a monarchist, became the only Australian prime minister to publicly oppose a constitutional amendment he had put to the people.\nIn 1999, Australia led a United Nations force into East Timor to help establish democracy and independence for that nation, following political violence. Australia also committed to other peacekeeping and stabilisation operations: notably in Bougainville, including Operation Bel Isi (1998\u20132003); as well as Operation Helpem Fren and the Australian-led Regional Assistance Mission to Solomon Islands (RAMSI) in the early 2000s; and the 2006 East Timorese crisis. Following the September 2001 terrorist attacks on the US and the subsequent War on Terror, Australia committed troops to the Afghanistan War and the Iraq War. These events, along with the 2002 Bali Bombings and other terrorist incidents, led to the creation of a National Security Committee and further anti-terrorist legislation.\nIn foreign affairs, the government advocated a policy of \"Asia first, but not Asia only\", emphasising traditional links to the Commonwealth and the US. Relations with Indonesia became strained over East Timor but generally improved after the Bali bombings. Australia's support of US policy during the War on Terror was followed by an Australia-United States Free Trade Agreement in 2004. Trade agreements with Singapore and Thailand were also secured and relations with China improved. Australia joined the US in refusing to ratify the Kyoto Protocol on greenhouse gas emissions, arguing that it would harm Australia's economy and would be ineffective without the participation of China and India.\nAfter initial cuts, the immigration intake increased steadily, with a bias towards skilled workers to meet the needs of a rapidly growing economy. Immigration also became more diverse, with the proportion of immigrants from South Asia increasing from 8 per cent in 1996\u201397 to 20 per cent in 2007\u201308. Inbound tourism also grew, helped by the Sydney Olympic games in 2000.\nThe economy continued its uninterrupted expansion since the early 1990s recession, with record jobs growth and the lowest unemployment rates since the 1970s. Exports, imports and foreign investment grew, and China became Australia's second largest trading partner after Japan. The coalition delivered budget surpluses in most years which, along with the proceeds of government asset sales, were partly invested in a Future Fund to reduce the national debt. Income inequality and private debt increased as the economy expanded, with the biggest increase in incomes accruing to the top 10 per cent of income earners.\nBy 2007, the Howard government was consistently trailing the Labor opposition in opinion polls, with key issues being rising interest rates, the unpopular Work Choices industrial relations reforms, and climate change policy. There were also leadership tensions between Howard and Costello, and opinion polls indicated a desire for a generational change in leadership. Labor won the November 2007 election with a swing of more than 5 per cent and Howard became only the second sitting prime minister to lose his seat in an election.\nLabor governments (2007\u20132013).\nThe Rudd government moved quickly to ratify the Kyoto protocols, dismantle the previous government's Work Choices industrial relations reforms, and issue an apology to Aboriginal Australians for past policies, particularly the removal of Aboriginal children from their families. The government was soon confronted by the 2008 financial crisis and the Great Recession, responding with a series of economic stimulus measures worth A$75 billion. Although economic growth slowed in 2008, Australia was one of the few advanced economies in the world to avoid recession.\nThe Rudd government proposed an emissions trading scheme (ETS) to address climate change, but the legislation was twice rejected in the Senate. After the failed December 2009 UN Climate Change Conference in Copenhagen, the government decided to postpone its ETS until 2013, a decision which saw Labor lose some electoral support to the Greens. The government's proposed a Resources Super Profits Tax adversely affected Labor's support in the resource-rich states of Queensland and Western Australia.\nThe government changed its predecessor's asylum seeker policy by closing the Nauru processing centre, abolished temporary protection visas and improving the legal rights and processing time for applicants for asylum. However, unauthorised arrivals by boat increased sharply from 2009 and the number in mandatory detention stretched capacity. The new leader of the opposition, Tony Abbot, promised that a coalition government would \"stop the boats.\"\nIn June 2010, with the government behind the opposition in polls and Rudd's popularity falling, the Labor caucus replaced Rudd with Julia Gillard as leader: Australia's first female prime minister. The new leader was able to negotiate concessions on a new mining tax with large mining companies but failed to reach agreement with East Timor on a proposed migration processing centre there.\nFollowing the August 2010 federal election, Gillard formed a minority Labor government with the support of the Australian Greens and three independents. The Gillard government passed enabling legislation for a National Broadband Network, a carbon pricing scheme, a mining tax, a National Disability Insurance Scheme, and school funding reforms. The government negotiated an agreement with Malaysia to process some asylum seekers there but the plan was struck down by the High Court. In response, the government reopened offshore processing centres on Manus Island and Nauru.\nFollowing mounting leadership speculation and poor polling for the government, Rudd defeated Gillard in a leadership ballot in June 2013 and returned as prime minister, promising to replace the carbon tax with an emissions trading scheme and to ensure that people arriving without authority by boat would not be settled in Australia. The opposition, promising to \"stop the boats,\" abolish the carbon tax and mining tax, and reduce the Budget deficit and government debt, won the September 2013 election.\nLiberal-National Coalition governments (2013\u20132022).\nThe return of the Liberal-National Coalition to power after six years in opposition initially failed to restore stability to the office of prime minister. Prime Minister Tony Abbott's rival Malcolm Turnbull challenged for and won the leadership of the Liberals within Abbott's first term. After Turnbull narrowly returned the coalition to office in 2016, Party dissatisfaction with his leadership saw him replaced by Scott Morrison in 2018.\nAbbott government (2013\u20132015).\nPrime Minister Tony Abbott's government began implementing its policies on unauthorised maritime arrivals, including Operation Sovereign Borders, boat turnbacks, the reintroduction of temporary protection visas, and the resettlement in third countries of those found to be refugees. The number of people arriving by boat fell from 20,587 in 2013 to none in 2015. The government continued Australia's economic engagement with Asia, signing trade agreements with China, South Korea and Japan. The government also embraced the intervention against Islamic State in Iraq and Syria, joining the air campaign, sending special forces and providing training for the Iraqi army.\nThe government's May 2014 Budget proved unpopular, with the perception that it had involved breaking a number of election promises. The government secured the passage of legislation abolishing the carbon tax (July 2014) and the mining tax (September 2014).\nThe Prime Minister announced a number of decisions \u2013 most notably the reintroduction of knighthoods and a knighthood for Prince Philip, Duke of Edinburgh \u2013 which had not been approved by cabinet and which were widely criticised in the media. By September 2015 the government had lost 30 Newspolls in a row and Malcolm Turnbull successfully challenged for the leadership.\nTurnbull government (2015\u20132018).\nThe new Turnbull government announced a National Innovation and Science Agenda and delivered a Budget featuring cuts to company tax. However, the elections of July 2016 saw the government returned with a majority on only one and a minority in the Senate. Following a national postal plebiscite, the government legalised same-sex marriage in December 2017.\nIn foreign affairs, Australia signed a refugee exchange deal with the US in September 2016, allowing those in detention on Manus Island and Nauru to be settled in the US. There was increased tension with China over its policies in the South China Sea, Australia's new laws targeting foreign influence in domestic politics, and a ban, on national security grounds, on Chinese companies supplying Australia's 5G communications network.\nIn 2017, the United States, Japan, India and Australia agreed to revive the Quadrilateral Security Dialogue in order to counter Chinese ambitions in the South China Sea. Australia signed a modified Trans-Pacific Partnership trade agreement with 10 other nations in March 2018 after the US withdrew from the original agreement.\nThe government lost five by-elections in July 2018. When, in August, the government made a commitment to meet Australia's emissions target under the Paris Agreement, a number of coalition members rebelled. The controversy harmed the government, which had already lost more than 30 consecutive Newspolls. The parliamentary Liberal Party elected Scott Morrison as its new leader and he was sworn in as prime minister.\nMorrison government (2018\u20132022).\nThe Morrison government committed to remaining in the Paris Agreement, but promised a greater focus on reduction of energy prices. In foreign affairs the government signed the Indonesia\u2013Australia Comprehensive Economic Partnership Agreement (IA-CEPA) in March 2019. The government was returned at the elections of May 2019 with a three-seat majority.\nIn 2017, a convention of 250 Aboriginal and Torres Strait Islander delegates had issued the Uluru Statement from the Heart, calling for constitutional recognition of Indigenous Australians and a \"voice to parliament\". In 2019, the government announced a process to ensure that Indigenous Australians would be heard at all levels of government.\nIn 2020, the government was confronted with the world COVID-19 pandemic and the subsequent recession, Australia's first in 29 years. The government banned foreign nationals entering Australia and formed a National Cabinet to address the crisis. The national cabinet announced restrictions on non-essential business, travel and gatherings of people. These restrictions were eased from May, although individual states and territories reimposed restrictions in response to particular outbreaks of COVID-19.\nThe Australian government made provision for $267 billion in economic stimulus measures, and $16.6 billion in health measures in response to COVID-19. As a result of the COVID-19 recession, the unemployment rate peaked at 7.5 per cent in July 2020 before falling to 5.6 per cent in March 2021.\nIn June 2021, Australia and the United Kingdom announced that they had struck a preliminary deal on a free-trade agreement. On 16 September 2021, the government announced that Australia, the United Kingdom and the United States had agreed to the creation of an enhanced trilateral security partnership, dubbed AUKUS. The first initiative under AUKUS would be for Australia to acquire nuclear-powered submarine technology. As a result of the agreement, Australia cancelled its 2016 contract for the diesel-electric \"Attack\"-class submarine with the French company Naval Group. The decision drew rebukes from China and France.\nPost-pandemic (2022\u2013present).\nAlbanese government (2022\u2013present).\nOn 23 May 2022, Anthony Albanese was sworn in as Australia's new prime minister. His Labor Party defeated Scott Morrison's conservative government in the election. Prime Minister Albanese formed Australia's first Labor government in almost a decade.\nThe global surge in inflation that began in 2021, continued. Australia's inflation rate, which had peaked at 7.8% in late 2022, declined steadily through 2023 and 2024, falling to around 3.3% before edging higher again in late 2025, reaching 3.8% in October.\nA referendum on an Indigenous Voice to Parliament was held on 14 October 2023 and was rejected nationally. The Yes23 campaign co-chair Rachel Perkins called for a week of silence \"to grieve this outcome and reflect on its meaning and significance\".\nOn 6 October 2025, Australia and Papua New Guinea signed a landmark security and defence cooperation treaty \u2014 Australia's first new defence treaty in more than 70 years \u2014 aimed at strengthening regional stability, joint military training, and maritime security operations.\nSociety and culture (1960s\u2013present).\nSocial developments.\nIndigenous Australians.\nThe 1960s proved a key decade for Indigenous rights in Australia, with the demand for change led by Indigenous activists and organisations such as the Federal Council for the Advancement of Aborigines and Torres Strait Islanders, and embraced by the wider population as citizenship rights were extended.\nAt the start of the decade, Aboriginal affairs were still regulated by state governments and, in the Northern Territory, by the Australian government. In most states Aboriginal Australians were banned from drinking alcohol and their freedom of association, movement and control of property was restricted. Queensland, Western Australia and the Northern Territory banned Aboriginal people from voting and Queensland and Western Australia controlled their right to marry. Aboriginals were often subjected to unofficial \"colour bars\" restricting their access to many goods, services and public facilities, especially in country towns.\nThe official policy of the Australian government and most state governments, however, was the assimilation of Aboriginal people into mainstream culture. In 1962, the Menzies Government's \"Commonwealth Electoral Act\" gave Indigenous people the right to vote at federal elections. In 1965, Queensland became the last state to confer state voting rights on Aboriginal people.\nIn 1963, the Yolngu people of Arnhem Land sent a bark petition to the Australian parliament asking for recognition of their traditional land rights. They subsequently took their case to the Supreme Court of the Northern Territory which ruled against them in September 1971. In 1965, Charles Perkins, helped organise freedom rides into parts of Australia to expose discrimination and inequality. In 1966, the Gurindji people of Wave Hill station commenced the Gurindji strike in a quest for equal pay and recognition of land rights.\nIn 1966, the Australian government gave Aboriginal people the same rights to social security benefits as other Australians. A 1967 referendum changed the Australian constitution to include all Aboriginal Australians in the national census and allow the Federal parliament to legislate on their behalf. A Council for Aboriginal Affairs was established. Popular acclaim for Aboriginal artists, sportspeople and musicians also grew over the period. In 1968, boxer Lionel Rose was proclaimed Australian of the Year. That same year, artist Albert Namatjira was honoured with a postage stamp. Singer-songwriter Jimmy Little's 1963 Gospel song \"Royal Telephone\" was the first No.1 hit by an Aboriginal artist. Women's Tennis World No. 1 Evonne Goolagong Cawley was celebrated as Australian of the Year in 1971.\nNeville Bonner was appointed Liberal Senator for QLD in 1971, becoming the first federal parliamentarian to identify as Aboriginal. Eric Deeral (QLD) and Hyacinth Tungutalum (NT) followed at a state and territory level in 1974. In 1976, Sir Doug Nicholls was appointed Governor of South Australia, the first indigenous Australian to hold vice-regal office. By the 2020s, Aboriginal representation in the federal parliament had exceeded the proportion of Aboriginal people in the general population, and Australia had its first Aboriginal leader of a state or territory in 2016, when the Country Liberal Party's Adam Giles became Chief Minister of the Northern Territory.\nIn January 1972, Aboriginal activists erected an Aboriginal \"tent embassy\" on the lawns of parliament house, Canberra and issued a number demands including land rights, compensation for past loss of land and self-determination. The leader of the opposition Gough Whitlam was among those who visited the tent embassy to discuss their demands. \nThe Whitlam government came to power in December 1972 with a policy of self-determination for Aboriginal people. The government also passed legislation against racial discrimination and established a Royal Commission into land rights in the Northern Territory, which formed the basis for the Fraser government's Aboriginal Land Rights Act 1976.\nFollowing this, some states introduced their own land rights legislation; however, there were significant limitations on the returned lands, or that available for claim. In 1985, the Hawke government handed over Uluru (Ayers Rock) to traditional owners with a lease back to the Commonwealth.\nIn 1992, the High Court of Australia handed down its decision in the Mabo Case, holding that Indigenous native title survived reception of English law and continued to exist unless extinguished by conflicting law or interests in land. The Keating government passed a Native Title Act in 1993 to regulate native title claims and established a Native Title Tribunal to hear those claims. In the subsequent Wik decision of 1996, the High Court found that a pastoral lease did not necessarily extinguish native title. In response, the Howard government amended the Native Title Act to provide better protection for pastoralists and others with an interest in land. By March 2019 the Native Titles Tribunal had determined that 375 Indigenous communities had established native title over 39 per cent of the Australian continent, with one third under exclusive title.\nFrom 1960 the Indigenous population grew faster than the Australian population as a whole. The Aboriginal population was 106,000 in 1961 (1 per cent of the total population) but by 2016 had grown to 786,900 (3 per cent of the population) with a third living in major cities. Despite the drift to large cities, the period from 1965 to 1980 also saw a movement of Indigenous Australians away from towns and settlements to small outstations (or homelands), particularly in Arnhem Land and Central Australia. The movement to outstations was associated with a wider trend for the revival of traditional culture. However, the expense of providing infrastructure to small remote communities has seen pressure from federal, state and territory governments to redirect funding towards larger Indigenous communities.\nFrom 1971 to 2006, indicators for Indigenous employment, median incomes, home ownership, education and life expectancy all improved, although they remained well below the level for those who were not indigenous. High rates of Indigenous incarceration and deaths in custody were highlighted by the report of the Royal Commission into Aboriginal Deaths in Custody in April 1991. The Keating government responded with $400 million in new spending to address some of the recommendations of the report. However, by 2001 Indigenous incarceration rates and deaths in custody had increased. Deaths in custody continued at an average of 15 per year during the decade to 2018.\nRichard Broome has concluded: \"To close the gap [between Indigenous and other Australians] on inequality and well being will take many years; some despairingly say generations. Compensation for lost wages, for missing out on native title settlements and for being removed from one's family and kin remain unresolved.\"\nWomen.\nHolmes and Pinto point out that in 1960 domesticity and motherhood were still the dominant conceptions of femininity. In 1961, women made up only 25 per cent of employed adults and twice as many women described their occupation as \"home duties\" compared with those in paid employment. The fertility rate fell from a post-war high of 3.5 to less than 2 in the 1970s and 1980s.\nThe reforming drive of the 1960s and the increasing influence of the women's movement led to a series of legislative and institutional changes. These included the abolition of the \"marriage bar\" in the Australian public service in 1966, the Arbitration Commission's equal pay decisions of 1969 and 1972, the introduction of paid maternity leave in the Australian public service in 1973, and the enactment of the federal Sex Discrimination Act in 1984 and the Affirmative Action Act of 1986.\nSingle mothers' benefits were introduced in 1973 and the Family Law Act 1975 bought in no-fault divorce. From the 1980s there was an increase in government funding of women's refuges, health centres, rape crisis centres and information services. The Australian government began funding child care with the Child Care Act of 1972, although state, territory and local government were still the main providers of funding. In 1984, the Australian government introduced standardised fee relief for child care, and funding was greatly expanded in 1990 by the decision to extend fee relief to commercial child care centres.\nAccording to Holmes and Pinto, reliable birth control, increased employment opportunities, and improved family welfare and childcare provision increased opportunities for women outside motherhood and domesticity. In 2019\u201320, women were more likely than men to hold a bachelor's degree or higher qualification. 68 per cent of women aged 20\u201374 years old participated in the labour force, compared with 78 per cent of men. However, 43 per cent of employed women were working part-time, compared with 16 per cent of men, and the average earnings of women working full-time was 14 per cent below that of men.\nIn the five-to-ten years to 2020, the number of women in private sector leadership roles, female federal Justices and Judges, and federal parliamentarians have all increased gradually. However, between 1999 and 2021, Australia has fallen from ninth to 50th in the Inter-Parliamentary Union's ranking of countries by women's representation in national parliaments.\nMigrants and cultural diversity.\nIn 1961, just over 90 per cent of the Australian population had been born in Australia, New Zealand, the UK or Ireland. Another eight per cent had been born in continental Europe. The White Australia policy was in force and migrants were expected to assimilate into the Australian way of life. As the White Australia policy was gradually dismantled in the 1960s and formally abolished in 1973, governments developed a policy of multiculturalism to manage Australia's increasing cultural diversity. In August 1973 Labor's immigration minister Al Grassby announced his vision of \"A Multi\u2010Cultural Society for the Future\" and a policy of cultural pluralism based on principles of social cohesion, equality of opportunity and cultural identity soon gained bipartisan support. The Galbally Report on migrant services in 1978 recommended that: \"every person should be able to maintain his or her culture without prejudice or disadvantage and should be encouraged to understand and embrace other cultures.\" In response to the report, the Fraser government expanded funding for settlement services, established the Australian Institute of Multicultural Affairs (AIMA), funded multicultural and community language education programs in schools and established the multi-lingual Special Broadcasting Service (SBS). State and territory government programs to support multiculturalism followed.\nBy the late 1980s Australia had a high migrant intake which included significant numbers of new arrivals from Asian and Middle\u2010Eastern countries, leading to public debate on immigration policy. In 1984, the historian Geoffrey Blainey called for a reduction in Asian immigration in the interests of social cohesion. In 1988, the opposition Leader, John Howard called for the abandonment of multiculturalism, a reduction in Asian immigration, and a focus on 'One Australia'. In the same year, the government's FitzGerald review of immigration recommended a sharper economic focus in the selection of immigrants. In 1989, the Hawke government released its \"National Agenda for a Multicultural Australia\" which endorsed respect for cultural diversity and the need for settlement services, but indicated that pluralism was limited by the need for \"an overriding and unifying commitment to Australia\".\nMulticultural programs continued to expand between 1986 and 1996 with an emphasis on addressing disadvantage in migrant communities as well as settlement services for recent migrants. James Walter argues that the Hawke and Keating governments (1983\u201396) also promoted high migration as a means of improving Australia's competitive advantage in a globalised market.\nIn 1996, Pauline Hanson, a newly elected independent member of parliament, called for a cut in Asian immigration and an end to multiculturalism. In 1998, her One Nation Party gained 23 per cent of the vote in the Queensland elections. The Howard government (1996 to 2007) initially abolished a number of multicultural agencies and reduced funding to some migrant services as part of a general program of budget cuts. In 1999, the government adopted a policy of \"Australian multiculturalism\" with an emphasis on citizenship and adherence to \"Australian values\".\nFollowing 11 September 2001 terrorist attacks in the US, the Bali bombings and other terrorist incidents, some media and political commentary sought to link terrorism with Islam. In 2004, the Human Rights and Equal Opportunity Commission (HREOC) reported an increase in vilification and violence against Australian Muslims and some other minority ethnic groups. The government increased funding for multicultural, citizenship and settlement programs, with an emphasis on the promotion of social cohesion and security. The annual immigration intake also increased substantially as the economy boomed, from 67,900 in 1998\u201399 to 148,200 in 2006\u201307. The proportion of migrants selected for their skills increased from 30 per cent in 1995\u201396 to 68 per cent in 2006\u201307.\nImmigration continued to grow under the Labor government (2007\u201313) with prime minister Kevin Rudd proclaiming a \"big Australia\" policy. The immigration intake averaged around 190,000 a year from 2011\u201312 to 2015\u201316, a level based on research indicating the optimum level to increase economic output per head of population. India and China became the largest source countries of new migrants. The immigration intake was reduced to 160,000 in 2018\u201319 as some State governments complained that high immigration was adding to urban congestion. The opposition also linked high immigration with low wages growth while the One Nation party continued to oppose high immigration while proclaiming: \"It's okay to be white.\".\nBy 2020, 30 per cent of the Australian population were born overseas. The top five countries of birth for those born overseas were England, China, India, New Zealand and the Philippines. Australia's population encompassed migrants born in almost every country in the world.\nArts and culture.\nThe 1960s and 1970s saw increased government support for the arts and the flourishing of distinctively Australian artistic works. The Gorton government (1968\u201371) established the Australian Council for the Arts, the Australian Film Development Corporation (AFDC) and the National Film and Television Training School. The Whitlam government (1972\u201375) established the Australia Council with funding to promote crafts, Aboriginal arts, literature, music, visual arts, theatre, film and television.\nIn 1966, a television drama quota was introduced requiring broadcasters to show 30 minutes of locally produced drama each week. The police series \"Homicide\" (1964\u201367) became the highest rating program and the family drama \"Skippy the Bush Kangaroo\" became a local and international success. By 1969 eight of the twelve most popular television programs were Australian. With these successes, locally produced dramas became a staple of Australian television in the 1970s and 1980s. Notable examples include \"Rush\" (1973\u201376), \"The Sullivans\" (1976\u201383) and \"Neighbours\" (1985\u2013present).\nFrom the late 1960s a \"new wave\" of Australian theatre emerged, initially centred on small theatre groups such as the Pram Factory, La Mama and the Australian Performing Group in Melbourne and the Jane Street Theatre and Nimrod Theatre Company in Sydney. Playwrights associated with the new wave included David Williamson, Alex Buzo, Jack Hibberd and John Romeril. Features of the new wave were the extensive use of Australian colloquial speech (including obscenities), the exploration of the Australian identity, and the critique of cultural myths. By the end of the 1970s new Australian plays were a feature of small and large theatre companies in most states.\nSupport through the AFDC (from 1975 the Australian Film Commission) and state funding bodies, and generous tax concessions for investors introduced in 1981, led to a large increase in Australian produced films. Almost 400 were produced between 1970 and 1985. Notable films include \"The Adventures of Barry McKenzie\" (1972), \"Picnic at Hanging Rock\" (1975), \"My Brilliant Career\" (1979), \"Breaker Morant\" (1980), \"Gallipoli\" (1981), the \"Mad Max\" trilogy (1979\u201385) and \"Crocodile Dundee\" (1986).\nIn 1973, Patrick White became the first Australian to win a Nobel Prize for Literature. While there were only around twenty Australian novels published in 1973, this had grown to around 300 in 1988. By 1985 more than 1,000 writers had received grants and more than 1,000 books had been subsidised by the Literature Board. Writers who published their first book between 1975 and 1985 include Peter Carey, David Malouf, Murray Bail, Elizabeth Jolley, Helen Garner and Tim Winton.\nThere was also a growing recognition of Indigenous cultural movements. In the early 1970s Aboriginal elders at Papunya began using acrylic paints to make \"dot\" paintings based on the traditional Honey Ant Dreaming. Indigenous artists from other regions also developed distinctive styles based on a fusion of modern art materials and traditional stories and iconography. Indigenous writers such as Oodgeroo Noonuccal (Kath Walker), Jack Davis and Kevin Gilbert produced significant work in the 1970s and 1980s. A National Black Theatre was established in Sydney in the early 1970s. The Aboriginal Islander Dance Theatre was established in 1976 and the Bangarra Dance Theatre in 1989. In 1991, the rock band Yothu Yindi, which drew on traditional Aboriginal music and dance, achieved commercial and critical success.\nIn music, ABC television's popular music show \"Countdown\" (1974\u201387) helped promote Australian music while radio station 2JJ (later JJJ) in Sydney promoted live performances and recordings by Australian independent artists and record labels.\nCarter and Griffen-Foley state that by the end of the 1970s: \"There was a widely shared sense of Australian culture as independent, no longer troubled by its relationship with Britain.\" However, by 1990 commentators as diverse as P. P. McGuiness and Geoffrey Serle were complaining that the large increase in artistic works had led to the celebration of mediocrity. Poet Chris Wallace-Crabbe questioned whether Australia had overcome its former \"cultural cringe\" only to fall into cultural overconfidence.\nIn the new millennium, the globalisation of the Australian economy and society, and developments in jet travel and the internet have largely overcome the \"\" which had influenced Australian arts and culture. Overseas cultural works could be more readily accessed in person or virtually. Australian performers such as the Australian Ballet and Australian Chamber Orchestra frequently toured abroad. The growing number of international art exhibitions, such as Art Basel Hong Kong and the Queensland Art Gallery's Asia-Pacific Triennial of Contemporary Art, have increased the exposure of Australian art in the region and the wider global market.\nIn film, the number of Australian productions averaged 14 per year in the 1970s but grew to 31 per year in the 2000s and 37 per year in the 2010s. A number of Australian directors and actors, including Baz Luhrmann, George Miller, Peter Weir, Cate Blanchett, Nicole Kidman, Geoffrey Rush and others, have been able to establish careers both in Australia and abroad. The technical expertise developed in the Australian industry, and the increasing number of internationally successful Australian directors and actors, encouraged foreign producers to make more films in Australia. Major international productions made in Australia in the past decade include \"\" and \"The Great Gatsby\".\nCarter and Griffen-Follet conclude: \"Australia is no longer a Dominion or client state within a closed imperial market, but a medium-sized player, exporter as well as importer, within globalised cultural industries and markets.\"\nHistoriography.\nThe first Australian histories, such as those by William Wentworth and James Macarthur, were written to influence public opinion and British policy in the colony. After the Australian colonies became self-governing in the 1850s, colonial governments commissioned histories aimed at promoting migration and investment from Britain. The beginning of professional academic history in Australian universities from 1891 saw the dominance of an Imperial framework for interpreting Australian history, in which Australia emerged from the successful transfer of people, institutions, and culture from Britain. Typical of the imperial school of Australian history was the Australian volume of the \"Cambridge History of the British Empire\" published in 1933.\nMilitary history received government support after the First World War, most prominently with Charles Bean's 12 volume \"History of Australia in the War of 1914\u20131918\" (1921\u201342). Bean's earlier work as Australia's official war correspondent had helped establish the Anzac legend which, according to McKenna: \"immediately supplanted all other narratives of nationhood \u2013 the march of the explorers, the advance of settlement, Eureka, Federation and Australia's record of progressive democratic legislation.\"\nRadical nationalist interpretations of Australian history became more prominent from the 1930s. Brian Fitzpatrick published a series of histories from 1939 to 1941 which sought to demonstrate the exploitative nature of Britain's economic relationship with Australia and the role of the labour movement in a struggle for social justice and economic independence. Russel Ward's \"The Australian Legend\" (1958) sought to trace the origins of a distinctive democratic national ethos from the experiences of the convicts, bushrangers, gold-diggers, drovers and shearers. In the 1960s, Marxist historians such as Bob Gollan and Ian Turner explored the relationship of the labour movement to radical nationalist politics.\nIn the first two volumes of his \"History of Australia\" (1962, 1968) Manning Clark developed an idiosyncratic interpretation of Australian history telling the story of \"epic tragedy\" in which \"the explorers, Governors, improvers, and perturbators vainly endeavoured to impose their received schemes of redemption on an alien, intractable setting\". Donald Horne's \"The Lucky Country\" (1964) was scathing in its observations of a complacent, dull, anti-intellectual and provincial Australia, with a swollen suburbia and absence of innovation. Geoffrey Blainey's \"\" (1966) argued that Australia's distance from Britain had shaped its history and identity.\nHumphrey McQueen's \"A New Britannia\" (1970) attacked radical nationalist historical narratives from a Marxist New Left perspective. Anne Summers in \"Damned Whores and God's Police\" (1975) and Miriam Dixson in \"The Real Matilda\" (1976) analysed the role of women in Australian history. Others explored the history of those marginalised because of their sexuality or ethnicity. Oral histories, such as Wendy Lowenstein's \"Weevils in the Flour\" (1978) became more prominent.\nFrom the 1970s, histories of the Aboriginal\u2013settler relationship became prominent. Charles Rowley's \"The Destruction of Aboriginal Society\" (1970), Henry Reynolds' \"The Other Side of the Frontier\" (1981) and Peter Reid's work on the \"stolen generations\" of Aboriginal children are notable.\nPost-structuralist ideas on the relationship between language and meaning were influential in the 1980s and 1990s, for example, in Greg Dening's \"Mr Bligh's Bad Language\" (1992)\".\" Memory studies and Pierre Nora's ideas on the relationship between memory and history influenced work in a number of fields including military history, ethnographic history, oral history and historical work in Australian museums. Interdisciplinary histories drawing on the insights of fields such as sociology, anthropology, cultural studies and environmental studies have become more common since the 1980s. Transnational approaches which analyse Australian history in a global and regional context have also flourished in recent decades.\nIn the 21st century, most historical works are not created by academic historians, and public conceptions of Australia's history are more likely to be shaped by popular histories, historical fiction and drama, the media, the internet, museums and public institutions. Popular histories by amateur historians regularly outsell work by academic historians. Local histories and family histories have proliferated in recent decades.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReference books.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nHistorical surveys.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nEarly recorded history.\nBooks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nJournal articles, scholarly papers, essays.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nPrimary sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39592", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=39592", "title": "Possesive case", "text": ""}
{"id": "39593", "revid": "16208158", "url": "https://en.wikipedia.org/wiki?curid=39593", "title": "Possessive and possessed cases", "text": "References.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39594", "revid": "39150892", "url": "https://en.wikipedia.org/wiki?curid=39594", "title": "Duchy of Anjou", "text": "Former French province (1360\u20131482)\nThe Duchy of Anjou (; , ; ) was a French province straddling the lower Loire. Its capital was Angers, and its area was roughly co-extensive with the diocese of Angers. Anjou was bordered by Brittany to the west, Maine to the north, Touraine to the east and Poitou to the south. The adjectival form is Angevin, and inhabitants of Anjou are known as Angevins. In 1482, the duchy became part of the Kingdom of France and then remained a province of the Kingdom under the name of the Duchy of Anjou. After the decree dividing France into departments in 1791, the province was disestablished and split into six new \"d\u00e9partements\". The majority of its area formed the new Maine-et-Loire department and its remaining area split between the departments of Deux-S\u00e8vres, Indre-et-Loire, Loire-Atlantique, Sarthe, and Vienne.\nDuchy of Anjou.\nThe county of Anjou was united to the royal domain between 1205 and 1246, when it was turned into an apanage for the king's brother, Charles I of Anjou. This second Angevin dynasty, a branch of the Capetian dynasty, established itself on the thrones of Naples and Sicily, and the joint throne of Croatia and Hungary. Anjou itself was united to the royal domain again in 1328, but was detached in 1360 as the Duchy of Anjou for the king's son, Louis I of Anjou. The third Angevin dynasty, a branch of the House of Valois, also ruled for a time the Kingdom of Naples. The dukes had the same autonomy as the earlier counts, but the duchy was increasingly administered in the same fashion as the royal domain and the royal government often exercised the ducal power while the dukes were away.\nOn 17 February 1332, Philip VI bestowed it on his son John the Good, who, when he became king in turn (22 August 1350), gave the countship to his second son Louis I, raising it to a duchy in the peerage of France by letters patent of 25 October 1360. Louis I, who became in time count of Provence and titular king of Naples, died in 1384, and was succeeded by his son Louis II, who devoted most of his energies to his Neapolitan ambitions, and left the administration of Anjou almost entirely in the hands of his wife, Yolande of Aragon. On his death (29 April 1417), she took upon herself the guardianship of their young son Louis III, and, in her capacity of regent, defended the duchy against the English. Louis III, who also devoted himself to winning Naples, died on 15 November 1434, leaving no children. The duchy of Anjou then passed to his brother Ren\u00e9, second son of Louis II and Yolande of Aragon.\nProvince of Anjou.\nUnlike his predecessors, who had rarely stayed long in Anjou, Ren\u00e9 from 1443 onwards paid long visits to it, and his court at Angers became one of the most brilliant in the kingdom of France. But after the sudden death of his son John in December 1470, Ren\u00e9, for reasons which are not altogether clear, decided to move his residence to Provence and leave Anjou for good. After making an inventory of all his possessions, he left the duchy in October 1471, taking with him the most valuable of his treasures. On 22 July 1474 he drew up a will by which he divided the succession between his grandson Ren\u00e9 II of Lorraine and his nephew Charles II, count of Maine. On hearing this, King Louis XI, who was the son of one of King Ren\u00e9's sisters, seeing that his expectations were thus completely frustrated, seized the duchy of Anjou. He did not keep it very long, but became reconciled to Ren\u00e9 in 1476 and restored it to him, on condition, probably, that Ren\u00e9 should bequeath it to him. However that may be, on the death of the latter (10 July 1480) he again added Anjou to the royal domain.\nLater, King Francis I again gave the duchy as an appanage to his mother, Louise of Savoy, by letters patent of 4 February 1515. On her death, in September 1531, the duchy returned into the king's possession. In 1552 it was given as an appanage by Henry II to his son Henry of Valois, who, on becoming king in 1574, with the title of Henry III, conceded it to his brother Francis, duke of Alen\u00e7on, at the treaty of Beaulieu near Loches (6 May 1576). Francis died on 10 June 1584, and the vacant appanage definitively became part of the royal domain.\nGovernment.\nAt first Anjou was included in the \"gouvernement\" (or military command) of Orl\u00e9anais, but in the 17th century was made into a separate one. Saumur, however, and the Saumurois, for which King Henry IV had in 1589 created an independent military governor-generalship in favour of Duplessis-Mornay, continued till the Revolution to form a separate \"gouvernement\", which included, besides Anjou, portions of Poitou and Mirebalais. Attached to the \"g\u00e9n\u00e9ralit\u00e9\" (administrative circumscription) of Tours, Anjou on the eve of the Revolution comprised five \"\u00ealections\" (judicial districts):--Angers, Baug\u00e9, Saumur, Ch\u00e2teau-Gontier, Montreuil-Bellay and part of the \"\u00ealections\" of La Fl\u00e8che and Richelieu. Financially it formed part of the so-called \"pays de grande gabelle\", and comprised sixteen special tribunals, or \"greniers \u00e0 sel\" (salt warehouses):--Angers, Baug\u00e9, Beaufort, Bourgueil, Cand\u00e9, Ch\u00e2teau-Gontier, Cholet, Craon, La Fl\u00e8che, Saint-Florent-le-Vieil, Ingrandes, Le Lude, Pouanc\u00e9, Saint-R\u00e9my-la-Varenne, Richelieu, Saumur. From the point of view of purely judicial administration, Anjou was subject to the parlement of Paris; Angers was the seat of a presidial court, of which the jurisdiction comprised the \"s\u00e9n\u00e9chauss\u00e9es\" of Angers, Saumur, Beaug\u00e9, Beaufort and the duchy of Richelieu; there were besides presidial courts at Ch\u00e2teau-Gontier and La Fl\u00e8che. When the Constituent Assembly, on 26 February 1790, decreed the division of France into departments, Anjou and the Saumurois, with the exception of certain territories, formed the department of Maine-et-Loire, as at present constituted.\nUnder the kingdom of France, Anjou was practically identical with the diocese of Angers, bound on the north by Maine, on the east by Touraine, on the south by Poitou (Poitiers) and the Mauges, and the west by the countship of Nantes or the duchy of Brittany. Traditionally Anjou was divided into four natural regions: the Baugeois, the Haut-Anjou (or Segr\u00e9en), the Mauges, and the Saumurois.\nIt occupied the greater part of what is now the department of Maine-et-Loire. On the north, it further included: Craon; Cand\u00e9; Bazouges (Ch\u00e2teau-Gontier); and Le Lude. On the east, it further added Ch\u00e2teau-la-Valli\u00e8re and Bourgueil; while to the south, it lacked the towns of Montreuil-Bellay, Vihiers, Cholet, and Beaupr\u00e9au, as well as the district lying to the west of the Ironne and Thouet on the left bank of the Loire, which formed the territory of the Mauges.\nRegion of Anjou.\nSince the end of the provincial system in 1791, the name of 'Anjou' has been used to describe the former region in which the duchy and province occupied. This region roughly correlates to several regions: Mayenne angevine (north west), Haut Anjou (centre-northern), Segreen (western), Baugeois (eastern), Les Mauges (south western), and Saumurois (south).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\nAttribution"}
{"id": "39595", "revid": "1315214124", "url": "https://en.wikipedia.org/wiki?curid=39595", "title": "Human leg", "text": "Lower extremity or leg of the human body\nThe leg is the entire lower limb of the human body, including the foot, thigh or sometimes even the hip or buttock region. The major bones of the leg are the femur (thigh bone), tibia (shin bone), and adjacent fibula. There are thirty bones in each leg. \nThe thigh is located in between the hip and knee. \nThe shank - the calf (rear) and the shin (front) - is located between the knee and the ankle..\nLegs are used for standing, many forms of human movement, recreation such as dancing, and constitute a significant portion of a person's mass. Evolution has led to the human leg's development into a mechanism specifically adapted for efficient bipedal gait. While the capacity to walk upright is not unique to humans, other primates can only achieve this for short periods and at a great expenditure of energy. In humans, female legs generally have greater hip anteversion and tibiofemoral angles, while male legs have longer femur and tibial lengths.\nIn humans, each lower limb is divided into the hip, thigh, knee, leg, ankle and foot. \nIn anatomy, arm refers to the upper arm and leg refers to the lower leg.\nStructure.\nIn human anatomy, the lower leg or crus (or shank) is the part of the lower limb that lies between the knee and the ankle. In the lower leg, the calf is the back portion, and the tibia or shinbone together with the smaller fibula make up the shin, the front of the lower leg. Anatomists restrict the term \"leg\" to this use, rather than to the entire lower limb. The thigh is between the hip and knee and makes up the rest of the lower limb. The term lower limb or \"lower extremity\" is commonly used to describe all of the leg.\nEvolution has provided the human body with two distinct features: the specialization of the upper limb for visually guided manipulation and the lower limb's development into a mechanism specifically adapted for an efficient bipedal gait. While the capacity to walk upright is not unique to humans, other primates can only achieve this for short periods and at a great expenditure of energy.\nThe human adaption to bipedalism has also affected the location of the body's center of gravity, the reorganization of internal organs, and the form and biomechanism of the trunk. In humans, the double S-shaped vertebral column acts as a great shock-absorber which shifts the weight from the trunk over the load-bearing surface of the feet. The human legs are exceptionally long and powerful as a result of their exclusive specialization for support and locomotion\u2014in orangutans the leg length is 111% of the trunk; in chimpanzees 128%, and in humans 171%. Many of the leg's muscles are also adapted to bipedalism, most substantially the gluteal muscles, the extensors of the knee joint, and the calf muscles.\nBones.\nThe major bones of the leg are the femur (thigh bone), tibia (shin bone), and adjacent fibula, which are all long bones. The patella (kneecap) is a sesamoid bone (the largest in the body) in front of the knee. Most of the leg skeleton has bony prominences and margins that can be palpated, and some serve as anatomical landmarks that define the extent of the leg. These landmarks are the anterior superior iliac spine, the greater trochanter, the superior margin of the medial condyle of tibia, and the medial malleolus. Notable exceptions to palpation are the hip joint, and the neck and body, or shaft of the femur.\nUsually, the large joints of the lower limb are aligned in a straight line, which represents the mechanical longitudinal axis of the leg, the Mikulicz line. This line stretches from the hip joint (or more precisely the head of the femur), through the knee joint (the intercondylar eminence of the tibia), and down to the center of the ankle (the ankle mortise, the fork-like grip between the medial and lateral malleoli). In the tibial shaft, the mechanical and anatomical axes coincide, but in the femoral shaft they diverge 6\u00b0, resulting in the \"femorotibial angle\" of 174\u00b0 in a leg with normal axial alignment. A leg is considered straight when, with the feet brought together, both the medial malleoli of the ankle and the medial condyles of the knee are touching. Divergence from the normal femorotibial angle is called genu varum if the center of the knee joint is lateral to the mechanical axis (intermalleolar distance exceeds 3\u00a0cm), and genu valgum if it is medial to the mechanical axis (intercondylar distance exceeds 5\u00a0cm). These conditions impose unbalanced loads on the joints and stretching of either the thigh's adductors and abductors.\nThe angle of inclination formed between the neck and shaft of the femur (collodiaphysial angle) varies with age\u2014about 150\u00b0 in the newborn, it gradually decreases to 126\u2013128\u00b0 in adults, to reach 120\u00b0 in old age. Pathological changes in this angle result in abnormal posture of the leg: a small angle produces coxa vara and a large angle coxa valga; the latter is usually combined with genu varum, and coxa vara leads genu valgum. Additionally, a line drawn through the femoral neck superimposed on a line drawn through the femoral condyles forms an angle, the \"torsion\" angle, which makes it possible for flexion movements of the hip joint to be transposed into rotary movements of the femoral head. Abnormally increased torsion angles result in a limb turned inward and a decreased angle in a limb turned outward; both cases resulting in a reduced range of a person's mobility.\nMuscles.\nHip.\nThere are several ways of classifying the muscles of the hip:\nSome hip muscles also act either on the knee joint or on vertebral joints. Additionally, because the areas of origin and insertion of many of these muscles are very extensive, these muscles are often involved in several very different movements. In the hip joint, lateral and medial rotation occur along the axis of the limb; extension (also called dorsiflexion or retroversion) and flexion (anteflexion or anteversion) occur along a transverse axis; and abduction and adduction occur about a sagittal axis.\nThe anterior dorsal hip muscles are the iliopsoas, a group of two or three muscles with a shared insertion on the lesser trochanter of the femur. The psoas major originates from the last vertebra and along the lumbar spine to stretch down into the pelvis. The iliacus originates on the iliac fossa on the interior side of the pelvis. The two muscles unite to form the iliopsoas muscle, which is inserted on the lesser trochanter of the femur. The psoas minor, only present in about 50 per cent of subjects, originates above the psoas major to stretch obliquely down to its insertion on the interior side of the major muscle.\nThe posterior dorsal hip muscles are inserted on or directly below the greater trochanter of the femur. The tensor fasciae latae, stretching from the anterior superior iliac spine down into the iliotibial tract, presses the head of the femur into the acetabulum but also flexes, rotates medially, and abducts to hip joint. The piriformis originates on the anterior pelvic surface of the sacrum, passes through the greater sciatic foramen, and inserts on the posterior aspect of the tip of the greater trochanter. In a standing posture it is a lateral rotator, but it also assists extending the thigh. The gluteus maximus has its origin between (and around) the iliac crest and the coccyx, from where one part radiates into the iliotibial tract and the other stretches down to the gluteal tuberosity under the greater trochanter. The gluteus maximus is primarily an extensor and lateral rotator of the hip joint, and it comes into action when climbing stairs or rising from a sitting to a standing posture. Furthermore, the part inserted into the fascia latae abducts and the part inserted into the gluteal tuberosity adducts the hip. The two deep glutei muscles, the gluteus medius and minimus, originate on the lateral side of the pelvis. The medius muscle is shaped like a cap. Its anterior fibers act as a medial rotator and flexor; the posterior fibers as a lateral rotator and extensor; and the entire muscle abducts the hip. The minimus has similar functions and both muscles are inserted onto the greater trochanter.\nThe ventral hip muscles function as lateral rotators and play an important role in the control of the body's balance. Because they are stronger than the medial rotators, in the normal position of the leg, the apex of the foot is pointing outward to achieve better support. The obturator internus originates on the pelvis on the obturator foramen and its membrane, passes through the lesser sciatic foramen, and is inserted on the trochanteric fossa of the femur. \"Bent\" over the lesser sciatic notch, which acts as a fulcrum, the muscle forms the strongest lateral rotators of the hip together with the gluteus maximus and quadratus femoris. When sitting with the knees flexed it acts as an abductor. The obturator externus has a parallel course with its origin located on the posterior border of the obturator foramen. It is covered by several muscles and acts as a lateral rotator and a weak adductor. The inferior and superior gemelli muscles represent marginal heads of the obturator internus and assist this muscle. These three muscles form a three-headed muscle (tricipital) known as the triceps coxae. The quadratus femoris originates at the ischial tuberosity and is inserted onto the intertrochanteric crest between the trochanters. This flattened muscle act as a strong lateral rotator and adductor of the thigh.\nThe adductor muscles of the thigh are innervated by the obturator nerve, with the exception of pectineus which receives fibers from the femoral nerve, and the adductor magnus which receives fibers from the tibial nerve. The gracilis arises from near the pubic symphysis and is unique among the adductors in that it reaches past the knee to attach on the medial side of the shaft of the tibia, thus acting on two joints. It share its distal insertion with the sartorius and semitendinosus, all three muscles forming the pes anserinus. It is the most medial muscle of the adductors, and with the thigh abducted its origin can be clearly seen arching under the skin. With the knee extended, it adducts the thigh and flexes the hip. The pectineus has its origin on the iliopubic eminence laterally to the gracilis and, rectangular in shape, extends obliquely to attach immediately behind the lesser trochanter and down the pectineal line and the proximal part of the Linea aspera on the femur. It is a flexor of the hip joint, and an adductor and a weak medial rotator of the thigh. The adductor brevis originates on the inferior ramus of the pubis below the gracilis and stretches obliquely below the pectineus down to the upper third of the Linea aspera. Except for being an adductor, it is a lateral rotator and weak flexor of the hip joint.\nThe adductor longus has its origin at superior ramus of the pubis and inserts medially on the middle third of the Linea aspera. Primarily an adductor, it is also responsible for some flexion. The adductor magnus has its origin just behind the longus and lies deep to it. Its wide belly divides into two parts: One is inserted into the Linea aspera and the tendon of the other reaches down to adductor tubercle on the medial side of the femur's distal end where it forms an intermuscular septum that separates the flexors from the extensors. Magnus is a powerful adductor, especially active when crossing legs. Its superior part is a lateral rotator but the inferior part acts as a medial rotator on the flexed leg when rotated outward and also extends the hip joint. The adductor minimus is an incompletely separated subdivision of the adductor magnus. Its origin forms an anterior part of the magnus and distally it is inserted on the Linea aspera above the magnus. It acts to adduct and lateral rotate the femur.\nThigh.\nThe muscles of the thigh can be classified into three groups according to their location: anterior and posterior muscles and the adductors (on the medial side). All the adductors except gracilis insert on the femur and act on the hip joint, and so functionally qualify as hip muscles. The majority of the thigh muscles, the \"true\" thigh muscles, insert on the leg (either the tibia or the fibula) and act primarily on the knee joint. Generally, the extensors lie on anterior of the thigh and flexors lie on the posterior. Even though the sartorius flexes the knee, it is ontogenetically considered an extensor since its displacement is secondary.\nOf the anterior thigh muscles the largest are the four muscles of the quadriceps femoris: the central rectus femoris, which is surrounded by the three vasti, the vastus intermedius, medialis, and lateralis. Rectus femoris is attached to the pelvis with two tendons, while the vasti are inserted to the femur. All four muscles unite in a common tendon inserted into the patella from where the patellar ligament extends it down to the tibial tuberosity. Fibers from the medial and lateral vasti form two retinacula that stretch past the patella on either sides down to the condyles of the tibia. The quadriceps is \"the\" knee extensor, but the rectus femoris additionally flexes the hip joint, and articular muscle of the knee protects the articular capsule of the knee joint from being nipped during extension. The sartorius runs superficially and obliquely down on the anterior side of the thigh, from the anterior superior iliac spine to the pes anserinus on the medial side of the knee, from where it is further extended into the crural fascia. The sartorius acts as a flexor on both the hip and knee, but, due to its oblique course, also contributes to medial rotation of the leg as one of the pes anserinus muscles (with the knee flexed), and to lateral rotation of the hip joint.\nThere are four posterior thigh muscles. The biceps femoris has two heads: The long head has its origin on the ischial tuberosity together with the semitendinosus and acts on two joints. The short head originates from the middle third of the linea aspera on the shaft of the femur and the lateral intermuscular septum of thigh, and acts on only one joint. These two heads unite to form the biceps which inserts on the head of the fibula. The biceps flexes the knee joint and rotates the flexed leg laterally\u2014it is the only lateral rotator of the knee and thus has to oppose all medial rotator. Additionally, the long head extends the hip joint. The semitendinosus and the semimembranosus share their origin with the long head of the biceps, and both attaches on the medial side of the proximal head of the tibia together with the gracilis and sartorius to form the pes anserinus. The semitendinosus acts on two joints; extension of the hip, flexion of the knee, and medial rotation of the leg. Distally, the semimembranosus' tendon is divided into three parts referred to as the \"pes anserinus profondus\". Functionally, the semimembranosus is similar to the semitendinosus, and thus produces extension at the hip joint and flexion and medial rotation at the knee. Posteriorly below the knee joint, the popliteus stretches obliquely from the lateral femoral epicondyle down to the posterior surface of the tibia. The subpopliteal bursa is located deep to the muscle. Popliteus flexes the knee joint and medially rotates the leg.\nLower leg and foot.\nWith the popliteus (see above) as the single exception, all muscles in the leg are attached to the foot and, based on location, can be classified into an anterior and a posterior group separated from each other by the tibia, the fibula, and the interosseous membrane. In turn, these two groups can be subdivided into subgroups or layers\u2014the anterior group consists of the extensors and the peroneals, and the posterior group of a superficial and a deep layer. Functionally, the muscles of the leg are either extensors, responsible for the dorsiflexion of the foot, or flexors, responsible for the plantar flexion. These muscles can also classified by innervation, muscles supplied by the anterior subdivision of the plexus and those supplied by the posterior subdivision. The leg muscles acting on the foot are called the extrinsic foot muscles whilst the foot muscles located \"in\" the foot are called intrinsic.\nDorsiflexion (extension) and plantar flexion occur around the transverse axis running through the ankle joint from the tip of the medial malleolus to the tip of the lateral malleolus. Pronation (eversion) and supination (inversion) occur along the oblique axis of the ankle joint.\nExtrinsic.\nThree of the anterior muscles are extensors. From its origin on the lateral surface of the tibia and the interosseus membrane, the three-sided belly of the tibialis anterior extends down below the superior and inferior extensor retinacula to its insertion on the plantar side of the medial cuneiform bone and the first metatarsal bone. In the non-weight-bearing leg, the anterior tibialis dorsal flexes the foot and lifts the medial edge of the foot. In the weight-bearing leg, it pulls the leg towards the foot. The extensor digitorum longus has a wide origin stretching from the lateral condyle of the tibia down along the anterior side of the fibula, and the interosseus membrane. At the ankle, the tendon divides into four that stretch across the foot to the dorsal aponeuroses of the last phalanges of the four lateral toes. In the non-weight-bearing leg, the muscle extends the digits and dorsiflexes the foot, and in the weight-bearing leg acts similar to the tibialis anterior. The extensor hallucis longus has its origin on the fibula and the interosseus membrane between the two other extensors and is, similarly to the extensor digitorum, is inserted on the last phalanx of big toe (\"hallux\"). The muscle dorsiflexes the hallux, and acts similar to the tibialis anterior in the weight-bearing leg. Two muscles on the lateral side of the leg form the fibular (peroneal) group. The fibularis (peroneus) longus and fibularis (peroneus) brevis both have their origins on the fibula, and they both pass behind the lateral malleolus where their tendons pass under the fibular retinacula. Under the foot, the fibularis longus stretches from the lateral to the medial side in a groove, thus bracing the transverse arch of the foot. The fibularis brevis is attached on the lateral side to the tuberosity of the fifth metatarsal. Together, these two fibularis muscles form the strongest pronators of the foot. The fibularis muscles are highly variable, and several variants can occasionally be present.\nOf the posterior muscles three are in the superficial layer. The major plantar flexors, commonly referred to as the triceps surae, are the soleus, which arises on the proximal side of both leg bones, and the gastrocnemius, the two heads of which arises on the distal end of the femur. These muscles unite in a large terminal tendon, the Achilles tendon, which is attached to the posterior tubercle of the calcaneus. The plantaris closely follows the lateral head of the gastrocnemius. Its tendon runs between those of the soleus and gastrocnemius and is embedded in the medial end of the calcaneus tendon.\nIn the deep layer, the tibialis posterior has its origin on the interosseus membrane and the neighbouring bone areas and runs down behind the medial malleolus. Under the foot it splits into a thick medial part attached to the navicular bone and a slightly weaker lateral part inserted to the three cuneiform bones. The muscle produces simultaneous plantar flexion and supination in the non-weight-bearing leg, and approximates the heel to the calf of the leg. The flexor hallucis longus arises distally on the fibula and on the interosseus membrane from where its relatively thick muscle belly extends far distally. Its tendon extends beneath the flexor retinaculum to the sole of the foot and finally attaches on the base of the last phalanx of the hallux. It plantarflexes the hallux and assists in supination. The flexor digitorum longus, finally, has its origin on the upper part of the tibia. Its tendon runs to the sole of the foot where it forks into four terminal tendon attached to the last phalanges of the four lateral toes. It crosses the tendon of the tibialis posterior distally on the tibia, and the tendon of the flexor hallucis longus in the sole. Distally to its division, the quadratus plantae radiates into it and near the middle phalanges its tendons penetrate the tendons of the flexor digitorum brevis. In the non-weight-bearing leg, it plantar flexes the toes and foot and supinates. In the weight-bearing leg it supports the plantar arch. (For the popliteus, see above.)\nIntrinsic.\nThe intrinsic muscles of the foot, muscles whose bellies are located in the foot proper, are either dorsal (top) or plantar (sole).\nOn the dorsal side, two long extrinsic extensor muscles are superficial to the intrinsic muscles, and their tendons form the dorsal aponeurosis of the toes. The short intrinsic extensors and the plantar and dorsal interossei radiates into these aponeuroses. The extensor digitorum brevis and extensor hallucis brevis have a common origin on the anterior side of the calcaneus, from where their tendons extend into the dorsal aponeuroses of digits 1\u20134. They act to dorsiflex these digits.\nThe plantar muscles can be subdivided into three groups associated with three regions: those of the big digit, the little digit, and the region between these two. All these muscles are covered by the thick and dense plantar aponeurosis, which together with two tough septa, form the spaces of the three groups. These muscles and their fatty tissue function as cushions that transmit the weight of the body downward. As a whole, the foot is a functional entity.\nThe abductor hallucis stretches along the medial edge of the foot, from the calcaneus to the base of the first phalanx of the first digit and the medial sesamoid bone. It is an abductor and a weak flexor, and also helps maintain the arch of the foot. Lateral to the abductor hallucis is the flexor hallucis brevis, which originates from the medial cuneiform bone and from the tendon of the tibialis posterior. The flexor hallucis has a medial and a lateral head inserted laterally to the abductor hallucis. It is an important plantar flexor which comes into prominent use in classical ballet (i.e. for pointe work). The adductor hallucis has two heads; a stronger oblique head which arises from the cuboid and lateral cuneiform bones and the bases of the second and third metatarsals; and a transverse head which arises from the distal ends of the third-fifth metatarsals. Both heads are inserted on the lateral sesamoid bone of the first digit. The muscle acts as a tensor to the arches of the foot, but can also adduct the first digit and plantar flex its first phalanx.\nThe opponens digiti minimi originates from the long plantar ligament and the plantar tendinous sheath of the fibularis (peroneus) longus and is inserted on the fifth metatarsal. When present, it acts to plantar flex the fifth digit and supports the plantar arch. The flexor digiti minimi arises from the region of base of the fifth metatarsal and is inserted onto the base of the first phalanx of the fifth digit where it is usually merged with the abductor of the first digit. It acts to plantar flex the last digit. The largest and longest muscles of the little toe is the abductor digiti minimi. Stretching from the lateral process of the calcaneus, with a second attachment on the base of the fifth metatarsal, to the base of the fifth digit's first phalanx, the muscle forms the lateral edge of the sole. Except for supporting the arch, it plantar flexes the little toe and also acts as an abductor.\nThe four lumbricales have their origin on the tendons of the flexor digitorum longus, from where they extend to the medial side of the bases of the first phalanx of digits two-five. Except for reinforcing the plantar arch, they contribute to plantar flexion and move the four digits toward the big toe. They are, in contrast to the lumbricales of the hand, rather variable, sometimes absent and sometimes more than four are present. The quadratus plantae arises with two slips from margins of the plantar surface of the calcaneus and is inserted into the tendon(s) of the flexor digitorum longus, and is known as the \"plantar head\" of this latter muscle. The three plantar interossei arise with their single heads on the medial side of the third-fifth metatarsals and are inserted on the bases of the first phalanges of these digits. The two heads of the four dorsal interossei arise on two adjacent metatarsals and merge in the intermediary spaces. Their distal attachment is on the bases of the proximal phalanges of the second-fourth digits. The interossei are organized with the second digit as a longitudinal axis; the plantars act as adductors and pull digits 3\u20135 towards the second digit; while the dorsals act as abductors. Additionally, the interossei act as plantar flexors at the metatarsophalangeal joints. Lastly, the flexor digitorum brevis arises from underneath the calcaneus to insert its tendons on the middle phalanges of digit 2\u20134. Because the tendons of the flexor digitorum longus run between these tendons, the brevis is sometimes called \"perforatus\". The tendons of these two muscles are surrounded by a tendinous sheath. The brevis acts to plantar flex the middle phalanges.\nFlexibility.\nFlexibility can be simply defined as the available range of motion (ROM) provided by a specific joint or group of joints. For the most part, exercises that increase flexibility are performed with intentions to boost overall muscle length, reduce the risks of injury and to potentially improve muscular performance in physical activity. Stretching muscles after engagement in any physical activity can improve muscular strength, increase flexibility, and reduce muscle soreness. If limited movement is present within a joint, the \"insufficient extensibility\" of the muscle, or muscle group, could be restricting the activity of the affected joint.\nStretching.\nStretching prior to strenuous physical activity has been thought to increase muscular performance by extending the soft tissue past its attainable length in order to increase range of motion. Many physically active individuals practice these techniques as a \"warm-up\" in order to achieve a certain level of muscular preparation for specific exercise movements. When stretching, muscles should feel somewhat uncomfortable but not physically agonizing.\nBlood supply.\nThe arteries of the leg are divided into a series of segments.\nIn the pelvis area, at the level of the last lumbar vertebra, the abdominal aorta, a continuation the descending aorta, splits into a pair of common iliac arteries. These immediately split into the internal and external iliac arteries, the latter of which descends along the medial border of the psoas major to exits the pelvis area through the vascular lacuna under the inguinal ligament.\nThe artery enters the thigh as the femoral artery which descends the medial side of the thigh to the adductor canal. The canal passes from the anterior to the posterior side of the limb where the artery leaves through the adductor hiatus and becomes the popliteal artery. On the back of the knee the popliteal artery runs through the popliteal fossa to the popliteal muscle where it divides into anterior and posterior tibial arteries.\nIn the lower leg, the anterior tibial enters the extensor compartment near the upper border of the interosseus membrane to descend between the tibialis anterior and the extensor hallucis longus. Distal to the superior and extensor retinacula of the foot it becomes the dorsal artery of the foot. The posterior tibial forms a direct continuation of the popliteal artery which enters the flexor compartment of the lower leg to descend behind the medial malleolus where it divides into the medial and lateral plantar arteries, of which the posterior branch gives rise to the fibular artery.\nFor practical reasons the lower limb is subdivided into somewhat arbitrary regions: The regions of the hip are all located in the thigh: anteriorly, the subinguinal region is bounded by the inguinal ligament, the sartorius, and the pectineus and forms part of the femoral triangle which extends distally to the adductor longus. Posteriorly, the gluteal region corresponds to the gluteus maximus. The anterior region of the thigh extends distally from the femoral triangle to the region of the knee and laterally to the tensor fasciae latae. The posterior region ends distally before the popliteal fossa. The anterior and posterior regions of the knee extend from the proximal regions down to the level of the tuberosity of the tibia. In the lower leg the anterior and posterior regions extend down to the malleoli. Behind the malleoli are the lateral and medial retromalleolar regions and behind these is the region of the heel. Finally, the foot is subdivided into a dorsal region superiorly and a plantar region inferiorly.\nVeins.\nThe veins are subdivided into three systems. The deep veins return approximately 85 percent of the blood and the superficial veins approximately 15 percent. A series of perforator veins interconnect the superficial and deep systems. In the standing posture, the veins of the leg have to handle an exceptional load as they act against gravity when they return the blood to the heart. The venous valves assist in maintaining the superficial to deep direction of the blood flow.\nSuperficial veins:\nDeep veins:\nNerve supply.\nThe sensory and motor innervation to the lower limb is supplied by the lumbosacral plexus, which is formed by the ventral rami of the lumbar and sacral spinal nerves with additional contributions from the subcostal nerve (T12) and coccygeal nerve (Co1). Based on distribution and topography, the lumbosacral plexus is subdivided into the lumbar plexus (T12-L4) and the Sacral plexus (L5-S4); the latter is often further subdivided into the sciatic and pudendal plexuses:\nThe lumbar plexus is formed lateral to the intervertebral foramina by the ventral rami of the first four lumbar spinal nerves (L1-L4), which all pass through psoas major. The larger branches of the plexus exit the muscle to pass sharply downward to reach the abdominal wall and the thigh (under the inguinal ligament); with the exception of the obturator nerve which pass through the lesser pelvis to reach the medial part of the thigh through the obturator foramen. The nerves of the lumbar plexus pass in front of the hip joint and mainly support the anterior part of the thigh.\nThe iliohypogastric (T12-L1) and ilioinguinal nerves (L1) emerge from the psoas major near the muscle's origin, from where they run laterally downward to pass anteriorly above the iliac crest between the transversus abdominis and abdominal internal oblique, and then run above the inguinal ligament. Both nerves give off muscular branches to both these muscles. Iliohypogastric supplies sensory branches to the skin of the lateral hip region, and its terminal branch finally pierces the aponeurosis of the abdominal external oblique above the inguinal ring to supply sensory branches to the skin there. Ilioinguinalis exits through the inguinal ring and supplies sensory branches to the skin above the pubic symphysis and the lateral portion of the scrotum.\nThe genitofemoral nerve (L1, L2) leaves psoas major below the two former nerves, immediately divides into two branches that descends along the muscle's anterior side. The sensory femoral branch supplies the skin below the inguinal ligament, while the mixed genital branch supplies the skin and muscles around the sex organ. The lateral femoral cutaneous nerve (L2, L3) leaves psoas major laterally below the previous nerve, runs obliquely and laterally downward above the iliacus, exits the pelvic area near the iliac spine, and supplies the skin of the anterior thigh.\nThe obturator nerve (L2-L4) passes medially behind psoas major to exit the pelvis through the obturator canal, after which it gives off branches to obturator externus and divides into two branches passing behind and in front of adductor brevis to supply motor innervation to all the other adductor muscles. The anterior branch also supplies sensory nerves to the skin on a small area on the distal medial aspect of the thigh. The femoral nerve (L2-L4) is the largest and longest of the nerves of the lumbar plexus. It supplies motor innervation to iliopsoas, pectineus, sartorius, and quadriceps; and sensory branches to the anterior thigh, medial lower leg, and posterior foot.\nThe nerves of the sacral plexus pass behind the hip joint to innervate the posterior part of the thigh, most of the lower leg, and the foot. The superior (L4-S1) and inferior gluteal nerves (L5-S2) innervate the gluteus muscles and the tensor fasciae latae. The posterior femoral cutaneous nerve (S1-S3) contributes sensory branches to the skin on the posterior thigh. The sciatic nerve (L4-S3), the largest and longest nerve in the human body, leaves the pelvis through the greater sciatic foramen. In the posterior thigh it first gives off branches to the short head of the biceps femoris and then divides into the tibial (L4-S3) and common fibular nerves (L4-S2). The fibular nerve continues down on the medial side of biceps femoris, winds around the fibular neck and enters the front of the lower leg. There it divides into a deep and a superficial terminal branch. The superficial branch supplies the fibularis muscles and the deep branch enters the extensor compartment; both branches reaches into the dorsal foot. In the thigh, the tibial nerve gives off branches to semitendinosus, semimembranosus, adductor magnus, and the long head of the biceps femoris. The nerve then runs straight down the back of the leg, through the popliteal fossa to supply the ankle flexors on the back of the lower leg and then continues down to supply all the muscles in the sole of the foot. The pudendal nerve (S2-S4) and coccygeal plexus (S5-Co) supply the muscles of the pelvic floor and the surrounding skin.\nThe lumbosacral trunk is a communicating branch passing between the sacral and lumbar plexuses containing ventral fibers from L4. The coccygeal nerve, the last spinal nerve, emerges from the sacral hiatus, unites with the ventral rami of the two last sacral nerves, and forms the coccygeal plexus.\nLower leg and foot.\nThe lower leg and ankle need to keep exercised and moving well as they are the base of the whole body. The lower extremities must be strong in order to balance the weight of the rest of the body, and the gastrocnemius muscles take part in much of the blood circulation.\nExercises.\nIsometric and standard.\nThere are a number of exercises that can be done to strengthen the lower leg. For example, in order to activate plantar flexors in the deep plantar flexors one can sit on the floor with the hips flexed, the ankle neutral with knees fully extended as they alternate pushing their foot against a wall or platform. This kind of exercise is beneficial as it hardly causes any fatigue. Another form of isometric exercise for the gastrocnemius would be seated calf raises which can be done with or without equipment. One can be seated at a table with their feet flat on the ground, and then plantar flex both ankles so that the heels are raised off the floor and the gastrocnemius flexed. An alternate movement could be heel drop exercises with the toes being propped on an elevated surface\u2014as an opposing movement this would improve the range of motion. One-legged toe raises for the gastrocnemius muscle can be performed by holding one dumbbell in one hand while using the other for balance, and then standing with one foot on a plate. The next step would be to plantar flex and keep the knee joint straight or flexed slightly. The triceps surae is contracted during this exercise. Stabilization exercises like the BOSU ball squat are also important especially as they assist in the ankles having to adjust to the ball's form in order to balance.\nStrengthening the lower leg is essential for improving overall leg stability, balance, and injury prevention. Several effective exercises target the muscles in the lower leg, including the calves, tibialis anterior, and other supporting muscles. Calf raises are a foundational exercise: standing with feet hip-width apart, you raise your heels off the ground and lower them back down, effectively strengthening the gastrocnemius and soleus muscles. Seated calf raises, performed while sitting with a weight on your knees, focus specifically on the soleus muscle, which is crucial for endurance activities.\nTo target the tibialis anterior, toe raises are highly effective. Standing with feet flat, you lift your toes off the ground while keeping your heels planted, then lower them back down. For improved ankle mobility, ankle circles\u2014rotating your ankle clockwise and counterclockwise while seated or standing\u2014are beneficial. Similarly, heel walks, where you walk on your heels with toes lifted, strengthen the tibialis anterior and enhance balance.\nUsing equipment like resistance bands can add versatility to your routine. For example, looping a band around your foot and pulling it toward you strengthens various lower leg muscles. Jump rope is another excellent option, enhancing calf strength, coordination, and cardiovascular fitness. Finally, box jumps, where you jump onto a sturdy box or platform, develop explosive strength in the calves and lower legs.\nIncorporating these exercises into your workout routine can significantly improve lower leg strength and stability. Begin with a proper warm-up and gradually increase intensity to prevent injury. If you have specific fitness goals or medical conditions, consulting a fitness professional or physical therapist is recommended.\nClinical significance.\nLower leg injury.\nLower leg injuries are common while running or playing sports. About 10% of all injuries in athletes involve the lower extremities. The majority of athletes sprain their ankles; this is mainly caused by the increased loads onto the feet when they move into the foot down or in an outer ankle position. All areas of the foot, which are the forefoot, midfoot, and rearfoot, absorb various forces while running and this can also lead to injuries. Running and various activities can cause stress fractures, tendinitis, musculotendinous injuries, or any chronic pain to our lower extremities such as the tibia.\nTypes of activities.\nInjuries to quadriceps or hamstrings are caused by the constant impact loads to the legs during activities, such as kicking a ball. While doing this type of motion, 85% of that shock is absorbed to the hamstrings; this can cause strain to those muscles.\nRunning.\nThe most common injuries in running involve the knees and the feet. Various studies have focused on the initial cause of these running related injuries and found that there are many factors that correlate to these injuries. Female distance runners who had a history of stress fracture injuries had higher vertical impact forces than non-injured subjects. The large forces onto the lower legs were associated with gravitational forces, and this correlated with patellofemoral pain or potential knee injuries. Researchers have also found that these running-related injuries affect the feet as well, because runners with previous injuries showed more foot eversion and over-pronation while running than non-injured runners. This causes more loads and forces on the medial side of the foot, causing more stress on the tendons of the foot and ankle. Most of these running injuries are caused by overuse: running longer distances weekly for a long duration is a risk for injuring the lower legs.\nPrevention tools.\nVoluntary stretches to the legs, such as the wall stretch, condition the hamstrings and the calf muscle to various movements before vigorously working them. The environment and surroundings, such as uneven terrain, can cause the feet to position in an unnatural way, so wearing shoes that can absorb forces from the ground's impact and allow for stabilizing the feet can prevent some injuries while running as well. Shoes should be structured to allow friction-traction at the shoe surface, space for different foot-strike stresses, and for comfortable, regular arches for the feet.\nSummary.\nThe chance of damaging our lower extremities will be reduced by having knowledge about some activities associated with lower leg injury and developing a correct form of running, such as not over-pronating the foot or overusing the legs. Preventative measures, such as various stretches, and wearing appropriate footwear, will also reduce injuries.\nFracture.\nA fracture of the leg can be classified according to the involved bone into:\nPain management.\nLower leg and foot pain management is critical in reducing the progression of further injuries, uncomfortable sensations and limiting alterations while walking and running. Most individuals suffer from various pains in their lower leg and foot due to different factors. Muscle inflammation, strain, tenderness, swelling and muscle tear from muscle overuse or incorrect movement are several conditions often experienced by athletes and the common public during and after high impact physical activities. Therefore, suggested pain management mechanisms are provided to reduce pain and prevent the progression of injury.\nPlantar fasciitis.\nA plantar fasciitis foot stretch is one of the recommended methods to reduce pain caused by plantar fasciitis (Figure 1). To do the plantar fascia stretch, while sitting in a chair place the ankle on the opposite knee and hold the toes of the impaired foot, slowly pulling back. The stretch should be held for approximately ten seconds, three times per day.\nMedial tibial stress syndrome (shin splint).\nSeveral methods can be utilized to help control pain caused by shin splints. Placing ice on the affected area prior to and after running will aid in reducing pain. In addition, wearing orthoses (orthotic devices), including a neoprene sleeve (Figure 2) and wearing appropriate footwear such as a foot arch can help to eliminate the condition. Stretching and strengthening of the anterior tibia or medial tibia by performing exercises of plantar and dorsi flexors such as calf stretch can also help in easing the pain.\nAchilles tendinopathy.\nThere are numerous appropriate approaches to handling pain resulting from Achilles tendinitis. The primary action is to rest. Activities that do not provide additional stress to the affected tendon are also recommended. Wearing orthothics or prostheses will provide cushion and will prevent the affected Achilles tendon from experiencing further stress when walking and performing therapeutic stretches. A few stretch modalities or eccentric exercises such as toe extension and flexion and calf and heel stretch are beneficial in lowering pain with Achilles tendinopathy patients (Figure 4)\nSociety and culture.\nIn Norse mythology, the race of Jotuns was born from the legs of Ymir. In Finnic mythology, the Earth was created from the shards of the egg of a goldeneye that fell from the knees of Ilmatar. While this story isn't found in other Finno-Ugric mythologies, Pavel Melnikov-Pechersky has noted several times that the beauty of legs is commonly mentioned in Mordvin mythology as a characteristic of both female mythological characters and real Erzyan and Mokshan women.\nIn medieval Europe, showing legs was one of the biggest taboos for women, especially the ones with a high social status. In Victorian England several centuries later legs were not to be mentioned at all (not only human ones, but even those of a table or a piano), and referred to as \"limbs\" instead. Miniskirts and other clothing that reveal legs first became popular in mid-20th century science fiction. Since then, it became mainstream in Western cultures, with female legs frequently being focused on in films, TV ads, music videos, dance shows and various kinds of sports (i.e. ice skating or women's gymnastics).\nMany men who are attracted to female legs tend to regard them aesthetically almost as much as they do sexually, perceiving legs as more elegant, suggestive, sensual, or seductive (especially with clothing that makes legs easy to be revealed and concealed), whereas female breasts or buttocks are viewed as much more \"in your face\" sexual. That said, legs (especially the inside of the upper leg that has the most sensitive and delicate skin) are considered to be one of the most sexualized elements of a woman's body, especially in Hollywood movies.\nBoth men and women generally consider long legs attractive, which may explain the preference for tall fashion models. Men also tend to favor women who have a higher leg length to body ratio, but the opposite is true of women's preferences in men.\nAdolescent and adult women in many Western cultures often remove the hair from their legs. Toned, tanned, shaved legs are sometimes perceived as a sign of youthfulness and are often considered attractive in these cultures.\nMen generally do not shave their legs in any culture. However, leg-shaving is a generally accepted practice in modeling. It is also fairly common in sports where the hair removal makes the athlete appreciably faster by reducing drag; the most common case of this is competitive swimming.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nLiterature specified by multiple pages above:"}
{"id": "39596", "revid": "1284876455", "url": "https://en.wikipedia.org/wiki?curid=39596", "title": "Iberia (disambiguation)", "text": "Iberia, in its most common meaning, refers to the Iberian Peninsula in southwestern Europe. In history, it was also used to refer to anything pertaining to the former Kingdom of Iberia, an exonym for the Georgian kingdom of Kartli.\nIberia may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "39597", "revid": "51037360", "url": "https://en.wikipedia.org/wiki?curid=39597", "title": "Italian East Africa", "text": "Italian occupied Horn of Africa from 1936 to 1941\nItalian East Africa (, A.O.I.) was a territory of the Italian empire under Fascist Italy, existing from 1936 to 1941 in the Horn of Africa. It was established following the Second Italo-Ethiopian War, which led to the military occupation of the Ethiopian Empire (Abyssinia), and encompassed Italian Somaliland, Eritrea and the Ethiopian territories (although Italy did not have stable control over the whole of Ethiopia), all governed by a single administrative unit, the \"Governo Generale dell'Africa Orientale Italiana\". Its establishment contributed significantly to the outbreak of World War II by exposing the weaknesses of the League of Nations.\nItalian East Africa was divided into six governorates. Eritrea and Somalia, Italian possessions since the 1880s, were enlarged with captured Ethiopian territory and became the Eritrea and Somalia Governorates. The remainder of the occupied Ethiopian territories comprised the Harar, Galla-Sidamo, Amhara, and Scioa Governorates. At its largest extent, Italian East Africa occupied territories in British Somaliland, British Kenya, and Anglo-Egyptian Sudan. By 1939, it was settled by about 165,270 Italian colonists.\nDuring World War II, Italian East Africa became the battleground of the East African campaign (part of the Mediterranean and Middle East theater). After the Battle of Gondar in November 1941, it was occupied by a British-led force including colonial units and Ethiopian resistance fighters. All former Italian territories came under British administration; occupied Ethiopia was ruled until full sovereignty was restored in 1944. In 1950, Allied occupied Somalia became the United Nations Trust Territory of Somaliland, administered by Italy from 1950 until its independence in 1960. Allied occupied Eritrea became an autonomous part of Ethiopia in 1952. It was later annexed by the Ethiopian Empire in 1962 and gained its independence in 1993 as Eritrea.\nHistory.\nItalian ambitions and rise of fascism.\nIn the late 19th and early 20th centuries, Italy sought to expand its colonial empire, competing with other European powers for overseas territories. Early efforts included the colonization of Eritrea (1890) and Italian Somaliland (1905), followed by the unsuccessful invasion of the Ethiopian Empire in the First Italo-Ethiopian War (1895\u20131896). After World War I, nationalist sentiments grew, fueled by the belief that Italy had been denied its rightful territorial rewards for its contribution to the war effort, a sentiment known as the Mutilated Victory (). The combination of mobilization costs and the social unrest that followed the war is widely thought to have strengthened Italian irredentism and nationalism. This frustration contributed to the rise of Benito Mussolini and his Fascist regime in 1922. \nMussolini injected a new and aggressive impetus into these frustrations and ambitions, framing colonial expansion as a means to restore Roman greatness, enhance national prestige, and solve Italy's economic problems by providing land and resources. Mussolini believed the Italian people lacked a strong nationalistic and colonial conscience and thus sought to cultivate these sentiments through Fascist propaganda, particularly in the lead-up to the invasion and during occupation of the Ethiopia Empire. This propaganda emphasized Italy's need for colonial territories (), the perceived danger of Ethiopian aggression, and the injustice of international opposition to Italian expansion. The conquest of Ethiopia in 1936 and the subsequent formation of Italian East Africa were presented as major achievements of the Fascist regime, aimed at fulfilling long-standing Italian ambitions and establishing Italy as a major power. Despite earlier consideration of a protectorate over parts of Ethiopia, the Fascist government pursued full conquest, driven by Mussolini's determination to achieve a significant colonial victory to bolster his regime's legitimacy and international standing. This ambition, however, was met with Ethiopian resistance and international complications. This further aligned Italy with Nazi Germany, setting the stage for its involvement in World War II. \nSecond Italo-Ethiopian War.\nThe Italo-Ethiopian Treaty of 1928 stated that the border between Italian Somaliland and Ethiopia was 21 leagues parallel to the Benadir coast. In 1934, a border clash at Ual-Ual between Italian and Ethiopian forces during a boundary survey provided Mussolini with a pretext for a war. The Second Italo-Ethiopian War began without prior declaration of war in October 1935, with Italy sending a of two hundred thousand soldiers commanded by Marshal Emilio De Bono and General Rodolfo Graziani (and later Pietro Badoglio). This army was equipped with superior weaponry, including an air force and tanks. Italian troops used mustard gas in aerial bombardments (in violation of the Geneva Protocol and Geneva Conventions) against combatants and civilians in an attempt to discourage the Ethiopian people from supporting the resistance. This military superiority over the , combined with the then Ethiopian Emperor Haile Selassie's initial strategy of engaging in frontal battles led to significant Italian victories. During the late stages of the war, Italian forces violated international agreements by bombing Red Cross ambulances and hospitals, claiming these actions were in retaliation for alleged Ethiopian atrocities.\nWhile some Italians initially opposed the war due to fears of a broader European conflict and Italy's financial instability, the Italian public largely shifted to support Mussolini following reported victories. Selassie was forced to flee to England, with Italian forces entering the capital city, Addis Ababa by 5 May 1936. Shortly after, Mussolini proclaimed an \"Italian Empire of Ethiopia\" and the Italian Empire on May 9, 1936. Following the Italian conquest, Ethiopian patriotic resistance continued in various parts of the country, particularly in the north and northwest. The resistance, although facing the challenge of Italy's modern military and harsh reprisals, prevented Italy from fully controlling Ethiopia and hindered its plans for economic and demographic exploitation. This ongoing opposition set the stage for Ethiopia's eventual involvement in the Second World War.\nLeague of Nations.\nSince 28 September 1923, the Ethiopian Empire had been one of the member states of the League of Nations after its membership was supported by Mussolini. Following Italy's invasion, the League imposed economic sanctions on Italy, restricting trade in arms, rubber, and certain raw materials. However, the sanctions did not include oil, which was needed for Italy's war effort. France and Britain, concerned about maintaining good relations with Italy against the growing German threat, hesitated to enforce harsher penalties (appeasement). In 1936, the League lifted the sanctions, effectively legitimizing Italy's conquest. The failure of the League to protect the Ethiopian Empire exposed its weaknesses, demonstrating its inability to deter aggression from major powers. This failure was a key moment in the lead-up to World War II, undermining the League's credibility as a force for global peace. Although Selassie fled to England and appealed to the League of Nations for help, many nations gradually recognized the Italian annexation for political and economic reasons. On 18 November 1936, the Italian Empire was recognised by the Empire of Japan and Italy recognised the Japanese occupation of Manchuria, marking the end of the Stresa Front. Mussolini's international popularity further decreased as he endorsed the annexation of Austria by Nazi Germany, beginning a political tilt toward Germany that eventually led to the downfall of Mussolini and the Fascist regime in Italy in World War II. \nOn 12 May 1936, the League of Nations allowed Selassie to address the assembly. Selassie was introduced as \"His Imperial Majesty, the Emperor of Ethiopia\" (), to jeering and whistling by Italian journalists. The Romanian delegate Nicolae Titulescu shouted \"To the door with the savages!\" (), and the journalists were removed from the hall. Selassie responded with a speech. In a speech on 30 June 1936 to the assembly, he predicted \"It is us today, it will be you tomorrow\" if the League of Nations permitted Italian aggression. Mussolini grew frustrated over the continued recognition of Selassie by the League of Nations and on 11 December 1937 announced the withdrawal of Italy's delegation to the League.\nSecond World War.\nOn 10 June 1940, Italy declared war on Britain and France, which made Italian military forces in Libya a threat to Egypt and those in the Italian East Africa a danger to the British and French territories in the Horn of Africa. Italian belligerence also closed the Mediterranean to Allied merchant ships and endangered British supply routes along the coast of East Africa, the Gulf of Aden, Red Sea and the Suez Canal. (The Kingdom of Egypt remained neutral during World War II, but the Anglo-Egyptian Treaty of 1936 allowed the British to occupy Egypt and Anglo-Egyptian Sudan.) Egypt, the Suez Canal, French Somaliland and British Somaliland were also vulnerable to invasion, but the \"Comando Supremo\" (Italian General Staff) had planned for a war after 1942. In the summer of 1940, Italy was far from ready for a long war or for the occupation of large areas of Africa.\nHostilities began on 13 June 1940, with an Italian air raid on the base of 1 Squadron Southern Rhodesian Air Force (237 (Rhodesia) Squadron RAF) at Wajir in the East Africa Protectorate (Kenya). In August 1940, the protectorate of British Somaliland was occupied by Italian forces and absorbed into Italian East Africa, which lasted around six months. Anthony Eden, the Secretary of State for War, convened a conference in Khartoum at the end of October 1940 with Selassie, South African Prime Minister Jan Smuts, Wavell, Lieutenant-General William Platt and Lieutenant-General Alan Cunningham. A plan to attack Italian East Africa, including support for Ethiopian resistance forces, was agreed. General Wavell, commander of British troops in the Middle East, charged Colonel Sandford to make plans to aid and mobilize the Ethiopian patriots. \nBy early 1941, Italian forces had been largely pushed back from Kenya and Sudan. On 6 April 1941, Addis Ababa was occupied by the 11th (African) Division, which received the surrender of the city. The remnants of the Italian forces in the Italian East Africa surrendered after staging a last stand at the Battle of Gondar in November 1941. In Ethiopia, some Italian forces continued to fight in an Italian guerrilla war in Ethiopia against the British and Ethiopian forces until the Armistice of Cassibile (3 September 1943) ended hostilities between Italy and the Allies. In January 1942, with the final official surrender of the Italians, the British signed an interim Anglo-Ethiopian Agreement with Selassie, acknowledging Ethiopian sovereignty. Makonnen Endelkachew was named as Prime Minister and on 19 December 1944, the final Anglo-Ethiopian Agreement was signed.\nTreaty of Paris (1947).\nIn the peace treaty of February 1947, Italy officially renounced sovereignty over its African colonies of Libya, Eritrea and Somalia (art. 23) and recognized the independence of Ethiopia (art. 33). Italy further agreed to:\nEritrea was placed under British military administration and became an autonomous part of Ethiopia in 1952. After 1945, Britain controlled both Somalilands, as protectorates. In November 1949, the United Nations granted Italy trusteeship of Italian Somaliland under close supervision, on condition that Somalia achieve independence within ten years. British Somaliland became independent on 26 June 1960 as the State of Somaliland, the Trust Territory of Somalia (ex-Italian Somaliland) became independent on 1 July 1960 and the territories united as the Somali Republic. After the war, Italian Ethiopians were given a full pardon by the newly returned Selassie, as he saw the opportunity to continue the modernization efforts of the country.\nItalian occupation.\nAdministration.\nItalian East Africa was administered by a single administrative unit, the Governo Generale dell'AOI. (GGAOI), with the city of Addis Abeba as its capital. The colonial government was overseen by Ministry of Italian Africa () and was administered by a Viceroy of Ethiopia and Governor General of Italian East Africa, appointed by the Italian king. Victor Emmanuel III of Italy consequently adopted the title of \"Emperor of Ethiopia\". The dominion was further divided for administrative purposes into six governorates, further divided into forty \"commissariati.\"\nFascist colonial policy in Italian East Africa had a divide and conquer characteristic. To weaken the Orthodox Christian Amhara people who had run Ethiopia in the past, territory claimed by Eritrean Tigray-Tigrinyas and Somalis was given to the Eritrea Governorate and Somalia Governorate. Reconstruction efforts after the war in 1936 were partially focused on benefiting the Muslim peoples in the colony at the expense of the Amhara to strengthen support by Muslims for the Italian colony. \nIn 1938 Mussolini enacted The Italian Racial Laws (), which institutionalized racial discrimination against Italian Jews and African inhabitants of the Italian Empire. These laws, and later a policy of pacification by apartheid, enforced segregation and reinforced racial hierarchies in Italy's colonies, further aligning Italian fascism with Nazi ideology. Italians and Natives were racially segregated and lived in separate parts of towns. Interracial marriage was prohibited and so was having children between those of different races. However concubinages did exist. Those who were mixed-race could not get Italian citizenship or go to schools meant for Italians. Married Italian colonists had to bring their families and those who were unmarried could not employ servants.\nTerritory.\nWhen established in 1936, Italian East Africa consisted of the old Italian possessions in the Horn of Africa: Italian Eritrea and Italian Somaliland, combined with the recently conquered Empire of Ethiopia. The territory was divided into the six governorates: Eritrea and Somalia, consisting of the respective former colonies, enlarged with territory from Ethiopia. The remainder of \"Italian Ethiopia\" consisted of the Harar, Galla-Sidamo, Amhara, and Addis Abeba Governorates. The Addis Abeba Governorate was enlarged into the Scioa Governorate with territory from neighboring Harar, Galla-Sidamo and Amhara in November 1938.\nItalian East Africa was briefly enlarged in 1940, as Italian forces invaded British Somaliland, thereby bringing all Somali territories, aside from the small colony of French Somaliland, under Italian administration. At its largest extent, The colony occupied territories in British Somaliland, British Kenya, and Anglo-Egyptian Sudan. However, it was dismembered only a year later, when in the course of the East African campaign.\nEconomic development.\nItaly's Fascist regime encouraged Italian peasants to colonize Ethiopia by setting up farms and small manufacturing businesses. However, few Italians came to the Ethiopian colony, with most going to Eritrea and Somalia. While Italian Eritrea enjoyed some degree of development, supported by nearly 80,000 Italian colonists, by 1940 only 3,200 farmers had arrived in Ethiopia, less than ten percent of the Fascist regime's goal. Continued insurgency by native Ethiopians, lack of natural resources, rough terrain, and uncertainty of political and military conditions discouraged development and settlement in the countryside.\nTransportation and public works.\nThe Italians made a significant effort to build roads in the territory and most of the money given to the territory by the government for \"civilian works\" between 1937 and 1941 were for building roads. The number of laborers numbered in the thousands and were recruited from Italy, Ethiopia, Sudan and Yemen; though the most demanding work was left to laborers who were not Italian. It was reported that \"at the height of construction, nearly 64,000 Italians relocated to the AOI to build roads\". Italians constructed a road between Addis Ababa and Massaua, Addis Ababa and Mogadishu, and Addis Ababa to Assab. \nDams and hydroelectric plants were also built. Public companies were established in Ethiopian governorates, such as the Ethiopian Electricity Company (). \nItalians built additional airports and in 1936 started the \"Linea dell'Impero\", a flight connecting Addis Ababa to Rome. The line was opened after the Italian conquest of Ethiopia and was followed by the first air links with the AOI governorates. The route was enlarged to 6,379\u00a0km and initially joined Rome with Addis Ababa via Syracuse, Benghazi, Cairo, Wadi Halfa, Khartoum, Kassala, Asmara, Dire Dawa.\n900\u00a0km of railways were reconstructed or initiated (like the railway between Addis Ababa and Assab). The Djibouti\u2013Addis Ababa Railway, the most significant railway in Italian East Africa, was acquired following the Italian conquest of Ethiopia in 1936. Until 1935, steam trains operated the 784 km route, taking about 36 hours to travel between Addis Ababa and Djibouti. In 1938, Italy introduced four high-capacity \"Type 038\" rail-cars, derived from the Fiat ALn56 model, increasing speeds to 70 km/h and reducing travel time to 18 hours. These diesel railcars remained in use until the mid-1960s. Major stations offered bus connections to other cities in Italian East Africa, and a fire brigade unit was established near Addis Ababa station\u2014the only one of its kind in Africa at the time.\nItalians invested substantively in Ethiopian infrastructure development. However Ethiopia and Italian East Africa proved to be extremely expensive to maintain, as the budget for the fiscal year 1936\u201337 had been set at 19.136 billion lira to create the necessary infrastructure for the colony. At the time, Italy's entire yearly revenue was only 18.581 billion lira.\nAgriculture.\nFarming was encouraged particularly in Ethiopia for settlement purposes. Most of Italian East Africa's exports were bananas and pineapples. While most of Italy's exports to Italian East Africa were food products consisting of: \"wheat flour and semolina, pasta, salt, sugar, cheese, cured meats, canned vegetables and meat, olive oil, wine, spirits, and mineral water\". Farming was not as successful as anticipated by the Italians in Ethiopia due to factors such as having poor relations with native Ethiopians and having goals that were \"too ambitious to fulfill\". A sharecropping system was encouraged in Ethiopia by the Italian colonial authorities. The final wheat harvest in 1940 was bad due to a grasshopper plague and a fungus outbreak. Efforts were made to increase existing cotton production in Ethiopia which prior to the arrival of the Italians in Ethiopia was done at a small scale.\nEducation.\nPrior to Fascism, education in Italian Somaliland and Italian Eritrea had primarily been the responsibility of both Roman Catholic and Protestant missionaries. With Mussolini's rise to power, government schools were created which eventually incorporated the Catholic missionaries' educational programmes while those of the Protestant missionaries became marginalised and circumscribed. Andrea Festa, who was made director of the central office governing primary education in Eritrea in November 1932, declared in 1934 that Fascist efforts in education needed to ensure that native Africans were \"acquainted with a little of our civilisation\" and that they needed to \"know Italy, its glories, and ancient history, in order to, become a conscious militia man in the shade of our flag.\" Such education initiatives were designed to train Africans in a variety of practical tasks useful to the Fascist regime as well as to indoctrinate them with the tenets and lifestyle of Fascist ideology with the aim of creating citizens obedient and subservient to the state. \nThe propagandistic nature of the education was especially apparent in history textbooks issued to African children, which entirely omitted any discussion of events such as Italian disunity, Giuseppe Mazzini's \"Young Italy\" movement, the revolutions of 1848, or Giuseppe Garibaldi's Expedition of the Thousand and instead stressed the \"glories\" of the Roman Empire and those of the Italian state that claimed to be its successor. Glorification and lionisation of Mussolini and his \"great work\" likewise pervaded them, while periods during which Libya and other then-Italian possessions had been controlled by older, non-Italian empires, such as the Ottoman Empire, were portrayed through an unflattering lens. Use of the Fascist salute was mandatory in schools for African children, who were constantly encouraged to become \"little soldiers of the Duce\", and every day there was morning ceremony at which the Italian flag was hoisted and patriotic songs were sung. Italian children, whose education the Fascist government prioritised over that of Africans, received education similar to that in Fascist Italy's metropole, though with some aspects of it tailored to the local situation in East Africa. Fascist education in the colony proved to be a failure in the end, with only one twentieth of Italian colonial soldiers possessing any literacy.\nIn Italian East Africa, Fascist Italy sought to neutralize any educational institutions which provided instruction to Africans beyond the level expected by Fascist ideology. In particular the secondary education network in the Ethiopian Empire had prepared and enabled a relatively small but significant amount of Ethiopians to study abroad at universities in Europe. As a result of this policy and state-sponsored mass murder, post-World War II Ethiopia found itself impoverished of skilled workers due to the very limited and propagandistic education provided to its non-Italian inhabitants under Mussolini's rule. During World War II, few African natives displayed any loyalty to the Fascist state that the state's schools had so fervently tried to instill.\nBanknotes and postage stamps.\nOn 5 May 1936 the capital Addis Ababa was captured by the Italians: on 22 May three new stamps showing the King of Italy were issued. Four further values inscribed \"ETIOPIA\" were issued on 5 December 1936. After that date, the stamps were issued with the name \"Africa Orientale Italiana\" on it.\nDemographics.\nIn 1939, there were 165,267 Italian citizens in Italian East Africa, mostly concentrated in Asmara, Addis Ababa, and Mogadishu. The total population was estimated at 12.1 million, with an uneven distribution across the region. Eritrea had around 1.5 million people in 90,000 square miles (16.7 people per square mile), Ethiopia had 9.5 million people in 305,000 square miles (31 people per square mile), and Italian Somaliland had 1.1 million people in 271,000 square miles (4 people per square mile). Most Italians in Ethiopia were troops because Ethiopian resistance in the countryside made permanent settlement difficult. Frequent attacks disrupted efforts to establish enough farms and secure food supplies, preventing the troops from transitioning into farming colonists.\nAtrocities.\nWar Crimes.\nDuring the Second Italo-Ethiopian War, Italian forces used between 300 and 500 tons of mustard gas (yperite) against both military and civilian targets, despite being a signatory to the 1925 Geneva Protocol banning chemical warfare. This gas, produced during World War I, was a decisive factor in the conflict, with historian Walter Laqueur estimating that up to one-third of Ethiopian casualties resulted from chemical weapons. Although Italy justified its use of gas by citing the execution of pilot Tito Minniti, Mussolini had authorized gas attacks two months before Minniti's death and later expanded their use on a massive scale. Mustard gas was delivered via bombs and gas shells, sprayed from aircraft onto Ethiopian soldiers, villages, and even Red Cross medical units. The attacks, which the Italians attempted to keep secret, were exposed by the International Red Cross and foreign observers. Selassie, in his plea to the League of Nations, condemned the use of chemical weapons, detailing their widespread and horrific effects on people, animals, and the environment. \nEthiopian forces employed Dum-Dum bullets, banned under the Hague Convention, and reportedly mutilated captured Eritrean Askari and Italian soldiers. On 13 February 1936, a camp of civilian workers for the logistics company Gondrand, engaged at the time in road construction, was attacked and massacred at dawn by Ethiopian soldiers under the orders of Ras Imru. The massacre was publicized by Fascist Italy in an attempt to justify its ongoing invasion and the escalating brutality of the reprisals.\nYekatit 12.\nOn 19 February 1937, to celebrate the birth of the Prince of Naples, Rodolfo Graziani announced he would personally distribute alms to the poor at the Genete Leul Palace (also known as the Little Gebbi). Two young Eritreans living in Ethiopia named Abraha Deboch and Mogus Asgedom made an attempt on Graziani's life by throwing grenades. Following the attempt, Italian soldiers raided the Debre Libanos monastery, where the assassins were believed to have taken refuge, and executed hundreds of the monks and nuns. Italian forces, mostly Blackshirts, then continued to carry out brutal reprisals in Addis Ababa. Over the next three days, thousands of Ethiopian civilians were massacred: estimates range from 1,400 to 19,000 deaths, while Ethiopians claimed 30,000 victims. Italian troops, Blackshirt militias, and Fascist supporters set fire to homes, businesses, and churches, killing indiscriminately. The massacre has come to be known as Yekatit 12 (the date in the Ethiopian language). After the massacres, Graziani became known as \"the Butcher of Ethiopia\" and was subsequently removed by Mussolini. Mussolini viewed the action as a major setback for Fascist propaganda and was concerned that the growing resentment among the natives would increase the number of individuals joining the resistance. As a result, Graziani was replaced by Prince Amedeo, Duke of Aosta, whose pacification by apartheid approach minimized the risk of a united front against the Italians as an aftermath of the massacre. \nConcentration camps.\nNocra prison camp was a notorious Italian detention facility located on Nocra Island in the Dahlak Archipelago, Eritrea. Originally established by the Italians in the late 19th century, it was used throughout the colonial period as a remote and harsh prison for political dissidents, Ethiopian resistance fighters, and other individuals deemed threats to Italian rule. The prison was infamous for its inhumane conditions, including extreme heat, forced labor, and inadequate food. Prisoners were subjected to harsh punishments, with many dying due to disease or malnutrition. During the Italian occupation of Ethiopia (1936-1941), Nocra housed much of the intelligentsia of Ethiopia with some being executed and the remainder exiled to penal colonies. As a result, post-World War II Ethiopia found itself impoverished of skilled workers.\nThe Danane concentration camp was another Italian concentration camp established near Mogadishu in Italian East Africa. Danane concentration camp Prisoners were transported from Addis Ababa to Danane in covered trucks by night to avoid them being seen. By the time they arrived at Danane, a journey of more than four weeks, several had died of disease and hardships along the way. Conflicting reports make it hard to accurately assess the extent of death among the prisoners. Graziani ordered that they be given only enough food to survive, and the conditions in which they were held were dire. The facilities were poor, with insufficient latrines, and they faced a humid climate that contributed to outbreaks of malaria, stomach infections, and venereal diseases.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39599", "revid": "612852", "url": "https://en.wikipedia.org/wiki?curid=39599", "title": "Blum Blum Shub", "text": "Pseudorandom number generator\nBlum Blum Shub (B.B.S.) is a pseudorandom number generator proposed in 1986 by Lenore Blum, Manuel Blum and Michael Shub that is derived from Michael O. Rabin's one-way function.\nBlum Blum Shub takes the form\nformula_1,\nwhere \"M\" = \"pq\" is the product of two large primes \"p\" and \"q\". At each step of the algorithm, some output is derived from \"x\"\"n\"+1; the output is commonly either the bit parity of \"x\"\"n\"+1 or one or more of the least significant bits of \"x\"\"n\"+1.\nThe seed \"x\"0 should be an integer that is co-prime to \"M\" (i.e. \"p\" and \"q\" are not factors of \"x\"0) and not 1 or 0.\nThe two primes, \"p\" and \"q\", should both be congruent to 3 (mod 4) (this guarantees that each quadratic residue has one square root which is also a quadratic residue), and should be safe primes with a small gcd((\"p-3\")\"/2\", (\"q-3\")\"/2\") (this makes the cycle length large).\nAn interesting characteristic of the Blum Blum Shub generator is the possibility to calculate any \"x\"\"i\" value directly (via Euler's theorem):\nformula_2,\nwhere formula_3 is the Carmichael function. (Here we have formula_4).\nSecurity.\nThere is a proof reducing its security to the computational difficulty of factoring. When the primes are chosen appropriately, and \"O\"(log log \"M\") lower-order bits of each \"xn\" are output, then in the limit as \"M\" grows large, distinguishing the output bits from random should be at least as difficult as solving the quadratic residuosity problem modulo \"M\".\nThe performance of the BBS random-number generator depends on the size of the modulus \"M\" and the number of bits per iteration \"j\". While lowering \"M\" or increasing \"j\" makes the algorithm faster, doing so also reduces the security. A 2005 paper gives concrete, as opposed to asymptotic, security proof of BBS, for a given \"M\" and \"j\". The result can also be used to guide choices of the two numbers by balancing expected security against computational cost.\nExample.\nLet formula_5, formula_6 and formula_7 (where formula_8 is the seed). We can expect to get a large cycle length for those small numbers, because formula_9.\nThe generator starts to evaluate formula_10 by using formula_11 and creates the sequence formula_10, formula_13, formula_14, formula_15 formula_16 = 9, 81, 236, 36, 31, 202. The following table shows the output (in bits) for the different bit selection methods used to determine the output.\nThe following is a Python implementation that does check for primality.\nimport sympy\ndef blum_blum_shub(p1: int, p2: int, seed: int, iterations: int) -&gt; list[int]:\n \"\"\"Blum Blum Shub is a pseudorandom number generator.\"\"\"\n assert p1 % 4 == 3\n assert p2 % 4 == 3\n assert sympy.isprime(p1 // 2)\n assert sympy.isprime(p2 // 2)\n n = p1 * p2\n numbers = []\n for _ in range(iterations):\n seed = (seed**2) % n\n if seed in numbers:\n print(f\"The RNG has fallen into a loop at {len(numbers)} steps\")\n return numbers\n numbers.append(seed)\n return numbers\nprint(blum_blum_shub(11, 23, 3, 100))\nThe following Common Lisp implementation provides a simple demonstration of the generator, in particular regarding the three bit selection methods. It is important to note that the requirements imposed upon the parameters \"p\", \"q\" and \"s\" (seed) are not checked.\n \"Returns the number of 1-valued bits in the integer-encoded BITS.\"\n (declare (type (integer 0 *) bits))\n (the (integer 0 *) (logcount bits)))\n \"Returns the even parity bit of the integer-encoded BITS.\"\n (declare (type (integer 0 *) bits))\n (the bit (mod (get-number-of-1-bits bits) 2)))\n \"Returns the least significant bit of the integer-encoded BITS.\"\n (declare (type (integer 0 *) bits))\n (the bit (ldb (byte 1 0) bits)))\n \"Returns a function of no arguments which represents a simple\n Blum-Blum-Shub pseudorandom number generator, configured to use the\n generator parameters P, Q, and S (seed), and returning three values:\n (1) the number x[n+1],\n (2) the even parity bit of the number,\n (3) the least significant bit of the number.\n Please note that the parameters P, Q, and S are not checked in\n accordance to the conditions described in the article.\"\n (declare (type (integer 0 *) p q s))\n (let ((M (* p q)) ;; M = p * q\n (x[n] s)) ;; x0 = seed\n (declare (type (integer 0 *) M x[n]))\n #'(lambda ()\n ;; x[n+1] = x[n]^2 mod M\n (let ((x[n+1] (mod (* x[n] x[n]) M)))\n (declare (type (integer 0 *) x[n+1]))\n ;; Compute the random bit(s) based on x[n+1].\n (let ((even-parity-bit (get-even-parity-bit x[n+1]))\n (least-significant-bit (get-least-significant-bit x[n+1])))\n (declare (type bit even-parity-bit))\n (declare (type bit least-significant-bit))\n ;; Update the state such that x[n+1] becomes the new x[n].\n (setf x[n] x[n+1])\n (values x[n+1]\n even-parity-bit\n least-significant-bit))))))\n (declare (type (function () (values (integer 0 *) bit bit)) bbs))\n (format T \"~&amp;Keys: E = even parity, L = least significant\")\n (format T \"~2%\")\n (format T \"~&amp;x[n+1] | E | L\")\n (format T \"~&amp;--------------\")\n (loop repeat 6 do\n (multiple-value-bind (x[n+1] even-parity-bit least-significant-bit)\n (funcall bbs)\n (declare (type (integer 0 *) x[n+1]))\n (declare (type bit even-parity-bit))\n (declare (type bit least-significant-bit))\n (format T \"~&amp;~6d | ~d | ~d\"\n x[n+1] even-parity-bit least-significant-bit))))\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39602", "revid": "4637213", "url": "https://en.wikipedia.org/wiki?curid=39602", "title": "Roma", "text": "Roma or ROMA may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "39606", "revid": "50568205", "url": "https://en.wikipedia.org/wiki?curid=39606", "title": "Peckforton Castle", "text": "Grade I listed English country house in Cheshire East, United Kingdom\nPeckforton Castle is a Victorian country house built in the style of a medieval castle. It stands in woodland at the north end of Peckforton Hills northwest of the village of Peckforton, Cheshire, England. It is recorded in the National Heritage List for England as a designated Grade\u00a0I listed building. The house was built in the middle of the 19th\u00a0century as a family home for John Tollemache, a wealthy Cheshire landowner, estate manager, and member of parliament. It was designed by Anthony Salvin in the Gothic style. During the Second World War it was used as a hostel for physically disabled children.\nThe Tollemache family used the castle for occasional gatherings, but otherwise it was unused until 1969. From 1969 to 1980 the castle was leased by the 4th Lord Tollemache to George W. Barrett, and it again became a private residence and closed to the public. The right wing and tower and the castle gardens were restored by Barrett, an American employed by the U.S. Government. His daughter Pascale's wedding was the first to be held in the chapel and a special decree had to be obtained by the Archbishop of Canterbury to legally hold Catholic weddings in the grounds of the castle.\nDuring the 1970s and 1980s it was used as a location for shooting films and television programmes. The castle was bought in 1988 by Evelyn Graybill, who converted it into a hotel. In 2006 it was purchased by the Naylor family, who expanded its use to include hosting weddings, conferences and other functions.\nEarly history.\nPeckforton Castle was built between 1844 and 1850 for John Tollemache, the largest landowner in Cheshire at the time, who was described by William Ewart Gladstone as \"the greatest estate manager of his day\". Tollemache's first choice of architect was George Latham of Nantwich, but he was not appointed, and was paid \u00a32,000 in compensation. Instead Tollemache appointed Anthony Salvin, who had a greater reputation and more experience, and who had already carried out work on the Tollemache manor house, Helmingham Hall in Suffolk. The castle was built by Dean and Son of Leftwich, with Joseph Cookson of Tarporley acting as clerk of works. Stone was obtained from a quarry about to the west of the site, and a railway was built to carry the stone. The castle cost \u00a360,000.\nAlthough it was built as a family home its design was that of a medieval castle. It has a gatehouse, a portcullis, a dry moat, external windows that are little more than arrow slots, and large towers. In 1851 \"The Illustrated London News\" said that it \"seems to exhibit the peculiar beauties of Carnarvon Castle without its inconveniences\" and in 1858 Sir George Gilbert Scott called it \"the largest and most carefully and learnedly executed Gothic mansion of the present\" and that it was \"the very height of masquerading\". It is regarded as \"the last serious fortified home built in England\" and \"it was executed to the highest standards and is one of the great buildings of its age\".\nThere has been debate about the motives for building a more-or-less complete medieval-style castle in the 19th\u00a0century. Although he was a great estate manager, Tollemache was also perceived as \"a man of considerable eccentricity\". Dr Jill Allibone is of the opinion that he might have been protecting himself and his family from the political troubles of the time. In a defensive building he would be able to protect himself against any revolution by the masses from nearby Manchester or Liverpool. A possible practical reason for building such a solid residence rather than an Italianate-style villa was to provide shelter from the adverse weather conditions which could affect the Cheshire plain. However Durdey comes to the conclusion that the decisive factors were to use his \"vast inheritance\" to provide himself with a house that was \"impressive, dominant and suitable for Cheshire's greatest landowner\".\nLocation.\nPeckforton Castle stands in a wooded area near the northern extremity of Peckforton Hills at an elevation of . The land falls steeply downwards to the north and the west of the castle, and the Sandstone Trail, a long-distance footpath, runs along the base of these slopes. The ruins of Beeston Castle stand on a separate steeply sloping hill to the north. The village of Beeston is to the northeast and the village of Peckforton is to the southeast. Access is via the road between Beeston and Peckforton.\nArchitecture.\nCastle.\nExternal.\nThe castle is faced with red sandstone, and has lead, asphalt and tile roofs. It is mainly in three storeys with a five-storey tower. The buildings are arranged around a ward with the principal accommodation on the north side. It is surrounded by a dry moat which is bridged at the gatehouse. To the west of the inner ward are the stables, the coach house, a rectangular bell tower and the kitchens and service area. To the north is the great hall range which consists of 18\u00a0bays. Behind the entrance to the hall is the circular main tower. At the east end of the gallery wing is the octagonal library tower. The outer walls of the castle have full-height slender turrets at the changes in direction. Corbel tables support part of the battlements. The walls contain arrow slots, and in the gatehouse is a garderobe. The flat roof has a crenellated parapet.\nInternal.\nThe porch leads into the great hall which has a Minton tile floor and a large stone chimney piece. In the east wing is the long gallery which has oak panelling, a chimney-piece and a panelled ceiling. Behind the long gallery is an irregularly-shaped billiard room and the drawing room. To the south of these is the library. Behind the great hall is the main staircase. The circular tower at the north-west corner contains the octagonal dining room with a Minton tile floor, two fireplaces, and a vault of eight radial ribs running to a central boss. The room contains an oak sideboard with a carved Green Man. Below the dining room is a wine cellar. On the fifth floor of the circular tower is a room designed for playing rackets, which is approached by a stone spiral staircase.\nGarden.\nThe castle had no formal garden, but at the bottom of the drive were kitchen gardens which included vegetable gardens, an orchard, extensive glass houses and a large orangery. At one time, 17\u00a0gardeners were employed.\nChapel.\nOn the east side of the ward is the family's private chapel, a Grade\u00a0II* listed building. It was also designed by Salvin and is constructed of rock-faced sandstone with a tile roof. Its plan consists of a two-bay nave, a south aisle, a vestry, and a narrower and lower single-bay chancel. On the gable ends of the nave and chancel are stone cross finials. Over the chancel arch is a cruciform stone bellcote. Inside the chapel an arcade of three Gothic arches separates the south aisle from the nave. The reredos is made of oak and is inscribed with the Lord's Prayer and the Ten Commandments. The choirstalls and the benches in the nave are carved with poppyheads. The baptistry at the west end contains a carved stone font with a carved oak cover. Although it is described as a modest building, the chapel is considered to \"complete the ensemble\" of the castle.\nEntrance lodge.\nThe entrance lodge to the southeast of the castle is also listed at Grade\u00a0II*. It was designed by Salvin and is constructed in red brick and stone with a tile roof. It consists of an archway with a round turret behind and a two-storey lodge to the left.\nLater history and present use.\nBefore moving into the castle in the 1890s, Wilbraham Tollemache, 2nd Baron Tollemache added central heating and electric light. In 1922 a large scheme of afforestation was started on the Peckforton Hills, and the resulting woodland has been granted the status of a Site of Special Scientific Interest. Bentley Tollemache, 3rd Baron Tollemache, grandson of Wilbraham Tollemache, and his family left Peckforton at the outbreak of the Second World War in 1939.\nDuring the war the castle was used as a hostel for physically handicapped children who had been evacuated from the London area. Bentley Tollemache died in 1955 and, as he had no sons, the estate passed to his cousin, John Tollemache.\nDuring the 1970s, 1980s and early 1990s, the castle was used as a location for shooting films and television programmes. These include the \"Doctor Who\" serial \"The Time Warrior\", broadcast in 1973\u201374, and a 1991 movie of \"Robin Hood\" starring Patrick Bergin and Uma Thurman. From 1982 to 1986, \"Treasure Trap\", one of the world's first live action role-playing games, took place on the site. In 1988 the castle was purchased by an American, Evelyn Graybill, for \u00a31\u00a0million. She renovated most of the building and secured planning permission to convert it into a hotel.\nIn 2006 a member of the Naylor family was married in the castle, and the family subsequently bought it. Peckforton is now used as a hotel and for corporate events and weddings. The building was severely damaged by fire in June 2011 during a wedding, the cost of the damage being in the region of \u00a36 million. In December 2011 the bridegroom admitted to a charge of arson.\nReferences.\nCitations\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39608", "revid": "40511069", "url": "https://en.wikipedia.org/wiki?curid=39608", "title": "Walter Winterbottom", "text": "English football player and manager (1913\u20132002)\nSir Walter Winterbottom (31 March 1913\u00a0\u2013 16 February 2002) was an English football player and coach. He was the first manager of the England national team (1946\u20131962) and Director of Coaching for The Football Association (the FA). He resigned from the FA in 1962 to become General Secretary of the Central Council of Physical Recreation (CCPR) and was appointed as the first director of the Sports Council in 1965. He was knighted for his services to sport in 1978 when he retired. The Football Association marked the 100th anniversary of Winterbottom's birth by commissioning a bust which was unveiled by Roy Hodgson at St George's Park on 23 April 2013 in recognition of his outstanding contribution to the development of English football.\nEarly years.\nBorn in Oldham, Lancashire, Walter Winterbottom was the only son of James Winterbottom, a ring frame fitter in a textile machine works. At the age of 12 he was awarded a scholarship to Oldham High School where he excelled. He won a bursary to Chester Diocesan Teachers Training College, graduating as the top student in 1933 and took a teaching post at the Alexandra Road School, Oldham. Whilst teaching he played football for Royton Amateurs and then Mossley where he was spotted by Manchester United. He signed for United as a part-time professional in 1936 but continued teaching. In his first season (1936/37) at Manchester United he showed great promise, playing 21 first team League games and two FA cup games, appearing as wing half and centre half. But in the following two seasons he made only four first team appearances. and 41 Central League appearances, his playing career effectively ended by a spinal disease, later diagnosed as ankylosing spondylitis. Whilst still playing for Manchester United he left his teaching position to study at Carnegie College of Physical Education, Leeds. On graduating he was appointed as a lecturer.\nDuring World War II Winterbottom served as an officer in the Royal Air Force, reaching the rank of wing commander and working at the Air Ministry with overall responsibility for training PE instructors at home and overseas. He was also a guest player with Chelsea and ran coaching courses for the FA at grammar schools in London. In 1946 Stanley Rous, who was the secretary of The Football Association, persuaded the FA council to appoint Winterbottom as the FA's first Director of Coaching and suggested he take on the additional responsibility of being the first England team manager.\nEngland team manager.\nWalter Winterbottom has the distinction of being England's first, youngest and longest serving England team manager; he is also the only England manager to have had no previous professional managerial experience. In all matches in which he was in charge, England played 139, won 78, drew 33, and lost 28, scoring 383 and conceding 196. At home England lost six matches in sixteen years. England won the British Home Championship in thirteen out of his sixteen seasons (seven times outright and six times sharing top place). In the World Cup tournament England qualified on all four occasions, reaching the quarter-finals twice, playing 28 matches, winning 15, drawing 7 and losing 6; goals for 75 against 35 (including World Cup qualifying matches).\nAlthough he had coaching and managerial responsibilities, Winterbottom never had the power to pick his own team and it was instead chosen by a selection committee. Over time his technical knowledge increasingly influenced selectors. Finally, prior to Alf Ramsey's arrival in 1962, he convinced the FA that the team manager must have sole control of selection.\nDuring his time Winterbottom repeatedly warned the English football establishment that countries in Continental Europe and South America were overtaking England and that English football had to change. His sixteen years as England team manager helped greatly in creating a modern and competitive national team and four years after his departure in 1966 England won the World Cup. His innovations included the introduction of England B, Under 23, youth and schoolboy teams providing players with continuity and experience in international football before being selected for the full England team.\nNotable victories during his tenure were 10\u20140 away to Portugal in 1947, 4\u20140 away to Italy in 1948, 3\u20141 at home to recently crowned World Champions West Germany in 1954, 4\u20142 at home to Brazil in 1956 and 9\u20143 at home to Scotland in 1961. Notable defeats were losing 2\u20130 to the Republic of Ireland at Goodison Park, losing 1\u20140 to the USA in the 1950 World Cup and 6\u20143 at home to Hungary in 1953, when England lost their unbeaten home record to a foreign team at Wembley, followed by a 7\u20141 away defeat to the same team in 1954.\nAlso while he was manager, England visited Argentina, Brazil, Chile, Denmark, Mexico, Peru, Portugal, the Soviet Union, United States and Uruguay for the first time.\nFIFA World Cup record.\nWinterbottom led England to four consecutive World Cup finals, a record subsequently equalled only by Helmut Sch\u00f6n of West Germany. England entered the World Cup for the first time in 1950, qualifying for the tournament in Brazil by winning the British Home Championship. England had never before played in South America. They beat Chile by 2\u20130 but lost 1\u20140 to the USA and 1\u20140 to Spain to be eliminated in the first round.\nWinterbottom again led England to qualification in Switzerland in 1954 by winning the British Home championship. A 4\u20144 draw against Belgium and a 2\u20140 victory against Switzerland took them to the quarter-finals where they were beaten 4\u20142 by the defending champions, Uruguay.\nEngland qualified for the 1958 FIFA World Cup in Sweden with wins over the Republic of Ireland and Denmark, with a team that had lost only once in 17 games. Three months before the tournament began the Munich air disaster robbed the team of key players from Manchester United: Roger Byrne, Tommy Taylor and Duncan Edwards died. England drew against the USSR, Brazil and Austria but lost to the Soviet Union in a playoff for a quarter-final place.\nWinterbottom again led his team to qualification for the 1962 World Cup in Chile with wins over Portugal and Luxembourg. After progressing from their group on goal average, England reached the quarter-finals but were beaten 3\u20141 by the eventual winners, Brazil.\nFA Director of Coaching.\nAlthough Winterbottom is best known as the England team manager, it is in coaching that he made important contributions to the development of English football. He made no secret of his belief that his job as Director of Coaching was the more important of his two roles at the FA.\nWhen he joined the FA in 1946, club directors, managers and players were cynical about the need for coaching but Winterbottom had a passion for coaching and a vision of how it should develop.\nHe soon created a national coaching scheme with summer residential courses at Lilleshall, Shropshire, and persuaded some of his international players to take the courses that led to exams for the FA preliminary and full coaching badges. This gave the scheme credibility. They developed their teaching skills by coaching in schools and then moved into part-time coaching positions in junior clubs. He gathered around him a cadre of young FA staff coaches: men like Bill Nicholson, Don Howe, Alan Brown, Ron Greenwood, Dave Sexton, Malcolm Allison, Joe Mercer, Vic Buckingham, Jimmy Hill and Bobby Robson. Over time a new breed of managers emerged in the League clubs and began to change attitudes to coaching.\nWinterbottom's courses were expanded to include professional players, referees, schoolmasters, club trainers, schoolboys and youth leaders. In addition to Lilleshall they were held at Loughborough College, Carnegie College, Bisham Abbey and Birmingham University. In 1947 three hundred had taken the full coaching award and the numbers of qualified coaches grew each year.\nThe courses attracted international participation and praise. Winterbottom was regarded by many as a leading technical thinker and exponent of association football, of his generation, in the world and lectured internationally.\nHe inspired a new generation of managers, most notably Ron Greenwood and Bobby Robson, who graduated through every level of coaching, both eventually becoming England team manager.\nCriticism.\nIn assessing Winterbottom's tenure as England manager, Goldblatt writes that \"[Winterbottom] introduced a measure of tactical thinking and discussion to the England squad, though his inability to anticipate or learn significantly from the Hungarian debacle suggests that his grasp of tactics and communication with the players was limited.\" William Baker writes that Winterbottom, because of his \"upper-class origins [sic]\", could not \"effectively instruct, much less inspire, working-class footballers.\" Football journalist Brian Glanville said in an interview: \"I got on very well with Walter Winterbottom, but he was a rotten manager.\"\nPublishing.\nWinterbottom was also responsible for the publishing at the FA. The first coaching bulletin was launched in 1946 and this became the \"FA Bulletin\" and then the \"FA News\". The \"FA Year Book\" was introduced in 1948, along with the \"FA Book for Boys\" annual. The first coaching films and film strips followed in 1950.\nAn important landmark was the publication of Winterbottom's book, \"Soccer Coaching\", the first modern soccer coaching manual. This was followed by three more books, \"Skilful Soccer\", \"Modern Soccer\" and \"Training for Soccer\".\nSports administrator.\nIn 1962 Winterbottom resigned from the FA and took up an appointment as General Secretary of the Central Council of Physical Recreation and two years later became the director of the newly formed Sports Council. He stepped onto the wider stage of sport and emerged to have a profound effect on sport in Britain during the second half of the twentieth century.\nCentral Council of Physical Recreation.\nAt the Central Council of Physical Education (CCPR) Winterbottom worked to provide coaches and better facilities for sports governing bodies. He soon became involved in the ongoing political debate about the recommendations of the 1960 Report of the Wolfenden Committee on Sport, which had recommended the establishment of a Sports Council responsible for distributing government money to sport. He was in favour but the CCPR was divided on the issue. In 1965 the Government under set up a Sports Council and Winterbottom was seconded to become the first director of the Sports Council with Denis Howell as his chairman.\nSports Council.\nWinterbottom believed that participation in a sport played a much more important role in society that was generally accepted. For 16 years he battled to win significantly more investment in sport from national and local government to support a Sport for All campaign. Despite a harsh economic climate great progress was made in providing new facilities. In ten years 499 sports centres were built and 524 new swimming pools. Under his leadership sports governing bodies were helped to develop more professional organisations and provide more coaches. He conceived the idea of the Sports Aid Foundation, raising money from industry to back young elite sportsmen and women with Olympic medal winning chances.\nHe was a member of the Council of Europe and Chairman of the Committee for the Development of Sport and was influential in the acceptance of the Sport For All concept by Canada and UNESCO.\nLater life.\nIn 1978, after reaching the age of 65, Winterbottom retired from the Sports Council and was knighted for his services to sport. He became an advisor to the British government on ways in which British manufacturers of sports equipment could work with foreign firms. In 1979, he visited Australia and New Zealand to help their governments to support sport in the community.\nHe was head of the FIFA Technical Studies Group for the World Cup in 1966, 1970, 1974, 1978 and a member in 1982. In 1985 The Winterbottom Report, an FA enquiry into artificial playing surfaces was published and in 1987\u201389 he was a member of the Football League enquiry into artificial pitches.\nHe died in the Royal Surrey Hospital after an operation for cancer on 16 February 2002. He was 88 years old. A memorial service was held at St. Nicolas Church, Cranleigh, Surrey on 1 March 2002.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39610", "revid": "28164502", "url": "https://en.wikipedia.org/wiki?curid=39610", "title": "George Hamilton-Gordon", "text": "George Hamilton-Gordon may refer to:\nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Topics referred to by the same termThis page lists articles about people with the same name. "}
{"id": "39611", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=39611", "title": "OBE", "text": ""}
{"id": "39612", "revid": "4871659", "url": "https://en.wikipedia.org/wiki?curid=39612", "title": "CBE (disambiguation)", "text": "CBE is the initialism for Commander of the Most Excellent Order of the British Empire, a grade within the British order of chivalry.\nCBE may also refer to:\nOther organizations.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "39613", "revid": "266897083", "url": "https://en.wikipedia.org/wiki?curid=39613", "title": "Knighthood", "text": ""}
{"id": "39616", "revid": "51051919", "url": "https://en.wikipedia.org/wiki?curid=39616", "title": "Soho", "text": "District in London, England\nSoho is a district of Westminster, in the City of Westminster in the West End of London. Originally a fashionable district for the aristocracy, it has been one of the main entertainment districts in the capital since the 19th century.\nThe area was developed from farmland by Henry VIII in 1536, when it became a royal park. It became a parish in its own right in the late 17th century, when buildings started to be developed for the upper class, including the laying out of Soho Square in the 1680s. St Anne's Church was established during the late 17th century, and remains a significant local landmark; other churches are the Church of Our Lady of the Assumption and St Gregory and St Patrick's Church in Soho Square. The aristocracy had mostly moved away by the mid-19th century, when Soho was particularly badly hit by an outbreak of cholera in 1854. For much of the 20th century Soho had a reputation as a base for the sex industry in addition to its night life and its location for the headquarters of leading film companies. Since the 1980s, the area has undergone considerable gentrification. It is now predominantly a fashionable district of upmarket restaurants and media offices, with only a small remnant of sex industry venues. London's most prominent gay village is centred on Old Compton Street in Soho.\nSoho's reputation as a major entertainment district of London stems from theatres such as the Windmill Theatre on Great Windmill Street and the Raymond Revuebar owned by entrepreneur Paul Raymond, and music clubs such as the 2i's Coffee Bar and the Marquee Club. Trident Studios was based in Soho, and the nearby Denmark Street has hosted numerous music publishing houses and instrument shops from the 20th century onwards. The independent British film industry centres on Soho, including the British headquarters of Twentieth Century Fox and the British Board of Film Classification offices. The area has been popular for restaurants since the 19th century, including the long-standing Kettner's which was visited by numerous celebrities. Near to Soho is London's Chinatown, centred on Gerrard Street and containing several restaurants and shops.\nName.\nThe name \"Soho\" first appears around 1636. The name is derived from a former hunting cry. James Scott, 1st Duke of Monmouth, used \"soho\" as a rallying call for his men at the Battle of Sedgemoor on 6 July 1685, half a century after the name was first used for this area of London.\nThe Soho name has been reused by other entertainment and restaurant districts such as SoHo, Hong Kong, which derives its name from being located south of Hollywood Road, and the cultural and commercial area of in M\u00e1laga, Spain. The New York City neighbourhood of SoHo, Manhattan, gets its name from its location south of Houston Street, but is also a reference to London's Soho. The Pittsburgh neighbourhood of Uptown was also formerly called Soho, most likely having been named by its founder James Tustin after the London district, though it may refer to Soho, West Midlands.\nLocation.\nSoho has never been an administrative unit with formally defined boundaries; it is about in area, and is usually considered to be bounded by Shaftesbury Avenue to the south, Oxford Street to the north, Regent Street to the west, and Charing Cross Road to the east. Apart from Oxford Street, all of these roads are 19th-century metropolitan improvements. The area to the west is known as Mayfair, to the north Fitzrovia, to the east St Giles and Covent Garden, and to the south St James's. Soho is part of the West End electoral ward which elects three councillors to Westminster City Council.\nThe nearest London Underground stations are Oxford Circus, Piccadilly Circus, Tottenham Court Road, Leicester Square and Covent Garden.\nHistory.\nEarly history.\nDuring the Middle Ages, the area that is now Soho was farmland that belonged to the Abbot and Convent of Abingdon and the master of Burton St Lazar Hospital in Leicestershire, who managed a leper hospital in St Giles in the Fields. In 1536, the land was taken by Henry VIII as a royal park for the Palace of Whitehall. The area south of what is now Shaftesbury Avenue did not stay in the Crown possession for long; Queen Mary sold around in 1554, and most of the remainder was sold between 1590 and 1623. A small section of land remained, until sold by Charles II in 1676.\nIn the 1660s, ownership of Soho Fields passed to Henry Jermyn, 1st Earl of St Albans, who leased 19 out of the of land to Joseph Girle. He was granted permission to develop property and quickly passed the lease and development to bricklayer Richard Frith. Much of the land was granted freehold in 1698 by William III to William Bentinck, 1st Earl of Portland, while the southern part of Soho was sold piecemeal in the 16th and 17th centuries, partly to Robert Sidney, Earl of Leicester.\nSoho was part of the ancient parish of St Martin in the Fields, forming part of the Liberty of Westminster. As the population started to grow, a new church was provided, and in 1687 a new parish of St Anne was established for it. The parish stretched from Oxford Street in the north to Leicester Square in the south and from what is now Charing Cross Road in the east to Wardour Street in the west; it therefore included all of contemporary eastern Soho, including the Chinatown area. The western portion of modern Soho, around Carnaby Street, was part of the parish of St James, which was split off from St Martin in 1686.\nGentrification.\nBuilding progressed rapidly in the late 17th century, with large properties such as Monmouth House (built for James Scott, 1st Duke of Monmouth, Charles II's eldest illegitimate son), Leicester House, Fauconberg House, Carlisle House and Newport House.\nSoho Square was first laid out in the 1680s on the former Soho Fields; by 1691, 41 houses had been completed there. It was originally called King Square in honour of Charles II, and a statue of him was based in the centre. Several upper-class families moved into the area, including those of Richard Graham, 1st Viscount Preston, and Edward Howard, 2nd Earl of Carlisle. The square had become known as Soho Square by 1720, at which point it had fashionable houses on all sides. Only No\u00a010 and No\u00a015 from this period have survived into the 21st century.\nThough the Earls of Leicester and Portland had intended Soho to be an upper-class estate comparable to Bloomsbury, Marylebone and Mayfair, it never developed as such. Immigrants began to settle in the area from around 1680 onwards, particularly French Huguenots after 1688. The area became known as London's French quarter. The French church in Soho Square was founded by Huguenots and opened on 25 March 1893, with a fa\u00e7ade of terracotta and coloured brick designed by Aston Webb.\nCholera outbreak.\nA significant event in the history of epidemiology and public health was John Snow's study of an 1854 outbreak of cholera in Soho. He identified the cause of the outbreak as water from the public pump at the junction of Broad Street (now Broadwick Street) and Cambridge Street (now Lexington Street), close to the rear wall of what is today the John Snow public house.\nSnow mapped the addresses of the sick and noted that they were mostly people whose nearest access to water was the Broad Street pump. He persuaded the authorities to remove the handle of the pump, thus preventing any more of the infected water from being collected. The spring below the pump was later found to have been contaminated with sewage. This is an early example of epidemiology, public health medicine and the application of science\u2014the germ theory of disease\u2014in a real-life crisis. Science writer Steven Johnson has written about the changes related to the cholera outbreak, and notes that almost every building on the street that existed in 1854 has since been replaced. A replica of the pump, with a memorial plaque and without a handle (to signify Snow's action to halt the outbreak) was erected in 1992 near the location of the original.\nDecline.\nBy the mid-18th century, the aristocrats who had been living in Soho Square or Gerrard Street had moved away, as more fashionable areas such as Mayfair became available. The historian and topographer William Maitland wrote that the parish \"so greatly abound with French that is an easy Matter for a Stranger to imagine himself in France.\" Soho's character stems partly from the ensuing neglect by rich and fashionable London, and the lack of the redevelopment that characterised the neighbouring areas.\nThe aristocracy had mostly disappeared from Soho by the 19th century, to be replaced by prostitutes, music halls and small theatres. The population increased significantly, reaching 327 inhabitants per acre by 1851, making the area one of the most densely populated areas of London. Houses became divided into tenements with chronic overcrowding and disease. The 1854 cholera outbreak caused the remaining upper-class families to leave the area. Numerous hospitals were built to cope with the health problem; six were constructed between 1851 and 1874. Businesses catering to household essentials were established at the same time.\nThe restaurant trade in Soho improved dramatically in the early 20th century. The construction of new theatres along Shaftesbury Avenue and Charing Cross Road improved the reputation of the area, and a meal for theatre-goers became common. Public houses in Soho increased in popularity during the 1930s and were frequented by struggling authors, poets and artists.\nRecent history.\nSince the decline of the sex industry&lt;ref name=\"gettyimages/sex-soho\"&gt;&lt;/ref&gt;&lt;ref name=\"gettyimages/1163965755\"&gt;&lt;/ref&gt;&lt;ref name=\"gettyimages/1163965776\"&gt;&lt;/ref&gt;&lt;ref name=\"gettyimages/1163965774\"&gt;&lt;/ref&gt;&lt;ref name=\"gettyimages/592262468\"&gt;&lt;/ref&gt;&lt;ref name=\"gettyimages/strip-club\"&gt;&lt;/ref&gt; in Soho in the 1980s, the area has returned to being more residential. The Soho Housing Association was established in 1976 to provide reasonable rented accommodation. By the 21st century, it had acquired around 400 flats. St Anne's Church in Dean Street was refurbished after decades of neglect, and a Museum of Soho was established.\nOn 30 April 1999, the Admiral Duncan pub on Old Compton Street, which serves the gay community, was damaged by a nail bomb that left three dead and 30 injured. The bomb was the third that had been planted by David Copeland, a neo-Nazi who was attempting to stir up ethnic and homophobic tensions by carrying out a series of bombings.\nIn early February 2020, parts of an unexploded Second World War bomb was discovered by construction workers developing a new mixed residential building in Richmond Mews, near Dean Street. Residents, employees, and pedestrians on Richmond Mews, Dean Street, Meard Street and St Anne's Court were evacuated on both the 3 and 4 February 2020. All road junctions connecting to the streets closed during retrieval of the bomb fragments as well.\nProperties.\nTheatre and film.\nSoho is near the heart of London's theatre area. It is home to the Soho Theatre, built in 2000 to present new plays and comedy.\nThe Windmill Theatre was based on Great Windmill Street, and was named after a windmill at this location that was demolished in the 18th century. It initially opened as the Palais de Luxe in 1910 as a small cinema, but was unable to compete with larger venues and was converted into a theatre by Howard Jones. It re-opened in December 1931, but was still unsuccessful. In 1932, the general manager Vivian Van Damm introduced a non-stop variety show throughout the afternoon and evening. It was famous for its nude \"tableaux vivants\", in which the models had to remain motionless to avoid the censorship laws then in place. The theatre claimed that, aside from a compulsory closure between 4 and 16 September 1939, it was the only theatre in London which did not close during the Second World War; this led it to use the slogan \"We never closed\". Several prominent comedians including Harry Secombe, Jimmy Edwards and Tony Hancock began their careers at the Windmill. It closed on 31 October 1964 and was again turned into a cinema.\nThe Raymond Revuebar at No.\u00a011 Walker's Court was a small theatre specialising in striptease and nude dancing. It was owned by Paul Raymond and opened in 1958. The facade supported a brightly lit sign declaring it to be the \"World Centre of Erotic Entertainment\". Raymond subsequently bought the lease of the Windmill and ran it as a \"nude entertainment\" venue until 1981. The upstairs became known as the Boulevard Theatre and in 1980 was adopted as a comedy club called \"The Comic Strip\" by a small group of alternative comedians including Rik Mayall, Dawn French, Jennifer Saunders, Alexei Sayle and Adrian Edmondson, before they found wider recognition with the series \"The Comic Strip Presents\" on Channel 4. The name and control of the theatre (but not the property itself) were bought by Raymond's business associate G\u00e9rard Simi in 1996. The theatre suffered financial difficulties owing to increasing rent, leading to its closure in 2004. It became a gay bar and cabaret venue called Too2Much; in 2005, Elton John staged a joint bachelor party there with his longtime partner David Furnish in anticipation of their civil partnership. The venue was subsequently renamed to the Soho Revue Bar, but closed in 2009.\nSoho is a centre of the independent film and video industry as well as the television and film post-production industry. Audio post duo White Lightning (Robbie Weston and Rick Dzendzera) opened two audio post-production facilities in different parts of Soho between 1978 and 1987: Silk Sound at 13 Berwick Street, and The Bridge Facilities at 55-57 Great Marlborough Street. Silk Sound was acquired by Bubble TV in 2010, and was rebranded under Bubble's banner in 2018, while The Bridge went defunct altogether in 2009. White Lightning also opened a third studio at 16 Dufours Place, named Space Facilities, in late 1995. However, Space closed in 2008, just a year before The Bridge did. Twentieth Century House in Soho Square was built in 1937 for Twentieth Century Fox. Since 1947, Soho has also been home to De Lane Lea Studios, which is currently owned by Warner Bros. The British Board of Film Classification, formerly known as the British Board of Film Censors, has been based in Soho Square since 1950. Soho's key fibre communications network has been managed by Sohonet since 1995, which connects the Soho media and post-production community to British film studios such as Pinewood and Shepperton, along with other locations worldwide include HBO and Warner Brothers. In the 2010s, research commissioned by Westminster City Council showed 23 per cent of the workforce in Soho worked in the creative industries.\nRestaurants and clubs.\nMany small and easily affordable restaurants and cafes were established in Soho during the 19th century, particularly as a result of Greek and Italian immigration. The restaurants were not looked upon favourably at first, but their reputation changed at the start of the 20th century. In 1924, a guide reported \"of late years, the inexpensive restaurants of Soho have enjoyed an extraordinary vogue.\" Arthur Ransome's \"Bohemia in London\" (1907) mentions Old and New Soho, including details about Soho coffee-houses including The Moorish Caf\u00e9 and The Algerian.\nKettner's was a restaurant on Romilly Street, established in 1867 by Napoleon III's chef Auguste Kettner. It was frequently visited by Albert, Prince of Wales (where he is alleged to have dined with his mistress, Lillie Langtry) and Oscar Wilde. The restaurant survived both World Wars without incident, and was regularly visited by Agatha Christie and Bing Crosby.\nIn the 20th century, several Soho pubs and private members clubs gained notoriety for both their proprietors and clientele. Clive Jennings says of regular clientele such as Jeffrey Barnard and Francis Bacon that \"the lethal triangle of The French, The Coach &amp; Horses and The Colony were the staging points of the Dean Street shuffle, with occasional forays into other joints such as The Gargoyle or the Mandrake\u00a0... The Groucho or Blacks\". Christopher Howse notes of the coterie of bohemian heavy drinkers that \"There was no worry about pensions in Soho. People didn't live that long.\"\nThe Gargoyle Club opened at 69 Dean Street in 1925. It was founded by the socialite the Hon David Tennant as a place where writers, artists and musicians could mingle with the upper crust and eat and drink at affordable prices for the next three decades. In May 1979 the Gargoyle's uppermost room started hosting a weekly club-night on Saturdays called the Comedy Store, which made the reputations of many of the UK's upcoming \"alternative comedians\". Among the original lineup here were Alexei Sayle, Rik Mayall and Adrian Edmondson who broke away in 1980 to establish The Comic Strip team at Raymond's Revue Bar, before they found wider recognition with the series \"The Comic Strip Presents\" on Channel 4. The Gargoyle's success and Bohemian clientele led to other restaurants being founded around Soho, including the Eiffel Tower and Bellotti's.\nDuring the 1970s the building at 69 Dean Street housed another nightspot in its cellars, initially known as Billy's, and run by Soho's only Jamaican club owner, Vince Howard. The Blitz Kids, a group of London clubgoers who spearheaded the New Romantic movement in the early 1980s, originally met at Billy's. The club changed its name to Gossip's and became part of London's clubland heritage by spawning several weekly club-nights that influenced British music and fashion during the 1980s.\nGerrard Street is the centre of London's Chinatown, and along with Lisle Street and Little Newport Street, house a mix of import companies, oriental food shops and restaurants. Street festivals are held throughout the year, particularly on the Chinese New Year. In March 2022, Cadbury opened a temporary vegan chocolate shop at 15 Bateman Street.\nRadio.\nSoho Radio is an internet radio station on Great Windmill Street, next to the Windmill Theatre. Since May 2014 it has been streaming live and pre-recorded programming from its premises, which also function as a retail space and coffee shop. The station states on its website that it aims \"to reflect the culture of Soho through our vibrant and diverse content\". There is no playlist policy, and presenters are allowed to play any music they like. In 2016, it was voted the world's best radio station at Mixcloud's Online Radio Awards.\nReligion.\nSoho is home to numerous religious and spiritual groups. St Anne's Church on Wardour Street was built between 1677 and 1686, possibly to the design of Sir Christopher Wren or William Talman. An additional tower was built in 1717 by Talman and reconstructed in 1803. The church was damaged by a V1 flying bomb during World War II in 1940, but the tower survived. In 1976, John Betjeman campaigned to save the building. The church was fully restored in the late 1980s and formally re-opened by the Princess Royal on 12 March 1990. The Church of Our Lady of the Assumption and St Gregory on Warwick Street was built in 1788 and is the only remaining 18th-century Roman Catholic embassy chapel in London and principal church of the Personal Ordinariate of Our Lady of Walsingham. St Patrick's Church in Soho Square was built in 1792 to accommodate Irish immigrants who had moved to the area.\nOther religious buildings in Soho include the Hare Krishna Temple off Soho Square, which was part-funded by George Harrison and opened in 1979. There exists a small mosque on Berwick Street. The French Protestant Church of London, the only one of its kind in the city and constructed in the Flemish Gothic style, has been at Nos.\u00a08\u20139 Soho Square since 1893.\nMusic.\nThe music scene in Soho can be traced back to 1948 and Club Eleven, generally regarded as the first venue where modern jazz, or bebop, was performed in the UK. It closed in 1950 following a drugs raid. The Harmony Inn was a hang-out for musicians on Archer Street operating during the 1940s and 1950s.\nThe Ken Colyer Band's 51 Club, a venue for traditional jazz, opened on Great Newport Street in 1951. Blues guitarist and harmonica player Cyril Davies and guitarist Bob Watson launched the London Skiffle Centre, London's first skiffle club, on the first floor of the Roundhouse pub on Wardour Street in 1952. It was renamed the London Blues and Barrelhouse Club in the late 1950s, and closed in 1964.\nIn the early 1950s, Soho became the centre of the beatnik culture in London. The first coffee bar to open was Moka at No.\u00a029 Frith Street. It was formally opened in 1953 by the film star Gina Lollobrigida, and the frothed coffee produced from stainless steel machines was pioneering in British culture. \"Le Macabre\" on Wardour Street, had coffin-shaped tables, fostered beat poetry, jive dance and political debate. The Goings On, in Archer Street, was a Sunday afternoon club organised by the beat poet Pete Brown, active in the mid-1960s. For the rest of the week, it operated as an illegal gambling den. Pink Floyd played at the club at the beginning of their career.\nThe 2i's Coffee Bar was one of the first rock clubs in Europe. It initially opened on No.\u00a044 Gerard Street in 1956, but soon moved to its more famous venue of No.\u00a059 Old Compton Street. Soho quickly became the centre of the fledgling rock scene in London. Clubs included the Flamingo Club, a regular gig for Georgie Fame, Ronan O'Rahilly's The Scene, which opened in 1963 and catered for the Mod movement with regular attendees including Steve Marriot and Andrew Loog Oldham, and jazz clubs like Ronnie Scott's, which opened in 1959 at 39 Gerrard Street and moved to 47 Frith Street in 1965.\nSoho's Wardour Street was the home of the Marquee Club, which opened in 1958. In the 1960s, numerous major rock bands played at the venue, including early performances from the Rolling Stones in July 1962 and The Who in late 1964, Jimi Hendrix, David Bowie, Led Zeppelin, Pink Floyd, Jethro Tull, AC/DC and Iron Maiden. Eric Clapton and Brian Jones both lived for a time in Soho, sharing a flat with future rock publicist, Tony Brainsby.\nTrident Studios was based at 17 St Anne's Court, Soho and was a major London recording studio. It was established by Norman and Barry Sheffield in 1968, who wanted to expand from the small studio they had above their music shop. It became immediately successful after The Beatles decided to record several tracks on \"The White Album\" there, as the facilities were better than Abbey Road studios. Queen were originally managed by the Sheffields, and recorded their first four albums at Trident. Other artists who recorded at Trident include David Bowie, Elton John, Free and Thin Lizzy. It closed as a general-purpose recording studio in 1981, but has since reopened in various guises, including providing sound and mixing services for television.\nAlthough technically not part of Soho, the adjacent Denmark Street is known for its connections with British popular music, and is nicknamed the British Tin Pan Alley due to its large concentration of shops selling musical instruments. The Sex Pistols lived beneath No.\u00a06 and recorded their first demos there. Jimi Hendrix, the Rolling Stones and David Bowie have all recorded at studios on Denmark Street and Elton John wrote his hit \"Your Song\" in the street. Led Zeppelin's first rehearsal in 1968 was in a basement studio on Gerrard Street.\nSex industry.\nThe Soho area has been at the heart of London's sex industry for more than 200 years; between 1778 and 1801, 21 Soho Square was location of the White House, a brothel described by the magistrate Henry Mayhew as \"a notorious place of ill-fame\". Shortly before World War I, two rival gangs, one led by Chan Nan (also called \"Brilliant Chang\") and the other by Eddie Manning, controlled drugs and prostitution in Soho. Both were eventually arrested and imprisoned; Manning died midway through a three-year sentence in 1933. Following World War II, gangs set up rings of prostitutes in the area, concentrated around Brewer Street and Rupert Street. Photographers also visited Soho in the hope of being able to blackmail people caught in the act of visiting prostitutes.\nWhen the Street Offences Act 1959 drove prostitution off the streets, many clubs such as the Blue Lagoon at No.\u00a050 Carnaby Street became fronts for it. Gangs controlled the clubs and the prostitutes, and the police were bribed. In 1960 London's first sex cinema, the Compton Cinema Club (a members-only club to get around the law), opened at 56 Old Compton Street. It was owned by Michael Klinger and Tony Tenser who later produced two early Roman Polanski films, including \"Repulsion\" (1965). As post-war austerity relaxed into the \"swinging '60s\", clip joints also surfaced; these unlicensed establishments sold coloured water as champagne with the promise of sex to follow, thus fleecing tourists looking for a \"good time\". Harrison Marks, a \"glamour photographer\" and girlie magazine publisher, had a photographic gallery on Gerrard Street and published several magazines in the 1950s and '60s. The model Pamela Green prompted him to take up nude photography, and she remained the creative force in their business.\nBy the 1970s, the sex shops had grown from the handful opened by Carl Slack in the early 1960s. From 1976 to 1982, Soho had 54 sex shops, 39 sex cinemas and cinema clubs, 16 strip and peep shows, 11 sex-oriented clubs and 12 licensed massage parlours. The proliferation of sex shops dissuaded some people from visiting Soho. The growth of the sex industry in Soho during this time was partly caused by corruption in the Metropolitan Police. The vice squad at the time suffered from police officers enforcing against organised crime in the area, while simultaneously accepting bribes. This changed following the appointment of Robert Mark as chief constable, who began to crack down on corruption. In 1972 local residents started the Soho Society in order to control the increasing expansion of the sex industry in the area and improve it with a comprehensive redevelopment plan. This led to a series of corruption trials in 1975, following which several senior police officers were imprisoned. This caused a small recession in Soho which depressed property values at the time Paul Raymond had started buying freeholds there.\nBy the 1980s, purges of the police force along with pressure from the Soho Society and new and tighter licensing controls by the City of Westminster led to a crackdown on illegal premises. The number of sex industry premises dropped from 185 in 1982 to around 30 in 1991. By 2000, substantial relaxation of general censorship, the ready availability of non-commercial sex, and the licensing or closing of unlicensed sex shops had reduced the red-light area to just a small area around Berwick Street. Much of the business has been reported to have been run by Albanian gangs. By the end of 2014, gentrification and competition from the internet had reduced the number of flats in Soho used for prostitution (see Soho walk-up), but the area remains a red-light district and a centre of the sex industry in London.\nHealth and welfare.\nThe National Hospital for Diseases of the Heart and Paralysis was established at No.\u00a032 Soho Square in 1874. The property had previously been owned by the naturalist and botanist Sir Joseph Banks. It moved to Westmoreland Street in 1914, and then to Fulham Road in 1991.\nIn July 2019, Soho was reported to be the unhealthiest place to live in Britain. Researchers from the University of Liverpool found that the area had the greatest access to takeaways, pubs and off-licences and these were combined with high levels of air pollution and low levels of parks and green spaces.\nStreets.\nBerwick Street was built between 1687 and 1703, and is probably named after James FitzJames, 1st Duke of Berwick, the illegitimate son of James II of England. It has held a street market since the early 18th century, which has been formally organised since 1892.\nCarnaby Street was laid out in the late 17th century. It was named after Karnaby House, built on the street's eastern side in 1683. It was a popular residence for Huguenots at first, before becoming populated by shops in the 19th century. In 1957, a fashion boutique was opened, and Carnaby Street became the fashion centre of 1960s Swinging London, although it quickly became known for poor quality \"kitsch\" products.\nD'Arblay Street was laid out between 1735 and 1744. It was originally known as Portland Street after William Bentinck, 2nd Duke of Portland, and given its current named in 1909 in commemoration of Frances Burney, Madame D'Arblay, who had lived in Poland Street nearby, when she was young. The George public house at No.\u00a01 was opened in 1889, but there has been a tavern on this site since 1739. Several of the original houses have survived into the 21st century.\nDean Street was built in the 1680s and was originally settled by French immigrants. It is home to the Soho Theatre and a pub known as The French House, which during the Second World War was popular with the French government-in-exile. Karl Marx lived at No.\u00a064 Dean Street around 1850. The Colony Club was founded by Muriel Belcher and based at No.\u00a041 Dean Street from 1948 to 2008. It was frequented by several important artists including Francis Bacon, Lucian Freud and Frank Auerbach.\nFrith Street was named after Richard Frith, a local builder. On Rocque's Map of London it is marked as Thrift Street. It was a popular aristocratic residence, although without as many foreign residents as some other streets. A plaque above the stage door of the Prince Edward Theatre identifies the site where Mozart lived as a child between 1764 and 1765. John Logie Baird first demonstrated television in his laboratory at No\u00a022 in 1926; the site is now the location of Bar Italia. Ronnie Scott's Jazz Club is located at No\u00a046 Frith Street, having moved there from Gerrard Street in 1965.\nGreek Street was first laid out around 1680 and was named after a nearby Greek church. It initially housed several upper-class tenants including Arthur Annesley, 5th Earl of Anglesey, and Peter Plunket, 4th Earl of Fingall. Thomas De Quincey lived in the street after running away from Manchester Grammar School in 1802. Josiah Wedgwood ran his main pottery warehouse and showrooms at Nos.\u00a012\u201313 between 1774 and 1797. The street now mostly contains restaurants, and several historical buildings from the early 18th century are still standing.\nGerrard Street was built between 1677 and 1685 on land, called the Military Ground, which was owned by Charles Gerard, 1st Earl of Macclesfield. The initial development contained a large house belonging to the Earl of Devonshire, which was subsequently occupied by Charles Montagu, 4th Earl of Manchester, Thomas Wharton, Baron Wharton, and Richard Lumley, 1st Earl of Scarbrough. Several foreign restaurants had become established on Gerrard Street by the end of the 19th century, including the Hotel des Etrangers and the Mont Blanc. Ronnie Scott's Jazz Club opened at 39 Gerrard Street in 1959 and remained there until its move to No\u00a047 Frith Street in 1965. Scott kept 39 Gerrard Street open for up-and-coming British jazz musicians (referred to as 'the Old Place') until the lease ran out in 1967. The 43 Club was based on Gerrard Street. It was one of the most notorious clubs in Soho, run as a cover for organised crime and illegal after-hours selling of alcohol; following a police investigation, the owner Kate Meyrick was jailed in 1928. During the 1950s, the cheap rents on Gerrard Street attracted Chinese Londoners, many who moved from Poplar. By 1970, the street had become the centre of London's Chinatown, and it became pedestrianised and decorated with a Chinese gateway and lanterns. It continues to host numerous Chinese restaurants and shops into the 21st century.\nGolden Square is a garden square to the southwest of Soho. Built over land formerly used for grazing, its name is a corruption of gelding. Building began in 1675 and it was complete by the early 18th century. It was originally home to several upper-class residents, including Barbara Villiers, Duchess of Cleveland, James Brydges (later to become 1st Duke of Chandos), and Henry St John, 1st Viscount St John. By the mid-18th century the aristocracy had moved west towards Mayfair, and a number of foreign embassies were established around the square. In the 19th century, it became a popular residence for local musicians and instrument makers, while by the 20th it had become an established centre of woollen merchants. A statue of King George II sits in the centre of the square, designed by John Van Nost and constructed in 1753.\nGreat Marlborough Street was first laid out in the early 18th century, and named after the military commander John Churchill, 1st Duke of Marlborough. The street was initially fashionable and was home to numerous peers. The London College of Music was based at No.\u00a047 from 1896 to 1990, while the department store Liberty is on the corner with Regent Street. The street was the location of Philip Morris's original London factory and gave its name to the Marlboro brand of cigarettes. Marlborough Street Magistrates Court was based at No.\u00a020\u201321 and had become one of the country's most important magistrates courts by the late 19th century. The Marquess of Queensberry's libel trial against Oscar Wilde took place here in 1895. The Rolling Stones' Mick Jagger and Keith Richards were tried for drugs possession at the court in 1967, with fellow band member Brian Jones being similarly charged a year later.\nGreat Windmill Street was named after a 17th-century windmill on this location. The Scottish anatomist William Hunter opened his anatomical theatre at No.\u00a016 in 1766, running it until his death in 1783. It continued to be used for anatomical lectures until 1831. The principles of The Communist Manifesto were laid out by Karl Marx in 1850 at a meeting in the Red Lion pub.\nOld Compton Street is named after the Bishop of London Henry Compton, and was first laid out in the 1670s, fully developed by 1683. During the late 18th and 19th centuries, it became a popular meeting place for French exiles. The street was the birthplace of Europe's rock club circuit (2i's club) and contained the first adult cinema in England (The Compton Cinema Club). Dougie Millings, who was the famous tailor for The Beatles, had his first shop at 63 Old Compton Street, which opened in 1962. Old Compton Street is now the core of London's main gay village, where there are several businesses catering for the gay community.\nPoland Street was named for the King of Poland pub which stood at one end. It was destroyed by a bomb in 1940. Henry Howard, 6th Earl of Suffolk lived at No.\u00a015 in 1717, while Percy Bysshe Shelley briefly lodged in the street in 1811. William Blake lived at No.\u00a028 from 1785 to 1791, and wrote several works there, including \"Songs of Innocence\" and \"The Book of Thel\". The street was the site of the St James Workhouse whose infirmary is believed to be the predecessor to the St. James Infirmary. The first multi-storey car park in London opened on Poland Street in 1934.\nWardour Street dates back to 1585, when it was known as Commonhedge Lane and connected Oxford Street to the King's Mews (now part of Trafalgar Square). It began to be built up in the 1680s and was named after local landowner Edward Wardour. Most of the original houses were rebuilt in the 18th century, and the street became known for antiques and furniture dealers. Several music publishers were established along Wardour Street in the early 20th century, including Novello and Co at Nos.\u00a0152\u2013160. The most famous location of the Marquee Club was at No.\u00a090 Wardour Street between 1964 and 1988. Another seventies rock hangout was The Intrepid Fox pub (at 97/99 Wardour Street), originally dedicated to Charles James Fox (who is featured on a relief on the outside of the building). It subsequently became a haven for the Goth subculture before closing in 2006.\nCultural references.\nA detailed mural depicting Soho characters, including writer Dylan Thomas and jazz musician George Melly, is in Broadwick Street, at the junction with Carnaby Street.\nIn fiction, Robert Louis Stevenson had Dr. Henry Jekyll set up a home for Edward Hyde in Soho in his novel, \"Strange Case of Dr Jekyll and Mr Hyde\". Charles Dickens referred to Soho in several of his works; in \"A Tale of Two Cities\", Lucie Manette and her father Dr. Alexandre Manette live on Soho Square, while Golden Square is mentioned in \"Nicholas Nickleby\", in which Ralph Nickleby has a house on the square, and the George II statue in the centre is described as \"mournful\". Joseph Conrad used Soho as the home for \"The Secret Agent\", a French immigrant who ran a pornography shop. Dan Kavanagh (Julian Barnes)'s 1980 novel \"Duffy\" is set in Soho.\nLee Ho Fook's is mentioned in Warren Zevon's song \"Werewolves of London\". The Who song \"Pinball Wizard\", also covered by Elton John, contains the line \"From Soho down to Brighton, I must've played them all\", in reference to the locations frequented by the title character. The Pogues 1986 song \"A Rainy Night in Soho\", written and sung by Shane MacGowan (and covered by Bono and Johnny Depp) frames night-time in the district. \"Lola\" by The Kinks sees the story-teller of the song meeting a transgender woman by the name of Lola in a club in Soho.\nThe area is the setting for the 2021 film \"Last Night in Soho\" by Edgar Wright.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39617", "revid": "1317270866", "url": "https://en.wikipedia.org/wiki?curid=39617", "title": "Orders, decorations, and medals of the United Kingdom", "text": "In the United Kingdom and the British Overseas Territories, personal bravery, achievement, or service are rewarded with honours. The honours system consists of three types of award:\nAppointments to the various orders and awards of other honours are usually published in \"The London Gazette\".\nBrief history.\nAlthough the Anglo-Saxon monarchs are known to have rewarded their loyal subjects with rings and other symbols of favour, it was the Normans who introduced knighthoods as part of their feudal government. The first English order of chivalry, the Order of the Garter, was created in 1348 by Edward III. Since then, the system has evolved to address the changing need to recognise other forms of service to the United Kingdom.\nSystem.\nAs the head of state, the Sovereign is the fount of honour, but the system for identifying and recognising candidates to honour has changed considerably over time. Various orders of knighthood have been created (see below) as well as awards for military service, bravery, merit, and achievement which take the form of decorations or medals. Most medals are not graded. Each one recognises specific service so there are normally set criteria which must be met. These criteria may include a period of time and will often delimit a particular geographic region. Medals are not normally presented by the Sovereign. A full list is printed in the \"order of wear\", published (infrequently) by the \"London Gazette\".\nOrders of honours.\nHonours are split into classes (\"orders\") and are graded to distinguish different degrees of achievement or service, according to various criteria. Nominations are reviewed by honours committees made up of government officials and private citizens from different fields, who meet twice a year to discuss the candidates and make recommendations for appropriate honours to be awarded by the Sovereign.\nSelection and nomination.\nNew Year and Birthday honours.\nA list of approximately 1,350 names is published twice a year, at the New Year and on the date of the Sovereign's (official) birthday. Since decisions are inevitably subjective, the twice-yearly honours lists often provoke criticism from those who feel strongly about particular cases. Candidates are identified by public or private bodies, by government departments, or are nominated by members of the public. Depending on their roles, those people selected by the honours committee are submitted either to the Prime Minister, Secretary of State for Foreign, Commonwealth and Development Affairs, or Secretary of State for Defence for their approval before being sent to the Sovereign for final approval. Certain honours are conferred solely at the Sovereign's discretion, such as appointments to the Order of the Garter, the Order of the Thistle, the Royal Victorian Order, and the Order of Merit. The honours' insignia are then presented by the Sovereign or his representative at investitures held at Buckingham Palace, Windsor Castle or the Palace of Holyroodhouse; Prince Charles, Prince William and Princess Anne deputised for Queen Elizabeth II with William and Anne continuing to do so for King Charles III since his September 2022 accession.\nPrime Minister's Resignation Honours.\nBy convention, a departing prime minister is allowed to nominate Prime Minister's Resignation Honours, to reward political and personal service. In recent history, only Tony Blair and Gordon Brown have not taken up this privilege (although Brown did issue the 2010 Dissolution Honours).\nSpecial Honours.\nAs part of the British honours system, Special Honours are issued at the Monarch's pleasure at any given time. The Special Honours refer to the awards made within royal prerogative, operational honours, political honours and other honours awarded outside the New Years Honours and Birthday Honours.\nOrders of chivalry.\nCurrent orders.\nThe current system is made up of six orders of chivalry and four orders of merit. The statutes of each order specify matters such as the size of the order, the use of post-nominal letters and insignia design and display. These are ordered by the order of wear.\nDormant orders.\nOrders were created for particular reasons at particular times. In some cases these reasons have ceased to have any validity and orders have fallen into abeyance, primarily due to the decline of the British Empire during the twentieth century. Reforms of the system have sometimes made other changes. For example, the British Empire Medal temporarily ceased to be awarded in the UK in 1993, as was the companion level award of the Imperial Service Order (although its medal is still used). The British Empire Medal was revived, however, in 2012 with 293 BEMs awarded for the 2012 Birthday Honours, and has continued to be awarded in some other Commonwealth nations.\nThe Order of St Patrick was founded in 1783 by George III for the Kingdom of Ireland, and after the Acts of Union 1800 continued for Irish peers in the United Kingdom of Great Britain and Ireland. After the Irish Free State's secession in 1922, only members of the royal family were appointed to the order, the last in 1936. The last surviving knight was Prince Henry, Duke of Gloucester, who died on 10 June 1974. Although dormant, the order technically still exists, and may be used as an award at any time.\nOrders relating to the British Raj or the British Indian Empire are also dormant. The senior order, the Order of the Star of India, was divided into three grades, Knight Grand Commander, Knight Commander and Companion, of which the first and highest was conferred upon the Princes and Chiefs of Indian states and upon important British civil servants working in India. Women were not eligible to receive the award. The junior order, the Order of the Indian Empire, was divided into the same ranks and also excluded women. The third order, the Order of the Crown of India, was used exclusively to honour women. Its members, all sharing a single grade, consisted of the wives and close female relatives of Indian Princes or Chiefs; the Viceroy or Governor-General; the Governors of Bombay, Madras and Bengal; the Principal Secretary of State for India; and the Commander-in-Chief in India. Upon Indian independence in 1947, appointments to all these orders ceased.\nHH Maharaja Tej Singh Prabhakar Bahadur of Alwar, who was a KCSI and the last surviving member of the Order of the Star of India, died in February 2009, aged 97. The last surviving member of the Order of the Indian Empire, HH Maharaja Meghrajji III of Dhrangadhra-Halvad, a KCIE died in August 2010, aged 87. Queen Elizabeth II was appointed to the Order of the Crown of India (then as Princess Elizabeth) and was the last surviving former member of that order until her death in September 2022, aged 96.\nThe Order of Burma was created in May 1940 by King George VI of the United Kingdom to recognise subjects of the British colony of Burma (Myanmar) after it became a distinct colony, separate from British India. This order had one class which entitled the member to the postnominal letters OB but no title. It was originally intended to reward long and faithful service by military and police. In 1945 the Royal Warrant was altered to allow for membership for acts of gallantry as well as meritorious service. The Order was one of the rarest awarded with only 33 appointments by the time appointments were discontinued in 1948 when Burma declared independence.\nOrders no longer associated with the British monarch.\nThe Royal Guelphic Order, also known as the Hanoverian Guelphic Order, was an honour founded by George, Prince Regent in the name of his father King George III in 1815. In the United Kingdom, it was used only briefly, until the death of William IV in 1837. That is when the personal union with the Kingdom of Hanover ended due to the Hanover throne succession, which followed Salic Law, unlike the United Kingdom, where women could inherit the throne. The order continued for some time as a national order of Hanover, until the Kingdom of Hanover\u2019s defeat against Prussia and forced dissolution in 1866. Since then, it has been a house order to be awarded by the House of Hanover. The order's current head is Ernst August Prinz von Hanover, head of the House of Hanover. The Order includes two Divisions: Civil and Military. During the personal union of the United Kingdom and Hanover, it originally had three classes. Today, after several reorganizations since 1841, it is a house order with four classes and an additional Cross of Merit.\nDecorations.\nCurrent decorations.\nCurrent awarded decorations in order of wear:\n\u2021 In abeyance.\nOn 1 July 2009, BBC News reported that the Queen had approved a new posthumous award, the Elizabeth Cross, honouring members of the armed forces killed in action or by terrorist attack since World War II. The cross itself is given to the deceased's family.\nInactive decorations.\nThese decorations have not been awarded since the Independence of India in 1947.\nOther honours and appointments.\nHereditary peerage.\nThere are five ranks of hereditary peerage: duke, marquess, earl, viscount, and baron. Until the mid-20th century, peerages were usually hereditary. Until the end of the 20th century, English, Scottish, British, and UK peerages (except, until very recent times, those for the time being held by women) carried the right to a seat in the House of Lords.\nHereditary peerages are now normally given only to Royal Family members. The most recent were the grants to: Queen Elizabeth II's youngest son, Prince Edward, Earl of Wessex, on his marriage in 1999; the Queen's grandson Prince William, made the Duke of Cambridge on the morning before his marriage to Catherine Middleton on 29 April 2011; and the Queen's grandson Prince Harry, made the Duke of Sussex on the morning before his marriage to Meghan Markle on 19 May 2018. No hereditary peerages were granted to commoners after the Labour Party came to power in 1964, until Margaret Thatcher tentatively reintroduced them by two grants to men with no sons in 1983: Speaker of the House of Commons George Thomas and former deputy prime minister William Whitelaw. Both titles died with their holders. She followed this with an earldom in 1984 for former prime minister Harold Macmillan not long before his death, reviving a traditional honour for former prime ministers. Macmillan's grandson succeeded him on his death in 1986. No hereditary peerages have been created since, and Thatcher's own title was a life peerage (see further explanation below). The concession of a baronetcy (i.e., hereditary knighthood), was granted to her husband Denis following her resignation (explained below).\nHereditary peerages are not \"honours under the crown\" and cannot normally be withdrawn. A peerage can be revoked only by a specific Act of Parliament, and then only for the current holder, in the case of hereditary peerages. A hereditary peer can disclaim his peerage for his own lifetime under Peerage Act 1963 within a year of inheriting the title.\nLife peerage.\nModern life peerages were introduced under the Appellate Jurisdiction Act 1876, following a test case, the \"Wensleydale Peerage Case\" (1856), which established that non-statutory life peers would not have the right to sit in the House of Lords. At that time, life peerages were intended only for law lords, who would introduce appellate legal expertise into the chamber without conferring rights on future generations, which might not have legal experts.\nSubsequently, under the Life Peerages Act 1958, life peerages became the norm for all new grants outside the Royal Family. This was viewed as a modest reform of the second legislative chamber. However, its effects were gradual because hereditary peers and their successors retained their rights to attend and vote with the life peers. All hereditary peers, except 92 chosen in a secret ballot of all hereditary peers, have now lost their rights to sit in the second chamber. All hereditary peers, however, retain dining rights to the House of Lords, viewed as \"the best club in London\".\nAll life peers hold the rank of baron and automatically have the right to sit in the House of Lords. The title exists only for the duration of their own lifetime and is not passed to their heirs, although the children of life peers enjoy the same courtesy titles as those of hereditary peers. Some life peerages are created as an honour for achievement, some for the specific purpose of introducing legislators from the various political parties (known as working peers), and some under the Appellate Jurisdiction Act 1876, with a view to judicial work. There is a discrete number appointed as \"People's Peers\", recommended by the general public. Twenty-six Church of England bishops have a seat in the House of Lords.\nAs a life peerage is not technically an \"honour under the Crown\", it cannot normally be withdrawn once granted. Thus, while knighthoods have been withdrawn as \"honours under the Crown\", convicted criminals who have served their sentences have returned to the House of Lords. In the case of Lord Archer of Weston-super-Mare, he has chosen only to exercise dining rights and has not spoken in Parliament since released from his perjury conviction.\nBaronetcy.\nA baronetcy is the lowest hereditary title in the United Kingdom. It carries the title sir. In order of precedence, a Baronetcy is below a Barony but above most knighthoods. Baronetcies are not peerages. When a baronetcy becomes vacant on the holder's death, the heir is required to register the proofs of succession if he wishes to be addressed as \"Sir\". The Official Roll of Baronets is kept at the Ministry of Justice, transferred from the Home Office in 2001, by the Registrar of the Baronetage. Anyone who considers that he is entitled to be entered on the roll may petition the Crown through the Lord Chancellor. Anyone succeeding to a baronetcy must exhibit proofs of succession to the Lord Chancellor. A person who is not entered on the roll will not be addressed or mentioned as a baronet or accorded precedence as a baronet, effectively declining the honour. The baronetcy can be revived at any time on provision of acceptable proofs of succession. As of 2017, 208 baronetcies are listed as presumedly not extinct but awaiting proofs of succession.\nAs with hereditary peerages, baronetcies generally ceased to be granted after the Labour Party came to power in 1964. The sole subsequent exception was a baronetcy created in 1990 for the husband of Margaret Thatcher, Sir Denis Thatcher, later inherited by their son Mark Thatcher.\nKnighthood.\nDescended from medieval chivalry, knights exist both within the orders of chivalry and in a class known as \"Knights Bachelor\". Regular recipients include High Court judges and, to a lesser extent, Chief Constables of larger police forces. Knighthood carries the title \"Sir\"; the female equivalent \"Dame\" exists only within the orders of chivalry\u2014Dame Commander of the Order of the British Empire (DBE) is usually awarded as an equivalent of a Knight Bachelor.\nOrder of St John.\nThe Most Venerable Order of the Hospital of St John of Jerusalem established in 1888 with a royal charter, is a royal order of chivalry dedicated to charitable work, notably through St John Ambulance. Operating independently of the UK government under the patronage of the monarch as Sovereign Head, the Order\u2019s ranks include Bailiff or Dame Grand Cross (GCStJ), Knight or Dame of Justice or Grace (KStJ/DStJ), Commander (CStJ), Officer (OStJ), and Member (MStJ). Members may wear the Order\u2019s insignia, including those of Bailiff or Dame Grand Cross, as recognized in the UK order of wear. However, these ranks do not confer official rank in the order of precedence, and post-nominal initials (e.g., GCStJ, KStJ) are strictly unofficial and should never be used outside the Order's internal correspondence. Recipients of the highest grades are not addressed as \u201cSir\u201d or \u201cDame\u201d.\nOther orders.\nOther British and Commonwealth orders, decorations, and medals exist that do not carry titles but entitle the holder to place post-nominal letters after his or her name, as do a small number of Royal Family Orders.\nBritish honours in the Commonwealth realms.\nUntil the mid-20th century, the British honours system was the primary\u2014and generally the sole\u2014honours system utilised across the various dominions and territories of the British Empire, which became the Commonwealth of Nations from 1949. Today, British honours and decorations continue to be awarded to citizens of Commonwealth realms and British Overseas Territories. Within the Commonwealth realms, however, the nature of the British honour or decoration awarded and the permissibility of its conferment varies from government to government. The British honours system comprises both dynastic honours, the personal gift of the Sovereign, and British state honours or decorations (known as imperial honours or decorations outside the UK), which are not. Dynastic honours continue to be conferred by the Sovereign across the Commonwealth realms, although outside the United Kingdom they are typically non-titular honours, such as the Order of Merit or the lower grades of the Royal Victorian Order.\nIn 1917 and 1919, the Canadian House of Commons passed the Nickle Resolutions, which although non-binding gradually ended the conferment of titular honours\u2014peerages, baronetcies, and knighthoods\u2014to Canadians. Occasional conferments of knighthoods (in 1934 and 1935) and imperial honours, notably after the Second World War, continued until 1955, when the Canadian government officially ended all awards of imperial honours to Canadians. In 1967, Canada established its own honours system with the Order of Canada, created its own system of bravery decorations in 1972, and its own system of military decorations for valour in 1993. Canadian service personnel remained eligible to receive the imperial Victoria Cross until 1993, when a Canadian version was instituted. Canadian citizens remain eligible for imperial honours, including peerages and knighthoods, presumably for achievements within Great Britain, however such conferment may involve individual approval by the Canadian government.\nIn 1975, the Australian government established its own honours system, creating the Order of Australia and its own system of bravery decorations. Indigenous police and fire service decorations followed from 1986, with a new system of military decorations created in 1991. Imperial honours continued to be conferred on Australians through 1989, when the last recommendations were made. With effect from 5 October 1992, the Australian government discontinued the awarding of imperial honours.\nIn 1975, New Zealand also instituted its first indigenous honour, the Queen's Service Order, followed by the Order of New Zealand in 1987. In 1996, it replaced imperial honours with the New Zealand Order of Merit, and replaced imperial gallantry and bravery decorations with New Zealand decorations in 1999.\nOther Commonwealth realms have continued to apply the imperial honours system together with their own honours systems. Jamaica established its own honours system in 1969, passing the \"National Honours and Awards Act.\" Saint Lucia followed in 1980; the Solomon Islands in 1981; Belize in 1991; Antigua and Barbuda in 1998; Papua New Guinea in 2004; and Grenada in 2007. The government of the Bahamas continues to solely use imperial honours. In 2007, it passed a National Honours Act establishing a Bahamian honours system; however, it had not come into effect as of 2015.\nIn practice, legislation across the Commonwealth realms regulating the awarding of imperial honours to citizens of a realm, including knighthoods and damehoods, does not necessarily prevent a citizen of a Commonwealth realm from receiving a substantive award of an imperial honour for service in the United Kingdom or to its government. There continue to be numerous examples of Canadians, New Zealanders, and Australians who the British government have honored and can use the honour or its accompanying style when they are resident in their own nation or the United Kingdom.\nHonorary awards.\nCitizens of countries that do not have the King as their head of state sometimes have honours conferred upon them, in which case the awards are \"honorary\". In the case of knighthoods, the holders are entitled to place initials after their name but not style themselves \"Sir\" as they are not entitled to receive the accolade. Examples of foreigners with honorary knighthoods are Billy Graham, Bill Gates, Bob Geldof, Bono, Steven Spielberg, John Williams and Rudolph Giuliani, while Ars\u00e8ne Wenger and G\u00e9rard Houllier are honorary OBEs. Honorary knighthoods are of Orders of Chivalry rather than as Knights Bachelor as the latter confers no postnominal letters.\nRecipients of honorary awards who later become subjects of His Majesty may apply to convert their awards to substantive ones. Examples of this are Marjorie Scardino, American CEO of Pearson PLC, and Yehudi Menuhin, the American-born violinist and conductor. They were granted an honorary damehood and knighthood respectively while still American citizens, and converted them to substantive awards after they assumed British nationality, becoming Dame Marjorie and Sir Yehudi. Menuhin later accepted a life peerage with the title Lord Menuhin.\nSir Tony O'Reilly, who holds both British and Irish nationality, uses the style \"Sir\", but has also gained approval from the Irish Government to accept the award as is necessary under the Irish Constitution. Elisabeth Schwarzkopf, the German soprano, became entitled to be known as \"Dame Elisabeth\" when she took British nationality. Irish-born Sir Terry Wogan was initially awarded an honorary knighthood, but by the time he collected the accolade from the Queen in December 2005, he had obtained dual nationality and the award was upgraded to a substantive knighthood.\nBob Geldof is often erroneously referred to as \"Sir Bob\"; he is not entitled to this style as an honorary knight, as he is a citizen of the Republic of Ireland (i.e., he is not a citizen of a Commonwealth realm).\nThere is no law in the UK preventing foreigners from holding a peerage (e.g., Newburgh), though only Commonwealth and Irish citizens may sit in the House of Lords. This has yet to be tested under the new arrangements. However, some other countries have laws restricting the acceptances of awards granted to would-be recipients by foreign powers. In Canada, where the House of Commons of Canada (but not the Senate of Canada) has opposed the granting of titular honours with its (non-binding) Nickle Resolution, then Prime Minister Jean Chr\u00e9tien advised the Queen not to grant Conrad Black a titular honour while he remained a Canadian citizen (see \"Black v Chr\u00e9tien\").\nCeremony.\nEach year, around 2,600 people receive their awards personally from the monarch or another member of the Royal Family. The majority of investitures take place at Buckingham Palace, but an annual ceremony also takes place at the Palace of Holyroodhouse in Edinburgh (during Holyrood Week), and some happen at Windsor Castle. There are approximately 120 recipients at each Investiture. In recent years the King, the Prince of Wales and The Princess Royal have all held investitures.\nDuring the ceremony, the monarch enters the ballroom of Buckingham Palace attended by two Gurkha orderly officers, a tradition begun in 1876 by Queen Victoria. On duty on the dais are five members of the King's Body Guard of the Yeomen of the Guard, which was created in 1485 by Henry VII; they are the oldest, but not most senior, military corps in the United Kingdom. Three Lady or Gentleman Ushers are on duty to help look after the recipients and their guests.\nThe King or his representative is escorted by either the Lord Chamberlain or the Lord Steward. After the national anthem has been played, he stands to the right of the King and announces the name of each recipient and the achievement for which they are being decorated. The King or his representative is provided with a brief background for each recipient by their equerry as they approach to receive their decorations.\nMen who are to be knighted kneel on an investiture stool to receive the accolade, which the King bestows. Elizabeth II used the sword used by her father, George VI as Duke of York and Colonel of the Scots Guards. Only men are knighted. Women receive their honours in the same fashion as men receiving decorations or medals, even if they are receiving a damehood. Occasionally an award for gallantry may be made posthumously and in this case, the King or his representative presents the decoration or medal to the recipient's next-of-kin in private before the public investiture begins. The Elizabeth Cross was created especially for this purpose.\nAfter the investiture ceremony, those honoured are ushered out of the ballroom into the Inner Quadrangle of Buckingham Palace, where the royal rota's photographers are stationed. Here recipients are photographed with their awards. In some cases, members of the press may interview some of the more well-known people who have been honoured.\nRefusal.\nIn 2003, \"Sunday Times\" published a list of almost 300 people who had declined an honour between 1951 and 1999. In 2020, \"the Guardian\" reported based on a Freedom of Information request, that the number of people refusing an honour had more than doubled in the previous nine years.\nRevocation.\nHonours are sometimes revoked (forfeited), for example if a recipient is subsequently convicted of a serious criminal offence. The Honours Forfeiture Committee is an \"ad hoc\" committee convened under the chairmanship of the Head of the Home Civil Service, to consider cases where information has been received which indicates an individual is unsuitable to hold an award. Sometimes the original decision to grant an honour was made on the basis of inaccurate information (including through manipulation of the public nominations system), but normally cases relate to actions that took place after the award was made. Recommendations are made to the monarch of the United Kingdom, who has the sole authority to rescind an honour.\nIn 2009, Gordon Brown confirmed that the process remains as set out in 1994 by the then Prime Minister John Major in a written answer to the House of Commons:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The statutes of most orders of knighthood and the royal warrants of decorations and medals include provision for the Queen to \"cancel and annul\" appointments and awards. Cancellation is considered in cases where retention of the appointment or award would bring the honours system into disrepute. There are no set guidelines for cancellations, which are considered on a case-by-case basis. Since 1979, the \"London Gazette\" has published details of cancellations of 15 appointments and awards\u2014three knighthoods, one CBE, five OBEs, four MBEs and two BEMs.\nIn October 2016, the House of Commons approved a motion to ask the Honours Forfeiture Committee to strip Sir Philip Green of his knighthood for his role in the downfall of British Home Stores. It was the first time MPs voted to recommend rescinding a knighthood.\nOrder of Wear.\nHonours, decorations and medals are arranged in the \"Order of Wear\", an official list which describes the order in which they should be worn. Updates to the Order of Wear are published in \"The London Gazette\" when necessary. The current Order of Wear was published on 11 January 2019. Additional information on the social events at which an award may be worn is contained in the insignia case given to each recipient.\nThe list places the Victoria Cross and George Cross at the top, followed by the orders of knighthood arranged in order of date of creation. Below the Knights of the Garter and Thistle, individuals of a higher rank precede those of a lower rank. For instance, a Knight Grand Cross of any order precedes any Knight Commander. For those of equal rank, members of the higher-ranked Order take precedence. Within the same Order, precedence is accorded to that individual who received the honour earlier.\nNot all orders have the same number of ranks. The Order of Merit, the Order of the Companions of Honour, the Distinguished Service Order and the Imperial Service Order are slightly different, being single-rank honours, and have been placed at appropriate positions of seniority. The precedence of Knight Bachelor is below the knights of the different orders and above those with the rank of Commander or lower.\nDecorations are followed by medals of various categories, being arranged in date order within each section. These are followed by Commonwealth and honorary foreign awards of any level. Miscellaneous details are explained in notes at the bottom of the list.\nThe order of wear is not connected to and should not be confused with the order of precedence.\nStyle.\nFor peers, see forms of address in the United Kingdom.\nFor baronets, the style \"Sir John Smith, Bt\" (or \"Bart\") is used. Their wives are styled \"Lady Smith\". A baronetess is styled \"Dame Jane Smith, Btss\".\nFor knights, the style \"Sir John Smith [postnominals]\" is used, attaching the proper postnominal letters depending on rank and order (for knights bachelor, no postnominal letters are used). Their wives are styled \"Lady Smith\", with no postnominal letters. A dame is styled \"Dame Jane Smith, [postnominals]\". More familiar references or oral addresses use the first name only, e.g. \"Sir John\", or \"Dame Joan\".\nWives of knights and baronets are styled \"Lady Smith\", although customarily no courtesy title is automatically reciprocated to male consorts.\nRecipients of orders, decorations and medals receive no styling of \"Sir\" or \"Dame\", but they may attach the according postnominal letters to their name, e.g., \"John Smith, VC\". Recipients of gallantry awards may be referred to in Parliament as \"gallant\", in addition to \"honourable\", \"noble\", etc.: \"The honourable and gallant Gentleman\".\nBailiffs or Dames Grand Cross (GCStJ), Knights/Dames of Justice/Grace (KStJ/DStJ), Commander Brothers/Sisters (CStJ), Officer Brothers/Sisters (OStJ), Serving Brothers/Sisters (SBStJ/SSStJ), and Esquires (EsqStJ) of the Order of St John do not receive any special styling with regards to prenominal address i.e. Sir or Dame. They may, however, attach the relevant postnominal initials (solely) within internal correspondence of the Order. In the Priory of Australia, Canada and the United States, the rank of Serving Brother/Sister is no longer granted. The rank now awarded is referred to as Member of the Order of St John for both men and women.\nFor honours bestowed upon those in the entertainment industry (e.g., Anthony Hopkins, Maggie Smith), it is an accepted practice to omit the title for professional credits.\nReform.\nReforms of the system occur from time to time. In the last century notable changes to the system have included a Royal Commission in 1925 following the scandal in which Prime Minister David Lloyd George was found to be selling honours. The sale of British Honours, including titles, is now prohibited by the Honours (Prevention of Abuses) Act 1925. \nA significant set of reforms were enacted in 1993 under the Conservative Prime Minister John Major, with the intention of providing a more merits-based system for honouring service and achievement. Among other outcomes, this removed the rank distinctions that were explicitly linked to certain military awards, and ended (with the exception of High Court Judges) the awarding of \"ex officio\" honours. The reforms also provided for people to be nominated for an honour by members of the public, as well as placing more emphasis within the system on recognising and rewarding voluntary service in particular.\nIn July 2004, the Public Administration Select Committee (PASC) of the House of Commons and, concurrently, Sir Hayden Phillips, Permanent Secretary at the Department of Constitutional Affairs, both concluded reviews of the system. The PASC recommended some radical changes; Sir Hayden concentrated on issues of procedure and transparency. In February 2005, the Government responded to both reviews by issuing a Command paper detailing which of the proposed changes it had accepted. These included diversifying and opening up the system of honours selection committees for the Prime Minister's list and also the introduction of a miniature badge. Furthermore, the 2004 review suggested a regular report on the transparency and operation of the system at a suggested frequency of every 3 years. These reviews have taken place in https://, https://, https://, https:// and https://. The most recent review covers the period 2019 to 2023 and shows a notable shift towards community and voluntary recognition, and a continued emphasis on ensuring that the Honours System reflects British societal diversity.\nAs of 2012, same-sex civil partners of individual recipients of British honours are not themselves granted honours by way of their partnered relation. In July 2012, Conservative MP Oliver Colvile introduced a private member's bill, titled \"Honours (Equality of Titles for Partners) Bill\", to amend the honours system to both allow husbands of those made dames and for civil partners of recipients to receive honours by their relationship statuses.\nIn May 2020, a special COVID nomination process was introduced to provide a more streamlined consideration process and accessible route for the public to nominate those in society who had responded to the COVID-19 pandemic. Compared with the existing public nomination form, it required substantially less information in order to be accepted and processed. This process was ended in May 2022.\nControversies.\nA scandal in the 1920s was the sale by Maundy Gregory of honours and peerages to raise political funds for David Lloyd George.\nIn 1976, the Harold Wilson era was mired by controversy over the 1976 Prime Minister's Resignation Honours, which became known as the \"Lavender List\".\nIn 2006, \"The Sunday Times\" newspaper revealed that every donor who had given \u00a31,000,000 or more to the Labour Party since 1997 was given a Knighthood or a Peerage (see Cash-for-Honours scandal). Moreover, the government had given honours to 12 of the 14 individuals who have donated more than \u00a3200,000 to Labour and of the 22 who donated more than \u00a3100,000, 17 received honours. An investigation by the Crown Prosecution Service did not lead to any charges being made.\n\"The Times\" published an analysis of the recipients of honours in December 2015 which showed that 46% of those getting knighthoods and above in 2015 had been to fee-paying public schools. In 1955 it was 50%. Only 6.55% of the population attends such schools. 27% had been to Oxford or Cambridge universities (18% in 1955).\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "39619", "revid": "19257869", "url": "https://en.wikipedia.org/wiki?curid=39619", "title": "Electroporation", "text": "Method in molecular biology to make pores in cell membranes\nElectroporation, also known as electropermeabilization, is a microbiological and biotechnological technique in which an electric field is applied to cells to briefly increase the permeability of the cell membrane. The application of a high-voltage electric field induces a temporary destabilization of the lipid bilayer, resulting in the formation of nanoscale pores that permit the entry or exit of macromolecules. \nThis method is widely employed to introduce molecules\u2014including small molecules, DNA, RNA, and proteins\u2014into cells. Electroporation can be performed on cells in suspension using electroporation cuvettes, or directly on adherent cells \"in situ\" within their culture vessels.\nIn microbiology, electroporation is frequently utilized for the transformation of bacteria or yeast cells, often with plasmid DNA. It is also used in the transfection of plant protoplasts and mammalian cells. Notably, electroporation plays a critical role in the \"ex vivo\" manipulation of immune cells for the development of cell-based therapies, such as CAR T-cell therapy. Moreover, \"in vivo\" applications of electroporation have been successfully demonstrated in various tissue types.\nBulk electroporation confers advantages over other physical delivery methods, including microinjection and gene gun techniques. However, it is limited by reduced cell viability. To address these issues, researchers have developed miniaturized approaches such as micro-electroporation and nanotransfection. These techniques utilize nanochannel-mediated electroporation to deliver molecular cargo to cells in a more controlled and less invasive manner. \nAlternative methods for intracellular delivery include the use of cell-penetrating peptides, cell squeezing techniques, and chemical transformation, with selection depending on the specific cell type and cargo characteristics.\nElectroporation is also employed to induce cell fusion. A prominent application of cell fusion is hybridoma technology, where antibody-producing B lymphocytes are fused with immortal myeloma cell lines to produce monoclonal antibodies.\nLaboratory research.\nElectroporation is widely utilized in laboratory settings due to its ability to achieve high transformation efficiencies, particularly for plasmid DNA, with reported yields approaching 1010 colony-forming units per microgram of DNA. Electroporation is generally more costly than chemical transformation methods due to the specialized equipment required. This includes electroporators\u2014devices designed to generate controlled electrostatic fields for cell suspension\u2014and electroporation cuvettes, which are typically constructed from glass or plastic and contain parallel aluminum electrodes.\nA standard bacterial transformation protocol involves several steps. First, electro-competent cells are prepared by washing to remove ions that could cause arcing. These cells are then mixed with plasmid DNA and transferred into an electroporation cuvette. A high-voltage electric pulse is applied, with specific parameters such as voltage and pulse duration tailored to the particular cell type being used. Following electroporation, recovery medium is added, and the cells are incubated at an appropriate temperature to allow for outgrowth. Finally, the cells are plated onto selective agar plates to assess transformation efficiency.\nThe success of electroporation depends on several factors, including the purity of the plasmid DNA solution, salt concentration, and electroporation parameters. High salt concentrations can lead to arcing (electrical discharge), significantly reducing the viability of electroporated cells. Therefore, the electroporation conditions must be optimized for each cell type to achieve an effective balance between cell viability and DNA uptake.\nIn addition to \"in vitro\" applications, electroporation is employed \"in vivo\" to enhance cell membrane permeability during injections and surgical procedures. The effectiveness of \"in vivo\" electroporation depends greatly on selected parameters such as voltage, pulse duration, and number of pulses. Developing central nervous systems are particularly suitable for \"in vivo\" electroporation, as ventricles provide clear visibility for nucleic acid injections, and dividing cells exhibit increased permeability. Electroporation of embryos injected in utero is performed through the uterine wall, often using forceps-type electrodes to minimize embryo damage.\nHistory.\nResearchers in the 1960s discovered that applying an external electric field would create a large membrane potential at the two poles of a cell. In the 1970s, it was found that when a critical membrane potential is reached, the cellular membrane would break down and subsequently recover. By the 1980s, this temporary membrane breakdown was exploited to introduce various molecules into cells. \n\"In vivo\" gene electroporation was first described in 1991. This method delivers a large variety of therapeutic genes for the potential treatment of several diseases, including immune disorders, tumors, metabolic disorders, monogenetic diseases, cardiovascular diseases, and analgesia.\nRegarding irreversible electroporation, the first successful treatment of malignant cutaneous tumors implanted in mice was accomplished in 2007 by a group of scientists who achieved complete tumor ablation in 12 of 13 mice. They accomplished this by sending 80 pulses of 100 microseconds at 0.3 Hz with an electrical field magnitude of 2500 V/cm to treat the cutaneous tumors.\nThe first group to apply electroporation used a reversible procedure in conjunction with impermeable macromolecules. The first research on how nanosecond pulses might be used on human cells was published in 2003.\nMedical applications.\nThe first medical application of electroporation was used for introducing poorly permeant anti-cancer drugs into tumor nodules. Gene electro-transfer soon became of interest because of its low cost, ease of implementation, and alleged safety. Viral vectors have since been found to have limitations in terms of immunogenicity and pathogenicity when used for DNA transfer.\nIrreversible electroporation is being used and evaluated as cardiac ablation therapy to kill specific areas of heart muscle. This is done to treat irregularities of heart rhythm. A cardiac catheter delivers trains of high-voltage, ultra-rapid electrical pulses that form irreversible pores in cell membranes, resulting in cell death.\nN-TIRE.\nNon-thermal irreversible electroporation (N-TIRE) is a technique that treats many different types of tumors and other unwanted tissue. This procedure is done using small electrodes (about 1mm in diameter), placed either inside or surrounding the target tissue to apply short, repetitive bursts of electricity at a predetermined voltage and frequency. These bursts of electricity increase the resting transmembrane potential (TMP) so that nanopores form in the plasma membrane. When the electricity applied to the tissue is above the electric field threshold of the target tissue, the cells become permanently permeable from the formation of nanopores. As a result, the cells are unable to repair the damage and die due to a loss of homeostasis. N-TIRE is unique to other tumor ablation techniques in that it does not create thermal damage to the tissue around it.\nReversible electroporation.\nIn contrast, reversible electroporation occurs when the electricity applied with the electrodes is below the target tissue's electric field threshold. Because the electricity applied is below the cells' threshold, it allows the cells to repair their phospholipid bilayer and continue with their normal cell functions. Reversible electroporation is typically done with treatments that involve inserting a drug or gene (or other molecule that is not normally permeable to the cell membrane) into the cell. Not all tissues have the same electric field threshold; therefore, to improve safety and efficacy, careful calculations need to be made prior to a treatment.\nN-TIRE, when done correctly, only affects the target tissue. Proteins, the extracellular matrix, and critical structures such as blood vessels and nerves are all unaffected and left healthy by this treatment. This facilitates a more rapid replacement of dead tumor cells and a faster recovery.\nImaging technology such as CT scans and MRIs are commonly used to create a 3D image of the tumor. Computed tomography is used to help with the placement of electrodes during the procedure, particularly when the electrodes are being used to treat tumors in the brain.\nThe procedure takes five minutes with a high success rate. It may be used for future treatment in humans. One disadvantage of using N-TIRE is that the electricity delivered from the electrodes can stimulate muscle cells to contract, which could have lethal consequences, depending on the situation. Therefore, a paralytic agent must be used when performing the procedure. The paralytic agents that have been used in such research have risks when using anesthetics.\nH-FIRE.\nHigh-frequency irreversible electroporation (H-FIRE) uses electrodes to apply bipolar bursts of electricity at a high frequency, as opposed to unipolar bursts of electricity at a low frequency. This type of procedure has the same tumor ablation success as N-TIRE. However, it has one distinct advantage: H-FIRE does not cause muscle contraction in the patient, and therefore, there is no need for a paralytic agent. Furthermore, H-FIRE has been demonstrated to produce more predictable ablations due to the lesser difference in the electrical properties of tissues at higher frequencies.\nDrug and gene delivery.\nElectroporation can also be used to help deliver drugs or genes into the cell by applying short and intense electric pulses that transiently permeabilize cell membrane, thus allowing the transport of molecules otherwise not transported through a cellular membrane. This procedure is referred to as electrochemotherapy when the molecules to be transported are chemotherapeutic agents or gene electrotransfer when the molecule to be transported is DNA. Scientists from Karolinska Institute and the University of Oxford use electroporation of exosomes to deliver siRNAs, antisense oligonucleotides, chemotherapeutic agents, and proteins specifically to neurons after injecting them systemically (in blood). Because these exosomes can cross the blood-brain barrier, this protocol could solve the problem of poor delivery of medications to the central nervous system and may potentially treat Alzheimer's disease, Parkinson's disease, and brain cancer, among other conditions.\nResearch has shown that shock waves could be used for pre-treating the cell membrane prior to electroporation. This synergistic strategy has shown to reduce external voltage requirement and create larger pores. Also, application of shock waves allow scope to target desired membrane site. This procedure allows to control the size of the pore.\nPhysical mechanism.\nElectroporation allows cellular introduction of large highly charged molecules, such as DNA, that cannot passively diffuse across the hydrophobic bilayer core. This phenomenon indicates that the mechanism is the creation of nm-scale water-filled holes in the membrane. Electropores were optically imaged in lipid bilayer models like droplet interface bilayers and giant unilamellar vesicles, while addition of cytoskeleton proteins such as actin networks to the giant unilamellar vesicles seem to prevent the formation of visible electropores. Experimental evidences for actin networks in regulating the cell membrane permeability has also emerged. Although electroporation and dielectric breakdown both result from application of an electric field, the mechanisms involved are fundamentally different. In dielectric breakdown the barrier material is ionized, creating a conductive pathway. The material alteration is thus chemical in nature. In contrast, during electroporation the lipid molecules are not chemically altered but simply shift position, opening up a pore which acts as the conductive pathway through the bilayer as it is filled with water.\nElectroporation is a dynamic phenomenon that depends on the local transmembrane voltage at each point on the cell membrane. It is generally accepted that for a given pulse duration and shape, a specific transmembrane voltage threshold exists for the manifestation of the electroporation phenomenon (from 0.5 V to 1 V). This leads to the definition of an electric field magnitude threshold for electroporation (Eth). That is, only the cells within areas where E\u2267Eth are electroporated. If a second threshold (Eir) is reached or surpassed, electroporation will compromise the viability of the cells, \"i.e.\", irreversible electroporation (IRE).\nElectroporation is a process with several distinct phases. First, a short electrical pulse is applied. Typical parameters would be 300\u2013400 mV for &lt; 1 ms across the membrane (note- the voltages used in cell experiments are typically much larger because they are being applied across large distances to the bulk solution so the resulting field across the actual membrane is only a small fraction of the applied bias). Application of this potential causes migration of ions from the surrounding solution to the membrane which charges like a capacitor. Rapid localized rearrangements in lipid morphology occur once the critical level is achieved. The resulting structure is believed to be a \"pre-pore\" since it is not electrically conductive but leads rapidly to the creation of a conductive pore. Evidence for the existence of such pre-pores comes mostly from the \"flickering\" of pores, which suggests a transition between conductive and insulating states. It has been suggested that these pre-pores are small (~3 \u00c5) hydrophobic defects. If this theory is correct, then the transition to a conductive state could be explained by a rearrangement at the pore edge, in which the lipid heads fold over to create a hydrophilic interface. Finally, these conductive pores can either heal, resealing the bilayer or expand, eventually rupturing it. The resultant fate depends on whether the critical defect size was exceeded which in turn depends on the applied field, local mechanical stress and bilayer edge energy.\nGene electroporation.\nApplication of electric pulses of sufficient strength to the cell causes an increase in the trans-membrane potential difference, which provokes the membrane destabilization. Cell membrane permeability is increased, and otherwise non-permeant molecules enter the cell. Although the mechanisms of gene electrotransfer are not yet fully understood, it was shown that the introduction of DNA only occurs in the part of the membrane facing the cathode and that several steps are needed for successful transfection: electrophoretic migration of DNA towards the cell, DNA insertion into the membrane, translocation across the spoke membrane, migration of DNA towards the nucleus, transfer of DNA across the nuclear envelope and finally gene expression. There are a number of factors that can influence the efficiency of gene electrotransfer, such as: temperature, parameters of electric pulses, DNA concentration, electroporation buffer used, cell size and the ability of cells to express transfected genes. In \"in vivo\" gene electrotransfer, DNA diffusion through extracellular matrix, properties of tissue, and overall tissue conductivity may be crucial.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "39620", "revid": "16185737", "url": "https://en.wikipedia.org/wiki?curid=39620", "title": "Freddie Prinze", "text": "American actor and comedian (1954\u20131977)\nFreddie Prinze (born Frederick Karl Pruetzel; June 22, 1954 \u2013 January 29, 1977) was an American stand-up comedian and actor, and the star of the NBC-TV sitcom \"Chico and the Man\" from 1974 until his death in 1977. He was described in a \"Vulture\" magazine article as \"having blown up like no other comedian in history.\" Prinze is the father of actor Freddie Prinze Jr.\nEarly life.\nPrinze was born Frederick Karl Pruetzel (German spelling: Pr\u00fctzel) at Saint Clare's Hospital in Manhattan, New York City, the son of Maria de Gracia Pruetzel (n\u00e9e Graniela y Ramirez) and Edward Karl Pruetzel. His mother was a Puerto Rican Catholic and his father was a German Lutheran immigrant who had arrived in the U.S. as a youth in 1934. Prinze was raised in a mixed neighborhood in Washington Heights, New York City. When Prinze was a small child, his mother enrolled him in ballet classes to deal with a weight problem. Without telling his parents, Prinze successfully auditioned for the High School of Performing Arts, where he was introduced to drama and continued ballet\u2014and where he discovered his gift for comedy while entertaining crowds in the boys' restroom. He dropped out of school in his senior year to become a stand-up comedian.\nCareer.\nPrinze worked at several comedy clubs in New York City, including The Improv and Catch a Rising Star, where he introduced himself to audiences as a \"Hunga-rican\" (part Hungarian, part Puerto Rican). Although his mother was in fact Puerto Rican, his father was a German immigrant with Hungarian ancestry. Prinze's son, Freddie Prinze Jr., has stated many times that his father was half German/half Puerto Rican. This is also verified by census records as well as Pr\u00fctzel/Pruetzel family accounts. For the sake of his budding comedic career, he legally changed his surname to \"Prinze\". According to his friend David Brenner, Prinze originally wanted to be known as the king of comedy, but since Alan King already had that last name and sobriquet, he would be the prince of comedy instead. During 1973, Prinze made his first television appearance on one of the last episodes of \"Jack Paar Tonite\". In December 1973, his biggest break came with an appearance on \"The Tonight Show Starring Johnny Carson\". Prinze was the first young comedian to be asked to have a sit-down chat with Carson on his first appearance. Prinze appeared on and guest-hosted \"The Tonight Show\" on several other occasions. He also appeared on\" The Midnight Special\" to perform his comic routine.\nFrom September 1974 until his death in January 1977, Prinze starred as Francisco \"Chico\" Rodriguez in the NBC TV series \"Chico and the Man\" with Jack Albertson. The show was an instant hit. Prinze made several appearances on \"The Dean Martin Celebrity Roasts\", most notably the roasts for Sammy Davis Jr. and Muhammad Ali. In 1975, he released a comedy album that was taped live at Mister Kelly's in Chicago entitled \"Looking Good\"\u2014his catch phrase from \"Chico and the Man\". In 1976, he starred in a made-for-TV movie, \"The Million Dollar Rip-Off\". Prinze had a little-known talent for singing, examples of which could be heard in the background of the title song of the Tony Orlando and Dawn album \"To Be With You\", in his appearances on their variety show, and on rare occasions on his own sitcom. About four months before his death, Prinze signed a five-year deal with NBC worth $6 million.\nPersonal life.\nOn October 13, 1975, Prinze married Katherine \"Kathy\" Elaine (Barber) Cochran, with whom he had one child, son Freddie Prinze Jr., who was born March 8, 1976. Prinze was arrested for driving under the influence of Quaalude on November 16, 1976. A few weeks later his wife filed for divorce.\nPrinze had been romantically linked to actresses Raquel Welch and Pam Grier, whom he met in 1973. Grier recalls their relationship in chapter nineteen of her memoir, \"My Life in Three Acts\". After his break-up with Grier, Prinze dated actress Lonette McKee for a time during 1976. Prinze also dated Joanna Kerns a short time before he died. The two had worked together on the 1976 TV movie \"The Million Dollar Rip-Off\".\nPrinze was very close friends with singer Tony Orlando; Orlando appeared on \"Chico and the Man\", and Prinze appeared on Orlando's variety show. As he started to make more money, Prinze took martial arts lessons from Robert Wall, a student of Bruce Lee, who appeared in \"Enter the Dragon\" and \"The Way of the Dragon\". Soon after, Wall became godfather to Prinze's newborn son.\nDeath.\nPrinze suffered from depression. On the night of January 28, 1977, after talking on the telephone with his estranged wife, Prinze received a visit from his business manager, Marvin \"Dusty\" Snyder. During the visit, Prinze put a gun to his head and shot himself. He had purchased this gun in the presence of fellow actors Jimmie \"JJ\" Walker and Alan Bursky, although it was long rumored that Bursky gave him the gun. Prinze was rushed to the UCLA Medical Center and placed on life support following emergency surgery. His family removed him from life support, and he died at 1 p.m. on January 29.\nPrinze made farewell phone calls to numerous family members and friends prior to shooting himself and left a note stating that he had decided to kill himself. In 1977, the death was ruled a suicide. However, in a 1983 civil case brought by his mother, wife, and son against Crown Life Insurance Company, the jury found that his death was medication-induced and accidental, which enabled the family to collect $200,000.00 in life insurance. This followed a $1,000,000.00 out of court settlement with his psychiatrist and doctor to end a malpractice suit for allowing him access to a gun and overprescribing him Quaalude (as a tranquilizer).\nPrinze is interred at Forest Lawn Memorial Park in the Hollywood Hills of Los Angeles, near his father, Edward Karl Pruetzel. His son, Freddie Prinze Jr., who was under one year old when his father died, did not speak publicly about his father's death until he discussed it in the documentary \"Misery Loves Comedy\" (2015), directed by Kevin Pollak.\nLegacy.\nPrinze's mother wrote a book about her son, \"The Freddie Prinze Story\" (1978). \n\"Can You Hear the Laughter? The Story of Freddie Prinze\" (September 1979) is a TV biopic detailing Prinze\u2019s life. Prinze's life and death were a focal point of one of the storylines in the movie \"Fame\" (1980), set in Prinze's alma mater, the LaGuardia High School of Performing Arts. \nPrinze received a star on the Hollywood Walk of Fame on December 14, 2004, honoring his contribution to the television industry. Actor and comedian George Lopez has acknowledged that he paid for Prinze's star.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
